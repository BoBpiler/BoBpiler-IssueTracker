### Total Bugs Detected: 4649
### Current Chunk: 18 of 30
### Bugs in this Chunk: 160 (From bug 2721 to 2880)
---


### compiler : `gcc`
### title : `Failure to optimize separated multiplications by x and square of x`
### open_at : `2020-05-29T19:53:28Z`
### last_modified_date : `2023-08-24T21:38:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95423
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int a, int b, int x)
{
    return a * (x * x) + (b * x);
}

This can be optimized to `return (((a * x) + b) * x);`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to optimize division with numerator of 1`
### open_at : `2020-05-29T20:21:50Z`
### last_modified_date : `2023-08-24T21:36:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95424
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
unsigned f(unsigned x)
{
    return 1 / x;
}

This can be optimized to `return (x == 1);`, and for `int` to `return (unsigned)(x + 1) < 3 ? x : 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize bit twiddling on statics that does nothing`
### open_at : `2020-05-29T22:33:18Z`
### last_modified_date : `2023-08-24T21:36:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95425
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
uint32_t f()
{
    static uint32_t x = 0;
    x = (x >> 16) | (x << 16);
    return x;
}

This can be optimized to `return 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to avoid emitting rbp initialization when doing 256-bit memory store`
### open_at : `2020-05-29T23:59:38Z`
### last_modified_date : `2023-08-24T21:36:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95427
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
void f(int64_t *p)
{
    p[0] = 0;
    p[1] = 0;
    p[2] = 0;
    p[3] = 0;
}

With -O3 -max, LLVM produces this : 

f(long*): # @f(long*)
  vxorps xmm0, xmm0, xmm0
  vmovups ymmword ptr [rdi], ymm0
  vzeroupper
  ret

GCC produces this :

f(long*):
  push rbp
  vpxor xmm0, xmm0, xmm0
  vmovdqu XMMWORD PTR [rdi], xmm0
  vextractf128 XMMWORD PTR [rdi+16], ymm0, 0x1
  mov rbp, rsp
  vzeroupper
  pop rbp
  ret


---


### compiler : `gcc`
### title : `Failure to completely optimize simple compare after operations`
### open_at : `2020-05-30T03:16:40Z`
### last_modified_date : `2023-08-24T21:36:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95433
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
auto f(int x)
{
    return (2 * x) + 5 == 3;
}

This can be optimized to `x == -1`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to reuse flag from float compare`
### open_at : `2020-05-30T20:17:15Z`
### last_modified_date : `2023-08-24T21:36:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95441
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int f(double x)
{
    return (x < 0) - (x > 0);
}

With -Ofast, GCC outputs this :

f(double):
  pxor xmm1, xmm1
  xor eax, eax
  comisd xmm1, xmm0
  seta al
  xor edx, edx
  comisd xmm0, xmm1
  seta dl
  sub eax, edx
  ret

LLVM outputs this :

f(double):
  xorpd xmm1, xmm1
  xor eax, eax
  xor ecx, ecx
  ucomisd xmm0, xmm1
  setb al
  seta cl
  sub eax, ecx
  ret


---


### compiler : `gcc`
### title : `LRA inserts a reload insn for REG_DEAD register`
### open_at : `2020-05-30T21:50:14Z`
### last_modified_date : `2021-12-22T10:13:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95442
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
match_reload has

  if (find_reg_note (curr_insn, REG_UNUSED, out_rtx) == NULL_RTX)
    {    
      start_sequence ();
      lra_emit_move (out_rtx, copy_rtx (new_out_reg));
      emit_insn (*after);
      *after = get_insns ();
      end_sequence ();
    }    

For

(insn 68 67 69 10 (parallel [
            (set (reg:CC 17 flags)
                (if_then_else:CC (ne (reg:DI 104 [ _1 ])
                        (const_int 0 [0]))
                    (compare:CC (mem:BLK (reg/v/f:DI 86 [ s2 ]) [0 MEM <char[1:(sizetype) _1]> [(void *)s2_5(D)]+0 A8])
                        (mem:BLK (reg/v/f:DI 85 [ s1 ]) [0 MEM <char[1:(sizetype) _1]> [(void *)s1_6(D)]+0 A8]))
                    (const_int 0 [0])))
            (use (const_int 8 [0x8]))
            (use (reg:CC 17 flags))
            (clobber (reg/f:DI 102 [ s2 ]))
            (clobber (reg/f:DI 103 [ s1 ]))
            (clobber (reg:DI 104 [ _1 ]))
        ]) "foo.c":9:7 1011 {*cmpstrnqi_1}
     (expr_list:REG_DEAD (reg:DI 104 [ _1 ])
        (expr_list:REG_UNUSED (reg:DI 104 [ _1 ])
            (expr_list:REG_UNUSED (reg/f:DI 103 [ s1 ])
                (expr_list:REG_UNUSED (reg/f:DI 102 [ s2 ])
                    (expr_list:REG_EQUAL (if_then_else:CC (ne (reg:DI 82 [ _1 ])
                                (const_int 0 [0]))
                            (compare:CC (mem:BLK (reg/v/f:DI 86 [ s2 ]) [0 MEM <char[1:(sizetype) _1]> [(void *)s2_5(D)]+0 A8])
                                (mem:BLK (reg/v/f:DI 85 [ s1 ]) [0 MEM <char[1:(sizetype) _1]> [(void *)s1_6(D)]+0 A8]))
                            (const_int 0 [0]))
                        (nil)))))))
(insn 69 68 70 10 (set (reg:QI 105)
        (gtu:QI (reg:CC 17 flags)
            (const_int 0 [0]))) "foo.c":9:7 732 {*setcc_qi}
     (nil))

LRA inserts a reload insn for REG_DEAD (reg:DI 104 [ _1 ] after it.  Should
it be

diff --git a/gcc/lra-constraints.c b/gcc/lra-constraints.c
index bf6d4a2fd4b..570ee37e34d 100644
--- a/gcc/lra-constraints.c
+++ b/gcc/lra-constraints.c
@@ -1068,7 +1068,8 @@ match_reload (signed char out, signed char *ins, signed char *outs,
     return;
   /* See a comment for the input operand above.  */
   narrow_reload_pseudo_class (out_rtx, goal_class);
-  if (find_reg_note (curr_insn, REG_UNUSED, out_rtx) == NULL_RTX)
+  if (find_reg_note (curr_insn, REG_UNUSED, out_rtx) == NULL_RTX
+      && find_reg_note (curr_insn, REG_DEAD, out_rtx) == NULL_RTX)
     {
       start_sequence ();
       lra_emit_move (out_rtx, copy_rtx (new_out_reg));


---


### compiler : `gcc`
### title : `cmpstrn peepholes are out of date`
### open_at : `2020-05-31T01:34:19Z`
### last_modified_date : `2020-06-02T07:06:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95447
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
cmpstrn peepholes are out of date.  i386.md has

;; Peephole optimizations to clean up after cmpstrn*.  This should be
;; handled in combine, but it is not currently up to the task. 
;; When used for their truth value, the cmpstrn* expanders generate
;; code like this:
;;
;;   repz cmpsb
;;   seta       %al
;;   setb       %dl
;;   cmpb       %al, %dl
;;   jcc        label
;;
;; The intermediate three instructions are unnecessary.

But GCC no longer generates such code sequence.  Instead, we generate

[hjl@gnu-cfl-2 pr95151]$ cat s1.i
extern void bar (void);

void
func (char *d, unsigned int l)
{
 if (__builtin_strncmp (d, "foo", l))
  bar ();
}
[hjl@gnu-cfl-2 pr95151]$ gcc -O2 -minline-all-stringops -S s1.i
[hjl@gnu-cfl-2 pr95151]$ cat s1.s
	.file	"s1.i"
	.text
	.section	.rodata.str1.1,"aMS",@progbits,1
.LC0:
	.string	"foo"
	.text
	.p2align 4
	.globl	func
	.type	func, @function
func:
.LFB0:
	.cfi_startproc
	cmpl	$4, %esi
	movl	%esi, %ecx
	movl	$4, %eax
	movq	%rdi, %r8
	cmova	%rax, %rcx
	movl	$.LC0, %edi
	movq	%r8, %rsi
	cmpq	%rcx, %rcx
	repz cmpsb
	seta	%al
	sbbb	$0, %al
	testb	%al, %al
	jne	.L4
	ret
	.p2align 4,,10
	.p2align 3
.L4:
	jmp	bar
	.cfi_endproc
.LFE0:
	.size	func, .-func
	.ident	"GCC: (GNU) 10.1.1 20200507 (Red Hat 10.1.1-1)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-2 pr95151]$ 

The cmpstrn peepholes never kick in.


---


### compiler : `gcc`
### title : `Missing optimization: pointer untag, re-tag should be no-op`
### open_at : `2020-05-31T01:37:00Z`
### last_modified_date : `2023-10-25T18:59:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95448
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Consider the following code. make_vector2() ought to be equivalent to just "return make_vector_val()", and under Clang (10.0), it is. But GCC generates this code for make_vector2:

make_vector2:
        sub     rsp, 8
        call    make_vector_val
        add     rsp, 8
        and     rax, -8
        or      rax, 1
        ret

The assume() in the code ought to be sufficient for convincing GCC that it doesn't need to munge the return value of make_vector_val(). Emacs uses patterns like this one pretty frequently.

---

#include <stdint.h>
#include <stddef.h>
#include <stdbool.h>

#define assume(x) do { if(!(x)) __builtin_unreachable(); } while(0)

enum { tagbits = 3 };

#define TAG_MASK (((uintptr_t)(1)<<tagbits) - 1)
#define TAG_VECTOR ((uintptr_t) 1)

typedef struct vector { double x; } vector;
typedef struct val {
    uintptr_t v;
} val;

static inline void *PTR_OF(val v)
{
    return (void *)(v.v & ~TAG_MASK);
}

static inline val TAG(void *v, uintptr_t tag)
{
    uintptr_t l = (uintptr_t) v;
    assume((l & TAG_MASK) == 0);
    return (val){l | tag};
}

extern val make_vector_val(void);

val make_vector2(void)
{
    val v = make_vector_val();
    assume((v.v & TAG_MASK) == TAG_VECTOR);
    struct vector *vs = PTR_OF(v);
    return TAG(vs, TAG_VECTOR);
}


---


### compiler : `gcc`
### title : `Failure to avoid useless sign extension`
### open_at : `2020-05-31T14:53:17Z`
### last_modified_date : `2023-08-24T21:36:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95453
### status : `RESOLVED`
### tags : `ABI, missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
void f2(int);
void f1(short x)
{
    f2(x);
}

With -O3, GCC outputs this :

f1(short):
  movsx edi, di
  jmp f2(int)

LLVM outputs this :

f1(short): # @f1(short)
  jmp f2(int) # TAILCALL


---


### compiler : `gcc`
### title : `Failure to optimize infinite recursion for empty struct types`
### open_at : `2020-06-02T16:55:40Z`
### last_modified_date : `2023-08-24T21:35:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95481
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
struct A {};

struct A foo()
{
  return foo();
}

This can be optimized to an infinite loop. With -O3, GCC currently outputs an explicit recursive call like this :

foo():
  sub rsp, 8
  call foo()
  xor eax, eax
  add rsp, 8
  ret

When it could just do this :

foo():
.L2:
  jmp .L2


---


### compiler : `gcc`
### title : `Suboptimal multiplication codegen for v16qi`
### open_at : `2020-06-03T03:00:12Z`
### last_modified_date : `2021-08-21T18:26:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95488
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c

---
typedef unsigned char v16qi __attribute__ ((vector_size (16)));
v16qi
foo (v16qi a, v16qi b)
{
    return  a*b;
}
---

gcc -O2 -march=skylake-avx512

---
foo(unsigned char __vector(16), unsigned char __vector(16)):
        vpunpcklbw      xmm3, xmm0, xmm0
        vpunpcklbw      xmm2, xmm1, xmm1
        vpunpckhbw      xmm0, xmm0, xmm0
        vpunpckhbw      xmm1, xmm1, xmm1
        vpmullw xmm2, xmm2, xmm3
        vpmullw xmm1, xmm1, xmm0
        vmovdqa xmm3, XMMWORD PTR .LC0[rip]
        vpand   xmm0, xmm3, xmm2
        vpand   xmm3, xmm3, xmm1
        vpackuswb       xmm0, xmm0, xmm3
        ret
.LC0:
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
---

icc generate
---
foo(unsigned char __vector(16), unsigned char __vector(16)):
        vpmovzxbw ymm2, xmm0                                    #5.15
        vpmovzxbw ymm3, xmm1                                    #5.15
        vpmullw   ymm4, ymm2, ymm3                              #5.15
        vpmovwb   xmm0, ymm4                                    #5.15
        vzeroupper                                              #5.15
        ret                                                     #5.15
---

we can do better in ix86_expand_vecop_qihi, problem is how can i get sign info for an rtx operand.


---


### compiler : `gcc`
### title : `Failure to optimize x && (x & y) to x & y`
### open_at : `2020-06-03T04:20:18Z`
### last_modified_date : `2023-08-24T21:35:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95489
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x, int y)
{
    return x && (x & y);
}

This can be optimized to `return x & y;` (see also the summary). This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Subtimal codegen for shift by constant for v16qi/v32qi under -march=skylake`
### open_at : `2020-06-04T06:24:50Z`
### last_modified_date : `2021-08-21T18:27:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95524
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c
---
typedef char v16qi __attribute__ ((vector_size (16)));
typedef char v32qi __attribute__ ((vector_size (32)));
typedef unsigned char v16uqi __attribute__ ((vector_size (16)));
typedef unsigned char v32uqi __attribute__ ((vector_size (32)));

v16qi
ashift (v16qi a)
{
    return  a<<5;
}

v32qi
ashift2 (v32qi a, v32qi b)
{
    return  a<<5;
}

v16qi
ashiftrt (v16qi a)
{
    return  a>>5;
}

v32qi
arshiftrt2 (v32qi a)
{
    return  a>>5;
}

v16uqi
lshiftrt (v16uqi a)
{
    return  a>>5;
}

v32uqi
lshiftrt2 (v32uqi a)
{
    return  a>>5;
}
---

gcc11 -O2 -march=skylake

---
ashift(char __vector(16)):
        vpaddb  xmm0, xmm0, xmm0
        vpaddb  xmm0, xmm0, xmm0
        vpaddb  xmm0, xmm0, xmm0
        vpaddb  xmm0, xmm0, xmm0
        vpaddb  xmm0, xmm0, xmm0
        ret
ashift2(char __vector(32), char __vector(32)):
        vpaddb  ymm0, ymm0, ymm0
        vpaddb  ymm0, ymm0, ymm0
        vpaddb  ymm0, ymm0, ymm0
        vpaddb  ymm0, ymm0, ymm0
        vpaddb  ymm0, ymm0, ymm0
        ret
ashiftrt(char __vector(16)):
        vpmovsxbw       xmm2, xmm0
        vpsrldq xmm1, xmm0, 8
        vpmovsxbw       xmm1, xmm1
        vpsraw  xmm0, xmm2, 5
        vmovdqa xmm2, XMMWORD PTR .LC0[rip]
        vpsraw  xmm1, xmm1, 5
        vpand   xmm0, xmm2, xmm0
        vpand   xmm2, xmm2, xmm1
        vpackuswb       xmm0, xmm0, xmm2
        ret
arshiftrt2(char __vector(32)):
        vmovdqa ymm1, ymm0
        vextracti128    xmm1, ymm1, 0x1
        vmovdqa ymm2, YMMWORD PTR .LC1[rip]
        vpmovsxbw       ymm0, xmm0
        vpmovsxbw       ymm1, xmm1
        vpsraw  ymm1, ymm1, 5
        vpsraw  ymm0, ymm0, 5
        vpand   ymm0, ymm2, ymm0
        vpand   ymm2, ymm2, ymm1
        vpackuswb       ymm0, ymm0, ymm2
        vpermq  ymm0, ymm0, 216
        ret
lshiftrt(unsigned char __vector(16)):
        vpmovzxbw       xmm2, xmm0
        vpsrldq xmm1, xmm0, 8
        vpmovzxbw       xmm1, xmm1
        vpsrlw  xmm0, xmm2, 5
        vmovdqa xmm2, XMMWORD PTR .LC0[rip]
        vpsrlw  xmm1, xmm1, 5
        vpand   xmm0, xmm2, xmm0
        vpand   xmm2, xmm2, xmm1
        vpackuswb       xmm0, xmm0, xmm2
        ret
lshiftrt2(unsigned char __vector(32)):
        vmovdqa ymm1, ymm0
        vextracti128    xmm1, ymm1, 0x1
        vmovdqa ymm2, YMMWORD PTR .LC1[rip]
        vpmovzxbw       ymm0, xmm0
        vpmovzxbw       ymm1, xmm1
        vpsrlw  ymm1, ymm1, 5
        vpsrlw  ymm0, ymm0, 5
        vpand   ymm0, ymm2, ymm0
        vpand   ymm2, ymm2, ymm1
        vpackuswb       ymm0, ymm0, ymm2
        vpermq  ymm0, ymm0, 216
        ret
.LC0:
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
.LC1:
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
        .value  255
---

icc has
---
ashift(char __vector(16)):
        vpsllw    xmm1, xmm0, 5                                 #9.16
        vpand     xmm0, xmm1, XMMWORD PTR .L_2il0floatpacket.0[rip] #9.16
        ret                                                     #9.16
ashift2(char __vector(32), char __vector(32)):
        vpsllw    ymm2, ymm0, 5                                 #15.16
        vpand     ymm0, ymm2, YMMWORD PTR .L_2il0floatpacket.1[rip] #15.16
        ret                                                     #15.16
ashiftrt(char __vector(16)):
        vpsrlw    xmm1, xmm0, 5                                 #21.16
        vpand     xmm0, xmm1, XMMWORD PTR .L_2il0floatpacket.2[rip] #21.16
        ret                                                     #21.16
arshiftrt2(char __vector(32)):
        vpsrlw    ymm1, ymm0, 5                                 #27.16
        vpand     ymm0, ymm1, YMMWORD PTR .L_2il0floatpacket.3[rip] #27.16
        ret                                                     #27.16
lshiftrt(unsigned char __vector(16)):
        vpsrlw    xmm1, xmm0, 5                                 #33.16
        vpand     xmm0, xmm1, XMMWORD PTR .L_2il0floatpacket.2[rip] #33.16
        ret                                                     #33.16
lshiftrt2(unsigned char __vector(32)):
        vpsrlw    ymm1, ymm0, 5                                 #39.16
        vpand     ymm0, ymm1, YMMWORD PTR .L_2il0floatpacket.3[rip] #39.16
        ret                                                     #39.16
.L_2il0floatpacket.1:
        .long   0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0
.L_2il0floatpacket.3:
        .long   0x07070707,0x07070707,0x07070707,0x07070707,0x07070707,0x07070707,0x07070707,0x07070707
.L_2il0floatpacket.0:
        .long   0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0,0xe0e0e0e0
.L_2il0floatpacket.2:
        .long   0x07070707,0x07070707,0x07070707,0x07070707
---

icc take much less instructions than gcc.


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_ffs == 0`
### open_at : `2020-06-04T09:42:32Z`
### last_modified_date : `2023-08-24T21:35:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95527
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int x)
{
    return __builtin_ffs(x) == 0;
}

This can be optimized to `x == 0`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to reuse flags generated by TZCNT for cmovcc on BMI-capable targets`
### open_at : `2020-06-04T10:05:58Z`
### last_modified_date : `2023-08-24T21:34:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95529
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x)
{
    return x == 0 ? 24 : ffs(x) - 1;
}

With -O3, LLVM outputs this :

f(int): # @f(int)
  bsf ecx, edi
  mov eax, 24
  cmovne eax, ecx
  ret

GCC outputs this :

f(int):
  xor eax, eax
  mov edx, 24
  rep bsf eax, edi
  test edi, edi
  cmove eax, edx
  ret

The `test` can be eliminated here.


---


### compiler : `gcc`
### title : `Failure to use TZCNT for __builtin_ffs`
### open_at : `2020-06-04T12:21:27Z`
### last_modified_date : `2023-08-24T21:34:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95531
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x)
{
    return __builtin_ffs(x);
}

With -O3 -mbmi, LLVM generates this :

f(int): # @f(int)
  tzcnt ecx, edi
  mov eax, -1
  cmovae eax, ecx
  add eax, 1
  ret

GCC outputs this :

f(int):
  bsf eax, edi
  mov edx, -1
  cmove eax, edx
  add eax, 1
  ret


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_ctz & 0x1F to __builtin_ctz on x86 with BMI`
### open_at : `2020-06-04T12:41:06Z`
### last_modified_date : `2023-08-24T21:34:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95532
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x)
{
  return __builtin_ctz(x) & 0x1F;
}

With -mbmi, this can be optimized to `__builtin_ctz(x)` (directly using `tzcnt` without an `and` afterwards). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize out cdqe after __bultin_ctz`
### open_at : `2020-06-04T14:58:48Z`
### last_modified_date : `2023-08-24T21:33:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95535
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
long long f(int n)
{
    return __builtin_ctz(n);
}

With -O3, GCC outputs this :

f(int):
  xor eax, eax
  tzcnt eax, edi
  cdqe
  ret

LLVM outputs this :

f(int): # @f(int)
  tzcnt eax, edi
  ret

Looks like the cdqe can be omitted.


---


### compiler : `gcc`
### title : `x86 instruction selection --- some REX prefixes unnecessary`
### open_at : `2020-06-07T10:01:47Z`
### last_modified_date : `2021-08-20T14:09:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95566
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Created attachment 48696
sample code

Consider the code attached, compiled with

gcc -O3 sample.c -o sample

Gcc produces unrolled loop code that follows the pattern below.

        movzx   ecx, WORD PTR [rsp-62]
        cmp     rdx, rcx

Here, rdx has the value of k >> 48.  The top 32 bits of rdx are zero after the shift, so the entirety of k >> 48 is in edx.  Thus, the cmp instructions could be

        cmp     edx, ecx

instead.  This difference avoids the REX prefix, and thus the instructions are shorter.  After sufficient unrolling (or with e.g. more complex comparisons that depend on k >> 48), shorter instructions without the REX prefix will be better even accounting for the partial register dependency (or an instruction to break the dependency).  The Intel optimization manual says shorter instructions are better.

The attachment is the entirety of sample.c.  I did not include other files because this attachment appears to qualify for that exemption due to excuse (ii): the attached test case is small and does not include any other file.

I originally found this behavior looking at the disassembly of gcc (Gentoo 9.2.0-r2 p3) 9.2.0.  I verified the same behavior with gcc 10.1 and gcc trunk at godbolt.


---


### compiler : `gcc`
### title : `[OpenMP] Offloading – Missed optimization / before LTO stream out, do more IPA optimizations affecting host←→target ABI`
### open_at : `2020-06-08T18:15:49Z`
### last_modified_date : `2020-06-08T18:15:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95583
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Follow up to PR 95551 + PR 94848 and some other PRs.

Currently, the flow is:

1. ipa_passes (); → calls output_offload_tables, writes LTO for target lto1
   (with -flto, stop here and continue with host lto1).
On host compiler or host lto1 compiler – and on target lto1 compiler, do:
2. execute_ipa_pass_list (g->get_passes ()->all_late_ipa_passes);
3. omp_finish_file()

As all_late_ipa_passes runs after writing the output_offload_tables, function arguments (→ PR92029), inlining, global variable elimination etc. cannot happen if they affect the host←→target ABI. And adding libgomp calls (e.g. for global variable mapping) hamper optimizations which could be done otherwise.

Hence, optimizations influencing the host←→target ABI need to be handled before output_offload_tables.

An example is the test case of PR 95551, where -O3 eliminates the static, read-only array variable created for a Fortran array constructor but one still needs to keep it as it is referenced in the output_offload_tables. And as it is referenced in a complicated way, the local-only variable cannot be eliminated anymore on the device side either.

Another example is PR90591 where value propagation could be done (and mapping could be optimized to avoid pointless copyin/copyout or ...).


---


### compiler : `gcc`
### title : `[10 Regression] relocation truncated to fit: R_X86_64_PC32 against `.bss'`
### open_at : `2020-06-09T20:53:10Z`
### last_modified_date : `2023-07-07T08:54:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95620
### status : `RESOLVED`
### tags : `link-failure, missed-optimization, patch`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
It's a test-case reduced from opa-ff project:

$ cat intel.i
double a[353783808];
int b, c, d;

int
main() {
  for (; b;)
#pragma omp parallel
    a[c] = 1;
  for (;; b++)
    if (a[c])
      d++;
}

$ gcc intel.i -mcmodel=medium -O2 -fopenmp
$ gcc intel.i -mcmodel=medium -O2 -flto -fopenmp
/tmp/ccaU2BEX.ltrans0.ltrans.o: in function `main':
<artificial>:(.text.startup+0x2): relocation truncated to fit: R_X86_64_PC32 against `.bss'
<artificial>:(.text.startup+0x22): relocation truncated to fit: R_X86_64_PC32 against `.bss'
collect2: error: ld returned 1 exit status

They have a huge .bss array (~350MB).
Since r10-5244-g6d8fd122c4f856e9 we can't handle it, the maximum value supported by LTO is ~270MB.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] force_output flag on a variable prevents optimization  / regresses c-c++-common/goacc/kernels-alias-ipa-pta{-2,-4,}.c`
### open_at : `2020-06-10T09:08:12Z`
### last_modified_date : `2023-05-29T10:02:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95622
### status : `NEW`
### tags : `deferred, missed-optimization, openacc, openmp`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
The following only shows up as FAIL in the testsuite if  ENABLE_OFFLOAD  is true, even though it is only a compile-time test.   [Hence, it can make sense to not only "make check-target-libgomp" with an offloading compiler but also gcc/.]

Somewhat similar to PR 68773.

The testcase: c-c++-common/goacc/kernels-alias-ipa-pta-2.c
…
  {
    a[0] = 0;
    b[0] = 1;
    c[0] = a[0];
  }
…

Without ENABLE_OFFLOAD or before r11-1075-g1c0fdaf79e3618fd7512608a2e5c62b6b306e9e8 (for PR94848 + PR95551):

  MEM[(unsigned int *)_5] = 0;  // a[0] = 0
  MEM[(unsigned int *)_4] = 1;  // b[0] = 1
  MEM[(unsigned int *)_3] = 0;  // c[0] = a[0]

With r11-1075 + offloading compiler, force_output is on offloading variables (to ensure they are not optimized away), the last line becomes

  _8 = MEM[(unsigned int *)_5];  // _8 = a[0]
  MEM[(unsigned int *)_3] = _8;  // c[0] = _8

Expected: force_output does not affect the optimization. (Especially as "a" cannot be optimized away as it is also used for "copyout".)

[A very well optimizing compiler could see that nothing uses a/b/c after the target section and could replace the whole function body by "{ }"…]


---


### compiler : `gcc`
### title : `Redundant zero extension`
### open_at : `2020-06-11T04:06:40Z`
### last_modified_date : `2022-12-27T23:32:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95632
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Command line: bin/riscv64-unknown-elf-gcc -march=rv32imafc -mabi=ilp32f -O2 foo.c -S

==========
 C Source
==========
unsigned short foo(unsigned short crc) {
  crc ^= 0x4002;
  crc >>= 1;
  crc |= 0x8000;

  return crc;
}

=========
 GCC asm
=========
foo:
	li	a5,-24576  #
	addi	a5,a5,1    # a5 = 0xffffa001
	srli	a0,a0,1
	xor	a0,a0,a5
	slli	a0,a0,16   # 
	srli	a0,a0,16   # redundant zero-extension
	ret

=======================
 Ideal Code Generation 
=======================
foo:
	li	a5,40960   #
	addi	a5,a5,1    # a5 = 0x0000a001
	srli	a0,a0,1
	xor	a0,a0,a5
	ret


---


### compiler : `gcc`
### title : `-Warray-bounds while iterating over an escaped constant local array`
### open_at : `2020-06-11T08:06:28Z`
### last_modified_date : `2020-06-11T14:37:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95635
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `normal`
### contents :
Created attachment 48716
Bug demo

Hi. I'm running gcc-10 from Debian:

  dima@shorty:~$ gcc-10 --version
  gcc-10 (Debian 10.1.0-3) 10.1.0

I'm building the attached source like this:

  gcc-10 -Warray-bounds -O2 -c -o /dev/null tst.c

And I get this:

tst.c: In function 'a':
tst.c:12:27: warning: array subscript <unknown> is outside array bounds of 'int[0]' [-Warray-bounds]
   12 |         if(L[i] > 0 && arr[L[i]] )
      |                        ~~~^~~~~~
tst.c:8:9: note: while referencing 'arr'
    8 |     int arr[0];
      |         ^~~
tst.c:12:27: warning: array subscript <unknown> is outside array bounds of 'int[0]' [-Warray-bounds]
   12 |         if(L[i] > 0 && arr[L[i]] )
      |                        ~~~^~~~~~
tst.c:8:9: note: while referencing 'arr'
    8 |     int arr[0];


The array arr[] has 0 elements, and gcc is telling me I'm accessing outside of those bounds. But L[i]>0 is always false, so we'll never actually look at arr[anything]. gcc knows this most of the time. If I remove the -O2 or the b() call or lots of little unrelated-looking things, the issue goes away. Thanks.


---


### compiler : `gcc`
### title : `Optimizer fails to realize that a variable tested twice in a row is the same both times`
### open_at : `2020-06-11T16:32:07Z`
### last_modified_date : `2023-08-09T22:16:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95643
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Consider this code, compiled at -O3:

extern int e;
void f(int x, int y) {
    if(y) {
        if(y && !x) __builtin_unreachable();
        if(x) ++e;
    }
}

GCC 10.1 on AMD64 produces the following assembly:

f:
        testl   %edi, %edi
        je      .L1
        testl   %esi, %esi
        jne     .L10
.L1:
        ret
.L10:
        addl    $1, e(%rip)
        ret

Godbolt link: https://godbolt.org/z/Z75QTM
The "y" in "if(y && !x)" is necessarily true, but GCC doesn't realize this, since changing "if(y && !x)" to the equivalent "if(!x)" results in much better assembly:

f:
        testl   %esi, %esi
        je      .L1
        addl    $1, e(%rip)
.L1:
        ret

We should be able to generate this assembly even with the redundant check of "y".


---


### compiler : `gcc`
### title : `aarch64: Missed optimization storing addition of two shorts`
### open_at : `2020-06-12T08:52:28Z`
### last_modified_date : `2022-02-04T21:25:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95650
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
With the following C code:

void foo(unsigned short a, unsigned short b, unsigned short *ptr)
{
    *ptr = a + b;
}

AArch64 GCC at -O2 generates:

foo:
        and     w1, w1, 65535
        add     w0, w1, w0, uxth
        strh    w0, [x2]
        ret

but the and is redundant (and there's no need for the zero-extended add). Indeed, clang generates:

foo:                                    // @foo
        add     w8, w1, w0
        strh    w8, [x2]
        ret

Notably AArch32 GCC gives the optimal sequence:

foo:
        add     r1, r0, r1
        strh    r1, [r2]        @ movhi
        bx      lr

and comparing the RTL at expand time shows that AArch64 has HI -> SI zero_extends to contend with (which AArch32 doesn't).

AArch32 RTL:

(insn 2 6 3 2 (set (reg/v:SI 111 [ a ])
        (reg:SI 0 r0 [ a ])) "./example.c":2 -1
     (nil))
(insn 3 2 4 2 (set (reg/v:SI 112 [ b ])
        (reg:SI 1 r1 [ b ])) "./example.c":2 -1
     (nil))
(insn 4 3 5 2 (set (reg/v/f:SI 113 [ ptr ])
        (reg:SI 2 r2 [ ptr ])) "./example.c":2 -1
     (nil))
(note 5 4 8 2 NOTE_INSN_FUNCTION_BEG)
(debug_insn 8 5 9 2 (debug_marker) "./example.c":3 -1
     (nil))
(insn 9 8 10 2 (set (reg:SI 114)
        (plus:SI (reg/v:SI 111 [ a ])
            (reg/v:SI 112 [ b ]))) "./example.c":3 -1
     (nil))
(insn 10 9 0 2 (set (mem:HI (reg/v/f:SI 113 [ ptr ]) [1 *ptr_5(D)+0 S2 A16])
        (subreg:HI (reg:SI 114) 0)) "./example.c":3 -1
     (nil))

AArch64 RTL:

(note 1 0 6 NOTE_INSN_DELETED)
(note 6 1 2 2 [bb 2] NOTE_INSN_BASIC_BLOCK)
(insn 2 6 3 2 (set (reg/v:SI 91 [ a ])
        (zero_extend:SI (reg:HI 0 x0 [ a ]))) "./example.c":2 -1
     (nil))
(insn 3 2 4 2 (set (reg/v:SI 92 [ b ])
        (zero_extend:SI (reg:HI 1 x1 [ b ]))) "./example.c":2 -1
     (nil))
(insn 4 3 5 2 (set (reg/v/f:DI 93 [ ptr ])
        (reg:DI 2 x2 [ ptr ])) "./example.c":2 -1
     (nil))
(note 5 4 8 2 NOTE_INSN_FUNCTION_BEG)
(debug_insn 8 5 9 2 (debug_marker) "./example.c":3 -1
     (nil))
(insn 9 8 10 2 (set (reg:SI 94)
        (plus:SI (reg/v:SI 91 [ a ])
            (reg/v:SI 92 [ b ]))) "./example.c":3 -1
     (nil))
(insn 10 9 11 2 (set (reg:HI 95)
        (subreg:HI (reg:SI 94) 0)) "./example.c":3 -1
     (nil))
(insn 11 10 0 2 (set (mem:HI (reg/v/f:DI 93 [ ptr ]) [1 *ptr_5(D)+0 S2 A16])
        (reg:HI 95)) "./example.c":3 -1
     (nil))


---


### compiler : `gcc`
### title : `static_cast checks for null even when the pointer is dereferenced`
### open_at : `2020-06-13T10:52:58Z`
### last_modified_date : `2022-01-03T09:54:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95663
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `normal`
### contents :
Consider this C++ code:

    struct Base1
    {
        int x;
    };
    
    struct Base2
    {
        int y;
    };
    
    struct Derived : Base1, Base2
    {
        int get_y() const { return y; }
    };
    
    int field(Base2* base)
    {
        return static_cast<Derived*>(base)->y;
    }
    
    int getter(Base2* base)
    {
        return static_cast<Derived*>(base)->get_y();
    }

Both field() and getter() produce this with -O2 or -O3:

    test    rdi, rdi
    je      .L2
    mov     eax, DWORD PTR [rdi]
    ret
    .L2:
    mov     eax, DWORD PTR ds:4
    ud2

That's fair, it traps if we dereference a null pointer.  But I need the best performance and don't want the null check, so I add -fno-isolate-erroneous-paths-dereference and see:

    lea     rax, [rdi-4]
    test    rdi, rdi
    cmovne  rdi, rax
    mov     eax, DWORD PTR [rdi+4]
    ret

If I read that correctly, it checks if the pointer is null so it can dereference 0x4 instead of 0x0.  That's hardly an improvement over the naive and optimal code:

    mov     eax, DWORD PTR [rdi]
    ret

Which is what Clang generates for field() in all versions through 10, and for getter() up to 3.6 (3.7 through 10 generate a cmovne like GCC with no-isolate).

I tried adding __attribute__((nonnull)) to the function declarations, but it didn't help.

Live demo: https://godbolt.org/z/XnhZoz


---


### compiler : `gcc`
### title : `-O3 generates more complicated code to return 8-byte struct of zeros, sometimes`
### open_at : `2020-06-14T08:52:21Z`
### last_modified_date : `2021-05-30T22:49:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95669
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Consider this C++ code:

    struct res
    {
        int val;
        bool ok;
        bool dummy;
    };

    res fun(int a, int b)
    {
        if (a < b)
            return {0, false};
        return {a * b, true};
    }

10.1 (or 8.x) with -O2 or -O3 on x86_64 looks good:

        cmp     edi, esi ; a < b
        jge     .L2
        xor     eax, eax ; {0, false}
        ret
    .L2:
        imul    edi, esi ; a * b
        mov     rax, rdi
        bts     rax, 32  ; ok = true
        ret

But if you remove "dummy" and compile with -O3, it looks worse:

        cmp     edi, esi ; a < b
        jl      .L3
        imul    edi, esi ; a * b
        mov     esi, 1   ; ok = true
        sal     rsi, 32
        mov     eax, edi
        or      rax, rsi
        ret
    .L3:
        xor     esi, esi
        xor     edi, edi
        mov     eax, edi
        sal     rsi, 32
        or      rax, rsi
        ret

Both branch cases are longer, but especially strange is how .L3 uses five instructions instead of "xor eax, eax".

Similar happens in every version I tried except for 4.5 and 4.6.

Live demo: https://godbolt.org/z/4Lisy6


---


### compiler : `gcc`
### title : `Unnecessary move when doing division-by-multiplication`
### open_at : `2020-06-14T18:21:38Z`
### last_modified_date : `2023-04-22T16:55:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95674
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.1.0`
### severity : `normal`
### contents :
When GCC does division-by-multiplication, it seems to forget that multiplication is commutative. It moves the numerator into %rax, but if it moved the magic number there instead, then it wouldn't have to move the numerator at all.

Input:

unsigned long f(unsigned long x) {
        return x / 7;
}

Actual assembly:

f:
        movabsq $2635249153387078803, %rdx
        movq    %rdi, %rax
        mulq    %rdx
        subq    %rdx, %rdi
        shrq    %rdi
        leaq    (%rdx,%rdi), %rax
        shrq    $2, %rax
        ret

Desired assembly:

f:
        movabsq $2635249153387078803, %rax
        mulq    %rdi
        subq    %rdx, %rdi
        shrq    %rdi
        leaq    (%rdx,%rdi), %rax
        shrq    $2, %rax
        ret

https://godbolt.org/z/RE45qg


---


### compiler : `gcc`
### title : `Loop invariants can't be moved out of the loop`
### open_at : `2020-06-15T15:47:01Z`
### last_modified_date : `2021-05-30T22:31:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95685
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.1.0`
### severity : `normal`
### contents :
Command line: bin/riscv64-unknown-elf-gcc -march=rv32imafc -mabi=ilp32f -O2 -funroll-loops bar.c -S

==========
 C Source
==========
unsigned short bar(unsigned char data, unsigned short crc) {
  unsigned char i = 0;

  for (i = 0; i < 3; i++) {
    data >>= 1;
    if ((data & 1) == 1)
      crc ^= 0x2001;
  }
  return crc;
}

=========
 GCC asm
=========
bar:
	srli	a4,a0,1
	andi	t0,a4,1
	mv	a5,a0
	mv	a0,a1
	beq	t0,zero,.L2
	li	t1,8192     #
	addi	t2,t1,1     # 0x2001
	xor	a0,a1,t2
.L2:
	srli	a1,a5,2
	andi	a2,a1,1
	beq	a2,zero,.L3
	li	a3,8192     #
	addi	a6,a3,1     # 0x2001
	xor	a0,a0,a6
.L3:
	srli	a7,a5,3
	andi	t3,a7,1
	beq	t3,zero,.L4
	li	t4,8192     #
	addi	t5,t4,1     # 0x2001
	xor	a0,a0,t5
.L4:
	ret

If I compile it without -funroll-loops, loop invariant code motion works:
bar:
	li	a2,8192     #
	mv	a4,a0
	li	a5,3
	mv	a0,a1
	addi	a2,a2,1     # 0x2001
.L3:
	srli	a4,a4,1
	addi	a5,a5,-1
	andi	a3,a4,1
	andi	a5,a5,0xff
	beq	a3,zero,.L2
	xor	a0,a0,a2
.L2:
	bne	a5,zero,.L3
	ret

I guess the problem is the order of optimization passes.
Because cunroll pass makes the loop no longer a loop, loop2_invariant can't work on it.

=======================
 Ideal Code Generation 
=======================
bar:
	srli	a4,a0,1
	andi	t0,a4,1
	mv	a5,a0
	mv	a0,a1
	li	t1,8192     #
	addi	t2,t1,1     # t2 = 0x2001
	beq	t0,zero,.L2
	xor	a0,a1,t2
.L2:
	srli	a1,a5,2
	andi	a2,a1,1
	beq	a2,zero,.L3
	xor	a0,a0,t2    # Use t2
.L3:
	srli	a7,a5,3
	andi	t3,a7,1
	beq	t3,zero,.L4
	xor	a0,a0,t2    # Use t2
.L4:
	ret


---


### compiler : `gcc`
### title : `Failure to optimize float comparison of converted integer to integer comparison`
### open_at : `2020-06-16T15:17:44Z`
### last_modified_date : `2023-08-24T21:33:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95697
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(int n)
{
    return (float)n > 0.0;
}

This can be optimized to `return n > 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `ranges::transform missing vectorization opportunity`
### open_at : `2020-06-16T17:03:46Z`
### last_modified_date : `2021-07-22T13:54:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95702
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.1.0`
### severity : `normal`
### contents :
GCC 10 is able to vectorize this code (https://godbolt.org/z/tsbfQw):

#include <algorithm>
#include <vector>

void foo(std::vector<double> &u, std::vector<double> const &v)
{
  std::transform(std::begin(u), std::end(u),
                 std::begin(v),
                 std::begin(u),
                 std::plus());
}


However, it fails to vectorize the ranges equivalent (https://godbolt.org/z/D49hJa):

#include <algorithm>
#include <vector>

void foo(std::vector<double> &u, std::vector<double> const &v)
{
  std::ranges::transform(u, v, std::begin(u), std::plus());
}

The option -fopt-info-vec-missed reveals why:

bits/ranges_algo.h:986:29: missed: not vectorized: number of iterations cannot be computed.

Would an optimization like the following one make sense?

        template<input_iterator _Iter1, sentinel_for<_Iter1> _Sent1,
	         input_iterator _Iter2, sentinel_for<_Iter2> _Sent2,
	         weakly_incrementable _Out, copy_constructible _Fp,
	         typename _Proj1 = identity, typename _Proj2 = identity>
          requires indirectly_writable<_Out,
                                       indirect_result_t<_Fp&,
                                       projected<_Iter1, _Proj1>,
                                       projected<_Iter2, _Proj2>>>
          constexpr binary_transform_result<_Iter1, _Iter2, _Out>
          operator()(_Iter1 __first1, _Sent1 __last1,
                     _Iter2 __first2, _Sent2 __last2,
                     _Out __result, _Fp __binary_op,
                     _Proj1 __proj1 = {}, _Proj2 __proj2 = {}) const
          {
+           if constexpr (random_access_iterator<_Iter1>
+                      && random_access_iterator<_Iter2>)
+             {
+               auto __d1 = ranges::distance(__first1, __last1);
+               auto __d2 = ranges::distance(__first2, __last2);
+               auto __n = std::min(__d1, __d2);
+               for (decltype(__n) __i = 0; __i < __n;
+                    (void)++__i, (void)++__first1, (void)++__first2, ++__result)
+             }
+           else
	          for (; __first1 != __last1 && __first2 != __last2;
	               ++__first1, (void)++__first2, ++__result)
	            *__result = std::__invoke(__binary_op,
                                              std::__invoke(__proj1, *__first1),
                                              std::__invoke(__proj2, *__first2));
	        return {std::move(__first1), std::move(__first2), std::move(__result)};
          }


---


### compiler : `gcc`
### title : `PPC: int128 shifts should be implemented branchless`
### open_at : `2020-06-16T17:14:45Z`
### last_modified_date : `2023-06-02T03:56:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95704
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `enhancement`
### contents :
Created attachment 48741
input with branchless 128-bit shifts

PowerPC processors don't like branches and branch mispredicts lead to large overhead.

shift left/right unsigned __in128 can be implemented in 8 instructions which can be processed on 2 pipelines almost in parallel leading to ~5 cycle latency on Power 7 and 8.
shift right algebraic __int128 can be implemented in 10 instructions.
Overall comparable in latency of the branching code.

In attached file you find the branch less implementations in C. And I know that this is using undefined behavior. But the resulting assembly is the interesting part. 

The unnecessary rldicl 8,5,0,32 at the beginning of the routines are also not necessary.


---


### compiler : `gcc`
### title : `Failure to optimize away certain returns when the condition to reach them is a calculation that already results in that value`
### open_at : `2020-06-17T20:17:46Z`
### last_modified_date : `2023-08-24T21:33:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95729
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int a, int b)
{
    if (a != b)
        return a - b;
    else
        return 0;
}

This can be optimized to `return a - b;`. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize a >= 0 && b >= 0 to (a | b) >= 0`
### open_at : `2020-06-17T21:58:23Z`
### last_modified_date : `2023-08-24T21:32:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95731
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(int a, int b)
{
    return a >= 0 && b >= 0;
}

This can be optimized to `return (a | b) >= 0;`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `PPC: Unnecessary extsw after negative less than`
### open_at : `2020-06-18T08:10:57Z`
### last_modified_date : `2022-07-29T02:52:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95737
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
unsigned long long negativeLessThan(unsigned long long a, unsigned long long b)
{
   return -(a < b);
}

gcc -m64 -O2 -save-temps negativeLessThan.C

creates

_Z16negativeLessThanyy:
.LFB0:
        .cfi_startproc
        subfc 4,4,3
        subfe 3,3,3
        extsw 3,3
        blr

The extsw is not necessary.


---


### compiler : `gcc`
### title : `Failure to optimize comparison of vector after sign xor to unsigned comparison`
### open_at : `2020-06-18T09:41:18Z`
### last_modified_date : `2023-09-04T00:29:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95738
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef int64_t v2i64 __attribute__((vector_size(16)));
typedef uint64_t v2u64 __attribute__((vector_size(16)));

v2i64 f(v2i64 a, v2i64 b)
{
    auto sign = (v2i64){(int64_t)0x8000000000000000u, (int64_t)0x8000000000000000u};
    return (a ^ sign) > (b ^ sign);
}

This can be optimized to `(v2u64)a > (v2u64)b`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to avoid using the stack when interpreting a float as an integer when it is modified afterwards`
### open_at : `2020-06-18T11:06:11Z`
### last_modified_date : `2023-08-24T21:32:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95740
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int32_t f(float p)
{
    int32_t tmp;
    memcpy(&tmp, &p, sizeof(float));
    ++tmp;
    return tmp;
}

With -O3, LLVM outputs this :

f(float):
  movd eax, xmm0
  add eax, 1
  ret

GCC outputs this :

f(float):
  movd DWORD PTR [rsp-4], xmm0
  mov eax, 1
  add eax, DWORD PTR [rsp-4]
  ret


---


### compiler : `gcc`
### title : `[x86] Use dummy atomic insn instead of mfence in __atomic_thread_fence(seq_cst)`
### open_at : `2020-06-18T19:32:26Z`
### last_modified_date : `2020-12-10T09:42:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95750
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `normal`
### contents :
Currently, __atomic_thread_fence(seq_cst) on x86 and x86-64 generates mfence instruction. A dummy atomic instruction (a lock-prefixed instruction or xchg with a memory operand) would provide the same sequential consistency guarantees while being more efficient on most current CPUs. The mfence instruction additionally orders non-temporal stores, which is not relevant for atomic operations and are not ordered by seq_cst atomic operations anyway.

Regarding performance, some data is available in Agner Fog's instruction tables:

https://www.agner.org/optimize/

Also, there is this article:

https://shipilev.net/blog/2014/on-the-fence-with-dependencies/

TL;DR: There is benefit on every CPU except Atom; on Atom there is no difference.

Regarding the dummy instruction and target memory location, here are some considerations:

- The lock-prefixed instruction should preferably not alter flags or registers and should require minimum number of registers.
- The memory location should not be shared with other threads.
- The memory location should likely be in cache.
- The memory location should not alias existing data on the stack, so that we don't introduce a false data dependency on previous or subsequent instructions.

Based on the above, a good candidate is "lock not" on a dummy variable on the top of the stack. Note that the variable would be accessible through esp/rsp, it is likely to be in hot memory and is likely thread-private.

I've implemented this optimization in Boost.Atomic, and a similar optimization is done in MSVC:

https://github.com/microsoft/STL/pull/740


---


### compiler : `gcc`
### title : `Failure to optimize complicated usage of __builtin_ctz with conditionals properly`
### open_at : `2020-06-18T20:18:18Z`
### last_modified_date : `2023-08-24T21:32:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95752
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
unsigned long f(uint64_t value)
{
    unsigned int result;
    
    if ((value & 0xFFFFFFFF) == 0)
    {
        result = __builtin_ctz(value >> 32) + 32;
    }
    else
    {
        if ((unsigned int)value != 0)
            result = __builtin_ctz((unsigned int)value);
    }

    return result;
}

With -O3 -mbmi, LLVM outputs this :

f(unsigned long):
  mov rax, rdi
  shr rax, 32
  tzcnt ecx, eax
  or ecx, 32
  tzcnt eax, edi
  cmovb eax, ecx
  ret

GCC outputs this :

f(unsigned long):
  test edi, edi
  jne .L2

  shr rdi, 32
  xor eax, eax
  tzcnt eax, edi
  add eax, 32
  mov eax, eax
  ret

.L2:
  xor edx, edx
  mov eax, 0
  tzcnt edx, edi
  test edi, edi
  cmovne eax, edx
  mov eax, eax
  ret

This may be related to how GCC handles undefined behaviour in relation to `__builtin_ctz` and uninitialized variables, but this still seems like it could be heavily optimized. At least, it could emit something like this if the `cmovcc` is not the best behaviour here :

f(unsigned long):
  test edi, edi
  jne .L2

  shr rdi, 32
  tzcnt eax, edi
  add eax, 32
  ret

.L1:
  tzcnt eax, edi
  ret

Using this code :

unsigned long f(uint64_t value)
{
    unsigned int result;
    
    if ((value & 0xFFFFFFFF) == 0)
    {
        result = __builtin_ctz(value >> 32) + 32;
    }
    else
    {
        if ((unsigned int)value != 0)
            result = __builtin_ctz((unsigned int)value);
        else
            __builtin_unreachable();
    }

    return result;
}

(i.e. adding __builtin_unreachable where an undefined value is created) generates better code :

f(unsigned long):
  xor eax, eax
  tzcnt eax, edi
  test edi, edi
  jne .L3
  shr rdi, 32
  tzcnt edi, edi
  lea eax, [rdi+32]
.L3:
  mov eax, eax
  ret

This looks like something tree-ssa optimizers could do (inserting __builtin_unreachable when invoking UB through usage of undefined values) since https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94861 indicates that GCC doesn't do this even for the simplest cases (and, looking at tree dumps, tree-ssa doesn't look like it makes any assumptions on the initial value of variables).


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_ia32_cmpb512_mask with anded first operand, 0 second operand and neq operation`
### open_at : `2020-06-18T23:55:31Z`
### last_modified_date : `2023-08-24T21:32:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95754
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef char v64i8 __attribute__((vector_size(64)));

int64_t f(v64i8 a, v64i8 b)
{
    return __builtin_ia32_cmpb512_mask((a & b), (v64i8){0}, 4, -1);
}

This can be optimized to _mm512_test_epi8_mask(a, b). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `_Complex return with constant 0 could be improved`
### open_at : `2020-06-19T00:43:34Z`
### last_modified_date : `2023-08-24T21:32:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95756
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
float _Complex f()
{
    return {0, 0};
}

With -O3, LLVM outputs this :

f(): # @f()
  xorps xmm0, xmm0
  ret

GCC outputs this :

f():
  mov DWORD PTR [rsp-8], 0x00000000
  mov DWORD PTR [rsp-4], 0x00000000
  movq xmm0, QWORD PTR [rsp-8]
  ret


---


### compiler : `gcc`
### title : `ivopts with loop variables`
### open_at : `2020-06-19T06:17:39Z`
### last_modified_date : `2022-08-06T20:03:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95760
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `normal`
### contents :
C Source:

int **matrix;
int n;
void foo()
{
    static int row;
    static int col;
    static int sum = 0;

    for( row = 0 ; row < n ; row++ )
    {
        for( col = 0 ; col < n ; col++ )
        {
            sum += matrix[row][col];
        }
    }
}

Compiling:
$./RISCV-GCC-10.1/bin/riscv64-unknown-elf-gcc foo.c -Os -S -o foo.s -march=rv32imac -mabi=ilp32

Asm:

foo:
...skip load/store variables...
.L5:
    slli    a7,a1,2 #row*4 
    add a7,t3,a7    #matrix + (row*4) 
    li  a0,0
.L3:
    bgt a5,a0,.L4
    addi    a1,a1,1 #row++
    mv  a0,a5
    li  a7,1
    j   .L2
.L4:
    lw  t1,0(a7)
    slli    t4,a0,2 #col*4
    addi    a0,a0,1 #col++
    add t1,t1,t4    #*matrix + (col*4)
    lw  t1,0(t1)
    add a6,a6,t1
    li  t1,1
    j   .L3

The calculation of matrix offset is not increasing by 4 after each iteration. I also check that with RISCV-GCC-8.3, it can be emitted code like "add a7, a7, 4" after each iteration. GCC-10.1 takes two instructions to do this, while GCC-8.3 takes one. I think it might affect code size / performance slightly. 

I am also wondering if "col" can be optimized to the add by 4 operation, because gcc-8.3 doesn't optimize it too. 

Thanks.


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_convertvector from vector of 16 chars to vector of 16 shorts in a single instruction on AVX2`
### open_at : `2020-06-19T09:35:44Z`
### last_modified_date : `2023-08-24T21:32:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95762
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef int8_t v16i8 __attribute__((vector_size(16)));
typedef int16_t v16i16 __attribute__((vector_size(32)));

auto f(v16i8 a)
{
    return __builtin_convertvector(a, v16i16);
}

With -O3 -mavx2, LLVM outputs this :

f(signed char __vector(16)):
  vpmovsxbw ymm0, xmm0
  ret

GCC outputs this :

f(signed char __vector(16)):
  vpmovsxbw xmm1, xmm0
  vpsrldq xmm0, xmm0, 8
  vpmovsxbw xmm0, xmm0
  vinserti128 ymm0, ymm1, xmm0, 0x1
  ret


---


### compiler : `gcc`
### title : `Failure to optimize usage of _mm512_set1_epi32 to a single instruction`
### open_at : `2020-06-19T10:45:49Z`
### last_modified_date : `2023-08-24T21:32:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95764
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
__m512i f(__m512i a)
{
    return (_mm512_set1_epi32(0x7FFFFFFF) & a);
}

With -O3 -mavx512f, LLVM outputs this :

.LCPI0_0:
  .quad 9223372034707292159
f(long long __vector(8)):
  vpandq zmm0, zmm0, qword ptr [rip + .LCPI0_0]{1to8}
  ret

GCC outputs this :

f(long long __vector(8)):
  mov eax, 2147483647
  vpbroadcastd zmm1, eax
  vpandq zmm0, zmm0, zmm1
  ret

I'm not completely sure the LLVM version is better, but I'd rather file a bug report (and be able to file one back to LLVM if I learn that GCC's code is better) than just do nothing.


---


### compiler : `gcc`
### title : `Failure to directly use vpbroadcastd for _mm_set1_epi32 when passing unsigned short`
### open_at : `2020-06-19T12:48:17Z`
### last_modified_date : `2023-08-24T21:32:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95766
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
__m128i f(unsigned short a)
{
    return _mm_set1_epi32(a);
}

With -O3 -mavx512cd -mavx512vl, LLVM outputs this :

f(unsigned short):
  vpbroadcastd xmm0, edi
  ret

GCC outputs this :

f(unsigned short):
  kmovw k0, edi
  vpbroadcastmw2d xmm0, k0
  ret


---


### compiler : `gcc`
### title : `Failure to optimize popcount idiom when argument is unsigned char`
### open_at : `2020-06-19T17:26:12Z`
### last_modified_date : `2023-08-24T21:32:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95771
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(unsigned char x)
{
    int i = 0;
    while (x)
    {
        x &= x - 1;
        ++i;
    }
    return i;
}

This can be optimized to __builtin_popcount(x). LLVM does this transformation, but GCC does not.

PS : GCC does this optimization if x is int and a few other types. I've also seen that GCC does not do this optimization for __int128 (which it could do with adding a popcount of the low and high parts of x).


---


### compiler : `gcc`
### title : `Inefficient use of the stack when a function takes the address of its argument`
### open_at : `2020-06-20T06:34:56Z`
### last_modified_date : `2020-06-22T07:57:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95783
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `normal`
### contents :
Consider this C code:

void g(long *);
long f(long x) {
    g(&x);
    return x;
}

At either "-O3" or "-Os", it results in this assembly:

f:
        subq    $24, %rsp
        movq    %rdi, 8(%rsp)
        leaq    8(%rsp), %rdi
        call    g
        movq    8(%rsp), %rax
        addq    $24, %rsp
        ret

There are two problems with this: it's unnecessarily complicated with extra instructions, and it wastes 16 bytes of stack space. I'd rather see this assembly instead:

f:
        pushq   %rdi
        movq    %rsp, %rdi
        call    g
        popq    %rax
        ret

https://godbolt.org/z/PuNB6Y


---


### compiler : `gcc`
### title : `Failure to optimize usage of __builtin_add_overflow with return statement properly`
### open_at : `2020-06-20T10:13:13Z`
### last_modified_date : `2023-08-24T21:31:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95784
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(uint8_t operand, int8_t *result)
{
    if (__builtin_add_overflow(operand, 0, result))
    {
        *result = 0;
        return 10213;
    }
    return 0;
}

With -O3, LLVM outputs this :

f(unsigned char, signed char*): # @f(unsigned char, signed char*)
  movsx eax, dil
  xor ecx, ecx
  cmp edi, eax
  cmovne edi, ecx
  mov byte ptr [rsi], dil
  mov eax, 10213
  cmove eax, ecx
  ret

GCC outputs this :

f(unsigned char, signed char*):
  movzx eax, dil
  movsx di, dil
  cmp ax, di
  setne dl
  mov r8d, edx
  sal r8d, 31
  sar r8d, 31
  and r8d, 10213
  test dl, dl
  mov edx, 0
  cmovne eax, edx
  mov BYTE PTR [rsi], al
  mov eax, r8d
  ret


---


### compiler : `gcc`
### title : `Complete lack of optimization on assignment to some types when followed by`
### open_at : `2020-06-20T17:00:49Z`
### last_modified_date : `2023-08-24T21:31:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95787
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
extern unsigned short a;

int f()
{
    a = 0;
    return 0;
}

With -O3, LLVM outputs this :

f(): # @f()
  mov word ptr [rip + a], 0
  xor eax, eax
  ret

GCC outputs this :

f():
  xor eax, eax
  mov WORD PTR a[rip], ax
  xor eax, eax
  ret

GCC has the same problem for every single architecture, the final tree representation looks like this :

;; Function f (_Z1fv, funcdef_no=1, decl_uid=4740, cgraph_uid=2, symbol_order=1)

f ()
{
  <bb 2> [local count: 1073741824]:
  a = 0;
  return 0;

}

And the final RTL representation looks like this :


;; Function f (_Z1fv, funcdef_no=1, decl_uid=4740, cgraph_uid=2, symbol_order=1)



f

Dataflow summary:
;;  fully invalidated by EH 	 0 [ax] 1 [dx] 2 [cx] 4 [si] 5 [di] 8 [st] 9 [st(1)] 10 [st(2)] 11 [st(3)] 12 [st(4)] 13 [st(5)] 14 [st(6)] 15 [st(7)] 17 [flags] 18 [fpsr] 20 [xmm0] 21 [xmm1] 22 [xmm2] 23 [xmm3] 24 [xmm4] 25 [xmm5] 26 [xmm6] 27 [xmm7] 28 [mm0] 29 [mm1] 30 [mm2] 31 [mm3] 32 [mm4] 33 [mm5] 34 [mm6] 35 [mm7] 36 [r8] 37 [r9] 38 [r10] 39 [r11] 44 [xmm8] 45 [xmm9] 46 [xmm10] 47 [xmm11] 48 [xmm12] 49 [xmm13] 50 [xmm14] 51 [xmm15] 52 [xmm16] 53 [xmm17] 54 [xmm18] 55 [xmm19] 56 [xmm20] 57 [xmm21] 58 [xmm22] 59 [xmm23] 60 [xmm24] 61 [xmm25] 62 [xmm26] 63 [xmm27] 64 [xmm28] 65 [xmm29] 66 [xmm30] 67 [xmm31] 68 [k0] 69 [k1] 70 [k2] 71 [k3] 72 [k4] 73 [k5] 74 [k6] 75 [k7]
;;  hardware regs used 	 7 [sp]
;;  regular block artificial uses 	 7 [sp]
;;  eh block artificial uses 	 7 [sp] 16 [argp]
;;  entry block defs 	 0 [ax] 1 [dx] 2 [cx] 4 [si] 5 [di] 7 [sp] 20 [xmm0] 21 [xmm1] 22 [xmm2] 23 [xmm3] 24 [xmm4] 25 [xmm5] 26 [xmm6] 27 [xmm7] 36 [r8] 37 [r9]
;;  exit block uses 	 0 [ax] 7 [sp]
;;  regs ever live 	 0 [ax] 17 [flags]
;;  ref usage 	r0={3d,3u} r1={1d} r2={1d} r4={1d} r5={1d} r7={1d,2u} r17={2d} r20={1d} r21={1d} r22={1d} r23={1d} r24={1d} r25={1d} r26={1d} r27={1d} r36={1d} r37={1d} 
;;    total ref usage 25{20d,5u,0e} in 5{5 regular + 0 call} insns.
(note 1 0 14 NOTE_INSN_DELETED)
(note 14 1 16 2 [bb 2] NOTE_INSN_BASIC_BLOCK)
(note 16 14 2 2 NOTE_INSN_PROLOGUE_END)
(note 2 16 22 2 NOTE_INSN_FUNCTION_BEG)
(insn 22 2 23 2 (parallel [
            (set (reg:SI 0 ax)
                (const_int 0 [0]))
            (clobber (reg:CC 17 flags))
        ]) "./example.cpp":27:7 67 {*movsi_xor}
     (expr_list:REG_UNUSED (reg:CC 17 flags)
        (nil)))
(insn 23 22 24 2 (set (mem/c:HI (symbol_ref:DI ("a") [flags 0x40]  <var_decl 0x7f9720202cf0 a>) [1 a+0 S2 A16])
        (reg:HI 0 ax)) "./example.cpp":27:7 76 {*movhi_internal}
     (expr_list:REG_DEAD (reg:HI 0 ax)
        (nil)))
(insn 24 23 11 2 (parallel [
            (set (reg:DI 0 ax)
                (const_int 0 [0]))
            (clobber (reg:CC 17 flags))
        ]) "./example.cpp":29:1 68 {*movdi_xor}
     (expr_list:REG_UNUSED (reg:CC 17 flags)
        (nil)))
(insn 11 24 25 2 (use (reg/i:SI 0 ax)) "./example.cpp":29:1 -1
     (nil))
(note 25 11 18 2 NOTE_INSN_EPILOGUE_BEG)
(jump_insn 18 25 19 2 (simple_return) "./example.cpp":29:1 819 {simple_return_internal}
     (nil)
 -> simple_return)
(barrier 19 18 13)
(note 13 19 0 NOTE_INSN_DELETED)

So it looks like this is a problem with RTL (it explicitly sets a register to 0 and sets `a` to that register).


---


### compiler : `gcc`
### title : `Unnecessary vzeroupper when only using zmm16 through zmm31`
### open_at : `2020-06-20T20:00:47Z`
### last_modified_date : `2020-06-22T13:36:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95791
### status : `RESOLVED`
### tags : `missed-optimization, patch, ssemmx`
### component : `target`
### version : `10.1.0`
### severity : `normal`
### contents :
Consider this C code:

void f(void) {
    __asm__ __volatile__("" ::: "zmm16");
}

When compiled with "-O2 -mavx512f", it generates a vzeroupper instruction, but this is unnecessary, since zmm16 through zmm31 don't cause the performance penalty, and in fact they aren't even affected by vzeroupper.


---


### compiler : `gcc`
### title : `Failure to optimize assignment of struct members to memset`
### open_at : `2020-06-20T21:45:01Z`
### last_modified_date : `2023-08-24T21:31:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95792
### status : `NEW`
### tags : `compile-time-hog, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
#define F0(a) i##a, 
#define F1(a) F0(a##0) F0(a##1) F0(a##2) F0(a##3) F0(a##4) F0(a##5) F0(a##6) F0(a##7)
#define F2(a) F1(a##0) F1(a##1) F1(a##2) F1(a##3) F1(a##4) F1(a##5) F1(a##6) F1(a##7)
#define F3(a) F2(a##0) F2(a##1) F2(a##2) F2(a##3) F2(a##4) F2(a##5) F2(a##6) F2(a##7)
#define F4(a) F3(a##0) F3(a##1) F3(a##2) F3(a##3) F3(a##4) F3(a##5) F3(a##6) F3(a##7)

#define H0(a) s->i##a = 0;
#define H1(a) H0(a##0) H0(a##1) H0(a##2) H0(a##3) H0(a##4) H0(a##5) H0(a##6) H0(a##7)
#define H2(a) H1(a##0) H1(a##1) H1(a##2) H1(a##3) H1(a##4) H1(a##5) H1(a##6) H1(a##7)
#define H3(a) H2(a##0) H2(a##1) H2(a##2) H2(a##3) H2(a##4) H2(a##5) H2(a##6) H2(a##7)
#define H4(a) H3(a##0) H3(a##1) H3(a##2) H3(a##3) H3(a##4) H3(a##5) H3(a##6) H3(a##7)

struct foo {
	int
	F4(0)
	bar;
};

void f (struct foo *s)
{
	H4(0)
}

With -O3, GCC "optimizes" this by storing words containing 0 for the entire struct. LLVM instead just calls memset, which looks more reasonable.


---


### compiler : `gcc`
### title : `Nested function multi-versioning doesn't work`
### open_at : `2020-06-20T22:34:39Z`
### last_modified_date : `2020-06-22T08:34:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95793
### status : `UNCONFIRMED`
### tags : `missed-optimization, wrong-code`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-2 pr95790]$ cat x.cc
#include <x86intrin.h>

extern char *buf;

__attribute__ ((target ("default")))
static int foo(const char *buf) {
  return 1;
}

__attribute__ ((target ("avx2")))
static int foo(const char *buf) {
  __m256i x = *(const __m256i_u *) buf;
  return __builtin_ia32_pmovmskb256 (x);
}

__attribute__ ((target ("avx512f")))
static int foo(const char *buf) {
  __m128i x = *(const __m128i_u *) buf;
  return __builtin_ia32_pmovmskb128 (x);
}

__attribute__ ((target ("default")))
int bar(int unsigned size) {
  int acc = 0;
  for (int i = 0; i < size; i++) {
    acc += foo(&buf[i]);
  }
  return acc;
}

__attribute__ ((target ("avx2")))
int bar(int unsigned size) {
  int acc = 0;
  for (int i = 0; i < size; i++) {
    acc += foo(&buf[i]);
  }
  return acc;
}
[hjl@gnu-cfl-2 pr95790]$ /usr/gcc-10.1.1-x32/bin/gcc -O2 -flax-vector-conversions -fno-asynchronous-unwind-tables -S x.cc
cat[hjl@gnu-cfl-2 pr95790]$ cat x.s
	.file	"x.cc"
	.text
	.p2align 4
	.globl	_Z3barj
	.type	_Z3barj, @function
_Z3barj:
.LFB5599:
	.cfi_startproc
	movl	%edi, %eax
	ret
	.cfi_endproc
.LFE5599:
	.size	_Z3barj, .-_Z3barj
	.p2align 4
	.globl	_Z3barj.avx2
	.type	_Z3barj.avx2, @function
_Z3barj.avx2:
.LFB5600:
	.cfi_startproc
	testl	%edi, %edi
	je	.L7
	movq	buf(%rip), %rdx
	leal	-1(%rdi), %ecx
	xorl	%r8d, %r8d
	leaq	1(%rdx), %rax
	addq	%rax, %rcx
	jmp	.L6
	.p2align 4,,10
	.p2align 3
.L11:
	addq	$1, %rax
.L6:
	vmovdqu	(%rdx), %xmm1
	vinserti128	$0x1, 16(%rdx), %ymm1, %ymm0
	vpmovmskb	%ymm0, %edx
	addl	%edx, %r8d
	movq	%rax, %rdx
	cmpq	%rcx, %rax
	jne	.L11
	vzeroupper
	movl	%r8d, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L7:
	xorl	%r8d, %r8d
	movl	%r8d, %eax
	ret
	.cfi_endproc
.LFE5600:
	.size	_Z3barj.avx2, .-_Z3barj.avx2
	.ident	"GCC: (GNU) 10.1.1 20200523"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-2 pr95790]$ 

_Z3barj.avx2 is unused.


---


### compiler : `gcc`
### title : `strnlen of a constant string plus variable offset not folded when bound exceeds size`
### open_at : `2020-06-20T23:19:46Z`
### last_modified_date : `2022-03-22T11:07:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95794
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
GCC folds comparisons of the results of calls to strlen with constant strings and variable offsets but it doesn't do the same for the equivalent strnlen with a bound in excess of the string size (strnlen with a bound greater than the string length are equivalent to strlen with the same argument).

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
const char a[] = "12345";

void f (int i)
{
  if (__builtin_strlen (&a[i]) > 5)       // folded to false
    __builtin_abort ();
}

void g (int i)
{
  if (__builtin_strnlen (&a[i], 7) > 5)   // not folded
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1931, cgraph_uid=1, symbol_order=1)

f (int i)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1934, cgraph_uid=2, symbol_order=2)

g (int i)
{
  const char * _1;
  long unsigned int _2;
  sizetype _6;

  <bb 2> [local count: 1073741824]:
  _6 = (sizetype) i_3(D);
  _1 = &a + _6;
  _2 = __builtin_strnlen (_1, 7);
  if (_2 > 5)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `[10 Regression] Initialization code --- suboptimal`
### open_at : `2020-06-21T07:16:47Z`
### last_modified_date : `2023-07-07T08:55:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95798
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Created attachment 48764
sample code

This is similar to (but not the same as) bug 87223 for structs.  Further, this bug expands on this issue for gcc 10.x.  Originally, this was noted in gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0, compiling with -O3.

First, note the initialization code that trivially sets values to zero in an array,

        mov     eax, edi
        sub     rsp, 8080
        xor     edx, edx
        and     eax, 127
        mov     QWORD PTR [rsp-120+rax*8], 0
        mov     QWORD PTR [rsp-112+rax*8], 0
        mov     QWORD PTR [rsp-104+rax*8], 0
        mov     QWORD PTR [rsp-96+rax*8], 0
        mov     QWORD PTR [rsp-88+rax*8], 0
        mov     QWORD PTR [rsp-80+rax*8], 0
        mov     QWORD PTR [rsp-72+rax*8], 0
        mov     QWORD PTR [rsp-64+rax*8], 0
        xor     eax, eax

would be better by first setting a register to zero, then writing the value of the register.  Further, note that there is already a zero register available (edx), but it is not used.  This is similar to 87223 for structs, and here the issue manifests for arrays.

Second, using gcc 10 versions and -O3 at godbolt.org results in this code:

        mov     eax, edi
        mov     edx, edi
        sub     rsp, 8072
        and     eax, 127
        and     edx, 127
        mov     QWORD PTR [rsp-120+rdx*8], 0
        lea     edx, [rax+1]
        movsx   rdx, edx
        mov     QWORD PTR [rsp-120+rdx*8], 0
        lea     edx, [rax+2]
        movsx   rdx, edx
        mov     QWORD PTR [rsp-120+rdx*8], 0
        lea     edx, [rax+3]
        movsx   rdx, edx
        mov     QWORD PTR [rsp-120+rdx*8], 0
        lea     edx, [rax+4]
        movsx   rdx, edx
        mov     QWORD PTR [rsp-120+rdx*8], 0
        lea     edx, [rax+5]
        movsx   rdx, edx
        mov     QWORD PTR [rsp-120+rdx*8], 0
        lea     edx, [rax+6]
        add     eax, 7
        movsx   rdx, edx
        cdqe
        mov     QWORD PTR [rsp-120+rdx*8], 0
        xor     edx, edx
        mov     QWORD PTR [rsp-120+rax*8], 0
        xor     eax, eax

This is much, much more verbose than in gcc 9.3, for no apparent gain.


---


### compiler : `gcc`
### title : `Assumed conjunctions are not broken down into clauses if their pureness is checked first`
### open_at : `2020-06-21T09:26:05Z`
### last_modified_date : `2020-06-22T07:59:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95799
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
In the attached sample, foo is compiled into returning a constant 1, while bar will compare a to 0, even though bar’s assumption implies foo’s.

If I replace __builtin_pure_p() testing with a constant 1, both functions compile to the same assembly code, returning a constant 1. If I change bar to return (a >= 0 && b >= 0), it also compiles to returning a constant 1.

It seems as if the optimiser becomes unaware that conjunction elimination is a valid rule of inference if the conjunction’s pureness is checked first.


---


### compiler : `gcc`
### title : `Optimiser does not exploit the fact that an integer divisor cannot be zero`
### open_at : `2020-06-21T12:29:03Z`
### last_modified_date : `2020-06-22T08:40:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95801
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int always1(int a, int b) {
    if (a / b)
        return b != 0;
    return 1;
}

The function above should be possible to optimise to a constant 1, as integer division by zero is undefined. I’m surprised this isn’t caught already; it seems like very low-hanging fruit.

In case someone wants to claim there is no value to be gained from this, this is where it came up:

#define SIGNUM(value) ({ \
        __auto_type _value = (value); \
        (__typeof__(_value)) ((_value > 0) - (_value < 0)); \
    })

#define DIV_FLOOR(a, b) ({ \
        __auto_type _a = (a); \
        __auto_type _b = (b); \
        (_a / _b) - ((SIGNUM(_a) != SIGNUM(_b) ? _a % _b : 0) != 0); \
    })

For unsigned types, DIV_FLOOR(a, b) should compile to the same code as truncating division (a / b), but unless a statement to the effect of (b == 0 ? __builtin_unreachable() : (void) 0); is inserted before the division, DIV_FLOOR will generate considerably longer code, at least on x86.


---


### compiler : `gcc`
### title : `Failure to optimize strlen in certain situations properly, instead leading to weird code`
### open_at : `2020-06-21T16:55:46Z`
### last_modified_date : `2023-08-24T21:31:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95803
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(int i)
{
    if (i < 4)
        i = 4;

    const char *s = &"abc"[i];

    return strlen(s) > 3;
}

This can be optimized to a simple `return true` or `return false` (considering UB is invoked here). LLVM does this transformation, but GCC does not. Obviously this code is pretty weird and may not be very realistic itself, but this optimization would probably be effective for other code, and currently it results in rather weird code generation :

f(int):
  mov ecx, 4
  cmp edi, 4
  mov eax, 3
  mov edx, ecx
  cmovge edx, edi
  movsx rdx, edx
  sub rax, rdx
  cmp rdx, 3
  mov edx, 0
  cmova rax, rdx
  cmp rax, 3
  seta al
  ret

accompanied by very weird final tree optimized code :

;; Function f (_Z1fi, funcdef_no=287, decl_uid=9588, cgraph_uid=216, symbol_order=215)

f (int i)
{
  const char * s;
  long unsigned int _1;
  int _3;
  bool _6;
  sizetype _7;

  <bb 2> [local count: 1073741824]:
  # DEBUG BEGIN_STMT
  _3 = MAX_EXPR <i_2(D), 4>;
  # DEBUG i => _3
  # DEBUG BEGIN_STMT
  _7 = (sizetype) _3;
  s_4 = "abc" + _7;
  # DEBUG s => s_4
  # DEBUG BEGIN_STMT
  _1 = __builtin_strlen (s_4);
  _6 = _1 > 3;
  return _6;
}

Whereas this code :

bool f(int i)
{
    if (i < 4)
        i = 4;

    return strlen(&"abc"[i]) > 3;
}

Optimizes to this :

;; Function f (_Z1fi, funcdef_no=287, decl_uid=9588, cgraph_uid=216, symbol_order=215)

f (int i)
{
  <bb 2> [local count: 1073741824]:
  # DEBUG BEGIN_STMT
  # DEBUG i => NULL
  # DEBUG BEGIN_STMT
  return 0;

}


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_ia32_rsqrtss properly`
### open_at : `2020-06-22T10:25:58Z`
### last_modified_date : `2023-08-24T21:31:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95814
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef float v4f32 __attribute__((vector_size(16)));

float f(float x)
{
    return __builtin_ia32_rsqrtss((v4f32){x, 0, 0, 0})[0];
}

With -O3, LLVM outputs this :

f(float):
  rsqrtss xmm0, xmm0
  ret

GCC outputs this :

f(float):
  pxor xmm1, xmm1
  movss xmm1, xmm0
  movaps xmm0, xmm1
  rsqrtss xmm0, xmm1
  ret


---


### compiler : `gcc`
### title : `Failure to optimize shift with constant to compare`
### open_at : `2020-06-22T11:00:11Z`
### last_modified_date : `2023-10-25T22:04:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95817
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int a)
{
    return (!(a >> 7));
}

This can be optimized to `return !((unsigned)a > 127);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize strchr to use memchr for string constant`
### open_at : `2020-06-22T17:07:10Z`
### last_modified_date : `2023-08-24T21:31:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95821
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
auto f(char c)
{
    return strchr("123", c);
}

This can be optimized to `return memchr("123", c, 4)`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `x86 immediates --- some redundant`
### open_at : `2020-06-23T08:49:46Z`
### last_modified_date : `2020-06-23T11:16:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95834
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Created attachment 48774
sample code

In some cases, literals could be synthesized with simple instructions instead of loading them from scratch.  Consider the case of foo(), with 4 literals that can be obtained by shifting the first one: gcc emit 5 eight-byte literals instead.

In some other cases, the same literals are loaded.  Consider the case of baz(), where 6 is loaded twice in consecutive instructions when an instruction reorder could eliminate one of the literals.

The Intel optimization manual suggests not to have many instructions that load literals in a row due to decreased code density, more difficulty to cache the decoded instructions due to lack of space, and because shorter instruction encodings are better.

Verified with gcc 9.3.0 and gcc 10.x (at godbolt), with -O3.


---


### compiler : `gcc`
### title : `Failure to remove strchr converted to bool with string literal when bit magic would be much more efficient (and sometimes smaller)`
### open_at : `2020-06-23T09:41:53Z`
### last_modified_date : `2023-08-24T21:31:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95836
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(char c)
{
    return strchr("1234567890", c);
}

This (and a lot of other similar strchr calls with string literals) can be optimized to using a few bitwise operations (here, to something like `return ((unsigned)c < 64) & ((((uint64_t)1 << c) & 287948901175001089) != 0);`). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize addition of vector elements to vector addition`
### open_at : `2020-06-23T12:26:30Z`
### last_modified_date : `2023-08-24T21:31:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95839
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float __attribute__((vector_size(8))) v2f32;

v2f32 f(v2f32 a, v2f32 b)
{
    return (v2f32){a[0] + b[0], a[1] + b[1]};
}

This can be optimized to `return a + b;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize vector load made in separate operations to single load`
### open_at : `2020-06-23T15:43:57Z`
### last_modified_date : `2023-08-24T21:31:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95845
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float __attribute__((vector_size(8))) v2f32;

v2f32 f(const float *ptr)
{
    v2f32 r;
    r[0] = ptr[0];
    r[1] = ptr[1];
    return r;
}

This can be optimized to `return (v2f32){ptr[0], ptr[1]};`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_mul_overflow pattern`
### open_at : `2020-06-23T21:50:31Z`
### last_modified_date : `2023-08-24T21:31:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95852
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(unsigned x, unsigned y, unsigned* res)
{
    *res = x * y;
    return x && ((*res / x) != y);
}

This can be optimized to `return __builtin_mul_overflow(x, y, res);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize add overflow pattern to __builtin_add_overflow`
### open_at : `2020-06-23T22:09:41Z`
### last_modified_date : `2023-08-24T21:30:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95853
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
struct result_struct
{
    bool p;
    uint64_t r;
};

result_struct size_add_overflow(uint64_t x, uint64_t y)
{
    unsigned __int128 z = (unsigned __int128)x + (unsigned __int128)y;
    return {z > SIZE_MAX, (size_t)z};
}

This can be optimized to 

result_struct size_add_overflow(uint64_t x, uint64_t y)
{ 
    uint64_t result;
    return {__builtin_add_overflow(x, y, &result), result};
}

This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `A missing ifcvt optimization to generate fcsel`
### open_at : `2020-06-24T02:48:07Z`
### last_modified_date : `2021-05-30T23:07:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95855
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For the following case,

double test(double* d1, double* d2, double* d3, int num, double* ip) {
  double dmax[3];
  for (int i = 0; i < num; i++) {
    dmax[0] = d1[i] < dmax[0] ? dmax[0] : d1[i];
    dmax[1] = d2[i] < dmax[1] ? dmax[1] : d2[i];
    dmax[2] = d3[i] < dmax[2] ? dmax[2] : d3[i];
    ip[i] = dmax[2];
  }
  return dmax[0] + dmax[1] + dmax[2];
}

gcc -O3 generates:

        movi    d0, #0
        mov     x5, 0
        cmp     w3, 0
        ble     .L3
        .p2align 3,,7
.L2:
        ldr     d4, [x0, x5, lsl 3]
        ldr     d3, [x1, x5, lsl 3]
        ldr     d0, [x2, x5, lsl 3]
        fcmpe   d1, d4
        fcsel   d1, d1, d4, gt
        fcmpe   d2, d3
        fcsel   d2, d2, d3, gt
        fcmpe   d5, d0
        bgt     .L8
        str     d0, [x4, x5, lsl 3]
        add     x5, x5, 1
        cmp     w3, w5
        ble     .L3
        fmov    d5, d0
        b       .L2
        .p2align 2,,3
.L8:
        str     d5, [x4, x5, lsl 3]
        add     x5, x5, 1
        cmp     w3, w5
        bgt     .L2
        fmov    d0, d5
.L3:
        fadd    d1, d1, d2
        fadd    d0, d1, d0
        ret

Gcc generates fcsel instruction for "dmax[0] = d1[i] < dmax[0] ? dmax[0] : d1[i];dmax[1] = d2[i] < dmax[1] ? dmax[1] : d2[i];" and doesn't do so for "dmax[2] = d3[i] < dmax[2] ? dmax[2] : d3[i];". 

Pass_split_paths splits the corresponding bb so that pass_rtl_ifcvt failed to generate the fcsel instruction. Moreover, I have found that pass_split_paths has already added some checks to aovid spoiling if-conversion if, while the above case is not covered. I plan to add some checks in pass_split_paths to fix this problem and have prepared the following patch：

diff -uprN a/gcc/gimple-ssa-split-paths.c b/gcc/gimple-ssa-split-paths.c
--- a/gcc/gimple-ssa-split-paths.c
+++ b/gcc/gimple-ssa-split-paths.c
@@ -34,6 +34,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "gimple-ssa.h"
 #include "tree-phinodes.h"
 #include "ssa-iterators.h"
+#include "cfghooks.h"
 
 /* Given LATCH, the latch block in a loop, see if the shape of the
    path reaching LATCH is suitable for being split by duplication.
@@ -254,6 +255,44 @@ is_feasible_trace (basic_block bb)
 	}
     }
 
+  /* Canonicalize the form.  */
+  if (single_pred_p (pred1) && single_pred (pred1) == pred2
+      && empty_block_p (pred1))
+    std::swap (pred1, pred2);
+
+  /* This is meant to catch another kind of cases that are likely opportunities
+     for if-conversion. After canonicalizing, PRED2 must be an empty block and
+     PRED1 must be the only predecessor of PRED2. Moreover, PRED1 is supposed
+     to end with a cond_stmt which has the same args with the PHI in BB. */
+  if (single_pred_p (pred2) && single_pred (pred2) == pred1
+      && empty_block_p (pred2))
+    {
+      gimple *cond_stmt = last_stmt (pred1);
+      if (cond_stmt && gimple_code (cond_stmt) == GIMPLE_COND)
+	{
+	  tree lhs = gimple_cond_lhs (cond_stmt);
+	  tree rhs = gimple_cond_rhs (cond_stmt);
+
+	  gimple_stmt_iterator gsi;
+	  for (gsi = gsi_start_phis (bb); !gsi_end_p (gsi); gsi_next (&gsi))
+	    {
+	      gimple *phi = gsi_stmt (gsi);
+	      if ((gimple_phi_arg_def (phi, 0) == lhs
+		   && gimple_phi_arg_def (phi, 1) == rhs)
+		  || (gimple_phi_arg_def (phi, 0) == rhs
+		      && gimple_phi_arg_def (phi, 1) == lhs))
+		{
+		  if (dump_file && (dump_flags & TDF_DETAILS))
+		    fprintf (dump_file,
+			     "Block %d appears to be optimized to a join "
+			     "point for if-convertable half-diamond.\n",
+			     bb->index);
+		  return false;
+		}
+	    }
+	}
+    }
+
   /* If the joiner has no PHIs with useful uses there is zero chance
      of CSE/DCE/jump-threading possibilities exposed by duplicating it.  */
   bool found_useful_phi = false;

With this patch, gcc -O3 generates:

        cmp     w3, 0
        ble     .L2
        mov     x5, 0
        .p2align 3,,7
.L9:
        ldr     d5, [x0, x5, lsl 3]
        ldr     d4, [x1, x5, lsl 3]
        ldr     d3, [x2, x5, lsl 3]
        fcmpe   d0, d5
        fcsel   d0, d0, d5, gt
        fcmpe   d2, d4
        fcsel   d2, d2, d4, gt
        fcmpe   d1, d3
        fcsel   d1, d1, d3, gt
        str     d1, [x4, x5, lsl 3]
        add     x5, x5, 1
        cmp     w3, w5
        bgt     .L9
.L2:
        fadd    d0, d0, d2
        fadd    d0, d0, d1
        ret


---


### compiler : `gcc`
### title : `Failure to optimize usage of __builtin_mul_overflow to small __int128-based check`
### open_at : `2020-06-24T10:51:55Z`
### last_modified_date : `2023-08-24T21:30:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95862
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(int32_t a, int32_t b)
{
    uint64_t r;
    return __builtin_mul_overflow (a, b, &r);
}

This can be optimized to `return ((__uint128_t)(a * (__int128_t)b) >> 64) & 1;` (idk if this might invoke UB, but rn at least it generates much better code on x86 than what GCC currently generates for the example I gave). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to detect table-based ctz implementation`
### open_at : `2020-06-24T11:36:09Z`
### last_modified_date : `2023-08-24T21:30:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95863
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(unsigned x)
{
    static const char table[32] = {0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8, 31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 10, 9};
    return table[((unsigned)((x & -x) * 0x077CB531U)) >> 27];
}

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90838 claims to optimize this code to `return __builtin_ctz(x)`, but this does not occur.


---


### compiler : `gcc`
### title : `vectorized shift with scalar argument not correctly costed`
### open_at : `2020-06-24T12:34:14Z`
### last_modified_date : `2020-06-25T10:30:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95866
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int x[4];
void foo(int i)
{
  i = (i+1) & 31;
  x[0] = (x[0] << i) + i;
  x[1] = (x[1] << i) + i;
  x[2] = (x[2] << i) + i;
  x[3] = (x[3] << i) + i;
}

for this testcase we're not correctly assessing that we leave the
scalar computation for (i+1) & 31 around for the generated
vectorized shift by a scalar argument.  Since we're in need of
the vector of { i,... } for the vectorized add we ideally should
simply use lane zero for the shift operand rather than the original
SSA name as we do.  Currently:

  _22 = {i_14(D), i_14(D), i_14(D), i_14(D)};
  vect_cst__23 = _22;
  vect_cst__24 = { 1, 1, 1, 1 };
  vect__1.6_25 = vect_cst__23 + vect_cst__24;
  vect_cst__26 = { 31, 31, 31, 31 };
  vect_i_15.7_27 = vect__1.6_25 & vect_cst__26;
  _1 = i_14(D) + 1;
  i_15 = _1 & 31;
  vect__2.5_21 = MEM <vector(4) int> [(int *)&x];
  vect__3.8_28 = vect__2.5_21 << i_15;
  vect__4.9_29 = vect__3.8_28 + vect_i_15.7_27;
  MEM <vector(4) int> [(int *)&x] = vect__4.9_29;
  return;

where arguably we should have done the (i+1) & 31 compute with scalars
and broadcast the result.  Assembly:

foo:
.LFB0:
        .cfi_startproc
        leal    1(%rdi), %eax
        movdqa  x(%rip), %xmm1
        movd    %edi, %xmm3
        andl    $31, %eax
        pshufd  $0, %xmm3, %xmm0
        paddd   .LC0(%rip), %xmm0
        movq    %rax, %xmm2
        pand    .LC1(%rip), %xmm0
        pslld   %xmm2, %xmm1
        paddd   %xmm1, %xmm0
        movaps  %xmm0, x(%rip)
        ret

which is really bad.  Even with that optimization simulated by using
'i' as provided by the function argument we generate

foo:
.LFB0:
        .cfi_startproc
        movdqa  x(%rip), %xmm1
        movslq  %edi, %rax
        movd    %edi, %xmm3
        movq    %rax, %xmm2
        pshufd  $0, %xmm3, %xmm0
        pslld   %xmm2, %xmm1
        paddd   %xmm1, %xmm0
        movaps  %xmm0, x(%rip)
        ret

so RTL optimizations do not recover here either.  combine sees

(insn 8 3 9 2 (set (reg:DI 89 [ i ])
        (sign_extend:DI (reg/v:SI 86 [ i ]))) "t.c":5:16 147 {*extendsidi2_rex64}
     (nil))
(insn 10 9 11 2 (set (reg:V4SI 90 [ vect__2.6 ])
        (ashift:V4SI (reg:V4SI 91 [ MEM <vector(4) int> [(int *)&x] ])
            (reg:DI 89 [ i ]))) "t.c":5:16 3739 {ashlv4si3} 
     (expr_list:REG_DEAD (reg:V4SI 91 [ MEM <vector(4) int> [(int *)&x] ])
        (expr_list:REG_DEAD (reg:DI 89 [ i ])
            (nil))))
(insn 11 10 12 2 (set (reg:V4SI 92)
        (vec_duplicate:V4SI (reg/v:SI 86 [ i ]))) "t.c":5:22 5169 {*vec_dupv4si}
     (expr_list:REG_DEAD (reg/v:SI 86 [ i ])
        (nil)))
(insn 12 11 13 2 (set (reg:V4SI 93 [ vect__3.7 ])
        (plus:V4SI (reg:V4SI 90 [ vect__2.6 ])
            (reg:V4SI 92))) "t.c":5:22 3545 {*addv4si3}

so the opportunity to "back-propagate" the vec_duplicate for the shift
isn't appearant - nor would it ever consider such thing.

So we probably should try to fix this in the vectorizer.  IIRC this support
for non-external/constant scalar shift args is reasonably new (g:49eab32e6e7).
Also if there's a vector-vector shift we should probably prefer that if
we already have a vector.


---


### compiler : `gcc`
### title : `Failure to optimize successive multiplications of ___uint128_t`
### open_at : `2020-06-24T12:40:36Z`
### last_modified_date : `2023-08-24T21:29:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95867
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
__uint128_t f(__uint128_t n)
{
    return n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n*n;
}

This can be optimized to 

__uint128_t f2(__uint128_t n)
{
    __uint128_t tmp = n * n;
    tmp = (tmp * tmp) * n;
    tmp *= tmp;
    tmp *= tmp;
    tmp *= tmp;
    tmp = (tmp * tmp) * n;
    tmp = (tmp * tmp) * n;
    tmp *= tmp;
    return (tmp * n) * tmp;
}

This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `suboptimal memcpy with embedded zero bytes`
### open_at : `2020-06-24T21:14:20Z`
### last_modified_date : `2022-02-23T04:01:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95886
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
While testing the fix for pr95189 I noticed that the memcpy expansion into copy-by-pieces is less than optimal for sequences containing embedded null bytes.  For example, in the test case below, the memcpy call in f() is expanded into what looks like a more efficient sequence than the equivalent memcpy call in g().  The only difference between the two is that the former copies a sequence of non-zero bytes while among the bytes copied by the latter is a null byte.  Clang emits the same code for g() as GCC does for f().

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout -o/dev/stdout z.c
const char a[10] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };
const char b[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8 };

void f (void *d)
{
  __builtin_memcpy (d, a, 9);   // optimal
}

void g (void *d)
{
  __builtin_memcpy (d, b, 9);   // suboptimal
}


	.file	"z.c"
	.text

;; Function f (f, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=2)

f (void * d)
{
  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (d_2(D), &a, 9); [tail call]
  return;

}


	.p2align 4
	.globl	f
	.type	f, @function
f:
.LFB0:
	.cfi_startproc
	movabsq	$578437695752307201, %rax
	movb	$9, 8(%rdi)
	movq	%rax, (%rdi)
	ret
	.cfi_endproc
.LFE0:
	.size	f, .-f

;; Function g (g, funcdef_no=1, decl_uid=1935, cgraph_uid=2, symbol_order=3)

g (void * d)
{
  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (d_2(D), &b, 9); [tail call]
  return;

}


	.p2align 4
	.globl	g
	.type	g, @function
g:
.LFB1:
	.cfi_startproc
	movq	b(%rip), %rax
	movq	%rax, (%rdi)
	movzbl	b+8(%rip), %eax
	movb	%al, 8(%rdi)
	ret
	.cfi_endproc
.LFE1:
	.size	g, .-g
	.globl	b
	.section	.rodata
	.align 8
	.type	b, @object
	.size	b, 10
b:
	.string	""
	.string	"\001\002\003\004\005\006\007\b"
	.globl	a
	.align 8
	.type	a, @object
	.size	a, 10
a:
	.string	"\001\002\003\004\005\006\007\b\t"
	.ident	"GCC: (GNU) 10.1.1 20200527"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `suboptimal memcmp with embedded zero bytes`
### open_at : `2020-06-24T21:30:13Z`
### last_modified_date : `2020-06-24T23:36:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95887
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Similar to pr95886, the memcmp expansion into compare-by-pieces is less than optimal for sequences containing embedded null bytes.  For example, in the test case below, the memcmp call in f() is expanded into what looks like a more efficient sequence than the equivalent memcmp call in g().  The only difference between the two is that the former copies a sequence of non-zero bytes while among the bytes copied by the latter is a null byte.  Clang emits the same code for g() as GCC does for f().

I believe the root cause of the problem in both cases is working with nul-terminated strings (using the result of c_getstr() without the size of what it points to) instead of with arbitrary byte sequences.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout -o/dev/stdout z.c
const char a[8] = { 1, 2, 3, 4, 5, 6, 7, 8 };
const char b[8] = { 0, 1, 2, 3, 4, 5, 6, 7 };

int f (void *d)
{
  return __builtin_memcmp (d, a, 8);
}

int g (void *d)
{
  return __builtin_memcmp (d, b, 8);
}


	.file	"z.c"
	.text

;; Function f (f, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=2)

f (void * d)
{
  int _3;

  <bb 2> [local count: 1073741824]:
  _3 = __builtin_memcmp (d_2(D), &a, 8); [tail call]
  return _3;

}


	.p2align 4
	.globl	f
	.type	f, @function
f:
.LFB0:
	.cfi_startproc
	movl	$8, %edx
	movl	$a, %esi
	jmp	memcmp
	.cfi_endproc
.LFE0:
	.size	f, .-f

;; Function g (g, funcdef_no=1, decl_uid=1935, cgraph_uid=2, symbol_order=3)

g (void * d)
{
  int _3;

  <bb 2> [local count: 1073741824]:
  _3 = __builtin_memcmp (d_2(D), &b, 8); [tail call]
  return _3;

}


	.p2align 4
	.globl	g
	.type	g, @function
g:
.LFB1:
	.cfi_startproc
	movzbl	(%rdi), %eax
	ret
	.cfi_endproc
.LFE1:
	.size	g, .-g
	.globl	b
	.section	.rodata
	.align 8
	.type	b, @object
	.size	b, 8
b:
	.string	""
	.ascii	"\001\002\003\004\005\006\007"
	.globl	a
	.align 8
	.type	a, @object
	.size	a, 8
a:
	.ascii	"\001\002\003\004\005\006\007\b"
	.ident	"GCC: (GNU) 10.1.1 20200527"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `Missing optimization comparing equals two fields of the same struct that is passed via a register`
### open_at : `2020-06-25T08:38:57Z`
### last_modified_date : `2021-08-16T01:05:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95891
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `enhancement`
### contents :
I'm sorry, this is perhaps not the correct component but my knowledge of gcc internals does not allow me to do more than guess.

For all version of gcc I've tried, the following code:

struct point {
    int x, y;
};

bool f(point a, point b) {
    return a.x == b.x && a.y == b.y;
}

bool f(unsigned long long a, unsigned long long b) {
    return a == b;
}

is compiled to

f(point, point):
        xor     eax, eax
        cmp     edi, esi
        je      .L5
        ret
.L5:
        sar     rdi, 32
        sar     rsi, 32
        cmp     edi, esi
        sete    al
        ret
f(unsigned long long, unsigned long long):
        cmp     rdi, rsi
        sete    al
        ret

I'd expect f(point, point) to have the same assembly as f(unsigned long long, unsigned long long).

Yours,

-- Jean-Marc Bourguet


---


### compiler : `gcc`
### title : `vector shift by lane zero generates inter unit move`
### open_at : `2020-06-25T10:04:08Z`
### last_modified_date : `2020-06-25T10:12:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95894
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef int v4si __attribute__((vector_size(16)));
v4si foo (v4si x)
{
  return x << x[0];
}

generates

foo:
.LFB0:
        .cfi_startproc
        movd    %xmm0, %eax
        cltq
        movq    %rax, %xmm1
        pslld   %xmm1, %xmm0
        ret

while we could use sth like

         pxor %xmm1, %xmm1
         punpckldq %xmm0, %xmm1
         pslld %xmm1, %xmm0

to zero-extend x[0] to DImode in a SSE reg.  Unfortunately even

typedef long v2di __attribute__((vector_size(16)));
v2di bar (v2di x)
{
  return x << x[0];
}

shows this behavior useless behavior:

bar:
.LFB1:
        .cfi_startproc
        movq    %xmm0, %rax
        cltq
        movq    %rax, %xmm1
        psllq   %xmm1, %xmm0
        ret

because the gimplifier casts the shift amount to unsigned int :(


---


### compiler : `gcc`
### title : `Failure to optimize _mm_unpacklo_epi8 with 0 as right operand to _mm_cvtepu8_epi16`
### open_at : `2020-06-26T01:11:49Z`
### last_modified_date : `2023-08-24T21:29:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95905
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
__m128i f(__m128i a)
{
    return _mm_unpacklo_epi8(a, _mm_setzero_si128());
}

This can be optimized to `return _mm_cvtepu8_epi16(a);` (with -msse4`). LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to recognize max pattern with mask`
### open_at : `2020-06-26T04:17:55Z`
### last_modified_date : `2023-08-24T21:29:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95906
### status : `ASSIGNED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef int8_t v16i8 __attribute__((__vector_size__ (16)));

v16i8 f(v16i8 a, v16i8 b)
{
    v16i8 cmp = (a > b);
    return (cmp & a) | (~cmp & b);
}

int f2(int a, int b)
{
    int cmp = -(a > b);
    return (cmp & a) | (~cmp & b);
}

f can be optimized to `__builtin_ia32_pmaxsb128` (on x86 with `-msse4`) (the `pmax` instructions can be used for the same pattern with similar types) and `f2` can be optimized to using `MAX_EXPR` (they're essentially the same but I've included the pattern for vectorized types because I originally found this in a function (which was made before SSE4) made for SSE). LLVM does these transformations, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to optimize saturated add properly`
### open_at : `2020-06-26T14:26:39Z`
### last_modified_date : `2023-08-24T21:29:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95914
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
uint16_t add16(uint16_t a, uint16_t b)
{
    uint16_t c = a + b;
    return c < a ? -1 : c;
}

With -O3, LLVM outputs this on x86 :

add16(unsigned short, unsigned short):
  add di, si
  mov eax, 65535
  cmovae eax, edi
  ret

GCC outputs this :

add16(unsigned short, unsigned short):
  xor eax, eax
  add si, di
  jc .L8
.L2:
  test ax, ax
  mov eax, -1
  cmove eax, esi
  ret
.L8:
  mov eax, 1
  jmp .L2

I don't know if this doesn't also occur on also architectures but the output seems rather crippled on x86, at least.


---


### compiler : `gcc`
### title : `Failure to optimize `((b ^ a) & c) ^ a` to `(a & ~c) | (b & c)` the right way on architectures with andnot`
### open_at : `2020-06-26T23:17:23Z`
### last_modified_date : `2023-08-24T21:29:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95922
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int a, int b, int c)
{
    return ((b ^ a) & c) ^ a;
}

This can be optimized to `return (a & ~c) | (b & c);` on processors that have andnot instructions (at least according to LLVM, which does this transformation on x86 with -mbmi).


---


### compiler : `gcc`
### title : `Failure to optimize bool checks into and`
### open_at : `2020-06-27T00:49:11Z`
### last_modified_date : `2023-08-24T21:29:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95923
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(bool a, bool b)
{
    if (!a && !b)
        return 0;
    if (!a && b)
        return 0;
    if (a && !b)
        return 0;
    return 1;
}

This can be optimized to `return a & b;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize some bit magic to one of the operands`
### open_at : `2020-06-27T01:06:58Z`
### last_modified_date : `2023-08-24T21:29:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95924
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(bool a, bool b)
{
    return (a | !b) ? (~a & b) ? 0 : a : 0;
}

This can be optimized to `return a;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize xor pattern when using temporary variable`
### open_at : `2020-06-27T02:08:10Z`
### last_modified_date : `2023-09-11T02:25:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95926
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(bool a, bool b)
{
    bool tmp = a & b;
    return (tmp ^ b) | (tmp ^ a);
}

This code does not optimize as well as this code :

bool f(bool a, bool b)
{
    return ((a & b) ^ b) | ((a & b) ^ a);
}

which optimizes to `return a ^ b;`. This seems rather odd considering that they are equivalent and the first example is just a simplification of the second (i.e. made more readable).


---


### compiler : `gcc`
### title : `Failure to optimize tautological comparisons of comparisons to a single one`
### open_at : `2020-06-27T03:07:38Z`
### last_modified_date : `2023-08-24T22:16:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95929
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int a, int b)
{
    return (((b != 0) & (a == 0)) | ((a != 0) & (b == 0)));
}

This can be optimized to `(a != 0) ^ (b != 0)`. I originally found this while compiling this code :

inline bool nand(bool a, bool b)
{
    return !(a && b);
}

int f(int a, int b)
{
    return nand(nand(b, nand(a, a)), nand(a, nand(b, b)));
}

Which GCC compiles to the above example, and that LLVM optimizes with the transformation I gave (strangely, LLVM does not seem to optimize the example at the top of this bug report to the transformed version if directly given the top example, which is why I'm giving these details as I'm thinking there could be some kind of UB weirdness or something like that with the transformations here)


---


### compiler : `gcc`
### title : `Passing a struct by value wastes 16 bytes on the stack`
### open_at : `2020-06-27T16:44:28Z`
### last_modified_date : `2021-08-16T00:57:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95941
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `normal`
### contents :
Consider this C code:

extern struct foo {
    long x, y, z;
} s;

void f(struct foo);

void g(void) {
    f(s);
}

At "-O3", it compiles to this:

g:
        subq    $16, %rsp
        pushq   s+16(%rip)
        pushq   s+8(%rip)
        pushq   s(%rip)
        call    f
        addq    $40, %rsp
        ret

There's no reason at all to use that extra 16 bytes of stack space. It should have compiled to this instead:

g:
        pushq   s+16(%rip)
        pushq   s+8(%rip)
        pushq   s(%rip)
        call    f
        addq    $24, %rsp
        ret


---


### compiler : `gcc`
### title : `[meta-bug] auto-parallelization`
### open_at : `2020-06-28T10:50:29Z`
### last_modified_date : `2020-06-28T10:53:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95946
### status : `UNCONFIRMED`
### tags : `meta-bug, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :



---


### compiler : `gcc`
### title : `[meta-bug] Inefficient arm_neon.h code for AArch64`
### open_at : `2020-06-29T11:03:55Z`
### last_modified_date : `2021-09-02T07:59:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95958
### status : `NEW`
### tags : `meta-bug, missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
This meta-bug is to collect AArch64 performance improvements that
require or have a strong relationship with arm_neon.h intrinsics.

This PR overlaps to some extent with PR47562, but that PR is more
general in two respects: it covers general arm_neon.h improvements,
and it covers both AArch32 and AArch64.


---


### compiler : `gcc`
### title : `GCC should re-vectorize vector code with larger VF`
### open_at : `2020-06-29T12:09:28Z`
### last_modified_date : `2021-08-16T21:36:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95960
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
For the following simple case GCC should vectorize it using AVX:

typedef double __attribute__((vector_size(16))) v2df;
double a[4];
double b[4];
void f()
{
  *(v2df *)&b[0] += *(v2df *)&a[0];
  *(v2df *)&b[2] += *(v2df *)&a[2];
}

Similar opportunities may exist for loops where also Neon -> SVE transforms
may be profitable.  Starting with BB vectorization might be easier since
you only need to consider SLP.


---


### compiler : `gcc`
### title : `Inefficient code for simple arm_neon.h iota operation`
### open_at : `2020-06-29T12:49:23Z`
### last_modified_date : `2021-12-03T17:05:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95962
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
For:

#include <arm_neon.h>

int32x4_t
foo (void)
{
  int32_t array[] = { 0, 1, 2, 3 };
  return vld1q_s32 (array);
}

we produce:

foo:
.LFB4217:
        .cfi_startproc
        sub     sp, sp, #16
        .cfi_def_cfa_offset 16
        mov     x0, 2
        mov     x1, 4294967296
        movk    x0, 0x3, lsl 32
        stp     x1, x0, [sp]
        ldr     q0, [sp]
        add     sp, sp, 16
        .cfi_def_cfa_offset 0
        ret

In contrast, clang produces essentially perfect code:

        adrp    x8, .LCPI0_0
        ldr     q0, [x8, :lo12:.LCPI0_0]
        ret

I think the problem is a combination of two things:

- __builtin_aarch64_ld1v4si & co. are treated as general
  functions rather than pure functions, so in principle
  it could write to the given address.  This stops us
  promoting the array to a constant.

- The loads could be reduced to native gimple-level
  operations, at least on little-endian targets.

IMO this a bug rather than an enhancement.  Intrinsics only
exist to optimise code, and what GCC is doing falls short
of what users should reasonably expect.


---


### compiler : `gcc`
### title : `AArch64 arm_neon.h arithmetic functions lack appropriate attributes`
### open_at : `2020-06-29T13:03:16Z`
### last_modified_date : `2021-08-20T11:45:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95964
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
For:

---------------------------------------
#include <arm_neon.h>
#include <vector>

std::vector<float32x4_t> a, b, c;

void
foo (size_t n)
{
  for (size_t i = 0; i < n; ++i)
    a[i] = vfmaq_f32(a[i], b[i], c[i]);
}
---------------------------------------

we generate code that loads the start of a, b and c
in every iteration of the loop:

---------------------------------------
        .cfi_startproc
        cbz     x0, .L4
        adrp    x3, .LANCHOR0
        add     x3, x3, :lo12:.LANCHOR0
        mov     x2, 0
        .p2align 3,,7
.L6:
        ldr     x4, [x3]
        lsl     x1, x2, 4
        ldr     x6, [x3, 24]
        add     x2, x2, 1
        ldr     x5, [x3, 48]
        ldr     q0, [x4, x1]
        ldr     q2, [x6, x1]
        ldr     q1, [x5, x1]
        fmla    v0.4s, v2.4s, v1.4s
        str     q0, [x4, x1]
        cmp     x0, x2
        bne     .L6
.L4:
        ret
        .cfi_endproc
---------------------------------------

The problem is that __builtin_aarch64_fmav4sf and similar
operations are treated as general functions that can read
memory, write memory, and call other functions.  If the
intrinsic is replaced by arithmetic then the start addresses
are hoisted, as expected.


---


### compiler : `gcc`
### title : `soft float operations are not tail called`
### open_at : `2020-06-29T15:00:26Z`
### last_modified_date : `2021-08-14T09:43:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95966
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
i'd expect this to be a tail call into the soft float add
operation on soft float targets:

fp_t foo(fp_t a, fp_t b)
{
    return a + b;
}

e.g. on x86 with 'typedef __float128 fp_t' the generated code is

foo:
        sub     rsp, 8
        call    __addtf3
        add     rsp, 8
        ret

on aarch64 with 'typedef long double fp_t' the generated code is

foo:
        stp     x29, x30, [sp, -16]!
        mov     x29, sp
        bl      __addtf3
        ldp     x29, x30, [sp], 16
        ret

i see similar code on other softfp targets.


---


### compiler : `gcc`
### title : `Poor aarch64 vector constructor code when using arm_neon.h`
### open_at : `2020-06-29T15:20:13Z`
### last_modified_date : `2023-05-12T06:06:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95967
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
We generate poor code for the attached functions:

f1:
        movi    v4.4s, 0
        ins     v4.s[0], v0.s[0]
        ins     v4.s[1], v1.s[0]
        ins     v4.s[2], v2.s[0]
        mov     v0.16b, v4.16b
        ins     v0.s[3], v3.s[0]
        ret

f2:
        dup     v0.4s, v0.s[0]
        ins     v0.s[1], v1.s[0]
        ins     v0.s[2], v2.s[0]
        ins     v0.s[3], v3.s[0]
        ret

f3:
        sub     sp, sp, #16
        stp     s0, s1, [sp]
        stp     s2, s3, [sp, 8]
        ldr     q0, [sp]
        add     sp, sp, 16
        ret

g1:
        movi    v0.4s, 0
        ld1     {v0.s}[0], [x0]
        ld1     {v0.s}[1], [x1]
        ld1     {v0.s}[2], [x2]
        ld1     {v0.s}[3], [x3]
        ret

g2:
        ld1r    {v0.4s}, [x0]
        ld1     {v0.s}[1], [x1]
        ld1     {v0.s}[2], [x2]
        ld1     {v0.s}[3], [x3]
        ret

g3:
        sub     sp, sp, #16
        ldr     s0, [x3]
        ldr     s3, [x0]
        ldr     s2, [x1]
        ldr     s1, [x2]
        stp     s3, s2, [sp]
        stp     s1, s0, [sp, 8]
        ldr     q0, [sp]
        add     sp, sp, 16
        ret

All three f functions should generate:

        mov     v0.s[1], v1.s[0]
        mov     v0.s[2], v2.s[0]
        mov     v0.s[3], v3.s[0]
        ret

and all three g functions should generate:

        ldr     s0, [x0]
        ld1     { v0.s }[1], [x1]
        ld1     { v0.s }[2], [x2]
        ld1     { v0.s }[3], [x3]
        ret

which is what current Clang does.

Getting the right code for f3 and g3 depends on the fix for PR95962.
There's a reasonable chance that PR95962 will be enough on its own
to fix f3 and g3, but I included them just in case it isn't.


---


### compiler : `gcc`
### title : `Use of __builtin_aarch64_im_lane_boundsi in AArch64 arm_neon.h interferes with gimple optimisation`
### open_at : `2020-06-29T15:50:24Z`
### last_modified_date : `2021-09-13T15:20:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95969
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
For:

----------------------------------------------------
#include <arm_neon.h>

void
f (float32x4_t **ptr)
{
  float32x4_t res = vsetq_lane_f32 (0.0f, **ptr, 0);
  **ptr = res;
}
----------------------------------------------------

the final gimple .optimized dump looks like:

----------------------------------------------------
  float32x4_t __vec;
  float32x4_t * _1;
  __Float32x4_t _2;
  float32x4_t * _3;

  <bb 2> [local count: 1073741824]:
  _1 = *ptr_5(D);
  _2 = *_1;
  __builtin_aarch64_im_lane_boundsi (16, 4, 0);
  __vec_8 = BIT_INSERT_EXPR <_2, 0.0, 0>;
  _3 = *ptr_5(D);
  *_3 = __vec_8;
  return;
----------------------------------------------------

where we still have two loads from *ptr.  This is because
__builtin_aarch64_im_lane_boundsi has no attributes:
it's modelled a general function that could do pretty
much anything.

Although we fix this testcase in the RTL optimisers, it's easy
for the issue to cause unoptimised code in larger, more realistic
testcases.

The problem is similar to PR95964.  The difficulty is that
here we have to model the function as having some kind of
side-effect, otherwise it will simply get optimised away.

Ideally we'd fix this by implementing the intrinsics directly
in the compiler and doing the checks in the frontend via
TARGET_CHECK_BUILTIN_CALL.  That's obviously a big change
though.  Until then, we should optimise away calls whose
arguments are already correct so that they don't clog
up the IL.

If not returning a value makes it harder to fold the call
for some reason, perhaps an alternative would be to pass
a vector value through a dummy const function, e.g.:

  _4 = __builtin_aarch64_... (_2, 16, 4, 0);
  __vec_8 = BIT_INSERT_EXPR <_4, 0.0, 0>;

That might not be necessary though -- haven't checked.


---


### compiler : `gcc`
### title : `AArch64 arm_neon.h stores interfere with gimple optimisations`
### open_at : `2020-06-29T16:38:44Z`
### last_modified_date : `2021-09-04T21:38:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95974
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
For:

---------------------------------------
#include <arm_neon.h>
#include <vector>

std::vector<float> a;

void
f (size_t n, float32x4_t v)
{
  for (size_t i = 0; i < n; i += 4)
    vst1q_f32 (&a[i], v);
}
---------------------------------------

we generate code that loads the start address of
"a" in every iteration of the loop:

---------------------------------------
        cbz     x0, .L4
        adrp    x4, .LANCHOR0
        add     x4, x4, :lo12:.LANCHOR0
        mov     x1, 0
        .p2align 3,,7
.L6:
        ldr     x3, [x4]
        lsl     x2, x1, 2
        add     x1, x1, 4
        str     q0, [x3, x2]
        cmp     x0, x1
        bhi     .L6
.L4:
        ret
---------------------------------------

This is really the store equivalent of PR95962.  The problem is
that __builtin_aarch64_st1v4sf is modelled as a general function
that could read and write from arbitrary memory.  As with PR95962,
one option would be to lower to gimple accesses where possible,
at least for little-endian targets.


---


### compiler : `gcc`
### title : `Copy elision with conditional`
### open_at : `2020-06-30T14:24:49Z`
### last_modified_date : `2023-06-06T16:11:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96004
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
Consider the example:


struct Struct {
    Struct() = default;
    Struct(Struct&&);
};

Struct question10(bool b) {
    if (b) {
        Struct s{};
        return s;
    } else {
        return {};
    }
}


It is possible to elide move constructor call as the lifetimes of object `s` and `return {}` do not intersect.

(some other compilers already do copy elision in that place https://godbolt.org/z/wdpLkT )


---


### compiler : `gcc`
### title : `missed optimization with floating point operations and integer literals`
### open_at : `2020-06-30T17:35:54Z`
### last_modified_date : `2020-07-01T10:11:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96009
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `enhancement`
### contents :
Consider the two variants below:

double foo(char i) {
    double f = i * 100;
    return f / 100;
}

double bar(char i) {
    return i;
}

Compiled under -Ofast with gcc 9.1.0, we get the following:

foo:
movsbl  %dil, %edi
pxor  %xmm0, %xmm0
imull  $100, %edi, %edi
cvtsi2sdl %edi, %xmm0
mulsd  .LC0(%rip), %xmm0
ret

bar:
movsbl  %dil, %edi
pxor  %xmm0, %xmm0
cvtsi2sdl %edi, %xmm0
ret

.LC0:
.long  1202590843
.long  1065646817

But I think foo should be to be simplified to the same as bar, right?

This seems somewhat similar to PR91739 and PR84997, although not quite the same as far as I can discern.

(Also, separately, aren't the first two instructions of bar unnecessary? Can't we just cvtsi2sdl and ret?)

seth@dev4:$ /toolchain14/bin/g++ -v
Using built-in specs.
COLLECT_GCC=/toolchain14/bin/g++
COLLECT_LTO_WRAPPER=/toolchain14/libexec/gcc/x86_64-pc-linux-gnu/9.1.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc_9_1_0/configure --prefix=/toolchain14 --enable-languages=c,c++,fortran --enable-lto --disable-plugin --program-suffix=-9.1.0 --disable-multi-lib
Thread model: posix
gcc version 9.1.0 (GCC)


---


### compiler : `gcc`
### title : `Powerpc suboptimal register spill in likely path`
### open_at : `2020-07-01T09:57:38Z`
### last_modified_date : `2022-03-08T16:21:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96017
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
-- test.c --
extern int foo;
extern void slowpath(int *);

int test(int *val)
{
        int ret = foo;

        if (__builtin_expect(*val != 0, 0))
                slowpath(val);

        return ret;
}
--

Compiling with -O2 gives the following asm. It seems possible for the fast path case to avoid the stack frame by using a volatile register to save the val argument in case the slow path needs it (or alternatively to save the load from 'foo', as r31 is used now, but that requires an extra register move on a critical path for the return value). This should be smaller and faster code even for the slow path too.

        addis   r2,r12,0
        addi    r2,r2,0
        lwz     r9,0(r3)
        addis   r10,r2,0
        ld      r10,0(r10)
        std     r31,-8(r1)
        stdu    r1,-48(r1)
        lwa     r31,0(r10)
        cmpwi   r9,0
        bne     1f
        addi    r1,r1,48
        mr      r3,r31
        ld      r31,-8(r1)
        blr
        nop
        ori     r2,r2,0
1:      mflr    r0
        std     r0,64(r1)
        bl      slowpath
        nop
        ld      r0,64(r1)
        addi    r1,r1,48
        mr      r3,r31
        ld      r31,-8(r1)
        mtlr    r0
        blr


---


### compiler : `gcc`
### title : `suboptimal codegen for store low 16-bits value`
### open_at : `2020-07-02T12:45:07Z`
### last_modified_date : `2022-10-16T17:20:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96031
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
For the following code, as instruction strh only store the low 16-bits value, so the 'and     w2, w2, 65535 ' is redundant.
test base on the ARM64 gcc 8.2 on https://gcc.godbolt.org/, so get complicated assemble.

typedef unsigned int UINT32;
typedef unsigned short UINT16;


UINT16 array[12];

void foo (UINT32 len, UINT32 step)		
{
    UINT32 index = 1;

    for (index = 1 ; index < len; index++ )
        {
            array[index] = index * step;
        }
}

// the assemble of kernel loop body --------------------------
        b       .L4         //
.L6:
        add     x3, x3, 2 // ivtmp.6, ivtmp.6,
.L4:
        strh    w2, [x4, 2]     // ivtmp.4, MEM[base: _2, offset: 2B]
        add     w2, w1, w2        // tmp105, _12, ivtmp.4
        and     w2, w2, 65535     // ivtmp.4, tmp105 ????
        cmp     x3, x0    // ivtmp.6, _23
        mov     x4, x3    // ivtmp.6, ivtmp.6
        bne     .L6             //,


---


### compiler : `gcc`
### title : `alloca(0) triggers TCO for VLA`
### open_at : `2020-07-03T21:56:27Z`
### last_modified_date : `2021-12-05T06:31:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96051
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.1`
### severity : `enhancement`
### contents :
Created attachment 48831
TCO with alloca(0)

I have written a function which is properly translated into tail-call-optimized code.
Then I tried to replace a call to alloca inside that function by a VLA.
Doing so I found a strange behaviour.

Using the VLA in place of alloca, but leaving the statement '(int*)alloca(0)' in the source code, still triggers TCO. Removing the (unused) call to alloca(0) prevents the TCO.
It is weird because I expected that alloca(0) is removed because it should have no effect.

Summary:
1. alloca(n); // n > 0; works as expected (TCO)
2. using VLA and alloca(0); the same output (assembler) as in case 1 (TCO)
3. using VLA without alloca; works as expected (no TCO)


Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Debian 8.3.0-6' --with-bugurl=file:///usr/share/doc/gcc-8/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-8 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 8.3.0 (Debian 8.3.0-6)


---


### compiler : `gcc`
### title : `Miss optimization:Finding SLP sequences from reductions sometimes is better than finding from reduction chains`
### open_at : `2020-07-04T01:51:05Z`
### last_modified_date : `2020-07-20T09:02:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96053
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
command:
gcc -S -O2 -ftree-vectorize test.c -funsafe-math-optimizations  -fno-tree-reassoc -march=armv8.2-a+sve -msve-vector-bits=128

gcc version 11.0.0 20200629

In vectorization, finding SLP sequences from reduction chains has priority over from reductions.  But sometimes, finding SLP sequences from reductions is a better way to do vectorization than from reduction chains.

testcase:
double f(double *a, double *b)
{
  double res1 = 0;
  double res0 = 0;
  for (int i = 0 ; i < 1000; i+=4) {
    res0 += a[i] * b[i];
    res1 += a[i+1] * b[i*1];
    res0 += a[i+2] * b[i+2];
    res1 += a[i+3] * b[i+3];
  }
  return res0 + res1;
}

I have two imperfect solutions, one is to add a control option, and the other is to use the cost model to evaluate which is better.  The first one is very difficult for users to use, and the second one is difficult to implement.

Does anyone have a better suggestion?


---


### compiler : `gcc`
### title : `Partial register stall caused by avoidable use of SETcc, and useless MOVZBL`
### open_at : `2020-07-04T17:29:10Z`
### last_modified_date : `2020-07-06T16:13:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96062
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `normal`
### contents :
Consider this C code:

long ps4_syscall0(long n) {
    long ret;
    int carry;
    __asm__ __volatile__(
        "syscall"
        : "=a"(ret), "=@ccc"(carry)
        : "a"(n)
        : "rcx", "r8", "r9", "r10", "r11", "memory"
    );
    return carry ? -ret : ret;
}

With "-O3", it results in this assembly:

ps4_syscall0:
        movq    %rdi, %rax
        syscall
        setc    %dl
        movq    %rax, %rdi
        movzbl  %dl, %edx
        negq    %rdi
        testl   %edx, %edx
        cmovne  %rdi, %rax
        ret

On modern Intel CPUs, doing "setc %dl" creates a false dependency on rdx. Doing "movzbl %dl, %edx" doesn't do anything to fix that. Here's some ways that we could improve this code, without having to fall back to a conditional branch:

1. Get rid of "movzbl %dl, %edx" (since it doesn't help), and then do "testb %dl, %dl" instead of "testl %edx, %edx".
2. Possibly in addition to #1, use dh instead of dl, since high-byte registers are still renamed.
3. Instead of #1 and #2, replace the whole sequence between "syscall" and "ret" with this:

        sbbq    %rcx, %rcx
        xorq    %rcx, %rax
        subq    %rcx, %rax

On Intel (but not AMD), the sbb has a false dependency too, but it's still a lot less shuffling values around.
4. Instead of #1, #2, and #3, replace the whole sequence between "syscall" and "ret" with this:

        leaq    -1(%rax), %rcx
        notq    %rcx
        cmovc   %rcx, %rax

I like this one the best. No false dependencies at all, and still way less shuffling values around.


---


### compiler : `gcc`
### title : `Move elision of returned automatic variable doesn't happen the variable is enclosed in a block`
### open_at : `2020-07-05T08:19:29Z`
### last_modified_date : `2020-07-05T09:44:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96065
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.1.0`
### severity : `normal`
### contents :
Consider the following code (at Godbolt's: https://gcc.godbolt.org/z/CyqPF9 ):

```
struct A
{
    A();
    A(A&&);
    A(A const&);
    A& operator=(A&&);
    A& operator=(A const&);
};

A getA()
{
    {
        A a;
        return a;
    }
}

int main()
{
    const A a=getA();
}
```

Here we get A::A() call followed by A::A(A&&) call. If we remove the inner braces in getA(), move elision happens, so only A::A() is called. I'd expect that without removal of braces move elision would also happen.

This problem of missing move elision also affects the case when the block belongs to an if statement. Same pattern happens with copy elision if we comment out the move constructor.

For comparison, MSVC 19.24 (with /O2 flag) and Clang 10.0 (by default) both elide the move.


---


### compiler : `gcc`
### title : `Range insertion into unordered_map is less effective than a loop with insertion`
### open_at : `2020-07-06T18:37:25Z`
### last_modified_date : `2021-10-21T14:59:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96088
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `11.0`
### severity : `normal`
### contents :
Consider the function f1:

static constexpr std::initializer_list<std::pair<const char*, int>> lst = {
    {"long_str_for_dynamic_allocating", 1}};

void f1() {
    std::unordered_map<std::string, int> m(1);
    m.insert(lst.begin(), lst.end());
}


It creates a temporary and as a result makes 4 allocations. Meanwhile f2 does not create a temporary and does aonly 3 allocations:

void f2() {
    std::unordered_map<std::string, int> m(1);
    for (const auto& x : lst) {
        m.insert(x);
    }
}


Godbolt playground: https://godbolt.org/z/VapmBU


---


### compiler : `gcc`
### title : `Failure to optimize bool division`
### open_at : `2020-07-07T14:00:04Z`
### last_modified_date : `2023-09-03T12:15:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96094
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(bool a, bool b)
{
    return a / b;
}

This can be optimized to `return a;`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Different behavior in DSE pass`
### open_at : `2020-07-08T08:29:51Z`
### last_modified_date : `2020-07-09T07:48:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96108
### status : `WAITING`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
There are different behavior in DSE pass between gcc10 and gcc7.3. BUT I do not known the correctness of dse_classify_store.

In gcc7.3, we could get DSE_STORE_DEAD when we get DSE_STORE_MABEY_PARTIAL_DEAD with gcc10.


$ cat tauth.c 
struct aa;
static inline struct aa* get_aa(void) {
  struct aa* a;
  return a;
}
struct aa {
  int b;
};
void test()
{
  get_aa()->b &= 0xfff0;
}

$(7.3)aarch64_be-linux-gnu-gcc -S tauth.c -O2  -o - 
	.arch armv8-a
	.file	"tauth.c"
	.text
	.align	2
	.p2align 3,,7
	.global	test
	.type	test, %function
test:
.LFB1:
	.cfi_startproc
	ret
	.cfi_endproc
.LFE1:
	.size	test, .-test
	.ident	"GCC: 7.3.0"
	.section	.note.GNU-stack,"",@progbits

$(10.1)aarch64_be-linux-gnu-gcc -S tauth.c -O2 -o -
	.arch armv8-a
	.file	"tauth.c"
	.text
	.align	2
	.p2align 4,,11
	.global	test
	.type	test, %function
test:
.LFB1:
	.cfi_startproc
	mov	x0, 0
	ldr	w1, [x0]
	and	w1, w1, 65520
	str	w1, [x0]
	ret
	.cfi_endproc
.LFE1:
	.size	test, .-test
	.ident	"GCC:  10.1.0"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] bswap not detected by bswap pass, unexpected results between optimization levels`
### open_at : `2020-07-09T14:01:07Z`
### last_modified_date : `2023-07-07T10:37:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96135
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
This is an odd one, and it seems different from the other bswap bugs that I could find in bugzilla.  This is on x64.

Compiler Explorer link is here: https://godbolt.org/z/arTf5T

Full source code:
==============================================================
constexpr long long bswap64(long long in) // unsigned long long behaves the same
{
    union {
        long long v;
        char c[8];
    } u{in};
    union {
        char c[8];
        long long v;
    } v{ u.c[7], u.c[6], u.c[5], u.c[4], u.c[3], u.c[2], u.c[1], u.c[0]};
    return v.v;
}

long long f(long long i)
{
    return bswap64(i);
}

constexpr long long bswapD(double x)
{
    return bswap64(*(long long*)&x);
}

long long g(double x)
{
    return bswapD(x);
}
===============================================================

There are three observations / bugs:
1) bswapD is never recognized as byte-swapping
2) bswap64 is optimized to bswap at -O2 but not at -O3
3) 131t.bswap never shows bswap, apparently the pass doesn't detect this way of writing bswap, leaving it to the RTL optimizers.  Hence I classified this as tree-optimization bug.

Verified at -O2 with 9.3, 10.1 and trunk on the compiler explorer.

I'm flagging this as a regression because at -O2 gcc 8.3 detects bswap in both cases, but I'm guessing that this is by some accident.  In 7.5 neither function is compiled as bswap.


---


### compiler : `gcc`
### title : `gcc.dg/vect/slp-46.c on aarch64`
### open_at : `2020-07-10T12:52:17Z`
### last_modified_date : `2021-08-31T05:15:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96149
### status : `NEW`
### tags : `missed-optimization, xfail`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
gcc.dg/vect/slp-46.c fails on aarch64 since it was introduced.

In the logs I can see:
PASS: gcc.dg/vect/slp-46.c execution test
gcc.dg/vect/slp-46.c: pattern found 0 times
FAIL: gcc.dg/vect/slp-46.c scan-tree-dump-times vect "vectorizing stmts using SLP" 2


---


### compiler : `gcc`
### title : `[10 Regression] -O3/-ftree-slp-vectorize turns ROL into a mess`
### open_at : `2020-07-11T16:01:41Z`
### last_modified_date : `2023-07-07T08:55:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96166
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
inline void swap(int &x, int &y)
{
  int tmp = x;
  x = y;
  y = tmp;
}

void bar(int (&x)[2])
{
  int y[2];
  __builtin_memcpy(&y, &x, sizeof x);
  swap(y[0], y[1]);
  __builtin_memcpy(&x, &y, sizeof x);
}


GCC 9 (-Os/O2/O3) produces:
  rolq $32, (%rdi)

GCC 10/trunk (-O3/-ftree-slp-vectorize) produces:
  movq (%rdi), %rax
  movd (%rdi), %xmm1
  sarq $32, %rax
  movq %rax, %xmm0
  punpckldq %xmm1, %xmm0
  movq %xmm0, (%rdi)


https://godbolt.org/z/5h3bW8


---


### compiler : `gcc`
### title : `fails to detect ROL pattern in simple case, but succeeds when operand goes through memcpy`
### open_at : `2020-07-11T16:10:26Z`
### last_modified_date : `2023-08-04T22:31:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96167
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
inline void swap(int &x, int &y)
{
  int tmp = x;
  x = y;
  y = tmp;
}

void foo(int (&x)[2])
{
  swap(x[0], x[1]);
}

void bar(int (&x)[2])
{
  int y[2];
  __builtin_memcpy(&y, &x, sizeof x);
  swap(y[0], y[1]);
  __builtin_memcpy(&x, &y, sizeof x);
}


foo:
  movl (%rdi), %eax
  movl 4(%rdi), %edx
  movl %eax, 4(%rdi)
  movl %edx, (%rdi)
  ret

bar:
  rolq $32, (%rdi)
  ret

https://godbolt.org/z/Tcz7YG


---


### compiler : `gcc`
### title : `Failure to optimize direct assignment to bitfield through shifts`
### open_at : `2020-07-11T21:02:22Z`
### last_modified_date : `2023-09-03T12:13:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96172
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
struct ret_struct
{
    union
    {
        struct
        {
            unsigned int a : 1;
            unsigned int b : 1;
            unsigned int c : 1;
        };
        unsigned char as_char;
    };
};

ret_struct f1(uint32_t x)
{
    x >>= 16;
    ret_struct result;
    result.a = x;
    result.b = (x >> 1);
    result.c = (x >> 2);
    return result;
}
// Compiling f1 with GCC yields code equivalent to f2
ret_struct f2(uint32_t x)
{
    uint32_t a = (x >> 17) & 1;
    uint32_t d = x;
    a += a;
    x = ((x >> 18) & 1) << 2;

    ret_struct result;
    result.as_char = ((a | ((d >> 16) & 1)) | x);
    return result;
}
// Compiling f2 with GCC yields code equivalent to f3
ret_struct f3(uint32_t x)
{
    x >>= 16;
    ret_struct result;
    result.as_char = (x & 1) | ((x & 2) | (x & 4));
    return result;
}
// Compiling f3 with GCC yields code equivalent to f4
ret_struct f4(uint32_t x)
{
    ret_struct result;
    result.as_char = (x >> 16) & 7;
    return result;
}

f1 and f2 can be directly optimized to f4. That transformation is done by LLVM, but not by GCC.

Additionally, even directly compiling f4 with GCC doesn't yield an optimal code generation on architectures like x86. GCC generates this :

f4(unsigned int):
  shr edi, 16
  and edi, 7
  xor eax, eax
  mov al, dil
  ret

and LLVM generates this :

f4(unsigned int):
  mov eax, edi
  shr eax, 16
  and eax, 7
  ret


---


### compiler : `gcc`
### title : `Failure to optimize memory stores of double literals properly`
### open_at : `2020-07-12T17:02:55Z`
### last_modified_date : `2023-09-03T12:13:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96175
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
void f(double *x)
{
    *x = 1.0;
}

With -O3, LLVM generates this :

f(double*): # @f(double*)
  movabs rax, 4607182418800017408
  mov qword ptr [rdi], rax
  ret

GCC generates this :

f(double*):
  mov rax, QWORD PTR .LC0[rip]
  mov QWORD PTR [rdi], rax
  ret
.LC0:
  .long 0
  .long 1072693248

This also occurs on every other architecture I tested (arm and powerpc64le, though only with GCC 9). GCC is weirdly insistent on having a literal stored in const data instead of just having it in the code, which seems inefficient.


---


### compiler : `gcc`
### title : `Failure to omit extraneous movzx in atomic compare exchange with unsigned char`
### open_at : `2020-07-12T23:49:55Z`
### last_modified_date : `2023-09-03T12:13:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96176
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
void f(unsigned char *addr, unsigned char old_val, unsigned char new_val)
{
    __atomic_compare_exchange_n(addr, &old_val, new_val, 0, 0, 0);
}

On x86 with -O3, LLVM generates this :

f(unsigned char*, unsigned char, unsigned char): # @f(unsigned char*, unsigned char, unsigned char)
  mov eax, esi
  lock cmpxchg byte ptr [rdi], dl
  ret

GCC generates this :

f(unsigned char*, unsigned char, unsigned char):
  mov eax, esi
  movzx edx, dl
  lock cmpxchg BYTE PTR [rdi], dl
  ret


---


### compiler : `gcc`
### title : `GCC at -O2 generates branch for code that should be branch-free`
### open_at : `2020-07-13T15:13:52Z`
### last_modified_date : `2023-06-09T17:32:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96187
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Created attachment 48870
Test case

The attached code contains two related functions, get_length2() and get_length4(). For get_length4(), GCC generates branch-free instructions as expected, but for get_length2() it introduces a branch, which is unfortunate because the code was specifically written to avoid branches. The target is x86_64.

You can compare the output here: https://godbolt.org/z/bhEzhY

This happens only at -O2, not at -O1 or -O3. It seems to have started with GCC 7, as GCC 6.* is the last version on Compiler Explorer that generates branch-free code for both functions.

PS: I hope "tree-optimization" is the right component, I'm not a compiler expert. If the problem lies elsewhere, please edit my report.


---


### compiler : `gcc`
### title : `-Wstringop-overflow false positive on std::vector::push_back with -O3`
### open_at : `2020-07-13T15:51:49Z`
### last_modified_date : `2021-12-02T21:30:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96188
### status : `NEW`
### tags : `alias, diagnostic, missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `normal`
### contents :
This is strange issue that started appearing in gcc 10.1. It also seems to require -O3 and -std=gnu++11 (gnu++14 etc appear unaffected).

Godbolt demo: https://godbolt.org/z/W8MWbz

Test case copy/pasted below.

Use -Werror -Wstringop-overflow -std=gnu++11 -O3

#include <string>
#include <vector>

bool b;

void F() {
  static bool b2 = b;
  for (const int fx : {0}) {
    struct Expectation {
      std::string out;
    };

    std::vector<Expectation> expect = {
        {std::string()},
        {std::string()},
        {std::string()},
    };
    if (b2) {
      expect.push_back({std::string()});
    }

  }
}


---


### compiler : `gcc`
### title : `Failure to use eflags from cmpxchg on x86`
### open_at : `2020-07-13T19:44:23Z`
### last_modified_date : `2023-09-03T12:12:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96189
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
bool f(unsigned char* addr, unsigned char old_val, unsigned char new_val)
{
    auto old_val_cpy = old_val;
    __atomic_compare_exchange_n(addr, &old_val, new_val, 0, 0, 0);
    return old_val == old_val_cpy;
}

With -O3, LLVM outputs this :

f(unsigned char*, unsigned char, unsigned char): # @f(unsigned char*, unsigned char, unsigned char)
  mov eax, esi
  lock cmpxchg byte ptr [rdi], dl
  sete al
  ret

GCC outputs this :

f(unsigned char*, unsigned char, unsigned char):
  mov eax, esi
  movzx edx, dl
  lock cmpxchg BYTE PTR [rdi], dl
  cmp al, sil
  sete al
  ret

GCC could use the EFLAGS generated from cmpxchg, but it does not.


---


### compiler : `gcc`
### title : `x86 movsd/movsq string instructions and alignment inference`
### open_at : `2020-07-15T01:50:24Z`
### last_modified_date : `2020-09-15T11:15:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96201
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.1`
### severity : `normal`
### contents :
Taking the time to record some observations and extract minimal test code for alignment (inference) and x86 string instruction selection.

GCC9 and GCC10 are not generating x86 string instructions in cases apparently due to the compiler believing the addresses are not aligned.

GCC10 appears to have an additional issue whereby x86 string instructions are not selected unless the address is aligned to twice the natural alignment.

Two observations:

* (GCC9/10) integer alignment is not inferred from expressions i.e. x & ~3
* (GCC10) __builtin_assume_aligned appears to require double the alignment

The double alignment issue was observed with both int/movsd and long/movsq whereby GCC10 will only generate movsd or movsq if the alignment is double the type's natural alignment. The test case here is for int.


--- BEGIN SAMPLE CODE ---

void f1(long d, long s, unsigned n)
{
    int *sn = (int*)( (long)(s    ) & ~3l );
    int *dn = (int*)( (long)(d    ) & ~3l );
    int *de = (int*)( (long)(d + n) & ~3l );

    while (dn < de) *dn++ = *sn++;
}

void f2(long d, long s, unsigned n)
{
    int *sn = (int*)( (long)(s    ) & ~7l );
    int *dn = (int*)( (long)(d    ) & ~7l );
    int *de = (int*)( (long)(d + n) & ~7l );

    while (dn < de) *dn++ = *sn++;
}

void f3(long d, long s, unsigned n)
{
    int *sn = __builtin_assume_aligned( (int*)( (long)(s    ) & ~3l ), 4 );
    int *dn = __builtin_assume_aligned( (int*)( (long)(d    ) & ~3l ), 4 );
    int *de = __builtin_assume_aligned( (int*)( (long)(d + n) & ~3l ), 4 );

    while (dn < de) *dn++ = *sn++;
}

void f4(long d, long s, unsigned n)
{
    int *sn = __builtin_assume_aligned( (int*)((long)(s    ) & ~3l ), 8 );
    int *dn = __builtin_assume_aligned( (int*)((long)(d    ) & ~3l ), 8 );
    int *de = __builtin_assume_aligned( (int*)((long)(d + n) & ~3l ), 8 );

    while (dn < de) *dn++ = *sn++;
}

--- END SAMPLE CODE ---


GCC9 generates this for f1, f2 and GCC10 generates this for f1, f2, f3

.Ln:
	leaq	(%rax,%rsi), %rcx
	movq	%rax, %rdx
	addq	$4, %rax
	movl	(%rcx), %ecx
	movl	%ecx, (%rdx)
	cmpq	%rax, %rdi
	ja	.Ln

GCC9 generates this for f3, f4 and GCC10 generates this only for f4

.Ln:
	movsl
	cmpq	%rdi, %rdx
	ja	.Ln


---


### compiler : `gcc`
### title : `non-grouped load can be SLP vectorized for 2-element vectors case`
### open_at : `2020-07-15T15:01:36Z`
### last_modified_date : `2023-06-27T07:49:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96208
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 48879
initial implementation

Current loop vectorizer only vectorize loops with groups size being power-of-2 or 3 due to vector permutation generation algorithm specifics.
However, in case of 2-element vectors, simple permutation schema can be used to support any group size: insert each vector element into required position, which leads to reasonable amount of operations in case of 2-element vectors.

Initial version is attached.


---


### compiler : `gcc`
### title : `Failure to optimize shift+not to rotate`
### open_at : `2020-07-16T21:06:16Z`
### last_modified_date : `2023-09-03T12:11:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96226
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int32_t f(int32_t x)
{
    return ~(1 << (x & 0x1F));
}

This can be transformed to doing a rotate of -2 and x. This transformation is done by LLVM, but not by GCC.

PS: GCC seems capable of doing this optimization, but only if `x & 0x1F` is replaced with `x`, which either means that GCC is underoptimizing this or that LLVM is somehow wrong.


---


### compiler : `gcc`
### title : `-Wstack-usage does not understand constant __builtin_alloca calls`
### open_at : `2020-07-16T22:05:53Z`
### last_modified_date : `2020-07-17T08:51:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96228
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.3.0`
### severity : `normal`
### contents :
Hi All,
when compiling the following piece of code:

```
#include <stdio.h>

int main(void) {
  char *a = __builtin_alloca(4);
  a[0] = 0;
  printf("%c", a[0]);
  return 0;
}
```

with:
$ gcc -Wstack-usage=2048 stack.c

gives:
stack.c:3:5: warning: stack usage might be unbounded [-Wstack-usage=]
    3 | int main(void) {

which is too pessimistic on `__builtin_alloca` behavior.
No warning when "char *a = __builtin_alloca(4);" is changed to "char a[4];"

Thank you!


---


### compiler : `gcc`
### title : `Failure to optimize bool pattern equivalent to minus 1`
### open_at : `2020-07-17T12:27:20Z`
### last_modified_date : `2023-09-03T12:11:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96232
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(bool x)
{
    return x ? 0 : -1;
}

This can be optimized to `x - 1`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Sub-optimal register allocation with a signed integer literal.`
### open_at : `2020-07-17T15:01:38Z`
### last_modified_date : `2021-08-20T00:19:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96234
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `enhancement`
### contents :
The following code:

    #include <time.h>
    #include <stdint.h>
    
    namespace A {
    
    inline uint64_t as_nanoseconds(struct timespec* ts) {
        return ts->tv_sec * 1000000000L + ts->tv_nsec;
    }
    
    uint64_t f(uint64_t c, struct timespec* a, struct timespec* b) {
        return c + (as_nanoseconds(a) - as_nanoseconds(b));
    }
    
    }
    
    namespace B {
    
    inline uint64_t as_nanoseconds(struct timespec* ts) {
        return ts->tv_sec * 1000000000UL + ts->tv_nsec;
    }
    
    uint64_t f(uint64_t c, struct timespec* a, struct timespec* b) {
        return c + (as_nanoseconds(a) - as_nanoseconds(b));
    }
    
    }

When compiled with `gcc-10.1 -O3 -march=skylake` produces a superflows instruction in the version a signed constant `1000000000L`:

    A::f(unsigned long, timespec*, timespec*):
            mov     r8, rdx <----------------------------- superflows instruction
            imul    rax, QWORD PTR [rsi], 1000000000
            imul    rdx, QWORD PTR [rdx], 1000000000
            add     rax, QWORD PTR [rsi+8]
            add     rdx, QWORD PTR [r8+8]
            sub     rax, rdx
            add     rax, rdi
            ret
    B::f(unsigned long, timespec*, timespec*):
            imul    rax, QWORD PTR [rsi], 1000000000
            add     rdi, QWORD PTR [rsi+8]
            sub     rdi, QWORD PTR [rdx+8]
            imul    rdx, QWORD PTR [rdx], 1000000000
            add     rax, rdi
            sub     rax, rdx
            ret

`clang` produces the same code for both versions and also optimizes away one multiplication:

    A::f(unsigned long, timespec*, timespec*):                 # @A::f(unsigned long, timespec*, timespec*)
            mov     rax, qword ptr [rsi]
            sub     rax, qword ptr [rdx]
            imul    rax, rax, 1000000000
            add     rdi, qword ptr [rsi + 8]
            sub     rdi, qword ptr [rdx + 8]
            add     rax, rdi
            ret
    B::f(unsigned long, timespec*, timespec*):                 # @B::f(unsigned long, timespec*, timespec*)
            mov     rax, qword ptr [rsi]
            sub     rax, qword ptr [rdx]
            imul    rax, rax, 1000000000
            add     rdi, qword ptr [rsi + 8]
            sub     rdi, qword ptr [rdx + 8]
            add     rax, rdi
            ret

https://gcc.godbolt.org/z/Kf4q7z


---


### compiler : `gcc`
### title : `Failure to recognize and pattern composed of and+or after shift`
### open_at : `2020-07-17T21:16:35Z`
### last_modified_date : `2023-09-03T12:11:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96237
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
unsigned char f(unsigned char x)
{
    auto a = (x >> 3) & 1;
    if (x & 16)
        a |= 2;
    if (x & 32)
        a |= 4;
    return a;
}

This can be optimized to `return (x >> 3) & 7;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to recognize __builtin_bswap16 pattern`
### open_at : `2020-07-18T00:39:19Z`
### last_modified_date : `2023-09-03T12:10:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96239
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
union BytesOverlay
{
    uint8_t u8[2];
    uint16_t u16;
};

uint16_t f(uint16_t from)
{
    BytesOverlay overlay;
    overlay.u16 = from;
    uint8_t byte1 = overlay.u8[0];
    uint8_t byte2 = overlay.u8[1];
    overlay.u8[0] = byte2;
    overlay.u8[1] = byte1;
    return overlay.u16;
}

This can be transformed to `return __builtin_bswap16(from);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Redudant mask load generated`
### open_at : `2020-07-20T03:38:49Z`
### last_modified_date : `2021-03-24T08:34:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96244
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c

---
typedef int v8si __attribute__ ((__vector_size__ (32)));
v8si
foo (v8si a, v8si b, v8si c, v8si d)
{
  v8si e;
    for (int i = 0; i != 8; i++)
     e[i] = a[i] > b[i] ? c[i] : d[i];
    return e;
}
---

gcc -Ofast -mavx2 test.c

cat test.c.238t.optimized
---
foo (v8si a, v8si b, v8si c, v8si d)
{
  vector(8) int vect_iftmp.19;
  vector(8) int vect_iftmp.18;
  vector(8) <signed-boolean:32> mask__31.15;
  vector(8) int vect_iftmp.14;
  vector(8) <signed-boolean:32> mask__28.11;

  <bb 2> [local count: 119292720]:
  mask__28.11_40 = b_50(D) < a_53(D);
  vect_iftmp.14_43 = .MASK_LOAD (&c, 32B, mask__28.11_40); ---> redundant
  mask__31.15_44 = b_50(D) >= a_53(D);
  vect_iftmp.18_47 = .MASK_LOAD (&d, 32B, mask__31.15_44); ---> redundant
  vect_iftmp.19_49 = .VCOND (b_50(D), a_53(D), vect_iftmp.18_47, vect_iftmp.14_43, 110);
  return vect_iftmp.19_49;
---

could be optimized to 
---
vect_iftmp.19_49 = .VCOND (b_50(D), a_53(D), d, c);
---


---


### compiler : `gcc`
### title : `Failure to optimize arithmetic pattern in switch`
### open_at : `2020-07-20T04:20:58Z`
### last_modified_date : `2023-09-03T12:10:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96245
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
void f(int x)
{
    switch (x)
    {
        case 0:
            putchar('0');
            break;
        case 1:
            putchar('1');
            break;
        case 2:
            putchar('2');
            break;
        case 3:
            putchar('3');
            break;
        case 4:
            putchar('4');
            break;
        case 5:
            putchar('5');
            break;
        case 6:
            putchar('6');
            break;
        case 7:
            putchar('7');
            break;
        case 8:
            putchar('8');
            break;
        case 9:
            putchar('9');
            break;
    }
}

This can be optimized to `if ((unsigned)x < 10) putchar('0' + x);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[10/11 Regression] ICE: in lra_assign, at lra-assigns.c:1648 with -O -flive-range-shrinkage -fnon-call-exceptions -msse4 --param=max-sched-ready-insns=1 since r10-4373-gc265dfbf748e9fc3`
### open_at : `2020-07-21T10:49:52Z`
### last_modified_date : `2021-01-14T09:05:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96263
### status : `RESOLVED`
### tags : `ice-on-valid-code, missed-optimization, ra`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 48904
reduced testcase

Compiler output:
$ x86_64-pc-linux-gnu-gcc -O -flive-range-shrinkage -fnon-call-exceptions -msse4 --param=max-sched-ready-insns=1 testcase.c 
testcase.c: In function 'm':
testcase.c:12:1: warning: AVX512F vector return without AVX512F enabled changes the ABI [-Wpsabi]
   12 | a m(unsigned short n, __int128 o) {
      | ^
during RTL pass: reload
testcase.c:27:1: internal compiler error: in lra_assign, at lra-assigns.c:1648
   27 | }
      | ^
0xefe25b lra_assign(bool&)
        /repo/gcc-trunk/gcc/lra-assigns.c:1648
0xef7f3c lra(_IO_FILE*)
        /repo/gcc-trunk/gcc/lra.c:2465
0xea7949 do_reload
        /repo/gcc-trunk/gcc/ira.c:5525
0xea7949 execute
        /repo/gcc-trunk/gcc/ira.c:5711
Please submit a full bug report,
with preprocessed source if appropriate.
Please include the complete backtrace with any bug report.
See <https://gcc.gnu.org/bugs/> for instructions.

$ x86_64-pc-linux-gnu-gcc -v
Using built-in specs.
COLLECT_GCC=/repo/gcc-trunk/binary-latest/bin/x86_64-pc-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/repo/gcc-trunk/binary-trunk-r11-2246-20200721142816-gc850a642e1d-checking-yes-rtl-df-extra-nobootstrap-amd64/bin/../libexec/gcc/x86_64-pc-linux-gnu/11.0.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: /repo/gcc-trunk//configure --enable-languages=c,c++ --enable-valgrind-annotations --disable-nls --enable-checking=yes,rtl,df,extra --disable-bootstrap --with-cloog --with-ppl --with-isl --build=x86_64-pc-linux-gnu --host=x86_64-pc-linux-gnu --target=x86_64-pc-linux-gnu --with-ld=/usr/bin/x86_64-pc-linux-gnu-ld --with-as=/usr/bin/x86_64-pc-linux-gnu-as --disable-libstdcxx-pch --prefix=/repo/gcc-trunk//binary-trunk-r11-2246-20200721142816-gc850a642e1d-checking-yes-rtl-df-extra-nobootstrap-amd64
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.0.0 20200721 (experimental) (GCC)


---


### compiler : `gcc`
### title : `Failure to optimize memcmp of doubles to avoid going through memory`
### open_at : `2020-07-21T16:41:40Z`
### last_modified_date : `2023-09-03T12:10:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96271
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(double a, double b)
{
   return memcmp(&a, &b, sizeof(double)) == 0;
}

With -O3, LLVM outputs this :

f(double, double):
  movq rax, xmm0
  movq rcx, xmm1
  cmp rax, rcx
  sete al
  ret

GCC outputs this:

f(double, double):
  movsd QWORD PTR [rsp-8], xmm0
  movsd QWORD PTR [rsp-16], xmm1
  mov rax, QWORD PTR [rsp-16]
  cmp QWORD PTR [rsp-8], rax
  sete al
  ret


---


### compiler : `gcc`
### title : `Failure to optimize overflow check`
### open_at : `2020-07-21T17:36:00Z`
### last_modified_date : `2023-09-03T12:10:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96272
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
static inline unsigned f(unsigned a, unsigned b)
{
    if (b > UINT_MAX - a)
        return UINT_MAX;

    return a + b;
}

With -O3, LLVM outputs this:

f(unsigned int, unsigned int):
  add edi, esi
  mov eax, -1
  cmovae eax, edi
  ret

GCC outputs this:

f(unsigned int, unsigned int):
  mov eax, edi
  not eax
  add edi, esi
  cmp eax, esi
  mov eax, -1
  cmovnb eax, edi
  ret


---


### compiler : `gcc`
### title : `Vectorizer doesn't take into account bitmask condition from branch conditions.`
### open_at : `2020-07-22T02:57:13Z`
### last_modified_date : `2020-12-28T16:36:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96275
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
https://godbolt.org/z/Gfebjd

With gcc trunk 20200720

If the loop to be vectorized is inside a if condition that check for loop counter, or there is preceding assert / function return on such condition, the gcc seems to forgot about it and not take into account in the optimizer / vectorizer, and still emits the backup scalar code to take care of stragglers despite it being a dead code.

#include "assert.h"

void fillArray(const unsigned int N, float * restrict a, const float* restrict b, const float* restrict c) {
    //assert(N >= 1024);
    for (int i = 0; i < (N & ~31u); i++) {
        a[i] = b[0] * c[i];
    }
}


produces:

fillArray:
        and     edi, -32
        je      .L8
        shr     edi, 3
        vbroadcastss    ymm1, DWORD PTR [rdx]
        xor     eax, eax
        mov     edx, edi
        sal     rdx, 5
.L3:
        vmulps  ymm0, ymm1, YMMWORD PTR [rcx+rax]
        vmovups YMMWORD PTR [rsi+rax], ymm0
        add     rax, 32
        cmp     rax, rdx
        jne     .L3
        vzeroupper
.L8:
        ret




but:

#include "assert.h"

void fillArray(const unsigned int N, float * restrict a, const float* restrict b, const float* restrict c) {
    //assert(N >= 1024);
    if ((N & 31u) == 0) {
        for (int i = 0; i < N; i++) {
            a[i] = b[0] * c[i];
        }
    }
}

produces this sub-optimal code:

fillArray:
        mov     eax, edi
        and     eax, 31
        jne     .L14
        test    edi, edi
        je      .L14
        lea     r8d, [rdi-1]
        vmovss  xmm1, DWORD PTR [rdx]
        cmp     r8d, 6
        jbe     .L8
        mov     edx, edi
        vbroadcastss    ymm2, xmm1
        xor     eax, eax
        shr     edx, 3
        sal     rdx, 5
.L4:
        vmulps  ymm0, ymm2, YMMWORD PTR [rcx+rax]
        vmovups YMMWORD PTR [rsi+rax], ymm0
        add     rax, 32
        cmp     rdx, rax
        jne     .L4
        mov     eax, edi
        and     eax, -8
        mov     edx, eax
        cmp     edi, eax
        je      .L16
        vzeroupper
.L3:
        mov     r9d, edi
        sub     r8d, eax
        sub     r9d, eax
        cmp     r8d, 2
        jbe     .L6
        mov     eax, eax
        vshufps xmm0, xmm1, xmm1, 0
        vmulps  xmm0, xmm0, XMMWORD PTR [rcx+rax*4]
        vmovups XMMWORD PTR [rsi+rax*4], xmm0
        mov     eax, r9d
        and     eax, -4
        add     edx, eax
        cmp     r9d, eax
        je      .L14
.L6:
        movsx   rax, edx
        vmulss  xmm0, xmm1, DWORD PTR [rcx+rax*4]
        vmovss  DWORD PTR [rsi+rax*4], xmm0
        lea     eax, [rdx+1]
        cmp     edi, eax
        jbe     .L14
        cdqe
        add     edx, 2
        vmulss  xmm0, xmm1, DWORD PTR [rcx+rax*4]
        vmovss  DWORD PTR [rsi+rax*4], xmm0
        cmp     edi, edx
        jbe     .L14
        movsx   rdx, edx
        vmulss  xmm1, xmm1, DWORD PTR [rcx+rdx*4]
        vmovss  DWORD PTR [rsi+rdx*4], xmm1
.L14:
        ret
.L16:
        vzeroupper
        ret
.L8:
        xor     edx, edx
        jmp     .L3


Adding `assert(N == (N & ~31u));` doesn't help.


---


### compiler : `gcc`
### title : `TBAA does not work as expected for a simple test case`
### open_at : `2020-07-22T09:07:43Z`
### last_modified_date : `2021-12-22T10:20:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96281
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Test case: foo.c

typedef struct state_t {
    int threadid;
} state_t;

int history_h[8][12][64];

void history_good (state_t *s) {
    int i, j;

    if (s->threadid >= 0 && s->threadid < 8) {
        for (i = 0; i < 12; i++) {
            for (j = 0; j < 64; j++) {
                history_h[s->threadid][i][j] = (history_h[s->threadid][i][j] + 1) >> 1;
            }
        }
    }
}

$ gcc -S -O2 -ftree-loop-vectorize -funroll-loops foo.c -fopt-info
foo.c:14:13: optimized: loop unrolled 6 times

When the input parameter s is specified to be unaliased with __restrict__ type qualifier like:
void history_good (state_t * __restrict__ s)

The inner loop could be auto-vectorized:
$ gcc -S -O2 -ftree-loop-vectorize -funroll-loops foo.c -fopt-info
foo.c:14:13: optimized: loop vectorized using 16 byte vectors
foo.c:8:6: optimized: loop with 15 iterations completely unrolled (header execution count 16535624)

Looks like TBAA is not working here for this case.  Then I noticed the following logic in tree-ssa-alias.c:

1939   /* When we are trying to disambiguate an access with a pointer dereference
1940      as base versus one with a decl as base we can use both the size
1941      of the decl and its dynamic type for extra disambiguation.
1942      ???  We do not know anything about the dynamic type of the decl
1943      other than that its alias-set contains base2_alias_set as a subset
1944      which does not help us here.  */
1945   /* As we know nothing useful about the dynamic type of the decl just
1946      use the usual conflict check rather than a subset test.
1947      ???  We could introduce -fvery-strict-aliasing when the language
1948      does not allow decls to have a dynamic type that differs from their
1949      static type.  Then we can check
1950      !alias_set_subset_of (base1_alias_set, base2_alias_set) instead.  */
1951   if (base1_alias_set != base2_alias_set
1952       && !alias_sets_conflict_p (base1_alias_set, base2_alias_set))
1953     return false;

This was introduced by: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=42834 
From the comments, this depends on the language of the source code. 
So at least for C & C++, could we check !alias_set_subset_of (base1_alias_set, base2_alias_set) instead here?
Any other languages supported by GCC that makes a difference?


---


### compiler : `gcc`
### title : `Unnecessary saving and re-testing of the carry flag with __builtin_usub_overflow`
### open_at : `2020-07-22T16:41:05Z`
### last_modified_date : `2020-08-12T23:16:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96289
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Consider this C code:

unsigned f(unsigned x, unsigned y) {
    if (__builtin_usub_overflow(x, y, &x)) {
        x += 100;
    }
    return x;
}

When compiled with -O3, it produces the following assembly:

f:
        xorl    %edx, %edx
        subl    %esi, %edi
        setb    %dl
        leal    100(%rdi), %eax
        testl   %edx, %edx
        cmove   %edi, %eax
        ret

https://godbolt.org/z/WMo377

But "lea" doesn't affect the carry flag (or any flags for that matter), so there's no need to save it to a register and then re-test it. It should have produced this assembly instead:

f:
        subl    %esi, %edi
        leal    100(%rdi), %eax
        cmovae  %edi, %eax
        ret


---


### compiler : `gcc`
### title : `Redundant zero extension after inlining the function`
### open_at : `2020-07-23T09:03:41Z`
### last_modified_date : `2023-03-21T23:52:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96297
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Command line: bin/riscv64-unknown-elf-gcc -march=rv32imafc -mabi=ilp32f -O3 call_is_digit.c -S

==========
 C Source
==========
unsigned char is_digit(unsigned char c) {
  return ((c >= '0') & (c <= '9')) ? 1 : 0;
}

int call_is_digit(unsigned char c) {
  if (is_digit(c))
    return 0xa;
  else
    return 0;
}

=========
 GCC asm
=========
is_digit:
	addi	a0,a0,-48
	sltiu	a0,a0,10
	ret

call_is_digit:
	addi	a0,a0,-48
	andi	a0,a0,0xff  # redundant zero extension
	li	a5,9
	bleu	a0,a5,.L5
	li	a0,0
	ret
.L5:
	li	a0,10
	ret

The zero extension instruction in the function is_digit is eliminated in combine pass, but it in the function call_is_digit can not be eliminated.


---


### compiler : `gcc`
### title : `not detecting widen multiple after a widen multiply with shift`
### open_at : `2020-07-24T03:22:00Z`
### last_modified_date : `2021-09-27T07:39:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96305
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.1`
### severity : `enhancement`
### contents :
In presence of a signed variable multiplied by itself, the compiler seems to recognize that the result will necessarily be positive, then considers the result as unsigned going forward, causing unnecessarily complicated code down the line.
I have initially reproduced the issue on 7.2.1 for arm, but I have verified the same issue happens in the latest supported by the gotbolt compiler.

---
 [nenik@Pix2 ~]$ arm-none-eabi-gcc --version
arm-none-eabi-gcc (GNU Tools for Arm Embedded Processors 7-2017-q4-major) 7.2.1 20170904 (release) [ARM/embedded-7-branch revision 255204]
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[nenik@Pix2 ~]$ cat mull-issue.c
inline int hmull(int a, int b) {
    return ((long long)a * b) >> 32;
}

int compute(int a, int b) {
    int t = hmull(a,a);
    return hmull(t, b);
}

[nenik@Pix2 ~]$ arm-none-eabi-gcc -Os -S -mcpu=cortex-m3 mull-issue.c 

[nenik@Pix2 ~]$ cat mull-issue.s 
	.cpu cortex-m3
	.eabi_attribute 20, 1
	.eabi_attribute 21, 1
	.eabi_attribute 23, 3
	.eabi_attribute 24, 1
	.eabi_attribute 25, 1
	.eabi_attribute 26, 1
	.eabi_attribute 30, 4
	.eabi_attribute 34, 1
	.eabi_attribute 18, 4
	.file	"mull-issue.c"
	.text
	.align	1
	.global	compute
	.syntax unified
	.thumb
	.thumb_func
	.fpu softvfp
	.type	compute, %function
compute:
	@ args = 0, pretend = 0, frame = 0
	@ frame_needed = 0, uses_anonymous_args = 0
	smull	r2, r3, r0, r0
	push	{r4, r6, r7, lr}
	asrs	r7, r1, #31
	mul	r0, r3, r7
	asrs	r4, r3, #31
	mla	r0, r1, r4, r0
	umull	r2, r3, r3, r1
	add	r0, r0, r3
	pop	{r4, r6, r7, pc}
	.size	compute, .-compute
	.ident	"GCC: (GNU Tools for Arm Embedded Processors 7-2017-q4-major) 7.2.1 20170904 (release) [ARM/embedded-7-branch revision 255204]"
---
https://godbolt.org/z/v186Yz


Expected code should be pretty much:
	smull	r2, r3, r0, r0
	smull	r2, r0, r3, r1
	bx	lr
under the simple reasoning, that r3, after the first smull, would be, at most, 0x40000000 for any argument and thus while certainly positive, never having the highest bit set. r4 after second asrs will always be zero and so would be the multiplicative part of the following mla, removing the need to go with umull and fixing the result.

I have got clang to generate optimal code in a more complicated piece of SW.
I can also get gcc to generate two smulls (and smaller code overall) if I add an unknown extra argument (or even a small constant) to the "t" variable before the  second hmull call, but if I try with a constant of zero and the compiler manages  to learn that, it gets back to suboptimal code.


---


### compiler : `gcc`
### title : `Inefficient increment through pointer to volatile on x86`
### open_at : `2020-07-27T01:47:08Z`
### last_modified_date : `2020-07-30T19:23:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96327
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
Although the code generation for increment (++, --) through a pointer to volatile has improved greatly over the past 15 years, there is a case in which the address calculation is needlessly done separately instead of by the x86 increment instruction itself.  Here is some example code:

struct task {
    int other;
    int rcu_count;
};

struct task *current;

void rcu_read_lock()
{
    (*(volatile int*)&current->rcu_count)++;
}

As can be seen in godbolt.org (https://godbolt.org/z/fGze8E), the address calculation is split by GCC. The shorter code sequence generated by clang/LLVM is preferable.

Fixing this would allow the Linux kernel to use safer code sequences for certain fastpaths, in this example, rcu_read_lock() and rcu_read_unlock() for kernels built with CONFIG_PREEMPT=y.


---


### compiler : `gcc`
### title : `Multiple multiplications fail to optimize`
### open_at : `2020-07-27T15:14:01Z`
### last_modified_date : `2021-08-03T23:30:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96336
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `enhancement`
### contents :



---


### compiler : `gcc`
### title : `[SVE] Unnecessary register saves in exception handler`
### open_at : `2020-07-27T15:39:37Z`
### last_modified_date : `2020-07-28T06:04:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96338
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Compiling the following testcase with -march=armv8.2-a+sve -O2:

---------------------------------------------------------------------
void bar (__SVFloat32_t);
void
foo (__SVFloat32_t x)
{
  try { bar (x); } catch (...) { bar (x); throw; }
}
---------------------------------------------------------------------

gives an EH handler like the following;

---------------------------------------------------------------------
        .cfi_restore_state
        str     z8, [sp, #2, mul vl]
        .cfi_escape 0x10,0x48,0x8,0x8f,0,0x92,0x2e,0,0x40,0x1e,0x22
        str     z9, [sp, #3, mul vl]
        .cfi_escape 0x10,0x49,0x8,0x8f,0,0x92,0x2e,0,0x48,0x1e,0x22
        str     z10, [sp, #4, mul vl]
        .cfi_escape 0x10,0x4a,0x9,0x8f,0,0x92,0x2e,0,0x8,0x20,0x1e,0x22
        str     z11, [sp, #5, mul vl]
        .cfi_escape 0x10,0x4b,0x9,0x8f,0,0x92,0x2e,0,0x8,0x28,0x1e,0x22
        str     z12, [sp, #6, mul vl]
        .cfi_escape 0x10,0x4c,0x9,0x8f,0,0x92,0x2e,0,0x8,0x30,0x1e,0x22
        str     z13, [sp, #7, mul vl]
        .cfi_escape 0x10,0x4d,0x9,0x8f,0,0x92,0x2e,0,0x8,0x38,0x1e,0x22
        str     z14, [sp, #8, mul vl]
        .cfi_escape 0x10,0x4e,0x9,0x8f,0,0x92,0x2e,0,0x8,0x40,0x1e,0x22
        str     z15, [sp, #9, mul vl]
        .cfi_escape 0x10,0x4f,0x9,0x8f,0,0x92,0x2e,0,0x8,0x48,0x1e,0x22
        str     z16, [sp, #10, mul vl]
        str     z17, [sp, #11, mul vl]
        str     z18, [sp, #12, mul vl]
        str     z19, [sp, #13, mul vl]
        str     z20, [sp, #14, mul vl]
        str     z21, [sp, #15, mul vl]
        str     z22, [sp, #16, mul vl]
        str     z23, [sp, #17, mul vl]
        str     p5, [sp, #1, mul vl]
        str     p6, [sp, #2, mul vl]
        str     p7, [sp, #3, mul vl]
        str     p8, [sp, #4, mul vl]
        str     p9, [sp, #5, mul vl]
        str     p10, [sp, #6, mul vl]
        str     p11, [sp, #7, mul vl]
        str     p12, [sp, #8, mul vl]
        str     p13, [sp, #9, mul vl]
        str     p14, [sp, #10, mul vl]
        str     p15, [sp, #11, mul vl]
        str     p4, [sp]
        bl      __cxa_begin_catch
        add     x0, sp, 32
        ldr     z0, [x0, #18, mul vl]
.LEHB1:
        bl      _Z3bar13__SVFloat32_t
        bl      __cxa_rethrow
.LEHE1:
.L5:
        mov     x19, x0
        bl      __cxa_end_catch
        mov     x0, x19
.LEHB2:
        bl      _Unwind_Resume
---------------------------------------------------------------------

The spills of z8-z23 and p4-p15 are completely unnecessary,
since the noreturn _Unwind_Resume call ensures that this code
never returns to foo's caller.

I think the fix is to make ira.c:update_equiv_regs_prescan
a bit smarter: if a call occurs in a block that has no
path to the exit block, then we only need to consider
(parts of) registers that are clobbered by the callee
but preserved by both eh_edge_abi and the current
function's abi.

If, as is usual, eh_edge_abi is the lowest common denominator
among the available ABIs, no callees will clobber something
that is preserved by eh_edge_abi.


---


### compiler : `gcc`
### title : `[SVE] Optimise svlast[ab]`
### open_at : `2020-07-27T16:22:51Z`
### last_modified_date : `2023-06-13T07:07:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96339
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
For:

#include <arm_sve.h>
int8_t
foo (svint8_t x)
{
  return svlasta (svptrue_pat_b8 (SV_VL1), x);
}

we generate:

        ptrue   p0.b, vl1
        lasta   w0, p0, z0.b
        ret

But this is really a BIT_FIELD_REF with a constant index,
so we can generate:

        umov    w0, v0.b[0]
        ret

(works even on big-endian targets).

We should make svlast_impl fold calls to a BIT_FIELD_REF if:
(1) the predicate input is a constant,
(2) we know at compile time which element the constant selects, and
(3) the selected element ends on or before byte 64

For other cases it is probably better to keep the original call.

(2) excludes variable-length predicate constants if (a) we can't
determine the index of the final 1 bit at compile time or
(b) that index might be beyond the end of the vector

(3) restricts the optimisation to cases that can be handled by
*vec_extract<mode><Vel>_v128 and *vec_extract<mode><Vel>_dup.


---


### compiler : `gcc`
### title : `missed opportunity to optimize out redundant loop`
### open_at : `2020-07-28T07:32:56Z`
### last_modified_date : `2020-11-26T08:21:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96351
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
inline unsigned int
stringLen(const short* const src)
{
    if (src == 0 || *src == 0) {
        return 0;
    } else {
        const short* pszTmp = src + 1;

        while (*pszTmp)
            ++pszTmp;

        return (unsigned int)(pszTmp - src);
    }
}

extern void bar();

void foo(const short* const str) {
    unsigned int len = stringLen(str);
    if (!len) {
        bar();
    }
}

When stringLen is inlined into foo, the else block in stringLen can be simplified into non-zero, thus eliminating the while loop. This looks like a tree VRP issue, but this pass does not work as expected for this test case.

$ g++ -S -O2 foo.cpp -fdump-tree-vrp

Consider function foo, value ranges after VRP does not help here:
 48
 49 .MEM_1: <<< error >>> VARYING
 50 str_3(D): const short int * const VARYING
 51 _6: short int VARYING
 52 str_7: const short int * const [1B, +INF]  EQUIVALENCES: { str_3(D) } (1 elements)
 53 pszTmp_8: const short int * [1B, +INF]  EQUIVALENCES: { pszTmp_10 } (1 elements)
 54 pszTmp_9: const short int * const [1B, +INF]
 55 pszTmp_10: const short int * const [1B, +INF]
 56 _11: short int VARYING
 57 pszTmp_12: const short int * [1B, +INF]
 58 _13: unsigned int [0, 0]
 59 _14: long int VARYING
 60 _15: long int [-4611686018427387904, 4611686018427387903]
 61 _16: unsigned int VARYING
 62 _18: unsigned int [0, 0]
 63 pszTmp_19: const short int * [1B, +INF]  EQUIVALENCES: { pszTmp_10 } (1 elements)

 ......

 93   <bb 4> [local count: 439750964]:
 94   pszTmp_9 = str_3(D) + 2;
 95
 96   <bb 5> [local count: 3997736055]:
 97   # pszTmp_10 = PHI <pszTmp_9(4), pszTmp_12(6)>
 98   _11 = *pszTmp_10;
 99   if (_11 == 0)
100     goto <bb 7>; [11.00%]
101   else
102     goto <bb 6>; [89.00%]
103
104   <bb 6> [local count: 3557985095]:
105   pszTmp_12 = pszTmp_10 + 2;
106   goto <bb 5>; [100.00%]
107
108   <bb 7> [local count: 439750964]:
109   # pszTmp_8 = PHI <pszTmp_10(5)>
110   _14 = pszTmp_8 - str_3(D);
111   _15 = _14 /[ex] 2;
112   _16 = (unsigned int) _15;
113   if (_16 == 0)
114     goto <bb 8>; [3.91%]
115   else
116     goto <bb 9>; [96.09%]
117
118   <bb 8> [local count: 354334798]:
119   bar ();
120
121   <bb 9> [local count: 1073741824]:
122   return;

Any suggestions to proceed?


---


### compiler : `gcc`
### title : `GCC report impossible constraint on possible inline assembly output constraints`
### open_at : `2020-07-29T09:15:00Z`
### last_modified_date : `2023-06-14T12:38:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96365
### status : `RESOLVED`
### tags : `documentation, inline-asm, missed-optimization, ra`
### component : `middle-end`
### version : `9.3.0`
### severity : `normal`
### contents :
$ cat a.c
int main() {
  int x = 0, y = 0, z = 0;
  asm("" : "=ab"(x), "=bc"(y), "=c"(z));
  return 0;
}

Inline assembly at line 3 is a possible constraint, compiler can allocate eax for x, ebx for y, ecx for z, but GCC report that this asm has impossible constraint as below:

-> tmp $ gcc a.c
asm.c: In function ‘main’:
asm.c:3:3: error: ‘asm’ operand has impossible constraints
    3 |   asm("" : "=ab"(x), "=bc"(y), "=c"(z));
      |   ^~~

My gcc version:

-> tmp $ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none:hsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.3.0-10ubuntu2' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 9.3.0 (Ubuntu 9.3.0-10ubuntu2)


---


### compiler : `gcc`
### title : `Optimize x+0.0 if x is an integer`
### open_at : `2020-07-30T19:08:50Z`
### last_modified_date : `2022-02-09T14:24:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96392
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.1.1`
### severity : `enhancement`
### contents :
One way to convert an integer to a floating point number in C is to multiply it by 1.0. In this case, gcc is clever enough to optimize away the multiplication.

Another way is to add 0.0. However, in this case, GCC does not optimize away the addition. 

Example C code:

    double times1(int x)
    {
        return x * 1.0;
    }
    
    double plus0(int x)
    {
        return x + 0.0;
    }

Output of objdump -d after compiling with gcc -O2 -c:

    0000000000000000 <times1>:
       0:	66 0f ef c0          	pxor   %xmm0,%xmm0
       4:	f2 0f 2a c7          	cvtsi2sd %edi,%xmm0
       8:	c3                   	retq   
       9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    
    0000000000000010 <plus0>:
      10:	66 0f ef c0          	pxor   %xmm0,%xmm0
      14:	f2 0f 2a c7          	cvtsi2sd %edi,%xmm0
      18:	f2 0f 58 05 00 00 00 	addsd  0x0(%rip),%xmm0
      1f:	00 
      20:	c3                   	retq   

I believe that the reason that GCC does not optimize x+0.0 is that it is worried that x could be negative zero. However, promoting an integer to floating point can never yield negative zero so it should be possible to optimize in this particular case. (For the matter, Clang does optimize it.)


---


### compiler : `gcc`
### title : `GCC Fails to exploit ranges from overflow tests`
### open_at : `2020-07-31T04:18:38Z`
### last_modified_date : `2021-10-02T22:25:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96397
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Compile with -O2.  We should be able to eliminate the x > p1 test if we were smart about back propagating equivalences to generate a range from the __builtin_add_overflow.

This was derived from a bogus warning in tpm2-pkcs11's testsuite.

#include <stddef.h>
#include <stdlib.h>
extern void frob (void);

void
foo(size_t p1)
{
  size_t x = p1 - 4;
  size_t y;
  if (__builtin_add_overflow (x, 8, &y))
    {
      frob ();
    }
  else
    {
      if (x > p1)
        abort ();
    }
}


---


### compiler : `gcc`
### title : `Constant propoagation works on global variable, but not in a function`
### open_at : `2020-08-02T18:22:49Z`
### last_modified_date : `2023-01-19T07:00:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96419
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.1.0`
### severity : `enhancement`
### contents :
I tried to implement a map-like data structure, with compile-time keys:

#include <algorithm>
#include <array>
#include <type_traits>

namespace fixed_flatmap_detail
{
    template<class T, size_t N, class Compare>
    constexpr auto sort(std::array<T, N> const& x, Compare const& compare)
    {
        auto tmp = x;
        std::ranges::sort(tmp, compare);
        return tmp;
    }
}

template<class Keys, class Value, class Compare = std::less<decltype(Keys::items[0])>>
class FixedFlatmap
{
public:
    using key_type   = std::remove_reference_t<decltype(Keys::items[0])>;
    using value_type = Value;

    static constexpr auto size() { return std::size(Keys::items); }

    static constexpr auto const keys() { return s_keys; }

    constexpr auto const values() const { return m_vals; }

    constexpr auto values() { return m_vals; }

    constexpr auto find(key_type const& key) const
    {
        auto i = std::ranges::lower_bound(s_keys, key, Compare{});
        if(i != std::end(s_keys) && !Compare{}(key, *i)) [[likely]]
        {
            return std::begin(m_vals) + (i - std::begin(s_keys));
        }

        return static_cast<value_type const*>(nullptr);
    }

    constexpr auto find(key_type const& key)
    {
        return const_cast<value_type*>(std::as_const(*this).find(key));
    }

private:
    static constexpr auto s_keys = fixed_flatmap_detail::sort(Keys::items, Compare{});
    std::array<value_type, size()> m_vals;
};

struct KeyStruct
{
    static constexpr std::array<std::string_view, 3> items{"Foo", "Bar", "Kaka"};
};

FixedFlatmap<KeyStruct, int> my_vals{};
auto this_value_is_computed_at_compile_time = my_vals.find("Kaka");

int* test_lookup(FixedFlatmap<KeyStruct, int>& vals)
{
    return vals.find("Foo");  // == static_cast<int*>(static_cast<byte*>(&vals) + sizeof(int))
}

Interestingly gcc succeeds to compute `find` on a global variable, but fails as soon as the same structure is allocated in a function. I am not an expert in compilers, but realize that it could be trickier to compute it on a non-global object (base address is not known at compile-time). However, the binary search does not even use *this. Thus, `std::lower_bound` and outcome validation should be possible to compute.

Godbolt: https://gcc.godbolt.org/z/c7E3P9


---


### compiler : `gcc`
### title : `False positive -Wstringop-overflow with -O3 due to loop unrolling`
### open_at : `2020-08-03T20:51:59Z`
### last_modified_date : `2022-10-23T00:34:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96447
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 48990
test.c

The attached reduced test case results in the following false positive warning at -03 (but not -O2) without any additional compiler flags with both GCC 10.2.0 and GCC 11.0.0 (compiled from git today):

test.c: In function ‘load’:
test.c:23:25: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
   23 |                 data[i] = get8u(s);
      |                 ~~~~~~~~^~~~~~~~~~
test.c:21:23: note: at offset 4 to object ‘data’ with size 4 declared here
   21 |         unsigned char data[4];
      |                       ^~~~


---


### compiler : `gcc`
### title : `PRE gets confused by punned load handling`
### open_at : `2020-08-04T11:44:19Z`
### last_modified_date : `2021-07-28T20:28:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96457
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int flag;
union { double f; unsigned long long i; } u;
void foo();
unsigned long long i;
unsigned long long
test ()
{
  double f = 0.;
  if (flag)
    {
      foo ();
      f = u.f;
    }
  else
    i = u.i;
  f = u.f + f;
  return f;
}

here we fail to PRE the load of u.f because PHI translation to the else
path fails since we run into

            /* If we'd have to convert things we would need to validate
               if we can insert the translated expression.  So fail
               here for now - we cannot insert an alias with a different
               type in the VN tables either, as that would assert.  */
            if (result
                && !useless_type_conversion_p (ref->type, TREE_TYPE (result)))
              return NULL;

because we found a value for u.f there, that of u.i (because we value-number
them the same).  If one replaces 'i = u.i;' with 'foo ();' we appropriately
insert a load from u.f in the else path and remove the partial redundancy.


---


### compiler : `gcc`
### title : `[SVE] Use the HISTCNT instruction for simple histogram loops`
### open_at : `2020-08-04T15:42:19Z`
### last_modified_date : `2020-08-05T07:23:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96461
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
SVE2 has a HISTCNT instruction that, for each element, counts
the number of matching elements in that lane and previous lanes.
It allows things like:

void
update (uint32_t *histogram, uint32_t *records, uint32_t num_records)
{
  for (uint32_t i = 0; i < num_records; i++)
    histogram[records[i]] += 1;
}

to be vectorised using a gather load, HISTCNT and scatter store.

The instruction is intended for loops that do more than the bare
minimum above, but as far as autovec goes, we need to start somewhere.
This PR is therefore about recognising the:

    histogram[…] += 1;

gather/HISTCNT/scatter pattern and generating something like the
asm below for the function above:

        mov     x3, #0
        whilelo p0.s, xzr, x2
.loop
        ld1w    z1.s, p0/z, [x1, x3, lsl #2]
        ld1w    z2.s, p0/z, [x0, z1.s, uxtw #2]
        histcnt z0.s, p0/z, z1.s, z1.s
        add     z2.s, p0/m, z2.s, z0.s
        st1w    z2.s, p0, [x0, z1.s, uxtw #2]
        incw    x3
        whilelo p0.s, x3, x2
        b.first .loop
        ret

Once that's done we could move on to fancier cases.


---


### compiler : `gcc`
### title : `[SVE] Optimise svld1rq from vectors`
### open_at : `2020-08-04T16:04:44Z`
### last_modified_date : `2023-06-17T07:49:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96463
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
The code:

#include <arm_sve.h>
#include <arm_neon.h>

svint32_t
foo (int32x4_t x)
{
  return svld1rq (svptrue_b8 (), &x[0]);
}

currently generates:

        sub     sp, sp, #16
        ptrue   p0.b, all
        str     q0, [sp]
        ld1rqw  z0.s, p0/z, [sp]
        add     sp, sp, 16
        ret

but we should instead be able to generate:

        dup     z0.q, z0.q[0]

(at least on little-endian targets).  Perhaps svld1rq_impl should
lower the call to a VEC_PERM_EXPR if the argument is based on a
vector.  (And perhaps more generally, although that would need
testing.)


---


### compiler : `gcc`
### title : `direct threaded interpreter with computed gotos generates suboptimal dispatch loop`
### open_at : `2020-08-05T00:14:08Z`
### last_modified_date : `2022-08-22T15:15:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96475
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 48999
test case

The attached test case code generation with -O2 for run_program_goto generates a central indirect branch dispatch to handlers that branch back to the central dispatcher.

Direct threaded code with indirect branches between handlers is faster on a POWER9 when there are no branch mispredictions due to fewer branches, and it should generally do better with branch prediction when there is an indirect branch from each handler.


---


### compiler : `gcc`
### title : `[10 Regression] missed optimisation: unnecessary compare in standard algorithms`
### open_at : `2020-08-05T08:53:43Z`
### last_modified_date : `2023-07-07T09:01:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96480
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Consider the C++ code:

#include <algorithm>
#include <array>

enum ENUM { A, B, C, D, E, F, G, };

const std::array<ENUM, 4> foo{A, B, C, D};

bool is_foo(ENUM e) {
    return std::any_of(foo.begin(), foo.end(),
    [e] (auto ee) { return e == ee; });
}

GCC 7.4 optimizes if_foo to a single compare operation e <= 3,
but newer GCC versions do two comare operations e <= 2 || e == 3.

see https://godbolt.org/z/5a6aPa


---


### compiler : `gcc`
### title : `SLP fail to vectorize VEC_COND_EXPR pattern.`
### open_at : `2020-08-05T09:26:10Z`
### last_modified_date : `2021-08-20T10:46:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96481
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
testcase not vectorized:
-----
#include <x86intrin.h>

inline unsigned opt(unsigned a, unsigned b, unsigned c, unsigned d) {
    return a > b ? c : d;
}

void opt( unsigned * __restrict dst, const unsigned *pa, const unsigned *pb,
        const unsigned *pc, const unsigned  *pd )
{
    
     *dst++ = opt(*pa++, *pb++, *pc++, *pd++);
     *dst++ = opt(*pa++, *pb++, *pc++, *pd++);
     *dst++ = opt(*pa++, *pb++, *pc++, *pd++);
     *dst++ = opt(*pa++, *pb++, *pc++, *pd++);
}
----


testcase successfully vectorized:

----
inline unsigned opt(unsigned a, unsigned b, unsigned c, unsigned d) {
    return a > b ? c : d;
}

void opt( unsigned * __restrict dst, const unsigned *pa, const unsigned *pb,
        const unsigned *pc, const unsigned  *pd )
{
    for (int i = 0; i != 4; i++)
     *dst++ = opt(*pa++, *pb++, *pc++, *pd++);
}
----

llvm can handle both case
refer to https://godbolt.org/z/jYoPxT


---


### compiler : `gcc`
### title : `cddce1 optimizer depends on order of basic blocks`
### open_at : `2020-08-05T18:08:35Z`
### last_modified_date : `2023-09-18T06:31:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96487
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 49007
c and c++ dump files

If the test case gcc.dg/tree-ssa/ssa-dce-3.c is compiled as C++ code
instead of as C, it fails to reduce to an empty loop as expected.

It seems to be triggered by a slight difference in the input coming
into the cddce1 pass.  The C front end canonicalizes the test to the
end of the loop so the latch (bb 5) falls through to the header (bb
6).  The C++ front end orders the latch (bb 6) last with a goto to the
header (bb 3).

I did some additional tracing of the flow through the pass beyond the
dump file output.  Because the latch in the C input does not end with
a control statement, it is ignored by mark_last_stmt_necessary, via
the call to mark_control_dependent_edges_necessary at the end of
find_obviously_necessary_stmts.  So in the C case, nothing gets added
to the work list, while for C++ it does process the latch block,
follow the control flow out of it, and ends up marking the loop end
test etc as necessary.

I am wondering if this is a bug in the way the C output is handled and
it is incorrectly optimizing away the loop body.  It seems like it
should not matter if the control transfer between blocks is done via
explicit goto or via fallthrough, anyway; either it ought to handle
the fallthrough case like the explicit goto case, or vice versa.

I originally noticed this problem in conjunction with these patches to
unify the loop handling in the C and C++ front ends:

https://gcc.gnu.org/pipermail/gcc-patches/2019-November/534142.html

But it can be reproduced with unmodified sources just by compiling
with g++ instead of gcc.  The commands used to produce the attached
dump files were

x86_64-linux-gnu-gcc /path/to/gcc/testsuite/gcc.dg/tree-ssa/ssa-dce-3.c  -O2 -fdump-tree-dse1 -fdump-tree-cddce1-details -S

x86_64-linux-gnu-g++ /path/to/gcc/testsuite/gcc.dg/tree-ssa/ssa-dce-3.c  -O2 -fdump-tree-dse1 -fdump-tree-cddce1-details -S


---


### compiler : `gcc`
### title : `[11 Regression] vector comparisons on ARM`
### open_at : `2020-08-07T17:17:20Z`
### last_modified_date : `2020-10-02T12:11:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96528
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
(see the discussion after https://gcc.gnu.org/pipermail/gcc-patches/2020-August/551468.html )

I am using a compiler configured with --target=arm-none-linux-gnueabihf --with-float=hard --with-cpu=cortex-a9 --with-fpu=neon-fp16

typedef unsigned int vec __attribute__((vector_size(16)));
typedef int vi __attribute__((vector_size(16)));
vi f(vec a,vec b){
    return a==5 | b==7;
}

Compiling with -O yields very long scalar code. Adding -fno-tree-forwprop gets back the nice, vector code. (at higher optimization levels, one may also need to disable vrp)

This is due to the fact that while the ARM target handles VEC_COND_EXPR<v == w, -1, 0> just fine, it does not handle a plain v == w that is not fed directly to a VEC_COND_EXPR. I was surprised to notice that "grep vec_cmp" gives a number of lines in the aarch64/ directory, but none in arm/, while AFAIK those neon instructions are the same. Would it be possible to implement this on ARM as well? Other middle-end options are also possible, but the difference with aarch64 makes it tempting to handle it in the target.


---


### compiler : `gcc`
### title : `[10 Regression] GCC 10 ignoring function __attribute__ optimize for all x86 since r11-1019`
### open_at : `2020-08-08T10:23:53Z`
### last_modified_date : `2020-08-25T18:24:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96535
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Hey GCC team!

In GCC 10.x, it seems like any argument to __attribute__((optimize(...)) is ignored at the function level. GCC 9.x and previous do not have this issue. [Or maybe only -funroll-loops is ignored not 100% sure]

Detailed example at: https://gcc.godbolt.org/z/PTK4WE

3 Scenarios

1. [GCC 10.2: -O2 -ffast-math -march=haswell -std=c++2a -fopenmp] + [__attribute__((optimize("O2","fast-math","unroll-loops")))] DOES NOT unroll.

2. [GCC 10.2: -funroll-loops -O2 -ffast-math -march=haswell -std=c++2a -fopenmp] + [__attribute__((optimize("O2","fast-math","unroll-loops")))] DOES unroll.

3. [GCC 9.3:  -O2 -ffast-math -march=haswell -std=c++2a -fopenmp] + [__attribute__((optimize("O2","fast-math","unroll-loops")))] DOES unroll.

It seems that in GCC 10.x, you have to place -funroll-loops in the compilation string, and function level __attribute__s are ignored?

PS: Code in godbolt is a matrix multiplication kernel. It multiplies 1 column * 1 row of a matrix.


---


### compiler : `gcc`
### title : `Unnecessary no-op copy with Os and tail call with struct argument`
### open_at : `2020-08-09T00:04:44Z`
### last_modified_date : `2020-08-25T08:06:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96539
### status : `REOPENED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.1.0`
### severity : `normal`
### contents :
Test C code,

```
struct A {
    int a;
    int b;
    int c;
    int d;
    int e;
    int f;
    void *p1;
    void *p2;
    void *p3;
    void *p4;
    void *p5;
    void *p6;
    void *p7;
};

int k(int a);
int f(int a, int b, int c, void *p, struct A s);

int g(int a, int b, int c, void *p, struct A s)
{
    k(a);
    return f(a, b, c, p, s);
}
```

At `-O2`, the code produced is

```
g:
        pushq   %r14
        movq    %rcx, %r14
        pushq   %r13
        movl    %edx, %r13d
        pushq   %r12
        movl    %esi, %r12d
        pushq   %rbp
        movl    %edi, %ebp
        subq    $8, %rsp
        call    k@PLT
        addq    $8, %rsp
        movq    %r14, %rcx
        movl    %r13d, %edx
        movl    %r12d, %esi
        movl    %ebp, %edi
        popq    %rbp
        popq    %r12
        popq    %r13
        popq    %r14
        jmp     f@PLT
```

I'm not sure why the spill of register and save the argument in those registers (maybe for latency for the final call?) but both clang and gcc does that so I assume that's good for performance. However, when I tried `-Os`, the code produced is,

```
g:
        pushq   %r14
        movq    %rcx, %r14
        pushq   %r12
        movl    %esi, %r12d
        pushq   %rbp
        movl    %edi, %ebp
        subq    $16, %rsp
        movl    %edx, 12(%rsp)
        call    k@PLT
        leaq    48(%rsp), %rdi
        movl    $20, %ecx
        movq    %rdi, %rsi
        rep movsl
        movq    %r14, %rcx
        movl    %r12d, %esi
        movl    %ebp, %edi
        movl    12(%rsp), %edx
        addq    $16, %rsp
        popq    %rbp
        popq    %r12
        popq    %r14
        jmp     f@PLT
```

AFAICT, the

```
        movq    %rdi, %rsi
        rep movsl
```

is basically always a no-op (moving from and to the same memory location) other than potentially triggering memory fault.

The memory being copied in place here is the area where the argument is stored (80 bytes starting at `rsp + 48`) so maybe it's the copying of the argument that failed to be removed when it becomes an no-op for tail call?

At `-O1`, the code produced is

```
g:
        pushq   %r13
        pushq   %r12
        pushq   %rbp
        pushq   %rbx
        subq    $8, %rsp
        movl    %edi, %ebx
        movl    %esi, %ebp
        movl    %edx, %r12d
        movq    %rcx, %r13
        call    k@PLT
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        pushq   120(%rsp)
        movq    %r13, %rcx
        movl    %r12d, %edx
        movl    %ebp, %esi
        movl    %ebx, %edi
        call    f@PLT
        addq    $88, %rsp
        popq    %rbx
        popq    %rbp
        popq    %r12
        popq    %r13
        ret
```
which shows the copying of 10 pointers that was not no-op without tail call.


---


### compiler : `gcc`
### title : `Failure to optimize simple code to a constant when storing part of the operation in a variable`
### open_at : `2020-08-09T22:10:10Z`
### last_modified_date : `2023-09-03T12:06:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96542
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
uint8_t f(uint32_t x)
{
    bool tmp = x;
    return (0xFF >> tmp) * 2;
}

This can be optimized to `return -2;`. This transformation is done by LLVM, but not by GCC. Strangely, this only seems to happen if `(bool)x` is stored in an intermediate variable, otherwise the transformation is done by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize loop with condition to simple arithmetic`
### open_at : `2020-08-11T01:30:10Z`
### last_modified_date : `2023-09-03T12:05:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96563
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x)
{
    int i = 0;
    while (i <= 9)
    {
        if (i == x)
            return 8;
        ++i;
    }
    return 4;
}

This can be optimized to `return 4 + ((x <= 9) * 4);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] New maybe use of uninitialized variable warning since r11-959`
### open_at : `2020-08-11T07:30:04Z`
### last_modified_date : `2023-05-29T10:03:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96564
### status : `NEW`
### tags : `alias, diagnostic, missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
Consider the following MWE:

extern void* malloc (long unsigned int);
void fun (unsigned *x) {
  unsigned *a = malloc (*x);
  if (a == 0)
    return;
  if (a != x)            // (A)
    *a = *x;
  *x = *a;
}

If compiled with GCC 10, then no warning is emitted.

If compiled with GCC HEAD (currently 84005b8abf9), then the following warning is emitted:

gcc -W -c -O2 mwe.c
mwe.c: In function 'fun':
mwe.c:8:8: warning: '*(a)' may be used uninitialized [-Wmaybe-uninitialized]
    8 |   *x = *a;
      |        ^~

Rational why this example is strictly speaking fine:

1) Assume x is a valid pointer.  Then malloc will either return a nullpointer and we return, or a pointer to a fresh object which address is different from any other existing object.  Thus (A) always evaluates to true which means *a is initialized.

2) Assume x is an invalid pointer.  Then dereferencing x prior to the call to malloc already results in UB.

Thus the only case in which condition (A) may evaluate to false is when x is an invalid pointer (e.g. previously malloc'd and then free'd such that a further call to malloc returns the very same address rendering (A) false) results in UB.

Since this is a maybe warning I'm wondering whether this is considered a bug or is acceptable.  Any thoughts?


---


### compiler : `gcc`
### title : `Failure to optimize out VLA even though it is left unused`
### open_at : `2020-08-11T10:56:09Z`
### last_modified_date : `2023-09-03T12:05:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96565
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool g(void);

void f(int x)
{
    char arr[x];

    arr[0] = 0;

    if (g())
        abort();
}

The VLA can be removed. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize out branch when it always results in UB from dereferencing a pointer to an undefined value set in there`
### open_at : `2020-08-11T17:07:06Z`
### last_modified_date : `2023-09-03T12:05:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96572
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int *p, bool cond)
{
  int i;
  int *q;
  if (cond)
    q = &i;
  else
    q = p;
  return *q + *p;
}

This can be optimized to `return (*p) * 2;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[10 Regression] Regression in optimization on x86-64 with -O3`
### open_at : `2020-08-11T17:45:09Z`
### last_modified_date : `2023-07-07T09:03:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96573
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 49046
This file contains the source code of the function described in the report.

Hello,

I'd like to describe here what seems to be a regression/missed optimization since GCC 10.

The host and target architecture is x86-64.
I'm using Fedora 32 with Linux 5.7, but I could reproduce it on many other Linux platforms.
I'm using GCC 10.2, but I could reproduce it with any GCC 10 minor version.


Here is a function, written in pure ANSI C99:

#include <stdlib.h>
#include <stdint.h>

void *
ReverseBytesOfPointer(void * const pointer)
{
  const size_t maxIndex = sizeof(pointer) - 1;
  const uint8_t * const oldPointerPointer = (uint8_t*)&pointer;
  void *newPointer;
  uint8_t * const newPointerPointer = (uint8_t *)&newPointer;
  uint8_t i;

  for (i = 0; i <= maxIndex; ++i) {
    newPointerPointer[maxIndex - i] = oldPointerPointer[i];
  }

  return newPointer;
}


What this function does is simply to reverse all the bytes of a pointer. It is written in pure C99 and is extremely portable, as it works from 16-bit to 64-bit machines. (I wrote it and use it for embedded development and I'm happy with it).

What makes it magical is that when compiled with -O3 (and -std=c99 -Wall -Werror -Wextra), GCC 9 (yes, 9) is clever enough to deduce the intent of this function and compiles it all as:

ReverseBytesOfPointer:
        mov     rax, rdi
        bswap   rax
        ret


However, since GCC 10, the magic seems to have disappeared. This is the ASM code that is generated now, with the exact same command line invocation:

ReverseBytesOfPointer:
        movq    %rdi, %rax
        movb    %dil, -1(%rsp)
        movzbl  %ah, %edx
        shrq    $56, %rax
        movb    %dl, -2(%rsp)
        movq    %rdi, %rdx
        shrq    $16, %rdx
        movb    %al, -8(%rsp)
        movb    %dl, -3(%rsp)
        movq    %rdi, %rdx
        shrq    $24, %rdx
        movb    %dl, -4(%rsp)
        movq    %rdi, %rdx
        shrq    $32, %rdx
        movb    %dl, -5(%rsp)
        movq    %rdi, %rdx
        shrq    $40, %rdx
        movb    %dl, -6(%rsp)
        movq    %rdi, %rdx
        shrq    $48, %rdx
        movb    %dl, -7(%rsp)
        movq    -8(%rsp), %rax
        ret


For your convenience, I include here a link to a snippet on compiler-explorer to show the comparison between versions 9.3 and 10.2. This snippet also includes a few unit tests:

https://godbolt.org/z/YG1KPf


Regards,

Remi Andruccioli


---
