### Total Bugs Detected: 4649
### Current Chunk: 14 of 30
### Bugs in this Chunk: 160 (From bug 2081 to 2240)
---


### compiler : `gcc`
### title : `Missed optimization: sar and shr equivalent for non-negative numbers`
### open_at : `2019-02-02T20:50:19Z`
### last_modified_date : `2023-10-24T20:47:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89163
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `enhancement`
### contents :
For the minimized test case below, shift is equivalent to same_as, because shr and sar are equivalent for non-negative numbers. The generated code is not the same.

int shift(int i) {
	return i >= 0 ? (unsigned)i >> 8 : i >> 8;
}
int same_as(int i) {
	return i >> 8;
}

shift:
	movl	%edi, %edx
	movl	%edi, %eax
	shrl	$8, %edx
	sarl	$8, %eax
	testl	%edi, %edi
	cmovns	%edx, %eax
	ret

same_as:
	movl	%edi, %eax
	sarl	$8, %eax
	ret


---


### compiler : `gcc`
### title : `Vectorizer fails to consider narrower vector width for res[i] = v1[i] < v2[i] ? v2[i] : v1[i]`
### open_at : `2019-02-03T15:48:59Z`
### last_modified_date : `2021-08-16T09:00:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89176
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
[hjl@gnu-cfl-1 pr89028]$ cat 2c.i
float v1[] = { 8.3, 3.4, 8.3, 3.4, 5.8, 9.7, 5.8, 9.7, 8.3, 3.4, 8.3, 3.4 };
float v2[] = { 5.8, 9.7, 8.3, 3.4, 8.3, 3.4, 8.3, 3.4, 8.3, 3.4, 5.8, 9.7 };

float res[12];

void
foo (void)
{
  int i;

  for (i = 0; i < sizeof (res) / sizeof (res[0]); i++)
    res[i] = v1[i] < v2[i] ? v2[i] : v1[i];
}
[hjl@gnu-cfl-1 pr89028]$ make 2c.s 
/export/build/gnu/tools-build/gcc-mmx-debug/build-x86_64-linux/gcc/xgcc -B/export/build/gnu/tools-build/gcc-mmx-debug/build-x86_64-linux/gcc/ -O3  -march=haswell -S 2c.i
[hjl@gnu-cfl-1 pr89028]$ cat 2c.s
	.file	"2c.i"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	vmovaps	v2(%rip), %ymm1
	vmaxps	v1(%rip), %ymm1, %ymm0
	vmovups	%ymm0, res(%rip)
	vmovss	v2+32(%rip), %xmm0
	vmaxss	v1+32(%rip), %xmm0, %xmm0
	vmovss	%xmm0, res+32(%rip)
	vmovss	v2+36(%rip), %xmm0
	vmaxss	v1+36(%rip), %xmm0, %xmm0
	vmovss	%xmm0, res+36(%rip)
	vmovss	v2+40(%rip), %xmm0
	vmaxss	v1+40(%rip), %xmm0, %xmm0
	vmovss	%xmm0, res+40(%rip)
	vmovss	v2+44(%rip), %xmm0
	vmaxss	v1+44(%rip), %xmm0, %xmm0
	vmovss	%xmm0, res+44(%rip)
	vzeroupper
	ret
	.cfi_endproc

We generate 4 scalar res[i] = v1[i] < v2[i] ? v2[i] : v1[i].  But this
works:

[hjl@gnu-cfl-1 pr89028]$ cat 3a.i
float v1[] = { 8.3, 3.4, 8.3, 3.4, 5.8, 9.7, 5.8, 9.7, 8.3, 3.4, 8.3, 3.4 };
float v2[] = { 5.8, 9.7, 8.3, 3.4, 8.3, 3.4, 8.3, 3.4, 8.3, 3.4, 5.8, 9.7 };

float res[12];


void
foo (void)
{
  int i;

  for (i = 0; i < sizeof (res) / sizeof (res[0]); i++)
    res[i] = v2[i] * v1[i];
}
[hjl@gnu-cfl-1 pr89028]$ make 3a.s
/export/build/gnu/tools-build/gcc-mmx-debug/build-x86_64-linux/gcc/xgcc -B/export/build/gnu/tools-build/gcc-mmx-debug/build-x86_64-linux/gcc/ -O3  -march=haswell -S 3a.i
[hjl@gnu-cfl-1 pr89028]$ cat 3a.s
	.file	"3a.i"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	vmovaps	v2(%rip), %ymm1
	vmulps	v1(%rip), %ymm1, %ymm0
	vmovaps	v1+32(%rip), %xmm2
	vmovups	%ymm0, res(%rip)
	vmulps	v2+32(%rip), %xmm2, %xmm0
	vmovaps	%xmm0, res+32(%rip)
	vzeroupper
	ret


---


### compiler : `gcc`
### title : `GCC does not simplify expressions involving shifts, bitwise operators and comparisons.`
### open_at : `2019-02-04T09:31:08Z`
### last_modified_date : `2023-08-08T02:22:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89184
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
GCC compiles the function

int foo (unsigned i)
{
    return ((i >> 1) & 3) == 2;
}

to (in x86-64 assembly)

foo:
        shrl    %edi
        xorl    %eax, %eax
        andl    $3, %edi
        cmpl    $2, %edi
        sete    %al
        ret

This is more complicated than necessary; the following assembly produced by clang needs one instruction less:

foo:
        andl    $6, %edi
        xorl    %eax, %eax
        cmpl    $4, %edi
        sete    %al
        retq


---


### compiler : `gcc`
### title : `missed optimization for 16/8-bit vector shuffle`
### open_at : `2019-02-04T12:19:22Z`
### last_modified_date : `2021-08-10T23:02:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89189
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Testcase `-O2 -msse2`, further missed optimization with SSSE3 / SSE4.1 (cf. https://godbolt.org/z/Yx6aLo):

using vshort [[gnu::vector_size(16)]] = short;
vshort f(vshort x) {
    return vshort{x[3], x[7]};
}

using vchar [[gnu::vector_size(16)]] = char;
vchar g(vchar x) {
    return vchar{x[7], x[15]};
}

f is compiled to 2x pextrw, movd, pinsrw + unpacks for zeroing high bits. The latter unpacks are unnecessary since movd already zeros the high bits [127:32].

With SSE4.1 g is compiled to a similar pattern using pextrb/pinsrb. In this case movd is used, but note that pextrb zeros the bits [31:8] in the GPR, so that the unpacks for zeroing are also unnecessary.

Using SSSE3, both functions can also be compiled to a single pshufb instruction using a suitable constant shuffle vector (6,7,14,15,-1,-1,... and 7,15,-1,-1,...).


---


### compiler : `gcc`
### title : `GCC generates/fails to optimize unnecessary sign extension instruction`
### open_at : `2019-02-04T20:28:45Z`
### last_modified_date : `2021-06-27T22:34:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89198
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC generates/fails to optimize unnecessary sign extension instruction.

unsigned char foo(char c)
{
    unsigned i = c;
    return ++i;
}

Results in:
    movsx   eax, dil
    add     eax, 1
    ret

While this one:

unsigned char bar(char c)
{
    unsigned char i = c;
    return ++i;
}

Results in:
    lea     eax, [rdi+1]
    ret

https://godbolt.org/z/I54SZr

I found a lot of reports about redundant sign/zero extension instructions, did not open them all but the ten I had opened have more complex examples.


---


### compiler : `gcc`
### title : `Optimize V2DI shifts by a constant on power8 & above systems.`
### open_at : `2019-02-05T20:54:33Z`
### last_modified_date : `2023-07-22T03:09:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89213
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
ISA 2.07 (i.e. -mcpu=power8) and above added support for doing various operations on V2DI (i.e. vector long long) data types, including shifts.

If you generate code to shift a V2DI type by a constant, the compiler generates sub-optimal code:

For example:

#include <altivec.h>

typedef vector long long vi64_t;
typedef vector unsigned long long vui64_t;

vi64_t
shiftra_test64 (vi64_t a)
{
  vui64_t x = {4, 4};
  return (vi64_t) vec_vsrad (a, x);
}

Generates:

shiftra_test64:
        xxspltib 0,4
        xxlor 32,0,0
        vextsb2d 0,0
        vsrad 2,2,0
        blr

when it could generate:

shiftra_test64:
        vspltisw 0,4
        vsrad 2,2,0
        blr

This is true of all 3 shift operations (shift left, logical shift right, and arithmetic shift right).


---


### compiler : `gcc`
### title : `Vector load/store aren't used to initialize large memory`
### open_at : `2019-02-08T12:30:00Z`
### last_modified_date : `2021-08-01T17:20:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89252
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
[hjl@gnu-cfl-2 tmp]$ cat /tmp/x.i
struct S
{
  void *s1;
  unsigned s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14;
};

const struct S array[] = {
  { (void *) 0, 60, 640, 2112543726, 39682, 48, 16, 33, 10, 96, 2, 0, 0, 4 },
  { (void *) 0, 60, 2112543726, 192, 18251, 16, 33, 10, 96, 2, 0, 0, 4, 212 }
};

void
foo (struct S *x)
{
  x[0] = array[0];
  x[5] = array[1];
}
[hjl@gnu-cfl-2 tmp]$ gcc -S -O2 x.i
[hjl@gnu-cfl-2 tmp]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4,,15
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movq	$0, (%rdi)
	movl	$60, 8(%rdi)
	movl	$640, 12(%rdi)
	movl	$2112543726, 16(%rdi)
	movl	$39682, 20(%rdi)
	movl	$48, 24(%rdi)
	movl	$16, 28(%rdi)
	movl	$33, 32(%rdi)
	movl	$10, 36(%rdi)
	movl	$96, 40(%rdi)
	movl	$2, 44(%rdi)
	movl	$0, 48(%rdi)
	movl	$0, 52(%rdi)
	movl	$4, 56(%rdi)
	movq	$0, 320(%rdi)
	movl	$60, 328(%rdi)
	movl	$2112543726, 332(%rdi)
	movl	$192, 336(%rdi)
	movl	$18251, 340(%rdi)
	movl	$16, 344(%rdi)
	movl	$33, 348(%rdi)
	movl	$10, 352(%rdi)
	movl	$96, 356(%rdi)
	movl	$2, 360(%rdi)
	movl	$0, 364(%rdi)
	movl	$0, 368(%rdi)
	movl	$4, 372(%rdi)
	movl	$212, 376(%rdi)
	ret
	.cfi_endproc

We can do

foo:
.LFB0:
	.cfi_startproc
	movdqa	array(%rip), %xmm0
	movdqa	array+16(%rip), %xmm1
	movdqa	array+32(%rip), %xmm2
	movdqa	array+48(%rip), %xmm3
	movdqa	array+64(%rip), %xmm4
	movdqa	array+80(%rip), %xmm5
	movups	%xmm0, (%rdi)
	movdqa	array+96(%rip), %xmm6
	movdqa	array+112(%rip), %xmm7
	movups	%xmm1, 16(%rdi)
	movups	%xmm2, 32(%rdi)
	movups	%xmm3, 48(%rdi)
	movups	%xmm4, 320(%rdi)
	movups	%xmm5, 336(%rdi)
	movups	%xmm6, 352(%rdi)
	movups	%xmm7, 368(%rdi)
	ret
	.cfi_endproc


---


### compiler : `gcc`
### title : `No optimized division by constant for __int128`
### open_at : `2019-02-08T14:46:56Z`
### last_modified_date : `2021-08-15T11:20:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89256
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.2.0`
### severity : `normal`
### contents :
Division by constant is not optimized for __int128 dividend.

// This function will use shift+multiply
int64_t func64( int64_t val )
{
  return val / 1000;
}

// This function will call __divti3
__int128 func128( __int128 val )
{
  return val / 1000;
}

It would be nice if GCC would use the same optimisation for __int128 and unsigned __int128.


---


### compiler : `gcc`
### title : `Simplify bool expression to OR`
### open_at : `2019-02-09T10:03:00Z`
### last_modified_date : `2023-06-07T02:46:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89263
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
bool foo(bool a, bool b)
{
    if (a)
        return true;
    return b;   
}

Current:
foo(bool, bool):
        test    dil, dil
        mov     eax, esi
        cmovne  eax, edi
        ret

Better:
foo(bool, bool):
  mov eax, edi
  or eax, esi
  ret


---


### compiler : `gcc`
### title : `[9/10 Regression] gcc.target/powerpc/vsx-simode2.c stopped working in GCC 9`
### open_at : `2019-02-09T22:06:09Z`
### last_modified_date : `2019-05-08T23:17:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89271
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
In GCC 8 and earlier, this generates

        mtvsrwz 32,3
#APP
 # 10 "vsx-simode2.c" 1
        xxlor 32,32,32  # v, v constraints
 # 0 "" 2
#NO_APP
        mfvsrwz 3,32
        blr

but in GCC 9 it is

        std 3,-16(1)
        ori 2,2,0
        lwz 9,-12(1)
        mtvsrwz 32,9
#APP
 # 10 "vsx-simode2.c" 1
        xxlor 32,32,32  # v, v constraints
 # 0 "" 2
#NO_APP
        mfvsrwz 3,32
        blr

and soon it will be

        std 3,-16(1)
        addi 9,1,-12
        lxsiwzx 32,0,9
        blr

[ Hrm, no ori 2,2,0?  And it is better to do li 9,-12 etc. ]

This is because IRA does

     r125: preferred NO_REGS, alternative NO_REGS, allocno NO_REGS

   a1(r125,l0) costs: BASE_REGS:14004,14004 GENERAL_REGS:14004,14004-
   LINK_REGS:24010,24010 CTR_REGS:24010,24010 LINK_OR_CTR_REGS:24010,24010-
   SPEC_OR_GEN_REGS:24010,24010 MEM:12000,12000

and it then chooses disposition mem for r125.

In GCC 8 and before combine already has decided to use GPR3 (the first
argument register) for this, so there was no RA here before.


---


### compiler : `gcc`
### title : `Poor code generation returning float field from a struct`
### open_at : `2019-02-12T17:43:47Z`
### last_modified_date : `2020-07-22T01:34:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89310
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The following test case shows poor code generation on powerpc64le-linux:

bergner@pike:~/gcc/BUGS/PR88845$ cat struct.i
struct s {
  int i;
  float f;
};

float
foo (struct s arg)
{
  return arg.f;
}
bergner@pike:~/gcc/BUGS/PR88845$ gcc -O2 -S struct.i
bergner@pike:~/gcc/BUGS/PR88845$ cat struct.s 
foo:
	srdi 3,3,32
	sldi 9,3,32
	mtvsrd 1,9
	xscvspdpn 1,1
	blr

The srdi followed by a sldi could be replaced with a simple: rldicr 9,3,0,31


---


### compiler : `gcc`
### title : `Ineffective code from std::copy`
### open_at : `2019-02-12T18:15:50Z`
### last_modified_date : `2022-12-12T07:55:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89317
### status : `RESOLVED`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
gcc produces ineffective code when std::copy is used to copy data. For test I created my own version of std::copy and this version is optimized properly.

Compiles using g++ (GCC-Explorer-Build) 9.0.1 20190211 (experimental)
Options: -O3 -std=c++11 -march=skylake

[code]
#include <stdint.h>
#include <algorithm>

#define Size 8

class Test
{
public:
    void test1(void*__restrict ptr);
    void test2(void*__restrict ptr);

private:
    int16_t data1[Size];
    int16_t data2[Size];
};

template<typename T1, typename T2>
void mycopy(T1 begin, T1 end, T2 dest)
{
    while (begin != end)
    {
        *dest = *begin;
        ++dest;
        ++begin;
    }
}

void Test::test1(void*__restrict ptr)
{
    uint16_t* p = (uint16_t*)ptr;

    std::copy(data1, data1 + Size, p);
    p += Size;
    std::copy(data2, data2 + Size, p);
}

void Test::test2(void*__restrict ptr)
{
    int16_t* p = (int16_t*)ptr;

    mycopy(data1, data1 + Size, p);
    p += Size;
    mycopy(data2, data2 + Size, p);
}
[/code]

[asm]
Test::test1(void*):
        movzx   eax, WORD PTR [rdi]
        mov     edx, 16
        mov     WORD PTR [rsi], ax
        movzx   eax, WORD PTR [rdi+2]
        add     rsi, 16
        mov     WORD PTR [rsi-14], ax
        movzx   eax, WORD PTR [rdi+4]
        mov     WORD PTR [rsi-12], ax
        movzx   eax, WORD PTR [rdi+6]
        mov     WORD PTR [rsi-10], ax
        movzx   eax, WORD PTR [rdi+8]
        mov     WORD PTR [rsi-8], ax
        movzx   eax, WORD PTR [rdi+10]
        mov     WORD PTR [rsi-6], ax
        movzx   eax, WORD PTR [rdi+12]
        mov     WORD PTR [rsi-4], ax
        movzx   eax, WORD PTR [rdi+14]
        mov     WORD PTR [rsi-2], ax
        mov     rax, rdx
        sar     rax
        test    rdx, rdx
        jle     .L69
        movzx   edx, WORD PTR [rdi+16]
        mov     WORD PTR [rsi], dx
        cmp     rax, 1
        je      .L69
        movzx   edx, WORD PTR [rdi+18]
        mov     WORD PTR [rsi+2], dx
        cmp     rax, 2
        je      .L69
        movzx   edx, WORD PTR [rdi+20]
        mov     WORD PTR [rsi+4], dx
        cmp     rax, 3
        je      .L69
        movzx   edx, WORD PTR [rdi+22]
        mov     WORD PTR [rsi+6], dx
        cmp     rax, 4
        je      .L69
        movzx   edx, WORD PTR [rdi+24]
        mov     WORD PTR [rsi+8], dx
        cmp     rax, 5
        je      .L69
        movzx   edx, WORD PTR [rdi+26]
        mov     WORD PTR [rsi+10], dx
        cmp     rax, 6
        je      .L69
        movzx   edx, WORD PTR [rdi+28]
        mov     WORD PTR [rsi+12], dx
        cmp     rax, 7
        je      .L69
        movzx   edx, WORD PTR [rdi+30]
        mov     WORD PTR [rsi+14], dx
        cmp     rax, 8
        je      .L69
        movzx   edx, WORD PTR [rdi+32]
        mov     WORD PTR [rsi+16], dx
        cmp     rax, 9
        je      .L69
        movzx   edx, WORD PTR [rdi+34]
        mov     WORD PTR [rsi+18], dx
        cmp     rax, 10
        je      .L69
        movzx   edx, WORD PTR [rdi+36]
        mov     WORD PTR [rsi+20], dx
        cmp     rax, 11
        je      .L69
        movzx   edx, WORD PTR [rdi+38]
        mov     WORD PTR [rsi+22], dx
        cmp     rax, 12
        je      .L69
        movzx   edx, WORD PTR [rdi+40]
        mov     WORD PTR [rsi+24], dx
        cmp     rax, 13
        je      .L69
        movzx   edx, WORD PTR [rdi+42]
        mov     WORD PTR [rsi+26], dx
        cmp     rax, 14
        je      .L69
        movzx   eax, WORD PTR [rdi+44]
        mov     WORD PTR [rsi+28], ax
.L69:
        ret
Test::test2(void*):
        vmovdqu xmm0, XMMWORD PTR [rdi]
        vmovups XMMWORD PTR [rsi], xmm0
        vmovdqu xmm1, XMMWORD PTR [rdi+16]
        vmovups XMMWORD PTR [rsi+16], xmm1
        ret
[/asm]


---


### compiler : `gcc`
### title : `PowerPC generates poor code when using attribute((vector_size(32))`
### open_at : `2019-02-12T19:26:13Z`
### last_modified_date : `2022-03-08T16:20:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89319
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 45677
Source code to show the problem

I tried out some code using the vector_size(32) attribute on PowerPC, and it generates poor code.  I expected that it would just generate two loads, two adds, and two stores, but instead it pushed things to the stack.

See the attached code and asm file.


---


### compiler : `gcc`
### title : `Missed detection of dead stores to array in a loop`
### open_at : `2019-02-13T11:54:45Z`
### last_modified_date : `2023-05-14T23:57:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89332
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Hi,
For the following test-case:

#define ARR_MAX 6

__attribute__((const)) int f(int);

int foo()
{
  int arr[ARR_MAX];

  for (int i = 0; i < ARR_MAX; i++)
    arr[i] = f(i);

  return arr[0];
}

With -O3, gcc generates call to f(i) and store to arr[i] on every iteration,
while clang detects the stores to arr are dead (except for arr[0]), removes the loop and emits a tail-call to f(0).

aarch64-linux-gnu-gcc -O3:
foo:
.LFB0:
        .cfi_startproc
        stp     x29, x30, [sp, -64]!
        .cfi_def_cfa_offset 64
        .cfi_offset 29, -64
        .cfi_offset 30, -56
        mov     x29, sp
        stp     x19, x20, [sp, 16]
        .cfi_offset 19, -48
        .cfi_offset 20, -40
        add     x20, sp, 40
        mov     w19, 0
        .p2align 3,,7
.L2:
        mov     w0, w19
        bl      f
        str     w0, [x20], 4
        add     w19, w19, 1
        cmp     w19, 6
        bne     .L2
        ldr     w0, [sp, 40]
        ldp     x19, x20, [sp, 16]
        ldp     x29, x30, [sp], 64
        .cfi_restore 30
        .cfi_restore 29
        .cfi_restore 19
        .cfi_restore 20
        .cfi_def_cfa_offset 0
        ret


clang -O3 --target=aarch64-linux-gnu:
foo:                                    // @foo
// %bb.0:
        mov     w0, wzr
        b       f

It seems, clang takes advantage of loop unrolling for the above-case,
while gcc doesn't seem to. After increasing ARR_MAX from 6 to 512, clang generates same/similar code as gcc.

I doubt tho if such code is written in practice or can result due to abstraction lowering ? It was just a contrived test-case I made up.

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `Unnecessary EVEX encoding`
### open_at : `2019-02-13T22:28:55Z`
### last_modified_date : `2020-03-11T12:07:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89346
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-skx-1 gcc]$ cat x.c
#include <immintrin.h>

long long *p;
volatile __m256i yy;

void
foo (void)
{
   _mm256_store_epi64 (p, yy);
}
[hjl@gnu-skx-1 gcc]$ gcc -S -O2 x.c -march=skylake-avx512
[hjl@gnu-skx-1 gcc]$ cat x.s
	.file	"x.c"
	.text
	.p2align 4,,15
	.globl	foo
	.type	foo, @function
foo:
.LFB5168:
	.cfi_startproc
	vmovdqa64	yy(%rip), %ymm0   <<< No need for EVEX.
	movq	p(%rip), %rax
	vmovdqa64	%ymm0, (%rax)     <<< No need for EVEX.
	vzeroupper
	ret
	.cfi_endproc
.LFE5168:
	.size	foo, .-foo
	.comm	yy,32,32
	.comm	p,8,8
	.ident	"GCC: (GNU) 8.2.1 20190209 (Red Hat 8.2.1-8)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-skx-1 gcc]$


---


### compiler : `gcc`
### title : `Unnecessary ENDBR with -mmanual-endbr`
### open_at : `2019-02-14T13:37:51Z`
### last_modified_date : `2019-02-23T02:42:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89353
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-mic-2 gcc]$ cat x.c
#ifdef FOO
__attribute__ ((cf_check))
#endif
int test(int* val)
{
    int status = 99;

    if((val == 0))
    {
        status = 22;
        goto end;
    }

    extern int x;
    *val = x;

    status = 0;
end:
    return status;
}
[hjl@gnu-mic-2 gcc]$ ./xgcc -B./ -m32 -O2 -fcf-protection x.c -S -mmanual-endbr
[hjl@gnu-mic-2 gcc]$ cat x.s
	.file	"x.c"
	.text
	.p2align 4
	.globl	test
	.type	test, @function
test:
.LFB0:
	.cfi_startproc
	movl	4(%esp), %eax
	testl	%eax, %eax
	je	.L3
	movl	x, %edx
	movl	%edx, (%eax)
	xorl	%eax, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L3:
.L2:
	endbr32 <<<<<<<< This should be there.
	movl	$22, %eax
	ret
	.cfi_endproc
.LFE0:
	.size	test, .-test
	.ident	"GCC: (GNU) 9.0.1 20190214 (experimental)"
	.section	.note.GNU-stack,"",@progbits
	.section	.note.gnu.property,"a"
	.align 4
	.long	 1f - 0f
	.long	 4f - 1f
	.long	 5
0:
	.string	 "GNU"
1:
	.align 4
	.long	 0xc0000002
	.long	 3f - 2f
2:
	.long	 0x3
3:
	.align 4
4:
[hjl@gnu-mic-2 gcc]$


---


### compiler : `gcc`
### title : `Unnecessary ENDBR`
### open_at : `2019-02-14T14:47:08Z`
### last_modified_date : `2022-05-27T08:26:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89355
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-2 gcc]$ cat x.i
int
test (int* val)
{
  int status = 99;

  if((val == 0))
    {
      status = 22;
      goto end;
    }

  extern int x;
  *val = x;

  status = 0;
end:
  return status;
}

[hjl@gnu-cfl-2 gcc]$ ./xgcc -B./ -S -O2 -fcf-protection  x.i
[hjl@gnu-cfl-2 gcc]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4
	.globl	test
	.type	test, @function
test:
.LFB0:
	.cfi_startproc
	endbr64
	testq	%rdi, %rdi
	je	.L3
	movl	x(%rip), %eax
	movl	%eax, (%rdi)
	xorl	%eax, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L3:
.L2:
	endbr64  <<< Why is ENDBR here?  There is no indirect branch.
	movl	$22, %eax
	ret
	.cfi_endproc
.LFE0:
	.size	test, .-test
	.ident	"GCC: (GNU) 9.0.1 20190214 (experimental)"
	.section	.note.GNU-stack,"",@progbits
	.section	.note.gnu.property,"a"
	.align 8
	.long	 1f - 0f
	.long	 4f - 1f
	.long	 5
0:
	.string	 "GNU"
1:
	.align 8
	.long	 0xc0000002
	.long	 3f - 2f
2:
	.long	 0x3
3:
	.align 8
4:
[hjl@gnu-cfl-2 gcc]$


---


### compiler : `gcc`
### title : `GCC doesn't emit cmovcc instruction in some cases`
### open_at : `2019-02-15T09:58:36Z`
### last_modified_date : `2023-05-06T19:02:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89360
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Here are two function:

void sort2_ternary(int a, int b, int *pa, int *pb)
{
  *pa = a < b ? a : b;
  *pb = a < b ? b : a;
}

void sort2_if(int a, int b, int *pa, int *pb)
{
  if (a < b) {
    *pa = a;
    *pb = b;
  }
  else
  {
    *pa = b;
    *pb = a;
  }
}

GCC does not emit CMOVcc (conditional move) on GCC 8.2 if we compile it as C++ code

https://godbolt.org/z/ytE0Ix

sort2_ternary(int, int, int*, int*):
        cmp     edi, esi
        jge     .L2
        mov     eax, edi
        mov     edi, esi
        mov     esi, eax
.L2:
        mov     DWORD PTR [rdx], esi
        mov     DWORD PTR [rcx], edi
        ret
sort2_if(int, int, int*, int*):
        cmp     edi, esi
        jge     .L5
        mov     DWORD PTR [rdx], edi
        mov     DWORD PTR [rcx], esi
        ret
.L5:
        mov     DWORD PTR [rdx], esi
        mov     DWORD PTR [rcx], edi
        ret

but if compile it as C code, sort2_ternary have MOVcc:

sort2_ternary:
        cmp     esi, edi
        mov     eax, edi
        cmovle  eax, esi
        cmovl   esi, edi
        mov     DWORD PTR [rdx], eax
        mov     DWORD PTR [rcx], esi
        ret

but sort2_if remains same.


On GCC trunk there is no difference between compiling this as C or C++ for sort2_ternary(), but sort2_if() still doesn't have CMOVcc instruction in it for both case.


Measuring cmov vs branch-mov performance: https://github.com/xiadz/cmov


---


### compiler : `gcc`
### title : `missed vectorisation with "#pragma omp simd collapse(2)"`
### open_at : `2019-02-15T19:00:17Z`
### last_modified_date : `2021-08-10T23:10:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89371
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
void ff(double* res, double const* a, double const* b, int ncell, int neq)
{
#pragma omp simd collapse(2)
  for(int icell=0; icell < ncell; ++icell)
  {
      for(int ieq=0; ieq<neq; ++ieq)
      {
          res[icell*neq+ieq] = a[icell*neq+ieq]-b[icell*neq+ieq];
      }
  }
}
built by gcc 8.2 on x86_64 with "-std=c++14 -O3 -mavx -fopenmp-simd" results in simd instruction emitted. Run time tests with ncell=100'000 and neq=3 for instance confirm that the code is slower with "#pragma omp simd collapse(2)".

Am I missing something?

Ideally, I would like to be able to flatten the loop:
void ff(double* res, double const* a, double const* b, int ncell, int neq)
{
  for(int j=0; j < ncell*neq; ++j)
    res[j] = a[j]-b[j];
}


---


### compiler : `gcc`
### title : `A missing ifcvt optimization to generate csel`
### open_at : `2019-02-21T08:28:02Z`
### last_modified_date : `2022-11-28T22:18:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89430
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
For a small case,

unsigned *a;
void test(unsigned k, unsigned b) {
        if (b < a[k]) {
                a[k] = b;
        }
}

"gcc -O3 -S" generates,

        adrp    x2, a
        uxtw    x0, w0
        ldr     x2, [x2, #:lo12:a]
        ldr     w3, [x2, x0, lsl 2]
        cmp     w3, w1
        bls     .L1
        str     w1, [x2, x0, lsl 2]

Actually we should use csel instruction instead of conditional branch, so expect to have the followings generated,

        adrp    x2, a
        uxtw    x0, w0
        ldr     x2, [x2, #:lo12:a]
        ldr     w3, [x2, x0, lsl 2]
        cmp     w3, w1
        csel    w1, w1, w3, hi
        str     w1, [x2, x0, lsl 2]

RTL optimization ifcvt misses this opportunity.


---


### compiler : `gcc`
### title : `adding aligned attribute to struct causes too much to be copied`
### open_at : `2019-02-22T20:57:54Z`
### last_modified_date : `2019-02-27T01:49:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89458
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `8.2.0`
### severity : `normal`
### contents :
If I add __attribute__ ((aligned (256))) to a small struct, the whole 256 bytes are copied by the default copy constructor. I don't think it's correct. This is with -O3 optimization. Attached is a piece of code to demo:

struct Obj {
void clone(Obj& r);
void clone2(Obj& r);
        int a;
        int b;
        int c; int c1; int c2; int c3; long c4; long c5; long c6;
}__attribute__ ((aligned (256)));

void Obj::clone(Obj& r) {
  *this = r;
}

built like so: 
g++ -O3 -std=c++14 -march=skylake 

The compilation result is a giant "clone" function that copies 256 bytes of memory


---


### compiler : `gcc`
### title : `Teach ccp about __builtin_bswap{16,32,64}`
### open_at : `2019-02-23T17:57:22Z`
### last_modified_date : `2019-04-30T12:12:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89475
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
When looking at PR89435, I've noticed ccp should be able, but doesn't, optimize __builtin_bswap{16,32,64}.

Those builtins should preserve both the mask and INTEGER_CST from the argument, just bswapped.

Short testcase:

void link_error (void);

unsigned short
f0 (unsigned short x)
{
  x &= 0xaa55;
  x = __builtin_bswap16 (x);
  if (x & 0xaa55)
    link_error ();
  return x;
}

unsigned short
f1 (unsigned short x)
{
  x &= 0x55aa;
  x = __builtin_bswap16 (x);
  if (x & 0x55aa)
    link_error ();
  return x;
}

unsigned int
f2 (unsigned int x)
{
  x &= 0x55aa5aa5U;
  x = __builtin_bswap32 (x);
  if (x & 0x5aa555aaU)
    link_error ();
  return x;
}

unsigned long long int
f3 (unsigned long long int x)
{
  x &= 0x55aa5aa544cc2211ULL;
  x = __builtin_bswap64 (x);
  if (x & 0xeedd33bb5aa555aaULL)
    link_error ();
  return x;
}

unsigned short
f4 (unsigned short x)
{
  x = __builtin_bswap32 (x);
  if (x != 0)
    link_error ();
  return x;
}

unsigned int
f5 (unsigned int x)
{
  x = __builtin_bswap64 (x);
  if (x != 0)
    link_error ();
  return x;
}

unsigned short
f6 (unsigned short x)
{
  x |= 0xaa55;
  x = __builtin_bswap16 (x);
  if ((x | 0xaa55) != 0xffff)
    link_error ();
  return x;
}

unsigned short
f7 (unsigned short x)
{
  x |= 0x55aa;
  x = __builtin_bswap16 (x);
  if ((x | 0x55aa) != 0xffff)
    link_error ();
  return x;
}

unsigned int
f8 (unsigned int x)
{
  x |= 0x55aa5aa5U;
  x = __builtin_bswap32 (x);
  if ((x | 0x5aa555aaU) != 0xffffffffU)
    link_error ();
  return x;
}

unsigned long long int
f9 (unsigned long long int x)
{
  x |= 0x55aa5aa544cc2211ULL;
  x = __builtin_bswap64 (x);
  if ((x | 0xeedd33bb5aa555aaULL) != 0xffffffffffffffffULL)
    link_error ();
  return x;
}

Perhaps somewhat related to PR55177.


---


### compiler : `gcc`
### title : `missed optimization for lambda expression when variable is uninitialized when declared`
### open_at : `2019-02-23T21:13:41Z`
### last_modified_date : `2022-03-11T00:32:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89478
### status : `UNCONFIRMED`
### tags : `c++-lambda, missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
testcase:

constexpr void mul2(int &a, const int b)
{
  a = b * 2;
}

int test = []() {int a; mul2(a, 123); return a;}(); 

int test2 = []() {int a = 0; mul2(a, 123); return a;}();


output for GCC trunk with -O3 -std=c++17
_GLOBAL__sub_I_test:
        movl    $246, test(%rip)
        ret
test2:
        .long   246
test:
        .zero   4


simple testcase:

int test3 = []() {int a; a = 5; return a;}();

_GLOBAL__sub_I_test:
        movl    $5, test(%rip)
        ret
test:
        .zero   4


---


### compiler : `gcc`
### title : `__restrict on a pointer ignored when disambiguating against a call`
### open_at : `2019-02-23T22:39:19Z`
### last_modified_date : `2023-07-07T17:49:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89479
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `normal`
### contents :
(This is all illustrated at: https://godbolt.org/z/nz2YXE )

Let us make our language C++17. Consider the following function:

  int foo(const int* x, void g())
  {
      int result = *x;
      g();
      result += *x;
      return result;
  }

since we have no aliasing guarantees, we must assume the invocation of g() may change the value at address x, so we must perform two reads from x to compute the result - one before the call and one after.

If, however, we add __restrict__ specifier to x:

  int bar(const int* __restrict__ x, void g())
  {
      int result = *x;
      g();
      result += *x;
      return result;
  }

we may assume x "points to an unaliased integer" (as per https://gcc.gnu.org/onlinedocs/gcc/Restricted-Pointers.html ). That means we can read from address x just once, and double the value to get our result. I realize there's a subtle point here, which is whether being "unaliased" also applies to g()'s behavior. It is my understanding that it does.

Well, clang 7.0 understands things they way I do, and indeed optimizes one of the reads away in `bar()`. But - g++ 8.3 (and g++ "trunk", whatever that means on GodBolt) doesn't do so, and reads _twice_ from x both in `foo()` and in `bar()`.


---


### compiler : `gcc`
### title : `Inline jump tables`
### open_at : `2019-02-25T09:03:51Z`
### last_modified_date : `2021-07-19T04:33:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89491
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
int square(int x) {
    return x*x;
}

int add(int x) {
    return x + x;
}

typedef int (*p) (int);

p arr[4] = {square, add};

int test(int x) {
    int res = arr[1](x);
    return res;
}

Expected:
test(int):
        lea     eax, [rdi+rdi]
        ret

Currently:
test(int):
        jmp     [QWORD PTR arr[rip+8]]


------------
If the index to the jump table isn't a constant, should not be better to expand
"if else" chain like:
if (x==1)  arr[1](x);
else if (x==2)  arr[2](x);

..and then take an opportunity to inline functions?


---


### compiler : `gcc`
### title : `restrict doesnt work with subfield accesses`
### open_at : `2019-02-26T14:13:04Z`
### last_modified_date : `2019-05-02T14:09:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89509
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
struct S { int i; void *p; int j; };
int
foo (struct S * __restrict p, int *q, int flag)
{
  int *x = &p->j;
  if (flag)
    x = &p->i;
  *q = 1;
  *x = 2;
  return *q;
}

Is not optimized to return 1.  This is the missed-optimization part of
PR89505.


---


### compiler : `gcc`
### title : `missed optimisation for array address calculations`
### open_at : `2019-02-27T11:41:08Z`
### last_modified_date : `2021-08-08T17:42:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89518
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `normal`
### contents :
Considering:

int f(int index, int stride) {
    const int i1 = index / stride;
    const int i2 = index % stride;
    const int index2 = i1*stride+i2;
    return index2; // expect: index2 = index
}

gcc 8.3 with "-O3" on x84_64 emits:
f(int, int):
        mov     eax, edi
        cdq
        idiv    esi
        imul    eax, esi
        add     eax, edx
        ret

By contrast, clang 7 with "-O3" emits
f(int, int):
        mov     eax, edi
        ret

MSVC 2017 with "/O2" emits:
int f(int,int)
        mov     eax, ecx
        ret     0

Is there a way to persuade gcc to simplify this expression at compile time?


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] 4*movq to 2*movaps IPC performance regression on znver1 with -Og`
### open_at : `2019-03-02T14:36:40Z`
### last_modified_date : `2023-07-07T10:34:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89557
### status : `WAITING`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
Approximate C++ source code:

  struct __attribute__((aligned(16))) A {
    union {
      struct {
        uint64_t a;
        double b;
      };
      uint64_t data[2];
    };
  };

  A a;
  a.a = 2;
  a.b = x*y;
  return a;

CPU: AMD Ryzen 5 1600 Six-Core Processor

GCC 7.4.0 generates (no -march/mtune):

  movq $2, 0x80(%rsp)
  movsd %xmm0, 0x88(%rsp)
  mov 0x80(%rsp), %rax
  mov 0x88(%rsp), %rdx
  mov %rax, 0x30(%rsp)
  mov %rdx, 0x38(%rsp)

GCC 7.4.0 generates (no -march, -mtune=native):

  movq $2, 0x80(%rsp)
  movsd %xmm0, 0x88(%rsp)
  movaps 0x80(%rsp), %xmm6
  movaps %xmm6, 0x30(%rsp)

GCC 8.2.0 generates (no -march/mtune):

  movq $2, 0x80(%rsp)
  movsd %xmm0, 0x88(%rsp)
  movdqa 0x80(%rsp), %xmm6
  movaps %xmm6, 0x30(%rsp)

GCC 8.2.0 generates (no -march, -mtune=native):

  movq $2, 0x80(%rsp)
  movsd %xmm0, 0x88(%rsp)
  movaps 0x80(%rsp), %xmm6
  movaps %xmm6, 0x30(%rsp)

IPC of an executable which uses the above code (perf stat):

  GCC 7.4.0 (no -march/mtune):
        617.233116      task-clock (msec)         #    0.997 CPUs utilized 
     4,139,124,553      instructions              #    1.94  insn per cycle

  GCC 7.4.0 (no -march, -mtune=native):
       1106.252920      task-clock (msec)         #    1.000 CPUs utilized          
     3,995,268,509      instructions              #    1.02  insn per cycle

  GCC 8.2.0 (no -march/mtune):
       1096.852485      task-clock (msec)         #    1.000 CPUs utilized
     3,790,839,401      instructions              #    0.97  insn per cycle

  GCC 8.2.0 (no -march, -mtune=native):
       1105.693441      task-clock (msec)         #    1.000 CPUs utilized     
     4,041,957,928      instructions              #    1.04  insn per cycle

Summary: Using 2*movaps instead of 4*movq severely lowers IPC on znver1 CPUs


---


### compiler : `gcc`
### title : `Unneeded stack alignment on windows x86`
### open_at : `2019-03-04T15:58:53Z`
### last_modified_date : `2020-12-16T23:26:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89581
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
On windows, when compiling the following code with ` gcc -mavx2 a.c -o - -S -O3 -g0 -fno-asynchronous-unwind-tables -fomit-frame-pointer -Wall -Wextra`

```
typedef struct {
    double x1;
    double x2;
} vdouble __attribute__((aligned(16)));

vdouble f(vdouble x, vdouble y)
{
    return (vdouble){x.x1 + y.x1, x.x2 + y.x2};
}
```

I got

```
        pushq   %rbp
        vmovdqa (%r8), %xmm0
        movq    %rcx, %rax
        vaddpd  (%rdx), %xmm0, %xmm0
        movq    %rsp, %rbp
        andq    $-16, %rsp
        vmovaps %xmm0, (%rcx)
        leave
        ret
```

which include 4 extra instructions to align the stack without actually using it....

FWIW, clang has a similar problem on linux...
https://bugs.llvm.org/show_bug.cgi?id=40844

Also worth noting that with -O2 all three vector instructions are splitted into scalar ones whereas clang does this transformation at -O2...


---


### compiler : `gcc`
### title : `Suboptimal code generated for floating point struct in -O2 compare to -O1`
### open_at : `2019-03-04T16:10:27Z`
### last_modified_date : `2023-08-26T23:37:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89582
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `normal`
### contents :
When testing the code for https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89581 on linux, I noticed that the code seems suboptimum when compiled under -O3 rather than -O2 on linux x64.

```
typedef struct {
    double x1;
    double x2;
} vdouble __attribute__((aligned(16)));

vdouble f(vdouble x, vdouble y)
{
    return (vdouble){x.x1 + y.x1, x.x2 + y.x2};
}
```

Compiled with `-O2` produces
```
f:
        addsd   %xmm3, %xmm1
        addsd   %xmm2, %xmm0
        ret
```

With `-O3` or `-Ofast`, however, the code produced is,

```
f:
        movq    %xmm0, -40(%rsp)
        movq    %xmm1, -32(%rsp)
        movapd  -40(%rsp), %xmm4
        movq    %xmm2, -24(%rsp)
        movq    %xmm3, -16(%rsp)
        addpd   -24(%rsp), %xmm4
        movaps  %xmm4, -40(%rsp)
        movsd   -32(%rsp), %xmm1
        movsd   -40(%rsp), %xmm0
        ret
```

It seems that gcc tries to use the vector instruction but had to use the stack for that. I did a quick benchmark which confirms that the -O3 version is much slower than the -O2 version.

Clang produces

```
f:
        addsd   %xmm2, %xmm0
        addsd   %xmm3, %xmm1
        retq
```

As long as any optimizations are on, which seems appropriate.


---


### compiler : `gcc`
### title : `Extra mov after structure load instructions on aarch64`
### open_at : `2019-03-06T13:56:42Z`
### last_modified_date : `2023-04-03T13:59:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89606
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `8.3.0`
### severity : `enhancement`
### contents :
Code to reproduce,

```
#include <arm_neon.h>

#ifdef __aarch64__
float64x2x2_t f(const double *p1, const double *p2)
{
    float64x2x2_t v = vld2q_f64(p1);
    return vld2q_lane_f64(p2, v, 1);
}

float32x2x2_t f2(const float *p1, const float *p2)
{
    float32x2x2_t v = vld2_f32(p1);
    return vld2_lane_f32(p2, v, 1);
}
#endif

void f3(float32x2x2_t *p, const float *p1, const float *p2)
{
    float32x2x2_t v = vld2_f32(p1);
    *p = vld2_lane_f32(p2, v, 1);
}
```

GCC produces (aarch64, -O1/-O2/-O3/-Ofast/-Os),

```
f:
        ld2     {v4.2d - v5.2d}, [x0]
        mov     v0.16b, v4.16b
        mov     v1.16b, v5.16b
        ld2     {v0.d - v1.d}[1], [x1]
        ret
f2:
        ld2     {v0.2s - v1.2s}, [x0]
        mov     v2.8b, v0.8b
        mov     v3.8b, v1.8b
        ld2     {v2.s - v3.s}[1], [x1]
        mov     v1.8b, v3.8b
        mov     v0.8b, v2.8b
        ret
f3:
        ld2     {v2.2s - v3.2s}, [x1]
        mov     v0.8b, v2.8b
        mov     v1.8b, v3.8b
        ld2     {v0.s - v1.s}[1], [x2]
        stp     d0, d1, [x0]
        ret
```

For all three functions, none of the mov's seems necessary. Even if there's some performance issue when reusing the registers (I highly doubt it...) at least the `-Os` version should not have those mov's.

Clang produces what I expect in this case,

```
f:
        ld2     { v0.2d, v1.2d }, [x0]
        ld2     { v0.d, v1.d }[1], [x1]
        ret
f2:
        ld2     { v0.2s, v1.2s }, [x0]
        ld2     { v0.s, v1.s }[1], [x1]
        ret
f3:
        ld2     { v0.2s, v1.2s }, [x1]
        ld2     { v0.s, v1.s }[1], [x2]
        stp     d0, d1, [x0]
        ret
```

Aarch32 doesn't have this issue either with GCC,

```
f3:
        vld2.32 {d16-d17}, [r1]
        vld2.32 {d16[1], d17[1]}, [r2]
        vst1.64 {d16-d17}, [r0:64]
        bx      lr
```

so this seems to be aarch64 specific.


---


### compiler : `gcc`
### title : `Missing optimization for store of multiple registers on aarch64`
### open_at : `2019-03-06T14:28:01Z`
### last_modified_date : `2019-03-19T14:04:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89607
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
Test code, Compiled for arm/aarch64 with -O1/-O2/-O3/-Os/-Ofast

```
#include <arm_neon.h>

void f4(float32x4x2_t *p, const float *p1)
{
    *p = vld2q_f32(p1);
}

void f5(float32x4x2_t *p, float32x4_t v1, float32x4_t v2)
{
    p->val[0] = v1;
    p->val[1] = v2;
}
```

arm:

```
f4:
        vld2.32 {d16-d19}, [r1]
        vst1.64 {d16-d19}, [r0:64]
        bx      lr
f5:
        vst1.64 {d0-d1}, [r0:64]
        vstr    d2, [r0, #16]
        vstr    d3, [r0, #24]
        bx      lr
```

aarch64:

```
f4:
        ld2     {v0.4s - v1.4s}, [x1]
        str     q0, [x0]
        str     q1, [x0, 16]
        ret
f5:
        str     q0, [x0]
        str     q1, [x0, 16]
        ret
```

For arm, it seems that f5 could follow f4 and uses a `vst1.64 {d0-d3}, [r0:64]` instead. For aarch64, both function should have used a `stp q0, q1, [x0]`

Clang produces what I expected on aarch64 but it only uses pair store instruction on arm, which use one more instuction for `f4` and one fewer for `f5`. (I'm not sure why GCC decided to use a pair store and then two single stores....)

Similar to pr89606, this optimization should at least happen with `-Os` if not for all other optimization levels.

Tested with 8.2.1 on arm and 8.3.0 on aarch64.


---


### compiler : `gcc`
### title : `Missing optimization for store of multiple registers on arm`
### open_at : `2019-03-06T20:01:25Z`
### last_modified_date : `2019-03-07T09:20:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89614
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
Separated from pr89607 as requested. Test code and result compiled with any non-zero optimization levels,

```
#include <arm_neon.h>

void f4(float32x4x2_t *p, const float *p1)
{
    *p = vld2q_f32(p1);
}

void f5(float32x4x2_t *p, float32x4_t v1, float32x4_t v2)
{
    p->val[0] = v1;
    p->val[1] = v2;
}
```

```
f4:
        vld2.32 {d16-d19}, [r1]
        vst1.64 {d16-d19}, [r0:64]
        bx      lr
f5:
        vst1.64 {d0-d1}, [r0:64]
        vstr    d2, [r0, #16]
        vstr    d3, [r0, #24]
        bx      lr
```

I believe `f5` should use a single `vst1.64 {d0-d3}, [r0:64]` just like `f4`.

If for some reason doing that is bad for performance (doubt it...) it should at least be used for -Os.


---


### compiler : `gcc`
### title : `memmove used even after runtime guard against overlap`
### open_at : `2019-03-07T06:22:11Z`
### last_modified_date : `2019-03-07T09:19:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89617
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `enhancement`
### contents :
For the following example, GCC 8.3 uses memmove unless compiling with -DRESTRICT.  With -DRESTRICT it optimizes the copy to just 4 mov instructions.  The copy code is only invoked after checking that the addresses differ, so moving the call to std::copy out of line just to decorate it with __restrict__ should not be necessary to achieve optimal code generation, given strict aliasing.

    #include <algorithm>
    #include <iterator>

    typedef int Arr[3];

    inline void Copy(Arr& __restrict__ dest, const Arr& __restrict__ src)
    {
        std::copy(std::begin(src), std::end(src), dest);
    }

    void f2(Arr& dest, const Arr& src)
    {
        if (&dest != &src) // make sure there is no aliasing
        {
    #ifdef RESTRICT
            Copy(dest, src); // generates better code, why?
    #else
            std::copy(std::begin(src), std::end(src), dest);
    #endif
        }
    }

Demo: https://godbolt.org/z/mT18Hb

Discussion: https://stackoverflow.com/questions/55015151/can-arrays-overlap-alias-or-is-gcc-being-overly-cautious


---


### compiler : `gcc`
### title : `Inner loop won't vectorize unless dummy statement is included`
### open_at : `2019-03-07T10:15:18Z`
### last_modified_date : `2019-03-11T09:05:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89618
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
We have a loop in which we are scattering data to an array of length "n" where can assure no write conflicts only within confined ranges of length "m". Our implementation includes splitting this loop into an outer and an inner loop and specifying "#pragma GCC ivdep" for the inner loop:


https://godbolt.org/z/ulnRrk
=======================================================
const int m = 32;

for (int j = 0; j < n/m; ++j)
{
  int const start = j*m;
  int const end = (j+1)*m;

  #pragma GCC ivdep
  for (int i = start; i < end; ++i)
  {
    a[off[i]] = a[i] < 0 ? a[i] : 0;
  }

#ifdef VECTORIZE
  // dummy statement required for vectorization
  if (a[0] == 0.) a[0] = 0.; 
#endif
}
=======================================================

The issue is that GCC (trunk and any earlier version) won't vectorize the code unless we add the obviously useless dummy statement (guarded by "#ifdef VECTORIZE"). This is counterintuitive, involves some overhead which we want to avoid, and may be cumbersome or even impossible to implemented depending on the specific structure of the inner loop (the body may be passed as a lambda, etc.).

Without knowing about the internals of GCC, I can imagine that in the absence of the dummy statment, GCC jams the loops and tries and fails to vectorize the remaining (outer) loop because it doesn't have an "ivdep" pragma. Can we avoid this behavior? If my thinking is correct, something like ICC's "#pragma nounroll_and_jam" could work, but GCC doesn't (officially?) support anything like it as far as I can see. 

Thanks,
Moritz


---


### compiler : `gcc`
### title : `Freeing memory under the lock in __deregister_frame_info_bases`
### open_at : `2019-03-07T15:47:49Z`
### last_modified_date : `2021-09-10T00:37:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89625
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libgcc`
### version : `9.0`
### severity : `normal`
### contents :
`__deregister_frame_info_bases` in file unwind-dw2-fde.c calls `free (ob->u.sort);` under the locked `object_mutex`. This can be avoided by remembering the pointer to free and freeing it outside the critical section.

This has been fixed in upstream glibc https://github.com/bminor/glibc/commit/2604882cefd3281679b8177245fdebc7061b8695#diff-17235859a5d2697ce97070a69ab9a602


---


### compiler : `gcc`
### title : `aarch64_vector_pcs does not use v24-v31 as temp regs`
### open_at : `2019-03-07T18:22:46Z`
### last_modified_date : `2019-03-08T16:58:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89628
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
consider

typedef __Float32x4_t vec;

__attribute__((aarch64_vector_pcs))
vec f(vec a0, vec a1, vec a2, vec a3, vec a4, vec a5, vec a6, vec a7)
{
	vec t0, t1, t2, t3, t4, t5, t6, t7, s0, s1, s2, s3;
	t0 = a0 - a7;
	t1 = a1 - a6;
	t2 = a2 - a5;
	t3 = a3 - a4;
	t4 = a4 - a3;
	t5 = a5 - a2;
	t6 = a6 - a1;
	t7 = a7 - a0;
	s0 = t0 * t1;
	s1 = t2 * t3;
	s2 = t4 * t5;
	s3 = t6 * t7;
	return s0 * s1 * s2 * s3 * a0 * a1 * a2 * a3 * a4 * a5 * a6 * a7;
}

the aarch64 vpcs has 8 arg + 8 temp regs to use, so i think such code should not need to spill, however current gcc seems to compile it as

f:
	stp	q16, q17, [sp, -96]!
	fsub	v16.4s, v2.4s, v5.4s
	stp	q18, q19, [sp, 32]
	fsub	v17.4s, v0.4s, v7.4s
	stp	q20, q21, [sp, 64]
	fsub	v18.4s, v1.4s, v6.4s
	fsub	v20.4s, v3.4s, v4.4s
	fsub	v21.4s, v5.4s, v2.4s
	fsub	v19.4s, v4.4s, v3.4s
	fmul	v17.4s, v17.4s, v18.4s
	fmul	v16.4s, v16.4s, v20.4s
	fsub	v18.4s, v6.4s, v1.4s
	fsub	v20.4s, v7.4s, v0.4s
	fmul	v19.4s, v19.4s, v21.4s
	fmul	v16.4s, v17.4s, v16.4s
	fmul	v17.4s, v18.4s, v20.4s
	ldp	q20, q21, [sp, 64]
	fmul	v16.4s, v16.4s, v19.4s
	ldp	q18, q19, [sp, 32]
	fmul	v16.4s, v16.4s, v17.4s
	fmul	v16.4s, v16.4s, v0.4s
	fmul	v1.4s, v16.4s, v1.4s
	ldp	q16, q17, [sp], 96
	fmul	v2.4s, v1.4s, v2.4s
	fmul	v3.4s, v2.4s, v3.4s
	fmul	v4.4s, v3.4s, v4.4s
	fmul	v5.4s, v4.4s, v5.4s
	fmul	v6.4s, v5.4s, v6.4s
	fmul	v0.4s, v6.4s, v7.4s
	ret

note that v24..v31 regs are not used but there are 6 spills.


---


### compiler : `gcc`
### title : `Missing vectorization of loop containing std::min/std::max and temporary`
### open_at : `2019-03-11T10:03:01Z`
### last_modified_date : `2023-07-05T20:29:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89653
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Godbolt worksheet: https://godbolt.org/z/F6m5hl

GCC (trunk and all earlier versions) fails to vectorize (SSE/AVX2/AVX-512) the following loop because of a "complicated access pattern" (similarly for std::max()):

== loop1 - FAIL ====================================
for (int i = 0; i < end; ++i)
{
  vec[i] = std::min(vec[i], vec[i]/x);
}
====================================================

If we don't use std::min(), but implement the same loop using a ternary operator, the vectorization is successful:

== loop2 - OK ======================================
for (int i = 0; i < end; ++i)
{
  vec[i] = vec[i] < vec[i]/x ? vec[i] : vec[i]/x;
}
====================================================

However, the problem does not seem to be that GCC is unable to vectorize std::min() itself, because the following loop _does_ get vectorized (note the different logic and the absence of an implicit temporary for vec[i]/x):

== loop3 - OK ======================================
for (int i = 0; i < end; ++i)
{
  vec[i] = std::min(vec[i], x);
}
====================================================

The C++ standard prescribes that std::min() returns the result as a const reference, so an implementation might look like this:

== std::min() ======================================
double const & min(double const &a, double const &b)
{
    if (a<b) return a;
    return b;
}
====================================================

Implementing our own min() method along the same lines, but returning the result per value, also enables vectorization of the original loop (see loop4 @ godbolt). 


All in all, it seems like mixing the return-by-reference with the implicitly created temporary for the second argument in loop1 is the problem here. So, it might not be an issue in GCC's vectorizer component, but rather in the access analysis. While I can imagine that the present semantics (mixture of temporary values and references) are not trivial for access analysis, I have some doubts as to whether it's impossible to safely vectorize loop1. However, it may well be the case that I'm missing something obvious here, so any help is greatly appreciated.


Moritz


---


### compiler : `gcc`
### title : `[8 Regression] Redundant moves for long long shift on 32bit x86`
### open_at : `2019-03-12T09:15:52Z`
### last_modified_date : `2021-05-14T11:38:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89676
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
unsigned long long
foo (unsigned long long i)
{
  return i << 3;
}
--cut here--

compiles for 32bit x86 to (-O2):

        pushl   %ebx
        movl    8(%esp), %ecx
        movl    12(%esp), %ebx
        movl    %ecx, %eax
        movl    %ebx, %edx
        popl    %ebx
        shldl   $3, %eax, %edx
        sall    $3, %eax
        ret

There is unnecessary move to %ecx/%ebx pair in the above code.

Clang creates:

        movl    4(%esp), %eax
        movl    8(%esp), %edx
        shldl   $3, %eax, %edx
        shll    $3, %eax
        retl


---


### compiler : `gcc`
### title : `Redundant moves with -march=skylake for long long shift on 32bit x86`
### open_at : `2019-03-12T13:07:44Z`
### last_modified_date : `2023-05-20T01:40:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89680
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
unsigned long long
foo (unsigned long long i)
{
  return i << 3;
}
--cut here--

compiles with -O2 -march=skylake -m32 to:

        subl    $28, %esp
        movl    32(%esp), %eax
        movl    36(%esp), %edx
        movl    %eax, (%esp)
        movl    %edx, 4(%esp)
        vmovdqa (%esp), %xmm1
        addl    $28, %esp
        vpsllq  $3, %xmm1, %xmm0
        vmovd   %xmm0, %eax
        vpextrd $1, %xmm0, %edx
        ret

but with -O2 -march=haswell -m32 to:

        vmovq   4(%esp), %xmm0
        vpsllq  $3, %xmm0, %xmm0
        vmovd   %xmm0, %eax
        vpextrd $1, %xmm0, %edx
        ret

The difference starts in IRA pass with:

Pass 0 for finding pseudo/allocno costs

     a0 (r88,l0) best DREG, allocno DREG
     a1 (r87,l0) best AREG, allocno AREG
     a2 (r85,l0) best NO_REX_SSE_REGS, allocno NO_REX_SSE_REGS
-    a3 (r83,l0) best NO_REX_SSE_REGS, allocno NO_REX_SSE_REGS
+    a3 (r83,l0) best NO_REGS, allocno NO_REGS

 Pass 1 for finding pseudo/allocno costs

     r88: preferred DREG, alternative GENERAL_REGS, allocno GENERAL_REGS
     r87: preferred AREG, alternative GENERAL_REGS, allocno GENERAL_REGS
     r85: preferred NO_REX_SSE_REGS, alternative NO_REGS, allocno NO_REX_SSE_REGS
-    r83: preferred NO_REX_SSE_REGS, alternative NO_REGS, allocno NO_REX_SSE_REGS
+    r83: preferred NO_REGS, alternative NO_REGS, allocno NO_REGS

and going downhill from there.


---


### compiler : `gcc`
### title : `[9 Regression] -Wstringop-overflow confused by const 2D array of char`
### open_at : `2019-03-12T19:04:09Z`
### last_modified_date : `2021-07-22T20:43:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89688
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Also, for some reason it repeats the warning 3 times.

https://godbolt.org/z/SStUsl

// Fine
extern const char a[2] = {'1'};
auto z = __builtin_strlen(a);

// Warns
extern const char aa[1][2] = {{'2'}};
auto zz = __builtin_strlen(aa[0]);



> ~/opensource/gcc/prefix/bin/g++ -Wall str.cpp  -fsyntax-only
str.cpp:7:27: warning: ‘strlen’ argument missing terminating nul [-Wstringop-overflow=]
    7 | auto zz = __builtin_strlen(aa[0]);
      |           ~~~~~~~~~~~~~~~~^~~~~~~
str.cpp:6:19: note: referenced argument declared here
    6 | extern const char aa[1][2] = {{'2'}};
      |                   ^~
str.cpp:7:27: warning: ‘strlen’ argument missing terminating nul [-Wstringop-overflow=]
    7 | auto zz = __builtin_strlen(aa[0]);
      |           ~~~~~~~~~~~~~~~~^~~~~~~
str.cpp:6:19: note: referenced argument declared here
    6 | extern const char aa[1][2] = {{'2'}};
      |                   ^~
str.cpp:7:32: warning: ‘strlen’ argument missing terminating nul [-Wstringop-overflow=]
    7 | auto zz = __builtin_strlen(aa[0]);
      |                            ~~~~^
str.cpp:6:19: note: referenced argument declared here
    6 | extern const char aa[1][2] = {{'2'}};
      |                   ^~


Reduced example from real code at: https://github.com/boostorg/date_time/blob/b0437e2999a65668dc4178dbb817a89a382ece94/include/boost/date_time/special_values_formatter.hpp#L89-L92 + https://github.com/boostorg/date_time/blob/b0437e2999a65668dc4178dbb817a89a382ece94/include/boost/date_time/special_values_formatter.hpp#L43-L45


---


### compiler : `gcc`
### title : `unexpected copying of trivially copyable prvalue arguments`
### open_at : `2019-03-13T04:50:56Z`
### last_modified_date : `2021-10-28T14:18:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89695
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.3.0`
### severity : `normal`
### contents :
When a function that has a non-reference parameter of a trivially copyable class-type is invoked with a prvalue expression as argument, e.g.:

    #include <stdio.h>

    struct Foo {
        int data[32];
        Foo() { printf( "%p (constructor)\n", this ); }
    };

    Foo make_foo() {  return Foo{};  }

    void f( Foo x ) {  printf( "%p (parameter)\n", &x );  }

    int main() {
        f( Foo{} );
        f( make_foo() );
        return 0;
    }

g++ initializes a temporary from this expression and then copy-constructs the parameter from it:

    0x7fffce1caa80 (constructor)
    0x7fffce1cab00 (parameter)
    0x7fffce1caa80 (constructor)
    0x7fffce1cab00 (parameter)

This appears to happen whenever C++17 permits it and at any optimization level, even though Foo is large and expensive to copy. For example on armhf at -O2, g++ produces this particularly silly-looking output:

        mov     r1, r5
        mov     r0, r7
        bl      printf(PLT)
        mov     r2, r8
        mov     r1, r5
        mov     r0, r4
        bl      memcpy(PLT)
        mov     r1, r4
        mov     r0, r6
        bl      printf(PLT)
        mov     r1, r5
        mov     r0, r7
        bl      printf(PLT)
        mov     r2, r8
        mov     r1, r5
        mov     r0, r4
        bl      memcpy(PLT)
        mov     r1, r4
        mov     r0, r6
        bl      printf(PLT)

If a user-provided destructor, copy-constructor, or move-constructor is added to the class, no copying is done even at -O0 (as expected due to C++17 requirements):

        mov     r3, r7
        mov     r0, r3
        bl      _ZN3FooC1Ev(PLT)
        mov     r3, r7
        mov     r0, r3
        bl      _Z1f3Foo(PLT)
        add     r3, r7, #128
        mov     r0, r3
        bl      _Z8make_foov(PLT)
        add     r3, r7, #128
        mov     r0, r3
        bl      _Z1f3Foo(PLT)


---


### compiler : `gcc`
### title : `Optimize away an empty loop whose finiteness can not be analytically determined`
### open_at : `2019-03-14T07:07:28Z`
### last_modified_date : `2023-07-02T16:42:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89713
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
C++ stl container-based loop is very common, but its low representation looks like irregular, not standard form of [index, step], there is no way to estimate iteration, or even check whether it is finite or infinite only via static analysis. Current gcc can not optimize away an empty stl loop as the following (case in bug 89134,comment 6), although we know a vector/map must be a finite set.

  void f (std::map<int, int> m)
  {
    for (auto it = m.begin (); it != m.end (); ++it);
  }

Gimple dump is:

  <bb 2> [local count: 118111600]:
  # VUSE <.MEM_2(D)>
  _4 = MEM[(struct _Rb_tree_node_base * *)m_3(D) + 24B];
  _11 = &MEM[(struct _Rb_tree *)m_3(D)]._M_impl.D.34825._M_header;
  if (_4 != _11)
    goto <bb 3>; [89.00%]
  else
    goto <bb 4>; [11.00%]

  <bb 3> [local count: 955630224]:
  # it_14 = PHI <_4(2), _7(3)>
  # VUSE <.MEM_2(D)>
  _7 = std::_Rb_tree_increment (it_14);
  if (_7 != _11)
    goto <bb 3>; [89.00%]
  else
    goto <bb 4>; [11.00%]

  <bb 4> [local count: 118111601]:

The iteration variable is re-assigned by std::_Rb_tree_increment(), a pure function, more than a simple add/dec instruction.

As mentioned in bug 82776, comment 8, we might assume it is a finite empty loop covered by implementation suggestion of C++ spec.

"[intro.progress]
The implementation may assume that any thread will eventually do one of the following:
(1.1) — terminate,
(1.2) — make a call to a library I/O function,
(1.3) — perform an access through a volatile glvalue, or
(1.4) — perform a synchronization operation or an atomic operation.
[ Note: This is intended to allow compiler transformations such as removal of empty loops, even when termination cannot be proven. — end note ]"

We may do something on this. A rough thought is to introduce a flag(such as -fno-infinite-loop, or whatever name) to tell gcc characteristic of being compiled source, like -fstrict-aliasing. This flag is disabled by default, enabled explicitly, and could be correlated with -faggressive-loop-optimization. And invoke a dce optimization dedicated to this, late in gcc passes, in hope of not impacting true infinite loop.


---


### compiler : `gcc`
### title : `Bogus maybe-uninitialized warning with -Og (jump threading)`
### open_at : `2019-03-14T21:06:33Z`
### last_modified_date : `2021-11-25T03:32:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89723
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
gcc seems to disable this warning when building without optimization, but it is enabled with -Og:

$ cat test.c
typedef struct node234_Tag node234;

struct node234_Tag {
    node234 *parent;
    node234 *kids[4];
    int counts[4];
};

int add234_internal(node234 *r, void *e, int index) {
    node234 *n;
    int ki;

    if (r == 0) {
        return 0;
    }

    n = r;
    while (n) {
                if (index <= n->counts[0]) {
                    ki = 0;
                } else if (index -= n->counts[0] + 1, index <= n->counts[1]) {
                    ki = 1;
                } else
                    return 0;
        if (!n->kids[ki])
            break;
        n = n->kids[ki];
    }

    return ki;
}
$ gcc -c -O0 -Wall -Werror test.c
$ gcc -c -O1 -Wall -Werror test.c
$ gcc -c -Og -Wall -Werror test.c
test.c: In function ‘add234_internal’:
test.c:11:9: error: ‘ki’ may be used uninitialized in this function [-Werror=maybe-uninitialized]
     int ki;
         ^~
cc1: all warnings being treated as errors
$ 

Some upstream software builds with -Werror, which makes this a real problem when building a whole distribution like Yocto with -Og for debugging purposes.

This can be reproduced on amd64 with all gcc versions supporting -Og from 4.8 to a recent gcc 9 snapshot.


---


### compiler : `gcc`
### title : `powerpc-none-eabi-gcc emits code using stfiwx to misaligned address`
### open_at : `2019-03-17T23:13:41Z`
### last_modified_date : `2019-07-31T19:19:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89746
### status : `RESOLVED`
### tags : `missed-optimization, wrong-code`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
The following code produces an alignment exception on powerpc 750 and I presume on other powerpc ISAs where unaligned floating point is not allowed.


typedef struct
{
	int i;
	short x, y, z;
} foo;

void bar(foo *p, float f)
{
	int d = f;
	p->y = d>>16;
	p->z = d&0xFFFF;
}

file	"stfiwx-test.c"
	.section	".text"
	.align 2
	.globl bar
	.type	bar, @function
bar:
.LFB0:
	.cfi_startproc
	fctiwz %f1,%f1
	addi %r3,%r3,6
	stfiwx %f1,0,%r3
	blr
	.cfi_endproc
.LFE0:
	.size	bar, .-bar

compiling with -mstrict-align generates working code

.file	"stfiwx-test.c"
	.section	".text"
	.align 2
	.globl bar
	.type	bar, @function
bar:
.LFB0:
	.cfi_startproc
	stwu %r1,-16(%r1)
	.cfi_def_cfa_offset 16
	fctiwz %f1,%f1
	addi %r9,%r1,8
	stfiwx %f1,0,%r9
	lwz %r9,8(%r1)
	srawi %r10,%r9,16
	sth %r9,8(%r3)
	sth %r10,6(%r3)
	addi %r1,%r1,16
	.cfi_def_cfa_offset 0
	blr
	.cfi_endproc
.LFE0:
	.size	bar, .-bar


---


### compiler : `gcc`
### title : `Very odd vector constructor`
### open_at : `2019-03-18T02:49:05Z`
### last_modified_date : `2021-08-14T23:28:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89749
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
[hjl@gnu-cfl-2 pr88828]$ cat i1.i
typedef short __v8hi __attribute__ ((__vector_size__ (16)));
typedef long long int __m128i __attribute__ ((__vector_size__ (16), __may_alias__));

__m128i
foo (__m128i __A)
{
  return (__m128i) (__v8hi) { ((__v8hi)__A)[0], ((__v8hi)__A)[1],
                              ((__v8hi)__A)[2], ((__v8hi)__A)[3],
                              ((__v8hi)__A)[4], ((__v8hi)__A)[5],
                              ((__v8hi)__A)[6], ((__v8hi)__A)[7] };
}
[hjl@gnu-cfl-2 pr88828]$ gcc -S -O2 i2.i
[hjl@gnu-cfl-2 pr88828]$ cat i2.s 
	.file	"i2.i"
	.text
	.p2align 4,,15
	.globl	foo
	.type	foo, @function
foo:
.LFB1:
	.cfi_startproc
	pextrw	$3, %xmm0, %edx
	pextrw	$2, %xmm0, %eax
	pextrw	$6, %xmm0, %ecx
	salq	$16, %rdx
	pextrw	$0, %xmm0, %esi
	orq	%rax, %rdx
	pextrw	$1, %xmm0, %eax
	salq	$16, %rdx
	orq	%rax, %rdx
	pextrw	$7, %xmm0, %eax
	salq	$16, %rax
	salq	$16, %rdx
	orq	%rcx, %rax
	pextrw	$5, %xmm0, %ecx
	orq	%rsi, %rdx
	salq	$16, %rax
	movq	%rdx, -24(%rsp)
	orq	%rcx, %rax
	pextrw	$4, %xmm0, %ecx
	salq	$16, %rax
	orq	%rcx, %rax
	movq	%rax, -16(%rsp)
	movdqa	-24(%rsp), %xmm0
	ret
	.cfi_endproc

I am expecting an empty function body.


---


### compiler : `gcc`
### title : `Inefficient runtime alias check`
### open_at : `2019-03-18T10:04:04Z`
### last_modified_date : `2019-03-18T10:04:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89755
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
For the testcase in PR87561 the runtime alias check ends up too complicated
since it could simply compare a single index that is even loop invariant in
the enclosing loop.  The vectorizer should also consider applying versioning
to outer loops for the runtime alias check.


---


### compiler : `gcc`
### title : `memchr for a character not in constant nul-padded string not folded`
### open_at : `2019-03-20T00:00:42Z`
### last_modified_date : `2020-07-29T19:47:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89772
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Only the first of the two calls to memchr below is folded, the second one isn't.  Looks like it's simply because fold_const_call considers only string lengths (it calls c_getstr()) and doesn't take advantage of the knowledge that all the other characters past the terminating NUL are also NUL.

If fold_const_call either called string_constant instead, or if c_getstr() exposed the size of the constant string in addition to its length, it would be able to fold the second call as well.

$ cat u.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout u.c
const char a[5] = "123";

void f3 (void)
{
  if (!__builtin_memchr (a, '3', sizeof a))   // folded to false
    __builtin_abort ();
}

void f7 (void)
{
  if (__builtin_memchr (a, '7', sizeof a))   // not folded
    __builtin_abort ();
}

;; Function f3 (f3, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=1)

f3 ()
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function f7 (f7, funcdef_no=1, decl_uid=1910, cgraph_uid=2, symbol_order=2)

f7 ()
{
  void * _1;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_memchr (&a, 55, 5);
  if (_1 != 0B)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `Add flag to force single precision`
### open_at : `2019-03-20T13:28:43Z`
### last_modified_date : `2019-04-23T01:04:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89774
### status : `REOPENED`
### tags : `missed-optimization`
### component : `c`
### version : `unknown`
### severity : `normal`
### contents :
It would be helpful if there was a flag (e.g. -fsingle-precision-literals) that would cause gcc to treat floating point literals (e.g. 0.5 or 0.25 ...) in the source code as single precision (float) and not double precision (double).

This could help improve performance of single precision code as right now there are many conversion instructions (see objdump) which use quite a lot of runtime (see perf).


---


### compiler : `gcc`
### title : `optimization opportunity: move variable from stack to register`
### open_at : `2019-03-24T06:36:49Z`
### last_modified_date : `2021-08-16T01:25:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89804
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `enhancement`
### contents :
Description basically is here https://godbolt.org/z/vULPAZ

As I know from https://stackoverflow.com/questions/55314885/compiler-optimization-move-variable-from-stack-to-register a compiler is allowed to optimize a stack variable here.


---


### compiler : `gcc`
### title : `movzwl is not utilized when uint16_t is loaded with bit-shifts (while memcpy does)`
### open_at : `2019-03-24T16:51:17Z`
### last_modified_date : `2021-09-04T20:53:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89809
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
#include <cstdint>
#include <cstring>

// could be just a single movzwl
std::uint16_t foo(unsigned char const* p)
{
    return static_cast<std::uint16_t>(p[0])
        | (static_cast<std::uint16_t>(p[1]) << 8)
        ;
}

// the same, but for uint32_t is recognized
std::uint32_t bar(unsigned char const* p)
{
    return static_cast<std::uint32_t>(p[0])
        | (static_cast<std::uint32_t>(p[1]) << 8)
        | (static_cast<std::uint32_t>(p[2]) << 16)
        | (static_cast<std::uint32_t>(p[3]) << 24)
        ;
}

// memcpy for uint16_t is also good, and generates movzwl
std::uint16_t qaz(unsigned char const* p)
{
    std::uint16_t tmp = 0;
    std::memcpy(&tmp, p, sizeof(tmp));
    return tmp;
}


https://godbolt.org/z/ZQl1tF


movzwl could be also utilized in other byte-size integers load. See the attachment or https://godbolt.org/z/PQwc87


---


### compiler : `gcc`
### title : `Suboptimal codegen: integer load/assemble from in-register array of uint8_t`
### open_at : `2019-03-24T17:45:34Z`
### last_modified_date : `2023-06-23T03:15:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89810
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Clang optimizes it perfectly, while GCC does byte-per-byte assemble (except for 32/64bit cases) https://godbolt.org/z/8b_D9A

#include <cstdint>
#include <array>

// suboptimal
std::uint16_t foo(std::array<std::uint8_t, 2> x)
{
    return static_cast<std::uint16_t>(x[0])
        | (static_cast<std::uint16_t>(x[1]) << 8)
        ;
}

// suboptimal
std::uint32_t foo(std::array<std::uint8_t, 3> x)
{
    return static_cast<std::uint32_t>(x[0])
        | (static_cast<std::uint32_t>(x[1]) << 8)
        | (static_cast<std::uint32_t>(x[2]) << 16)
        ;
}

// good
std::uint32_t foo(std::array<std::uint8_t, 4> x)
{
    return static_cast<std::uint32_t>(x[0])
        | (static_cast<std::uint32_t>(x[1]) << 8)
        | (static_cast<std::uint32_t>(x[2]) << 16)
        | (static_cast<std::uint32_t>(x[3]) << 24)
        ;
}

// suboptimal
std::uint64_t foo(std::array<std::uint8_t, 5> x)
{
    return static_cast<std::uint64_t>(x[0])
        | (static_cast<std::uint64_t>(x[1]) << 8)
        | (static_cast<std::uint64_t>(x[2]) << 16)
        | (static_cast<std::uint64_t>(x[3]) << 24)
        | (static_cast<std::uint64_t>(x[4]) << 32)
        ;
}

// suboptimal
std::uint64_t foo(std::array<std::uint8_t, 6> x)
{
    return static_cast<std::uint64_t>(x[0])
        | (static_cast<std::uint64_t>(x[1]) << 8)
        | (static_cast<std::uint64_t>(x[2]) << 16)
        | (static_cast<std::uint64_t>(x[3]) << 24)
        | (static_cast<std::uint64_t>(x[4]) << 32)
        | (static_cast<std::uint64_t>(x[5]) << 40)
        ;
}

// suboptimal
std::uint64_t foo(std::array<std::uint8_t, 7> x)
{
    return static_cast<std::uint64_t>(x[0])
        | (static_cast<std::uint64_t>(x[1]) << 8)
        | (static_cast<std::uint64_t>(x[2]) << 16)
        | (static_cast<std::uint64_t>(x[3]) << 24)
        | (static_cast<std::uint64_t>(x[4]) << 32)
        | (static_cast<std::uint64_t>(x[5]) << 40)
        | (static_cast<std::uint64_t>(x[6]) << 48)
        ;
}

// good
std::uint64_t foo(std::array<std::uint8_t, 8> x)
{
    return static_cast<std::uint64_t>(x[0])
        | (static_cast<std::uint64_t>(x[1]) << 8)
        | (static_cast<std::uint64_t>(x[2]) << 16)
        | (static_cast<std::uint64_t>(x[3]) << 24)
        | (static_cast<std::uint64_t>(x[4]) << 32)
        | (static_cast<std::uint64_t>(x[5]) << 40)
        | (static_cast<std::uint64_t>(x[6]) << 48)
        | (static_cast<std::uint64_t>(x[7]) << 56)
        ;
}


---


### compiler : `gcc`
### title : `uint32_t load is not recognized if shifts are done in a fixed-size loop`
### open_at : `2019-03-24T18:31:15Z`
### last_modified_date : `2021-09-04T21:33:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89811
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
I was expecting that fixed-size loop will be unrolled and the uint32_t load pattern is recognized, but it does not happen. Clang has no problems with this https://godbolt.org/z/8ES09V

#include <cstdint>
#include <cstring>

// recognized
std::uint32_t good(const unsigned char *p)
{
    std::uint32_t result = 0;
    result |= (static_cast<std::uint32_t>(p[0]) << 0);
    result |= (static_cast<std::uint32_t>(p[1]) << 8);
    result |= (static_cast<std::uint32_t>(p[2]) << 16);
    result |= (static_cast<std::uint32_t>(p[3]) << 24);
    return result;
}

// not recognized if done in a loop
std::uint32_t loop(const unsigned char *p)
{
  std::uint32_t result = 0;
  for (int i = 0; i < 4; ++i)
      result |= (static_cast<std::uint32_t>(p[i]) << (i * 8));

  return result;
}

// other variations are not recognized too
std::uint32_t bad(const unsigned char *p)
{
  std::uint32_t result = 0;
  //result <<= 8;
  result |= static_cast<std::uint32_t>(p[3]);
  result <<= 8;
  result |= static_cast<std::uint32_t>(p[2]);
  result <<= 8;
  result |= static_cast<std::uint32_t>(p[1]);
  result <<= 8;
  result |= static_cast<std::uint32_t>(p[0]);

  return result;
}

std::uint32_t loop2(const unsigned char *p)
{
  std::uint32_t result = 0;
  for (int i = 0; i < 4; ++i) {
      result <<= 8;
      result |= static_cast<std::uint32_t>(p[3 - i]);
  }

  return result;
}


---


### compiler : `gcc`
### title : `[9 Regression] std::variant move construction regressed since GCC 8.3`
### open_at : `2019-03-25T13:41:42Z`
### last_modified_date : `2019-03-27T11:10:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89816
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.0`
### severity : `normal`
### contents :
The following code


#include <variant>

struct my_type{
    my_type(my_type&&) noexcept;
};

using V1 = std::variant<int, float, long, double, my_type>;
V1 test1(V1 v ) { return v; }


Was producing a jump table of size 5 on GCC 8.3. GCC 9 produces huge jump tables with over 30 entries. This leads to 3 times bigger binaries with GCC 9. https://godbolt.org/z/SUWL5T


---


### compiler : `gcc`
### title : `[9 Regression] std::variant operators regressed since GCC 8.3`
### open_at : `2019-03-25T17:29:37Z`
### last_modified_date : `2019-04-25T08:55:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89819
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.0`
### severity : `normal`
### contents :
The following code


#include <variant>

struct my_type{};
bool operator==(const my_type&, const my_type&) noexcept;

using V1 = std::variant<int, float, long, double, my_type>;
auto test1(const V1& v) { return v == v; }


Was producing a jump table of size 5 on GCC 8.3. GCC 9 produces huge jump tables with over 30 entries. This leads to ~15 times bigger binaries with GCC 9 and ~25% compilation slowdown. https://godbolt.org/z/yoAIrP

This could be fixed by changing the `_VARIANT_RELATION_FUNCTION_TEMPLATE` from binary visitation to unary via first checking the `index()` of `__lhs` + `__rhs` and doing the visitation only if they match (hold the same type).

Pseudo-code:

#define _VARIANT_RELATION_FUNCTION_TEMPLATE(__OP, __NAME) \
  template<typename... _Types> \
    constexpr bool operator __OP(const variant<_Types...>& __lhs, \
				 const variant<_Types...>& __rhs) \
    { \
      bool __ret = true; \
      if ((__lhs.index() + 1) != (__rhs.index() + 1)) { \
          return (__lhs.index() + 1) __OP (__rhs.index() + 1); \
      } \
      __do_visit([&__ret, &__lhs] \
		 (auto&& __rhs_mem) mutable	\
		   -> __detail::__variant::__variant_cookie \
        { \
          using __Type = remove_reference_t<decltype(__rhs_mem)>; \
	  if constexpr (!is_same_v< \
	                          __Type, \
				  __detail::__variant::__variant_cookie>) \
	    __ret = __detail::__variant::__get<__detail::__variant::__index_of_v<__Type, _Types...>>(__this_mem) __OP __rhs_mem; \
	  return {}; \
	}, __rhs); \
      return __ret; \
    } \
\
  constexpr bool operator __OP(monostate, monostate) noexcept \
{ return 0 __OP 0; }


---


### compiler : `gcc`
### title : `Returning empty type produces unnecessary instructions`
### open_at : `2019-03-25T20:31:43Z`
### last_modified_date : `2019-03-25T20:36:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89820
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
Consider the following code


struct my_type_impl {};

my_type_impl foo0() { return {}; }
my_type_impl foo1() { my_type_impl tmp; return tmp; }


For `foo0` and `foo1` GCC generates the following assembly:

        xor     eax, eax
        ret


However xoring the `eax` seems unnecessary and some of the other compilers just generate the `ret` instruction.

The additional `xor` instruction could significantly increase the code size for generic C++ programs. For example in Bug 89819 and Bug 89816 each of he 36 jump table entries has that additional instruction.


---


### compiler : `gcc`
### title : `self mov on x86_64 and not optimized-out sub on ARM/ARM64 in a jump table switch`
### open_at : `2019-03-26T01:24:21Z`
### last_modified_date : `2021-08-03T01:34:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89822
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 46020
A reproducer

A simple switch that will be generated as a jump table:

int f1();
int f2();
int f3();
int f4();
int f5();

int foo(int i)
{
    switch (i) {
        case 1: return f1();
        case 2: return f2();
        case 3: return f3();
        case 4: return f4();
        case 5: return f5();
    }
    __builtin_unreachable();
}

Compiles into (first two rows):

i686:
  movl 4(%esp), %eax
  jmp *.L4(,%eax,4)

x86_64:
  movl %edi, %edi
  jmp *.L4(,%rdi,8)

ARM:
  sub r0, r0, #1
  cmp r0, #16

ARM64:
  sub w0, w0, #1
  cmp w0, 16


I am not sure why on ARM there is even cmp+bls. https://godbolt.org/z/hi66cD


Possibly a useful info:
GCC  x86_64
4.1  mov %edi, %eax
4.4  mov %edi, %edi
4.6  movl %edi, %edi
4.8  bogus jump became jump to ret
8.1  jump to ret removed, but self mov is still there


---


### compiler : `gcc`
### title : `Variant jump table reserves space for __variant_cookie twice`
### open_at : `2019-03-26T10:17:00Z`
### last_modified_date : `2019-07-12T13:40:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89824
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.0`
### severity : `normal`
### contents :
Space for the `__variant_cookie` state is already reserved in _Multi_array `_Multi_array<_Tp, __rest...> _M_arr[__first + __do_cookie];`.

Additionally reserving it inside the `__gen_vtable` produces jump table with gaps https://godbolt.org/z/Vx_wEU.


Fix: remove the `+ (is_same_v<_Result_type, __variant_cookie> ? 1 : 0)` from `__gen_vtable`.


This removes zeros from jump table and slightly reduces the binary size https://godbolt.org/z/gyo0-j


---


### compiler : `gcc`
### title : `Jump table for variant visitation could be shortened for never empty variants`
### open_at : `2019-03-26T10:52:13Z`
### last_modified_date : `2019-03-27T11:35:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89825
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.0`
### severity : `normal`
### contents :
The `__do_cookie` computation in `_Multi_array` seems suboptimal. There are variant types that are never empty, so they never need the cookie value at all. `_Variant_storage<true, _Types...>::_M_valid()` already use that knowledge to always return `true`. The same logic could be used for the `__do_cookie`.

Pseudo-code:

+  template<typename _Variant>
+  struct _Never_empty;

+  template<typename... _Types>
+  struct _Never_empty<variant<_Types...>>
+  {
+    static constexpr bool _S_value = (is_trivially_copyable_v<_Types> && ...);
+  };

  template<typename _Ret,
	   typename _Visitor,
	   typename... _Variants,
	   size_t __first, size_t... __rest>
    struct _Multi_array<_Ret(*)(_Visitor, _Variants...), __first, __rest...>
    {
+      static constexpr size_t __index = sizeof...(_Variants) - sizeof...(__rest) - 1;
+      using _Variant_current = __remove_cvref_t<typename _Nth_type<__index, _Variants...>::type>;
      static constexpr int __do_cookie =
-	is_same_v<_Ret, __variant_cookie> ? 1 : 0;
+	is_same_v<_Ret, __variant_cookie> && _Never_empty<_Variant_current>::_S_value ? 1 : 0;
      using _Tp = _Ret(*)(_Visitor, _Variants...);
      template<typename... _Args>
	constexpr const _Tp&
	_M_access(size_t __first_index, _Args... __rest_indices) const
        { return _M_arr[__first_index + __do_cookie]._M_access(__rest_indices...); }

      _Multi_array<_Tp, __rest...> _M_arr[__first + __do_cookie];
  };


  template<size_t... __var_indices>
	static constexpr void
	_S_apply_all_alts(_Array_type& __vtable,
			  std::index_sequence<__var_indices...>)
	{
-	  if constexpr (is_same_v<_Result_type, __variant_cookie>)
+	  if constexpr (is_same_v<_Result_type, __variant_cookie>
+	    && !_Never_empty<remove_cv_t<_Next>>::_S_value)
	    (_S_apply_single_alt<true, __var_indices>(
	      __vtable._M_arr[__var_indices + 1],
	      &(__vtable._M_arr[0])), ...);
	  else
	    (_S_apply_single_alt<false, __var_indices>(
	      __vtable._M_arr[__var_indices]), ...);
	}



The above patch reduces jump table size on up to 2*sizeof...(_Types) for binary visitations.


---


### compiler : `gcc`
### title : `The RISC-V target uses amoswap.w for relaxed stores`
### open_at : `2019-03-26T19:51:00Z`
### last_modified_date : `2023-07-31T16:18:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89835
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
Stores with memory_order_relaxed can be implemented as simple store instructions. For the RISC-V target, gcc generates less efficient atomic instruction amoswap.w.

Try to compile with code with "riscv64-linux-gnu-gcc-8 -S -o - -O2 relaxed.c":

#include <stdatomic.h>

_Atomic int value;

int load(void)
{
        return atomic_load_explicit(&value, memory_order_relaxed);
}

void store(int x)
{
        atomic_store_explicit(&value, x, memory_order_relaxed);
}


Result:

        .file   "relaxed.c"
        .option pic
        .text
        .align  1
        .globl  load
        .type   load, @function
load:
        la      a5,value
        lw      a0,0(a5)
        sext.w  a0,a0
        ret
        .size   load, .-load
        .align  1
        .globl  store
        .type   store, @function
store:
        la      a5,value
        amoswap.w zero,a0,0(a5)
        ret
        .size   store, .-store
        .comm   value,4,4
        .ident  "GCC: (Debian 8.3.0-2) 8.3.0"


---


### compiler : `gcc`
### title : `Consider improving division and modulo by constant if highpart multiply is cheap`
### open_at : `2019-03-27T10:04:45Z`
### last_modified_date : `2022-05-30T20:24:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89845
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
The
https://arxiv.org/pdf/1902.01961.pdf
paper shows some possible improvements for the division and modulo by constant (and modulo by constant equality comparison against 0).


---


### compiler : `gcc`
### title : `Simplify subexpressions of % constant`
### open_at : `2019-03-27T10:12:51Z`
### last_modified_date : `2021-07-28T21:02:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89847
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
+++ This bug was initially created as a clone of Bug #89845 +++

The
https://arxiv.org/pdf/1902.01961.pdf
paper also mentions clang is able to optimize:
int f1 (int x) { return (31 * x + 27961) & 15; }
unsigned f2 (unsigned x) { return (31 * x + 27961) & 15; }
into:
return (9 - x) & 15;
while gcc can't.  Of course it should be done only if the operand of %/& (or narrowing cast) isn't used multiple times (to be precise, isn't used outside of the &/&/cast operand) and after giving say SCCVN a chance to merge the same computations from multiple different spots, but then we can really simplify the constants there (modulo the outer constant) and multiplications by constants etc.


---


### compiler : `gcc`
### title : `worse code compared to clang with alloca()`
### open_at : `2019-03-29T17:08:24Z`
### last_modified_date : `2021-12-23T07:24:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89889
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Example: https://godbolt.org/z/MLZAA6.

Why is the push/lea/leave necessary? Shouldn't modifying the stack pointer be enough?


---


### compiler : `gcc`
### title : `Unable to sink high half of widening multiply out of loop`
### open_at : `2019-03-31T07:54:05Z`
### last_modified_date : `2019-04-02T23:36:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89895
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.3.0`
### severity : `enhancement`
### contents :
This is part of gcc's general problem with double-word values, but
I was encouraged to submit a PR, since it's a particularly simple but
real-world-applicable test case.

Lemire's algorithm for uniform random number generation in a range
(https://arxiv.org/abs/1805.10941) has the following core:

static uint64_t __attribute__((noinline)) get_random_u64(void);

u64 get_random_range(uint64_t range, uint64_t lim)
{
	unsigned __int128 prod;

	do {
		prod = (unsigned __int128)range * get_random_u64();
	} while ((uint64_t)prod < lim);
	return prod >> 64;
}

(In practice, get_random_u64() would be inlined, but I've left it
out of line for exposition.)

GCC's isn't sinking generation of the high half of the product out
of the loop.  This particularly applies on platforms with a separate
multiply-high instruction like alpha:
$L9:
	bsr $26,get_random_u64		!samegp
	mulq $0,$9,$1
	umulh $0,$9,$0
	cmpule $10,$1,$1
	beq $1,$L9
and PowerPC:
.L12:
	bl get_random_u64
	mulld 9,3,31
	mulhdu 3,3,31
	cmpld 7,30,9
	bgt+ 7,.L12
But is also applies to MIPS, where the mfhi could be sunk out of the
loop:
.L10:
	jal	get_random_u64
	nop

	dmultu	$2,$17
	mflo	$2
	sltu	$6,$2,$16
	bne	$6,$0,.L10
	mfhi	$3

In this case, there's nothing *better* to do in the delay slot than mfhi,
but that's kind of an accident.

The code I'd hope to see is
Alpha:
$L9:
	bsr $26,get_random_u64
	mulq $0,$9,$1
	cmpule $10,$1,$1
	beq $1,$L9
	umulh $0,$9,$0
PowerPC:
.L12:
	bl get_random_u64
	mulld 9,3,31
	cmpld 7,30,9
	bgt+ 7,.L12
	mulhdu 3,3,31
and (when the mulditi3 expander is added) MIPS r6:
.L10:
	balc	get_random_u64
	dmulu	$3, $2, $17
	sltu	$3, $3, $16
	bnezc	$3, .L10
	dmuhu	$2, $2, $17

In these cases, since the low-half multiply is the last multiply in
the loop, the high half will still catch the hardware-optimized case
for both halves of a multiply.


---


### compiler : `gcc`
### title : `Unnecessary rejection of dependence for outer loop vectorisation`
### open_at : `2019-04-01T17:23:52Z`
### last_modified_date : `2019-04-03T08:53:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89908
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
This loop:

void __attribute__ ((noipa))
f (int a[][N], int b[restrict])
{
  for (int i = N - 1; i-- > 0; )
    for (int j = 0; j < N - 1; ++j)
      a[j + 1][i] = a[j][i + 1] + b[i];
}

should be vectorisable using outer loop vectorisation, since the
dependence between the lhs and rhs is in the same nonzero direction
for both loops.

See https://gcc.gnu.org/ml/gcc-patches/2019-03/msg01224.html for
some discussion about how the dependence checks could be handled.


---


### compiler : `gcc`
### title : `The jump threading increases the size a lot when having an huge inline-asm`
### open_at : `2019-04-02T10:02:16Z`
### last_modified_date : `2023-06-25T21:08:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89921
### status : `NEW`
### tags : `inline-asm, missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `enhancement`
### contents :
// sizeof-asm-dom.c:

#define LARGE_ASM "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n" \
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"

extern unsigned bar (void);

unsigned foo (unsigned v)
{
  unsigned u;
  unsigned o;
  if (v > 10)
    u = bar ();

  asm (LARGE_ASM:"=g"(o)::); // a non-volatile asm.

  if (v < 5)
    return u;

  return 0;
}

// EOF

gcc -O -fdump-tree-dom -c sizeof-asm-dom.c
grep asm sizeof-asm*dom2 | wc -l

This yields 2, i.e. the large asm has been duplicated.

My understanding (without having read the actual code of the dom pass) is that
the dom pass relies on a cost model to decide whether to perform the duplication
(inserting a simple loop in place of the asm prevents it).  Also, at some point
GCC computes an estimate of inline asm sizes based on the number of instructions
they contain (as per "Size of an 'asm'" section of the manual).  It seems, the
dom pass could use that information to avoid bloating the resulting code.

Is it just an unimplemented thing that needs fixing?


---


### compiler : `gcc`
### title : `Loop on fixed size array is not unrolled and poorly optimized at -O2`
### open_at : `2019-04-02T10:49:14Z`
### last_modified_date : `2021-12-27T04:11:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89922
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Consider the example:


struct array {
   int data[5];
};

array test(int i) {
    array a = {1, i, 2, 3, 4};
    
    for (int j = 0; j < 5; ++j) {
      a.data[j] += j;
    }
    
    return a;
}


GCC-9 generates ~20 instructions with jmps.

Rewriting the same function with unrolled loop makes the assembly much better:

array test2(int i) {
    array a = {1, i, 2, 3, 4};
    a.data[0] += 0;
    a.data[1] += 1;
    a.data[2] += 2;
    a.data[3] += 3;
    a.data[4] += 4;
    
    return a;
}


Assembly for `test2` takes only ~8 instructions:
test2(int):
        add     esi, 1
        mov     DWORD PTR [rdi], 1
        mov     rax, rdi
        movabs  rdx, 25769803780
        mov     DWORD PTR [rdi+4], esi
        mov     QWORD PTR [rdi+8], rdx
        mov     DWORD PTR [rdi+16], 8
        ret


---


### compiler : `gcc`
### title : `missed optimization for signed extension for x86-64`
### open_at : `2019-04-04T01:31:43Z`
### last_modified_date : `2021-09-30T17:38:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89954
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `enhancement`
### contents :
This code snippet 

/////////////////////
char c;
int f()
{
    return c ^ 1;
}
/////////////////////

is compiled into these instructions with options "-O2 -S"


;;;;;;;;;;;;;;;;;;;;;
	movq	_c@GOTPCREL(%rip), %rax
	movzbl	(%rax), %eax
	xorl	$1, %eax
	movsbl	%al, %eax
;;;;;;;;;;;;;;;;;;;;;

Only movsbl is needed because xor by 1 doesn't change high bits.


---


### compiler : `gcc`
### title : `likely/unlikely attributes don't work on a compound-statement`
### open_at : `2019-04-04T10:59:52Z`
### last_modified_date : `2020-07-09T12:50:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89962
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
int signum(int i)
{
  if (i > 0) [[likely]]
    return 1;
  if (i < 0) [[likely]]
  {
    return -1;
  }
  return 0;
}

The first attribute works fine, the second doesn't:

f.cc: In function 'int signum(int)':
f.cc:5:14: warning: attributes at the beginning of statement are ignored [-Wattributes]
    5 |   if (i < 0) [[likely]]
      |              ^~~~~~~~~~


The example from [dcl.attr.likelihood] similarly warns:

void g(int);

int f(int n) {
  if (n > 5) [[unlikely]] { // n > 5 is considered to be arbitrarily unlikely
    g(0);
    return n * 2 + 1;
  }

  switch (n) {
  case 1:
    g(1);
    [[fallthrough]];

  [[likely]] case 2: // n == 2 is considered to be arbitrarily more
    g(2);            // likely than any other value of n
    break;
  }
  return 3;
}

f.cc: In function 'int f(int)':
f.cc:3:14: warning: attributes at the beginning of statement are ignored [-Wattributes]
    3 |   if (n > 5) [[unlikely]] {  // n > 5 is considered to be arbitrarily unlikely
      |              ^~~~~~~~~~~~


---


### compiler : `gcc`
### title : `Inefficient code generation for vld2q_lane_u8 under aarch64`
### open_at : `2019-04-04T15:11:33Z`
### last_modified_date : `2023-08-23T15:57:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89967
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
Using vld2q_lane_u8 can generate very inefficient code under aarch64. Consider the following code (compile it with gcc -march=armv8-a -Wall -Wextra -O3):

#include <arm_neon.h>

void func(unsigned char *in, int *out, int n, int32x4_t o1v, int32x4_t o2v,
          int32x4_t s1v, int32x4_t s2v, int32x4_t m1v, int32x4_t m2v,
          int32x4_t m3v, int32x4_t m4v)
{
  int i;
  int32x4_t l1v, l2v, g1v, g2v, g3v, g4v, gv;
  uint8x16x2_t g1b, g2b;

  l1v = o1v;
  l2v = o2v;
  g1b.val[0] = vdupq_n_u8(0);
  g1b.val[1] = vdupq_n_u8(0);
  g2b.val[0] = vdupq_n_u8(0);
  g2b.val[1] = vdupq_n_u8(0);
  for (i=0; i<n; i+=4)
  {
    g1b = vld2q_lane_u8(&in[vgetq_lane_s32(l1v, 0)], g1b, 0);
    g1b = vld2q_lane_u8(&in[vgetq_lane_s32(l1v, 1)], g1b, 4);
    g1b = vld2q_lane_u8(&in[vgetq_lane_s32(l1v, 2)], g1b, 8);
    g1b = vld2q_lane_u8(&in[vgetq_lane_s32(l1v, 3)], g1b, 12);
    g2b = vld2q_lane_u8(&in[vgetq_lane_s32(l2v, 0)], g2b, 0);
    g2b = vld2q_lane_u8(&in[vgetq_lane_s32(l2v, 1)], g2b, 4);
    g2b = vld2q_lane_u8(&in[vgetq_lane_s32(l2v, 2)], g2b, 8);
    g2b = vld2q_lane_u8(&in[vgetq_lane_s32(l2v, 3)], g2b, 12);
    g1v = vreinterpretq_s32_u8(g1b.val[0]);
    g2v = vreinterpretq_s32_u8(g1b.val[1]);
    g3v = vreinterpretq_s32_u8(g2b.val[0]);
    g4v = vreinterpretq_s32_u8(g2b.val[1]);
    gv = vmlaq_s32(vmlaq_s32(vmlaq_s32(vmulq_s32(m4v, g4v), m3v, g3v),
                             m2v, g2v), m1v, g1v);
    vst1q_s32(&out[i], gv);
    vaddq_s32(l1v, s1v);
    vaddq_s32(l2v, s2v);
  }
}

The calls to vld2q_lane_u8 generate the following assembler code:

        mov     v30.16b, v2.16b
        mov     v31.16b, v28.16b
        mov     v24.16b, v26.16b
        mov     v25.16b, v27.16b
        ld2     {v30.b - v31.b}[0], [x7]
        ld2     {v24.b - v25.b}[0], [x10]
        mov     v22.16b, v30.16b
        mov     v23.16b, v31.16b
        mov     v20.16b, v24.16b
        mov     v21.16b, v25.16b
        ld2     {v22.b - v23.b}[4], [x6]
        ld2     {v20.b - v21.b}[4], [x9]
        mov     v18.16b, v22.16b
        mov     v19.16b, v23.16b
        mov     v0.16b, v20.16b
        mov     v1.16b, v21.16b
        ld2     {v18.b - v19.b}[8], [x5]
        ld2     {v0.b - v1.b}[8], [x8]
        mov     v2.16b, v18.16b
        mov     v3.16b, v19.16b
        mov     v16.16b, v0.16b
        mov     v17.16b, v1.16b
        ld2     {v2.b - v3.b}[12], [x0]
        ld2     {v16.b - v17.b}[12], [x4]

There is a large amount of unnecessary register copying going on. Since the compiler was smart enough to replace the vgetq_lane_s32 and vaddq_s32 calls with direct register accesses and manipulations, I would have expected the vld2q_lane_u8 calls to look like this (saving 16 unnecessary register copy instructions:

        ld2     {v0.b - v1.b}[0], [x7]
        ld2     {v2.b - v3.b}[0], [x10]
        ld2     {v0.b - v1.b}[4], [x6]
        ld2     {v2.b - v3.b}[4], [x9]
        ld2     {v0.b - v1.b}[8], [x5]
        ld2     {v2.b - v2.b}[8], [x8]
        ld2     {v0.b - v1.b}[12], [x0]
        ld2     {v2.b - v3.b}[12], [x4]

In general (i.e., in cases the compiler isn't able to replace the vgetq_lane_s32 and vaddq_s32 calls with direct register accesses and manipulations), I would expect the code to look roughly like this (modulo the instruction order and register numbers):


        umov w0, v4.s[0]
        umov w1, v4.s[1]
        umov w2, v4.s[2]
        umov w3, v4.s[3]
        add  x0, x5, w0, sxtw
        add  x1, x5, w1, sxtw
        add  x2, x5, w2, sxtw
        add  x3, x5, w3, sxtw
        ld2  {v0.b, v1.b}[0], [x0]
        ld2  {v0.b, v1.b}[4], [x1]
        ld2  {v0.b, v1.b}[8], [x2]
        ld2  {v0.b, v1.b}[12], [x3]
        umov w0, v5.s[0]
        umov w1, v5.s[1]
        umov w2, v5.s[2]
        umov w3, v5.s[3]
        add  x0, x5, w0, sxtw
        add  x1, x5, w1, sxtw
        add  x2, x5, w2, sxtw
        add  x3, x5, w3, sxtw
        ld2  {v2.b, v3.b}[0], [x0]
        ld2  {v2.b, v3.b}[4], [x1]
        ld2  {v2.b, v3.b}[8], [x2]
        ld2  {v2.b, v3.b}[12], [x3]

Tested with gcc 8.3.0 as an aarch64 cross compiler on an x86_64 system (built with crosstool-ng 1.24.0-rc3). Output of gcc -v:

Using built-in specs.
COLLECT_GCC=[...]/gcc-8.3-aarch64/bin/aarch64-unknown-linux-gnu-gcc
COLLECT_LTO_WRAPPER=[...]/gcc-8.3-aarch64/bin/../libexec/gcc/aarch64-unknown-linux-gnu/8.3.0/lto-wrapper
Target: aarch64-unknown-linux-gnu
Configured with: [...]/.build/aarch64-unknown-linux-gnu/src/gcc/configure --build=x86_64-build_pc-linux-gnu --host=x86_64-build_pc-linux-gnu --target=aarch64-unknown-linux-gnu --prefix=[...]/gcc-8.3-aarch64 --with-sysroot=[...]/gcc-8.3-aarch64/aarch64-unknown-linux-gnu/sysroot --enable-languages=c,c++,fortran --with-pkgversion='crosstool-NG 1.24.0-rc3' --enable-__cxa_atexit --disable-libmudflap --enable-libgomp --disable-libssp --disable-libquadmath --disable-libquadmath-support --disable-libsanitizer --disable-libmpx --with-gmp=[...]/.build/aarch64-unknown-linux-gnu/buildtools --with-mpfr=[...]/.build/aarch64-unknown-linux-gnu/buildtools --with-mpc=[...]/.build/aarch64-unknown-linux-gnu/buildtools --with-isl=[...]/.build/aarch64-unknown-linux-gnu/buildtools --enable-lto --enable-threads=posix --enable-target-optspace --disable-plugin --disable-nls --disable-multilib --with-local-prefix=[...]/gcc-8.3-aarch64/aarch64-unknown-linux-gnu/sysroot --enable-long-long
Thread model: posix
gcc version 8.3.0 (crosstool-NG 1.24.0-rc3)


---


### compiler : `gcc`
### title : `Extra register move`
### open_at : `2019-04-05T12:37:58Z`
### last_modified_date : `2023-09-21T14:02:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89984
### status : `RESOLVED`
### tags : `easyhack, missed-optimization, ra`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
[hjl@gnu-cfl-1 xxx]$ cat x.i 
float
check_f_pos (float x, float y)
{
  return x * __builtin_copysignf (1.0f, y);
}
[hjl@gnu-cfl-1 xxx]$ cat x.i 
float
check_f_pos (float x, float y)
{
  return x * __builtin_copysignf (1.0f, y);
}
[hjl@gnu-cfl-1 xxx]$ make
/export/build/gnu/tools-build/gcc-wip-debug/build-x86_64-linux/gcc/xgcc -B/export/build/gnu/tools-build/gcc-wip-debug/build-x86_64-linux/gcc/ -mavx2 -O2  -S x.i
[hjl@gnu-cfl-1 xxx]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4
	.globl	check_f_pos
	.type	check_f_pos, @function
check_f_pos:
.LFB0:
	.cfi_startproc
	vandps	.LC0(%rip), %xmm1, %xmm1
	vxorps	%xmm0, %xmm1, %xmm1
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Why not just do

vxorps	%xmm0, %xmm1, %xmm0

	vmovaps	%xmm1, %xmm0
	ret
	.cfi_endproc
.LFE0:
	.size	check_f_pos, .-check_f_pos
	.section	.rodata.cst16,"aM",@progbits,16
	.align 16
.LC0:
	.long	2147483648
	.long	0
	.long	0
	.long	0
	.ident	"GCC: (GNU) 9.0.1 20190403 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 xxx]$


---


### compiler : `gcc`
### title : `optimizing local exceptions, or are they an observable side effect`
### open_at : `2019-04-09T18:38:37Z`
### last_modified_date : `2019-04-10T05:39:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90029
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `enhancement`
### contents :
Hello, 

this is partly a question, and partly a feature request

Consider following functions

----
int bar_ex_noexcept(int i) noexcept {
    struct foo{};
    try {
        if(i<0){
            throw foo{};
        }
        return i;
    }catch(foo){
        return -1;
    }
}

int bar_ex(int i) {
    struct foo{};
    try {
        if(i<0){
            throw foo{};
        }
        return i;
    }catch(foo){
        return -1;
    }
}

int bar_ret(int i) noexcept {
        if(i<0){
            return -1;
        }
        return i;
}

int bar_goto(int i) noexcept {
        if(i<0){
            goto ex;
        }
        return i;
        ex: return -1;
}
----

All this functions, unless I overlooked something, do exactly the same with different control structure (goto, exception, early return): return the given value i positive or 0, -1 if less than 0.
gcc is smart, and all functions, except those that use as implementation detail an exception, generate the same assembly with (testing with `-O3`).
`bar_ex_noexcept` also has a call to `std::terminate`, even if it will never get executed (example here: https://godbolt.org/z/XVtgXG).
Since `foo` is defined locally to `bar_ex`/`bar_ex_noexcept`, I expected that gcc would have been able to optimize the throw and catch clause completely (calls to `__cxa_allocate_exception` and `__cxa_throw`), but it's not the case.
Also the conditional call to `std::terminate` in `bar_ex_noexcept` surprised me, since the only thrown exception is always caught and ignored.

Therefore my question: are exceptions an observable behavior, and thus the compiler can't optimize them completely away, or is simply gcc "not smart enough" to do those optimization.

If they are not observable, the feature request would be to
 * optimize local exceptions
 * thus remove typeinfo information for types that can never escape the local scope (since if the exceptions are optimized, those information are not used by anyone)
 * remove dead calls to `std::terminate`


---


### compiler : `gcc`
### title : `extraneous setne after asm statement with flag output operands`
### open_at : `2019-04-11T08:12:58Z`
### last_modified_date : `2021-09-13T21:33:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90044
### status : `UNCONFIRMED`
### tags : `inline-asm, missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 46140
reduced testcase

For the attached code, gcc -O3 generates (simplified):
$ x86_64-pc-linux-gnu-gcc -O3 tste.c -masm=intel -S -o-
...
foo:
        xor     eax, eax
        # SETNE
        setne   al
        je      .L2
        xor     eax, eax
        # TRUE
        ret
.L2:
        # FALSE
        ret

which is way too complex; "setne" is useless there, and it even only modifies the value 0 already in eax; this one would be better:

foo:
        xor     eax, eax
        # SETNE
        je      .L2
        # TRUE
        ret
.L2:
        # FALSE
        ret


---


### compiler : `gcc`
### title : `Add optimization for optimizing small integer values by fp integral constant`
### open_at : `2019-04-12T17:42:33Z`
### last_modified_date : `2022-03-08T16:21:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90070
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
I was looking at the Spec 2017 imagick benchmark, and in particular at the hot function in enhance.c.

The code has many places where it has:

typedef struct _PixelPacket
{
  unsigned short blue;
  unsigned short green;
  unsigned short red;
  unsigned short opacity;
} PixelPacket;

typedef struct _MagickPixelPacket
{
  float red;
  float green;
  float blue;
  float opacity;
  float index;
} MagickPixelPacket;

/* ... */

foo () {
  MagickPixelPacket aggregate;

  /* ... */

  aggregate.red+=(5.0)*((r)->red);

  /* ... */
}

In particular this becomes:

  double temp1 = (double)r->red;
  double temp2 = (double)aggregate.red;
  double temp3 = temp2 + (temp1 * 5.0);
  aggregate.red = (float) temp3;

This is due to 5.0 being considered a double precision constant.

It occurs to me that on many machines, multiplying an int by 5 is cheaper than multiplying a double by 5.0.  In particular, since you are multiply an unsigned short by 5.0, you know the value will fit in a 32-bit or 64-bit integer.  This would mean the example might be executed as:

  long temp1 = (long)r->red;
  long temp2 = 5 * temp1;
  float temp3 = (float) temp2;
  aggregate.red += temp3;

Perhaps for non-fast-math it would need to be optimized as in case there are rounding issues:

  long temp1 = (long)r->red;
  long temp2 = 5 * temp1;
  double temp3 = aggregate.red;
  double temp4 = (float) temp2;
  double temp5 = temp3 * temp4;
  aggregate.red = (float) temp5;


---


### compiler : `gcc`
### title : `c++17 template argument deduction results in missed optimization`
### open_at : `2019-04-14T13:12:34Z`
### last_modified_date : `2021-08-04T08:57:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90085
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `8.2.1`
### severity : `normal`
### contents :
Created attachment 46164
test case

When deducing template arguments of a constexpr std::array, all the elements (strings in the test case) are stored in the data section, regardless if there are used in the program or not. Without template argument deduction only the accessed elements are stored in the data section. Template argument deduction somehow prevents the reduction of the constants.

https://godbolt.org/z/q_COuU


---


### compiler : `gcc`
### title : `Suboptimal codegen for x < 0 ? x - INT_MIN : x`
### open_at : `2019-04-14T20:33:37Z`
### last_modified_date : `2023-05-24T07:57:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90087
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
int foo(int x)
{
    return x < 0 ? x - INT_MIN : x;
}

       | GCC                            | Clang
-------+--------------------------------+-------------------------
x86-64 | movl   %edi, %eax              | movl %edi, %eax       
       | leal   -2147483648(%rdi), %edx | andl $2147483647, %eax
       | testl  %edi, %edi              |
       | cmovs  %edx, %eax              |
-------+--------------------------------+-------------------------
ARM64  | cmp    w0, 0                   | and w0, w0, #0x7fffffff
       | mov    w1, -2147483648         |
       | add    w1, w0, w1              |
       | csel   w0, w1, w0, lt          |

https://godbolt.org/z/VX0Ou2


---


### compiler : `gcc`
### title : `3 ops LEA should be avoided on Intel CPUs`
### open_at : `2019-04-14T21:36:06Z`
### last_modified_date : `2022-08-21T15:44:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90088
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Originally I filed a bug report to LLVM about

int foo(int x)
{
    return (x << 1) | 1;
}

But got an answered that 3 ops LEA is intentionally avoided due to

> For LEA instructions with three source operands and some specific
> situations, instruction latency has increased to 3 cycles, and must
> dispatch via port 1:
> — LEA that has all three source operands: base, index, and offset.
> — LEA that uses base and index registers where the base is EBP, RBP, or R13.
> — LEA that uses RIP relative addressing mode.
> — LEA that uses 16-bit addressing mode.

From 3.5.1.3 (Using LEA) of Intel's Optimization Reference Manual.

https://godbolt.org/z/OqkdAO


---


### compiler : `gcc`
### title : `Missing optimization, elimination of empty data dependant loops`
### open_at : `2019-04-14T21:48:02Z`
### last_modified_date : `2019-04-14T21:57:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90089
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
void * x;
void test(){ for(void * p = x; p; p=*(void**)p); }

With -O3 gives:

test():
        mov     rax, QWORD PTR x[rip]
        test    rax, rax
        je      .L1
.L3:
        mov     rax, QWORD PTR [rax]
        test    rax, rax
        jne     .L3
.L1:
        ret

As you can see it currently traverses pointers even though it could be optimized away. 

6.8.2.2 Forward progress
The implementation may assume that any thread will eventually do one of the following:

    (1.1) terminate,
    (1.2) make a call to a library I/O function,
    (1.3) perform an access through a volatile glvalue, or
    (1.4) perform a synchronization operation or an atomic operation.

[ Note: This is intended to allow compiler transformations such as removal of empty loops, even when termination cannot be proven. — end note ]

MSVC and ICC performs this optimization.


---


### compiler : `gcc`
### title : `better handling of x == LONG_MIN on x86-64`
### open_at : `2019-04-15T11:05:42Z`
### last_modified_date : `2023-06-14T09:00:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90094
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
Compile the following on x86-64:

unsigned f(long a)
{
  return a == LONG_MIN;
}

The result for -O3 is:

f:	movabs $0x8000000000000000,%rax
	cmp    %rax,%rdi
	sete   %al
	movzbl %al,%eax
	retq   

With -Os it looks like this:

f:	mov    $0x1,%eax
	shl    $0x3f,%rax
	cmp    %rax,%rdi
	sete   %al
	movzbl %al,%eax
	retq   

I think for both optimization directions the code should be compiled as if for this:

unsigned f(long a)
{
  long r;
  return __builtin_sub_overflow(a, 1, &r);
}

This compiled to

f:	xor    %eax,%eax
	add    $0xffffffffffffffff,%rdi
	seto   %al
	retq   

This should be faster and is definitely shorter than even the -Os version.

For 32-bit x86 the problem doesn't exist is this form, I think.  But it might apply to some RISC targets as well.


---


### compiler : `gcc`
### title : `builtin sqrt() ignoring libm's sqrt call result`
### open_at : `2019-04-16T00:21:59Z`
### last_modified_date : `2021-08-10T18:31:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90106
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.3.0`
### severity : `normal`
### contents :
GCC 7.3 and 8.2 (from Ubuntu 18.04 and MinGW-w64) seems to generate wrong code for sqrtf() -- and sqrt() -- for multiple platforms (tested under x86-64 and ARM Arch64, gcc 7.3, and Arch32 with gcc 6.3). Try to compile this simple function:

  /* test.c */
  #include <math.h>
  float f( float x ) { return sqrtf( x ); }

And I've got, for x86-64 using SSE:

  # Compiled with gcc -O2 -S test.c
  f:
    pxor %xmm2,%xmm2
    sqrtss %xmm0,%xmm1
    ucomiss %xmm0,%xmm2
    ja .L8
    movaps %xmm1,%xmm0
    ret
  .L8:
    subq $24,%rsp
    movss %xmm1, 12(%rsp)    # save xmm1 from sqrtss
    call  sqrtf@PLT
    movss 12(%rsp),%xmm1     # restore xmm1.
    addq $24,%rsp
    movaps %xmm1,%xmm0       # use xmm1 anyway?!
    ret

Notice, when 0 > x sqrt@PLT is called, but the result is ignored.
A similar code is created by GCC for ARM.

As expected, -ffast-math, creates:

f:
  sqrtss %xmm0,%xmm0
  ret

Which is correct.

My environment:

  $ gcc --version
  gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0

  $ gcc-8 --version
  gcc-8 (Ubuntu 8.2.0-1ubuntu2~18.04) 8.2.0

  $  arm-none-eabi-gcc --version
  arm-none-eabi-gcc (15:6.3.1+svn253039-1build1) 6.3.1 20170620

  $  aarch64-linux-gnu-gcc --version
  aarch64-linux-gnu-gcc (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) 7.3.0

  $  uname -srvp
  Linux 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64

  $  lsb_release -a
  No LSB modules are available.
  Distributor ID:	Ubuntu
  Description:	Ubuntu 18.04.2 LTS
  Release:	18.04
  Codename:	bionic


---


### compiler : `gcc`
### title : `inequality of addresses of distinct objects not folded`
### open_at : `2019-04-16T20:24:36Z`
### last_modified_date : `2019-04-16T22:26:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90122
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
In the test case below GCC folds the second test (as expected, on the assumption that distinct declarations refer to distinct objects) but fails to fold the first.

Clang folds both (into false and true, respectively).  GCC will only do that if a and b are static or local.

Same with extern arrays of known size.

$ cat x.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout -fno-common x.c
extern int a, b;

void foo ();
void bar ();

void f (void)
{
  if (&a == &b)
    foo ();

  int i = a;
  b = 0;
  if (i != a)
    bar ();
}

;; Function f (f, funcdef_no=0, decl_uid=1910, cgraph_uid=1, symbol_order=2)

Removing basic block 5
f ()
{
  <bb 2> [local count: 1073741824]:
  if (&a == &b)
    goto <bb 3>; [17.43%]
  else
    goto <bb 4>; [82.57%]

  <bb 3> [local count: 187153200]:
  foo ();

  <bb 4> [local count: 1073741824]:
  b = 0;
  return;

}


---


### compiler : `gcc`
### title : `Bad register spill due to top-down allocation order`
### open_at : `2019-04-19T05:42:48Z`
### last_modified_date : `2020-06-25T17:20:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90174
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Current regional RA uses a top-down allocation order, which may not properly split a long live range that crosses sub-region with high register pressure. 

In the following graph, lv0 is live in whole outer region, and suppose inner region is under high register pressure due to lots of live ranges inside it.

According to RA algorithm, out region is processed firstly, lv0 is picked up as spill candidate. And then turn to inner region, also the part of lv0 in inner region is marked as being spilled. Finally result is that the whole lv0 should be spilled. But if in area excluding inner region, there is with low register pressure, we can get a better choice to place lv0 in register instead of memory, and only spill/reload lv0 at boundary of entry-into(A)/exist-from(B) inner region. In other word, inner region boundary are split points for lv0.


               |  
       outer   | lv0 
       region  | __________  split point
               |/
    .----------A---------------.
    |          |               |
    |          |  |         |  |
    |  inner   |  | lv1     |  | 
    |  region  |  |         |  |
    |          |  |     lv2 |  |
    |          |  |         |  |
    |          |               |
    '----------B---------------'
               |\__________
               |            split point
               |

Here is an example to show this. gcc produces bad spills as we point out(-m64 -O3, for x86-64), but llvm generates better spill/reload as we expect.

  int value[20];

  int user0;
  int user1;
  int user2[100];
  int user3;

  int fncall(void);

  void foo(int cond)
  {
      int lv_out = value[0];
      int i;

      user0 = lv_out;   /* Better to place lv_out in register. */

      if (cond) {
          int sum = 0;
          int lv_in_1 = value[1];
          int lv_in_2 = value[2];
          int lv_in_3 = value[3];
          int lv_in_4 = value[4];
          int lv_in_5 = value[5];
          int lv_in_6 = value[6];
          int lv_in_7 = value[7];
          int lv_in_8 = value[8];
          int lv_in_9 = value[9];
          int lv_in_10 = value[10];
          int lv_in_11 = value[11];
          int lv_in_12 = value[12];
          int lv_in_13 = value[13];
          int lv_in_14 = value[14];
          int lv_in_15 = value[15];

          /* Better to spill lv_out here */

          for (i = 0; i < 1000; i++) {
              sum += lv_in_1;
              sum += lv_in_2;
              sum += lv_in_3;
              sum += lv_in_4;
              sum += lv_in_5;
              sum += lv_in_6;
              sum += lv_in_7;
              sum += lv_in_8;
              sum += lv_in_9;
              sum += lv_in_10;
              sum += lv_in_11;
              sum += lv_in_12;
              sum += lv_in_13;
              sum += lv_in_14;
              sum += lv_in_15;

              fncall();

              lv_in_1 ^= i;
              lv_in_2 ^= i;
              lv_in_3 ^= i;
              lv_in_4 ^= i;
              lv_in_5 ^= i;
              lv_in_6 ^= i;
              lv_in_7 ^= i;
              lv_in_8 ^= i;
              lv_in_9 ^= i;
              lv_in_10 ^= i;
              lv_in_11 ^= i;
              lv_in_12 ^= i;
              lv_in_13 ^= i;
              lv_in_14 ^= i;
              lv_in_15 ^= i;
          }

          /* Better to reload lv_out here */

          user1 = sum;
      }

      for (i = 0; i < 100; i++) {
          user2[i ^ 100] = lv_out; /* Better to place lv_out in register */
      }
    
      user3 = lv_out;  /* Better to place lv_out in register */
  }


For top-down allocation, we can only adjust inner region allocation result, but no way to refine decision that has been made on outside live-range, it is an intrinsic weakness of the top-down algorithm. To fix that, we may need to add a new pass to explicitly split live ranges based on region boundary, or adopt a reverse means, from inner to outer to perform allocation.


---


### compiler : `gcc`
### title : `[9 Regression] Missed optimization: duplicated terminal basic block with -mavx`
### open_at : `2019-04-19T08:30:59Z`
### last_modified_date : `2022-05-27T09:01:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90178
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
The following short C program,

int* find_ptr(int* mem, int sz, int val) {
    for (int i = 0; i < sz; i++) {
        if (mem[i] == val) { 
            return &mem[i];
        }
    }
    return nullptr;
}

compiles to the following on GCC (trunk) with -O3 -march=skylake on Godbolt.

find_ptr(int*, int, int):
        mov     rax, rdi
        test    esi, esi
        jle     .L4                  ## Why not .L8?
        lea     ecx, [rsi-1]
        lea     rcx, [rdi+4+rcx*4]
        jmp     .L3
.L9:
        add     rax, 4
        cmp     rax, rcx
        je      .L8
.L3:
        cmp     DWORD PTR [rax], edx
        jne     .L9
        ret
.L8:
        xor     eax, eax
        ret
.L4:
        xor     eax, eax
        ret

Godbolt link: https://godbolt.org/z/WczJ3J

Here the terminal basic blocks .L8 and .L4 are identical. It seems to me that there is no benefit to keeping .L4 around, and jumps should be redirected to .L8. Disabling AVX via -mno-avx eliminates the duplicate. However, a similar code generation quirk exists in Clang for this program, so I apologize if there is a microarchitectural subtlety I'm missing here.

Godbolt link for Clang comparison: https://godbolt.org/z/2uVZ8v


---


### compiler : `gcc`
### title : `ICE in expand_debug_expr, at cfgexpand.c:5244`
### open_at : `2019-04-20T11:26:36Z`
### last_modified_date : `2019-12-19T19:42:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90194
### status : `RESOLVED`
### tags : `ice-checking, ice-on-valid-code, missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
g++-9.0.0-alpha20190407 snapshot (r270192) ICEs when compiling the following testcase at any optimization level (except -Og) and w/ -g:

struct cb {
  int yr;
};

void *
operator new (__SIZE_TYPE__, void *nq)
{
  return nq;
}

void
af (int xn)
{
  new (&xn) cb { };
}

% g++-9.0.0-alpha20190407 -O1 -g -c vquxpovp.cc
 <constructor 0x7fd5336356d8
    type <record_type 0x7fd53360e690 cb asm_written type_5 type_6 SI
        size <integer_cst 0x7fd5334d9078 constant 32>
        unit-size <integer_cst 0x7fd5334d9090 constant 4>
        align:32 warn_if_not_align:0 symtab:860696976 alias-set 1 canonical-type 0x7fd53360e690
        fields <function_decl 0x7fd533613f00 __dt  type <method_type 0x7fd533628738>
            public abstract external autoinline decl_3 QI vquxpovp.cc:1:8 align:16 warn_if_not_align:0 context <record_type 0x7fd53360e690 cb>
            full-name "cb::~cb() noexcept (<uninstantiated>)"
            not-really-extern chain <function_decl 0x7fd533629100 __dt_base >> context <translation_unit_decl 0x7fd5334c4168 vquxpovp.cc>
        full-name "struct cb"
        X() X(constX&) this=(X&) n_parents=0 use_template=0 interface-unknown
        pointer_to_this <pointer_type 0x7fd53360ebd0> reference_to_this <reference_type 0x7fd5336282a0> chain <type_decl 0x7fd5335da688 cb>>
    constant static tree_0 length:0>
during RTL pass: expand
vquxpovp.cc: In function 'void af(int)':
vquxpovp.cc:12:1: internal compiler error: in expand_debug_expr, at cfgexpand.c:5244
   12 | af (int xn)
      | ^~
0xb29d93 expand_debug_expr
	/var/tmp/portage/sys-devel/gcc-9.0.0_alpha20190407/work/gcc-9-20190407/gcc/cfgexpand.c:5244
0xb2a044 expand_debug_expr
	/var/tmp/portage/sys-devel/gcc-9.0.0_alpha20190407/work/gcc-9-20190407/gcc/cfgexpand.c:4560
0xb359e3 expand_debug_locations
	/var/tmp/portage/sys-devel/gcc-9.0.0_alpha20190407/work/gcc-9-20190407/gcc/cfgexpand.c:5442
0xb359e3 execute
	/var/tmp/portage/sys-devel/gcc-9.0.0_alpha20190407/work/gcc-9-20190407/gcc/cfgexpand.c:6512

g++ 8.3, 7.4, 6.3 fail differently:

% g++-8.3.0 -O1 -fchecking -g -c vquxpovp.cc
vquxpovp.cc: In function 'void af(int)':
vquxpovp.cc:12:1: error: invalid reference prefix
 af (int xn)
 ^~
{}
vquxpovp.cc:14:3: note: in statement
   new (&xn) cb { };
   ^~~~~~~~~~~~~~~~
xn_5 = VIEW_CONVERT_EXPR<int>({});
vquxpovp.cc:12: confused by earlier errors, bailing out


---


### compiler : `gcc`
### title : `AVX-512 instructions not used`
### open_at : `2019-04-22T09:53:09Z`
### last_modified_date : `2019-04-24T12:09:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90202
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Consider the following test program:


struct v {
    int val[16];
};

v test(v a, v b) {
    v res;

    for (int i = 0; i < 16; i++)
        res.val[i] = a.val[i] + b.val[i];

    return res;
}


When compiled with `g++ -O3 -march=skylake-avx512` the following assembly is produced:
test(v, v):
  push rbp
  mov rax, rdi
  mov rbp, rsp
  vmovdqu32 ymm1, YMMWORD PTR [rbp+16]
  vmovdqu32 ymm2, YMMWORD PTR [rbp+48]
  vpaddd ymm0, ymm1, YMMWORD PTR [rbp+80]
  vmovdqu32 YMMWORD PTR [rdi], ymm0
  vpaddd ymm0, ymm2, YMMWORD PTR [rbp+112]
  vmovdqu32 YMMWORD PTR [rdi+32], ymm0
  vzeroupper
  pop rbp
  ret

it seems suboptimal, as the 512 registers are available and a better assembly is possible:
test(v, v):
  vmovdqu32 zmm0, zmmword ptr [rsp + 72]
  vpaddd zmm0, zmm0, zmmword ptr [rsp + 8]
  vmovdqu32 zmmword ptr [rdi], zmm0
  mov rax, rdi
  vzeroupper
  ret


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] C code is optimized worse than C++`
### open_at : `2019-04-22T09:59:43Z`
### last_modified_date : `2023-07-07T10:35:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90204
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Consider the example:


struct v {
    int val[16];
};

struct v test(struct v a, struct v b) {
    struct v res;

    for (int i = 0; i < 16; i++)
        res.val[i] = a.val[i] + b.val[i];

    return res;
}


Compiling that snippet with `g++ -O3 -march=skylake-avx512` gives a short assembly:
test(v, v):
  push rbp
  mov rax, rdi
  mov rbp, rsp
  vmovdqu32 ymm1, YMMWORD PTR [rbp+16]
  vmovdqu32 ymm2, YMMWORD PTR [rbp+48]
  vpaddd ymm0, ymm1, YMMWORD PTR [rbp+80]
  vmovdqu32 YMMWORD PTR [rdi], ymm0
  vpaddd ymm0, ymm2, YMMWORD PTR [rbp+112]
  vmovdqu32 YMMWORD PTR [rdi+32], ymm0
  vzeroupper
  pop rbp
  ret


Compiling the same sample with the C compiler and same flags produces a ~150 lines of assembly with a lot of jumps and comparisons. The regression appeared after GCC-7.3


---


### compiler : `gcc`
### title : `Stack Pointer decrementing even when not loading extra data to stack.`
### open_at : `2019-04-23T21:43:25Z`
### last_modified_date : `2021-08-16T01:14:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90216
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.2.0`
### severity : `enhancement`
### contents :
Hi,

Using ARM GCC 8.2, when I instantiate an object the corresponding Assembly instructions involve a decrementing, then incrementing of the stack pointer. However, no values are being transferred between the registers and the empty stack space. 
 
Please check out this link for details, lines 5 and 7 in the Assembly panel show how the stack pointer is decremented/incremented unnecessarily.

https://godbolt.org/z/h-H7Ox

In the C++ panel when you comment out line 53 and uncomment the line below, the Assembly instructions involving the stack pointer disappear. The same is true if you uncomment just line 55.

Can you please explain why the stack pointer inc/dec operations are not optimized out in the first line of code (line 53)? Can you please try to release a patch where this unnecessary stack pointer inc/dec is no longer an issue?

Thanks


---


### compiler : `gcc`
### title : `Greater optimization of C++ Code`
### open_at : `2019-04-23T22:22:24Z`
### last_modified_date : `2021-05-24T18:30:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90217
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.2.0`
### severity : `normal`
### contents :
Hi,

This is not so much a bug, but more of an enhancement. There are 2 pieces of code I have listed below which should translate to identical assembly instructions at high levels of compiler optimization (level 3) but currently do not.

https://godbolt.org/z/Zn7FMK
https://godbolt.org/z/wB8eZd


Using ARM GCC 8.2, the code in the second link involves the stack pointer and extra load/store operations to the newly-created stack space. There are more assembly instructions in link 2's code than in link 1's code. 

However, in Godbolt if you switch the compiler to Clang, at optimization 3 both pieces of code manage to compile down to the same minimal Assembly instructions.

Switching the compiler to x86-64 GCC (trunk), the code in the second link also has a few extra operations compared the first link's code. 

Is it possible to set ARM GCC and x86-64 GCC to a particular optimization setting that allows both links' code to have matching assembly instructions? If not, is it possible that in a future release, both compilers could apply enough optimizations so that the assembly in link 1 matches link 2?

Thanks


---


### compiler : `gcc`
### title : `Unnecessary save and restore frame pointer with AVX/AVX512 pseudo registers`
### open_at : `2019-04-24T16:18:47Z`
### last_modified_date : `2019-04-25T11:28:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90235
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
From PR 90202:

[hjl@gnu-cfl-1 pr90202]$ cat x.ii
struct v {
    int val[16];
};

struct v test(struct v a, struct v b) {
    struct v res;

    for (int i = 0; i < 16; i++)
        res.val[i] = a.val[i] + b.val[i];

    return res;
}
[hjl@gnu-cfl-1 pr90202]$ make CC=gcc
gcc -O3 -march=skylake  -S x.ii
[hjl@gnu-cfl-1 pr90202]$ cat x.s
	.file	"x.ii"
	.text
	.p2align 4,,15
	.globl	_Z4test1vS_
	.type	_Z4test1vS_, @function
_Z4test1vS_:
.LFB0:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rdi, %rax
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	vmovdqu	16(%rbp), %ymm1
	vmovdqu	48(%rbp), %ymm2
	vpaddd	80(%rbp), %ymm1, %ymm0
	vmovdqu	%ymm0, (%rdi)
	vpaddd	112(%rbp), %ymm2, %ymm0
	vmovdqu	%ymm0, 32(%rdi)
	vzeroupper
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc

Since there is

rtx
gen_reg_rtx (machine_mode mode)
{
  rtx val; 
  unsigned int align = GET_MODE_ALIGNMENT (mode);

  gcc_assert (can_create_pseudo_p ()); 

  /* If a virtual register with bigger mode alignment is generated,
     increase stack alignment estimation because it might be spilled
     to stack later.  */
  if (SUPPORTS_STACK_ALIGNMENT
      && crtl->stack_alignment_estimated < align
      && !crtl->stack_realign_processed)
    {    
      unsigned int min_align = MINIMUM_ALIGNMENT (NULL, mode, align);
      if (crtl->stack_alignment_estimated < min_align)
        crtl->stack_alignment_estimated = min_align;
    }    

and IRA has

  frame_pointer_needed
    = (! flag_omit_frame_pointer
       || (cfun->calls_alloca && EXIT_IGNORE_STACK)
       /* We need the frame pointer to catch stack overflow exceptions if
          the stack pointer is moving (as for the alloca case just above).  */
       || (STACK_CHECK_MOVING_SP
           && flag_stack_check
           && flag_exceptions
           && cfun->can_throw_non_call_exceptions)
       || crtl->accesses_prior_frames
       || (SUPPORTS_STACK_ALIGNMENT && crtl->stack_realign_needed)
       || targetm.frame_pointer_required ());

generate AVX/AVX512 pseudo registers via gen_reg_rtx will mark frame
pointer as needed.  Stack realignment is needed to

1. Align the outgoing stack.
2. Support aligned spill of AVX/AVX512 registers.

But we won't know if spill is needed before RA. As the result, we
save and restore frame pointer even if not needed.  Since 

(define_insn "mov<mode>_internal"
  [(set (match_operand:VMOVE 0 "nonimmediate_operand"
         "=v,v ,v ,m")
        (match_operand:VMOVE 1 "nonimmediate_or_sse_const_operand"
         " C,BC,vm,v"))]
  "TARGET_SSE
   && (register_operand (operands[0], <MODE>mode)
       || register_operand (operands[1], <MODE>mode))"

now supports both aligned and unaligned load/store of AVX/AVX512
registers, we can change gen_reg_rtx to

  /* If a virtual register with bigger mode alignment is generated,
     increase stack alignment estimation because it might be spilled
     to stack later.  */
  if (SUPPORTS_STACK_ALIGNMENT
      && !SUPPORTS_MISALIGNED_SPILL
      && crtl->stack_alignment_estimated < align
      && !crtl->stack_realign_processed)
    {    
      unsigned int min_align = MINIMUM_ALIGNMENT (NULL, mode, align);
      if (crtl->stack_alignment_estimated < min_align)
        crtl->stack_alignment_estimated = min_align;
    }


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Code size regression on thumb2 due to sub-optimal register allocation starting with r265398`
### open_at : `2019-04-25T13:20:38Z`
### last_modified_date : `2023-07-07T10:35:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90249
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 46244
testcase

GCC 9 has regressed on code size due to some sub-optimal register allocation.  For this example, the only difference in the output is that the assignments for r7 and r8 have been switched, but the result is significant growth in code size since r8 requires predominantly 32-bit instructions to be used while r7 requires predominantly 16-bit instructions.

cc1 -fpreprocessed binding2.i -quiet -dumpbase binding2.i -mthumb -mcpu=cortex-a8 -march=armv7-a -auxbase-strip binding.o -Os -w -version -fno-short-enums -fgnu89-inline -o binding2.s

In gcc-8 the output was
DefineConnectorBinding:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        push    {r0, r1, r2, r3, r4, r5, r6, r7, r8, lr}
        mov     r4, r1
        mov     r8, r0
        mov     r1, r2
        mov     r0, r4
        mov     r5, r2
        mov     r7, r3
        bl      LookupBinding
        mov     r6, r0
        cbz     r0, .L2
        ldr     r7, .L5
        mov     r1, r4
        ldr     r0, [r7]         // 16-bit instruction
        bl      GetAtomString
        mov     r1, r5
        mov     r4, r0
        ldr     r0, [r7]         // 16-bit instruction
        bl      GetAtomString
        ldrh    r1, [r6, #8]
        mov     r5, r0
        ldr     r0, [r7]         // 16-bit instruction
        bl      GetAtomString
        ldrh    r3, [r6, #10]
        ldr     r2, .L5+4
        movs    r1, #103
        str     r5, [sp]
        strd    r0, r3, [sp, #4]
        mov     r3, r4
        mov     r0, r8
        bl      SemanticError
        add     sp, sp, #16
        @ sp needed
        pop     {r4, r5, r6, r7, r8, pc}
.L2:
        mov     r3, r7
        mov     r2, r5
        mov     r1, r4
        mov     r0, r8
        bl      NewConnectorBindingTree
        add     sp, sp, #16
        @ sp needed
        pop     {r4, r5, r6, r7, r8, lr}
        b       AddBinding

In gcc-9 we get

DefineConnectorBinding:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        push    {r0, r1, r2, r3, r4, r5, r6, r7, r8, lr}
        mov     r4, r1
        mov     r7, r0
        mov     r1, r2
        mov     r0, r4
        mov     r5, r2
        mov     r8, r3
        bl      LookupBinding
        mov     r6, r0
        cbz     r0, .L2
        ldr     r8, .L5+4
        mov     r1, r4
        ldr     r0, [r8]         // 32-bit instruction
        bl      GetAtomString
        mov     r1, r5
        mov     r4, r0
        ldr     r0, [r8]         // 32-bit instruction
        bl      GetAtomString
        ldrh    r1, [r6, #8]
        mov     r5, r0
        ldr     r0, [r8]         // 32-bit instruction
        bl      GetAtomString
        ldrh    r3, [r6, #10]
        ldr     r2, .L5
        movs    r1, #103
        str     r5, [sp]
        strd    r0, r3, [sp, #4]
        mov     r3, r4
        mov     r0, r7
        bl      SemanticError
        add     sp, sp, #16
        @ sp needed
        pop     {r4, r5, r6, r7, r8, pc}
.L2:
        mov     r3, r8
        mov     r2, r5
        mov     r1, r4
        mov     r0, r7
        bl      NewConnectorBindingTree
        add     sp, sp, #16
        @ sp needed
        pop     {r4, r5, r6, r7, r8, lr}
        b       AddBinding

R8 is used more often than R7, so it seems odd that it is preferred over the latter.


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] r266385 caused code size regressions on Arm, thumb and thumb2`
### open_at : `2019-04-25T17:27:29Z`
### last_modified_date : `2023-07-07T10:35:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90255
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 46247
testcase

Overall r266385 caused
 0.16% regression on Arm and thumb1 -Os
 0.08% regression on thumb2 -Os.
when building CSiBE

Some non-trivial files, however, regressed significantly, some by over 3%.

For example, teem-1.6.0-src src/dye/test/bow regresses by 3.36% on Arm due to additional spills and the need for another register to be allocated.


---


### compiler : `gcc`
### title : `[10 Regression] 8% degradation on cpu2006 403.gcc starting with r270484`
### open_at : `2019-04-25T19:34:14Z`
### last_modified_date : `2019-04-29T15:29:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90257
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Will add more detail as I discover it.


---


### compiler : `gcc`
### title : `Inline small constant memmoves`
### open_at : `2019-04-26T16:45:22Z`
### last_modified_date : `2021-08-29T08:32:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90262
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
GCC does not inline fixed-size memmoves. However memmove can be as easily inlined as memcpy. The existing memcpy infrastructure could be reused/expanded for this - all loads would be emitted first, followed by the stores. Large memmoves could be inlined on targets with vector registers. Targets without vector registers could emit an overlap check and use inlined memcpy for the no overlap case.

void copy(int *p, int *q)
{
  __builtin_memcpy(p, q, 24);
}

void move(int *p, int *q)
{
  __builtin_memmove(p, q, 24);
}

copy:
	ldp	x2, x3, [x1]
	stp	x2, x3, [x0]
	ldr	x1, [x1, 16]
	str	x1, [x0, 16]
	ret

move:
	mov	x2, 24
	b	memmove

The memmove could be expanded using the same number of registers:

	ldp	x2, x3, [x1]
	ldr	x1, [x1, 16]
	stp	x2, x3, [x0]
	str	x1, [x0, 16]


---


### compiler : `gcc`
### title : `Calls to mempcpy should use memcpy`
### open_at : `2019-04-26T16:54:48Z`
### last_modified_date : `2020-02-03T18:50:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90263
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
While GCC now inlines fixed-size mempcpy like memcpy, GCC still emits calls to mempcpy rather than converting to memcpy. Since most libraries, including GLIBC, do not have optimized mempcpy for most targets, calling mempcpy is less efficient than calling memcpy and emitting an extra addition to compute the result.

int *mempcopy1 (int *p, int *q)
{
  return  __builtin_mempcpy (p, q, 16);
}

int *mempcopy2 (int *p, int *q, long n)
{
  return __builtin_mempcpy (p, q, n);
}

mempcopy1:
	ldp	x2, x3, [x1]
	stp	x2, x3, [x0], 16
	ret

mempcopy2:
	b	mempcpy


---


### compiler : `gcc`
### title : `loop distribution defeated by clobbers`
### open_at : `2019-04-27T14:49:48Z`
### last_modified_date : `2019-05-03T13:45:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90269
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
#include <memory>
#ifndef EASY
typedef std::unique_ptr<int> T;
#else
typedef int* T;
#endif
void f(T*__restrict a,T*__restrict b){
    for(int i=0;i<1024;++i){
	new(a+i)T(std::move(b[i]));
	b[i].~T();
    }
}

Compiling with -O3, with -DEASY I get a nice call to memcpy, while without it I keep a loop. The difference, before the ldist pass, is that the version with unique_ptr has clobbers:

  _8 = MEM[(int * const &)_3];
  MEM[(struct  &)_10] ={v} {CLOBBER};
  MEM[(struct _Head_base *)_10]._M_head_impl = _8;
  MEM[(struct  &)_3] ={v} {CLOBBER};

ldist checks gimple_has_side_effects (stmt) on the first clobber and gives up. Vectorization does not seem to have any problem with those clobbers.

(by the way, I believe DSE could remove the first clobber, it seems pretty useless when it is immediately followed by a write to the full object)


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Do not select best induction variable optimization`
### open_at : `2019-04-28T06:20:51Z`
### last_modified_date : `2023-07-07T10:35:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90270
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `normal`
### contents :
Using built-in specs.
COLLECT_GCC=/home/jojo/work/csky/cskytoolchain/csky-toolchain-build-riscv/riscv-install/bin/riscv64-unknown-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/home/jojo/work/csky/cskytoolchain/csky-toolchain-build-riscv/riscv-install/libexec/gcc/riscv64-unknown-linux-gnu/8.1.0/lto-wrapper
Target: riscv64-unknown-linux-gnu
Configured with: /home/jojo/work/csky/cskytoolchain/csky-toolchain-build-riscv/riscv-gcc/configure --target=riscv64-unknown-linux-gnu --prefix=/home/jojo/work/csky/cskytoolchain/csky-toolchain-build-riscv/riscv-install --with-sysroot=/home/jojo/work/csky/cskytoolchain/csky-toolchain-build-riscv/riscv-install/sysroot --with-system-zlib --enable-shared --enable-tls --enable-languages=c,c++ --disable-libmudflap --disable-libssp --disable-libquadmath --disable-nls --disable-bootstrap --src=.././riscv-gcc --enable-checking=yes --with-pkgversion= --disable-multilib --with-abi=lp64 --with-arch=rv64imac 'CFLAGS_FOR_TARGET=-O2  -mcmodel=medlow' 'CXXFLAGS_FOR_TARGET=-O2  -mcmodel=medlow' CFLAGS='-O0 -g' CXXFLAGS='-O0 -g'
Thread model: posix
gcc version 8.1.0 ()


The following case do not select the best iv vars:

extern unsigned short int crcu32(unsigned int newval, unsigned short int crc);
unsigned short int func(unsigned short int crc)
{
 unsigned int final_counts[8];
 unsigned int track_counts[8];
 unsigned int i;

for (i=0; i< 8; i++) {
  crc=crcu32(final_counts[i],crc);
  crc=crcu32(track_counts[i],crc);
 }
 return crc;
}

the asm code:

.L2:
        slli    s0,s1,2
        add     a5,sp,s0
        lw      a0,-4(a5)
        addi    s1,s1,1
        call    crcu32
        addi    a5,sp,32
        add     s0,a5,s0
        mv      a1,a0
        lw      a0,-4(s0)
        call    crcu32
        mv      a1,a0
        bne     s1,s2,.L2

i debug and found some info from "ivopts" tree optimization,

the bellow additional code will adjust cost of some type address in file
tree-ssa-loop-ivopts.c:

/* Cost of small invariant expression adjusted against loop niters
	 is usually zero, which makes it difficult to be differentiated
	 from candidate based on loop invariant variables.  Secondly, the
	 generated invariant expression may not be hoisted out of loop by
	 following pass.  We penalize the cost by rounding up in order to
	 neutralize such effects.  */
cost.cost = adjust_setup_cost (data, cost.cost, true);
cost.scratch = cost.cost;


when i remove the two lines, the created asm code will better:

.L2:
        lw      a0,0(s0)
        addi    s0,s0,4
        addi    s1,s1,4
        call    crcu32
        mv      a1,a0
        lw      a0,-4(s1)
        call    crcu32
        mv      a1,a0
        bne     s0,s2,.L2


---


### compiler : `gcc`
### title : `[missed-optimization] failure to keep variables in registers during "faux" memcpy`
### open_at : `2019-04-28T11:33:52Z`
### last_modified_date : `2019-05-10T07:53:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90271
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Example on GodBolt: https://godbolt.org/z/Q17L1u

Consider the following functions:

template<typename T1, typename T2>
inline void replace_bytes (T1& v1 ,const T2& v2 ,std::size_t k) noexcept
{
   if (k > sizeof(T1) - sizeof(T2)) { return; }

   std::memcpy( (void*) (((char*)&v1)+k) , (const void*) &v2 , sizeof(T2) );
}

For plain-old-data types, this is nothing but the manipulation of v1's bytes (and there are no pointer aliasing issues). So, at least when k is known at compile-time, the compiler should IMHO keep the activity to within registers.

And yet - GCC doesn't: With the extra code

int foo1()
{
  int x = 3;
  char c = 1;
  replace_bytes(x,c,1);
  return x;
}

we get (at maximum optimization):

foo1():
        mov     DWORD PTR [rsp-4], 3
        mov     BYTE PTR [rsp-3], 1
        mov     eax, DWORD PTR [rsp-4]
        ret

This, while clang _does_ optimize fully and has foo1() simply return 259 (= 256+3).

Even if we make k a template parameter - it doesn't help.


---


### compiler : `gcc`
### title : `Poor optimised codegen for memmove() back on top of oneself`
### open_at : `2019-04-29T20:23:58Z`
### last_modified_date : `2019-05-02T13:17:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90285
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
The following code produces poor optimised codegen on trunk GCC at the time of writing (2019-04-29):

// Reinterprets a T into its array of bytes
// Currently defined behaviour in C++ 20 for
// trivially copyable types only. The proposal
// would be that this would become defined 
// behaviour for most possible C++ types.
template<class T>
constexpr inline byte_array_ref<T> detach_cast(T &v) noexcept
{
    byte_array_ref<T> ret = reinterpret_cast<byte_array_ref<T>>(v);
    byte temp[sizeof(T)];

    // Reinterpret bytes by copying (not UB for TC types)
    memmove(temp, &v, sizeof(T));

    // Put reinterpreted bytes back. This avoids the UB
    // of reinterpret casting without creating new objects.
    memmove(ret, temp, sizeof(T));
    return ret;
}

You can see GCC's codegen here (it does two copies of 40Kb): https://godbolt.org/z/sJWSc1

You can see clang's codegen here (which is optimal, nothing is copied): https://godbolt.org/z/ou8VFT

I think GCC ought to not perform memory copies for the above code sequence.

Niall


---


### compiler : `gcc`
### title : `-O3 vectorization gets worse when code is inlined with respect to restrict`
### open_at : `2019-05-01T20:03:10Z`
### last_modified_date : `2023-08-05T05:02:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90304
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
The following code will get much better assembly if it is not in main():

for (int i = 0; i < 1024; ++i)
    result[i] = first[i] * second[i];

It simply multiplies two integers and stores the result.

When this code is moved into main(), it appears part of the loop no longer gets vectorized and uses imul instead.

This does not happen when the loop is moved to it's own function.

The two left panes contain the assembly of both versions of this code: https://godbolt.org/z/pNdbE5

NOTE: the noinline attribute has no effect on this issue, it is simply to make the assembly more readable.


---


### compiler : `gcc`
### title : `-Wuninitialized only at -O1, not at -O2`
### open_at : `2019-05-02T02:39:27Z`
### last_modified_date : `2021-04-13T20:24:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90307
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 46271
testcase

Starting at 00ee3e3e4fea0457ef84bada636ae26ffcc95fff -Wall -O1 on the attached testcase will print

distributed_test.ii:27:24: warning: ‘*((void*)& s +8)’ is used uninitialized in this function [-Wuninitialized]

But the warning goes away at -O2 an higher. Should this perhaps be in -Wmaybe-uninitialized?


---


### compiler : `gcc`
### title : `powerpc should convert equivalent sequences to vec_sel()`
### open_at : `2019-05-03T00:25:48Z`
### last_modified_date : `2023-09-04T18:02:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90323
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Something like this:

xi = xi & ~is_subnormal;
xi |= subnormal & is_subnormal;

should be converted to:

xi = sel_vec(xi, subnormal, is_subnormal);


---


### compiler : `gcc`
### title : `Not optimal code when compiling switch-case for size, code increase +35%`
### open_at : `2019-05-04T05:25:17Z`
### last_modified_date : `2019-05-14T10:45:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90340
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.1.0`
### severity : `normal`
### contents :
Created attachment 46284
Testcase with example code <vsprintf.c> from Linux C library

When compiling some old Linux kernel library code for vsnprintf, the code generated seems not optimal, and code size increased almost 17% for Cortex.M.

This was starting with gcc-9.1.0, previous versions did not show this.

(Generally when testing against CSiBE the overall average code size increased with gcc-9.1.0 compared to previous version for the first time since gcc-4.6.0)
http://gcc.hederstierna.com/csibe/

Attached stripped example file from Linux library.
Compliled with -Os (makefile attached)

Gcc-8.2.0 generated more compact code size 806 bytes,
Gcc-9.1.0 generated some large switch table code size 942 bytes.
Difference is +136 bytes (+16.9%).


---


### compiler : `gcc`
### title : `too pessimistic check whether pointer may alias a local variable`
### open_at : `2019-05-04T16:14:03Z`
### last_modified_date : `2021-04-16T09:23:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90345
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `9.1.0`
### severity : `normal`
### contents :
Consider the following example (reduced from a real program):

#include <cstdlib>
#include <cstdint>

struct big_integer
{
    void push_back(uint32_t const&);
    size_t size;
    uint32_t* digits;
};

big_integer& operator*=(big_integer& a, uint32_t b)
{
    uint64_t const BASE = 1ull << 32;

    uint32_t carry = 0;
    for (size_t i = 0; i != a.size; i++)
    {
        uint64_t sum = 1ull * a.digits[i] * b + carry;
        carry = static_cast<uint32_t>(sum / BASE);
        a.digits[i] = static_cast<uint32_t>(sum % BASE);
    }

    if (carry)
    {
        a.push_back(carry);
        //a.push_back(uint32_t(carry));
    }

    return a;
}

GCC 9.1 compiles the inner loop to this:
.L9:
        mov     esi, DWORD PTR [rsp+12]      ; load carry
.L5:
        mov     edx, DWORD PTR [rcx]
        add     rcx, 4
        imul    rdx, r8
        add     rdx, rsi
        mov     rsi, rdx
        shr     rsi, 32
        mov     DWORD PTR [rsp+12], esi      ; store carry
        mov     DWORD PTR [rcx-4], edx
        cmp     r9, rcx
        jne     .L9

As one can see carry is spilled to stack and it is loaded and stored at each iteration of the loop. Loading and storing carry at each iteration is not needed: it is a local variable and its address is not taken.

My guess is that GCC believes that it escapes because of the push_back after the loop. At least if I make a copy of carry before push_back'ing it (as shown in the comment) the problem goes away.

I think that alias analysis can be improved here: carry may not alias a.digits[i] because it escapes only after the loop.


---


### compiler : `gcc`
### title : `Missed optimization for variables initialized to 0.0`
### open_at : `2019-05-05T19:39:19Z`
### last_modified_date : `2022-03-21T21:58:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90356
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `enhancement`
### contents :
For the following example:

float doit(float k){
    float c[2]={0.0};
    c[1]+=k;
    return c[0]+c[1];
}

the resulting assembler (-O2) is (https://gcc.godbolt.org/z/sSi9OC):

doit:
        pxor    %xmm1, %xmm1
        addss   %xmm1, %xmm0
        addss   %xmm1, %xmm0
        ret

but should be more like:

doit:                                   # 
        pxor    %xmm1, %xmm1  ; or maybe xorps
        addss   %xmm1, %xmm0
        retq


because c[0] is 0.0.


---


### compiler : `gcc`
### title : `[9/10 regression][MIPS] New FAIL: gcc.c-torture/execute/20080502-1.c  -O0  start with r269880`
### open_at : `2019-05-06T08:26:22Z`
### last_modified_date : `2019-07-03T03:35:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90357
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 46300
preprocessed .i file

Hi:

There are some regressions start with r269880. The r269880 try to forward SRC
into the next instruction. Take 20080502-1.c as a example, it fails cause they use the same reg between src and dest.

good asm:
 81         ld      $2,%got_page(.LC1)($28)
 82         ld      $3,%got_ofst(.LC1+8)($2)
 83         ld      $2,%got_ofst(.LC1)($2)
 84         dmtc1   $2,$f12
 85         dmtc1   $3,$f13

after r269880:
 81         ld      $2,%got_page(.LC1)($28)
 82         ld      $3,%got_ofst(.LC1+8)($2)
 83         ld      $2,%got_ofst(.LC1)($2)
 84         ldc1    $f12,%got_ofst(.LC1)($2)
 85         ldc1    $f13,%got_ofst(.LC1+8)($2)

The line 83 $2 is dead, line 84 get the wrong address.

build cmd:
cc1 -fpreprocessed 20080502-1.i -mel -quiet -dumpbase 20080502-1.c -march=mips64r2 -mabi=64 -mllsc -mips64r2 -mno-shared -auxbase 20080502-1 -O0 -w -version -fdiagnostics-color=never -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -o 20080502-1.s


---


### compiler : `gcc`
### title : `Better alias analysis based on libc functions with arguments which cannot alias`
### open_at : `2019-05-07T07:42:07Z`
### last_modified_date : `2019-05-07T09:28:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90373
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
bool foo(int *a, int *b, unsigned n) {
    memcpy(a, b, n);
    return a == b;
}

GCC trunk X86-64 -O3
foo(int*, int*, unsigned int):
        push    rbx
        mov     edx, edx
        mov     rbx, rsi
        call    memcpy
        cmp     rax, rbx
        pop     rbx
        sete    al
        ret

if a and b aliases, it would be UB to call memcpy since "The memory areas must not overlap.". Currently GCC emits "a == b", but possibly, info that a cannot alias b in range n could be propagated more.


Same for strcpy
"The strings may not overlap, and the destination string dest must be large enough to receive the copy."


---


### compiler : `gcc`
### title : `>= -O2 suddenly generates code`
### open_at : `2019-05-09T11:58:05Z`
### last_modified_date : `2021-09-05T16:44:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90408
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `normal`
### contents :
Created attachment 46324
example code

The attached code should produce an empty main(), but does produce code with optimizations >= -O2. It appears to be related to the switch-case.

-std=c++11 -O1
no code generated

-std=c++11 -O2
code generated in GCC 5.5.0 and GCC >= 7.1

Also using -Os produces more code than -O1.


---


### compiler : `gcc`
### title : `std::move[_backward] could be more optimized for deque iterators`
### open_at : `2019-05-09T12:07:41Z`
### last_modified_date : `2020-06-30T20:00:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90409
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.1.0`
### severity : `enhancement`
### contents :
The standard library currently contains optimized versions of std::move and std::move_backward for std::deque iterators, which move everything block per block. However those overloads only work when both the input and output iterators are std::deque iterators, and not when the types of iterators differ.

I noticed that lately when checking the performance of merge algorithms on std::deque algorithms: I expected memmove to be called when calling std::move to move the contents of a std::deque to a temporary buffer (represented by a simple pointer) and when moving the elements back from the buffer to the std::deque, but while memmove was called as expected when using libc++, it wasn't when using libstdc++. I looked at the standard library code and that's when I realized that the libstdc++ overloads for deque iterators didn't accept mixed iterators.

Would it be possible to add std::[_backward] overloads to take this case into account? If I'm not mistaken it should improve std::inplace_merge and std::stable_sort out of the box for std::deque<T> iterators when T is trivially copyable.


---


### compiler : `gcc`
### title : `memcpy into vector builtin not optimized`
### open_at : `2019-05-10T11:27:43Z`
### last_modified_date : `2021-09-04T21:02:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90424
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.1.0`
### severity : `enhancement`
### contents :
Testcase (cf. https://godbolt.org/z/LsKcii):

template <class T>
using V [[gnu::vector_size(16)]] = T;

template <class T, unsigned M = sizeof(V<T>)>
V<T> load(const void *p) {
  using W = V<T>;
  W r;
  __builtin_memcpy(&r, p, M);
  return r;
}

// movq or movsd
template V<char> load<char, 8>(const void *);     // bad
template V<short> load<short, 8>(const void *);   // bad
template V<int> load<int, 8>(const void *);       // bad
template V<long> load<long, 8>(const void *);     // good
template V<float> load<float, 8>(const void *);   // bad
template V<double> load<double, 8>(const void *); // good (movsd?)

// movd or movss
template V<char> load<char, 4>(const void *);   // bad
template V<short> load<short, 4>(const void *); // bad
template V<int> load<int, 4>(const void *);     // good
template V<float> load<float, 4>(const void *); // good

All of these partial loads should be translated to a single mov[qd] or movs[sd] instruction. But most of them are not.


---


### compiler : `gcc`
### title : `missing "sign flipping" optimization`
### open_at : `2019-05-10T15:14:10Z`
### last_modified_date : `2021-12-23T00:53:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90427
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `enhancement`
### contents :
This test case comes from this blog post: https://nfrechette.github.io/2019/05/08/sign_flip_optimization/
(which also says that clang 8 performs this optimization).

Consider

float foo_ref(float value) {
  value = value * 0.5f;   // mulss value, 0.5f
                          // movss tmp, 1.0f
  return 1.0f - value;    // subss tmp, value
}

float foo_ref2(float value) {
  value = value * -0.5f;  // mulss value, -0.5f
  return value + 1.0f; // addss value, 1.0f
}


According to the post, these are equivalent.  However, gcc
generates different code for them, with the latter being better.

The comments for the first function seem to omit an
instruction that gcc emits, making that version even worse:

	movss	.LC0(%rip), %xmm1
	mulss	%xmm0, %xmm1
	movss	.LC1(%rip), %xmm0
	subss	%xmm1, %xmm0

However the comments in the second one are correct:

	mulss	.LC2(%rip), %xmm0
	addss	.LC1(%rip), %xmm0


Tested with git master from today, using gcc -O2, on x86-64 Fedora 29.

I just made a guess at which component to use.


---


### compiler : `gcc`
### title : `POINTER_DIFF_EXPR in vectorizer prologue`
### open_at : `2019-05-11T11:47:16Z`
### last_modified_date : `2021-12-13T06:29:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90433
### status : `UNCONFIRMED`
### tags : `missed-optimization, wrong-code`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
I am picking a random code that gets vectorized to illustrate the issue

#include <vector>
#include <memory>
#include <new>
inline void* operator new(std::size_t n){return malloc(n);}
inline void operator delete(void*p){free(p);}
typedef std::unique_ptr<int> T;
typedef std::vector<T> V;
void f(V&v){v.reserve(1024);}

Compiling with -O3 on x86_64, the vectorizer produces

  <bb 3> [local count: 354334802]:
  _9 = MEM[(struct unique_ptr * *)v_2(D) + 8B];
  _10 = _9 - _4;
  _24 = malloc (8192);
  if (_4 == _9)
    goto <bb 5>; [11.00%]
  else
    goto <bb 15>; [89.00%]

  <bb 15> [local count: 315357974]:
  _26 = (unsigned long) _9;
  _16 = (unsigned long) _4;
  _13 = _26 - _16; ****************
  _23 = _13 + 18446744073709551608;
  _11 = _23 /[ex] 8;
  _3 = _11 & 2305843009213693948;
  _38 = _3 != 0;
  _39 = _4 + 15;
  _40 = _39 - _24; ****************
  _41 = (sizetype) _40;
  _42 = _41 > 30;
  _43 = _38 & _42;
  if (_43 != 0)
    goto <bb 16>; [80.00%]
  else
    goto <bb 17>; [20.00%]

  <bb 16> [local count: 252286381]:
  _55 = (unsigned long) _9;
  _56 = (unsigned long) _4;
  _57 = _55 - _56; ****************
  _58 = _57 + 18446744073709551608;
  _59 = _58 /[ex] 8;
  _60 = _59 & 2305843009213693951;
  niters.75_54 = _60 + 1;
  bnd.76_71 = niters.75_54 >> 1;

Note the lines marked with stars. To compute the size of the memory region, it casts the pointers to unsigned long and subtracts those, whereas it could perfectly well use POINTER_DIFF_EXPR. On the other hand, (for the aliasing check?) it subtracts _39 and _24 which are completely independent pointers (one was freshly returned by malloc) using POINTER_DIFF_EXPR, while IIRC this is only supposed to be used for subtraction of pointers into the same object/region.


---


### compiler : `gcc`
### title : `Redundant size checking in vector`
### open_at : `2019-05-12T06:40:01Z`
### last_modified_date : `2020-06-21T18:03:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90436
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.0`
### severity : `normal`
### contents :
(split from bug 87544)
#include <vector>
void f(std::vector<int>&v){v.push_back(42);}

Compiled with -O3 -fdump-tree-optimized, one can see for _M_realloc_insert (slightly edited, _40 is the size)

  if (_40 == 2305843009213693951)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 4> [local count: 1073312328]:
  if (_40 == 0)
    goto <bb 15>; [34.00%]
  else
    goto <bb 5>; [66.00%]

  <bb 5> [local count: 708386133]:
  __len_45 = _40 * 2;
  if (_40 > __len_45)
    goto <bb 15>; [53.03%]
  else
    goto <bb 6>; [46.97%]

  <bb 6> [local count: 438358647]:
  if (__len_45 != 0)
    goto <bb 14>; [0.00%]
  else
    goto <bb 7>; [100.00%]

  <bb 14> [local count: 0]:
  _46 = MIN_EXPR <__len_45, 2305843009213693951>;
  _10 = _46 * 4;


That is, we first check if size is exactly max_size, then if it is 0, then if newsize=2*size overflows, then if newsize is 0, and finally we take the min of newsize and max_size. That's more than twice as many checks as needed.


---


### compiler : `gcc`
### title : `Overflow detection too late for VRP`
### open_at : `2019-05-12T09:08:50Z`
### last_modified_date : `2021-11-22T09:06:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90437
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
bool f(unsigned a, unsigned b, unsigned*c){
  if(a>10)__builtin_unreachable();
  if(b>10)__builtin_unreachable();
  *c = a + b;
  return *c < a;
}

In the .optimized dump, we still have

  _9 = .ADD_OVERFLOW (a_2(D), b_3(D));
  _1 = REALPART_EXPR <_9>;
  _8 = IMAGPART_EXPR <_9>;
  *c_5(D) = _1;
  _7 = _8 != 0;
  return _7;

We have code in VRP to simplify IFN_ADD_OVERFLOW when the arguments have interesting ranges, but this call only appears in the widening_mul pass. In this particular case, expand optimizes the code properly, but in general _8 would be used in a GIMPLE_COND and we would have a number of further optimizations to make.

We could either have an earlier pass than widening_mul detect a+b<a and introduce IFN_ADD_OVERFLOW, or VRP could detect a+b<a and optimize it as if it was IFN_ADD_OVERFLOW (without introducing it).


---


### compiler : `gcc`
### title : `Missed opportunities to use adc (worse when -1 is involved)`
### open_at : `2019-05-13T00:27:00Z`
### last_modified_date : `2021-12-12T20:36:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90447
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `enhancement`
### contents :
The following are three attempts to get gcc to generate adc instructions from C++:

#include <x86intrin.h>

unsigned constexpr X = 0;

unsigned f1(unsigned a, unsigned b) {
    b += a;
    auto c = b < a;
    b += X + c;
    return b;
}

unsigned f2(unsigned a, unsigned b) {
    b += a;
    b += X + (b < a);
    return b;
}

unsigned f3(unsigned a, unsigned b) {
    b += a;
    unsigned char c = b < a;
    _addcarry_u32(c, b, X, &b);
    return b;
}

The 3 functions above (-O3 -std=c++17) generate:

  addl    %edi, %esi
  movl    %esi, %eax
  adcl    $0, %eax
  ret

This is great and I would expect that changing X would only affect the immediate value and nothing more. I was wrong. Changing X to 1, makes f1 and f3 change as I expected but f2 becomes:

f2(unsigned int, unsigned int):
  xorl    %eax, %eax
  addl    %edi, %esi
  setc    %al
  addl    $1, %eax
  addl    %esi, %eax
  ret

I thought I could blame "b += X + (b < a);" for being undefined behaviour. However, I believe that, at least in c++17 this is not the case given the addition of this sentence:

    "The right operand is sequenced before the left operand."

to [expr.ass]. As far as Standard C++ is concerned, I expect f1 to be equivalent to f2.

Things got worse when X == -1:

f1(unsigned int, unsigned int):
  xorl %eax, %eax
  addl %edi, %esi
  setc %al
  leal -1(%rax,%rsi), %eax
  ret
f2(unsigned int, unsigned int):
  xorl %eax, %eax
  addl %edi, %esi
  setnc %al
  subl %eax, %esi
  movl %esi, %eax
  ret
f3(unsigned int, unsigned int):
  addl %esi, %edi
  movl $-1, %eax
  setc %dl
  addb $-1, %dl
  adcl %edi, %eax
  ret

No adc whatsoever. I'm not an assembly guy but if I understand f3 correctly, "setc %dl / addb $-1, dl" is simply storing the CF in dl and adding dl to 0xff to force CF to get the same value it already had before instruction setc was executed. Basically, this is a convoluted-register-wasteful nop.

I thought the problem could be related to issue [1] but this one has already being resolved in trunk where this issue also happens and -fno-split-paths doesn't seem to change anything.

The example in godbold is https://godbolt.org/z/3GUyLj but if you play with the site's settings (particularly, lib.f) be aware of their issue [2].

[1] https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88797 but this 
[2] https://github.com/mattgodbolt/compiler-explorer/issues/1377


---


### compiler : `gcc`
### title : `Inefficient vector construction from pieces`
### open_at : `2019-05-14T11:35:00Z`
### last_modified_date : `2021-07-19T04:45:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90460
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Split out from PR90424

template <class T>
using V [[gnu::vector_size(16)]] = T;

template <class T, unsigned... I>
V<T> load(const void *p) {
  const T* q = static_cast<const T*>(p);
  V<T> r = {q[I]...};
  return r;
}

// movq or movsd
template V<char  > load<char  , 0,1,2,3,4,5,6,7>(const void *);
template V<short > load<short , 0,1,2,3>(const void *);
template V<int   > load<int   , 0,1>(const void *);
template V<long  > load<long  , 0>(const void *);
template V<float > load<float , 0,1>(const void *);
template V<double> load<double, 0>(const void *);

// movd or movss
template V<char > load<char , 0,1,2,3>(const void *);
template V<short> load<short, 0,1>(const void *);
template V<int  > load<int  , 0>(const void *);
template V<float> load<float, 0>(const void *);


ends up with IL like

load<int, 0, 1> (const void * p)
{
  V r;
  int _1;
  int _2;

  <bb 2> [local count: 1073741824]:
  _1 = MEM[(const int *)p_3(D)];
  _2 = MEM[(const int *)p_3(D) + 4B];
  r_5 = {_1, _2};
  return r_5;

which looks like a job for bswap.


---


### compiler : `gcc`
### title : `input to ptest not optimized`
### open_at : `2019-05-15T10:53:54Z`
### last_modified_date : `2021-08-21T22:36:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90483
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
The (V)PTEST instruction of SSE4.1/AVX produces ZF = `(a & b) == 0` and CF = `(~a & b) == 0`. Generic usage of PTEST simply sets `b = ~__m128i()` (or `~__m256i()`), i.e. tests `a` and `~a` for having only zero bits. (cf. _mm_test_all_ones)

Consequently, if `a` is the result of a vector comparison which only depends on a bitmask, the compare instruction can be elided and the `~__m128i()` mask replaced with the corresponding bitmask.

Examples:

// test sign bit
bool bad(__v16qu x) {
  return __builtin_ia32_ptestz128(~__v16qu(), x > 0x7f);
}

Since x > 0x7f can be rewritten as a test for the sign bit, we can optimize to (with 0x808080... at LC0):
        vptest .LC0(%rip), %xmm0
        sete %al
        ret

// test for zero
bool bad2(__v16qu x) {
  return __builtin_ia32_ptestz128(~__v16qu(), x == 0);
}

This equivalent to testing scalars for 0, i.e. we can optimize to:
        vptest %xmm0, %xmm0
        sete %al
        ret

// test for certain bits
bool bad3(__v16qu x, __v16qu k) {
  return __builtin_ia32_ptestz128(~__v16qu(), (x & k) == 0);
}

With the above transformation we already get PTEST(x&k, x&k) which can consequently be reduced to PTEST(x, k):
        vptest %xmm0, %xmm1
        sete %al
        ret

Further optimization of e.g. `(x & ~k) == 0` using CF instead of ZF might also be interesting.

And of course, these transformations apply to all vector types, not just __v16qu.


---


### compiler : `gcc`
### title : `optimize SSE & AVX char compares with subsequent movmskb [negation]`
### open_at : `2019-05-15T14:38:34Z`
### last_modified_date : `2021-12-20T03:28:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90487
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.1.0`
### severity : `enhancement`
### contents :
Testcase (cf. https://godbolt.org/z/7NiU7O):

#include <x86intrin.h>

template <typename T, size_t N>
using V [[gnu::vector_size(N)]] = T;

int good0(V<unsigned char, 16> a) { return 0xffff ^ _mm_movemask_epi8   (reinterpret_cast<__m128i>(a)); }
int good1(V<unsigned char, 16> a) { return _mm_movemask_epi8   (reinterpret_cast<__m128i>(!a)); }

// the following should be optimized to either good0 (prefer e.g. if compared against 0xffff) or good1:
int f0(V<unsigned char, 16> a) { return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a <= 0x7f)); }
int f1(V<unsigned char, 16> a) { return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a <  0x80)); }
int f0(V<  signed char, 16> a) { return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a >=  0)); }
int f1(V<  signed char, 16> a) { return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a >  -1)); }
int f0(V<         char, 16> a) { return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a >=  0)); }
int f1(V<         char, 16> a) { return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a >  -1)); }

#ifdef __AVX2__
int good0(V<unsigned char, 32> a) { return 0xffffffff ^ _mm256_movemask_epi8   (reinterpret_cast<__m256i>(a)); }
int good1(V<unsigned char, 32> a) { return _mm256_movemask_epi8   (reinterpret_cast<__m256i>(!a)); }

// the following should be optimized to either good0 (prefer e.g. if compared against 0xffffffff) or good1:
int f0(V<unsigned char, 32> a) { return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a <= 0x7f)); }
int f1(V<unsigned char, 32> a) { return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a <  0x80)); }
int f0(V<  signed char, 32> a) { return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a >=  0)); }
int f1(V<  signed char, 32> a) { return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a >  -1)); }
int f0(V<         char, 32> a) { return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a >=  0)); }
int f1(V<         char, 32> a) { return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a >  -1)); }
#endif

Compile with -O2 and either -mavx2 or -msse2. This PR is simply the negation of PR88152. I failed to cover these cases in the other PR and they are just as likely to appear as the ones in PR88152.


---


### compiler : `gcc`
### title : `simple operation with unsigned long integer and conversion to float not vectorized`
### open_at : `2019-05-15T16:35:46Z`
### last_modified_date : `2023-02-15T20:32:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90491
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.1`
### severity : `normal`
### contents :
snip

#include <array>
#include <iostream>

int main(const int argc, const char** argv)
{
	using value_type = float; // or double
	using array_type = std::array<value_type, 16>; // size does not matter

	array_type	a;

	/*
	 * this loop not vectorized
	 * explicite conversion a[i] = argc + int(i) works
	 */
	for (size_t i=0; i<a.size(); ++i)
		a[i] = argc + i;

	for (size_t i=0; i<a.size(); ++i)
		std::cout<<(a[i])<<' ';

	return EXIT_SUCCESS;
}

gcc 8.3.1 and higher (and lower?) -O3 -march=native (Ryzen 7 2700)

COLLECT_GCC=gcc-8
COLLECT_LTO_WRAPPER=/usr/lib64/gcc/x86_64-suse-linux/8/lto-wrapper
OFFLOAD_TARGET_NAMES=hsa:nvptx-none
Target: x86_64-suse-linux
Configured with: ../configure --prefix=/usr --infodir=/usr/share/info --mandir=/usr/share/man --libdir=/usr/lib64 --libexecdir=/usr/lib64 --enable-languages=c,c++,objc,fortran,obj-c++,ada,go --enable-offload-targets=hsa,nvptx-none=/usr/nvptx-none, --without-cuda-driver --enable-checking=release --disable-werror --with-gxx-include-dir=/usr/include/c++/8 --enable-ssp --disable-libssp --disable-libvtv --disable-cet --disable-libcc1 --enable-plugin --with-bugurl=http://bugs.opensuse.org/ --with-pkgversion='SUSE Linux' --with-slibdir=/lib64 --with-system-zlib --enable-libstdcxx-allocator=new --disable-libstdcxx-pch --enable-version-specific-runtime-libs --with-gcc-major-version-only --enable-linker-build-id --enable-linux-futex --enable-gnu-indirect-function --program-suffix=-8 --without-system-libunwind --enable-multilib --with-arch-32=x86-64 --with-tune=generic --build=x86_64-suse-linux --host=x86_64-suse-linux
Thread model: posix
gcc version 8.3.1 20190226 [gcc-8-branch revision 269204] (SUSE Linux)


---


### compiler : `gcc`
### title : `simple array-copy not use available simd-registers`
### open_at : `2019-05-15T17:00:24Z`
### last_modified_date : `2019-12-20T18:13:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90492
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
snip

#include <array>
#include <iostream>

int main(const int argc, const char** argv)
{
	using value_type = int; // type does not matter
	using array_type = std::array<value_type, 32>;

	array_type	a, b;

	// simple init
	for (size_t i=0; i<a.size(); ++i)
		a[i] = argc;

	/*
	 * copy's use only sse-registers and never higher
	 */
	b = a;

	// or

	for (size_t i=0; i<a.size(); ++i)
		b[i] = a[i];



	for (size_t i=0; i<a.size(); ++i)
		std::cout<<(b[i])<<' ';

	return EXIT_SUCCESS;
}

gcc 8.3.1 and higher (and lower?) -O3

COLLECT_GCC=gcc-8
COLLECT_LTO_WRAPPER=/usr/lib64/gcc/x86_64-suse-linux/8/lto-wrapper
OFFLOAD_TARGET_NAMES=hsa:nvptx-none
Target: x86_64-suse-linux
Configured with: ../configure --prefix=/usr --infodir=/usr/share/info --mandir=/usr/share/man --libdir=/usr/lib64 --libexecdir=/usr/lib64 --enable-languages=c,c++,objc,fortran,obj-c++,ada,go --enable-offload-targets=hsa,nvptx-none=/usr/nvptx-none, --without-cuda-driver --enable-checking=release --disable-werror --with-gxx-include-dir=/usr/include/c++/8 --enable-ssp --disable-libssp --disable-libvtv --disable-cet --disable-libcc1 --enable-plugin --with-bugurl=http://bugs.opensuse.org/ --with-pkgversion='SUSE Linux' --with-slibdir=/lib64 --with-system-zlib --enable-libstdcxx-allocator=new --disable-libstdcxx-pch --enable-version-specific-runtime-libs --with-gcc-major-version-only --enable-linker-build-id --enable-linux-futex --enable-gnu-indirect-function --program-suffix=-8 --without-system-libunwind --enable-multilib --with-arch-32=x86-64 --with-tune=generic --build=x86_64-suse-linux --host=x86_64-suse-linux
Thread model: posix
gcc version 8.3.1 20190226 [gcc-8-branch revision 269204] (SUSE Linux)


---


### compiler : `gcc`
### title : `attribute((optimize(3))) not overriding -Os`
### open_at : `2019-05-20T19:06:33Z`
### last_modified_date : `2019-08-23T06:06:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90552
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
I test-compiled ( https://gcc.godbolt.org/z/8bhbNa ):

__attribute((optimize(3))) int div(int X) { return X/3; }

with -O{0,1,2,3,s}, expecting to get the same assembly in all cases, but __attribute((optimize(3))) is failing to override the last case, namely -Os.

(I'd like the function to not use the idiv instruction even if the rest of the file is compiled with -Os).

Please correct me if I'm wrong to expect `__attribute((optimize(3)))` to be able to override `-Os`.

This behavior appears to exist on all gcc versions.


---


### compiler : `gcc`
### title : `GCC bad optimization on recursive functions`
### open_at : `2019-05-21T23:04:57Z`
### last_modified_date : `2019-05-21T23:26:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90567
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
consider code:

int fibo (int n)
{
    if (n == 1 || n == 2)
        return 1;
    else
        return fibo(n - 1) + fibo(n - 2);
}
int main()
{
    printf ("%d\n", fibo (45));

    return 0;
}

with flags: -m64 -flto -march=native -O3

GCC-7
real	0m3,066s
user	0m3,065s
sys	0m0,000s

GCC-8
real	0m4,868s
user	0m4,864s
sys	0m0,004s

GCC-9
real	0m5,015s
user	0m5,010s
sys	0m0,004s


---


### compiler : `gcc`
### title : `stack protector should use cmp or sub, not xor, to allow macro-fusion on x86`
### open_at : `2019-05-22T01:03:17Z`
### last_modified_date : `2019-05-24T09:22:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90568
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
cmp/jne is always at least as efficient as xor/jne, and more efficient on CPUs that support macro-fusion of compare and branch.  Most support cmp/jne fusion (including all mainstream Intel and AMD, not low-power), but none support xor/jne fusion.

void foo() {
    volatile int buf[4];
    buf[1] = 2;
}

gcc trunk on Godbolt, but same code-gen all the way back to gcc4.9

foo:
        subq    $40, %rsp
        movq    %fs:40, %rax
        movq    %rax, 24(%rsp)
        xorl    %eax, %eax
        movl    $2, 4(%rsp)
        movq    24(%rsp), %rax
        xorq    %fs:40, %rax              ## This insn should be CMP
        jne     .L5
        addq    $40, %rsp
        ret
.L5:
        call    __stack_chk_fail

As far as I can tell, the actual XOR result value in RAX is not an input to __stack_chk_fail because gcc sometimes uses a different register.

Therefore we don't need it, and can use any other way to check for equality.

If we need to avoid "leaking" the canary value in a register, we can use SUB, otherwise CMP is even better and can macro-fuse on more CPUs.

Only Sandybridge-family can fuse SUB/JCC.  (And yes, it can fuse even with a memory-source and a segment override prefix.  SUB %fs:40(%rsp), %rax / JNE  is a single uop on Skylake; I checked this with perf counters in an asm loop.)

AMD can fuse any TEST or CMP/JCC, but only those instructions (so SUB is as bad as XOR for AMD).  See Agner Fog's microarch PDF.

----

Linux test program (NASM) that runs  sub (mem), %reg with an FS prefix to prove that it does macro-fuse and stays micro-fused as a single uop:


default rel
%use smartalign
alignmode p6, 64

global _start
_start:

cookie equ 12345
    mov  eax, 158       ; __NR_arch_prctl
    mov  edi, 0x1002    ; ARCH_SET_FS
    lea  rsi, [buf]
    syscall
   ;  wrfsbase   rsi    ; not enabled by the kernel
    mov  qword [fs: 0x28], cookie

    mov     ebp, 1000000000

align 64
.loop:
    mov   eax, cookie
    sub   rax, [fs: 0x28]
    jne   _start
    and   ecx, edx

    dec ebp
    jnz .loop
.end:

    xor edi,edi
    mov eax,231   ; __NR_exit_group
    syscall       ; sys_exit_group(0)


section .bss
align 4096
buf:    resb 4096



nasm -felf64  branch-fuse-mem.asm &&
ld -o branch-fuse-mem  branch-fuse-mem.o
to make a static executable

taskset -c 3 perf stat -etask-clock:u,context-switches,cpu-migrations,page-faults,cycles:u,branches:u,instructions:u,uops_issued.any:u,uops_executed.thread:u -r2 ./branch-fuse-mem

On my i7-6700k

 Performance counter stats for './branch-fuse-mem' (2 runs):

            240.78 msec task-clock:u              #    0.999 CPUs utilized            ( +-  0.23% )
                 2      context-switches          #    0.010 K/sec                    ( +- 20.00% )
                 0      cpu-migrations            #    0.000 K/sec                  
                 3      page-faults               #    0.012 K/sec                  
     1,000,764,258      cycles:u                  #    4.156 GHz                      ( +-  0.00% )
     2,000,000,076      branches:u                # 8306.384 M/sec                    ( +-  0.00% )
     6,000,000,088      instructions:u            #    6.00  insn per cycle           ( +-  0.00% )
     4,000,109,615      uops_issued.any:u         # 16613.222 M/sec                   ( +-  0.00% )
     5,000,098,334      uops_executed.thread:u    # 20766.367 M/sec                   ( +-  0.00% )

          0.240935 +- 0.000546 seconds time elapsed  ( +-  0.23% )

Note 1.0 billion cycles (1 per iteration), and 4B fused-domain uops_issued.any, i.e. 4 uops per loop iteration.

(5 uops *executed* is because one of those front-end uops has a load micro-fused).

Changing SUB to CMP has no effect.

With SUB changed to XOR, the loop takes 1.25 cycles per iteration, and the front-end issues 5 uops per iteration.  Other counters are the same.

Skylake's pipeline is 4-wide, like all Intel since Core2, so an extra uop for the front-end creates a bottleneck.

------

On Intel pre Haswell, the decoders will only make at most 1 fusion per decode group, so you may need to make the loop larger to still get fusion.  Or use this as the loop-branch, e.g. with a  1  in memory

   sub  rax, [fs: 0x28]
   jnz  .loop

or with a 0 in memory, sub or cmp or xor will all set flags according to the register being non-zero.  But sub or xor will introduce an extra cycle of latency on the critical path for the loop counter.


---


### compiler : `gcc`
### title : `Missed optimization opportunity when returning function pointers based on run-time boolean`
### open_at : `2019-05-22T12:25:47Z`
### last_modified_date : `2023-08-24T03:30:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90571
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Given the following two functions:

    int f() { return 0; }
    int g() { return 1; }

And the following code to invoke one of them depending on a boolean `b`:
    
    int t0(bool b) { return (b ? &f : &g)(); }
    int t1(bool b) { return b ? f() : g(); }
    int t2(bool b) { return b ? t0(true) : t0(false); }

Both `g++ (trunk)` and `clang++ (trunk)` with `-std=c++2a -Ofast -march=native` fail to optimize the following code:

    int main(int ac, char**) { return t0(ac & 1); }

Producing the following assembly:

>     main:
>       and edi, 1
>       mov eax, OFFSET FLAT:f()
>       mov edx, OFFSET FLAT:g()
>       cmove rax, rdx
>       jmp rax
> 

Invoking `t1` or `t2` (instead of `t0`) produces the following optimized assembly:

>     main:
>             mov     eax, edi
>             not     eax
>             and     eax, 1
>             ret

Everything can be reproduced live on **gcc.godbolt.org**: https://godbolt.org/z/gh7270


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Huge store forward stall due to vectorizer, missed CSE`
### open_at : `2019-05-22T19:01:10Z`
### last_modified_date : `2023-07-07T10:35:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90579
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.1`
### severity : `normal`
### contents :
loop/avx256 branch at

https://gitlab.com/x86-benchmarks/microbenchmark

shows huge store forward stall due to vectorizer in

---
extern double r[6];
extern double a[];

double
loop (int k, double x)
{
  int i;
  double t=0;
  for (i=0;i<6;i++)
    r[i] = x * a[i + k];
  for (i=0;i<6;i++)
    t+=r[5-i];
  return t;
}
---

when compiled with -O3 -march=skylake:

[hjl@gnu-cfl-1 microbenchmark]$ perf stat -e ld_blocks.store_forward ./event
loop: 229408

 Performance counter stats for './event':

                 1      ld_blocks.store_forward:u                                   

       0.000478529 seconds time elapsed

       0.000502000 seconds user
       0.000000000 seconds sys


[hjl@gnu-cfl-1 microbenchmark]$ perf stat -e ld_blocks.store_forward ./event-avx128
loop: 191390

 Performance counter stats for './event-avx128':

                 1      ld_blocks.store_forward:u                                   

       0.000526154 seconds time elapsed

       0.000507000 seconds user
       0.000000000 seconds sys


[hjl@gnu-cfl-1 microbenchmark]$ perf stat -e ld_blocks.store_forward ./event-avx256
loop: 1312864

 Performance counter stats for './event-avx256':

            30,001      ld_blocks.store_forward:u                                   

       0.000756643 seconds time elapsed

       0.000723000 seconds user
       0.000000000 seconds sys


[hjl@gnu-cfl-1 microbenchmark]$


---


### compiler : `gcc`
### title : `AArch64 stack-protector wastes an instruction on address-generation`
### open_at : `2019-05-22T22:37:19Z`
### last_modified_date : `2019-05-22T23:07:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90582
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
void protect_me() {
    volatile int buf[2];
    buf[1] = 3;
}

https://godbolt.org/z/xdlr5w AArch64 gcc8.2 -O3 -fstack-protector-strong

protect_me:
        stp     x29, x30, [sp, -32]!
        adrp    x0, __stack_chk_guard
        add     x0, x0, :lo12:__stack_chk_guard         ### this instruction
        mov     x29, sp                         # frame pointer even though -fomit-frame-pointer is part of -O3.  Goes away with explicit -fomit-frame-pointer

        ldr     x1, [x0]                        # copy the cookie
        str     x1, [sp, 24]
        mov     x1,0                            # and destroy the reg

        mov     w1, 3                           # right before it's already destroyed
        str     w1, [sp, 20]             # buf[1] = 3

        ldr     x1, [sp, 24]                    # canary
        ldr     x0, [x0]                        # key destroys the key pointer
        eor     x0, x1, x0
        cbnz    x0, .L5
        ldp     x29, x30, [sp], 32              # FP and LR save/restore (for some reason?)
        ret
.L5:
              # can the store of the link register go here, for backtracing?
        bl      __stack_chk_fail

A function that returns a global can embed the low 12 bits of the address into the load instruction.  AArch64 instructions are fixed-width, so there's no reason (AFAIK) not to do this.

f:
        adrp    x0, foo
        ldr     w0, [x0, #:lo12:foo]
        ret

I'm not an AArch64 performance expert; it's plausible that zero displacements are worth spending an extra instruction on for addresses that are used twice, but unlikely.

So we should be doing 

        adrp    x0, __stack_chk_guard
        ldr     x1, [x0, #:lo12:__stack_chk_guard]  # in prologue to copy cookie
        ... 
        ldr     x0, [x0, #:lo12:__stack_chk_guard]  # in epilogue to check cookie

This also avoids leaving an exact pointer right to __stack_chk_guard in a register, in case a vulnerable callee or code in the function body can be tricked into dereferencing it and leaking the cookie.  (In non-leaf functions, we generate the pointer in a call-preserved register like x19, so yes it will be floating around in a register for callees).

I'd hate to suggest destroying the pointer when copying to the stack, because that would require another adrp later.

Finding a gadget that has exactly the right offset (the low 12 bits of __stack_chk_guard's address) is a lot less likely than finding an  ldr from [x0].  Of course this will introduce a lot of LDR instructions with an #:lo12:__stack_chk_guard offset, but hopefully they won't be part of useful gadgets because they lead to writing the stack, or to EOR/CBNZ to __stack_chk_fail

----

I don't see a way to optimize canary^key == 0 any further, unlike x86-64 PR 90568.  I assume EOR / CBNZ is as at least as efficient as SUBS / BNE on all/most AArch64 microarchitectures, but someone should check.

----

-O3 includes -fomit-frame-pointer according to -fverbose-asm, but functions protected with -fstack-protector-strong still get a frame pointer in x29 (costing a MOV x29, sp instruction, and save/restore with STP/LDP along with x30.)

However, explicitly using -fomit-frame-pointer stops that from happening.  Is that a separate bug, or am I missing something?

----

Without stack-protector, the function is vastly simpler

protect_me:
        sub     sp, sp, #16
        mov     w0, 3
        str     w0, [sp, 12]
        add     sp, sp, 16
        ret

Does stack-protector really need to spill/reload x29/x30 (FP and LR)?  Bouncing the return address through memory seems inefficient, even though branch prediction does hide that latency.

Is that just so __stack_chk_fail can backtrace?  Can we move the store of the link register into the __stack_chk_fail branch, off the fast path?

Or if we do unconditionally store x30 (the link register), at least don't bother reloading it in a leaf function if register allocation didn't need to clobber it.  Unlike x86-64, the return address can't be attacked with buffer overflows if it stays safe in a register the whole function.

Obviously my test-case with a volatile array and no inputs at all is making -fstack-protector-strong look dumb by protecting a perfectly safe function.  IDK how common it is to have leaf functions with arrays or structs that just use them for some computation on function args or globals and then return, maybe after copying the array back to somewhere else.  A sort function might use a tmp array.

With -fstack-protector (not -strong), maybe instead of optimizing the store of the link register into the block that calls  __stack_chk_fail, we should use that as a sign that a leaf function which doesn't spill LR is not *itself* vulnerable to ROP attacks, and not stack-protect it.

-strong might still want to stack-protect to defend against overwriting the caller's stack frame, which might contain function pointers or classes with vtable pointers.  Or saved registers that are about to be restored.  Or to detect that something bad happened to it, even if it won't immediately result in a bad control transfer.

Or maybe not, and maybe -strong would also want to drop protection from leaf functions that don't (need to) spill LR.


----

The    mov     x1,0  in the prologue is redundant: mov  w1, 3 is about to destroy the value in x1 anyway, and it can't fault.  (Reporting this bug separately; it's separate and not AArch64-specific).


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] Spurious popcount emitted`
### open_at : `2019-05-23T11:30:18Z`
### last_modified_date : `2023-07-07T10:35:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90594
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The following testcase emits a popcount which computes the final pointer value. This is redundant given the loop already computes the pointer value. The popcount causes the code to be significantly larger and slower than previous GCC versions.

int *bad_popcount (unsigned x, int *p)
{
  for (; x != 0; )
    {
      int tmp = __builtin_ctz (x);
      x = x & (x - 1);
      *p++ = tmp;
    }
  return p;
}

GCC8:
        cbz     w0, .L2
.L3:
        rbit    w2, w0
        clz     w2, w2
        str     w2, [x1], 4
        sub     w2, w0, #1
        ands    w0, w0, w2
        bne     .L3
.L2:
        mov     x0, x1
        ret

GCC9:
	cbz	w0, .L12
	mov	x4, x1
	mov	w2, w0
.L11:
	rbit	w3, w2
	clz	w3, w3
	str	w3, [x4], 4
	sub	w3, w2, #1
	ands	w2, w2, w3
	bne	.L11
	fmov	s0, w0
	cnt	v0.8b, v0.8b
	addv	b0, v0.8b
	umov	w0, v0.b[0]
	sub	w0, w0, #1
	add	x0, x0, 1
	add	x0, x1, x0, lsl 2
	ret
.L12:
	mov	x0, x1
	ret


---


### compiler : `gcc`
### title : `Inefficient code for __builtin_memset`
### open_at : `2019-05-23T15:26:17Z`
### last_modified_date : `2021-08-02T17:52:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90599
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-1 pieces-6]$ cat x.i
extern char *dst;

void
foo (int x)
{
  __builtin_memset (dst, x, 12);
}
[hjl@gnu-cfl-1 pieces-6]$ make x.s
/export/build/gnu/tools-build/gcc-debug/build-x86_64-linux/gcc/xgcc -B/export/build/gnu/tools-build/gcc-debug/build-x86_64-linux/gcc/ -O3 -march=skylake  -S x.i
[hjl@gnu-cfl-1 pieces-6]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movzbl	%dil, %eax
	movabsq	$72340172838076673, %rcx
	movzbl	%dil, %edi
	imulq	%rcx, %rax  <<< x has been broadcasted to RAX.
	imull	$16843009, %edi, %edi
	movq	dst(%rip), %rdx
	movq	%rax, (%rdx)
	movl	%edi, 8(%rdx)  <<< We should just reuse RAX/EAX.
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 10.0.0 20190522 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 pieces-6]$


---


### compiler : `gcc`
### title : `Replace mfence with faster xchg for std::memory_order_seq_cst.`
### open_at : `2019-05-23T18:49:22Z`
### last_modified_date : `2019-10-10T06:06:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90606
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
The following example:

    #include <atomic>
    std::atomic<int> a;
    void foo_seq_cst(int b) { a = b; }

Compiles with `gcc-9.1 -O3 -std=c++17 -pthread` into 

    foo_seq_cst(int):
        mov     DWORD PTR a[rip], edi
        mfence
        ret

Whereas `clang++-9 -O3 -std=c++17 -pthread` compiles it into:

    foo_seq_cst(int):                       # @foo_seq_cst(int)
        xchg    dword ptr [rip + a], edi
        ret



xchg was benchmarked to be 2-3x faster than mfence and Linux kernel switched to xchg were possible. 

gcc should also switch to using xchg for std::memory_order_seq_cst.

See:

https://lore.kernel.org/lkml/20160112150032-mutt-send-email-mst@redhat.com/

https://stackoverflow.com/questions/56205324/why-do-gcc-inserts-mfence-where-clang-dont-use-it/


---


### compiler : `gcc`
### title : `Suboptimal code generated for accessing an aligned array.`
### open_at : `2019-05-24T14:57:42Z`
### last_modified_date : `2019-05-27T07:40:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90616
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `5.4.0`
### severity : `normal`
### contents :
Consider the following code:

    #include <stdint.h>
    
    uint8_t   globalArray[68]   __attribute__((aligned(256)));

    uint8_t foo(uint8_t index)
    {
        return globalArray[index];
    }

avr-gcc-5.4.0 generates the following code for the body of foo():

        mov    r30,r24    # r24 is where 'index' argument is stored
        ldi    r31,0
        subi   r30,lo8(-(globalArray))
        sbci   r31,hi8(-(globalArray))
        ld     r24,Z

which is suboptimal because the lower byte of the address of globalArray is always 0 due to the extended alignment enforced on that object.

Would it be possible to generate a better code for such specific indexing? For example, the following snippet takes full advantage of the overalignment:

        mov    r30,r24
        ldi    r31,hi8(globalArray)
        ld     r24,Z


---


### compiler : `gcc`
### title : `Suboptimal code generated for __builtin_avr_insert_bits`
### open_at : `2019-05-24T18:38:22Z`
### last_modified_date : `2023-05-21T17:15:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90622
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `5.4.0`
### severity : `enhancement`
### contents :
Please consider the following function:

    uint8_t copy_bit_5_to_bit_2(uint8_t dst, uint8_t src)
    {
        return __builtin_avr_insert_bits(0xFFFFF5FF, src, dst);
    }

That particular map value (magic hex constant) is supposed to copy the 5-th bit from argument 'src' to the 2-nd bit of argument 'dst' while leaving all other bits of src unmodified.

In other words, given that
bit representation of src is [s7 s6 s5 s4 s3 s2 s1 s0], and
bit representation of dst is [d7 d6 d5 d4 d3 d2 d1 d0],
it should return [d7 d6 d5 d4 d3 s5 d1 d0].

The code generated for such function is perfect:

    bst r22,5    # Take the 5-th bit of r22
    bld r24,2    # Put it as the 2-nd bit in r24

Similar code is generated for copying any n-th bit to any m-th bit, provided that n and m are different. Thus far everything is great.

However, the code generated for copying n-th bit to n-th bit is surprisingly suboptimal. A similar function

    uint8_t copy_bit_2_to_bit_2(uint8_t dst, uint8_t src)
    {
        return __builtin_avr_insert_bits(0xFFFFF2FF, src, dst);
    }

gives:

    eor r22,r24
    andi r22,lo8(4)
    eor r24,r22

which takes an extra word of program memory and an extra CPU cycle at runtime. I wonder what's wrong with using the same bst/bld idiom which is successfully used for n-to-m copy? I would expect that the following code is much better:

    bst r22,2
    bld r24,2

It would be great if the compiler can generate it.


---


### compiler : `gcc`
### title : `fold strcmp(a, b) == 0 to zero for strings of unequal but non-const lengths`
### open_at : `2019-05-24T21:37:04Z`
### last_modified_date : `2021-09-05T08:35:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90625
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
The following was prompted by pr83431 that seems harder to solve than I expected.  In the test case below, the lengths of the two strings are either the same and the function returns 0, or they are different and the function also returns zero because strcmp of two strings of unequal lengths must return non-zero.  So the strcmp equality can be folded to zero, and the whole function body then replaced with 'return 0.'

GCC already tries to optimize strcmp equalities in handle_builtin_string_cmp() in strlen.c (thanks to the solutin for pr83026 committed in r261039) but it only handles simple cases.

gcc -O2 -S -Wall -fdump-tree-strlen=/dev/stdout b.c
typedef __SIZE_TYPE__ size_t;

int f (const char *a, const char *b)
{
  size_t m = __builtin_strlen (a);
  size_t n = __builtin_strlen (b);

  if (m == n)
    return 0;

  return __builtin_strcmp (a, b) == 0;
}

;; Function f (f, funcdef_no=0, decl_uid=1908, cgraph_uid=1, symbol_order=0)

f (const char * a, const char * b)
{
  size_t n;
  size_t m;
  int _1;
  _Bool _2;
  int _3;
  int _9;

  <bb 2> [local count: 1073741824]:
  m_6 = __builtin_strlen (a_5(D));
  n_8 = __builtin_strlen (b_7(D));
  if (m_6 == n_8)
    goto <bb 4>; [20.97%]
  else
    goto <bb 3>; [79.03%]

  <bb 3> [local count: 848578164]:
  _1 = __builtin_strcmp (a_5(D), b_7(D));
  _2 = _1 == 0;
  _9 = (int) _2;

  <bb 4> [local count: 1073741824]:
  # _3 = PHI <_9(3), 0(2)>
  return _3;

}


---


### compiler : `gcc`
### title : `fold strcmp(a, b) == 0 to zero when one string length is exact and the other is unequal`
### open_at : `2019-05-24T21:43:00Z`
### last_modified_date : `2019-11-07T16:46:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90626
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Similar to pr90625 but simpler, the strcmp equality in the function below can safely be folded to zero because the two strings are of unequal lengths.  The code in handle_builtin_string_cmp() in strlen.c (committed in r261039) should have all it needs to implement this optimization, it just also needs to consider minimum string lengths.

$ cat b.c && gcc -O2 -S -Wall -fdump-tree-strlen=/dev/stdout b.c
int f (char * restrict a, char * restrict b)
{
  __builtin_memcpy (a, "1234", 4);       // length >= 4
  __builtin_strcpy (b, "123");           // length == 3

  return __builtin_strcmp (a, b) == 0;   // must be false
}

;; Function f (f, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=0)

f (char * restrict a, char * restrict b)
{
  int _1;
  _Bool _2;
  int _8;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (a_4(D), "1234", 4);
  __builtin_memcpy (b_6(D), "123", 4);
  _1 = __builtin_strcmp (a_4(D), b_6(D));
  _2 = _1 == 0;
  _8 = (int) _2;
  return _8;

}


---


### compiler : `gcc`
### title : `Call to __builtin_memcmp not folded for identical vectors`
### open_at : `2019-05-27T11:44:09Z`
### last_modified_date : `2021-08-22T08:23:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90644
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Hi,
For following test-case:

#include <stdint.h>

typedef int32_t vnx4si __attribute__((vector_size (32)));

void foo(int a, int b)
{
  vnx4si v = (vnx4si) { a, b, 1, 2 };
  vnx4si expected = (vnx4si) { a, b, 1, 2 };

  if (__builtin_memcmp (&v, &expected, sizeof (vnx4si)) != 0)
    __builtin_abort ();
}

-O2 -ftree-vectorize -march=armv8.2-a+sve folds call to __builtin_memcmp correctly, since both vectors are identical.

But with -msve-vector-bits=256, it fails to fold the call to __builtin_memcmp().

The issue can also be reproduced with AdvSIMD: Fails to fold the call to __builtin_memcmp with vector_size == 16 but folds with vector_size == 32.

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `strlen of a string in a vla plus offset not folded`
### open_at : `2019-05-28T21:26:37Z`
### last_modified_date : `2020-06-20T23:20:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90662
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
In the test case below, GCC can compute the string length in f() and h() but not in g().  

The root cause is that the get_stridx() function in tree-ssa-strlen.c that retrieves the length record for a non-constant string only handles POINTER_PLUS_EXPR but the &a[2] in in g() is represented as '&*a.1_9[2]' or ADDR_EXPR.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
void f (int n)
{ 
  char a[6];
  __builtin_strcpy (a, "12345");
  if (__builtin_strlen (&a[2]) != 3)   // folded to false
    __builtin_abort ();
}

void g (int n)
{
  char a[n];
  __builtin_strcpy (a, "12345");
  if (__builtin_strlen (&a[2]) != 5)   // not folded
    __builtin_abort ();
}

void h (int n)
{ 
  char *a = __builtin_malloc (6);
  __builtin_strcpy (a, "12345");
  if (__builtin_strlen (&a[2]) != 3)   // folded to false
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=0)

f (int n)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1910, cgraph_uid=2, symbol_order=1)

g (int n)
{
  char[0:D.1921] * a.1;
  sizetype _1;
  char * _6;
  long unsigned int _7;

  <bb 2> [local count: 1073741824]:
  _1 = (sizetype) n_2(D);
  a.1_9 = __builtin_alloca_with_align (_1, 8);
  __builtin_memcpy (a.1_9, "12345", 6);
  _6 = &*a.1_9[2];
  _7 = __builtin_strlen (_6);
  if (_7 != 5)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}



;; Function h (h, funcdef_no=2, decl_uid=1914, cgraph_uid=3, symbol_order=2)

h (int n)
{
  <bb 2> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] strcmp (&a[i], a + i) not folded for arrays and constant index`
### open_at : `2019-05-28T23:54:48Z`
### last_modified_date : `2023-07-07T10:35:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90663
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
The two functions below are more or less equivalent and should result in optimal code (a no-op) but starting with GCC 4.7 g() does not.  Prior to GCC 4.7 both were optimized into a return statement.  Clang also emits optimally efficient code here.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
extern char a[];

void f (const char *s)
{
  if (__builtin_strcmp (&s[2], s + 2))   // folded
    __builtin_abort ();
}

void g (void)
{
  if (__builtin_strcmp (&a[2], a + 2))   // not folded
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=0)

f (const char * s)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1910, cgraph_uid=2, symbol_order=1)

g ()
{
  int _1;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strcmp (&a[2], &MEM[(void *)&a + 2B]);
  if (_1 != 0)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `Suboptimal register allocation on ARM when compiling for size`
### open_at : `2019-06-01T22:10:39Z`
### last_modified_date : `2019-06-03T07:28:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90705
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
Created attachment 46441
test.c

When compiling this simple example for ARM (-mcpu=cortex-m0) with gcc-9.1.0,
the code generated looks ok when use -O2, but register allocations looks weird when compiling use -Os. Registers are pushed on stack, and code actually gets alot bigger.

Example

int k;
int test(int i)
{
  int r = 0;
  for (; i >= 0; i--) {
    k = i;
    r += k;
  }
  return r;
}


Compiling gcc-9.1.0, -mcpu=cortex-m0 using -O2:

00000000 <test>:
   0:   0003            movs    r3, r0
   2:   2000            movs    r0, #0
   4:   2b00            cmp     r3, #0
   6:   db05            blt.n   14 <test+0x14>
   8:   18c0            adds    r0, r0, r3
   a:   3b01            subs    r3, #1
   c:   d2fc            bcs.n   8 <test+0x8>
   e:   2200            movs    r2, #0
  10:   4b01            ldr     r3, [pc, #4]    ; (18 <test+0x18>)
  12:   601a            str     r2, [r3, #0]
  14:   4770            bx      lr
  16:   46c0            nop                     ; (mov r8, r8)
  18:   00000000        .word   0x00000000


but when compiling with same compiler with -Os:

00000000 <test>:
   0:   2200            movs    r2, #0
   2:   b530            push    {r4, r5, lr}
   4:   0003            movs    r3, r0
   6:   2501            movs    r5, #1
   8:   0010            movs    r0, r2
   a:   4906            ldr     r1, [pc, #24]   ; (24 <test+0x24>)
   c:   680c            ldr     r4, [r1, #0]
   e:   2b00            cmp     r3, #0
  10:   da03            bge.n   1a <test+0x1a>
  12:   2a00            cmp     r2, #0
  14:   d000            beq.n   18 <test+0x18>
  16:   600c            str     r4, [r1, #0]
  18:   bd30            pop     {r4, r5, pc}
  1a:   001c            movs    r4, r3
  1c:   18c0            adds    r0, r0, r3
  1e:   002a            movs    r2, r5
  20:   3b01            subs    r3, #1
  22:   e7f4            b.n     e <test+0xe>
  24:   00000000        .word   0x00000000

using 2 more registers and stack, also code size significantly larger.


---


### compiler : `gcc`
### title : `[10/11/12/13 Regression] Useless code generated for stack / register operations on AVR`
### open_at : `2019-06-01T22:35:21Z`
### last_modified_date : `2023-05-21T15:25:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90706
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.1.0`
### severity : `normal`
### contents :
given the following function:

unsigned char check(float x)
{
   return (0.0 < x);
}


in avr-gcc 8.3.0 the following code is generated:

00000098 <_Z5checkf>:
  98:	cf 93       	push	r28
  9a:	c1 e0       	ldi	r28, 0x01	; 1
  9c:	20 e0       	ldi	r18, 0x00	; 0
  9e:	30 e0       	ldi	r19, 0x00	; 0
  a0:	a9 01       	movw	r20, r18
  a2:	0e 94 a8 00 	call	0x150	; 0x150 <__gesf2>
  a6:	18 16       	cp	r1, r24
  a8:	0c f0       	brlt	.+2      	; 0xac <_Z5checkf+0x14>
  aa:	c0 e0       	ldi	r28, 0x00	; 0
  ac:	8c 2f       	mov	r24, r28
  ae:	cf 91       	pop	r28
  b0:	08 95       	ret

I don't see any room for improvements here. avr-gcc 9.1.0 compiles to the following. I've marked the lines that don't make sense to me.

  00000098 <_Z5checkf>:
  98:	cf 93       	push	r28
  9a:	df 93       	push	r29
*  9c:	00 d0       	rcall	.+0      	; 0x9e <_Z5checkf+0x6>
*  9e:	00 d0       	rcall	.+0      	; 0xa0 <_Z5checkf+0x8>
*  a0:	0f 92       	push	r0
*  a2:	cd b7       	in	r28, 0x3d	; 61
*  a4:	de b7       	in	r29, 0x3e	; 62
  a6:	21 e0       	ldi	r18, 0x01	; 1
  a8:	2d 83       	std	Y+5, r18	; 0x05
  aa:	20 e0       	ldi	r18, 0x00	; 0
  ac:	30 e0       	ldi	r19, 0x00	; 0
  ae:	a9 01       	movw	r20, r18
*  b0:	69 83       	std	Y+1, r22	; 0x01
*  b2:	7a 83       	std	Y+2, r23	; 0x02
*  b4:	8b 83       	std	Y+3, r24	; 0x03
*  b6:	9c 83       	std	Y+4, r25	; 0x04
*  b8:	69 81       	ldd	r22, Y+1	; 0x01
*  ba:	7a 81       	ldd	r23, Y+2	; 0x02
*  bc:	8b 81       	ldd	r24, Y+3	; 0x03
*  be:	9c 81       	ldd	r25, Y+4	; 0x04
  c0:	0e 94 dc 00 	call	0x1b8	; 0x1b8 <__gesf2>
  c4:	18 16       	cp	r1, r24
  c6:	0c f0       	brlt	.+2      	; 0xca <_Z5checkf+0x32>
  c8:	1d 82       	std	Y+5, r1	; 0x05
  ca:	8d 81       	ldd	r24, Y+5	; 0x05
  cc:	0f 90       	pop	r0
  ce:	0f 90       	pop	r0
  d0:	0f 90       	pop	r0
  d2:	0f 90       	pop	r0
  d4:	0f 90       	pop	r0
*  d6:	df 91       	pop	r29
*  d8:	cf 91       	pop	r28
  da:	08 95       	ret

The value is put to Y+1 to Y+4 and then immediately read back. why?
pushing r0 does not make sense at all since it is by definition a temporary register that can freely be modified. Maybe it's just pushed to make room for the stack operations?

compilation:
"D:\AVR\Toolchain\9.1.0\bin\avr-g++.exe" -funsigned-char -funsigned-bitfields -DNDEBUG  -I"C:\Program Files (x86)\Atmel\Studio\7.0\Packs\atmel\ATmega_DFP\1.2.272\include"  -Os -ffunction-sections -fdata-sections -fpack-struct -fshort-enums -Wall  -mmcu=atmega644  -c -MD -MP -MF "main.d" -MT"main.d" -MT"main.o"   -o "main.o" ".././main.cpp" 

linker:
"D:\AVR\Toolchain\9.1.0\bin\avr-g++.exe" -o BugTest.elf  main.o   -Wl,-Map="BugTest.map" -Wl,--start-group -Wl,-lm  -Wl,--end-group -Wl,--gc-sections  -mmcu=atmega644


---


### compiler : `gcc`
### title : `strlen(a + i) missing range for arrays of unknown bound with strings of known length and variable i`
### open_at : `2019-06-05T21:21:13Z`
### last_modified_date : `2020-01-28T11:41:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90766
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC manages to fold the first two conditionals involving strlen but not the third, even though it has all the information to do that.  The maybe_set_strlen_range() function in tree-ssa-strlen.c doesn't take the known length of the string stored in b into consideration.

$ cat a.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout a.c
const char s[] = "123";

void f (int i)
{
  if (__builtin_strlen (&s[i]) > 3)   // folded to false, good
    __builtin_abort ();
}

extern char a[8];

void g (int i)
{
  __builtin_strcpy (a, "123");
  if (__builtin_strlen (&s[i]) > 3)   // folded to false, good
    __builtin_abort ();
}

extern char b[];

void h (int i)
{
  __builtin_strcpy (b, "123");
  if (__builtin_strlen (&b[i]) > 3)   // not folded but could be
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=1)

f (int i)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1911, cgraph_uid=2, symbol_order=2)

g (int i)
{
  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "123", 4); [tail call]
  return;

}



;; Function h (h, funcdef_no=2, decl_uid=1915, cgraph_uid=3, symbol_order=3)

h (int i)
{
  char * _1;
  long unsigned int _2;
  sizetype _7;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&b, "123", 4);
  _7 = (sizetype) i_5(D);
  _1 = &b + _7;
  _2 = __builtin_strlen (_1);
  if (_2 > 3)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `better range analysis for converting bit tests into less-than greater-than`
### open_at : `2019-06-05T22:22:06Z`
### last_modified_date : `2023-09-04T08:18:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90768
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `enhancement`
### contents :
Converting the >= 8 to a & 8 results in one less instruction on power 9

https://godbolt.org/z/0QPN3z

#include <stdint.h>
#include <stdlib.h>
int bcmp_2(char *a, char *b, size_t s) {
    if (s < 16) {
        if (s >= 8)
            if (*(uint64_t *)a != *(uint64_t *)b)
                return 1;
        
    }
}


---


### compiler : `gcc`
### title : `Improve piecewise operation`
### open_at : `2019-06-06T22:57:53Z`
### last_modified_date : `2021-10-06T23:48:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90773
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
For

[hjl@gnu-cfl-1 pieces-6]$ cat copy.i 
extern char *dst, *src;

void
foo (unsigned int x)
{
  __builtin_memcpy (dst, src, 15);
}
[hjl@gnu-cfl-1 pieces-6]$ 

we generate

	movq	src(%rip), %rdx
	movq	dst(%rip), %rax
	movq	(%rdx), %rcx
	movq	%rcx, (%rax)
	movl	8(%rdx), %ecx
	movl	%ecx, 8(%rax)
	movzwl	12(%rdx), %ecx
	movw	%cx, 12(%rax)
	movzbl	14(%rdx), %edx
	movb	%dl, 14(%rax)
	ret

Instead, we can generate

	movq	src(%rip), %rdx
	movq	dst(%rip), %rax
	movq	(%rdx), %rcx
	movq	%rcx, (%rax)
	movq	7(%rdx), %rcx
	movq	%rcx, 7(%rax)
	ret


---


### compiler : `gcc`
### title : `avoid doing vector splat arithmetic where possible`
### open_at : `2019-06-06T23:49:40Z`
### last_modified_date : `2021-08-20T00:26:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90774
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `normal`
### contents :
When gcc knows that it is dealing with splats it should just do regular arithmetic, and only convert to splat at the end.

https://simd.godbolt.org/z/6P3Qcq


---


### compiler : `gcc`
### title : `missing strlen range after a char store at non-zero index`
### open_at : `2019-06-10T20:17:14Z`
### last_modified_date : `2020-01-28T11:47:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90821
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC folds the test in f() into a constant but doesn't do the same for the test in g(), even though both are necessarily false and even though all information to determine that is available in the strlen pass in both cases.

$ cat a.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout a.c
char a[4];

void f (void)
{
  a[0] = 1;
  a[1] = 2;
  a[2] = 0;
  if (__builtin_strlen (a) > 2)   // folded to false
    __builtin_abort ();
}

void g (void)
{ 
  a[0] = 1;
  a[2] = 0;
  if (__builtin_strlen (a) > 2)   // not folded
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=1)

f ()
{
  <bb 2> [local count: 1073741824]:
  MEM[(char *)&a] = 513;
  a[2] = 0;
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1910, cgraph_uid=2, symbol_order=2)

g ()
{
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  a[0] = 1;
  a[2] = 0;
  _1 = __builtin_strlen (&a);
  if (_1 == 3)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `PowerPC should generate better code for SFmode splats for power8`
### open_at : `2019-06-10T22:19:01Z`
### last_modified_date : `2023-07-07T08:28:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90824
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
For power7/power8 code generation, we should generate better code for splatting a SFmode value from memory.

Consider the code:
vector float foo2 (float *p) { return (vector float) { *p, *p, *p, *p }; }

On Power9 we generate the following since we have load word and splat:
foo2:
        lxvwsx 34,0,3
        blr

However on power8, we generate the following:
foo2:
        lxsspx 34,0,3
        xscvdpspn 34,34
        xxspltw 34,34,0
        blr

and on power7, we generate the similar:
foo2:
        lfs 0,0(3)
        xscvdpsp 0,0
        xxspltw 34,0,0
        blr

For this case, the better code to generate is:
foo2:
        lfiwzx 0,0,3
        xxspltw 34,0,0
        blr


---


### compiler : `gcc`
### title : `Missing popcount pattern matching`
### open_at : `2019-06-11T13:29:03Z`
### last_modified_date : `2019-11-21T18:36:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90836
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The following source:
int
foo (unsigned long long a)
{
    unsigned long long b = a;
    b -= ((b>>1) & 0x5555555555555555ULL);
    b = ((b>>2) & 0x3333333333333333ULL) + (b & 0x3333333333333333ULL);
    b = ((b>>4) + b) & 0x0F0F0F0F0F0F0F0FULL;
    b *= 0x0101010101010101ULL;
    return (int)(b >> 56);
}

implements a popcount operation. Some compilers can emit for x86:

        xorq      %rax, %rax
        popcnt    %rdi, %rax
        ret

but GCC doesn't detect this and emits the sequence of masks and shifts.
This code is important for some workloads, 531.deepsjeng_r from SPEC, for example.

Can this be matched in a match.pd pattern?


---


### compiler : `gcc`
### title : `Detect table-based ctz implementation`
### open_at : `2019-06-11T15:14:31Z`
### last_modified_date : `2023-02-17T16:32:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90838
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
531.deepsjeng_r has a hot function doing something like:
static const unsigned long long magic = 0x03f08c5392f756cdULL;

static const int table[64] = {
     0,  1, 12,  2, 13, 22, 17,  3,
    14, 33, 23, 36, 18, 58, 28,  4,
    62, 15, 34, 26, 24, 48, 50, 37,
    19, 55, 59, 52, 29, 44, 39,  5,
    63, 11, 21, 16, 32, 35, 57, 27,
    61, 25, 47, 49, 54, 51, 43, 38,
    10, 20, 31, 56, 60, 46, 53, 42,
     9, 30, 45, 41,  8, 40,  7,  6,
};

int
myctz (unsigned long long b) {
    unsigned long long lsb = b & -b;
    return table[(lsb * magic) >> 58];
}

This is equivalent to __builtin_ctzl (b).
Would it be possible to match this in GCC?


---


### compiler : `gcc`
### title : `Detect lsb ones counting loop (final value replacement?)`
### open_at : `2019-06-11T15:27:34Z`
### last_modified_date : `2021-12-15T22:00:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90839
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
The code:
int
foo (int a)
{
  int b = 0;
  while (a >>= 1)
    b++;

  return b;
}

Can calculates 31 - clz (a). A compiler could generate for x86:
        xorl      %eax, %eax
        bsr       %edi, %edx
        cmovne    %edx, %eax
        ret

But GCC generates the loop.
I believe we already have popcount-detecting code in final value replacement...


---


### compiler : `gcc`
### title : `Suboptimal codegen of structs arguments`
### open_at : `2019-06-13T00:12:33Z`
### last_modified_date : `2021-10-28T09:53:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90864
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.1.0`
### severity : `enhancement`
### contents :
I found two issues with handling of structures, which might be related.

Given structures like this:

  struct X { char v; };
  struct X2 { X x; char v; };
  struct X3 { X2 x2; char v; };

The code generated for the following function:

  X3 id(X3 value) { return value; }

with clang/clang++ (version 8.0.0) and -O2 is:

   0:   89 f8                   mov    %edi,%eax
   2:   c3     

but with gcc/g++ 9.1.0 and -O2 the generated code reads:

   0:   89 f8                   mov    %edi,%eax
   2:   0f b7 d7                movzwl %di,%edx
   5:   c1 e8 10                shr    $0x10,%eax
   8:   0f b6 c0                movzbl %al,%eax
   b:   48 c1 e0 10             shl    $0x10,%rax
   f:   48 09 d0                or     %rdx,%rax
  12:   c3                      retq

and with gcc/g++ 8.3.0 it was even worse:

   0:   89 fa                   mov    %edi,%edx
   2:   c1 ef 10                shr    $0x10,%edi
   5:   0f b6 c2                movzbl %dl,%eax
   8:   40 0f b6 ff             movzbl %dil,%edi
   c:   0f b6 d6                movzbl %dh,%edx
   f:   88 d4                   mov    %dl,%ah
  11:   48 c1 e7 10             shl    $0x10,%rdi
  15:   48 09 f8                or     %rdi,%rax
  18:   c3                      retq

I'm not sure whether this is a related issue or not, but something as simple as the following also fails to compile, even though the argument should be found in %rdi on x86_64:

  void test(X3 a) {
    __asm__ ("" :: "r" (a));
  }

With the output:

  warning: asm operand 0 probably doesn't match constraints
       __asm__ ("" :: "r" (a));
                            ^
  error: impossible constraint in 'asm'


---


### compiler : `gcc`
### title : `fold zero-equality of memcmp and strncmp involving strings of unequal lengths`
### open_at : `2019-06-13T20:50:08Z`
### last_modified_date : `2019-11-07T16:46:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90876
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Thanks to the solution for pr83026 committed in r261039 GCC optimizes strcmp inequalities involving strings of different lengths.  However, it only handles simple cases involving strcmp and misses other, equivalent cases involving strncmp or memcmp that could be easily handled as well.  The test case below shows the optimization in f0 and its absence in f1 and f2:

$ cat a.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout a.c
int f0 (void)
{ 
  char a[4], b[4];
  __builtin_strcpy (a, "12");
  __builtin_strcpy (b, "123");
  return 0 == __builtin_strcmp (a, b);   // folded to 0
}

int f1 (void)
{
  char a[4], b[4];
  __builtin_strcpy (a, "12");
  __builtin_strcpy (b, "123");
  return 0 == __builtin_strncmp (a, b, 3);   // not folded
}

int f2 (void)
{ 
  char a[4], b[4];
  __builtin_strcpy (a, "12");
  __builtin_strcpy (b, "123");
  return 0 == __builtin_memcmp (a, b, 3);   // not folded
}

;; Function f0 (f0, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=0)

f0 ()
{
  <bb 2> [local count: 1073741824]:
  return 0;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=1911, cgraph_uid=2, symbol_order=1)

f1 ()
{
  char b[4];
  char a[4];
  int _1;
  _Bool _2;
  int _6;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "12", 3);
  MEM <unsigned char[4]> [(char * {ref-all})&b] = MEM <unsigned char[4]> [(char * {ref-all})"123"];
  _1 = __builtin_strncmp (&a, &b, 3);
  _2 = _1 == 0;
  _6 = (int) _2;
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return _6;

}



;; Function f2 (f2, funcdef_no=2, decl_uid=1916, cgraph_uid=3, symbol_order=2)

f2 ()
{
  char b[4];
  char a[4];
  int _1;
  _Bool _2;
  int _6;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "12", 3);
  MEM <unsigned char[4]> [(char * {ref-all})&b] = MEM <unsigned char[4]> [(char * {ref-all})"123"];
  _1 = __builtin_memcmp_eq (&a, &b, 3);
  _2 = _1 == 0;
  _6 = (int) _2;
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return _6;

}


---


### compiler : `gcc`
### title : `[8/9/10 Regression] integer -> SSE register move isn't generated`
### open_at : `2019-06-13T22:17:43Z`
### last_modified_date : `2019-09-18T20:03:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90878
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `normal`
### contents :
[hjl@gnu-cfl-1 sse-move]$ cat x.i
union ieee754_float
  {
    float f;

    struct
      {
	unsigned int mantissa:23;
	unsigned int exponent:8;
	unsigned int negative:1;
      } ieee;
};

double
foo (float f)
{
  union ieee754_float u;
  u.f = f;
  u.ieee.negative = 0;
  return u.f;
}
[hjl@gnu-cfl-1 sse-move]$ /usr/gcc-9.1.1-x32-tuning/bin/gcc -S -O2 -march=skylake x.i
[hjl@gnu-cfl-1 sse-move]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	vmovd	%xmm0, %eax
	andl	$2147483647, %eax
	movl	%eax, -4(%rsp)
	vcvtss2sd	-4(%rsp), %xmm0, %xmm0
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 9.1.1 20190517"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 sse-move]$ 

Skylake cost has

 2, 2,                                 /* SSE->integer and integer->SSE moves */

which are the same cost as the normal register move.  But integer->SSE
isn't used since ix86_register_move_cost has

  /* Moves between SSE/MMX and integer unit are expensive.  */
  if (MMX_CLASS_P (class1) != MMX_CLASS_P (class2)
      || SSE_CLASS_P (class1) != SSE_CLASS_P (class2))

    /* ??? By keeping returned value relatively high, we limit the number
       of moves between integer and MMX/SSE registers for all targets.
       Additionally, high value prevents problem with x86_modes_tieable_p(),
       where integer modes in MMX/SSE registers are not tieable
       because of missing QImode and HImode moves to, from or between
       MMX/SSE registers.  */
    return MAX (8, MMX_CLASS_P (class1) || MMX_CLASS_P (class2)
                ? ix86_cost->mmxsse_to_integer : ix86_cost->ssemmx_to_integer);

integer->SSE is always 8 which much more expensive than integer
register store whose cost is 3.


---


### compiler : `gcc`
### title : `fold zero-equality of strcmp between a longer string and a smaller array`
### open_at : `2019-06-13T22:18:47Z`
### last_modified_date : `2019-11-07T16:46:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90879
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Here's another strcmp optimization opportunity (besides pr90626 and pr90876) that could easily be taken advantage of on top of the solution for pr83026.

When strncmp is called with a bound in excess of the sizes of the pointed-to arrays, GCC eliminates the pointless excessive bound and transforms the strncpy call to strcpy (in a subset of the cases when this is possible).  This can be seen in function f in the test case below.

But when, in a call to strcmp or strncmp, a string argument is longer than the size of the array it's being compared with for equality, the optimization fails to take advantage of the fact that a string that long cannot be equal to one stored in a smaller array.  This can be seen in function g in the test case.

The existing optimization has all the smarts to go this extra step, it just doesn't do it.

$ cat a.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout a.c
extern char a[4];

void f (void)
{
  if (0 == __builtin_strncmp (a, "12345", 8))   // transformed to strcmp
    __builtin_abort ();                         // when the whole expression could be folded to zero
}

void g (void)
{ 
  if (0 == __builtin_strcmp (a, "123456"))      // not folded
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=0)

f ()
{
  int _1;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strcmp (&a, "12345");
  if (_1 == 0)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1910, cgraph_uid=2, symbol_order=1)

g ()
{
  int _1;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strcmp (&a, "123456");
  if (_1 == 0)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `Generated code is worse if returned struct is unnamed`
### open_at : `2019-06-14T04:46:29Z`
### last_modified_date : `2023-05-02T01:13:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90883
### status : `REOPENED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `normal`
### contents :
Demo code:

    #include <array>

    class C
    {
        std::array<char, 7> a{};
        int b{};
    };

    C slow()
    {
        return {};
    }

    C fast()
    {
        C c;
        return c;
    }

On x86-64 with -O2 or -O3, fast() generates good assembly:

        xor     eax, eax
        xor     edx, edx

But slow() does this:

        xor     eax, eax
        mov     DWORD PTR [rsp-25], 0
        mov     BYTE PTR [rsp-21], 0
        mov     edx, DWORD PTR [rsp-24]
        mov     DWORD PTR [rsp-32], 0
        mov     WORD PTR [rsp-28], ax
        mov     BYTE PTR [rsp-26], 0
        mov     rax, QWORD PTR [rsp-32]

GCC has had this problem since version 7.  GCC 6 generated poor code for both functions.  GCC 4 and 5 generated different but not terrible code.  Clang does a good job in all versions.

StackOverflow discussion: https://stackoverflow.com/questions/56578196/why-does-gcc-fail-to-optimize-unless-the-return-value-has-a-name/


---


### compiler : `gcc`
### title : `std::swap bad code gen -- alias analysis insufficient to remove dead store`
### open_at : `2019-06-14T22:39:54Z`
### last_modified_date : `2019-06-17T15:15:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90888
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `9.1.0`
### severity : `normal`
### contents :
The following code optimizes well for `custom_swap` and `restrict_std_swap`, but has an additional `mov` instruction for `std_swap`:


void custom_swap(int * lhs, int * rhs) {
    int temp = *lhs;
    *lhs = *rhs;
    *rhs = temp;
}

void restrict_std_swap(int * __restrict lhs, int * __restrict rhs) {
    int temp = *lhs;
    *lhs = 0;
    *lhs = *rhs;
    *rhs = temp;
}

void std_swap(int * lhs, int * rhs) {
    int temp = *lhs;
    *lhs = 0;
    *lhs = *rhs;
    *rhs = temp;
}



Compiles into this for x86-64:

custom_swap(int*, int*):
        mov     eax, DWORD PTR [rdi]
        mov     edx, DWORD PTR [rsi]
        mov     DWORD PTR [rdi], edx
        mov     DWORD PTR [rsi], eax
        ret
restrict_std_swap(int*, int*):
        mov     eax, DWORD PTR [rdi]
        mov     edx, DWORD PTR [rsi]
        mov     DWORD PTR [rsi], eax
        mov     DWORD PTR [rdi], edx
        ret
std_swap(int*, int*):
        mov     eax, DWORD PTR [rdi]
        mov     DWORD PTR [rdi], 0
        mov     edx, DWORD PTR [rsi]
        mov     DWORD PTR [rdi], edx
        mov     DWORD PTR [rsi], eax
        ret


And this for ARM64:

custom_swap(int*, int*):
        ldr     w3, [x1]
        ldr     w2, [x0]
        str     w3, [x0]
        str     w2, [x1]
        ret
restrict_std_swap(int*, int*):
        ldr     w2, [x0]
        ldr     w3, [x1]
        str     w3, [x0]
        str     w2, [x1]
        ret
std_swap(int*, int*):
        ldr     w2, [x0]
        str     wzr, [x0]
        ldr     w3, [x1]
        str     w3, [x0]
        str     w2, [x1]
        ret


As we see from the example that annotates the parameters with __restrict, the problem appears to be that the risk of *lhs aliasing *rhs disables the optimizer's ability to remove the dead store in the second line of std_swap. It is able to see that if they don't alias, the store in line 2 is dead. It is not able to see that if they do alias, the store in line 3 is dead and the store in line 2 is dead.

See it live: https://godbolt.org/z/D3Ey9i . 




The real life problem here is that types that manage a resource but do not implement a custom std::swap, as well as all types that recursively contain a type that manages a resource, suffer from reduced performance for using std::swap. The larger, slightly more meaningful test case showing how I arrived at this reduction and its relationship to std::swap:

struct unique_ptr {
    unique_ptr():
        ptr(nullptr)
    {
    }
    unique_ptr(unique_ptr && other) noexcept:
        ptr(other.ptr)
    {
        other.ptr = nullptr;
    }
    unique_ptr & operator=(unique_ptr && other) noexcept {
        delete ptr;
        ptr = nullptr;
        ptr = other.ptr;
        other.ptr = nullptr;
        return *this;
    }
    ~unique_ptr() noexcept {
        delete ptr;
    }

    int * ptr;
};


void custom_swap(unique_ptr & lhs, unique_ptr & rhs) noexcept {
    int * temp = lhs.ptr;
    lhs.ptr = rhs.ptr;
    rhs.ptr = temp;
}

void inlined_std_swap(unique_ptr & lhs, unique_ptr & rhs) noexcept {
    int * temp_ptr = lhs.ptr;
    lhs.ptr = nullptr;

    delete lhs.ptr;
    lhs.ptr = nullptr;
    lhs.ptr = rhs.ptr;
    rhs.ptr = nullptr;

    delete rhs.ptr;
    rhs.ptr = nullptr;

    rhs.ptr = temp_ptr;
    temp_ptr = nullptr;

    delete temp_ptr;
}

void std_swap(unique_ptr & lhs, unique_ptr & rhs) noexcept {
    auto temp = static_cast<unique_ptr &&>(lhs);
    lhs = static_cast<unique_ptr &&>(rhs);
    rhs = static_cast<unique_ptr &&>(temp);
}

See it live: https://godbolt.org/z/yBFa4H


---


### compiler : `gcc`
### title : `[10 Regression] 456.hmmer regression with r272239`
### open_at : `2019-06-18T14:06:08Z`
### last_modified_date : `2019-07-04T13:56:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90911
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
There's a 4% regression in 456.hmmer observed on https://gcc.opensuse.org/gcc-old/SPEC/CINT/sb-czerny-head-64-2006/recent.html with r272239 at -Ofast -march=haswell [-flto].


---


### compiler : `gcc`
### title : `Propagate constants into loads if dominated by str(n)cmp/memcmp`
### open_at : `2019-06-18T16:49:45Z`
### last_modified_date : `2020-01-14T18:07:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90917
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Motivation:

char f(char* s) {
  if (strcmp(s, "test") == 0) return s[0];
  return '-';
}

--->

char f2(char* s) {
  if (strcmp(s, "test") == 0) return 't';
  return '-';
}


---


### compiler : `gcc`
### title : `-Os produces more code than -O1`
### open_at : `2019-06-23T22:19:27Z`
### last_modified_date : `2021-12-15T21:58:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90967
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.1`
### severity : `enhancement`
### contents :
Created attachment 46512
example code

The attached code produces an empty main when compiled with -std=c++11 -Os in GCC 4.9.4. Starting with GCC 5.1.0 it will produce code involving a basic_string move constructor. When compiled with -O1 it will always produce an empty function.


---


### compiler : `gcc`
### title : `simd integer division not optimized`
### open_at : `2019-06-25T10:40:03Z`
### last_modified_date : `2020-02-27T20:07:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90993
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Test case (https://godbolt.org/z/CYipz7):

template <class T> using V [[gnu::vector_size(16)]] = T;

V<char > f(V<char > a, V<char > b) { return a / b; }
V<short> f(V<short> a, V<short> b) { return a / b; }
V<int  > f(V<int  > a, V<int  > b) { return a / b; }
V<unsigned char > f(V<unsigned char > a, V<unsigned char > b) { return a / b; }
V<unsigned short> f(V<unsigned short> a, V<unsigned short> b) { return a / b; }
V<unsigned int  > f(V<unsigned int  > a, V<unsigned int  > b) { return a / b; }

(You can extend the test case to 32 and 64 bit vectors.)

All these divisions have no SIMD instruction on x86. However, conversion to float or double vectors is lossless (char & short -> float, int -> double) and enables implementation via divps/divpd. This leads to a considerable speedup (especially on divider throughput), even with the cost of the conversions. The division by 0 case is UB (http://eel.is/c++draft/expr.mul#4), so it doesn't matter that a potential SIGFPE turns into "whatever". ;-)

For reference, this is the result of my library implementation: https://godbolt.org/z/Xgo9Pk.

And benchmark results on Skylake i7:
                  TYPE            Latency     Speedup     Throughput     Speedup
                            [cycles/call]              [cycles/call]
 schar,                              24.5                       9.81
 schar, simd_abi::__sse              32.3        12.1           9.19        17.1
 schar, vector_size(16)               128        3.06            125        1.26
 schar, simd_abi::__avx              40.3        19.4           18.7        16.8
 schar, vector_size(32)               255        3.07            256        1.23
--------------------------------------------------------------------------------
 uchar,                              20.8                       7.55
 uchar, simd_abi::__sse              31.9        10.4            9.5        12.7
 uchar, vector_size(16)               121        2.74            116        1.04
 uchar, simd_abi::__avx              39.9        16.7           18.8        12.8
 uchar, vector_size(32)               230         2.9            224        1.08
--------------------------------------------------------------------------------
 short,                              22.7                        6.4
 short, simd_abi::__sse              23.6         7.7           4.52        11.3
 short, vector_size(16)              62.6        2.91           58.4       0.877
 short, simd_abi::__avx              30.6        11.9           9.55        10.7
 short, vector_size(32)               120        3.03            114         0.9
--------------------------------------------------------------------------------
ushort,                              19.4                       7.37
ushort, simd_abi::__sse              23.7        6.55           4.55        12.9
ushort, vector_size(16)              61.3        2.53           57.4        1.03
ushort, simd_abi::__avx              30.6        10.1           8.86        13.3
ushort, vector_size(32)               116        2.67            114        1.03
--------------------------------------------------------------------------------
   int,                              23.2                       7.14
   int, simd_abi::__sse              24.7        3.75           7.24        3.95
   int, vector_size(16)              40.3         2.3           30.9       0.924
   int, simd_abi::__avx              35.6        5.22           14.5        3.95
   int, vector_size(32)              64.2         2.9           61.4        0.93
--------------------------------------------------------------------------------
  uint,                              20.5                       7.14
  uint, simd_abi::__sse                44        1.86           7.73        3.69
  uint, vector_size(16)              39.7        2.07           30.9       0.925
  uint, simd_abi::__avx              56.9        2.89             16        3.57
  uint, vector_size(32)              71.4         2.3           71.5       0.798
--------------------------------------------------------------------------------

I have not investigated whether the same optimization makes sense for other targets than x86.

Since this optimization requires optimized vector conversions, PR85048 is relevant.


---


### compiler : `gcc`
### title : `Missed optimization on sequential memcpy calls`
### open_at : `2019-06-27T10:34:58Z`
### last_modified_date : `2021-12-13T02:47:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91019
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.1`
### severity : `enhancement`
### contents :
#include <stdint.h>
#include <string.h>

void encode_v1(uint8_t *buf, uint64_t a1, uint16_t a2) {
    memcpy(buf, &a1, 6);
    memcpy(buf+6, &a2, 2);
}

void encode_v2(uint8_t *buf, uint64_t a1, uint16_t a2) {
    memcpy(buf, &a1, 8);
    memcpy(buf+6, &a2, 2);
}


Two functions above should be equivalent, packing arguments into buffer. 

`encode_v1` copies 6 bytes, then 2 bytes.
`encode_v2` copies 8 bytes, then replaces last two bytes.

Functionally they are the same, while v2 generates better assembly.


This is the assembly with -O3 (https://godbolt.org/z/i6TMiY)

encode_v1(unsigned char*, unsigned long, unsigned short):
        mov     eax, esi
        shr     rsi, 32
        mov     WORD PTR [rdi+6], dx
        mov     DWORD PTR [rdi], eax
        mov     WORD PTR [rdi+4], si
        ret
encode_v2(unsigned char*, unsigned long, unsigned short):
        mov     QWORD PTR [rdi], rsi
        mov     WORD PTR [rdi+6], dx
        ret


---


### compiler : `gcc`
### title : `[10 Regression] switch expansion produces a jump table with trivial entries`
### open_at : `2019-06-28T01:33:32Z`
### last_modified_date : `2023-07-07T08:30:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91026
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 46531
testcase

In the attached testcase gcc lowers the switch to a jump table where each entry in the table just sets a value. The two labels in the table are

.L5:
        movl    $1, %eax
        ret

.L2:
        xorl    %eax, %eax
        ret

gcc could have replaced the jump table with a constant table, with L5 replaced by 1 and L2 by 0.

clang produces:

0000000000000000 <_Z3foo4kind>:
   0:   40 80 c7 ff             add    $0xff,%dil
   4:   40 80 ff 11             cmp    $0x11,%dil
   8:   77 0c                   ja     16 <_Z3foo4kind+0x16>
   a:   b8 b9 a7 02 00          mov    $0x2a7b9,%eax
   f:   89 f9                   mov    %edi,%ecx
  11:   d3 e8                   shr    %cl,%eax
  13:   24 01                   and    $0x1,%al
  15:   c3                      retq
  16:   31 c0                   xor    %eax,%eax
  18:   c3                      retq


---


### compiler : `gcc`
### title : `missed optimization regarding value of modulo operation`
### open_at : `2019-06-28T12:57:35Z`
### last_modified_date : `2020-11-19T23:03:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91029
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `normal`
### contents :
GCC's optimizers apparently don't know that, for b an integer >= 0, a % b > 0 implies that a >= 0 and a % b < 0 implies that a <= 0.

Test case:
============= foo.c =============
int xx;

void f (int i)
{
  if ((i % 7) == 3)
    xx = (i < 0);
}
=================================
$ gcc -O2 -m32 -S foo.c && fgrep -v .cfi foo.s
        .file   "foo.c"
        .text
        .p2align 4
        .globl  f
        .type   f, @function
f:
.LFB0:
        movl    4(%esp), %ecx
        movl    $-1840700269, %edx
        movl    %ecx, %eax
        imull   %edx
        movl    %ecx, %eax
        sarl    $31, %eax
        addl    %ecx, %edx
        sarl    $2, %edx
        subl    %eax, %edx
        leal    0(,%edx,8), %eax
        subl    %edx, %eax
        movl    %ecx, %edx
        subl    %eax, %edx
        cmpl    $3, %edx
        je      .L4
        ret
        .p2align 4,,10
        .p2align 3
.L4:
        shrl    $31, %ecx
        movl    %ecx, xx
        ret
.LFE0:
        .size   f, .-f
        .comm   xx,4,4
        .ident  "GCC: (GNU) 9.1.0"
        .section        .note.GNU-stack,"",@progbits

The two instructions after .L4 could be optimized to
        movl    $0, xx
by observing that for i < 0, i % 7 is <= 0 and therefore never == 3.


---


### compiler : `gcc`
### title : `does not reduce the size of a division by a constant on non-negative int / small unsigned long constant`
### open_at : `2019-07-03T13:54:41Z`
### last_modified_date : `2021-10-03T20:19:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91072
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
The generated division by a constant is suboptimal when its size could be reduced, e.g. on non-negative int divided by a small unsigned long constant (which fits in an int). Consider the following example (the i <= 0 condition is just there to have a similar source, otherwise this could just be i < 0 when i is signed).

int f1 (unsigned int i)
{
  return i <= 0 ? 0 : i / 3UL;
}

int f2 (int i)
{
  return i <= 0 ? 0 : i / 3UL;
}

int f3 (int i)
{
  return i <= 0 ? 0 : i / 3L;
}

With GCC 8.3.0 on x86_64, using -O3, f1 and f3 both use a 32-bit multiplication (mull / imull) by a 32-bit constant, but f2 uses a 64-bit multiplication (mulq) by a 64-bit constant.

With gcc (Debian 20190628-1) 10.0.0 20190628 (experimental) [trunk revision 272790], the only change is that f1 and f2 both use imulq (but still with a 32-bit constant). Wouldn't this be slower?

On a 64-bit PowerPC, this is similar with GCC 8.3.1 (mulhwu for f1 and f3, mulhdu for f2).


---
