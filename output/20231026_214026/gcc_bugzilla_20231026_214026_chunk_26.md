### Total Bugs Detected: 4649
### Current Chunk: 26 of 30
### Bugs in this Chunk: 160 (From bug 4001 to 4160)
---


### compiler : `gcc`
### title : `[aarch64] _complex integer return types could be improved`
### open_at : `2022-10-11T03:07:31Z`
### last_modified_date : `2023-06-25T22:14:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107208
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
* gcc now generate 2 redundant mov instrunction compared to llvm
```
mul64(unsigned long _Complex, unsigned long _Complex):
        mov     x4, x1    // redundant ??
        mov     x5, x0    // redundant ??
        mul     x6, x1, x2
        mul     x2, x0, x2
        madd    x1, x5, x3, x6
        msub    x0, x4, x3, x2
        ret
```

* test case, https://godbolt.org/z/EWE3bc5b3
```
unsigned long _Complex mul64 (unsigned long _Complex mul0,
                              unsigned long _Complex mul1)
{
    return mul0 * mul1;
}
```


---


### compiler : `gcc`
### title : `SLP reduction results fail to reduce to a single accumulator`
### open_at : `2022-10-13T12:37:56Z`
### last_modified_date : `2022-10-13T14:30:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107247
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
float fl[128];
int x[128];
float
foo (int n1)
{
  float sum0, sum1, sum2, sum3;
  sum0 = sum1 = sum2 = sum3 = 0.0f;

  int n = (n1 / 4) * 4;
  for (int i = 0; i < n; i += 4)
    {
      sum0 += fabs (fl[i]);
      sum1 += fabs (fl[i + 1]);
      sum2 += fabs (fl[i + 2]);
      sum3 += fabs (fl[i + 3]);
      x[i] = 1;
    }

  return sum0 + sum1 + sum2 + sum3;
}

shows how we fail to reduce the SLP reduction accumulators to a single one
before extracting the elements:

  <bb 3> [local count: 567644343]:
  # sum0_37 = PHI <sum0_29(7), 0.0(9)>
  # sum1_39 = PHI <sum1_30(7), 0.0(9)>
  # sum2_41 = PHI <sum2_31(7), 0.0(9)>
  # sum3_43 = PHI <sum3_32(7), 0.0(9)>
  # i_45 = PHI <i_34(7), 0(9)>
  # vectp_fl.8_89 = PHI <vectp_fl.8_90(7), &fl(9)>
  # vect_sum3_43.15_102 = PHI <vect_sum3_32.16_106(7), { 0.0, 0.0, 0.0, 0.0 }(9)>
  # vect_sum3_43.15_103 = PHI <vect_sum3_32.16_107(7), { 0.0, 0.0, 0.0, 0.0 }(9)>
  # vect_sum3_43.15_104 = PHI <vect_sum3_32.16_108(7), { 0.0, 0.0, 0.0, 0.0 }(9)>
  # vect_sum3_43.15_105 = PHI <vect_sum3_32.16_109(7), { 0.0, 0.0, 0.0, 0.0 }(9)>
...
  vect__12.14_98 = ABS_EXPR <vect__11.10_91>;
  vect__12.14_99 = ABS_EXPR <vect__11.11_93>;
  vect__12.14_100 = ABS_EXPR <vect__11.12_95>;
  vect__12.14_101 = ABS_EXPR <vect__11.13_97>;
  vect_sum3_32.16_106 = vect__12.14_98 + vect_sum3_43.15_102;
  vect_sum3_32.16_107 = vect__12.14_99 + vect_sum3_43.15_103;
  vect_sum3_32.16_108 = vect__12.14_100 + vect_sum3_43.15_104;
  vect_sum3_32.16_109 = vect__12.14_101 + vect_sum3_43.15_105;
...

  <bb 11> [local count: 94607391]:
  # sum0_48 = PHI <sum0_29(3)>
  # sum1_36 = PHI <sum1_30(3)>
  # sum2_35 = PHI <sum2_31(3)>
  # sum3_24 = PHI <sum3_32(3)>
  # vect_sum3_32.16_110 = PHI <vect_sum3_32.16_106(3)>
  # vect_sum3_32.16_111 = PHI <vect_sum3_32.16_107(3)>
  # vect_sum3_32.16_112 = PHI <vect_sum3_32.16_108(3)>
  # vect_sum3_32.16_113 = PHI <vect_sum3_32.16_109(3)>
  _114 = BIT_FIELD_REF <vect_sum3_32.16_110, 32, 0>;
  _115 = BIT_FIELD_REF <vect_sum3_32.16_110, 32, 32>;
  _116 = BIT_FIELD_REF <vect_sum3_32.16_110, 32, 64>;
  _117 = BIT_FIELD_REF <vect_sum3_32.16_110, 32, 96>;
  _118 = BIT_FIELD_REF <vect_sum3_32.16_111, 32, 0>;
  _119 = BIT_FIELD_REF <vect_sum3_32.16_111, 32, 32>;
  _120 = BIT_FIELD_REF <vect_sum3_32.16_111, 32, 64>;
  _121 = BIT_FIELD_REF <vect_sum3_32.16_111, 32, 96>;
  _122 = BIT_FIELD_REF <vect_sum3_32.16_112, 32, 0>;
  _123 = BIT_FIELD_REF <vect_sum3_32.16_112, 32, 32>;
  _124 = BIT_FIELD_REF <vect_sum3_32.16_112, 32, 64>;
  _125 = BIT_FIELD_REF <vect_sum3_32.16_112, 32, 96>;
  _126 = BIT_FIELD_REF <vect_sum3_32.16_113, 32, 0>;
  _127 = BIT_FIELD_REF <vect_sum3_32.16_113, 32, 32>;
  _128 = BIT_FIELD_REF <vect_sum3_32.16_113, 32, 64>;
  _129 = BIT_FIELD_REF <vect_sum3_32.16_113, 32, 96>;
  _130 = _114 + _118;
  _131 = _115 + _119;
  _132 = _116 + _120;
  _133 = _117 + _121;
  _134 = _130 + _122;
  _135 = _131 + _123;
  _136 = _132 + _124;
  _137 = _133 + _125;
  _138 = _134 + _126;
  _139 = _135 + _127;
  _140 = _136 + _128;
  _141 = _137 + _129;
...

instead of doing vector adds and a single series of extracts.


---


### compiler : `gcc`
### title : `Load unnecessarily happens before malloc`
### open_at : `2022-10-13T14:54:19Z`
### last_modified_date : `2022-10-14T07:55:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107250
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
The following code will be compiled with the load of next happening before the call to malloc. This generates worse code than if the load is delayed until after the call to malloc.


struct Foo {
    Foo* next;
};
void ctx_push(Foo* f) {
    Foo tmp = { f->next };
    Foo *n = (Foo*)malloc(sizeof(Foo));
    *n = tmp;
    f->next = n;
}

Manually moving the load in this example improves the generated code:

struct Foo {
    Foo* next;
};

void ctx_push(Foo* f) {
    Foo *n = (Foo*)malloc(sizeof(Foo));
    Foo tmp = { f->next };
    *n = tmp;
    f->next = n;
}

https://gcc.godbolt.org/z/TnMj1c636


---


### compiler : `gcc`
### title : `RISC-V linux kernel compiled with -mno-relax generates a lot of local symbols`
### open_at : `2022-10-13T15:44:55Z`
### last_modified_date : `2022-10-14T07:38:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107251
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Previous reported at https://github.com/riscv-collab/riscv-gnu-toolchain/issues/1036

$ du -h btrfs.ko.*
76M	btrfs.ko.riscv64
42M	btrfs.ko.x86_64

$ strip --strip-debug btrfs.ko.x86_64
$ riscv64-linux-gnu-strip --strip-debug btrfs.ko.riscv64 

$ du -h btrfs.ko.*
17M	btrfs.ko.riscv64
2.8M	btrfs.ko.x86_64

$ strip --strip-debug --discard-locals btrfs.ko.x86_64
$ riscv64-linux-gnu-strip --strip-debug --discard-locals btrfs.ko.riscv64

$ du -h btrfs.ko.*
4.2M	btrfs.ko.riscv64
2.8M	btrfs.ko.x86_64

The above demonstrates that RISCV kernel modules are compiled with -mno-relax option, and yet a lot of relocation symbols are emitted and remain in the modules keeping their size very large.

Over on github issue it was mentioned that "There is a known problem with GNU as that we still emit extra symbols and relocations when -mno-relax is used. Kernel modules are compiled -mno-relax. So fixing this would help reduce the symtab size." However I was unable to find a bug report pointing this out.

Could you please ensure that when compiling code, the extra symbols and relocations are not emitted, or otherwise stripped? Such that calling --discard-locals would have no particular effect on the binary sizes. It is inconvenient for distributions to call --discard-locals on riscv, even when otherwise one is keeping the rest of the debug symbols for example.

I have observed this with gcc-10 and upwords, and most recently with gcc-12. I can try newer builds of gcc and/or any patches to confirm behaviour.


---


### compiler : `gcc`
### title : `Vectorize VxHF _Float16 modes using partial vectors without -mavx512vl`
### open_at : `2022-10-14T09:06:00Z`
### last_modified_date : `2022-10-14T09:24:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107259
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
The following testcase:

--cut here--
#define N 16

_Float16 r[N], a[N], b[N];

void foo (void)
{
  for (int i = 0; i < N; i++)
    r[i] = a[i] + b[i];
}
--cut here--

can be vectorized with -O2 -mavx512fp16 -mavx512vl, but not when -mavx512vl is omitted. However, the test can still be vectorized using partial vectors.

The testcase should be vectorized for all N < 32, even without -mavx512vl.


---


### compiler : `gcc`
### title : `Memcpy not elided when initializing struct`
### open_at : `2022-10-14T13:13:28Z`
### last_modified_date : `2022-10-17T13:28:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107263
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
With the following code

struct Foo {
    Foo* next;
    char arr[580];
};

void ctx_push(Foo* f) {
    Foo tmp = { f->next };
    *f = tmp;
}

Clang is able to generate code that just memsets `arr`. GCC instead initializes the entire struct on the stack and then copies it into `f`.

https://gcc.godbolt.org/z/Yzcbs4G71


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] return for structure is not as good as before`
### open_at : `2022-10-15T01:08:10Z`
### last_modified_date : `2023-07-07T10:44:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107270
### status : `UNCONFIRMED`
### tags : `missed-optimization, needs-bisection`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Take:
```

struct b
{
  int t;
  int tt;
};

union a
{
  long long t;
  struct b b;
};

struct b f(int i, int t)
{
  return (struct b){i, t};
}

long long f1(int i, int t)
{
  struct b b = {i, t};
  union a a;
  a.b = b;
  return a.t;
}
```
In GCC 8.x, GCC -O2 would produce:
```
f:
        .cfi_startproc
        bfi     x0, x1, 32, 32
        ret
f1:
        .cfi_startproc
        bfi     x0, x1, 32, 32
        ret
```
But in GCC 9 and above GCC produces:
```
f:
        .cfi_startproc
        uxtw    x0, w0
        orr     x0, x0, x1, lsl 32
        ret
f1:
        .cfi_startproc
        uxtw    x0, w0
        orr     x0, x0, x1, lsl 32
        ret
```


---


### compiler : `gcc`
### title : `comparisations with u/int64_t constants not generate vector-result`
### open_at : `2022-10-16T20:24:08Z`
### last_modified_date : `2022-10-17T08:06:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107281
### status : `WAITING`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
If no 64-bit vector comparisons are available no vectorized results are produced for the cases <=, >=, <, and >.

The cases == and != works. The comparisons themselves are then carried out individually, but the result is combined with unpcklqdq.

It would be better if this works with all comparisons so that can better (auto)vectorized.

It might be possible to further optimize this so that no scalar comparisons are necessary - especially for the frequent case constant=0.

https://godbolt.org/z/cj8n9TenK

thx
Gero


---


### compiler : `gcc`
### title : `conversions u/int64_t to float64/32_t are not vectorized`
### open_at : `2022-10-16T23:22:14Z`
### last_modified_date : `2023-02-15T20:30:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107283
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
The conversions u/int64_t to float64/32_t are not vectorisized if no HW-support (eg AVX512) available.

But we can do that manually
https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx

In the case u/int64_t -> float32_t i first convert to float64_t and then to float32_t. There might be a better way to implement this.

With HW-support the standard implementation is of course faster.

https://godbolt.org/z/WTa663PrK

thx
Gero


---


### compiler : `gcc`
### title : `[aarch64] Init big const value should be improved compare to llvm`
### open_at : `2022-10-19T11:26:27Z`
### last_modified_date : `2022-11-28T22:42:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107316
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
test case: https://godbolt.org/z/ahreYnahE
```
int main (int argc, char** argv)
{
  if (lshift_1 (0xaaaa5555aaaa5555ull) != 0ll)
    abort();

  return 0;
}
```
* gcc use 4 instructions
```
        mov     x0, 21845
        movk    x0, 0xaaaa, lsl 16
        movk    x0, 0x5555, lsl 32
        movk    x0, 0xaaaa, lsl 48
```

* while llvm use 3 instructions:
```
        mov     x0, #6148914691236517205
        movk    x0, #43690, lsl #16
        movk    x0, #43690, lsl #48
```


---


### compiler : `gcc`
### title : `bzhi pattern not recognized`
### open_at : `2022-10-20T08:03:30Z`
### last_modified_date : `2022-10-20T09:25:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107328
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
This bug looks similar to #93346 and #94860 but the pattern is slightly different. My understanding is that it is still a valid one (my understanding is that the shift invokes UB for values of `sz' that would be troublesome for bzhi)

The pattern: 

unsigned bzhi(unsigned y, unsigned sz) {
    return y & (~0u >> (CHAR_BIT * sizeof(unsigned) - sz));
}

it should work if `sizeof(unsigned)` is either 4 or 8.

Reproducer on godbolt: https://godbolt.org/z/4PfesvY7Y


---


### compiler : `gcc`
### title : `Optimization opportunity where integer '/' corresponds to '>>'`
### open_at : `2022-10-21T08:36:48Z`
### last_modified_date : `2022-11-28T22:13:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107342
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Created attachment 53742
variant of 'gcc.dg/tree-ssa/pr107195-3.c'

Discovered during work on "Add 'gcc.dg/tree-ssa/pr107195-3.c' [PR107195]" (which is pending approval for push to master branch) an optimization opportunity where integer '/' corresponds to '>>'.  See discussion in <https://inbox.sourceware.org/gcc-patches/CAGm3qMV5_7hEED6_NKNAFaiE5dFXapsrRGEd_MAqNiSsF15nmw@mail.gmail.com> about 'f4b'/'foo4b', and the "variant of 'gcc.dg/tree-ssa/pr107195-3.c'" attached here.


---


### compiler : `gcc`
### title : `[aarch64] should avoid the punpklo/punpkhi compare to llvm`
### open_at : `2022-10-23T05:51:45Z`
### last_modified_date : `2022-11-13T10:10:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107359
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
* case, https://godbolt.org/z/38bcszxdo
```
int check (char *mask, double *result, int n) {
    int count = 0;
    for (int i=0; i<n; i++) {
        if (mask[i] == 0 && result[i] != 2.0)
          count++;
    }
    return count;
}
```
gcc may also choose more wider SVE to avoid the punpklo/punpkhi.


---


### compiler : `gcc`
### title : `Always propagate __builtin_assume_aligned`
### open_at : `2022-10-25T07:57:14Z`
### last_modified_date : `2022-11-28T22:15:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107389
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.2.1`
### severity : `enhancement`
### contents :
Consider

typedef __uint128_t aligned_type __attribute__((aligned(16)));
_Static_assert(__alignof(aligned_type) == 16);
__uint128_t foo(aligned_type *p) { return __atomic_load_n(p, 0); }

For s390x, atomic_loadti should expand this to LPQ.

For my purposes, it must also do this at -O0, not just with
optimization.  But the alignment seen by gen_atomic_loadti
is only 8, so it FAILs the expansion and falls back to libatomic.

The following appears to solve the problem:

--- a/gcc/builtins.cc
+++ b/gcc/builtins.cc
@@ -468,8 +468,11 @@ get_pointer_alignment_1
        }
       else
        {
+         /* Assume alignment from the type. */
+         tree ptr_type = TREE_TYPE (exp);
+         tree obj_type = TREE_TYPE (ptr_type);
+         *alignp = TYPE_ALIGN (obj_type);
          *bitposp = 0;
-         *alignp = BITS_PER_UNIT;
          return false;
        }
     }

but I have an inkling that would have undesired effects
for other usages.  If so, perhaps a special case could be
made for the usage in get_builtin_sync_mem.


---


### compiler : `gcc`
### title : `Perf loss ~5% on 519.lbm_r SPEC cpu2017 benchmark with r10-5090-ga9a4edf0e71bba`
### open_at : `2022-10-26T09:05:04Z`
### last_modified_date : `2023-07-12T21:04:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107409
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53773
Input and source files.

Below is some perf data executing the 519.lbm_r benchmark on aarch64 architecture (Graviton 3 processor). I have comparison of the baseline perf (mainline commit ID: f56d48b2471c388401174029324e1f4c4b84fcdb) vs. a fix for the same (revert the code change in commit ID: a9a4edf0e71bbac9f1b5dcecdcf9250111d16889).

Steps to compile:
$ gcc -std=c99 -mabi=lp64 -g -Ofast -mcpu=native lbm.i main.i -lm -flto -o 519_lbm_r_base

$ time ./519_lbm_r_base 3000 reference.dat 0 0 100_100_130_ldc.of
real    2m50.946s

Reverting the code changes in commit ID: a9a4edf0e71bbac9f1b5dcecdcf9250111d16889
$ time ./519_lbm_r_fix 3000 reference.dat 0 0 100_100_130_ldc.of
real    2m42.091s

The code change reverted was in the following file:
* tree-cfg.c (execute_fixup_cfg): Update also max_bb_count when scaling happen.

Author: Jan Hubicka <hubicka@ucw.cz>
Date:   Sat Nov 30 22:25:24 2019 +0100

Please find attached the files to reproduce this issue and the fix.


---


### compiler : `gcc`
### title : `Miss to fold LEN_{LOAD,STORE} when the specified length equal to vector length`
### open_at : `2022-10-26T09:13:53Z`
### last_modified_date : `2022-12-05T05:28:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107412
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
test case:
===
#define N 16
int src[N];
int dest[N];

void foo (){
  for (int i = 0; i < (N-1); i++)
   dest[i] = src[i];
}

===

Options: -mcpu=power10 -fno-tree-loop-distribute-patterns --param vect-partial-vector-usage=2 -O2 -ftree-vectorize -funroll-loops -fno-vect-cost-model

optimized gimple output:

void foo ()
{
  vector(16) unsigned char vect_2;
  vector(16) unsigned char vect_13;
  vector(16) unsigned char vect_34;
  vector(16) unsigned char vect_47;

  <bb 2> [local count: 67108864]:
  vect_2 = .LEN_LOAD (&src, 128B, 16, 0);
  .LEN_STORE (&dest, 128B, 16, vect_2, 0);
  vect_34 = .LEN_LOAD (&MEM <int[16]> [(void *)&src + 16B], 128B, 16, 0);
  .LEN_STORE (&MEM <int[16]> [(void *)&dest + 16B], 128B, 16, vect_34, 0);
  vect_47 = .LEN_LOAD (&MEM <int[16]> [(void *)&src + 32B], 128B, 16, 0);
  .LEN_STORE (&MEM <int[16]> [(void *)&dest + 32B], 128B, 16, vect_47, 0);
  vect_13 = .LEN_LOAD (&MEM <int[16]> [(void *)&src + 48B], 128B, 12, 0);
  .LEN_STORE (&MEM <int[16]> [(void *)&dest + 48B], 128B, 12, vect_13, 0); [tail call]
  return;

}

It's expected that we only have one separated .LEN_LOAD and .LEN_STORE with length 12, the others can adopt just normal vector load/store.


---


### compiler : `gcc`
### title : `Perf loss ~14% on 519.lbm_r SPEC cpu2017 benchmark with r8-7132-gb5b33e113434be`
### open_at : `2022-10-26T09:16:04Z`
### last_modified_date : `2022-12-02T02:30:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107413
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53775
Input and source files.

Below is some perf data executing the 519.lbm_r benchmark on aarch64 architecture (Graviton 3 processor). I have comparison of the baseline perf (mainline commit ID: f56d48b2471c388401174029324e1f4c4b84fcdb) vs. a fix for the same (revert the code change in commit ID: b5b33e113434be909e8a6d7b93824196fb6925c0).

Steps to compile:
$ gcc -std=c99 -mabi=lp64 -g -Ofast -mcpu=native lbm.i main.i -lm -flto -o 519_lbm_r_base

$ time ./519_lbm_r_base 3000 reference.dat 0 0 100_100_130_ldc.of
real    2m50.946s

Reverting the code changes in commit ID: b5b33e113434be909e8a6d7b93824196fb6925c0
$ time ./519_lbm_r_fix 3000 reference.dat 0 0 100_100_130_ldc.of
real    2m27.157s

The code change reverted was:
    [AArch64] PR84114: Avoid reassociating FMA

Author: Wilco Dijkstra <wdijkstr@arm.com>
Date:   Mon Mar 5 14:40:55 2018 +0000

Please find attached the files to reproduce this issue and the fix.


---


### compiler : `gcc`
### title : `__builtin_convertvector generates inefficient code`
### open_at : `2022-10-27T10:02:40Z`
### last_modified_date : `2022-10-31T13:02:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107432
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Example: conversion int64_t -> int32_t

avx512f + avx512vl
HW conversions are available.

avx2
There is a correctly working 32-bit-permutation (_mm256_permutevar8x32_epi32/vpermd) that can be used.

I have not (yet) evaluated whether other conversions (larger int -> smaller int) are also affected.
PS: On x86 it's already hell to optimize all cases depending on the instruction set.
PPS: What about -march=znver4 ?

https://godbolt.org/z/3s79bnh7v

thx
Gero


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Removing switch with __builtin_unreachable causes missed optimizations`
### open_at : `2022-10-27T18:52:09Z`
### last_modified_date : `2023-07-07T10:44:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107443
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
Compile the following with -O2 -fdump-tree-all -fdisable-tree-evrp

void dead (unsigned n);
void foo (unsigned n);

void func (unsigned n)
{
  if (n == 0)
    __builtin_unreachable();
  if (n == 1)
    __builtin_unreachable();
  if (n == 2)
    __builtin_unreachable();
  if (n == 3)
    __builtin_unreachable();
  if (n == 4)
    __builtin_unreachable();
  if (n == 5)
    __builtin_unreachable();
  if (n == 6)
    __builtin_unreachable();
  if (n == 7)
    __builtin_unreachable();
  foo (n);
  if (n < 8)
    dead (n);
}

iftoswitch converts the series of if's into a switch:

  <bb 2> :
  switch (n_2(D)) <default: <L26> [INV], case 0: <L18> [INV], case 1: <L19> [INV], case 2: <L20> [INV], case 3: <L21> [INV], case 4: <L22> [INV], case 5: <L23> [INV], case 6: <L24> [INV], case 7: <L25> [INV]>
  <bb 3> :
<L18>:
  __builtin_unreachable ();
  <bb 4> :
<L19>:
  __builtin_unreachable ();
  <bb 5> :
<L20>:
  __builtin_unreachable ();
  <bb 6> :
<L21>:
  __builtin_unreachable ();
  <bb 7> :
<L22>:
  __builtin_unreachable ();
  <bb 8> :
<L23>:
  __builtin_unreachable ();
  <bb 9> :
<L24>:
  __builtin_unreachable ();
  <bb 10> :
<L25>:
  __builtin_unreachable ();
  <bb 11> :
<L26>:
  foo (n_2(D));
  if (n_2(D) <= 7)
    goto <bb 12>; [INV]
  else
    goto <bb 13>; [INV]

  <bb 12> :
  dead (n_2(D));

 
 switch conversion then runs, eliminates all the blocks, then bails on the conversion, leaving the IL without any of the __builtin_unreachable calls:

Beginning to process the following SWITCH statement ((null):0) : -------
switch (n_2(D)) <default: <L26> [INV], case 0: <L18> [INV], case 1: <L19> [INV], case 2: <L20> [INV], case 3: <L21> [INV], case 4: <L22> [INV], case 5: <L23> [INV], case 6: <L24> [INV], case 7: <L25> [INV]>

Removing basic block 3
Removing basic block 4
Removing basic block 5
Removing basic block 6
Removing basic block 7
Removing basic block 8
Removing basic block 9
Removing basic block 10
Bailing out - switch is a degenerate case
--------------------------------
Merging blocks 2 and 11
void func (unsigned int n)
{
  <bb 2> :
  foo (n_2(D));
  if (n_2(D) <= 7)
    goto <bb 3>; [INV]
  else
    goto <bb 4>; [INV]

  <bb 3> :
  dead (n_2(D));

  <bb 4> :
  return;

}

And now the optimizers cannot remove the call to dead() because we lose all the range information inferred from the __builtin_unreachable calls.


---


### compiler : `gcc`
### title : `Redundant moves for subregs move`
### open_at : `2022-10-27T22:50:20Z`
### last_modified_date : `2022-10-27T23:00:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107445
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Here is the testcase:
https://godbolt.org/z/4bcfx5a86

coalesce:
        ld4d    {z24.d - z27.d}, p0/z, [x0]
        mov     z5.d, z24.d
        mov     z4.d, z25.d
        mov     z3.d, z26.d
        mov     z2.d, z27.d
        cmp     w1, 0
        ble     .L2
        mov     w0, 0
        ld1d    z1.d, p0/z, [x2]
        ld1d    z0.d, p0/z, [x3]
.L3:
        add     w0, w0, 1
        movprfx z5.d, p0/z, z5.d
        mad     z5.d, p0/m, z1.d, z0.d
        movprfx z4.d, p0/z, z4.d
        mad     z4.d, p0/m, z1.d, z0.d
        movprfx z3.d, p0/z, z3.d
        mad     z3.d, p0/m, z1.d, z0.d
        movprfx z2.d, p0/z, z2.d
        mad     z2.d, p0/m, z1.d, z0.d
        cmp     w1, w0
        bne     .L3

It's obvious that we should be able to directly use z24 - z27 to perform mad
oberation and remove the redundant moves in the prologue.

I tried to implement register coalescing in IRA to fix it, but I am not sure whether it's a good idea to fix it?


---


### compiler : `gcc`
### title : `CD-DCE fails to eliminate abnormal incoming edges`
### open_at : `2022-10-28T13:09:02Z`
### last_modified_date : `2022-10-28T13:09:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107449
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #107447 +++

int n;

void
bar (int, int);

__attribute__ ((noinline, returns_twice)) int
zero (void)
{
  return 0;
}

void
foo (void)
{
  (void) zero ();

  n = 0;

  for (;;)
    bar (zero (), n);
}

With this testcase GCC fails to eliminate the abnormal edges into the
first zero () call which is elided by CD-DCE.


---


### compiler : `gcc`
### title : `Suboptimal codegen for some branch-on-zero cases`
### open_at : `2022-10-29T09:53:45Z`
### last_modified_date : `2022-11-16T21:30:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107455
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Created attachment 53788
code sequence from https://github.com/embench/embench-iot

gcc -S -Os -march=rv32gc -mabi=ilp32 test.c


```
sglib_dllist_len:
	beq	a0,zero,.L6
	mv	a4,a0
	li	a5,0
.L3:
	lw	a4,8(a4)
	addi	a5,a5,1
	bne	a4,zero,.L3
	lw	a4,4(a0)
	li	a0,0
.L4:
	bne	a4,zero,.L5
	add	a0,a0,a5
	ret
.L5:
	lw	a4,4(a4)
	addi	a0,a0,1
	j	.L4
.L6:
	li	a0,0
	ret
```

li a0,0 is unnecessary, and this extra instruction might lead to a worse cfg and bad code size. I spotted several size suboptimal cases related to this pattern.


result on clang:
```
sglib_dllist_len:
	beqz	a0, .LBB0_4
	mv	a1, a0
	li	a0, -1
	mv	a2, a1
.LBB0_2:
	lw	a2, 8(a2)
	addi	a0, a0, 1
	bnez	a2, .LBB0_2
.LBB0_3:
	lw	a1, 4(a1)
	addi	a0, a0, 1
	bnez	a1, .LBB0_3
.LBB0_4:
	ret
```

Similar problem on arm64: https://godbolt.org/z/Yo6jsKMGz


---


### compiler : `gcc`
### title : `std::atomic::fetch_xxx generate LOCK CMPXCHG instead of simpler LOCK instructions`
### open_at : `2022-10-29T11:13:16Z`
### last_modified_date : `2022-11-01T14:20:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107456
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
The code generation for several std::atomic::fetch_ operations is suboptimal on IA-32 and AMD64.

Related clang issue: https://github.com/llvm/llvm-project/issues/58685

I verified this with "-c -O2" or "-c -O2 -m32 -march=i686" of the following:

#include <atomic>

// "void" functions generate the minimal IA-32 or AMD64 code
void lock_add(std::atomic<uint32_t> &a, uint32_t b) { a.fetch_add(b); }
void lock_sub(std::atomic<uint32_t> &a, uint32_t b) { a.fetch_sub(b); }
void lock_or(std::atomic<uint32_t> &a, uint32_t b) { a.fetch_or(b); }
void lock_and(std::atomic<uint32_t> &a, uint32_t b) { a.fetch_and(b); }
void lock_xor(std::atomic<uint32_t> &a, uint32_t b) { a.fetch_xor(b); }
// clang++-15: "lock inc"; g++-12: "lock add"
void lock_inc(std::atomic<uint32_t> &a) { a.fetch_add(1); }
// clang++-15: "lock dec"; g++-12: "lock sub"
void lock_dec(std::atomic<uint32_t> &a) { a.fetch_sub(1); }

// "lock add" degrades to lock xadd; add
uint32_t lock_add_result(std::atomic<uint32_t> &a, uint32_t b)
{
  return b + a.fetch_add(b);
}

// "lock sub" degrades to neg; lock xadd; sub
uint32_t lock_sub_result(std::atomic<uint32_t> &a, uint32_t b)
{
  return a.fetch_sub(b) - b;
}

// "lock or" degrades to lock cmpxchg
uint32_t lock_or_or(std::atomic<uint32_t> &a, uint32_t b)
{
  return a.fetch_or(b) | b;
}

// "lock or; and" degrades to lock cmpxchg
uint32_t lock_or_andneg(std::atomic<uint32_t> &a, uint32_t b)
{
  return a.fetch_or(b) & ~b;
}

// "lock and" degrades to lock cmpxchg
uint32_t lock_and_and(std::atomic<uint32_t> &a, uint32_t b)
{
  return a.fetch_and(b) & b;
}

// "lock and; or" degrades to lock cmpxchg
uint32_t lock_and_orneg(std::atomic<uint32_t> &a, uint32_t b)
{
  return a.fetch_and(b) | ~b;
}

// "lock xor; or" degrades to lock cmpxchg
uint32_t lock_xor_or(std::atomic<uint32_t> &a, uint32_t b)
{
  return a.fetch_xor(b) | b;
}

// "lock xor; and" degrades to lock cmpxchg
uint32_t lock_xor_andneg(std::atomic<uint32_t> &a, uint32_t b)
{
  return a.fetch_xor(b) & ~b;
}


---


### compiler : `gcc`
### title : `std::fma generates slow scalar-call`
### open_at : `2022-10-29T23:33:53Z`
### last_modified_date : `2022-10-30T00:13:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107458
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `unknown`
### severity : `normal`
### contents :
Please see https://godbolt.org/z/bxxc9ezeM

thx
Gero


---


### compiler : `gcc`
### title : `Missed optimization of std::atomic::fetch_xxx "null operations"  to std::atomic::load()`
### open_at : `2022-10-30T09:29:24Z`
### last_modified_date : `2023-05-19T03:08:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107462
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `enhancement`
### contents :
In g++-12 -c -O2 targeting x86-64, the fetch_xxx() variants are translated to LOCK XADD or loops around LOCK CMPXCHG, instead of the simple MOV. I did not test other targets.

#include <atomic>
uint32_t load(const std::atomic<uint32_t> &a)
{ return a.load(std::memory_order_relaxed); }
uint32_t load_add(std::atomic<uint32_t> &a)
{ return a.fetch_add(0, std::memory_order_relaxed); }
uint32_t load_sub(std::atomic<uint32_t> &a)
{ return a.fetch_sub(0, std::memory_order_relaxed); }
uint32_t load_or(std::atomic<uint32_t> &a)
{ return a.fetch_or(0, std::memory_order_relaxed); }
uint32_t load_xor(std::atomic<uint32_t> &a)
{ return a.fetch_xor(0, std::memory_order_relaxed); }
uint32_t load_and(std::atomic<uint32_t> &a)
{ return a.fetch_and(~uint32_t(0), std::memory_order_relaxed); }

clang++-15 would translate each function to the same x86-64 code:
   0:	8b 07                	mov    (%rdi),%eax
   2:	c3                   	ret
When using the default std::memory_order_seq_cst, an MFENCE instruction would be emitted before the MOV except in load().


---


### compiler : `gcc`
### title : `[13 Regression] 433.milc regressed by 6-8% on zen3 at -O2 -flto`
### open_at : `2022-11-01T17:27:45Z`
### last_modified_date : `2023-02-01T09:31:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107499
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
LNT reports a 8.5% regression of 433.milc when compiled with -O2 -flto
on a zen3 machine:

https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=469.70.0

On a similar machine I reproduced it as a 6.5% regression and bisected
it to 19295e8607d (tree-optimization/100756 - niter analysis and
folding).

Possibly related, possibly not:

- An intel machine results show something which might be a 2%
  regression around the same time at -O2 -flto -march=native.
  See https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=790.70.0 

- In the same time-frame there is also a 5% performance drop on zen2
  with -Ofast -flto -march=native, but those results have been rather
  flaky in the past (see PR 101296):
  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=289.70.0

- there is a 11% regression reported on the same zen3 machine when
  using O2 flto -march=native but either I cannot reproduce it or it
  does not seem to be caused by the same commit.  I'll wait for more
  LNT results before investigating.  See
  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=465.70.0


---


### compiler : `gcc`
### title : `Useless atexit entry for ~constant_init in eh_globals.cc`
### open_at : `2022-11-01T17:53:15Z`
### last_modified_date : `2023-05-08T12:25:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107500
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.2.0`
### severity : `normal`
### contents :
I have this embedded firmware project of mine, which uses Newlib:

https://github.com/rdiez/DebugDue

It is the template for other similar bare-metal projects I have. Even though some targets have very little SRAM (like 16 KiB), I am still using C++ and exceptions. The project above documents how I configure GCC to that effect.

Up until GCC 11.2, I have been doing this check:

  if ( _GLOBAL_ATEXIT != nullptr )
  {
    Panic( "Unexpected entries in atexit table." );
  }

On devices with very little RAM and without an operating system, initialisation and destruction can be tricky. With the check above, I am making sure that nothing unexpected comes up in that respect. I am initialising all static objects manually anyway, to prevent any ordering surprises (the initialisation order of C++ static objects can be problematic too).

The check above fails with GCC 12.2. Apparently, a destructor called constant_init::~constant_init() gets added to the atexit table on start-up.

Because of the way that Newlib works, that wastes 400 bytes of SRAM, which corresponds to sizeof( _global_atexit0 ). The structure has room for 32 atexit calls (because of some ANSI conformance), but we are only using 1 entry.

The interesting thing is that the destructor is supposed to be empty, see:

https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/libsupc%2B%2B/eh_globals.cc

~constant_init() { /* do nothing, union member is not destroyed */ }

GCC generates the following code for that empty destructor:

0008da68 <(anonymous namespace)::constant_init::~constant_init()>:
  8da68:  b580  push  {r7, lr}
  8da6a:  af00  add   r7, sp, #0
  8da6c:  bd80  pop   {r7, pc}

That does not make any sense. Is there a way to prevent GCC from registering such an empty atexit function? Failing that, is there a way to prevent GCC from registering a particular atexit function, even if it is not empty?

I find surprising that GCC emits such code. My project is building its own GCC/Newlib toolchain with optimisation level "-Os", so I would expect at least the "add r7, sp, #0" to be optimised away.


---


### compiler : `gcc`
### title : `GCC fails to detect foo(first_const,last)-first_const+first pattern`
### open_at : `2022-11-01T19:34:13Z`
### last_modified_date : `2022-11-01T22:56:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107502
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
GCC fails to detect foo(first_const,last)-first_const+first pattern.

This is to use C++ std::contiguous_iterator interfaces with char const* foo(char const* first, char const* last) noexcept; like APIs. However, GCC fails to detect they are the same in IR and make them tail calls.

See godbolt:

https://godbolt.org/z/67PvYPar8


---


### compiler : `gcc`
### title : `tail call missed with struct wrapper`
### open_at : `2022-11-01T19:50:22Z`
### last_modified_date : `2022-11-05T10:48:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107503
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int *g();
f h()
{
    return {g()};
}

f should tail call to g but currently does not.


---


### compiler : `gcc`
### title : `Optimize std::lerp(d, d, 0.5)`
### open_at : `2022-11-03T21:15:50Z`
### last_modified_date : `2022-11-04T11:34:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107520
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
In some C++ code I have, it would be convenient if the compiler, possibly with the help of the standard library, could make the following function cheap, ideally just the identity. I'll probably end up wrapping lerp with a function that first checks with __builtin_constant_p if the 2 bounds are equal, but I'll post this in case people have ideas how to improve things.

#include <cmath>
double f(double d){
  return std::lerp(d, d, .5);
}

Currently, with -O3, we generate

	movapd	%xmm0, %xmm1
	pxor	%xmm0, %xmm0
	comisd	%xmm1, %xmm0
	jnb	.L7
	comisd	%xmm0, %xmm1
	jb	.L6
.L7:
	pxor	%xmm0, %xmm0
	ucomisd	%xmm0, %xmm1
	jp	.L6
	je	.L11
.L6:
	movapd	%xmm1, %xmm0
	subsd	%xmm1, %xmm0
	mulsd	.LC1(%rip), %xmm0
	addsd	%xmm1, %xmm0
	maxsd	%xmm1, %xmm0
	ret
	.p2align 4,,10
	.p2align 3
.L11:
	mulsd	.LC1(%rip), %xmm1
	movapd	%xmm1, %xmm0
	addsd	%xmm1, %xmm0
	ret

(clang is better at avoiding the redundant comparison)

With -fno-trapping-math to help a bit, I see at the beginning

  if (d_2(D) == 0.0)
    goto <bb 3>; [34.00%]
  else
    goto <bb 4>; [66.00%]

  <bb 3> [local count: 475287355]:
  _7 = d_2(D) * 5.0e-1;
  _10 = _7 * 2.0e+0;

I think that even with the default -fsigned-zeros, simplifying to _10 = d_2(D) is valid.

Adding -fno-signed-zeros

  <bb 2> [local count: 1073741824]:
  if (d_2(D) == 0.0)
    goto <bb 5>; [34.00%]
  else
    goto <bb 3>; [66.00%]

  <bb 3> [local count: 598454470]:
  _13 = d_2(D) - d_2(D);
  _14 = _13 * 5.0e-1;
  __x_15 = d_2(D) + _14;
  if (d_2(D) u>= __x_15)
    goto <bb 5>; [50.00%]
  else
    goto <bb 4>; [50.00%]

  <bb 4> [local count: 299227235]:

  <bb 5> [local count: 1073741825]:
  # _12 = PHI <d_2(D)(2), __x_15(4), d_2(D)(3)>
  return _12;

_13 is 0 or NaN, which doesn't change for _14, and __x_15 is just d_2, so we always return d_2.


---


### compiler : `gcc`
### title : `Inefficient code sequence for fp16 testcase on aarch64`
### open_at : `2022-11-05T07:42:43Z`
### last_modified_date : `2023-05-19T04:12:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107533
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
Derived from PR92999 



struct phalf {
    __fp16 first;
    __fp16 second;
};

struct phalf phalf_copy(struct phalf* src) __attribute__((noinline));
struct phalf phalf_copy(struct phalf* src) {
    return *src;
}

Compiling for AArch64 with a recent enough compiler produces. 

phalf_copy:
        ldr	w0, [x0]
	ubfx	x1, x0, 0, 16
	lsr	w0, w0, 16
	dup	v0.4h, w1
	dup	v1.4h, w0
	ret


Couldn't it just be ldr h0, [x0]
                    ldr h1, [x0, 2] 

IIRC this is in base v8 rather than v8.2 


regards
Ramana


---


### compiler : `gcc`
### title : `[11/12 Regression] simd, redundant pcmpeqb and pxor`
### open_at : `2022-11-07T06:39:28Z`
### last_modified_date : `2023-07-07T10:44:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107546
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
Hello,

this code sample(https://godbolt.org/z/TnGMsfMs6):
```
#include <x86intrin.h>

auto foo(const char *p) {
    const auto substr = _mm_loadu_si128((const __m128i *)p);
    return _mm_cmplt_epi8(substr, _mm_set1_epi8('0'));
}

```
produces the following ASM code:
```
1: foo(char const*):
2:    movdqu  xmm0, XMMWORD PTR [rdi]
3:    pxor    xmm1, xmm1
4:    pcmpgtb xmm0, XMMWORD PTR .LC0[rip]
5:    pcmpeqb xmm0, xmm1
6:    ret
```
please look at line 5.
is there any reason for `pcmpeqb` and `pxor` instruction?

just for info, clang's output(https://godbolt.org/z/MPnvEMdhr):
```
1: foo(char const*):
2:    movdqu  xmm1, xmmword ptr [rdi]
3:    movdqa  xmm0, xmmword ptr [rip + .LCPI0_0]
4:    pcmpgtb xmm0, xmm1
5:    ret
```

it looks like the issue started at version 9 and up to the current trunk.


---


### compiler : `gcc`
### title : `STV doesn't consider vec_select`
### open_at : `2022-11-07T10:25:08Z`
### last_modified_date : `2022-12-26T13:27:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107548
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
typedef unsigned int v4si __attribute__((vector_size(16)));

unsigned f (v4si a, v4si b)
{
  a[0] += b[0];
  return a[0] + a[1];
}

gets optimized to

f:
.LFB0:
        .cfi_startproc
        vpextrd $1, %xmm0, %edx
        vmovd   %xmm0, %eax
        addl    %edx, %eax
        vmovd   %xmm1, %edx
        addl    %edx, %eax
        ret

with znver2 arch, but similar with others while it seems to be beneficial
to shuffle a[1] to a'[0] and perform the add on the vector side eliding
two xmm->gpr moves.  STV2 sees

   19: r94:V4SI=xmm0:V4SI
      REG_DEAD xmm0:V4SI
    2: r87:V4SI=r94:V4SI
      REG_DEAD r94:V4SI
   20: r95:V4SI=xmm1:V4SI
      REG_DEAD xmm1:V4SI
    3: NOTE_INSN_DELETED
    4: NOTE_INSN_FUNCTION_BEG
    7: r90:SI=vec_select(r87:V4SI,parallel)
    8: r91:SI=vec_select(r87:V4SI,parallel)
      REG_DEAD r87:V4SI
    9: {r92:SI=r90:SI+r91:SI;clobber flags:CC;}
      REG_DEAD r91:SI
      REG_DEAD r90:SI
      REG_UNUSED flags:CC
   10: r93:SI=vec_select(r95:V4SI,parallel)
      REG_DEAD r95:V4SI
   11: {r89:SI=r92:SI+r93:SI;clobber flags:CC;}
      REG_DEAD r93:SI
      REG_DEAD r92:SI
      REG_UNUSED flags:CC
   16: ax:SI=r89:SI
      REG_DEAD r89:SI
   17: use ax:SI

but it lacks vec_select support:

Created a new instruction chain #1
Building chain #1...
  Adding insn 9 to chain #1
  Adding insn 11 into chain's #1 queue
  r90 def in insn 7 isn't convertible
  Mark r90 def in insn 7 as requiring both modes in chain #1
  r91 def in insn 8 isn't convertible
  Mark r91 def in insn 8 as requiring both modes in chain #1
  Adding insn 11 to chain #1
  r89 use in insn 16 isn't convertible
  Mark r89 def in insn 11 as requiring both modes in chain #1
  r93 def in insn 10 isn't convertible
  Mark r93 def in insn 10 as requiring both modes in chain #1
Collected chain #1...
  insns: 9, 11
  defs to convert: r89, r90, r91, r93
Computing gain for chain #1...
  Instruction conversion gain: 0
  Registers conversion cost: 24
  Total gain: -24
Chain #1 conversion is not profitable


---


### compiler : `gcc`
### title : `[13 Regression] g++.dg/pr71488.C  and [g++.dg/warn/Warray-bounds-16.C -m32] regression due to -Wstringop-overflow problem`
### open_at : `2022-11-07T20:06:42Z`
### last_modified_date : `2023-03-30T11:17:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107561
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
g++.dg/pr17488.C fails after commit r13-3761 with:

warning: ‘void* __builtin_memcpy(void*, const void*, long unsigned int)’ writing 8 or more bytes into a region of size 0 overflows the destination [-Wstringop-overflow=]
...
...
note: destination object of size 0 allocated by ‘operator new’

There is no difference in the IL the -Wstringop-overflow pass sees.  For
that matter, there is no difference in the entire IL across through
the *.optimized dump.  However, there are different ranges for two SSA
names.

In the *.waccess1 pass we see:

--- /tmp/a.ii.025t.waccess1.orig.787460 2022-11-07 19:43:41.341855227 +0100
+++ /tmp/a.ii.025t.waccess1.new.787460  2022-11-07 19:43:41.344855238 +0100
@@ -331,8 +331,8 @@
   max_depth:          3
 
 pointer_query cache contents:
-  5.0[1]: _5 = _5 (base0); size: unknown
-  6.0[3]: _6 = _5 (base0); size: unknown
+  5.0[1]: _5 = _5 (base0); size: [16, 9223372036854775807]
+  6.0[3]: _6 = _5 (base0); size: [16, 9223372036854775807]
 
 __attribute__((malloc))
 struct valarray * std::__valarray_get_storage<std::valarray<long long int> > (size_t __n)
@@ -364,8 +364,8 @@
   max_depth:          3
 
 pointer_query cache contents:
-  5.0[1]: _5 = _5 (base0); size: unknown
-  6.0[3]: _6 = _5 (base0); size: unknown
+  5.0[1]: _5 = _5 (base0); size: [8, 9223372036854775807]
+  6.0[3]: _6 = _5 (base0); size: [8, 9223372036854775807]
 
 __attribute__((malloc))
 long long int * std::__valarray_get_storage<long long int> (size_t __n)

This coincides with two unsigned multiplications by 16 and 8, where we correctly restrict the range.  This is from the evrp dump:

-Global Exported: _2 = [irange] long unsigned int [0, +INF] NONZERO 0xfffffffffffffff0
+Global Exported: _2 = [irange] long unsigned int [0, 0][16, +INF] NONZERO 0xfffffffffffffff0

-Global Exported: _2 = [irange] long unsigned int [0, +INF] NONZERO 0xfffffffffffffff8
+Global Exported: _2 = [irange] long unsigned int [0, 0][8, +INF] NONZERO 0xfffffffffffffff8

This is correct as we know the ranges cannot be [1,15] and [1,7] respectively.


---


### compiler : `gcc`
### title : `__builtin_shufflevector fails to pshufd instructions under default x86_64 compilation toggle which is the sse2 one`
### open_at : `2022-11-07T22:56:27Z`
### last_modified_date : `2022-11-09T05:43:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107563
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
#if defined(__SSE2__)

using temp_vec_type [[__gnu__::__vector_size__ (16)]] = char;
void foo(temp_vec_type& v) noexcept
{
	v=__builtin_shufflevector(v,v,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0);
}

#endif

g++ -S pq.cc -Ofast
proves sse2 is enabled by default, but it does not call
https://www.felixcloutier.com/x86/pshufb
neither
https://www.felixcloutier.com/x86/pshufd

while g++ -S pq.cc -Ofast -msse4.2 will generate them correctly. Which is buggy


---


### compiler : `gcc`
### title : `Fail to recognize overflow check for addition of __uint128_t operands`
### open_at : `2022-11-08T00:00:40Z`
### last_modified_date : `2022-11-08T09:38:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107564
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
Consider:

char f128(__uint128_t m, __uint128_t n) {
    #if !defined(USE_BUILTIN_ADD_OVERFLOW)
        m += n;
        return m < n;
    #else
        __uint128_t r;
        return __builtin_add_overflow(m, n, &r);
    #endif
}

When USE_BUILTIN_ADD_OVERFLOW is undefined, GCC fails to recognise this is an overflow check and with -O3 generates this:

        mov     r8, rdi
        mov     rax, rsi
        mov     rdi, rax
        mov     rsi, r8
        mov     rax, rdx
        mov     rdx, rcx
        add     rsi, rax
        adc     rdi, rcx
        cmp     rsi, rax
        mov     rcx, rdi
        sbb     rcx, rdx
        setc    al
        ret

When USE_BUILTIN_ADD_OVERFLOW is defined, it generates better code but still suboptimal:

        mov     r8, rdi
        mov     rax, rsi
        mov     rsi, r8
        mov     rdi, rax
        add     rsi, rdx
        adc     rdi, rcx
        setc    al
        ret

For other unsigned integer types GCC generates the same optimal code for both methods. For instance for uint64_t:

        add     rdi, rsi
        setc    al
        ret

https://godbolt.org/z/bj4M5no4j


---


### compiler : `gcc`
### title : `[13 Regression] Failure to optimize std::isfinite since r13-3596`
### open_at : `2022-11-08T12:09:24Z`
### last_modified_date : `2023-03-23T16:22:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107569
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Since r13-3596-ge7310e24b1c0ca67b1bb507c1330b2bf39e59e32
we don't optimize foo in 1 into return 1:

namespace std
{
  constexpr bool isfinite (float x) { return __builtin_isfinite (x); }
  constexpr bool isfinite (double x) { return __builtin_isfinite (x); }
  constexpr bool isfinite (long double x) { return __builtin_isfinite (x); }
}

bool
foo (double x)
{
  if (!std::isfinite (x))
    __builtin_unreachable ();

  return std::isfinite (x);
}

bool
bar (double x)
{
  [[assume (std::isfinite (x))]];
  return std::isfinite (x);
}

While bar hasn't been optimized before, it would be nice to make it working too.


---


### compiler : `gcc`
### title : `range-op{,-float}.cc for x * x`
### open_at : `2022-11-09T15:37:06Z`
### last_modified_date : `2023-04-05T07:14:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107591
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
int
foo (int x)
{
  if (x < -13 || x > 26)
    return -1;
  return x * x;
}

results in
x_4(D) :      [irange] int [-13, 26]
_5  : [irange] int [-338, 676]
That is unnecessarily pessimized, because it only computes [-13, 26] * [-13, 26]
range without taking into account that both operands are the same.
For the powi (x, 2) case the range is actually [0, 26 * 26], i.e. we shouldn't
do a cross product for it, just compute the -13 * -13, 0 * 0 (if 0 is in the range) and 26 * 26 products and form from that the range (I admit I haven't thought about unsigned or wrapping stuff).

On the PR107569 testcase it is on frange:
  _3 = u_2(D)->x;
  _6 = _3 * _3;
  _7 = u_2(D)->y;
  _8 = _7 * _7;
  _9 = _6 + _8;
  if (_9 u>= 0.0)
If we don't know anything further about u_2(D)->x and u_2(D)->y, VARYING * VARYING is [0, +Inf] +-NAN, added twice is the same (note, unless -fno-signed-zeros, it should be really [+0, +Inf] +-NAN, not [-0, +Inf] +-NAN, but it doesn't matter for the u>= 0.0 comparison).
And then we can fold u>= 0.0 to true.


---


### compiler : `gcc`
### title : `Change SLOW_BYTE_ACCESS  into WIDEN_MODE_ACCESS_BITFIELD target hook`
### open_at : `2022-11-10T01:04:42Z`
### last_modified_date : `2023-10-09T20:46:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107601
### status : `ASSIGNED`
### tags : `internal-improvement, missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
This comes up every few years even. The name SLOW_BYTE_ACCESS and documentation is confusing even and does not describe what it does.
I found that HJL proposed a decent naming target macro back in 2006 which I will be using as a basis going forward.

HJL's patch
https://gcc.gnu.org/legacy-ml/gcc-patches/2006-09/msg00555.html

Wilco Dijkstra's patch which just removes SLOW_BYTE_ACCESS 
https://gcc.gnu.org/legacy-ml/gcc-patches/2017-11/msg01547.html

This is part of my bit-field lowering work too.


---


### compiler : `gcc`
### title : `[13 Regression] Failure on fold-overflow-1.c and pr95115.c`
### open_at : `2022-11-10T09:47:55Z`
### last_modified_date : `2023-01-27T11:30:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107608
### status : `RESOLVED`
### tags : `missed-optimization, wrong-code`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
With my https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107569#c20 patch
I get
+FAIL: gcc.dg/fold-overflow-1.c scan-assembler-times 2139095040 2
regression because
float foo(void)
{
  return  __FLT_MAX__ + __FLT_MAX__;
}
no longer raises overflow exception.  This case is canonicalized to
  return __FLT_MAX__ * 2.0f;
and ranger correctly determines the result is [+Inf, +Inf], but then
comes dom2 and happily replaces
  _1 = 3.4028234663852885981170418348451692544e+38 * 2.0e+0;
  return _1;
with
  _1 = 3.4028234663852885981170418348451692544e+38 * 2.0e+0;
  return  Inf;
(I think this is still correct) and then dce3 removes the
  _1 = 3.4028234663852885981170418348451692544e+38 * 2.0e+0;
statement.  That is not correct, because -ftrapping-math is on and this multiplication is supposed to trap.
Now, my patch isn't needed, if I do:
float bar(void)
{
  return  __FLT_MAX__ + (__FLT_MAX__ / 4.0f);
}
instead, then the same regressed already on vanilla trunk:
  _1 = 3.4028234663852885981170418348451692544e+38 + 8.507058665963221495292604587112923136e+37;
  return _1;
changes in dom2 to
  _1 = 3.4028234663852885981170418348451692544e+38 + 8.507058665963221495292604587112923136e+37;
  return  Inf;
and in dce3 to just
  return  Inf;


---


### compiler : `gcc`
### title : `Missing optimization of switch-statement`
### open_at : `2022-11-10T20:11:04Z`
### last_modified_date : `2023-04-17T06:50:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107622
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `13.0`
### severity : `normal`
### contents :
In the following example the default-case could not be reached. Therefore introducing std::unreachable() should be useless. But the compiler produces slightly better code with std::unreachable() as it removes one (unneccessary) comparison (tested for x86 and avr targets).

volatile uint8_t o;

enum class State : uint8_t {A, B, C};

void g(const State s) {
    switch(s) {
    case State::A:
        o = 10;
        break;
    case State::B:
        o = 11;
        break;
    case State::C:
        o = 12;
        break;
    default:
//        std::unreachable(); // __builtin_unreachable();
        break;
    }
}


---


### compiler : `gcc`
### title : `[13 Regression] int128_t shift generates extra xor/or.`
### open_at : `2022-11-11T02:06:50Z`
### last_modified_date : `2022-12-08T13:57:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107627
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
The case is from PR106220.

#include<stdint.h>
static __inline
unsigned __int128 mk_u128(uint64_t hi, uint64_t lo)
{
  return ((unsigned __int128)hi << 64) | lo;
}

static __inline
uint64_t shrdq(uint64_t hi, uint64_t lo, unsigned rshift)
{
  return (uint64_t)(mk_u128(hi, lo) >> (rshift % 64));
}


void foo1to1(uint64_t *dst, const uint64_t* src, unsigned rshift)
{
  uint64_t r0 = shrdq(src[0], src[1], rshift);
  dst[0] = r0;
}


GCC trunk generates 

foo1to1:
        mov     rax, QWORD PTR [rsi]
        mov     r8, QWORD PTR [rsi+8]
        mov     ecx, edx
        xor     r9d, r9d
        mov     rdx, rax
        xor     eax, eax
        or      rax, r8
        or      rdx, r9
        shrd    rax, rdx, cl
        mov     QWORD PTR [rdi], rax

GCC12.2 generates

foo1to1:
        mov     rax, QWORD PTR [rsi+8]
        mov     r8, rdi
        mov     rdi, QWORD PTR [rsi]
        mov     ecx, edx
        shrd    rax, rdi, cl
        mov     QWORD PTR [r8], rax
        ret


---


### compiler : `gcc`
### title : `GCC unable to reason about range of idx/len`
### open_at : `2022-11-11T15:44:35Z`
### last_modified_date : `2022-11-14T16:07:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107639
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Clang 14 is able to optimize this function to just 'ret'. GCC 12.2 is not.

#include <cstddef>

void do_checks(const int* begin, const size_t len){
    size_t idx = 0;
    const auto end = begin + len;
    for (const int* it = begin; it!=end; ++it, ++idx){
        if (idx <= len){
            // Do something useful
        }
        else
        {
            throw 5;
        }
    }
}

https://godbolt.org/z/f7PjjqG9T


---


### compiler : `gcc`
### title : `LOGICAL_OP_NON_SHORT_CIRCUIT vs BRANCH_COST confusion`
### open_at : `2022-11-11T16:36:46Z`
### last_modified_date : `2023-08-05T17:31:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107642
### status : `ASSIGNED`
### tags : `documentation, internal-improvement, missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
LOGICAL_OP_NON_SHORT_CIRCUIT is defined as:

@defmac LOGICAL_OP_NON_SHORT_CIRCUIT
Define this macro if a non-short-circuit operation produced by
@samp{fold_range_test ()} is optimal.  This macro defaults to true if
@code{BRANCH_COST} is greater than or equal to the value 2.
@end defmac

Some targets define it to 0 or something else:
apinski@xeond:~/src/upstream-gcc-git/gcc/gcc/config$ git grep LOGICAL_OP_NON_SHORT_CIRCUIT */*.h
arc/arc.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT \
arm/arm.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT                                  \
csky/csky.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT \
loongarch/loongarch.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT 0
mips/mips.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT 0
msp430/msp430.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT 0
nds32/nds32.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT 0
pru/pru.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT          0
riscv/riscv.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT 0
rs6000/rs6000.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT 0
visium/visium.h:#define LOGICAL_OP_NON_SHORT_CIRCUIT 0

arc.h:
#define LOGICAL_OP_NON_SHORT_CIRCUIT \
  (BRANCH_COST (optimize_function_for_speed_p (cfun), \
                false) > 9)

arm.h:
#define LOGICAL_OP_NON_SHORT_CIRCUIT                                    \
  ((optimize_size)                                                      \
   ? (TARGET_THUMB ? false : true)                                      \
   : TARGET_THUMB ? static_cast<bool> (current_tune->logical_op_non_short_circuit_thumb) \
   : static_cast<bool> (current_tune->logical_op_non_short_circuit_arm))

csky.h (which is the default it seems):
#define LOGICAL_OP_NON_SHORT_CIRCUIT \
  (csky_default_logical_op_non_short_circuit ())
...
bool
csky_default_logical_op_non_short_circuit (void)
{
  return BRANCH_COST (optimize_function_for_speed_p (cfun), false) >= 2;
}

LOGICAL_OP_NON_SHORT_CIRCUIT is used in many few places than fold_range_test these days too.
fold-const.cc:#ifndef LOGICAL_OP_NON_SHORT_CIRCUIT
fold-const.cc:#define LOGICAL_OP_NON_SHORT_CIRCUIT \
fold-const.cc:  bool logical_op_non_short_circuit = LOGICAL_OP_NON_SHORT_CIRCUIT;
fold-const.cc:  bool logical_op_non_short_circuit = LOGICAL_OP_NON_SHORT_CIRCUIT;
tree-ssa-ifcombine.cc:#ifndef LOGICAL_OP_NON_SHORT_CIRCUIT
tree-ssa-ifcombine.cc:#define LOGICAL_OP_NON_SHORT_CIRCUIT \
tree-ssa-ifcombine.cc:    bool logical_op_non_short_circuit = LOGICAL_OP_NON_SHORT_CIRCUIT;

fold_truth_andor in fold-const.cc
ifcombine_ifandif in tree-ssa-ifcombine.cc

What LOGICAL_OP_NON_SHORT_CIRCUIT is trying to say is that expansion (from gimple to RTL) of
things like `a & b` (where a and b are bools) is cheaper than expanding out using jumps.

Note this is not exactly true as there is code which does that again in dojump.cc:
      /* High branch cost, expand as the bitwise OR of the conditions.
         Do the same if the RHS has side effects, because we're effectively
         turning a TRUTH_OR_EXPR into a TRUTH_ORIF_EXPR.  */
      if (BRANCH_COST (optimize_insn_for_speed_p (), false) >= 4
          || TREE_SIDE_EFFECTS (TREE_OPERAND (exp, 1)))


---


### compiler : `gcc`
### title : `i386: Missed optimization: use of bt in bit test pattern (using -O2 -mtune=core2)`
### open_at : `2022-11-14T03:38:34Z`
### last_modified_date : `2023-07-09T08:08:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107671
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
int bt32_setb(const __UINT32_TYPE__ *p, __UINT32_TYPE__ bitnum)
{
    return ((p[bitnum >> 5] & (1  << (bitnum & 31)))) != 0;
}

int bt64_setb(const __UINT64_TYPE__ *p, __UINT64_TYPE__ bitnum)
{
    return ((p[bitnum >> 6] & (1L << (bitnum & 63)))) != 0;
}

int bt32_setb2(const __UINT32_TYPE__ *p, __UINT32_TYPE__ bitnum)
{
    return (p[bitnum >> 5] >> (bitnum & 31)) & 1;
}

int bt64_setb2(const __UINT64_TYPE__ *p, __UINT64_TYPE__ bitnum)
{
    return (p[bitnum >> 6] >> (bitnum & 63)) & 1;
}

int bt32_setae(const __UINT32_TYPE__ *p, __UINT32_TYPE__ bitnum)
{
    return ((p[bitnum >> 5] & (1  << (bitnum & 31)))) == 0;
}

int bt64_setae(const __UINT64_TYPE__ *p, __UINT64_TYPE__ bitnum)
{
    return ((p[bitnum >> 6] & (1L << (bitnum & 63)))) == 0;
}

int bt32_setae2(const __UINT32_TYPE__ *p, __UINT32_TYPE__ bitnum)
{
    return !((p[bitnum >> 5] >> (bitnum & 31)) & 1);
}

int bt64_setae2(const __UINT64_TYPE__ *p, __UINT64_TYPE__ bitnum)
{
    return !((p[bitnum >> 6] >> (bitnum & 63)) & 1);
}


---


### compiler : `gcc`
### title : `[11/12/13 Regressions] arm: MVE codegen regressions on VCTP and vector LDR/STR instructions`
### open_at : `2022-11-14T11:00:13Z`
### last_modified_date : `2023-04-18T15:12:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107674
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.1`
### severity : `normal`
### contents :
We've found a couple of performance regressions with Arm MVE.  These can be seen here:
https://godbolt.org/z/onPjfW4zj

* Between GCC 11 and 12 we seem to have started emitting a strange vmrs/sxth/vmsr instruction sequence after the vctp instruction.  I suspect this is something to do with the introduction of MODE_VECTOR_BOOL during that period.

* Between GCC 12 and 13 we are no longer merging the pointer increments by #16  into the ldr/strs and we have some random movs that aren't needed either.  This also happened in GCC 11, but we want to keep the improved codegen of GCC 12 here ;)

  This looks like a change in register allocation:

      Choosing alt 0 in insn 24:  (0) =w  (1) Ux  (2) Up {mve_vldrhq_z_sv8hi}
      Creating newreg=149, assigning class CORE_REGS to INC/DEC result r149
      Creating newreg=150 from oldreg=134, assigning class VPR_REG to r150
bad vs good
      Choosing alt 0 in insn 24:  (0) =w  (1) Ux  (2) Up {mve_vldrhq_z_sv8hi}
      Creating newreg=149 from oldreg=134, assigning class VPR_REG to r149

Does anyone have any further ideas on why these may have changed or how to fix them?

Thanks!


---


### compiler : `gcc`
### title : `[12/13/14 Regression] vectorization fails for std::ranges::transform due to IR changes since r12-3903-g0288527f47cec669`
### open_at : `2022-11-14T19:53:48Z`
### last_modified_date : `2023-05-08T12:25:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107690
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
GCC 11.3 vectorizes the following code.  GCC 12.2 fails to vectorize.

#include <algorithm>
#include <array>
#include <ranges>

std::array<int, 16> foo(std::array<int, 16> u, std::array<int, 16> const &v)
{
    std::ranges::transform(u, v, u.begin(), std::plus<int>());
    return u;
}

https://godbolt.org/z/KnhdPs6G3


---


### compiler : `gcc`
### title : `{,unsigned} __int128 to _Float16 conversion shouldn't use libgcc routines`
### open_at : `2022-11-15T16:13:01Z`
### last_modified_date : `2022-11-16T14:30:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107702
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
Seems for {,unsigned} __int128 to _Float16 conversions we always at least on x86_64 emit __floattihf calls.
_Float16 f1 (long long x) { return x; }
_Float16 f2 (unsigned long long x) { return x; }
_Float16 f3 (int x) { return x; }
_Float16 f4 (unsigned int x) { return x; }
_Float16 f5 (short x) { return x; }
_Float16 f6 (unsigned short x) { return x; }
_Float16 f7 (signed char x) { return x; }
_Float16 f8 (unsigned char x) { return x; }
_Float16 f9 (__int128 x) { return x; }
_Float16 f10 (__int128 x) { return x; }
_Float32 f11 (long long x) { return x; }
_Float32 f12 (unsigned long long x) { return x; }
_Float32 f13 (int x) { return x; }
_Float32 f14 (unsigned int x) { return x; }
_Float32 f15 (short x) { return x; }
_Float32 f16 (unsigned short x) { return x; }
_Float32 f17 (signed char x) { return x; }
_Float32 f18 (unsigned char x) { return x; }
_Float32 f19 (__int128 x) { return x; }
_Float32 f20 (__int128 x) { return x; }

Any reason for that?  _Float16 range is -65504 to 65504 (65504 is __FLT16_MAX__), and
int
main ()
{
  for (int i = 0; i < 65504; ++i)
    {
      volatile __int128 j = i;
      _Float16 a = j;
      _Float16 b = (float) i;
      if (a != b)
        __builtin_printf ("%d %a %a\n", i, (double) a, (double) b);
    }
}
verifies that the __floattihf implementation always gives the same answer as
does signed SImode -> SFmode cast followed by SFmode -> HFmode conversion.
Isn't a conversion of a value > 65504 && value < -65504 UB in both C and C++?
So, can't we just implement the TI -> HF conversions by say ignoring upper 64 bits of the __int128?


---


### compiler : `gcc`
### title : `[13/14 Regression] Testsuite regression after recent DCE changes`
### open_at : `2022-11-15T16:45:34Z`
### last_modified_date : `2023-10-13T00:59:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107704
### status : `NEW`
### tags : `missed-optimization, testsuite-fail`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
This change:

commit be2c74fdcd0e8d66c3667008ba2561ab5dcc379b
Author: Richard Biener <rguenther@suse.de>
Date:   Thu Nov 10 15:04:10 2022 +0100

    Make last DCE remove empty loops
    
    The following makes the last DCE pass CD-DCE and in turn the
    last CD-DCE pass a DCE one.  That ensues we remove empty loops
    that become empty between the two.  I've also moved the tail-call
    pass after DCE since DCE can only improve things here.
    
    The two testcases were the only ones scanning cddce3 so I've
    changed them to scan the dce7 pass that's now in this place.
    The testcases scanning dce7 also work when that's in the earlier
    position.
    
            PR tree-optimization/84646
            * tree-ssa-dce.cc (pass_dce::set_pass_param): Add param
            wheter to run update-address-taken.
            (pass_dce::execute): Honor it.
            * passes.def: Exchange last DCE and CD-DCE invocations.
            Swap pass_tail_calls and the last DCE.
    
            * g++.dg/tree-ssa/pr106922.C: Continue to scan earlier DCE dump.
            * gcc.dg/tree-ssa/20030808-1.c: Likewise.


Is causing minor regressions on the sh targets.  On sh3-linux-gnu for example:

Tests that now fail, but worked before (1 tests):

gcc.target/sh/pr51244-19.c scan-assembler-not movt


AFAICT this looks like a target issue -- the RTL looks suitable for avoiding the movt, but it doesn't happen for some reason.  I didn't consider it important enough to chase down further given the other things on my plate.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] if-to-switch conversion happens for simple predicate function when compiled with gcc but not with g++`
### open_at : `2022-11-17T20:46:18Z`
### last_modified_date : `2023-05-08T12:26:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107740
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For the following testcase, the ||'s are converted into a switch statement when compiling with gcc but not with g++ (despite only minor differences in their GENERIC trees) and ultimately leads to seemingly worse codegen when compiling with g++ vs gcc:

$ cat is_whitespace.c
int
is_whitespace (char c)
{
  return (c == ' '
          || c == '\n'
          || c == '\r'
          || c == '\t');
}


$ gcc -fdump-tree-{original,iftoswitch}=/dev/stdout -O2 is_whitespace.C

;; Function is_whitespace (null)
;; enabled by -tree-original


{
  return (c == 32 || c == 10) || (c == 13 || c == 9);
}


;; Function is_whitespace (is_whitespace, funcdef_no=0, decl_uid=2735, cgraph_uid=1, symbol_order=0)

;; Canonical GIMPLE case clusters: 9-10 13 32 
;; BT can be built: BT:9-32 
Removing basic block 3
Expanded into a new gimple STMT: switch (c_8(D)) <default: <L6> [INV], case 9: <L5> [INV], case 10: <L5> [INV], case 13: <L5> [INV], case 32: <L5> [INV]>

int is_whitespace (char c)
{
  _Bool _1;
  _Bool _2;
  _Bool _3;
  int iftmp.0_7;

  <bb 2> :
  _1 = c_8(D) == 32;
  _2 = c_8(D) == 10;
  _3 = _1 | _2;
  switch (c_8(D)) <default: <L6> [INV], case 9: <L5> [INV], case 10: <L5> [INV], case 13: <L5> [INV], case 32: <L5> [INV]>

  <bb 3> :
<L5>:

  <bb 4> :
  # iftmp.0_7 = PHI <1(3), 0(2)>
<L6>:
  return iftmp.0_7;

}


$ g++ -fdump-tree-{original,iftoswitch}=/dev/stdout -O2 is_whitespace.C

;; Function int is_whitespace(char) (null)
;; enabled by -tree-original


return <retval> = (int) ((c == 32 || c == 10) || (c == 13 || c == 9));


;; Function is_whitespace (_Z13is_whitespacec, funcdef_no=0, decl_uid=2757, cgraph_uid=1, symbol_order=0)

int is_whitespace (char c)
{
  bool _1;
  bool _2;
  bool _3;
  bool _4;
  bool _5;
  bool _6;
  bool iftmp.0_7;
  int _11;

  <bb 2> :
  _1 = c_8(D) == 32;
  _2 = c_8(D) == 10;
  _3 = _1 | _2;
  if (_3 != 0)
    goto <bb 4>; [INV]
  else
    goto <bb 3>; [INV]

  <bb 3> :
  _4 = c_8(D) == 13;
  _5 = c_8(D) == 9;
  _6 = _4 | _5;

  <bb 4> :
  # iftmp.0_7 = PHI <1(2), _6(3)>
  _11 = (int) iftmp.0_7;
  return _11;

}


---


### compiler : `gcc`
### title : `PPCLE: Inefficient vector constant creation`
### open_at : `2022-11-19T07:22:43Z`
### last_modified_date : `2022-11-22T08:31:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107757
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.1`
### severity : `normal`
### contents :
Due to the fact that vslw, vsld, vsrd, ... only use the modulo of bit width for shifting, the combination with 0xFF..FF vector can be used to create vector constants for:
vec_splats(-0.0) or vec_splats(1ULL << 31) and scalar -0.0
vec_splats(-0.0f) or vec_splats(1U << 31)
vec_splats((short)0x8000)
with only 2 2-cycle vector instructions.

Sample:

vector long long lsb64()
{
   return vec_splats(1LL);
}

creates:

lsb64():
.LCF5:
        addi 2,2,.TOC.-.LCF5@l
        addis 9,2,.LC12@toc@ha
        addi 9,9,.LC12@toc@l
        lvx 2,0,9
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0

while:

vector long long lsb64_opt()
{
   vector long long a = vec_splats(~0LL);
   __asm__("vsrd %0,%0,%0":"=v"(a):"v"(a),"v"(a));
   return a;
}

creates:
lsb64_opt():
        vspltisw 2,-1
        vsrd 2,2,2
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0


---


### compiler : `gcc`
### title : `missing (int)-(unsigned)int_val to just -int_val if int_val is known not to contain INT_MIN`
### open_at : `2022-11-19T23:27:53Z`
### last_modified_date : `2023-09-02T07:29:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107765
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
```
#include <limits.h>

int a(int input)
{
    if (input == INT_MIN) __builtin_unreachable();
    unsigned t = input;
    int tt =  -t;
    return tt == -input;
}
```
We should be able to optimize this at the tree level because we know we don't invoke undefined behavior.


---


### compiler : `gcc`
### title : `[13 Regression] switch to table conversion happening even though using btq is better`
### open_at : `2022-11-20T13:00:30Z`
### last_modified_date : `2023-01-11T12:14:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107767
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
See https://godbolt.org/z/rTfTondfP

```
#include <stdint.h>

int firewall(const uint8_t *restrict data) {
    const uint8_t ip_proto = *data;
    const uint16_t dst_port = *((const uint16_t *)data + 32);

    if (ip_proto == 17 && dst_port == 15) return 1;
    if (ip_proto == 17 && dst_port == 23) return 1;
    if (ip_proto == 17 && dst_port == 47) return 1;
    if (ip_proto == 17 && dst_port == 45) return 1;
    if (ip_proto == 17 && dst_port == 42) return 1;
    if (ip_proto == 17 && dst_port == 1) return 1;
    if (ip_proto == 17 && dst_port == 2) return 1;
    if (ip_proto == 17 && dst_port == 3) return 1;

    return 0;
}

int firewall2(const uint8_t *restrict data) {
    const uint16_t dst_port = *((const uint16_t *)data + 32);

    if (dst_port == 15) return 1;
    if (dst_port == 23) return 1;
    if (dst_port == 47) return 1;
    if (dst_port == 45) return 1;
    if (dst_port == 42) return 1;
    if (dst_port == 1) return 1;
    if (dst_port == 2) return 1;
    if (dst_port == 3) return 1;

    return 0;
}
```

Compile with -Os.

Second function IS NOT minimal, obviously. It's a bug. GCC 12.2 does not have it.


---


### compiler : `gcc`
### title : `function prologue generated even though it's only needed in an unlikely path`
### open_at : `2022-11-20T18:48:15Z`
### last_modified_date : `2022-11-28T20:32:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107772
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Consider


int g(int);

void f(int* b, int* e) {
    while (b != e) {
        if (__builtin_expect(*b != 0, false)) [[unlikely]] {
            *b = g(*b);
        }
        ++b;
    }
}

If we believe the __builtin_expect and/or unlikely annotations (had both for extra safety), the loop usually does nothing. So we would expect any register saving and restoring to be pushed to the unlikely section. Yet (-O3):

f(int*, int*):
        cmp     rdi, rsi
        je      .L10
        push    rbp
        mov     rbp, rsi
        push    rbx
        mov     rbx, rdi
        sub     rsp, 8
.L4:
        mov     edi, DWORD PTR [rbx]
        test    edi, edi
        jne     .L14
.L3:
        add     rbx, 4
        cmp     rbp, rbx
        jne     .L4
        add     rsp, 8
        pop     rbx
        pop     rbp
        ret
.L14:
        call    g(int)
        mov     DWORD PTR [rbx], eax
        jmp     .L3
.L10:
        ret


I count 8 instructions that could/should have been pushed to .L14.


---


### compiler : `gcc`
### title : `rtl failed to simplify subreg:vec_select to just vec_select.`
### open_at : `2022-11-21T02:18:13Z`
### last_modified_date : `2022-11-21T02:44:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107774
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
When i'm looking at https://gcc.gnu.org/pipermail/gcc-patches/2022-November/606373.html, I notice there's some misoptimization in rtl.

It failed to simplift (subreg:SF (vec_select:V2SF (reg:V4SF) (parallel [(const_int 2) (const_int 3)]) 0) to just (vec_select:SF (reg:V4SF) (parallel [(const_int 2)])

The optimization is exposed after adjustment of ix86_can_change_mode_class, w/o that, i don't have a case to reproduce this issue. But I can think backends canonicalizing vec_select to subreg may have such issue.


---


### compiler : `gcc`
### title : `missed optimization in vec_set lower part of vector in the memory.`
### open_at : `2022-11-21T02:43:29Z`
### last_modified_date : `2022-11-22T08:46:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107775
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
The case is found when i'm looking at https://gcc.gnu.org/pipermail/gcc-patches/2022-November/606373.html, currently x86 gcc can optimize set_lower same as set_lower1, but not after adjusting in can_change_mode_class. The issue can be reproduce with aarch64 gcc. I'm looking at rtl dump, the main difference comes from subreg1, where currently it will split 128-bit load/store into 2 64-bit load/stores which expose the opportunity to optimize the upper 64-bit load/store off.

typedef double v2df __attribute__((vector_size(16)));

v2df reg;
void
set_lower (double b)
{
  double v[2];
  *((v2df*)&v[0]) = reg;
  v[0] = b;
  reg = *((v2df*)&v[0]);
}


void
set_lower1 (double b)
{
    reg[0] = b;
}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] RTL SSA forwprop introduced regression since r11-6188`
### open_at : `2022-11-22T12:54:07Z`
### last_modified_date : `2023-05-29T16:17:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107812
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #107627 +++

static inline unsigned long long
qux (unsigned int x, unsigned int y)
{
  return ((unsigned long long) x << 32) | y;
}

static inline unsigned int
corge (unsigned int x, unsigned int y, unsigned z)
{
  return qux (x, y) >> (z % 32);
}

void
garply (unsigned int *x, const unsigned int *y, unsigned z)
{
  x[0] = corge (y[0], y[1], z);
}

with -m32 -O2 -mno-sse on x86_64-linux regressed with
r11-6188-g0b76990a9d75d97b84014e37519086b81824c307
Previously:
	pushl	%ebx
	movl	12(%esp), %ebx
	movl	16(%esp), %ecx
	movl	(%ebx), %edx
	movl	4(%ebx), %eax
	shrdl	%edx, %eax
	movl	8(%esp), %edx
	movl	%eax, (%edx)
	popl	%ebx
	ret
After:
	pushl	%edi
	xorl	%edi, %edi
	pushl	%esi
	pushl	%ebx
	movl	20(%esp), %ebx
	movl	24(%esp), %ecx
	movl	4(%ebx), %esi
	movl	(%ebx), %edx
	movl	%esi, %eax
	orl	%edi, %edx
	shrdl	%edx, %eax
	movl	16(%esp), %edx
	movl	%eax, (%edx)
	popl	%ebx
	popl	%esi
	popl	%edi
	ret


---


### compiler : `gcc`
### title : `[13/14/14 Regression] Dead Code Elimination Regression at -Os (trunk vs. 12.2.0)`
### open_at : `2022-11-22T17:51:01Z`
### last_modified_date : `2023-08-09T04:27:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107822
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53945
Code

cat case.c #230636
int b;
void foo();
void(a)();
int main() {
  int c;
  int *d = &c;
  *d = a && 8;
  b = 0;
  for (; b < 9; ++b)
    *d ^= 3;
  if (*d)
    ;
  else
    foo();
}

`gcc-e4faee8d02ec5d65bf418612f7181823eb08c078 (trunk) -Os` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -Os` can.

`gcc-e4faee8d02ec5d65bf418612f7181823eb08c078 (trunk) -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movl	$10, %edx
	xorl	%ecx, %ecx
	movl	$1, %eax
.L2:
	decl	%edx
	je	.L12
	xorl	$3, %eax
	movb	$1, %cl
	jmp	.L2
.L12:
	testb	%cl, %cl
	jne	.L4
	xorl	%esi, %esi
	movl	%esi, b(%rip)
	jmp	.L5
.L4:
	movl	$9, b(%rip)
.L5:
	testl	%eax, %eax
	jne	.L8
	pushq	%rdx
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rcx
	.cfi_def_cfa_offset 8
	ret
.L8:
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movl	$9, b(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=e7310e24b1c0ca67b1bb507c1330b2bf39e59e32


---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -Os (trunk vs. 12.2.0)`
### open_at : `2022-11-22T17:59:39Z`
### last_modified_date : `2023-08-25T23:38:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107823
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53946
Code

cat case.c #230633
int a;
void bar64_(void);
void foo();
int main() {
  char b = a = 6;
  for (; a; a = 0) {
    bar64_();
    b = 0;
  }
  if (b <= 0)
    ;
  else
    foo();
}

`gcc-e4faee8d02ec5d65bf418612f7181823eb08c078 (trunk) -Os` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -Os` can.

`gcc-e4faee8d02ec5d65bf418612f7181823eb08c078 (trunk) -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	pushq	%rcx
	.cfi_def_cfa_offset 16
	movl	$6, %eax
	movb	$6, %dl
.L2:
	movl	%eax, a(%rip)
	testl	%eax, %eax
	je	.L10
	call	bar64_
	xorl	%eax, %eax
	xorl	%edx, %edx
	jmp	.L2
.L10:
	testb	%dl, %dl
	je	.L4
	call	foo
.L4:
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	pushq	%rax
	.cfi_def_cfa_offset 16
	movl	$6, a(%rip)
	call	bar64_
	xorl	%edx, %edx
	xorl	%eax, %eax
	movl	%edx, a(%rip)
	popq	%rcx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=353fd1ec3df92fbe66ce1513c5a86bdd5c5e22d1


---


### compiler : `gcc`
### title : `switch table compression could shrink large tables (but missing), testcase cerf-lib`
### open_at : `2022-11-22T21:39:25Z`
### last_modified_date : `2022-11-23T09:51:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107827
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
The cerf-lib (self-contained numeric library that provides an efficient and accurate implementation of complex error functions) contains 2 switch-blocks having about 100 entries. Each entry represents a piece-wise taylor-polynomial-approximation (aiming nearly machine precision, type double).
https://jugit.fz-juelich.de/mlz/libcerf/-/tree/main/lib

Compiler Explorer (selected -O3 for x86-64-gcc-12) https://godbolt.org/
shows ASM output having huge jump-tables. Even code compiles fine on my PC with gcc11, you have to shrink to 20 cases in Compiler-Explorer to see result (some webservice limit, but this is not the issue here). Lets focus on 2 static functions.

1. erfcx.c / static double erfcx_y100() / case (int) 0..99 (100 cases, no gap, each case is a taylor-polynomial of grade 6). Gives Jump-Table of 100 x 8 bytes, plus code. The table could be easy compressed or replaced by linear formula.
a) assumning no function ever exceeded 4 GB of code, it would be possible to use only 4 byte offset (jumping-distance), and for 99% of funtions 65 KB range would be enough (at least in O2 is makes sence, but also in O3 it could be faster to decrease memory/cache usage - this could become a new option to control it).
b) for this erfcx_y100-sample it would be even possible to calc the offset by formula.
c) of course the parameters could be a separate lookup-table, but no speedup for other reasons (I tried these days). gcc could detect the similarity (idential cases beside the 7 hard-coded coefficients) which would be antother change request..

sample (I cutted the coefficients numbers for readability)
switch ((int) y100) {
  case 0: {
    double t = 2*y100 - 1;
    return 0.7 + (0.7 + (0.3 + (0.1 + (0.8 + (0.3 + 0.1 * t) *t) *t) *t) *t) *t;
  }
  case 1: {
    double t = 2*y100 - 3;
    return 0.2 + (0.7 + (0.3 + (0.1 + (0.8 + (0.3 + 0.1 *t) *t) *t) *t) *t) *t;
  }
 .. until case 99:
  // instead 8 x 100 byte table it could be 2 x 100 byte, or 10 byte linear equation.

2. im_w_of_x.c / static double w_im_y100() / case (int) 0..96 (97 cases, no gap, similar polynomials between 6th to 8th grade, plus another unique final formula). Here the code is (even neglecting the coefficients) not identical, but still similar. Address-Offset-Compression could still be used (but not linear all the way).
d) same as a) + b)
e) side note: " 2*y100 " seems no to be detected and calculated once at begin of switch.
f) detection of similar code (same equation, different parameters) could be used to decrease object-size (text) as only 4 kinds of equations are here, only offset in some created "const double parameters[]"-array differs.

PS: no code is wrong or shows different results, this is a ro-memory-consumption and timing issue only.

Thanks for reading.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Missed optimization: Using memcpy to load a struct unnecessary uses stack space since r8-5200`
### open_at : `2022-11-23T15:23:58Z`
### last_modified_date : `2023-07-07T10:44:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107837
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
I have a simple struct with array uint64_t[4]. When using memcpy() load it from a storage of bytes and then performing some additional operations, a temporary object on the stack is created.


struct uint256
{
    unsigned long v[4];
};

void load_bad(uint256* o, const char* src) noexcept
{
    uint256 x;
    __builtin_memcpy(&x, src, sizeof(x));
    uint256 y;
    y.v[0] = __builtin_bswap64(x.v[3]);
    y.v[1] = __builtin_bswap64(x.v[2]);
    y.v[2] = __builtin_bswap64(x.v[1]);
    y.v[3] = __builtin_bswap64(x.v[0]);
    *o = y;
}


load_bad(uint256*, char const*):
        movdqu  xmm0, XMMWORD PTR [rsi]
        movdqu  xmm1, XMMWORD PTR [rsi+16]
        movaps  XMMWORD PTR [rsp-40], xmm0
        mov     rdx, QWORD PTR [rsp-32]
        mov     rax, QWORD PTR [rsp-40]
        movaps  XMMWORD PTR [rsp-24], xmm1
        mov     rsi, QWORD PTR [rsp-16]
        mov     rcx, QWORD PTR [rsp-24]
        bswap   rdx
        bswap   rax
        mov     QWORD PTR [rdi+16], rdx
        bswap   rsi
        bswap   rcx
        mov     QWORD PTR [rdi], rsi
        mov     QWORD PTR [rdi+8], rcx
        mov     QWORD PTR [rdi+24], rax
        ret


The workaround is to use reinterpret_cast.

https://godbolt.org/z/WevYch8nv


---


### compiler : `gcc`
### title : `[12 Regression] Spurious warnings stringop-overflow and array-bounds copying data as bytes into vector`
### open_at : `2022-11-23T23:24:13Z`
### last_modified_date : `2023-06-29T19:16:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107852
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `libstdc++`
### version : `12.2.0`
### severity : `normal`
### contents :
Created attachment 53957
preprocessed source code, if that's useful.

Starting with gcc 12 and continuing into the current trunk, with optimizations enabled, the compiler emits warnings from static analysis regarding copying data internal to the vector class, both on Compiler Explorer, and on my local linux box running Centos9.

Options: g++ warnings.cpp -Werror -O2 -Warray-bounds 

Code:

    #include <vector>
    std::vector<char> bytes;
    int value{};

    void copyValueBytes() {
        bytes.clear();
        auto ptr = reinterpret_cast<const char *>(&value);
        bytes.insert(bytes.end(), ptr, ptr + sizeof(value));
    }

https://godbolt.org/z/MfjM9xKjP

Observations:
* if I don't clear() the vector first, it does not report any errors
* if I call bytes.resize(0) it reports the same output
* if I call bytes.resize(1) it compiles cleanly
* if I use a std::deque instead of a vector, it compiles cleanly

A reinterpret_cast always is eye-raising, but I think this is a valid use since its address is casted to a char*, a compatible type.

Output:

In file included from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/vector:60,
                 from <source>:1:
In static member function 'static _Tp* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(const _Tp*, const _Tp*, _Tp*) [with _Tp = char; bool _IsMove = true]',
    inlined from '_OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = true; _II = char*; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:495:30,
    inlined from '_OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = true; _II = char*; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:522:42,
    inlined from '_OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = true; _II = char*; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:529:31,
    inlined from '_OI std::copy(_II, _II, _OI) [with _II = move_iterator<char*>; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:620:7,
    inlined from 'static _ForwardIterator std::__uninitialized_copy<true>::__uninit_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = std::move_iterator<char*>; _ForwardIterator = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:147:27,
    inlined from '_ForwardIterator std::uninitialized_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = move_iterator<char*>; _ForwardIterator = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:185:15,
    inlined from '_ForwardIterator std::__uninitialized_copy_a(_InputIterator, _InputIterator, _ForwardIterator, allocator<_Tp>&) [with _InputIterator = move_iterator<char*>; _ForwardIterator = char*; _Tp = char]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:372:37,
    inlined from '_ForwardIterator std::__uninitialized_move_if_noexcept_a(_InputIterator, _InputIterator, _ForwardIterator, _Allocator&) [with _InputIterator = char*; _ForwardIterator = char*; _Allocator = allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:397:2,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_range_insert(iterator, _ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/vector.tcc:801:9,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_insert_dispatch(iterator, _InputIterator, _InputIterator, std::__false_type) [with _InputIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1779:19,
    inlined from 'std::vector<_Tp, _Alloc>::iterator std::vector<_Tp, _Alloc>::insert(const_iterator, _InputIterator, _InputIterator) [with _InputIterator = const char*; <template-parameter-2-2> = void; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1481:22,
    inlined from 'void copyValueBytes()' at <source>:9:17:
/opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:431:30: error: 'void* __builtin_memcpy(void*, const void*, long unsigned int)' offset 4 is out of the bounds [0, 4] [-Werror=array-bounds]
  431 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:431:30: error: 'void* __builtin_memcpy(void*, const void*, long unsigned int)' writing 1 or more bytes into a region of size 0 overflows the destination [-Werror=stringop-overflow=]
In file included from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/x86_64-linux-gnu/bits/c++allocator.h:33,
                 from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/allocator.h:46,
                 from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/vector:61:
In member function '_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = char]',
    inlined from 'static _Tp* std::allocator_traits<std::allocator<_Tp1> >::allocate(allocator_type&, size_type) [with _Tp = char]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/alloc_traits.h:464:28,
    inlined from 'std::_Vector_base<_Tp, _Alloc>::pointer std::_Vector_base<_Tp, _Alloc>::_M_allocate(std::size_t) [with _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:378:33,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_range_insert(iterator, _ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/vector.tcc:787:40,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_insert_dispatch(iterator, _InputIterator, _InputIterator, std::__false_type) [with _InputIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1779:19,
    inlined from 'std::vector<_Tp, _Alloc>::iterator std::vector<_Tp, _Alloc>::insert(const_iterator, _InputIterator, _InputIterator) [with _InputIterator = const char*; <template-parameter-2-2> = void; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1481:22,
    inlined from 'void copyValueBytes()' at <source>:9:17:
/opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/new_allocator.h:137:55: note: at offset [5, 8] into destination object of size 4 allocated by 'operator new'
  137 |         return static_cast<_Tp*>(_GLIBCXX_OPERATOR_NEW(__n * sizeof(_Tp)));
      |                                                       ^
cc1plus: all warnings being treated as errors
ASM generation compiler returned: 1
In file included from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/vector:60,
                 from <source>:1:
In static member function 'static _Tp* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(const _Tp*, const _Tp*, _Tp*) [with _Tp = char; bool _IsMove = true]',
    inlined from '_OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = true; _II = char*; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:495:30,
    inlined from '_OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = true; _II = char*; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:522:42,
    inlined from '_OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = true; _II = char*; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:529:31,
    inlined from '_OI std::copy(_II, _II, _OI) [with _II = move_iterator<char*>; _OI = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:620:7,
    inlined from 'static _ForwardIterator std::__uninitialized_copy<true>::__uninit_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = std::move_iterator<char*>; _ForwardIterator = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:147:27,
    inlined from '_ForwardIterator std::uninitialized_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = move_iterator<char*>; _ForwardIterator = char*]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:185:15,
    inlined from '_ForwardIterator std::__uninitialized_copy_a(_InputIterator, _InputIterator, _ForwardIterator, allocator<_Tp>&) [with _InputIterator = move_iterator<char*>; _ForwardIterator = char*; _Tp = char]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:372:37,
    inlined from '_ForwardIterator std::__uninitialized_move_if_noexcept_a(_InputIterator, _InputIterator, _ForwardIterator, _Allocator&) [with _InputIterator = char*; _ForwardIterator = char*; _Allocator = allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_uninitialized.h:397:2,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_range_insert(iterator, _ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/vector.tcc:801:9,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_insert_dispatch(iterator, _InputIterator, _InputIterator, std::__false_type) [with _InputIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1779:19,
    inlined from 'std::vector<_Tp, _Alloc>::iterator std::vector<_Tp, _Alloc>::insert(const_iterator, _InputIterator, _InputIterator) [with _InputIterator = const char*; <template-parameter-2-2> = void; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1481:22,
    inlined from 'void copyValueBytes()' at <source>:9:17:
/opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:431:30: error: 'void* __builtin_memcpy(void*, const void*, long unsigned int)' offset 4 is out of the bounds [0, 4] [-Werror=array-bounds]
  431 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_algobase.h:431:30: error: 'void* __builtin_memcpy(void*, const void*, long unsigned int)' writing 1 or more bytes into a region of size 0 overflows the destination [-Werror=stringop-overflow=]
In file included from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/x86_64-linux-gnu/bits/c++allocator.h:33,
                 from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/allocator.h:46,
                 from /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/vector:61:
In member function '_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = char]',
    inlined from 'static _Tp* std::allocator_traits<std::allocator<_Tp1> >::allocate(allocator_type&, size_type) [with _Tp = char]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/alloc_traits.h:464:28,
    inlined from 'std::_Vector_base<_Tp, _Alloc>::pointer std::_Vector_base<_Tp, _Alloc>::_M_allocate(std::size_t) [with _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:378:33,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_range_insert(iterator, _ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/vector.tcc:787:40,
    inlined from 'void std::vector<_Tp, _Alloc>::_M_insert_dispatch(iterator, _InputIterator, _InputIterator, std::__false_type) [with _InputIterator = const char*; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1779:19,
    inlined from 'std::vector<_Tp, _Alloc>::iterator std::vector<_Tp, _Alloc>::insert(const_iterator, _InputIterator, _InputIterator) [with _InputIterator = const char*; <template-parameter-2-2> = void; _Tp = char; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/stl_vector.h:1481:22,
    inlined from 'void copyValueBytes()' at <source>:9:17:
/opt/compiler-explorer/gcc-12.2.0/include/c++/12.2.0/bits/new_allocator.h:137:55: note: at offset [5, 8] into destination object of size 4 allocated by 'operator new'
  137 |         return static_cast<_Tp*>(_GLIBCXX_OPERATOR_NEW(__n * sizeof(_Tp)));
      |                                                       ^
cc1plus: all warnings being treated as errors
Execution build compiler returned: 1


---


### compiler : `gcc`
### title : `Fail to optimize rot13`
### open_at : `2022-11-24T13:32:03Z`
### last_modified_date : `2022-11-25T12:27:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107859
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
Compiled with -O2, the following functions produce different assembly although they compute the same things:

----------------------------

unsigned rot13_1(unsigned c) {
  if(c >= 'A' && c <= 'Z') return 'A' + ((c -'A') + 13)%26;
  __builtin_unreachable();
}

unsigned rot13_2(unsigned c) {
  if (c >= 'A' && c <= 'M' ) return c + 13;
  else if (c >= 'N' && c <= 'Z' ) return c - 13;
  __builtin_unreachable();
}

unsigned rot13_3(unsigned c) {
  if(c >= 'A' && c <= 'Z') return  c + (c > 'Z' - 13 ? -13 : 13);
  __builtin_unreachable();
}

unsigned rot13_4(unsigned c) {
  if(c >= 'A' && c <= 'Z') return  c + 13 + (c > 'Z' - 13 ? -26 : 0);
  __builtin_unreachable();
}

------------------------------

rot13_1(unsigned int):
        lea     edx, [rdi-52]
        mov     rax, rdx
        imul    rdx, rdx, 1321528399
        shr     rdx, 35
        imul    edx, edx, 26
        sub     eax, edx
        add     eax, 65
        ret
rot13_2(unsigned int):
        lea     edx, [rdi-65]
        lea     eax, [rdi+13]
        sub     edi, 13
        cmp     edx, 12
        cmova   eax, edi
        ret
rot13_3(unsigned int):
        cmp     edi, 78
        sbb     eax, eax
        and     eax, 26
        lea     eax, [rax-13+rdi]
        ret
rot13_4(unsigned int):
        cmp     edi, 78
        sbb     eax, eax
        not     eax
        and     eax, -26
        lea     eax, [rax+13+rdi]
        ret


---


### compiler : `gcc`
### title : `bool tautology missed optimisation`
### open_at : `2022-11-26T18:03:24Z`
### last_modified_date : `2023-10-25T22:22:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107880
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
From bug 91882 comment #1 (since the testcase in that was in bug 91882 comment #0 is now fixed):

bool impl(bool a, bool b)
{
    return (!a || b);
}

// bad optimisation
bool always_true(bool a, bool b)
{
    return (impl(a,b) == impl(b,a)) == (a == b);
           // ( (a -> b) = (b -> a) ) = (a = b) tautology
}


---


### compiler : `gcc`
### title : `(a <= b) == (b >= a) should be optimized to (a == b)`
### open_at : `2022-11-26T18:18:05Z`
### last_modified_date : `2023-09-12T15:03:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107881
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
```
bool f(int a, int b)
{
  bool t = b <= a;
  bool t1 = a <= b;
  return t == t1;
}
```

We should optimize this to just:
```
bool f1(int a, int b)
{
  return a == b;
}
```
LLVM is able to do it.
We can optimize for & and | but not ^ or ==. (note != gets changed into ^)


---


### compiler : `gcc`
### title : `(bool0 > bool1) | bool1 is not optimized to bool0 | bool1`
### open_at : `2022-11-27T18:59:44Z`
### last_modified_date : `2023-09-24T04:57:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107887
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
```
_Bool max(_Bool aa, _Bool bb)
{
  bool t = aa > bb;
  return t | bb;
}
```
This should be optimized to just `return aa | bb;`
I accidently found this while working on PR 101805 .

The original testcase which I found it:
```
int ii(_Bool aa, _Bool bb)
{
  int c;
  int a = aa;
  int b = bb;
  if (a > b)
    c = a;
  else
    c = b;
  if (c)
    return 100;
  return c;
}
```


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Missed min/max transformation in phiopt due to VRP`
### open_at : `2022-11-27T19:28:54Z`
### last_modified_date : `2023-10-25T22:46:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107888
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Take:
```
#define bool _Bool
int maxbool(bool ab, bool bb)
{
  int a = ab;
  int b = bb;
  int c;
  if (a > b)
    c = a;
  else
    c = b;
  return c;
}
```

We miss that c is max of a and b because VRP decides to change the phi.
We get out of VRP:
```
  if (a_3 > b_5)
    goto <bb 4>; [INV]
  else
    goto <bb 3>; [INV]

  <bb 3> :

  <bb 4> :
  # c_1 = PHI <1(2), b_5(3)>
```

What VRP is doing is correct just is harder to optimize to a max (and then a | ).

In the above case we could optimize `bool0 ? 1 : bool1` to `bool0 | bool1` But then we end up with PR 107887 too.

You can also end up with the above issue where you know the only overlap between the two arguments is [5,6] :
```
int max(int ab, int bb)
{
  if (ab < 5)  __builtin_trap();
  if (bb > 6)  __builtin_trap();
  int a = ab;
  int b = bb;
  int c;
  if (a >= b)
    c = a;
  else
    c = b;
  return c;
}
```
Which we cannot optimize based on zero/one any more. (note this version of max has been an issue since at least GCC 4.1, I suspect since VRP was added).


---


### compiler : `gcc`
### title : `Redudant "double" permutation from SLP vectorization (PR97832)`
### open_at : `2022-11-28T06:47:56Z`
### last_modified_date : `2022-11-28T07:58:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107891
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
#include <stddef.h>

void foo1x1(double* restrict y, const double* restrict x, int clen)
{
  int xi = clen & 2;
  double f_re = x[0+xi+0];
  double f_im = x[4+xi+0];
  ptrdiff_t clen2 = (clen+xi) * 2;
  //#pragma GCC unroll 0
  for (ptrdiff_t c = 0; c < clen2; c += 8) {
    // y[c] = y[c] - x[c]*conj(f);
    //#pragma GCC  unroll 4
    for (ptrdiff_t k = 0; k < 4; ++k) {
      double x_re = x[c+0+k];
      double x_im = x[c+4+k];
      double y_re = y[c+0+k];
      double y_im = y[c+4+k];
      y_re = y_re - x_re * f_re - x_im * f_im;;
      y_im = y_im + x_re * f_im - x_im * f_re;
      y[c+0+k] = y_re;
      y[c+4+k] = y_im;
    }
  }
}

-Ofast -mavx2 -mfma generate extra blendpd compared to -O3 -mavx2 -mfma
and blendpd is redundant since there're "doube" permutations for mult operand in FMA. 

They're computing the same thing since we also do the same "permutation" for the invariants: f_re and f_imm, can we eliminate that in the vectorizer?

  _232 = {f_im_36, f_im_36, f_im_36, f_im_36};
  _231 = {f_im_36, f_re_35, f_re_35, f_re_35}; ------- here
  _216 = {f_re_35, f_re_35, f_re_35, f_re_35};
  _215 = {f_re_35, f_im_36, f_im_36, f_im_36}; ------ and here.
  ivtmp.36_221 = (unsigned long) y_41(D);
  ivtmp.38_61 = (unsigned long) x_33(D);

  <bb 4> [local count: 214748368]:
  # ivtmp.32_66 = PHI <ivtmp.32_65(4), 0(3)>
  # ivtmp.36_64 = PHI <ivtmp.36_63(4), ivtmp.36_221(3)>
  # ivtmp.38_220 = PHI <ivtmp.38_60(4), ivtmp.38_61(3)>
  # DEBUG c => NULL
  # DEBUG k => 0
  # DEBUG BEGIN_STMT
  # DEBUG BEGIN_STMT
  # DEBUG D#78 => D#79 * 8
  # DEBUG D#77 => x_33(D) + D#78
  _62 = (void *) ivtmp.38_220;
  vect_x_im_61.13_228 = MEM <const vector(4) double> [(const double *)_62];
  vect_x_im_61.14_226 = MEM <const vector(4) double> [(const double *)_62 + 32B];
  vect_x_re_55.15_225 = VEC_PERM_EXPR <vect_x_im_61.14_226, vect_x_im_61.13_228, { 0, 5, 6, 7 }>; ----- here. 
  vect_x_re_55.23_209 = VEC_PERM_EXPR <vect_x_im_61.13_228, vect_x_im_61.14_226, { 0, 5, 6, 7 }>;  ----- here
  # DEBUG D#76 => *D#77
  # DEBUG x_re => D#76
  # DEBUG BEGIN_STMT
  # DEBUG D#74 => (long unsigned int) D#75
  # DEBUG D#73 => D#74 * 8
  # DEBUG D#72 => x_33(D) + D#73
  # DEBUG D#71 => *D#72
  # DEBUG x_im => D#71
  # DEBUG BEGIN_STMT
  # DEBUG D#70 => y_41(D) + D#78
  _59 = (void *) ivtmp.36_64;
  vect_y_re_63.9_235 = MEM <vector(4) double> [(double *)_59];
  vect_y_re_63.10_233 = MEM <vector(4) double> [(double *)_59 + 32B];
  vect__42.18_219 = .FMA (vect_x_im_61.13_228, _232, vect_y_re_63.10_233);
  vect_y_re_69.17_222 = .FNMA (vect_x_re_55.15_225, _231, vect_y_re_63.9_235);
  vect_y_re_69.25_206 = .FNMA (vect_x_re_55.23_209, _215, vect_y_re_69.17_222);
  vect_y_re_69.25_205 = .FNMA (_216, vect_x_im_61.14_226, vect__42.18_219);




and

  _233 = {f_im_36, f_re_35, f_re_35, f_re_35};
  _217 = {f_re_35, f_im_36, f_im_36, f_im_36};
...
vect_x_re_55.15_227 = VEC_PERM_EXPR <vect_x_im_61.14_228, vect_x_im_61.13_230, { 0, 5, 6, 7 }>;
  vect_x_re_55.23_211 = VEC_PERM_EXPR <vect_x_im_61.13_230, vect_x_im_61.14_228, { 0, 5, 6, 7 }>;
...
  vect_y_re_69.17_224 = .FNMA (vect_x_re_55.15_227, _233, vect_y_re_63.9_237);
  vect_y_re_69.25_208 = .FNMA (vect_x_re_55.23_211, _217, vect_y_re_69.17_224);

is equal to

  _233 = {f_im_36,f_im_36, f_im_36, f_im_36}
  _217 = {f_re_35, f_re_35, f_re_35, f_re_35};
...
  vect_y_re_69.17_224 = .FNMA (vect_x_im_61.14_228, _233, vect_y_re_63.9_237)
  vect_y_re_69.25_208 = .FNMA (vect_x_im_61.13_230, _217, vect_y_re_69.17_224)

A simplication in match.pd?


---


### compiler : `gcc`
### title : `Unnecessary move between ymm registers in loop using AVX2 intrinsic`
### open_at : `2022-11-28T07:41:05Z`
### last_modified_date : `2022-11-28T09:00:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107892
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
To reproduce with the latest trunk, compile the following .c file on x86_64 at -O2:

        #include <immintrin.h>

        int __attribute__((target("avx2")))
        sum_ints(const __m256i *p, size_t n)
        {
                __m256i a = _mm256_setzero_si256();
                __m128i b;

                do {
                        a = _mm256_add_epi32(a, *p++);
                } while (--n);

                b = _mm_add_epi32(_mm256_extracti128_si256(a, 0),
                                  _mm256_extracti128_si256(a, 1));
                b = _mm_add_epi32(b, _mm_shuffle_epi32(b, 0x31));
                b = _mm_add_epi32(b, _mm_shuffle_epi32(b, 0x02));
                return _mm_cvtsi128_si32(b);
        }

The assembly that gcc generates is:

	0000000000000000 <sum_ints>:
	   0:	c5 f1 ef c9          	vpxor  %xmm1,%xmm1,%xmm1
	   4:	0f 1f 40 00          	nopl   0x0(%rax)
	   8:	c5 f5 fe 07          	vpaddd (%rdi),%ymm1,%ymm0
	   c:	48 83 c7 20          	add    $0x20,%rdi
	  10:	c5 fd 6f c8          	vmovdqa %ymm0,%ymm1
	  14:	48 83 ee 01          	sub    $0x1,%rsi
	  18:	75 ee                	jne    8 <sum_ints+0x8>
	  1a:	c4 e3 7d 39 c1 01    	vextracti128 $0x1,%ymm0,%xmm1
	  20:	c5 f9 fe c1          	vpaddd %xmm1,%xmm0,%xmm0
	  24:	c5 f9 70 c8 31       	vpshufd $0x31,%xmm0,%xmm1
	  29:	c5 f1 fe c8          	vpaddd %xmm0,%xmm1,%xmm1
	  2d:	c5 f9 70 c1 02       	vpshufd $0x2,%xmm1,%xmm0
	  32:	c5 f9 fe c1          	vpaddd %xmm1,%xmm0,%xmm0
	  36:	c5 f9 7e c0          	vmovd  %xmm0,%eax
	  3a:	c5 f8 77             	vzeroupper
	  3d:	c3                   	ret

The bug is that the inner loop contains an unnecessary vmovdqa:

	   8:	vpaddd (%rdi),%ymm1,%ymm0
	        add    $0x20,%rdi
	        vmovdqa %ymm0,%ymm1
	        sub    $0x1,%rsi
	        jne    8 <sum_ints+0x8>

It should look like the following instead:

	   8:	vpaddd (%rdi),%ymm0,%ymm0
	        add    $0x20,%rdi
	        sub    $0x1,%rsi
	        jne    8 <sum_ints+0x8>

Strangely, the bug goes away if the __v8si type is used instead of __m256i and the addition is done using "+=" instead of _mm256_add_epi32():

	int __attribute__((target("avx2")))
	sum_ints_good(const __v8si *p, size_t n)
	{
		__v8si a = {};
		__m128i b;

		do {
			a += *p++;
		} while (--n);

		b = _mm_add_epi32(_mm256_extracti128_si256((__m256i)a, 0),
				  _mm256_extracti128_si256((__m256i)a, 1));
		b = _mm_add_epi32(b, _mm_shuffle_epi32(b, 0x31));
		b = _mm_add_epi32(b, _mm_shuffle_epi32(b, 0x02));
		return _mm_cvtsi128_si32(b);
	}

In the bad version, I noticed that the RTL initially has two separate insns for 'a += *p': one to do the addition and write the result to a new pseudo register, and one to convert the value from mode V8SI to V4DI and assign it to the original pseudo register.  These two separate insns never get combined.  (That sort of explains why the bug isn't seen with the __v8si and += method; gcc doesn't do a type conversion with that method.)  So, I'm wondering if the bug is in the instruction combining pass.  Or perhaps the RTL should never have had two separate insns in the first place?


---


### compiler : `gcc`
### title : `mt19937 bad performance on LP64`
### open_at : `2022-11-28T10:36:09Z`
### last_modified_date : `2022-11-28T13:48:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107895
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
Input
=====

#include <random>
#include <unistd.h>
static std::mt19937 rng;
int main() {
        for (size_t j = 0; j < 0x8000; ++j) {
                uint32_t numbers[65536];
                for (size_t i = 0; i < std::size(numbers); ++i)
                        numbers[i] = rng();
                // ensure number generation is not all optimized away
                write(STDOUT_FILENO, numbers, sizeof(numbers));
        }
}


Observed
========

Target: x86_64-suse-linux
gcc version 12.2.1 20221020 [revision 0aaef83351473e8f4eb774f8f999bbe87a4866d7] (SUSE Linux)

$ g++ x.cpp -O2 && time ./a.out >/dev/zero

                  -m32    -m64
===============  =====  ======
std::mt19937      3.9s   11.5s
std::mt19937_64  14.0s   11.6s
===============  =====  ======
error ±0.1s

With -ftree-loop-if-convert [Bug #80520], but still not at -m32 levels:

+-ftree-          -m32    -m64
===============  =====  ======
std::mt19937      3.9s    5.2s
std::mt19937_64  14.0s    5.4s
===============  =====  ======
error ±0.1s


Expected
========

Expected to see <= 4.7s on -m64 at all times. (3.9 + ~20% margin for wider transfers CPU<->caches/RAM)

The -m64 versions should have somewhat equal runtime or faster runtime (because more registers, more opportunities); concerns like https://gmplib.org/32vs64 apply to old CPUs, but I do not think it's indicative of how contemporary x86_64 systems perform.


Additional information
======================

CPUs:
 "11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz"
    [fam 6 model 140 stepping 1 microcode 0xa4] and
 "AMD Ryzen 7 3700X 8-Core Processor"
    [fam 23 model 113 stepping 0 microcode 0x8701013]
    (about 3.0 and 10.2 seconds runtime, respectively)


---


### compiler : `gcc`
### title : `2x slowdown versus CLANG and ICL`
### open_at : `2022-11-29T02:36:26Z`
### last_modified_date : `2022-11-30T15:01:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107905
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.3.0`
### severity : `normal`
### contents :
Hi,
I have encountered performance problem with MinGW GCC 11.3.0, nearly 2x slowdown with a simple C function (being the 3rd fastest wildcard matching function).

```
int Tcheburaschka_Wildcard_Iterative_Kaze_CaseSensitive (const char* mask, const char* name) {
        const char* maskSTACK;
        const char* nameSTACK;
#pragma nounroll
        for (name, mask; *name; ++name, ++mask) {
	        if (*mask == '*') {
		        goto Backtrack;
	        //} else if ((*mask != '?') && (*name != *mask)) {
/*
		} else if ((*mask - '?') * (*name - *mask)) {
		        return 0;
	        } 
*/
		}
		if ((*mask - '?') * (*name - *mask)) {
		        return 0;
	        } 
        }
Backtrack:
#pragma nounroll
        for (nameSTACK = name, maskSTACK = mask; *nameSTACK; ++nameSTACK, ++maskSTACK) {
	        if (*maskSTACK == '*') {
		        mask = maskSTACK+1;
		        if (!*mask) return 1;
		        name = nameSTACK;
		        goto Backtrack;
	        //} else if ((*maskSTACK != '?') && (*nameSTACK != *maskSTACK)) {
/*
		} else if ((*maskSTACK - '?') * (*nameSTACK - *maskSTACK)) {
		        name++;
		        goto Backtrack;
	        } 
*/
		}
		if ((*maskSTACK - '?') * (*nameSTACK - *maskSTACK)) {
		        name++;
		        goto Backtrack;
	        } 
        }
        while (*maskSTACK == '*') ++maskSTACK;
        return (!*maskSTACK);
}
```

```
[CPU: AMD Zen2 Ryzen7 4800H, @2.9GHz, Max. Boost Clock Up to 4.2GHz]
+-------------------------------------------------------------------------------+-------------------+-------------------------+-----------------------+
| Function \ Compiler                                                           | CLANG 14.0.1, -O3 | Intel's ICL 19.0.0, /O3 | MinGW gcc 11.3.0, -O3 |
+-------------------------------------------------------------------------------+-------------------+-------------------------+-----------------------+
| Dogan Kurt's 'Antimalware', 2016, Iterative (wild_iterative)                  |       70.605000 s |            102.610000 s |           83.398000 s |
| Dogan Kurt's 'Antimalware', 2016, Iterative Optimised (wild_iterative_opt)    |       61.322000 s |             74.243000 s |           66.538000 s |
| Tcheburaschka_r3, 2022, (Tcheburaschka_Wildcard_Iterative_Kaze_CaseSensitive) |       72.990000 s |             76.161000 s |          127.717000 s |
| JackHandy_Iterative, 2005, (IterativeWildcards)                               |       80.053000 s |             90.872000 s |           70.156000 s |
| Kirk J. Krauss, 2014, DrDobbs (FastWildCompare)                               |       44.113000 s |             48.109000 s |           51.018000 s |
| Alessandro Cantatore, 2003, (szWildMatch7)                                    |       98.729000 s |             85.986000 s |          121.965000 s |
| Nondeterministic Finite Automaton (wild_nfa)                                  |      162.561000 s |            200.022000 s |          176.440000 s |
+-------------------------------------------------------------------------------+-------------------+-------------------------+-----------------------+
[Note1: All functions returned 1,075,000,000 Matches - that is TRUEs, kinda means they passed the quality test, no, really, I printed all the 1's and 0's after each run - the sequences matched.]
[Note2: It is well-known that Maximum Turbo Modes are maintained for some 15-30 seconds, so it is good that each function takes 30+ seconds, to emulate some 8 billion real-world searches.]
```

For more info:
https://github.com/kirkjkrauss/MatchingWildcards/issues/1#issue-1467311771


---


### compiler : `gcc`
### title : `Missed optimization of struct members with mixed sizes`
### open_at : `2022-11-29T09:31:55Z`
### last_modified_date : `2022-11-30T08:38:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107910
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
Store-merging generates suboptimal code to copy members of structs with continuous memory:

#include <stdint.h>

#pragma pack(push, 1)
typedef struct StructA {
	int32_t v00;
	int32_t v01;
	int8_t v02_0;
	int8_t v02_1;
	int8_t v02_2;
	int8_t v02_3;
	int32_t v03;
	int32_t v04;
	int32_t v05;
	int32_t v06;
	int32_t v07;
	int32_t v08;
	int32_t v09;
	int32_t v10;
	int32_t v11;
} StructA;

typedef struct StructB {
	int32_t v00;
	int32_t v01;
	int32_t v02;
	int32_t v03;
	int32_t v04;
	int32_t v05;
	int32_t v06;
	int32_t v07;
	int32_t v08;
	int32_t v09;
	int32_t v10;
	int32_t v11;
} StructB;
#pragma pack(pop)

void copyA(StructA* __restrict dest, const StructA* __restrict src) {
	dest->v00 = src->v00;
	dest->v01 = src->v01;
	dest->v02_0 = src->v02_0;
	dest->v02_1 = src->v02_1;
	dest->v02_2 = src->v02_2;
	dest->v02_3 = src->v02_3;
	dest->v03 = src->v03;
}

void copyB(StructB* __restrict dest, const StructB* __restrict src) {
	dest->v00 = src->v00;
	dest->v01 = src->v01;
	dest->v02 = src->v02;
	dest->v03 = src->v03;
}

void copyAA(StructA* __restrict dest, const StructA* __restrict src) {
	dest->v00 = src->v00;
	dest->v01 = src->v01;
	dest->v02_0 = src->v02_0;
	dest->v02_1 = src->v02_1;
	dest->v02_2 = src->v02_2;
	dest->v02_3 = src->v02_3;
	dest->v03 = src->v03;
	dest->v04 = src->v04;
	dest->v05 = src->v05;
	dest->v06 = src->v06;
	dest->v07 = src->v07;
}

void copyBB(StructB* __restrict dest, const StructB* __restrict src) {
	dest->v00 = src->v00;
	dest->v01 = src->v01;
	dest->v02 = src->v02;
	dest->v03 = src->v03;
	dest->v04 = src->v04;
	dest->v05 = src->v05;
	dest->v06 = src->v06;
	dest->v07 = src->v07;
}

void copyAAA(StructA* __restrict dest, const StructA* __restrict src) {
	dest->v00 = src->v00;
	dest->v01 = src->v01;
	dest->v02_0 = src->v02_0;
	dest->v02_1 = src->v02_1;
	dest->v02_2 = src->v02_2;
	dest->v02_3 = src->v02_3;
	dest->v03 = src->v03;
	dest->v04 = src->v04;
	dest->v05 = src->v05;
	dest->v06 = src->v06;
	dest->v07 = src->v07;
	dest->v08 = src->v08;
	dest->v09 = src->v09;
	dest->v10 = src->v10;
	dest->v11 = src->v11;
}

void copyBBB(StructB* __restrict dest, const StructB* __restrict src) {
	dest->v00 = src->v00;
	dest->v01 = src->v01;
	dest->v02 = src->v02;
	dest->v03 = src->v03;
	dest->v04 = src->v04;
	dest->v05 = src->v05;
	dest->v06 = src->v06;
	dest->v07 = src->v07;
	dest->v08 = src->v08;
	dest->v09 = src->v09;
	dest->v10 = src->v10;
	dest->v11 = src->v11;
}

copyA* should generate the same code as its corresponding copyB* function.

Currently gcc 12 (or trunk) generates the following:

copyA(StructA*, StructA const*):
        mov     rax, QWORD PTR [rsi]
        mov     QWORD PTR [rdi], rax
        mov     rax, QWORD PTR [rsi+8]
        mov     QWORD PTR [rdi+8], rax
        ret
copyB(StructB*, StructB const*):
        movdqu  xmm0, XMMWORD PTR [rsi]
        movups  XMMWORD PTR [rdi], xmm0
        ret
copyAA(StructA*, StructA const*):
        mov     rax, QWORD PTR [rsi]
        movdqu  xmm0, XMMWORD PTR [rsi+12]
        mov     QWORD PTR [rdi], rax
        mov     eax, DWORD PTR [rsi+8]
        movups  XMMWORD PTR [rdi+12], xmm0
        mov     DWORD PTR [rdi+8], eax
        mov     eax, DWORD PTR [rsi+28]
        mov     DWORD PTR [rdi+28], eax
        ret
copyBB(StructB*, StructB const*):
        movdqu  xmm0, XMMWORD PTR [rsi+16]
        movdqu  xmm1, XMMWORD PTR [rsi]
        movups  XMMWORD PTR [rdi+16], xmm0
        movups  XMMWORD PTR [rdi], xmm1
        ret
copyAAA(StructA*, StructA const*):
        mov     rax, QWORD PTR [rsi]
        movdqu  xmm0, XMMWORD PTR [rsi+12]
        movdqu  xmm1, XMMWORD PTR [rsi+28]
        mov     QWORD PTR [rdi], rax
        mov     eax, DWORD PTR [rsi+8]
        movups  XMMWORD PTR [rdi+12], xmm0
        mov     DWORD PTR [rdi+8], eax
        mov     eax, DWORD PTR [rsi+44]
        movups  XMMWORD PTR [rdi+28], xmm1
        mov     DWORD PTR [rdi+44], eax
        ret
copyBBB(StructB*, StructB const*):
        movdqu  xmm1, XMMWORD PTR [rsi+16]
        movdqu  xmm0, XMMWORD PTR [rsi+32]
        movdqu  xmm2, XMMWORD PTR [rsi]
        movups  XMMWORD PTR [rdi+16], xmm1
        movups  XMMWORD PTR [rdi], xmm2
        movups  XMMWORD PTR [rdi+32], xmm0
        ret


---


### compiler : `gcc`
### title : `bigger vector_size than the target can handle causes extra load/stores inside loops`
### open_at : `2022-11-29T14:38:45Z`
### last_modified_date : `2022-11-30T08:48:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107916
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
https://github.com/openzfs/zfs/pull/14234

GCC codegen https://gcc.godbolt.org/z/bhPo9sWsx

Clang codegen https://gcc.godbolt.org/z/4rTEe3WMG

Clang is relatively compact and efficient
.LBB0_2:                                # =>This Inner Loop Header: Depth=1
        lxvd2x 1, 0, 4
        addi 4, 4, 16
        xxswapd 1, 1
        xxmrghw 40, 0, 1
        xxmrglw 41, 0, 1
        vaddudm 7, 7, 8
        vaddudm 6, 6, 9
        vaddudm 1, 7, 1
        vaddudm 5, 6, 5
        vaddudm 0, 1, 0
        vaddudm 4, 5, 4
        vaddudm 3, 0, 3
        vaddudm 2, 4, 2
        bdnz .LBB0_2

GCC is rather less efficient.


---


### compiler : `gcc`
### title : `PPC: Unnecessary rlwinm after lbzx`
### open_at : `2022-12-02T08:13:13Z`
### last_modified_date : `2023-04-11T05:37:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107949
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
extern unsigned char magic1[256];

unsigned int hash(const unsigned char inp[4])
{
   const unsigned long long INIT = 0x1ULL;
   unsigned long long h1 = INIT;
   h1 = magic1[((unsigned long long)inp[0]) ^ h1];
   h1 = magic1[((unsigned long long)inp[1]) ^ h1];
   h1 = magic1[((unsigned long long)inp[2]) ^ h1];
   h1 = magic1[((unsigned long long)inp[3]) ^ h1];
   return h1;
}

#ifdef __powerpc__
#define lbzx(b,c) ({ unsigned long long r; __asm__("lbzx %0,%1,%2":"=r"(r):"b"(b),"r"(c)); r; })
unsigned int hash2(const unsigned char inp[4])
{
   const unsigned long long INIT = 0x1ULL;
   unsigned long long h1 = INIT;
   h1 = lbzx(magic1, inp[0] ^ h1);
   h1 = lbzx(magic1, inp[1] ^ h1);
   h1 = lbzx(magic1, inp[2] ^ h1);
   h1 = lbzx(magic1, inp[3] ^ h1);
   return h1;
}
#endif

Extra rlwinm get added.

hash(unsigned char const*):
.LCF0:
        addi 2,2,.TOC.-.LCF0@l
        lbz 9,0(3)
        addis 10,2,.LC0@toc@ha
        ld 10,.LC0@toc@l(10)
        lbz 6,1(3)
        lbz 7,2(3)
        lbz 8,3(3)
        xori 9,9,0x1
        lbzx 9,10,9
        xor 9,9,6
        rlwinm 9,9,0,0xff <= not necessary
        lbzx 9,10,9
        xor 9,9,7
        rlwinm 9,9,0,0xff <= not necessary
        lbzx 9,10,9
        xor 9,9,8
        rlwinm 9,9,0,0xff <= not necessary
        lbzx 3,10,9
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0
hash2(unsigned char const*):
.LCF1:
        addi 2,2,.TOC.-.LCF1@l
        lbz 7,0(3)
        lbz 8,1(3)
        lbz 10,2(3)
        lbz 6,3(3)
        addis 9,2,.LC1@toc@ha
        ld 9,.LC1@toc@l(9)
        xori 7,7,0x1
        lbzx 7,9,7
        xor 8,8,7
        lbzx 8,9,8
        xor 10,10,8
        lbzx 10,9,10
        xor 10,6,10
        lbzx 3,9,10
        rldicl 3,3,0,32
        blr

Tiny sample:
unsigned long long tiny(const unsigned char *inp)
{
  return inp[0] ^ inp[1];
}

tiny(unsigned char const*):
        lbz 9,0(3)
        lbz 10,1(3)
        xor 3,9,10
        rlwinm 3,3,0,0xff
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0

unsigned long long tiny2(const unsigned char *inp)
{
  unsigned long long a = inp[0];
  unsigned long long b = inp[1];
  return a ^ b;
}

tiny2(unsigned char const*):
        lbz 9,0(3)
        lbz 10,1(3)
        xor 3,9,10
        rlwinm 3,3,0,0xff
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0

lbz/lbzx creates a value 0 <= x < 256. xor of 2 such values does not change value range.


---


### compiler : `gcc`
### title : `[AVR] Missed optimization in access to upper-half of a variable`
### open_at : `2022-12-03T15:41:10Z`
### last_modified_date : `2022-12-05T16:11:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107957
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
Hello, 

I think I've found an optimization opportunity for AVR GCC. This might be similar to bug 66511, but also affects variables smaller than 64 bits. Please consider the following C code:

uint64_t x;
uint32_t y;
uint16_t z;
uint8_t w;

void foo(void)
{
    y = x >> 32;
}

void bar(void)
{
    z = y >> 16;
}

void rawr(void)
{
    w = z >> 8;
}

As you can see, all three functions just assign upper half of one variable to the another. When compiled with avr-gcc and -Wall -Wextra and -O3 flags, the following assembly is produced:

foo():
	push r16
	lds r18,x
	lds r19,x+1
	lds r20,x+2
	lds r21,x+3
	lds r22,x+4
	lds r23,x+5
	lds r24,x+6
	lds r25,x+7
	ldi r16,lo8(32)
	rcall __lshrdi3
	sts y,r18
	sts y+1,r19
	sts y+2,r20
	sts y+3,r21
	pop r16
	ret
bar():
	lds r24,y
	lds r25,y+1
	lds r26,y+2
	lds r27,y+3
	sts z+1,r27
	sts z,r26
	ret
rawr():
	lds r24,z+1
	sts w,r24
	ret
        
I'm not a compiler expert, but I'd say that this is a missed optimization. In every case there are twice as many lds operations as needed. For comparison, GCC for x86_64 does generate code which performs DWORD read in foo(), WORD read in bar() and BYTE read in rawr(). I've found that the following definitions generate identical assembly on x86_64 and more optimal assembly on AVR:

void foo2(void)
{
    y = ((uint32_t*)&x)[1];
}

void bar2(void)
{
    z = ((uint16_t*)&y)[1];
}

void rawr2(void)
{
    w = ((uint8_t*)&z)[1];
}

foo2():
	lds r24,x+4
	lds r25,x+4+1
	lds r26,x+4+2
	lds r27,x+4+3
	sts y,r24
	sts y+1,r25
	sts y+2,r26
	sts y+3,r27
	ret
bar2():
	lds r24,y+2
	lds r25,y+2+1
	sts z+1,r25
	sts z,r24
	ret
rawr2():
	lds r24,z+1
	sts w,r24
	ret

I've checked my local installation of AVR GCC 12.2.0 on Manjaro and different AVR GCC versions on Godbolt. They all seem to produce the same machine code.

$ avr-gcc -v
Using built-in specs.
Reading specs from /usr/lib/gcc/avr/12.2.0/device-specs/specs-avr2
COLLECT_GCC=avr-gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/avr/12.2.0/lto-wrapper
Target: avr
Configured with: /build/avr-gcc/src/gcc-12.2.0/configure --disable-install-libiberty --disable-libssp --disable-libstdcxx-pch --disable-libunwind-exceptions --disable-linker-build-id --disable-nls --disable-werror --disable-__cxa_atexit --enable-checking=release --enable-clocale=gnu --enable-gnu-unique-object --enable-gold --enable-languages=c,c++ --enable-ld=default --enable-lto --enable-plugin --enable-shared --infodir=/usr/share/info --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --prefix=/usr --target=avr --with-as=/usr/bin/avr-as --with-gnu-as --with-gnu-ld --with-ld=/usr/bin/avr-ld --with-plugin-ld=ld.gold --with-system-zlib --with-isl --enable-gnu-indirect-function
Thread model: single
Supported LTO compression algorithms: zlib zstd
gcc version 12.2.0 (GCC)

I'd appreciate if you could look into this. Thank you!


---


### compiler : `gcc`
### title : `backward propagation of finite property not performed`
### open_at : `2022-12-05T13:35:28Z`
### last_modified_date : `2023-02-14T21:28:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107972
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Here is another example where with the help of the FP ranger capabilities the compiler should generate better code than it does today (trunk):

double f(double a, double b)
{
  if (!__builtin_isfinite(a))
    return -1.0;

  double res = a + b;
  if (! __builtin_isfinite(res))
    __builtin_unreachable();
  return res;
}

The condition guaranteed by the __builtin_unreachable implies that neither a nor b cannot be finite.  Hence the initial comparison can be elided.

The same is true for - and * and also for the first operand of /.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Bogus -Warray-bounds diagnostic with std::sort`
### open_at : `2022-12-06T07:28:01Z`
### last_modified_date : `2023-09-28T08:04:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107986
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
PR104165 comment 2 talks about a test case reduced in https://bugzilla.redhat.com/show_bug.cgi?id=2051783:

#include <algorithm>

bool cond;
int foo;
                                      
int func (void)
{
  int a[3], qa = 0;
  for(int i = 0; i < 3; i++)
    if (cond)
      a[qa++] = foo;

  std::sort (a, a + qa);
  return 0;
}

which still warns with both GCC 12 and 13 at -O2 -Wall like

In file included from /home/rguenther/install/gcc-13/usr/local/include/c++/13.0.0/algorithm:61,
                 from t.C:1:
In function 'void std::__final_insertion_sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = int*; _Compare = __gnu_cxx::__ops::_Iter_less_iter]',
    inlined from 'void std::__final_insertion_sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = int*; _Compare = __gnu_cxx::__ops::_Iter_less_iter]' at /home/rguenther/install/gcc-13/usr/local/include/c++/13.0.0/bits/stl_algo.h:1854:5,
    inlined from 'void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = int*; _Compare = __gnu_cxx::__ops::_Iter_less_iter]' at /home/rguenther/install/gcc-13/usr/local/include/c++/13.0.0/bits/stl_algo.h:1950:31,
    inlined from 'void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = int*; _Compare = __gnu_cxx::__ops::_Iter_less_iter]' at /home/rguenther/install/gcc-13/usr/local/include/c++/13.0.0/bits/stl_algo.h:1942:5,
    inlined from 'void std::sort(_RAIter, _RAIter) [with _RAIter = int*]' at /home/rguenther/install/gcc-13/usr/local/include/c++/13.0.0/bits/stl_algo.h:4860:1,
    inlined from 'int func()' at t.C:13:13:
/home/rguenther/install/gcc-13/usr/local/include/c++/13.0.0/bits/stl_algo.h:1859:32: warning: array subscript 16 is outside array bounds of 'int [3]' [-Warray-bounds=]
 1859 |           std::__insertion_sort(__first, __first + int(_S_threshold), __comp);
      |           ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
t.C: In function 'int func()':
t.C:8:7: note: at offset 64 into object 'a' of size 12
    8 |   int a[3], qa = 0;
      |       ^

I'll also note the lack of a non-included location in t.C.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Extra mov instructions with ternary on x86`
### open_at : `2022-12-06T12:17:09Z`
### last_modified_date : `2023-07-07T10:44:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107991
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `middle-end`
### version : `12.2.0`
### severity : `normal`
### contents :
Compiled with -O2 on x86, gcc trunk produces 3 mov instructions for each of the following functions:

int foo(bool b, int i, int j) {
    return b ? i - j : i;
}

int bar(bool b, int i, int j) {
    return i + (b ? -j : 0);
}

int baz(bool b, int i, int j) {
    return i - (b ? j : 0);
}

-------------

Whearas with gcc 7.5, only 1 mov was produced for foo and bar, and two for baz


---


### compiler : `gcc`
### title : `x-form logical operations with dot instructions are not emitted.`
### open_at : `2022-12-07T06:28:57Z`
### last_modified_date : `2023-01-18T06:37:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108004
### status : `SUSPENDED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
//test case
int foo (int a, int b, int c, int d)
{
  return (a & b) > 0 ? c : d;
}

//assemble on P9
        and 3,3,4
        cmpwi 0,3,0
        isel 5,5,6,1
        extsw 3,5

The "and" and "cmpwi" can be optimized to "and." instruction. The same as "or" and "xor".


---


### compiler : `gcc`
### title : `RISC-V：Bad codegen in scalar code comparing to LLVM`
### open_at : `2022-12-08T01:19:30Z`
### last_modified_date : `2022-12-08T07:03:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108016
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
This is the simple code example:
https://godbolt.org/z/jKGavbx7o

Post it here and let me not forget about it. 
If someone can fix it, I will really appreciate.

If not, I will fix it if I have time (Currently busy with working on RVV support).


---


### compiler : `gcc`
### title : `ptr+v >= ptr + d should converted into (long)v >= CST(/sizeof(*ptr))`
### open_at : `2022-12-08T03:52:20Z`
### last_modified_date : `2022-12-12T20:18:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108017
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Take:
```
_Bool f(int *a, __SIZE_TYPE__ t, __SIZE_TYPE__ t1)
{
    int *b = a + t;
    int *c = a + t1;
    return c >= b;
}

_Bool f1(int *a, __SIZE_TYPE__ t, __SIZE_TYPE__ t1)
{
    long tt = t;
    long tt1 = t1;
    return tt1 >= tt;
}
```
These two should give the same code.

LLVM does this optimization.

I noticed this while looking into the code from the blog at https://lemire.me/blog/2022/12/06/optimizing-compilers-reload-vector-constants-needlessly/ (but not vector constant issue though).


---


### compiler : `gcc`
### title : `Wide immediate sequences not scheduled for POWER10 fusion`
### open_at : `2022-12-08T06:40:10Z`
### last_modified_date : `2022-12-08T06:45:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108018
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
POWER10 has "wideimmediate" fusion sequences that include addi rx,ra,si ; addis rx,rx,SI and addis rx,ra,si ; addi rx,rx,SI

--- test.c ---
static int foo, bar;

unsigned long test(void)
{
        return (unsigned long)&foo + (unsigned long)&bar;
}
---

This test case when compiled with -O2 -mcpu=power10 -mno-pcrel results in

        addis 2,12,.TOC.-.LCF0@ha
        addi 2,2,.TOC.-.LCF0@l
        addis 3,2,.LANCHOR0@toc@ha
        addis 9,2,.LANCHOR0+4@toc@ha
        addi 3,3,.LANCHOR0@toc@l
        addi 9,9,.LANCHOR0+4@toc@l
        add 3,3,9
        blr

The TOC pointer generation is scheduled properly because it is the global entry prologue, but the variable address generation is scheduled to favour dependencies rather than the static fusion sequences that P10 has, which should be preferable.


---


### compiler : `gcc`
### title : ``std::experimental::simd` not inlined`
### open_at : `2022-12-09T11:29:17Z`
### last_modified_date : `2023-05-25T07:08:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108030
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.2.0`
### severity : `normal`
### contents :
Created attachment 54052
Diff we applied to a local copy of the <experimental/simd>headers.

We tried to explicitely vectorize a C++ function using `std::experimental::simd` in our particle-in-cell simulation [picongpu](https://github.com/ComputationalRadiationPhysics/picongpu). The function is already called from a long call tree of functions marked `__attribute__((always_inline))`. Profiling the code shows that several constructs of `std::experimental::simd` where not inlined, leading to catastrophic performance (several times slower than scalar code).

We compiled, among other flags, with:
```
-g
-march=native
-mtune=native
-fopenmp
-O3
-DNDEBUG
-pthread
-std=c++17
```

We mostly used multiplication/addition as well as the broadcast and generator constructors of SIMD types. We saw several calls to `_S_multiplies` (IIRC) and `_S_generate`/`_S_generator` that were not inlined, depending on whether we used `std::experimental::native_simd` or `std::experimental::fixed_size_simd`.

Upon inspection of the `<experimental/bits/simd_*>` headers, we saw that several functions are not annotated with `_GLIBCXX_SIMD_INTRINSIC` or other ways to force inlining. We think this is a missed optimization opportunity.

We tried `-finline-limit=1000000` without success.

We thus applied `_GLIBCXX_SIMD_INTRINSIC` and `__attribute__((always_inline))` to functions from the SIMD headers that showed up in the profiler (perf) until all calls were inlined.

Please apply further attributes to SIMD intrinsics to force their inlining. Mind, that this also affects lambda expressions.

I have attached a diff which our changes to the SIMD headers, but we also bulk replaced several declaration specifiers, so we may have added more force-inlines than potentially necessary.


---


### compiler : `gcc`
### title : `riscv: Access of members of a global structure is not optimized in atomic operations`
### open_at : `2022-12-09T13:01:21Z`
### last_modified_date : `2022-12-09T14:28:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108031
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
Consider the following test code:

struct s {
  int a;
  int b;
};

struct s s;

int f(void)
{
  return __atomic_fetch_add(&s.a, 1, 0) + __atomic_fetch_add(&s.b, 1, 0);
}

Using gcc -O2 -S -o - test.c yields:

f:
        lui     a5,%hi(s)
        li      a4,1
        addi    a5,a5,%lo(s)
        amoadd.w a0,a4,0(a5)
        lui     a5,%hi(s+4) <-- this should be: addi a5, a5, 4
        addi    a5,a5,%lo(s+4) <-- this should be removed
        amoadd.w a3,a4,0(a5)
        add     a0,a0,a3
        ret

This seems to be a backend issue since on arm we have for example (gcc -march=armv7-a -O2 -S -o - test.c):
f:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        movw    r3, #:lower16:.LANCHOR0
        movt    r3, #:upper16:.LANCHOR0
.L2:
        ldrex   r0, [r3]
        add     r2, r0, #1
        strex   r1, r2, [r3]
        cmp     r1, #0
        bne     .L2
        add     r3, r3, #4
.L3:
        ldrex   r2, [r3]
        add     r1, r2, #1
        strex   ip, r1, [r3]
        cmp     ip, #0
        bne     .L3
        add     r0, r0, r2
        bx      lr

clang produces (clang -O2 -S -o - test.c --target=riscv32):

f:
        lui     a0, %hi(s)
        addi    a0, a0, %lo(s)
        li      a1, 1
        amoadd.w        a2, a1, (a0)
        addi    a0, a0, 4
        amoadd.w        a0, a1, (a0)
        add     a0, a0, a2
        ret


---


### compiler : `gcc`
### title : `Unnecessary extension storing same value twice to small location`
### open_at : `2022-12-09T21:43:10Z`
### last_modified_date : `2022-12-09T22:01:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108039
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Compile the following code for rv64 with -O2:

typedef signed long int int64_t;
void replace_weaker_arc(int *id1, int *id2, int64_t number)
{
    *id1 = number;
    *id2 = number;
}

We generate:

replace_weaker_arc:
        sext.w  a2,a2
        sw      a2,0(a0)
        sw      a2,0(a1)
        ret


The key insns (from cse1 dump) are:

(insn 8 5 9 2 (set (reg:DI 134 [ _1 ])
        (sign_extend:DI (subreg:SI (reg/v:DI 137 [ number ]) 0))) "j.c":4:10 116 {extendsidi2}
     (nil))
(insn 9 8 10 2 (set (mem:SI (reg/v/f:DI 135 [ id1 ]) [1 *id1_4(D)+0 S4 A32])
        (subreg/s/u:SI (reg:DI 134 [ _1 ]) 0)) "j.c":4:10 178 {*movsi_internal}
     (nil))
(insn 10 9 0 2 (set (mem:SI (reg/v/f:DI 136 [ id2 ]) [1 *id2_6(D)+0 S4 A32])
        (subreg/s/u:SI (reg:DI 134 [ _1 ]) 0)) "j.c":5:10 178 {*movsi_internal}
     (nil))


fwprop tries to propagate insn 8 into insns 9 and 10, but that fails the complexity check:

cannot propagate from insn 8 into insn 9: would increase complexity of pattern
cannot propagate from insn 8 into insn 10: would increase complexity of pattern

But propagation in this case allows us to eliminate a subreg & extension, so it's profitable.

I haven't tested it, but something like this captures the RTL propagation generates and the fact that it should simplify.  We may need to tighten it a little by verifying modes:
diff --git a/gcc/fwprop.cc b/gcc/fwprop.cc
index fc652ab9a1f..a86e9320908 100644
--- a/gcc/fwprop.cc
+++ b/gcc/fwprop.cc
@@ -258,6 +258,14 @@ fwprop_propagation::classify_result (rtx old_rtx, rtx new_rtx)
       return CONSTANT | PROFITABLE;
     }
 
+  /* Allow replacements where we are able to eliminate a
+     (subreg (any_extend (...)).  */
+  if (GET_CODE (old_rtx) == SUBREG
+      && (GET_CODE (SUBREG_REG (old_rtx)) == SIGN_EXTEND
+         || GET_CODE (SUBREG_REG (old_rtx)) == ZERO_EXTEND)
+      && XEXP (SUBREG_REG (old_rtx), 0) == new_rtx)
+    return PROFITABLE;


---


### compiler : `gcc`
### title : `ivopts results in extra instruction in simple loop`
### open_at : `2022-12-09T22:45:01Z`
### last_modified_date : `2023-05-30T20:40:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108041
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
ivopts seems to make a bit of a mess out of this code resulting in the loop having an unnecessary instruction.  Compile with rv64 -O2:

typedef struct network
{
  long nr_group, full_groups, max_elems;
} network_t;
void marc_arcs(network_t* net)
{
  while (net->full_groups < 0) {
    net->full_groups = net->nr_group + net->full_groups;
    net->max_elems--;
  }
}





After slp1 we have this loop:
;;   basic block 3, loop depth 0
;;    pred:       2
  _1 = net_8(D)->nr_group;
  net__max_elems_lsm.4_16 = net_8(D)->max_elems;
;;    succ:       4

;;   basic block 4, loop depth 1
;;    pred:       7
;;                3
  # _13 = PHI <_2(7), _11(3)>
  # net__max_elems_lsm.4_5 = PHI <_4(7), net__max_elems_lsm.4_16(3)>
  _2 = _1 + _13;
  _4 = net__max_elems_lsm.4_5 + -1;
  if (_2 < 0)
    goto <bb 7>; [89.00%]
  else
    goto <bb 5>; [11.00%]
;;    succ:       7
;;                5

;;   basic block 7, loop depth 1
;;    pred:       4
  goto <bb 4>; [100.00%]
;;    succ:       4

;;   basic block 5, loop depth 0
;;    pred:       4
  # _12 = PHI <_2(4)>
  # _17 = PHI <_4(4)>
  net_8(D)->full_groups = _12;
  net_8(D)->max_elems = _17;
;;    succ:       6


Of particular interest is the max_elems computation into _4.  We accumulate it in the loop, then do the final store after the loop (thank you LSM!).  After ivopts we have:


;;   basic block 3, loop depth 0
;;    pred:       2
  _1 = net_8(D)->nr_group;
  net__max_elems_lsm.4_16 = net_8(D)->max_elems;
  _22 = net__max_elems_lsm.4_16 + -1;
  ivtmp.10_21 = (unsigned long) _22;
;;    succ:       4

;;   basic block 4, loop depth 1
;;    pred:       7
;;                3
  # _13 = PHI <_2(7), _11(3)>
  # ivtmp.10_3 = PHI <ivtmp.10_18(7), ivtmp.10_21(3)>
  _2 = _1 + _13;
  _4 = (long int) ivtmp.10_3;
  ivtmp.10_18 = ivtmp.10_3 - 1;
  if (_2 < 0)
    goto <bb 7>; [89.00%]
  else
    goto <bb 5>; [11.00%]
;;    succ:       7
;;                5

;;   basic block 7, loop depth 1
;;    pred:       4 
  goto <bb 4>; [100.00%]
;;    succ:       4
  
;;   basic block 5, loop depth 0
;;    pred:       4
  # _12 = PHI <_2(4)>
  # _17 = PHI <_4(4)>
  net_8(D)->full_groups = _12;
  net_8(D)->max_elems = _17;
;;    succ:       6

Note the introduction of the IV and its relationship to _4.  Essentially we compute both in the loop even _4 is always one greater than the IV.  Worse yet, the IV is only used to compute _4!  And since they differ by 1, we actually compute both and keep them alive resulting in this final code for rv64:




.L3:
        add     a5,a5,a2
        mv      a3,a4
        addi    a4,a4,-1
        blt     a5,zero,.L3
        sd      a5,8(a0)
        sd      a3,16(a0)


Note how we had to "stash away" the value of a4 before the decrement so that we could store it after the loop.  The induction variable doesn't really buy us anything in this loop -- it's actively harmful.  Not using the IV would probably be best.  Second best would be to realize that _4 (aka a3) can be derived from the IV (a4) after the loop by adding 1.


---


### compiler : `gcc`
### title : `PPCLE: gcc does not recognize that lbzx does zero extend`
### open_at : `2022-12-10T12:06:21Z`
### last_modified_date : `2022-12-10T12:09:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108048
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
extern unsigned char magic1[256];

unsigned int hash(const unsigned char inp[4])
{
   const unsigned long long INIT = 0x1ULL;
   unsigned long long h1 = INIT;
   h1 = magic1[((unsigned long long)inp[0]) ^ h1];
   h1 = magic1[((unsigned long long)inp[1]) ^ h1];
   h1 = magic1[((unsigned long long)inp[2]) ^ h1];
   h1 = magic1[((unsigned long long)inp[3]) ^ h1];
   return h1;
}

Generates:

hash(unsigned char const*):
.LCF0:
        addi 2,2,.TOC.-.LCF0@l
        lbz 9,0(3)
        addis 10,2,.LC0@toc@ha
        ld 10,.LC0@toc@l(10)
        lbz 6,1(3)
        lbz 7,2(3)
        lbz 8,3(3)
        xori 9,9,0x1
        lbzx 9,10,9
        xor 9,9,6
        rlwinm 9,9,0,0xff <= unnecessary
        lbzx 9,10,9
        xor 9,9,7
        rlwinm 9,9,0,0xff <= unnecessary
        lbzx 9,10,9
        xor 9,9,8
        rlwinm 9,9,0,0xff <= unnecessary
        lbzx 3,10,9
        blr


All XOR operations are done in unsigned long long (64-bit). gcc adds unnecessary rlwinm. lbz and lbzx does zero extension (no cleanup of upper bits required).


---


### compiler : `gcc`
### title : `s390: Compiler adds extra zero extend after xoring 2 zero extended values`
### open_at : `2022-12-10T12:23:50Z`
### last_modified_date : `2022-12-12T15:11:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108049
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
Same issue for PPC: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107949

extern unsigned char magic1[256];

unsigned int hash(const unsigned char inp[4])
{
   const unsigned long long INIT = 0x1ULL;
   unsigned long long h1 = INIT;
   h1 = magic1[((unsigned long long)inp[0]) ^ h1];
   h1 = magic1[((unsigned long long)inp[1]) ^ h1];
   h1 = magic1[((unsigned long long)inp[2]) ^ h1];
   h1 = magic1[((unsigned long long)inp[3]) ^ h1];
   return h1;
}

hash(unsigned char const*):
        llgc    %r4,1(%r2) <= zero extends to 64-bit
        lgrl    %r1,.LC0
        llgc    %r3,0(%r2) <= zero extends to 64-bit
        xilf    %r3,1 
        llgc    %r3,0(%r3,%r1)
        xr      %r3,%r4 <= should be 64-bit xor
        llgc    %r4,2(%r2) <= zero extends to 64-bit
        llgcr   %r3,%r3 <= unnecessary
        llgc    %r2,3(%r2)
        llgc    %r3,0(%r3,%r1)
        xr      %r3,%r4 <= should be 64-bit xor
        llgcr   %r3,%r3 <= unnecessary
        llgc    %r3,0(%r3,%r1) <= zero extends to 64-bit
        xrk     %r2,%r3,%r2 <= should be 64-bit xor
        llgcr   %r2,%r2 <= unnecessary
        llgc    %r2,0(%r2,%r1)
        br      %r14

Smaller sample:
unsigned long long tiny2(const unsigned char *inp)
{
  unsigned long long a = inp[0];
  unsigned long long b = inp[1];
  return a ^ b;
}

tiny2(unsigned char const*):
        llgc    %r1,0(%r2)
        llgc    %r2,1(%r2)
        xrk     %r2,%r1,%r2
        llgcr   %r2,%r2
        br      %r14


---


### compiler : `gcc`
### title : `Missed optimization with O3 and m32 when unwinding a loop`
### open_at : `2022-12-11T12:11:45Z`
### last_modified_date : `2022-12-11T18:50:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108058
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.1`
### severity : `normal`
### contents :
An unwinded loop produces a suboptimal code with O3 and m32 flags.
Example:

#include <stdint.h>
void copyFunc(uint8_t* dst, uint8_t* src, int n)
{
    while (n > 0) {
        *(uint32_t*)dst = *(uint32_t*)src;
	dst += 4;
	src += 4;
        n--;
    }
}

Since v 4.9.4 the loop is unwinded and produces two return points:
...
.L1:
        pop     ebx
        pop     esi
        pop     edi
        ret
...
        pop     ebx
        pop     esi
        pop     edi
        ret

Apparently this does not seem to happen if m32 is not set (probably there are enough registers to use).


---


### compiler : `gcc`
### title : `failure to combine range test to bit test`
### open_at : `2022-12-12T10:12:54Z`
### last_modified_date : `2023-10-12T09:17:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108070
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
(insn 8 7 11 2 (parallel [ 
            (set (reg:QI 89)
                (and:QI (reg:QI 90 [ *info_6(D) ]) 
                    (const_int 3 [0x3])))
            (clobber (reg:CC 17 flags))
        ]) "/home/rguenther/src/trunk/gcc/testsuite/c-c++-common/fold-masked-cmp-1.c":37:7 554 {*andqi_1}
     (expr_list:REG_DEAD (reg:QI 90 [ *info_6(D) ])
        (expr_list:REG_UNUSED (reg:CC 17 flags)
            (nil))))
(insn 11 8 12 2 (set (reg:CC 17 flags)
        (compare:CC (reg:QI 89)
            (const_int 1 [0x1]))) "/home/rguenther/src/trunk/gcc/testsuite/c-c++-common/fold-masked-cmp-1.c":37:6 9 {*cmpqi_1}
     (expr_list:REG_DEAD (reg:QI 89) 
        (nil)))  
(jump_insn 12 11 13 2 (set (pc)
        (if_then_else (gtu (reg:CC 17 flags)
                (const_int 0 [0]))
            (label_ref 15)
            (pc))) "/home/rguenther/src/trunk/gcc/testsuite/c-c++-common/fold-masked-cmp-1.c":37:6 974 {*jcc}
     (expr_list:REG_DEAD (reg:CC 17 flags)
        (int_list:REG_BR_PROB 633507684 (nil)))
 -> 15) 

here insns 8, 11 and 12 should be combined to

(insn 7 6 8 2 (parallel [
            (set (reg:QI 88) 
                (and:QI (reg:QI 89 [ *info_7(D) ])
                    (const_int 2 [0x2])))
            (clobber (reg:CC 17 flags))
        ]) "/home/rguenther/src/gcc-12-branch/gcc/testsuite/c-c++-common/fold-masked-cmp-1.c":37:7 534 {*andqi_1}
     (expr_list:REG_DEAD (reg:QI 89 [ *info_7(D) ])
        (expr_list:REG_UNUSED (reg:CC 17 flags)
            (nil))))
(insn 8 7 9 2 (set (reg:CCZ 17 flags)
        (compare:CCZ (reg:QI 88) 
            (const_int 0 [0]))) "/home/rguenther/src/gcc-12-branch/gcc/testsuite/c-c++-common/fold-masked-cmp-1.c":37:6 5 {*cmpqi_ccno_1}
     (expr_list:REG_DEAD (reg:QI 88)
        (nil)))

the jump also needs altering here.  This is required to avoid regressing
code like c-c++-common/fold-masked-cmp-1.c when
applying https://gcc.gnu.org/pipermail/gcc-patches/2022-December/608153.html
where we no longer fold

;; Function test_exe (null)
;; enabled by -tree-original


{
  if (info->type <= 1)

to

;; Function test_exe (null)
;; enabled by -tree-original


{ 
  if ((BIT_FIELD_REF <*info, 8, 0> & 2) == 0)

note that applying this folding on GIMPLE produces a more costly operation
(we need an extra BIT_AND_EXPR here).  On x86_64 the folded code produces

        testb   $2, (%rdi)
        jne     .L8 

while the unfolded has

        movzbl  (%rdi), %eax
        andl    $3, %eax
        cmpb    $1, %al
        ja      .L5

as CC modes are involved I'm not sure if combine is the correct vehicle to
perform this.  IIRC arm folks wanted to add some bit-test-and-branch
patterns (that doesn't seem to be applied yet) which could make it possible
to optimize this during RTL expansion itself.


---


### compiler : `gcc`
### title : `[rs6000] sub-optimal float member accessing on struct parameter`
### open_at : `2022-12-12T12:15:39Z`
### last_modified_date : `2022-12-21T06:46:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108073
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
For the below code:

typedef struct DF {double a[4]; long l; } DF;
double __attribute__ ((noipa)) foo_df (DF arg){return arg.a[3];}


At -O2, with gcc trunk(13.0), we get below sequence:

std 6,-24(1)
ori 2,2,0
lfd 1,-24(1)
blr

Actually, just one "mtvsrd 1, 6" is enough.

In this case, the argument is passed through integer registers. 

For below code, it is similar:

typedef struct SF {float a[4];short l; } SF;
float foo (SF arg){return arg.a[3];}  

        std 4,-24(1)
        ori 2,2,0
        lfs 1,-20(1)
vs. below seq seems faster.
        rldicr 4,4,0,31
        mtvsrd 1,4
        xscvspdpn 1,1


---


### compiler : `gcc`
### title : `Failure to merge conditional bit clears`
### open_at : `2022-12-15T18:26:49Z`
### last_modified_date : `2023-06-23T03:17:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108133
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
void foo (int *p)
{
  if (*p & (1<<3))
    *p &= ~(1<<3);
  if (*p & (1<<5))
    *p &= ~(1<<5);
  if (*p & (1<<6))
    *p &= ~(1<<6);
}

could be optimized to

  if (*p & ((1<<3)|(1<<5)|(1<<6)))
    *p &= ~((1<<3)|(1<<5)|(1<<6));

We have such code in tree-inline.cc:

      /* Clear flags that need revisiting.  */
      if (gcall *call_stmt = dyn_cast <gcall *> (copy))
        {
          if (gimple_call_tail_p (call_stmt))
            gimple_call_set_tail (call_stmt, false);
          if (gimple_call_from_thunk_p (call_stmt))
            gimple_call_set_from_thunk (call_stmt, false);


---


### compiler : `gcc`
### title : `Missed optimization opportunity. Complex function that starts with if (param == 0) return 0;`
### open_at : `2022-12-17T23:01:49Z`
### last_modified_date : `2022-12-23T04:11:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108162
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `normal`
### contents :
I had a lot of fun writing https://bolinlang.com/does-it-inline

I was intentionally trying to break the optimizer. I found a few cases in clang and two cases I don't think should apply but I think both can gain from the last one. Skipping a function call when the function starts with `if (param == 0) return 0;`. I tested with a recursive fibonacci. If you scroll to the very bottom you'll see it.

Cases that clang optimizes but gcc did not A) Round 3 a and b. I don't think locale will affect it and clang optimizes it. All of round 4 clang optimizes. I suspect there's something getting in the way of peeking through push and size. I vaguely remember thinking std::forward may get in the way in the past but it was long enough ago that I don't remember what I was doing to think that


---


### compiler : `gcc`
### title : `[missed optimization] Call to virtual function under runtime index should be optimized into jump with an offset`
### open_at : `2022-12-19T17:43:03Z`
### last_modified_date : `2022-12-20T12:42:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108181
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
Given code below compiled with g++ 11 or newer, compiler should be able to optimize 'get' into similar code as manually-optimized 'get_opt'.

g++ -std=c++20 -O2

#include <bit>
#include <cstdint>

struct foo
{
    virtual constexpr int& get0() noexcept = 0;
    virtual constexpr int& get1() noexcept  = 0;
    virtual constexpr int& get2() noexcept  = 0;
    virtual constexpr int& get3() noexcept  = 0;
    virtual constexpr int& get4() noexcept  = 0;
    virtual constexpr int& get5() noexcept  = 0;
    virtual constexpr int& get6() noexcept  = 0;
    virtual constexpr int& get7() noexcept  = 0;
    virtual constexpr int& get8() noexcept  = 0;
    virtual constexpr int& get9() noexcept  = 0;
};


template <typename T, unsigned idx>
constexpr auto memPtr = nullptr;

template <typename T>
constexpr auto memPtr<T, 0> = &T::get0;

template <typename T>
constexpr auto memPtr<T, 1> = &T::get1;

template <typename T>
constexpr auto memPtr<T, 2> = &T::get2;

template <typename T>
constexpr auto memPtr<T, 3> = &T::get3;

template <typename T>
constexpr auto memPtr<T, 4> = &T::get4;

template <typename T>
constexpr auto memPtr<T, 5> = &T::get5;

template <typename T>
constexpr auto memPtr<T, 6> = &T::get6;

template <typename T>
constexpr auto memPtr<T, 7> = &T::get7;

template <typename T>
constexpr auto memPtr<T, 8> = &T::get8;

template <typename T>
constexpr auto memPtr<T, 9> = &T::get9;


int& get(unsigned idx, foo* f) noexcept
{
    switch (idx)
    {
        case 0:
            return (f->*memPtr<foo, 0>)();
        case 1:
            return (f->*memPtr<foo, 1>)();
        case 2:
            return (f->*memPtr<foo, 2>)();
        case 3:
            return (f->*memPtr<foo, 3>)();
        case 4:
            return (f->*memPtr<foo, 4>)();
        case 5:
            return (f->*memPtr<foo, 5>)();
        case 6:
            return (f->*memPtr<foo, 6>)();
        case 7:
            return (f->*memPtr<foo, 7>)();
        case 8:
            return (f->*memPtr<foo, 8>)();
        case 9:
            return (f->*memPtr<foo, 9>)();
        default:
            __builtin_unreachable();
    }
}

int& get_opt(unsigned idx, foo* f) noexcept
{
     // assuming System V x64 ABI
    struct RawMemPtr
    {
        std::uintptr_t v[2];
    };

    using PtrType = int& (foo::*)() noexcept;

    const RawMemPtr rawPtr{{sizeof(void*) * idx + 1, 0}};
    const auto ptr = std::bit_cast<PtrType>(rawPtr);

    return (f->*ptr)();
}


---


### compiler : `gcc`
### title : `rs6000: Use optimize_function_for_speed_p too early`
### open_at : `2022-12-20T03:01:59Z`
### last_modified_date : `2023-07-27T09:24:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108184
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
In the review of patch [1] for PR105818, Honza pointed out 

"I think we should generally avoid doing decisions about size/speed
optimizations so early since the setting may change due to attributes or
profile feedback..."

I agreed that the current uses of optimize_function_for_speed_p in function rs6000_option_override_internal are too early and can be inaccurate. I tried to make the below test case to demonstrate it.

Compiled with -mdejagnu-tune=power8 -O3:

__attribute__ ((cold)) int
fusion_short (short *p)
{
  return p[0x12345];
}

Since the function is attributed with cold, it's considered not to optimize for speed, so we shouldn't break the sign extended loads and fuse it with the addis, that is it's unexpected to see extsh generated but we have:

        addis 3,3,0x2
        lhz 3,18058(3)
        extsh 3,3

[1] https://gcc.gnu.org/pipermail/gcc-patches/2022-November/607527.html


---


### compiler : `gcc`
### title : `[12/13/14 Regression] -Wstringop-overread emitted on simple boost small_vector code`
### open_at : `2022-12-22T10:10:51Z`
### last_modified_date : `2023-05-08T12:26:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108197
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
```

#include <boost/container/small_vector.hpp>

struct MyThing
{
    int d0 = {};
};

void modify(boost::container::small_vector<MyThing, 10> &pp)
{
    pp.resize(1);

    pp[0].d0 = 3;
}

void foo()
{
    boost::container::small_vector<MyThing, 10> pp2;

    boost::container::small_vector<MyThing, 10> pp;

    pp.resize(1);

    pp[0].d0 = 2;

    pp2 = std::move(pp);
}
```

gives

```
/opt/compiler-explorer/libs/boost_1_80_0/boost/container/detail/copy_move_algo.hpp:184:19: warning: 'void* __builtin_memcpy(void*, const void*, long unsigned int)' reading between 41 and 9223372036854775804 bytes from a region of size 40 [-Wstringop-overread]
  184 |       std::memmove(dest_raw, beg_raw, sizeof(value_type)*n);
```

https://godbolt.org/z/rs3oj3YoE

Even though modify is never called, it must be in the code to reproduce the bug.


---


### compiler : `gcc`
### title : `Does not optimize trivial case with bit operations`
### open_at : `2022-12-23T18:48:58Z`
### last_modified_date : `2023-01-09T13:41:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108215
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
https://godbolt.org/z/5e3eKqPqs

```C
#include <stdint.h>

int firewall3(const uint8_t *restrict data) {
    const uint32_t src = *((const uint32_t *)data);
    if ((src & 0xFFFF0000) == 0x11220000) return 1;
    if ((src & 0xFFFFFF00) == 0x11223300) return 1;
    return 0;
}

int firewall4(const uint8_t *restrict data) {
    const uint32_t src = *((const uint32_t *)data);
    if ((src & 0xFFFFFF00) == 0x11223300) return 1;
    if ((src & 0xFFFF0000) == 0x11220000) return 1;
    return 0;
}
```

```
firewall3:
        movl    (%rdi), %eax
        xorw    %ax, %ax
        cmpl    $287440896, %eax
        sete    %al
        movzbl  %al, %eax
        ret
firewall4:
        movl    (%rdi), %eax
        movl    $1, %edx
        movl    %eax, %ecx
        xorb    %cl, %cl
        cmpl    $287453952, %ecx
        je      .L3
        xorw    %ax, %ax
        xorl    %edx, %edx
        cmpl    $287440896, %eax
        sete    %dl
.L3:
        movl    %edx, %eax
        ret
```

firewall3(): Excellent!
firewall4(): FAIL!

It's obvious that order of comparisons in this example does not matter. So I think misoptimisation of firewall4() is a bug.


---


### compiler : `gcc`
### title : `bogus -Warray-bounds with pointer to constant local`
### open_at : `2022-12-24T10:53:55Z`
### last_modified_date : `2022-12-24T20:46:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108217
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
Repro:

void ExternFunc1();
void ExternFunc2(const int*);

char mem[32];

static void StaticFunc(const int* i) {
  void* ptr = (void*)0;
  switch (*i) {
    case 0:
      ExternFunc2(i);
      return;
    case 1:
      __builtin_memcpy(mem, &ptr, sizeof(ptr));
      return;
    case 2: {
      __builtin_memcpy(mem, &ptr, 32);
      return;
    }
  }
}

void Bad() {
  const int i = 1;
  ExternFunc1();
  StaticFunc(&i);
}

This reproduces on trunk according to Godbolt: https://godbolt.org/z/vYGo1z6bG

Godbolt also indicates a missed optimization, which is probably related to the bogus warning.  Clang correctly performs constant propagation of the local `i`, whereas GCC seems to think that all cases of the switch() are reachable.

It is true that &i escapes, but mutating `i` is UB because it is const, so it should be legal to perform constant propagation here.

Additionally, even if ExternFunc2() mutated `i`, it would be too late to change its value in time to affect the switch().


---


### compiler : `gcc`
### title : `Unnecessary division when looping over array with size of elements not a power of two`
### open_at : `2022-12-26T09:20:16Z`
### last_modified_date : `2023-01-09T13:58:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108227
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Consider

typedef struct coord {
  double x, y, z;
} coord;

void foo(coord *from, coord *to)
{
  unsigned long int n = to - from;
  for (unsigned long int i=0; i < n; i++)
    {
      from[i].x = from[i].x + 1.0;
    }
}

void bar (coord *from, coord *to)
{
  char *c_from = (char *) from, *c_to = (char *) to;
  coord *p = from;
  long int c_n = c_to - c_from;
  for (long int i=0; i < c_n; i+= sizeof(coord))
    {
      p->x = p->x + 1.0;
      p++;
    }
}

The code is functionally equivalent, but the assembly somewhat different:

foo has

foo:
.LFB0:
        .cfi_startproc
        movabsq $-6148914691236517205, %rax
        movq    %rsi, %rdx
        subq    %rdi, %rdx
        sarq    $3, %rdx
        imulq   %rax, %rdx
        cmpq    %rdi, %rsi
        je      .L1
        movsd   .LC0(%rip), %xmm1
        xorl    %eax, %eax
        .p2align 4,,10
        .p2align 3
.L3:
        movsd   (%rdi), %xmm0
        addq    $1, %rax
        addq    $24, %rdi
        addsd   %xmm1, %xmm0
        movsd   %xmm0, -24(%rdi)
        cmpq    %rdx, %rax
        jb      .L3
.L1:
        ret

so it first divides by 12 (efficiently) to determine n. There are 7 instructions in the loop itself.

bar has

bar:
.LFB1:
        .cfi_startproc
        subq    %rdi, %rsi
        testq   %rsi, %rsi
        jle     .L6
        movsd   .LC0(%rip), %xmm1
        xorl    %eax, %eax
        .p2align 4,,10
        .p2align 3
.L8:
        movsd   (%rdi,%rax), %xmm0
        addsd   %xmm1, %xmm0
        movsd   %xmm0, (%rdi,%rax)
        addq    $24, %rax
        cmpq    %rax, %rsi
        jg      .L8
.L6:
        ret

no need to divide, and one instruction less in the loop.

I would expect foo to match bar.


---


### compiler : `gcc`
### title : `assert() spuriously activates maybe-uninitialized warning`
### open_at : `2022-12-26T19:02:32Z`
### last_modified_date : `2022-12-27T16:28:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108230
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization, needs-bisection, needs-reduction`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
gcc-12.2 reports a maybe-uninitialized warning for the last line of the below code if-and-only-if the macro ADD_ASSERT is defined:

#include <bitset>
#include <Eigen/Core>

using Array = Eigen::Array<float, -1, 1, 0, 3, 1>;

struct Tree {
  Array array;
  std::bitset<8> bitset;
};

auto func(Tree* tree) {
  int c = tree->array.rows();
  int d = tree->bitset.count();
#ifdef ADD_ASSERT
  assert(c==d);
#endif  // ADD_ASSERT
  Array e(c);
  for (int k = 0; k < d; ++k) e(k) = k;
  return e.sum() / (e + 1);
}

int main() {
  Tree tree;
  func(&tree);
}

This is a bug because adding this assert() should have no bearing on whether or not the return statement might entail uninitialized memory usage.

gcc cmd1, generates warning: -Wmaybe-uninitialized -O3 -DADD_ASSERT
gcc cmd2, no warning:        -Wmaybe-uninitialized -O3

Godbolt link, which includes full gcc warning output: https://godbolt.org/z/nv11aaKb6

This example, convoluted as it seems, appears to be a minimal reproducible example. For example:

- Removing the for-loop makes the warning appear with both gcc cmds
- Replacing the return expression with something simpler like (e + 1) makes the warning disappear with both gcc cmds
- Replacing std::bitset with something else also makes the warning disappear with both gcc cmds

Eigen library version is 3.4.0.


---


### compiler : `gcc`
### title : `[11/12 Regression] Missed optimization for static const std::string_view(const char*)`
### open_at : `2022-12-28T16:31:55Z`
### last_modified_date : `2023-07-07T10:44:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108243
### status : `ASSIGNED`
### tags : `missed-optimization, wrong-code`
### component : `c++`
### version : `11.3.0`
### severity : `normal`
### contents :
Given the following code:

    #include <string_view>

    int main()
    {
        static const std::string_view foo("bar");
        return foo.size();
    }

With gcc 7.3.1, using "g++ -std=c++17 -O2", it treats foo as constexpr, and compiles main to:

    mov $0x3,%eax
    retq

With gcc 11.3.0, also using "g++ -std=c++17 -O2", it does not treat foo as constexpr, using runtime initialization instead (with static guard, etc.)

However, by using the string_view constructor that accepts a length:

    static const std::string_view foo("bar", 3);

gcc 11.3.0 is then able to treat foo as constexpr and compiles to just a mov and ret as 7.3.1 does.

Using Compiler Explorer it appears that gcc 9 is the first version where it lost the optimization; every subsequent version seems to behave as 11.3.0.

---

Additional info:

gcc 11.3.0:

Using built-in specs.
COLLECT_GCC=/usr/bin/g++
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/11/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none:amdgcn-amdhsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 11.3.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-11/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-11 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --enable-cet --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-11-xKiWfi/gcc-11-11.3.0/debian/tmp-nvptx/usr,amdgcn-amdhsa=/build/gcc-11-xKiWfi/gcc-11-11.3.0/debian/tmp-gcn/usr --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-serialization=2
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.3.0 (Ubuntu 11.3.0-1ubuntu1~22.04) 

gcc 7.3.1:

Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/opt/rh/devtoolset-7/root/usr/libexec/gcc/x86_64-redhat-linux/7/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-7/root/usr --mandir=/opt/rh/devtoolset-7/root/usr/share/man --infodir=/opt/rh/devtoolset-7/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --with-default-libstdcxx-abi=gcc4-compatible --with-isl=/builddir/build/BUILD/gcc-7.3.1-20180303/obj-x86_64-redhat-linux/isl-install --enable-libmpx --enable-gnu-indirect-function --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC)


---


### compiler : `gcc`
### title : `Missed opportunity to generate shNadd on risc-v`
### open_at : `2022-12-28T19:07:52Z`
### last_modified_date : `2023-04-20T14:52:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108247
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
int sub2(int a, long long b) {
  b = (b << 32) >> 31;
  unsigned int x = a + b;
  return x;
}


When compiled with Zba should ideally generate something like this:

  sh1add  a0, a1, a0
  sext.w  a0, a0
  ret


I suspect we need to match something like this as a define_split:

Failed to match this instruction:
(set (reg:SI 142 [ x ])
    (plus:SI (subreg:SI (and:DI (ashift:DI (reg:DI 146)
                    (const_int 1 [0x1]))
                (const_int 4294967294 [0xfffffffe])) 0)
        (subreg:SI (reg:DI 145) 0)))


Or this:


(set (reg:DI 144 [ x ])
    (sign_extend:DI (plus:SI (subreg:SI (and:DI (ashift:DI (reg:DI 146)
                        (const_int 1 [0x1]))
                    (const_int 4294967294 [0xfffffffe])) 0)
            (subreg:SI (reg:DI 145) 0))))


---


### compiler : `gcc`
### title : `Repeated address-of (lea) not optimized for size.`
### open_at : `2022-12-30T22:30:00Z`
### last_modified_date : `2023-01-09T14:10:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108255
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
https://godbolt.org/z/q5sx9e49j


void f(int *);

int g(int of) {
    int x = 13;
    f(&x);
    f(&x);
    f(&x);
    f(&x);
    f(&x);
    f(&x);
    f(&x);
    f(&x);
    return 0;
}


Got:

g(int):
        sub     rsp, 24
        lea     rdi, [rsp+12]
        mov     DWORD PTR [rsp+12], 13
        call    f(int*)
        lea     rdi, [rsp+12]             # compute, 5 bytes
        call    f(int*)
        lea     rdi, [rsp+12]             # recompute, 5 bytes
        call    f(int*)
        lea     rdi, [rsp+12]             # recompute, 5 bytes
        call    f(int*)
        lea     rdi, [rsp+12]
        call    f(int*)
        lea     rdi, [rsp+12]
        call    f(int*)
        lea     rdi, [rsp+12]
        call    f(int*)
        lea     rdi, [rsp+12]
        call    f(int*)
        xor     eax, eax
        add     rsp, 24
        ret


But, note that lea is 5 bytes.

Expected (generated by clang 3.0 - 15.0):

g(int):                                  # @g(int)
        push    rbx                              # extra, but just 1 byte
        sub     rsp, 16
        mov     dword ptr [rsp + 12], 13         # CSE temp
        lea     rbx, [rsp + 12]
        mov     rdi, rbx                         # use
        call    f(int*)@PLT
        mov     rdi, rbx                         # reuse, 3 bytes
        call    f(int*)@PLT
        mov     rdi, rbx                         # reuse, 3 bytes
        call    f(int*)@PLT
        mov     rdi, rbx
        call    f(int*)@PLT
        mov     rdi, rbx
        call    f(int*)@PLT
        mov     rdi, rbx
        call    f(int*)@PLT
        mov     rdi, rbx
        call    f(int*)@PLT
        mov     rdi, rbx
        call    f(int*)@PLT
        xor     eax, eax
        add     rsp, 16
        pop     rbx                          # extra, but just 1 byte
        ret


Technically this is more instructions.

But

mov rdi, rbx is 3 bytes, which is shorter than 5 bytes of lea. This is at minor expense of needing to save and restore rbx.

PS. Same happens when using temporary `int *const y = &x;`

Also same when optimizing for size (`-Os`).

It looks like gcc 4.8.5 produced expected code, but gcc 4.9.0 does not.

It is possible that the code produced by gcc 4.9.0 is faster, but it is also likely it contributes quite a bit to binary size.

clang uses CSE even if there are even just two uses of `&x` in the above example. It is likely a bit higher threshold is (3 or 4) is actually optimal (can be calculated knowing encoding sizes).


Weirdly tho, gcc -m32 does this:

g():
        push    ebp
        mov     ebp, esp
        push    ebx
        lea     ebx, [ebp-12]
        sub     esp, 32
        mov     DWORD PTR [ebp-12], 13
        push    ebx
        call    f(int*)
        mov     DWORD PTR [esp], ebx
        call    f(int*)
        mov     DWORD PTR [esp], ebx
        call    f(int*)
        mov     ebx, DWORD PTR [ebp-4]
        xor     eax, eax
        leave
        ret

Where, it does compute address and stores it in temporary. But does it on a stack, instead in a register (my guess is there are no free register to store it and it is spilled)., but in fact lea here would be likely faster (mov     DWORD PTR [esp], ebx, but requires memory/cache access, lea is 5 bytes, but does not require memory access)


---


### compiler : `gcc`
### title : `un-optimal vsetvl for multi-loop if avl is 0 ~ 31 immediate`
### open_at : `2023-01-03T01:51:48Z`
### last_modified_date : `2023-05-03T15:01:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108270
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Consider this following case:

#include "riscv_vector.h"
void f1 (void * restrict in, void * restrict out, int l, int n, int m)
{
  for (int i = 0; i < l; i++){
    for (int j = 0; j < m; j++){
      for (int k = 0; k < n; k++)
        {
          vint8mf8_t v = __riscv_vle8_v_i8mf8 (in + i + j, 17);
          __riscv_vse8_v_i8mf8 (out + i + j, v, 17);
        }
    }
  }
}

GCC ASM:
f1:
0:	mv	a7,a2
1:	mv	a6,a0
2:	mv	t1,a1
3:	mv	a2,a3
4:	vsetivli	zero,17,e8,mf8,ta,ma
5:	ble	a7,zero,.L1
6:	ble	a4,zero,.L1
7:	ble	a3,zero,.L1
8:	add	a1,a0,a4
9:	li	a0,0
10:.L4:
11:	add	a3,a6,a0
12:	add	a4,t1,a0
13:.L7:
14:	li	a5,0
15:.L5:
16:	vle8.v	v24,0(a3)
17:	addiw	a5,a5,1
18:	vse8.v	v24,0(a4)
19:	bne	a2,a5,.L5
20:	addi	a3,a3,1
21:	addi	a4,a4,1
22:	bne	a3,a1,.L7
23:	addi	a0,a0,1
24:	addi	a1,a1,1
25:	bne	a0,a7,.L4
26:.L1:
27:	ret

The vsetivli instruction is hoisted too early. The best location of vsetivli should be any point from 8 to 9.


---


### compiler : `gcc`
### title : `Missed RVV cost model`
### open_at : `2023-01-03T02:03:10Z`
### last_modified_date : `2023-08-25T07:43:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108271
### status : `UNCONFIRMED`
### tags : `internal-improvement, missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
#include "riscv_vector.h"

void f3 (int * restrict in, int * restrict out, void * restrict mask_in, int n)
{
  vfloat32mf2_t v = __riscv_vle32_v_f32mf2 ((float *)(in + 10000), 19);
  __riscv_vse32_v_f32mf2 ((float *)(out + 10000), v, 19);
  vbool64_t mask = *(vbool64_t*)mask_in;
  for (int i = 0; i < n; i++)
    {
      vint16mf2_t v1 = __riscv_vle16_v_i16mf2 ((int16_t *)(in + i + 1), 19);
      __riscv_vse16_v_i16mf2 ((int16_t *)(out + i + 1), v1, 19);

      vint32mf2_t v2 = __riscv_vle32_v_i32mf2 ((int32_t *)(in + i + 2), 19);
      __riscv_vse32_v_i32mf2 ((int32_t *)(out + i + 2), v2, 19);

      vint32mf2_t v3 = __riscv_vle32_v_i32mf2_tumu (mask, v2, (int32_t *)(in + i + 200), 13);
      *(vint32mf2_t*)(out + i + 200) = v3;

      vfloat64m1_t v4 = __riscv_vle64_v_f64m1_m (mask, (double *)(in + i + 300), 11);
      __riscv_vse64_v_f64m1 ((double *)(out + i + 300), v4, 11);

      vfloat64m1_t v5 = __riscv_vle64_v_f64m1_tum (mask, v4, (double *)(in + i + 500), 11);
      __riscv_vse64_v_f64m1 ((double *)(out + i + 500), v5, 11);

      vfloat64m1_t v6 = __riscv_vle64_v_f64m1_mu (mask, v5, (double *)(in + i + 600), 11);
      __riscv_vse64_v_f64m1_m (mask, (double *)(out + i + 600), v6, 11);

      vuint8mf4_t v7 = __riscv_vle8_v_u8mf4 ((uint8_t *)(in + i + 700), 11);
      __riscv_vse8_v_u8mf4 ((uint8_t *)(out + i + 700), v7, 11);
      
      vuint8mf4_t v8 = __riscv_vle8_v_u8mf4 ((uint8_t *)(in + i + 800), 11);
      __riscv_vse8_v_u8mf4 ((uint8_t *)(out + i + 800), v7, 11);
      
      vuint8mf4_t v9 = __riscv_vle8_v_u8mf4 ((uint8_t *)(in + i + 900), 11);
      __riscv_vse8_v_u8mf4 ((uint8_t *)(out + i + 900), v7, 11);
      
      vuint8mf4_t v10 = __riscv_vle8_v_u8mf4 ((uint8_t *)(in + i + 1000), 11);
      __riscv_vse8_v_u8mf4 ((uint8_t *)(out + i + 1000), v7, 11);
    }
}

-O3 -S ASM:
f3:
	li	a5,40960
	addi	a5,a5,-960
	addi	sp,sp,-64
	sd	s4,24(sp)
	add	a4,a0,a5
	add	a5,a1,a5
	vsetivli	zero,19,e32,mf2,ta,ma
	vle32.v	v24,0(a4)
	vse32.v	v24,0(a5)
	vsetvli	s4,zero,e8,mf8,ta,ma
	vlm.v	v0,0(a2)
	ble	a3,zero,.L1
	addi	a3,a3,1
	sd	s3,32(sp)
	slli	a3,a3,2
	li	s3,4096
	sd	s2,40(sp)
	sd	s5,16(sp)
	sd	s6,8(sp)
	addi	t6,s3,-1700
	addi	t5,s3,-1300
	addi	s6,s3,-900
	addi	s5,s3,-500
	sd	s0,56(sp)
	sd	s1,48(sp)
	addi	a0,a0,4
	addi	a4,a1,4
	add	s2,a1,a3
	addi	s3,s3,-100
.L3:
	vsetivli	zero,19,e16,mf2,ta,ma
	mv	a5,a4
	vle16.v	v24,0(a0)
	mv	a3,a0
	vse16.v	v24,0(a4)
	addi	a0,a0,4
	vsetivli	zero,19,e32,mf2,ta,ma
	addi	a4,a4,4
	vle32.v	v24,0(a0)
	addi	s1,a3,796
	vse32.v	v24,0(a4)
	vsetivli	zero,13,e32,mf2,tu,mu
	addi	s0,a5,796
	vle32.v	v24,0(s1),v0.t
	addi	a1,a3,1196
	addi	t4,a5,1196
	addi	t2,a3,1996
	addi	t3,a5,1996
	add	t0,a3,t6
	vsetvli	s4,zero,e32,mf2,ta,ma
	add	t1,a5,t6
	vse32.v	v24,0(s0)
	add	a7,a5,t5
	vsetivli	zero,11,e64,m1,tu,mu
	add	a6,a5,s6
	vle64.v	v24,0(a1),v0.t
	add	a2,a5,s5
	vse64.v	v24,0(t4)
	add	a3,a3,t5
	vle64.v	v24,0(t2),v0.t
	add	a5,a5,s3
	vse64.v	v24,0(t3)
	vle64.v	v24,0(t0),v0.t
	vse64.v	v24,0(t1),v0.t
	vsetivli	zero,11,e8,mf4,ta,ma
	vle8.v	v24,0(a3)
	vse8.v	v24,0(a7)
	vse8.v	v24,0(a6)
	vse8.v	v24,0(a2)
	vse8.v	v24,0(a5)
	bne	s2,a4,.L3
	ld	s0,56(sp)
	ld	s1,48(sp)
	ld	s2,40(sp)
	ld	s3,32(sp)
	ld	s5,16(sp)
	ld	s6,8(sp)
.L1:
	ld	s4,24(sp)
	addi	sp,sp,64
	jr	ra

GCC allocate redundant stack and generate a lot of redundant ld or sd instructions.

However, if we use -O3 -fno-schedule-insns ASM:
f3:
	li	a5,40960
	addi	a5,a5,-960
	add	a4,a0,a5
	add	a5,a1,a5
	vsetivli	zero,19,e32,mf2,ta,ma
	vle32.v	v24,0(a4)
	vse32.v	v24,0(a5)
	vsetvli	t3,zero,e8,mf8,ta,ma
	vlm.v	v0,0(a2)
	ble	a3,zero,.L1
	addi	a3,a3,1
	li	t1,4096
	slli	a3,a3,2
	addi	a4,a1,4
	addi	a7,t1,-1700
	addi	a6,t1,-1300
	addi	t5,t1,-900
	addi	t4,t1,-500
	addi	a2,a0,4
	add	a1,a1,a3
	addi	t1,t1,-100
.L3:
	mv	a3,a2
	vsetivli	zero,19,e16,mf2,ta,ma
	mv	a5,a4
	vle16.v	v24,0(a2)
	addi	a0,a3,796
	vse16.v	v24,0(a4)
	addi	a2,a2,4
	vsetivli	zero,19,e32,mf2,ta,ma
	addi	a4,a4,4
	vle32.v	v24,0(a2)
	vse32.v	v24,0(a4)
	vsetivli	zero,13,e32,mf2,tu,mu
	vle32.v	v24,0(a0),v0.t
	addi	a0,a5,796
	vsetvli	t3,zero,e32,mf2,ta,ma
	vse32.v	v24,0(a0)
	addi	a0,a3,1196
	vsetivli	zero,11,e64,m1,tu,mu
	vle64.v	v24,0(a0),v0.t
	addi	a0,a5,1196
	vse64.v	v24,0(a0)
	addi	a0,a3,1996
	vle64.v	v24,0(a0),v0.t
	addi	a0,a5,1996
	vse64.v	v24,0(a0)
	add	a0,a3,a7
	vle64.v	v24,0(a0),v0.t
	add	a3,a3,a6
	add	a0,a5,a7
	vse64.v	v24,0(a0),v0.t
	vsetivli	zero,11,e8,mf4,ta,ma
	vle8.v	v24,0(a3)
	add	a3,a5,a6
	vse8.v	v24,0(a3)
	add	a3,a5,t5
	vse8.v	v24,0(a3)
	add	a3,a5,t4
	add	a5,a5,t1
	vse8.v	v24,0(a3)
	vse8.v	v24,0(a5)
	bne	a1,a4,.L3
.L1:
	ret

This issue is gone. we should correctly adjust the RVV instruction COST model to make the codegen of with -fno-schedule-insns and without -fno-schedule-insns the same.


---


### compiler : `gcc`
### title : `Improved speed for float128 routines`
### open_at : `2023-01-03T20:55:32Z`
### last_modified_date : `2023-02-10T13:38:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108279
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libgcc`
### version : `unknown`
### severity : `enhancement`
### contents :
Our soft-float routines, which are used for the basic float128 arithmetic
(__addtf3, __subtf3, etc) are much slower than they need to be.

Michael S has some routines which are considerably faster, at
https://github.com/already5chosen/extfloat, which he would like to
contribute to gcc.  There is a rather lengthy thread in comp.arch
starting with https://groups.google.com/g/comp.arch/c/Izheu-k00Nw .

Current status of the discussion:

The routines currently do not support rounding modes, they support round to
nearest with tie even only. Adding such support would be feasible.

Handling the rounding mode it is currently done in libgcc, by
querying the hardware, leading to a high overhead for each
call. This would not be needed if -ffast-math (or a relevant
suboption) is specified.

It would also be suitable as is (with a different name) for Fortran
intrinsics such as matmul.

Fortran is a bit special because rounding modes are default on procedure
entry and are restored on procedure exit (which is why setting rounding
modes in a subroutine is a no-op). This would allow to keep a local
variable keeping track of the rounding mode.

The current idea would be something like this:

The current behavior of __addtf3 and friends could remain as is,
but its speed could be improved,. but it would still query the
hardware.

There can be two additional routines for each arithmetic operation. One
of them would implement the operation given a specified rounding mode
(to be called from Fortran when the correct IEEE module is in
use).

The other one would just implement round-to-nearest, for use from
Fortran intrinsics and from all other languages if the right flags
are given. It would be good to bolt this onto some flag which is
used for libgfortran, to make it accessible from C.

Probably gcc14 material.


---


### compiler : `gcc`
### title : `float value range estimation missing (vs. integer)`
### open_at : `2023-01-03T23:09:32Z`
### last_modified_date : `2023-01-09T18:49:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108281
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
"gcc -O3" and optional: "-funsafe-math-optimizations" (isnan)
GCC ignores ranges of float numbers for optimization, tested via https://godbolt.org/
For many frequent used functions (or even all in math.h) gcc could know a range limit and concidering it for comparisons.

#include <math.h>
int ranges(float x) {
    if (x!=x) return -1; // optional NAN check
    if (cos(x) < -1.0f) return -2;
    if (sin(x) >  1.0f) return -3;
    if (fabs(x) < 0.0f) return -4;
    if (atan(x) < -2.0f) return -5; // +-PI/2
    if (exp(x) < 0.0f) return -6;
    if (sqrt(x) < 0.0f) return -7;
    if (log(x)  < 90.0f) return -8; // ln(FLT_MAX)=88.8
    // ln(DBL_MAX) = 709.8
    return 0; // the only valid return (beside -1)
}
int sqr2(float x) { // squares give non-negative results
    return x*x < 0.0f; // == false
}
int ax2(float x) {
    return fabs(x) > -1.0f; // == true
}
int cmp_sqrt(float x, float y) { // similar (!sadly very often seen!)
    //x = fabs(x); y = fabs(y); // optional sign removal line
    return sqrtf(x) < sqrtf(y); // == (x < y), hint: sqrt=slow
}


---


### compiler : `gcc`
### title : `[12 Regression] false-positive -Warray-bounds warning emitted with -fsanitize=shift`
### open_at : `2023-01-05T20:46:18Z`
### last_modified_date : `2023-08-21T23:20:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108306
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54198
reduced PoC

This seems similar to bug #105679, but is still present in GCC 13 (and 12). Something about -fsanitize=shift really confuses -Warray-bounds:

poc2.c: In function 'e':
poc2.c:12:8: warning: array subscript 32 is above array bounds of 'int[4]' [-Warray-bounds=]
   12 |     c.d[f]++;
      |     ~~~^~~
poc2.c:4:7: note: while referencing 'd'
    4 |   int d[MAX];
      |       ^
poc2.c:12:8: warning: array subscript 32 is above array bounds of 'int[4]' [-Warray-bounds=]
   12 |     c.d[f]++;
      |     ~~~^~~
poc2.c:4:7: note: while referencing 'd'
    4 |   int d[MAX];
      |       ^


---


### compiler : `gcc`
### title : `Missing vector/array arithmetic optimization compared to valarray`
### open_at : `2023-01-06T21:44:18Z`
### last_modified_date : `2023-01-09T09:57:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108320
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `enhancement`
### contents :
The next code (with -O3 -mavx2 -mfma):

#include <valarray>
#include <vector>
#include <array>

using namespace std;

valarray<float> fma1(const valarray<float> &a, const valarray<float> &b, const valarray<float> &c) {
    return a * b + c;
}

template<class T>
struct vec : vector<T> {
    constexpr vec(size_t count) : vector<T>(count) {}
};

template<class T>
constexpr vec<T> operator*(const vec<T> &a, const vec<T> &b) {
    vec<T> c(a.size());
    for (size_t i = 0; i < c.size(); ++i) c[i] = a[i] * b[i];
    return c;
}

template<class T>
constexpr vec<T> operator+(const vec<T> &a, const vec<T> &b) {
    vec<T> c(a.size());
    for (size_t i = 0; i < c.size(); ++i) c[i] = a[i] + b[i];
    return c;
}

vec<float> fma2(const vec<float> &a, const vec<float> &b, const vec<float> &c) {
    return a * b + c;
}

template<class T, size_t N>
struct arr : array<T, N> {
};

template<class T, size_t N>
constexpr arr<T, N> operator*(const arr<T, N> &a, const arr<T, N> &b) {
    arr<T, N> c;
    for (size_t i = 0; i < c.size(); ++i) c[i] = a[i] * b[i];
    return c;
}

template<class T, size_t N>
constexpr arr<T, N> operator+(const arr<T, N> &a, const arr<T, N> &b) {
    arr<T, N> c;
    for (size_t i = 0; i < c.size(); ++i) c[i] = a[i] + b[i];
    return c;
}

constexpr size_t N = 1024;

arr<float, N> fma3(const arr<float, N> &a, const arr<float, N> &b, const arr<float, N> &c) {
    return a * b + c;
}

Only optimizes the valarray version (fma1) of the fma function (uses vfmadd132ps):

...

.L4:
        vmovups ymm0, YMMWORD PTR [rdi+rax]
        vmovups ymm1, YMMWORD PTR [rcx+rax]
        vfmadd132ps     ymm0, ymm1, YMMWORD PTR [rsi+rax]
        vmovups YMMWORD PTR [rdx+rax], ymm0
        add     rax, 32
        cmp     rax, r8
        jne     .L4
        mov     rax, r10
        and     rax, -8
        lea     r9, [0+rax*4]
        lea     r11, [rdx+r9]
        test    r10b, 7
        je      .L22
        vzeroupper
.L3:
        mov     r8, r10
        sub     r8, rax
        lea     r12, [r8-1]
        cmp     r12, 2
        jbe     .L6
        vmovups xmm0, XMMWORD PTR [rdi+rax*4]
        vmovups xmm2, XMMWORD PTR [rcx+rax*4]
        vfmadd132ps     xmm0, xmm2, XMMWORD PTR [rsi+rax*4]
        vmovups XMMWORD PTR [rdx+r9], xmm0
        test    r8b, 3
        je      .L1
        and     r8, -4
        add     rax, r8
        lea     r11, [r11+r8*4]
        lea     r9, [0+rax*4]

...

But it does not optimize the vector or array versions of the function (fma2 and fma3).

Note: For smaller N in fma3 optimizes, but for larger numbers like 1024 in the example it does not.

Compiler Explorer code: https://godbolt.org/z/v8dnx5aMo


---


### compiler : `gcc`
### title : `Using __restrict parameter with -ftree-vectorize (default with -O2) results in massive code bloat`
### open_at : `2023-01-06T23:27:11Z`
### last_modified_date : `2023-01-10T09:09:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108322
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.1`
### severity : `normal`
### contents :
While examining some code using the bloaty tool, I found that a function for deinterleaving Super Magic Drive ROM images was taking up ~5 KB when it should have been less than 1 KB. On examining the disassembly, there appeared to be a lot of unnecessary instructions; compiling with clang and MSVC resulted in significantly fewer instructions. Either removing __restrict from the function parameters (two pointers), or specifying -fno-tree-vectorize to disable auto-vectorization, fixes this issue with gcc-12.

The generated code isn't buggy as far as I can tell, and it benchmarks around the same as the non-bloated version.

I've narrowed it down to the following minimal test case:

#include <stdint.h>
#define SMD_BLOCK_SIZE 16384

void decodeBlock_cpp(uint8_t *__restrict pDest, const uint8_t *__restrict pSrc)
{
	// First 8 KB of the source block is ODD bytes.
	const uint8_t *pSrc_end = pSrc + (SMD_BLOCK_SIZE / 2);
	for (uint8_t *pDest_odd = pDest + 1; pSrc < pSrc_end; pDest_odd += 2, pSrc += 1) {
		pDest_odd[0] = pSrc[0];
	}
}

Assembly output with `g++ -O2 -fno-tree-vectorize` (or removing the __restrict qualifiers):

decodeBlock_cpp(unsigned char*, unsigned char const*):
        xor     eax, eax
.L2:
        movzx   edx, BYTE PTR [rsi+rax]
        mov     BYTE PTR [rdi+1+rax*2], dl
        add     rax, 1
        cmp     rax, 8192
        jne     .L2
        ret

Assembly output with `g++ -O2` (implying -ftree-vectorize with gcc-12) and __restrict qualifiers:

decodeBlock_cpp(unsigned char*, unsigned char const*):
        push    r15
        lea     rax, [rsi+8192]
        add     rdi, 1
        push    r14
        push    r13
        push    r12
        push    rbp
        push    rbx
        mov     QWORD PTR [rsp-8], rax
.L2:
        movzx   ecx, BYTE PTR [rsi+10]
        movzx   eax, BYTE PTR [rsi+14]
        add     rsi, 16
        add     rdi, 32
        movzx   edx, BYTE PTR [rsi-3]
        movzx   r15d, BYTE PTR [rsi-1]
        movzx   r11d, BYTE PTR [rsi-10]
        movzx   ebx, BYTE PTR [rsi-11]
        mov     BYTE PTR [rsp-11], cl
        movzx   ecx, BYTE PTR [rsi-16]
        movzx   ebp, BYTE PTR [rsi-12]
        mov     BYTE PTR [rsp-9], al
        movzx   r12d, BYTE PTR [rsi-13]
        movzx   eax, BYTE PTR [rsi-4]
        mov     BYTE PTR [rsp-10], dl
        movzx   r13d, BYTE PTR [rsi-14]
        movzx   edx, BYTE PTR [rsi-5]
        movzx   r14d, BYTE PTR [rsi-15]
        movzx   r8d, BYTE PTR [rsi-7]
        movzx   r9d, BYTE PTR [rsi-8]
        movzx   r10d, BYTE PTR [rsi-9]
        mov     BYTE PTR [rdi-32], cl
        movzx   ecx, BYTE PTR [rsp-11]
        mov     BYTE PTR [rdi-10], dl
        mov     BYTE PTR [rdi-30], r14b
        mov     BYTE PTR [rdi-28], r13b
        mov     BYTE PTR [rdi-26], r12b
        mov     BYTE PTR [rdi-24], bpl
        mov     BYTE PTR [rdi-22], bl
        mov     BYTE PTR [rdi-20], r11b
        mov     BYTE PTR [rdi-18], r10b
        mov     BYTE PTR [rdi-16], r9b
        mov     BYTE PTR [rdi-14], r8b
        mov     BYTE PTR [rdi-12], cl
        mov     BYTE PTR [rdi-8], al
        movzx   eax, BYTE PTR [rsp-9]
        movzx   edx, BYTE PTR [rsp-10]
        mov     BYTE PTR [rdi-2], r15b
        mov     BYTE PTR [rdi-4], al
        mov     rax, QWORD PTR [rsp-8]
        mov     BYTE PTR [rdi-6], dl
        cmp     rsi, rax
        jne     .L2
        pop     rbx
        pop     rbp
        pop     r12
        pop     r13
        pop     r14
        pop     r15
        ret

$ gcc --version
gcc (Gentoo Hardened 12.2.1_p20221008 p1) 12.2.1 20221008
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


---


### compiler : `gcc`
### title : `use mtvsrws for lowpart DI->SF conversion on P9`
### open_at : `2023-01-09T06:35:40Z`
### last_modified_date : `2023-10-07T08:03:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108338
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
In a mail-list discussion, https://gcc.gnu.org/pipermail/gcc-patches/2022-December/609054.html, as Segher points out, we could use 'mtvsrws' for the conversion from lowpart DI to SF on P9;  and use 'mtvsrd' for the conversion from highpart of DI to SF.

float sf_from_di_off0 (long l)
{
  char buff[16];
  *(long*)buff = l;
  float f = *(float*)(buff);
  return f;
}

float sf_from_di_off4 (long l)
{
  char buff[16];
  *(long*)buff = l;
  float f = *(float*)(buff + 4);
  return f;
}

With trunk, -O2 -mcpu=power9,  the asm code could be optimized.

        sldi 9,3,32
        mtvsrd 1,9
        xscvspdpn 1,1
        blr
==> mtvsrws;xscvspdpn  (p9 LE) lowpart


        srdi 3,3,32
        sldi 9,3,32
        mtvsrd 1,9
        xscvspdpn 1,1
        blr
==> (+ clean lowpart to 0) mtvsrd;xscvspdpn   highpart


---


### compiler : `gcc`
### title : `argument to `__builtin_ctz` should be assumed non-zero when  CTZ_DEFINED_VALUE_AT_ZERO says it is undefined`
### open_at : `2023-01-09T12:10:51Z`
### last_modified_date : `2023-07-05T19:02:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108341
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `enhancement`
### contents :
Godbolt: https://gcc.godbolt.org/z/PrPP4v9z1


```
extern int r;

int
bz(int value)
  {
    r = __builtin_ctz(value);
    return value != 0;  // always true
  }
```


According to GCC manual, if the argument to `__builtin_ctz()` is zero then the behavior is undefined, but GCC fails to assume that this function always returns `1`. 

But I have read https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94801, not sure whether it's related.


---


### compiler : `gcc`
### title : `gather/scatter loops optimized too often for znver4 (and other zens)`
### open_at : `2023-01-09T19:42:36Z`
### last_modified_date : `2023-01-17T07:07:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108346
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
The following two benchmarks tests gather/scatter codegen:
s4113.c:

#include <math.h>
#include <malloc.h>

//typedef float real_t;
#define iterations 1000000
#define LEN_1D 32000
#define LEN_2D 256
real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D];
real_t aa[LEN_2D][LEN_2D];
real_t bb[LEN_2D][LEN_2D];
real_t cc[LEN_2D][LEN_2D];
real_t qq;
int
main(void)
{
//    reductions
//    if to max reduction

    real_t x;
    int * __restrict__ ip = (int *) malloc(LEN_1D*sizeof(real_t));

    for (int i = 0; i < LEN_1D; i = i+5){
        (ip)[i]   = (i+4);
        (ip)[i+1] = (i+2);
        (ip)[i+2] = (i);
        (ip)[i+3] = (i+3);
        (ip)[i+4] = (i+1);
    }
    for (int nl = 0; nl < 2*iterations; nl++) {
        for (int i = 1; i < LEN_1D; i += 2) {
            a[ip[i]] = b[ip[i]] + c[i];
        }
        asm("":::"memory");
    }

    return x;
}


s4115.c:
#include <math.h>
#include <malloc.h>

#define iterations 1000000
#define LEN_1D 32000
#define LEN_2D 256
real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D];
real_t aa[LEN_2D][LEN_2D];
real_t bb[LEN_2D][LEN_2D];
real_t cc[LEN_2D][LEN_2D];
real_t qq;
int
main(void)
{
//    reductions
//    if to max reduction

    real_t x;
    int * __restrict__ ip = (int *) malloc(LEN_1D*sizeof(real_t));

    for (int i = 0; i < LEN_1D; i = i+5){
        (ip)[i]   = (i+4);
        (ip)[i+1] = (i+2);
        (ip)[i+2] = (i);
        (ip)[i+3] = (i+3);
        (ip)[i+4] = (i+1);
    }
    for (int nl = 0; nl < 2*iterations; nl++) {
        for (int i = 1; i < LEN_1D; i += 2) {
            x += a[i] * b[ip[i]];
        }
        asm("":::"memory");
    }

    return x;
}

On zver4 I get following times with disabling/enabling vectorization and disabling/enabling gather&scatter use:

                                         runtime
type      optimization    operation  scalar nogather gather parts instruction
char      avx256_optimal  load+store 14.23  N/A      N/A
char      avx256_optimal  load       14.25  N/A      N/A
char      ^avx256_optimal load+store 14.02  N/A      N/A
char      ^avx256_optimal load       14.25  N/A      N/A
short     avx256_optimal  load+store*14.23  N/A      N/A
short     avx256_optimal  load      *14.23  N/A      N/A
short     ^avx256_optimal load+store 15.22  N/A      N/A
short     ^avx256_optimal load       14.23  N/A      N/A
int       avx256_optimal  load+store*16.51  27.66    25.96  8     vpgatherdd ymm,vpscatterdd ymm
int       avx256_optimal  load       14.13  13.17   *12.71  8     vpgatherdd ymm
int       ^avx256_optimal load+store*16.57  33.25    26.06  16    vpgatherdd zmm,vpscatterdd zmm
int       ^avx256_optimal load       14.14  16.81   *13.63  16    vpgatherdd zmm
long      avx256_optimal  load+store*20.59  20.66    32.03  4     vpgatherdq zmm,vpscatterdq zmm
long      avx256_optimal  load       15.36 *15.36    15.82  4     vpgatherdq zmm
long      ^avx256_optimal load+store 22.42 *20.96    30.54  8     vpgatherdq zmm,vpscatterdq zmm
long      ^avx256_optimal load      *15.87  16.40    18.68  8     vpgatherdq zmm
float     avx256_optimal  load+store 16.88  27.78    26.08  8     vgatherdps ymm, vscatterdps ymm
float     avx256_optimal  load       26.01 *13.19    13.30  8     vgatherdps ymm
float     ^avx256_optimal load+store*16.89  33.22    26.19  16    vgatherdps zmm, vscatterdps zmm
float     ^avx256_optimal load       26.01  16.61   *13.85  16    vgatherdps zmm
double    avx256_optimal  load+store 21.94 *20.81    31.43  4     vgatherdpd ymm, vscatterdpd ymm
double    avx256_optimal  load       26.01  26.01   *15.20  4     vgatherdpd ymm
double    ^avx256_optimal load+store 21.44 *21.65    30.73  8     vgatherdpd zmm, vscatterdpd zmm
double    ^avx256_optimal load       26.01  26.01   *18.24  8     vgatherdpd zmm


We incorrectly vectorize for int load+store loop causing 60% regression.
Vectorizing avx512 long load loop seems to be also slight loss, but not that important.  I will post patch todisable scatter instructions since they does not seem to be win.


---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -O3 since r13-4240-gfeeb0d68f1c708`
### open_at : `2023-01-10T11:46:47Z`
### last_modified_date : `2023-08-25T23:38:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108351
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54226
case file

cat case.c #51
struct a {
  int b;
} b;
static short c = 7, d, e, a, k;
int g;
struct a h;
int i;
int j;
void foo();
static struct a f(int l, unsigned short m) {
  a = e;
  k = d = j < i;
  if (m)
    return h;
  foo();
  return b;
}
int main() {
  for (; g;)
    f(1, c);
  f(7, 7);
  f(9, 7);
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O3` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O3` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O3 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
f.isra.0:
.LFB3:
        .cfi_startproc
        testw   %di, %di
        je      .L4
        ret
        .p2align 4,,10
        .p2align 3
.L4:
        xorl    %eax, %eax
        jmp     foo
        .cfi_endproc
.LFE3:
        .size   f.isra.0, .-f.isra.0
        .section        .text.startup,"ax",@progbits
        .p2align 4
        .globl  main
        .type   main, @function
main:
.LFB1:
        .cfi_startproc
        movl    g(%rip), %eax
        testl   %eax, %eax
        je      .L6
.L7:
        jmp     .L7
        .p2align 4,,10
        .p2align 3
.L6:
        subq    $8, %rsp
        .cfi_def_cfa_offset 16
        movl    $7, %edi
        call    f.isra.0
        movl    $7, %edi
        call    f.isra.0
        xorl    %eax, %eax
        addq    $8, %rsp
        .cfi_def_cfa_offset 8
        ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O3 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movl	g(%rip), %eax
	testl	%eax, %eax
	je	.L4
.L5:
	jmp	.L5
	.p2align 4,,10
	.p2align 3
.L4:
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=feeb0d68f1c7085199c3734e6517a3a4b58309ef


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -O2 since r13-1960-gd86d81a449c036`
### open_at : `2023-01-10T12:21:55Z`
### last_modified_date : `2023-05-24T17:30:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108352
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54227
Code as file

cat case.c #30
long a;
int b;
void bar64_(void);
void foo();
int main() {
  char c = 0;
  unsigned d = 10;
  int e = 2;
  for (; d; d--) {
    bar64_();
    b = d;
    e && (c = (e = 0) != 4) > 1;
  }
  if (c < 1)
    foo();
  a = b;
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O2` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	movl	$1, %r13d
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	movl	$2, %r12d
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	xorl	%ebp, %ebp
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	movl	$10, %ebx
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	.p2align 4,,10
	.p2align 3
.L3:
	call	bar64_
	testl	%r12d, %r12d
	movl	%ebx, b(%rip)
	cmovne	%r13d, %ebp
	xorl	%r12d, %r12d
	subl	$1, %ebx
	jne	.L3
	movl	$1, %eax
	testb	%bpl, %bpl
	je	.L10
.L4:
	movq	%rax, a(%rip)
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 40
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
.L10:
	.cfi_restore_state
	xorl	%eax, %eax
	call	foo
	movslq	b(%rip), %rax
	jmp	.L4
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$10, %ebx
	call	bar64_
	movl	$10, %eax
	jmp	.L3
	.p2align 4,,10
	.p2align 3
.L6:
	call	bar64_
	movl	%ebx, %eax
.L3:
	movl	%eax, b(%rip)
	subl	$1, %ebx
	jne	.L6
	movq	$1, a(%rip)
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=d86d81a449c03641e079f23a2b3e1b2279a162fe


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -O2 since r13-3898-gaf96500eea72c6`
### open_at : `2023-01-10T12:32:10Z`
### last_modified_date : `2023-01-11T10:53:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108353
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54228
presented code as file

cat case.c #99
enum { a } b();
int d;
int e;
int f;
void foo();
void bar49_(void);
void(c)();
static short g(int h, int i) {
  int j = 2874288693, k = 1;
  if (h)
    for (; j; j = j + 9) {
      f = 0;
      for (; f <= 1; c())
        k = 90;
    }
  i = k;
  for (; e; ++e) {
    if (i)
      continue;
    foo();
    i = b();
  }
  return 4;
}
int l() {
  bar49_();
  return 1;
}
int main() { d = d || g(d, l()); }

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O2` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movl	d(%rip), %eax
	testl	%eax, %eax
	je	.L20
	movl	$1, d(%rip)
	xorl	%eax, %eax
	ret
.L20:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	call	bar49_
	cmpl	$0, d(%rip)
	jne	.L12
	movl	$1, %eax
.L9:
	cmpl	$0, e(%rip)
	je	.L5
.L7:
	addl	$1, e(%rip)
	je	.L5
.L11:
	testl	%eax, %eax
	jne	.L7
	call	foo
	xorl	%eax, %eax
	call	b
	addl	$1, e(%rip)
	jne	.L11
.L5:
	movl	$1, d(%rip)
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L12:
	.cfi_restore_state
	movl	$1112290355, %ebx
	.p2align 4,,10
	.p2align 3
.L6:
	movl	$0, f(%rip)
	.p2align 4,,10
	.p2align 3
.L8:
	xorl	%eax, %eax
	call	c
	cmpl	$1, f(%rip)
	jle	.L8
	subl	$1, %ebx
	jne	.L6
	movl	$90, %eax
	jmp	.L9
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movl	d(%rip), %eax
	testl	%eax, %eax
	je	.L19
	movl	$1, d(%rip)
	xorl	%eax, %eax
	ret
.L19:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	call	bar49_
	cmpl	$0, d(%rip)
	je	.L11
	movl	$1112290355, %ebx
	.p2align 4,,10
	.p2align 3
.L7:
	movl	$0, f(%rip)
	.p2align 4,,10
	.p2align 3
.L10:
	xorl	%eax, %eax
	call	c
	cmpl	$1, f(%rip)
	jle	.L10
	subl	$1, %ebx
	jne	.L7
.L11:
	cmpl	$0, e(%rip)
	je	.L6
	movl	$0, e(%rip)
.L6:
	movl	$1, d(%rip)
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=af96500eea72c674a5686b35c66202ef2bd9688f


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -O2 since r13-89-gb3e98eb3396e16`
### open_at : `2023-01-10T12:41:27Z`
### last_modified_date : `2023-08-21T21:40:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108354
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54229
presented code as file

cat case.c #617
int b;
int *c;
int e;
static int *f = &e;
int g;
void foo();
short(a)(short h, short i) { return h - i; }
int(d)(int h) { return h == 83647 ? 0 : -h; }
int main() {
  short j;
  int *k = &e, *l = &b;
  *f = 0 == c;
  j = a(0 != 2, *k);
  if (d(j ^ (0 == l || *k)) != *k)
    ;
  else
    foo();
  c = &g;
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O2` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	xorl	%eax, %eax
	cmpq	$0, c(%rip)
	setne	%dl
	sete	%al
	movzbl	%dl, %edx
	movl	%eax, e(%rip)
	orl	%eax, %edx
	je	.L12
	movq	$g, c(%rip)
	xorl	%eax, %eax
	ret
.L12:
	pushq	%rax
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	call	foo
	xorl	%eax, %eax
	movq	$g, c(%rip)
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	xorl	%eax, %eax
	cmpq	$0, c(%rip)
	movq	$g, c(%rip)
	sete	%al
	movl	%eax, e(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=b3e98eb3396e16ae8b20c94916bc2bd7862d2c97


---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -O2 since r13-2772-g9baee6181b4e42`
### open_at : `2023-01-10T12:46:29Z`
### last_modified_date : `2023-08-21T21:40:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108355
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54230
presented code as file

cat case.c #551
int a;
int *b = &a;
int **c = &b;
int d;
void bar25_(void);
void foo(void);
int main() {
  int e[][1] = {0, 0, 0, 0, 0, 1};
  for (;;) {
    bar25_();
    if (e[5][a])
      break;
    e[a][0] = 0;
    foo();
  }
  *c = &d;
}
`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O2` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movl	$1, 20(%rsp)
	jmp	.L3
	.p2align 4,,10
	.p2align 3
.L6:
	movl	$0, (%rsp,%rax,4)
	call	foo
.L3:
	call	bar25_
	movslq	a(%rip), %rax
	movl	20(%rsp,%rax,4), %edx
	testl	%edx, %edx
	je	.L6
	movq	c(%rip), %rax
	movq	$d, (%rax)
	xorl	%eax, %eax
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	call	bar25_
	movq	c(%rip), %rax
	movq	$d, (%rax)
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=9baee6181b4e427e0b5ba417e51424c15858dce7


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -O2 since r13-434-g6b156044c12bc4`
### open_at : `2023-01-10T12:50:54Z`
### last_modified_date : `2023-02-01T14:27:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108356
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54231
presented code as file

cat case.c #93
char a;
static char c = 3;
char d;
void foo();
short(b)(short e, short f) { return e + f; }
int main() {
  unsigned g = 0;
  if (c)
    ;
  else
    foo();
  for (; g < 2; g = b(g, 2)) {
    d = g ? 0 : a;
    if (g)
      c = 0;
  }
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O2` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	cmpb	$0, c(%rip)
	je	.L6
.L4:
	movzbl	a(%rip), %eax
	movb	%al, d(%rip)
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L6:
	.cfi_restore_state
	xorl	%eax, %eax
	call	foo
	jmp	.L4
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movzbl	a(%rip), %eax
	movb	%al, d(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=6b156044c12bc4582511fe270b10450c943476dd


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -O2 since r13-4607-g2dc5d6b1e7ec88`
### open_at : `2023-01-10T12:59:14Z`
### last_modified_date : `2023-04-14T11:35:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108357
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54232
presented code as file

cat case.c #411
static char b;
static unsigned c;
void foo();
short(a)(short d, short e) { return d * e; }
static short f(short d) {
  b = 0;
  if ((d && 0 >= c < d) ^ d)
    ;
  else
    foo();
  return d;
}
int main() {
  short g = a(5, b ^ 9854);
  f(g);
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O2` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movsbw	b(%rip), %ax
	movb	$0, b(%rip)
	xorw	$9854, %ax
	leal	(%rax,%rax,4), %eax
	cmpw	$1, %ax
	movswl	%ax, %edx
	setg	%al
	movzbl	%al, %eax
	cmpl	%eax, %edx
	je	.L9
	xorl	%eax, %eax
	ret
.L9:
	pushq	%rax
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movb	$0, b(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=2dc5d6b1e7ec8822f5bd78761962ca2c85d4a2b4


---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -Os since r13-3378-gf6c168f8c06047`
### open_at : `2023-01-10T13:03:03Z`
### last_modified_date : `2023-08-25T23:39:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108358
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54233
presented code as file

cat case.c #25
struct a {
  int b;
  int c;
  short d;
  int e;
  int f;
};
struct g {
  struct a f;
  struct a h;
};
int i;
void foo();
void bar31_(void);
int main() {
  struct g j, l = {2, 1, 6, 1, 1, 7, 5, 1, 0, 1};
  for (; i; ++i)
    bar31_();
  j = l;
  struct g m = j;
  struct g k = m;
  if (k.h.b)
    ;
  else
    foo();
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -Os` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -Os` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$.LC0, %esi
	movl	$10, %ecx
	subq	$80, %rsp
	.cfi_def_cfa_offset 96
	leaq	40(%rsp), %rdi
	leaq	40(%rsp), %rbx
	rep movsl
.L2:
	cmpl	$0, i(%rip)
	je	.L7
	call	bar31_
	incl	i(%rip)
	jmp	.L2
.L7:
	movq	%rsp, %rdi
	movl	$10, %ecx
	movq	%rbx, %rsi
	rep movsl
	cmpl	$0, 20(%rsp)
	jne	.L4
	xorl	%eax, %eax
	call	foo
.L4:
	addq	$80, %rsp
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	cmpl	$0, i(%rip)
	jne	.L7
	xorl	%eax, %eax
	ret
.L7:
	pushq	%rax
	.cfi_def_cfa_offset 16
.L3:
	call	bar31_
	incl	i(%rip)
	cmpl	$0, i(%rip)
	jne	.L3
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=f6c168f8c06047bfaa3005e570126831b8855dcc


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -Os since r13-1162-g9991d84d2a8435`
### open_at : `2023-01-10T13:07:47Z`
### last_modified_date : `2023-01-31T14:58:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108359
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54234
presented code as file

cat case.c #864
int b = 10;
int c;
char e;
void foo();
static char(a)(char f, char g) { return f && g == 1 ? 0 : f % g; }
short(d)(short f, short g) { return f * g; }
int main() {
  short h;
  int i;
  unsigned j;
  h = d(b && c, 5);
  j = h;
  i = a(h, 237);
  unsigned k = i;
  e = i < 0 || k >= 32 ? 0 : i >> k;
  if (e) {
    c = 0;
    foo();
  }
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -Os` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -Os` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movl	b(%rip), %eax
	testl	%eax, %eax
	je	.L3
	xorl	%eax, %eax
	cmpl	$0, c(%rip)
	setne	%al
.L3:
	movb	$5, %dl
	imull	%edx, %eax
	movsbl	%al, %edx
	movl	%eax, %ecx
	sarl	%cl, %edx
	movb	%dl, e(%rip)
	testl	%edx, %edx
	je	.L12
	pushq	%rax
	.cfi_def_cfa_offset 16
	xorl	%edx, %edx
	xorl	%eax, %eax
	movl	%edx, c(%rip)
	call	foo
	xorl	%eax, %eax
	popq	%rcx
	.cfi_def_cfa_offset 8
	ret
.L12:
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movb	$0, e(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=9991d84d2a84355fd3fc9afc89a963f45991bfa9


---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -Os since r13-2048-g418b71c0d535bf`
### open_at : `2023-01-10T13:10:43Z`
### last_modified_date : `2023-08-09T01:25:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108360
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54235
case.c as file

cat case.c #321
static short b, c;
unsigned char e;
char f;
void foo();
short(a)(short h, short i) { return h + i; }
static short(d)(short h, int i) { return h >= 32 || h > 7 >> i ? h : h << i; }
unsigned g(short h, short i) { return h + i; }
int main() {
  short j, k = -1;
  c = a(0 >= b, k);
  f = c <= 0xD315BEF1;
  e = f << 4;
  j = d(e, 5);
  if (3 >= g(j == b, k))
    foo();
  b = 0;
}

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -Os` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -Os` can.

`gcc-cb93c5f8008b95743b741d6f1842f9be50c6985c (trunk) -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB3:
	.cfi_startproc
	movw	b(%rip), %cx
	xorl	%edx, %edx
	testw	%cx, %cx
	setle	%dl
	movl	%edx, %eax
	movb	%dl, f(%rip)
	sall	$4, %eax
	movb	%al, e(%rip)
	movzbl	%al, %eax
	testw	%dx, %dx
	jne	.L4
	sall	$5, %eax
.L4:
	cmpw	%ax, %cx
	jne	.L7
	pushq	%rax
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	call	foo
	xorl	%eax, %eax
	movw	$0, b(%rip)
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
.L7:
	movw	$0, b(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB3:
	.cfi_startproc
	xorl	%eax, %eax
	cmpw	$0, b(%rip)
	movw	$0, b(%rip)
	setle	%al
	movb	%al, f(%rip)
	sall	$4, %eax
	movb	%al, e(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=418b71c0d535bf91df78bad2e198c57934682eaa


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Spurious stringop overflow, possibly alias-related since r12-145-gd1d01a66012a93cc`
### open_at : `2023-01-11T00:18:16Z`
### last_modified_date : `2023-05-08T12:26:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108366
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Created attachment 54241
Minimal C++ file that reproduces the bug

This seems similar but different from #107852.

Starting with gcc 12 and continuing into the current trunk (git aa966d54ee4), the compiler emits a stringop overflow warning.

In addition to the attachment: https://godbolt.org/z/3xorbqsha

I would expect either of the following behaviors:
- In line 20 (marked with "// !!!"), the compiler sees that even if `new` returned a nullptr for any reason, if execution continues in the make_vector() function, it can now assume that new_buffer is non-null, and therefore memset is safe.
- Alternatively, if the compiler cannot deduce that, then the warning should not be generated, since the compiler cannot now what pointer will be returned by data().

gcc trunk (git aa966d54ee4) → spurious warning
gcc 12.2.0 (Debian 12.2.0-13) → spurious warning
gcc 11.3.0 (Debian 11.3.0-10) → no warning
gcc 10.4.0 (Debian 10.4.0-6) → no warning
clang 13 and 14 → no warning

system type: Debian testing, x86_64
options during configure:
Debian gcc reports: ../src/configure -v --with-pkgversion='Debian 12.2.0-13' --with-bugurl=file:///usr/share/doc/gcc-12/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-12 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --enable-cet --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-12-PBog5r/gcc-12-12.2.0/debian/tmp-nvptx/usr,amdgcn-amdhsa=/build/gcc-12-PBog5r/gcc-12-12.2.0/debian/tmp-gcn/usr --enable-offload-defaulted --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
When I manually built it I used this:
../gcc-src/configure --enable-shared --disable-werror --enable-languages=c,c++ --enable-default-pie --enable-lto --enable-threads=posix --enable-initfini-array --with-linker-hash-style=gnu --disable-multilib
complete commandline that triggers the bug: g++ -O2 -o foo repro.cpp

compiler error message:
```
In function ‘Vector make_vector()’,
    inlined from ‘int main()’ at repro.cpp:36:31:
repro.cpp:27:11: warning: ‘void* memset(void*, int, size_t)’ writing 128 bytes into a region of size 40 overflows the destination [-Wstringop-overflow=]
   27 |     memset(buffer.data(), 'A', new_size);
      |     ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
repro.cpp: In function ‘int main()’:
repro.cpp:36:10: note: destination object ‘actual’ of size 40
   36 |     auto actual = make_vector();
      |          ^~~~~~
```

preprocessed file (*.i*) that triggers the bug: See next comments, I can't figure out how to put more than one attachment in a single comment.


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -O3 since r13-1759-gdbb093f4f15`
### open_at : `2023-01-11T13:16:01Z`
### last_modified_date : `2023-02-21T13:14:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108368
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54244
Case file

cat case.c #1409
int a;
int b;
short c;
static int d = 4;
void foo();
void e(int f) {}
int main() {
  unsigned g = -27;
  for (; g > 7; ++g) {
    e(c && g);
    b ? 0 : a;
    if (g) {
      if (0 >= d)
        foo();
    } else
      d = 0;
  }
}

`gcc-f99d7d669eaa2830eb5878df4da67e77ec791522 (trunk) -O3` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O3` can.

`gcc-f99d7d669eaa2830eb5878df4da67e77ec791522 (trunk) -O3 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$27, %ebx
.L5:
	movl	d(%rip), %eax
	testl	%eax, %eax
	jle	.L8
.L4:
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L8:
	.cfi_restore_state
	xorl	%eax, %eax
	call	foo
	subl	$1, %ebx
	jne	.L5
	jmp	.L4
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O3 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: r13-1759-gdbb093f4f15

commit dbb093f4f15ea66f2ce5cd2dc1903a6894563356
Author: Andrew MacLeod <amacleod@redhat.com>
Date:   Mon Jul 18 15:04:23 2022 -0400

    Resolve complicated join nodes in range_from_dom.
    
    Join nodes which carry outgoing ranges on incoming edges are uncommon,
    but can still be resolved by setting the dominator range, and then
    calculating incoming edges.  Avoid doing so if one of the incoing edges
    is not dominated by the same dominator.
    
            * gimple-range-cache.cc (ranger_cache::range_from_dom): Check
              for incoming ranges on join nodes and add to worklist.


---


### compiler : `gcc`
### title : `gcc doesn't merge bitwise-AND if an explicit comparison against 0 is given`
### open_at : `2023-01-11T13:47:36Z`
### last_modified_date : `2023-09-21T14:16:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108370
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.2.1`
### severity : `enhancement`
### contents :
Created attachment 54245
Demo code

If gcc sees a couple of calls to an inline function that does a bitwise-AND and returns whether the result was zero or non-zero (e.g. a flag check helper), gcc cannot merge them if the result of the AND is explicitly compared against 0, even if the function's return type is a bool (which would do that anyway).  For example:

   static inline bool bio_flagged(struct bio *bio, unsigned int bit)
   {
	return (bio->bi_flags & (1U << bit)) != 0;
   }

   void bio_release_pages(struct bio *bio, bool mark_dirty)
   {
	if (bio_flagged(bio, BIO_PAGE_REFFED) ||
	    bio_flagged(bio, BIO_PAGE_PINNED))
		__bio_release_pages(bio, mark_dirty);
   }

compiles bio_release_pages() to:

   0:   66 8b 07                mov    (%rdi),%ax
   3:   a8 01                   test   $0x1,%al
   5:   75 04                   jne    b <bio_release_pages+0xb>
   7:   a8 02                   test   $0x2,%al
   9:   74 09                   je     14 <bio_release_pages+0x14>
   b:   40 0f b6 f6             movzbl %sil,%esi
   f:   e9 00 00 00 00          jmp    14 <bio_release_pages+0x14>
  14:   c3                      ret    

but:

   static inline bool bio_flagged(struct bio *bio, unsigned int bit)
   {
	return bio->bi_flags & (1U << bit);
   }

gives:

   0:   f6 07 03                testb  $0x3,(%rdi)
   3:   74 09                   je     e <bio_release_pages+0xe>
   5:   40 0f b6 f6             movzbl %sil,%esi
   9:   e9 00 00 00 00          jmp    e <bio_release_pages+0xe>
   e:   c3                      ret    

Possibly the comparison against 0 could be optimised away.

I've attached some demo code that can be compiled with one of:

gcc -Os -c gcc-bool-demo.c
gcc -Os -c gcc-bool-demo.c -Dfix

The gcc I used above is the Fedora 37 system compiler:

gcc-12.2.1-4.fc37.x86_64
binutils-2.38-25.fc37.x86_64

but similar results can be seen with the Fedora arm cross-compiler:

   0:   e1d030b0        ldrh    r3, [r0]
   4:   e3130001        tst     r3, #1
   8:   1a000001        bne     14 <bio_release_pages+0x14>
   c:   e3130002        tst     r3, #2
  10:   012fff1e        bxeq    lr
  14:   eafffffe        b       0 <__bio_release_pages>

vs

   0:   e1d030b0        ldrh    r3, [r0]
   4:   e3130003        tst     r3, #3
   8:   012fff1e        bxeq    lr
   c:   eafffffe        b       0 <__bio_release_pages>


---


### compiler : `gcc`
### title : `gcc for x86_64 may sign/zero extent arguments unnecessarily`
### open_at : `2023-01-11T14:04:40Z`
### last_modified_date : `2023-01-11T14:28:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108371
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.1`
### severity : `normal`
### contents :
When compiling for x86_64, bool, char and short arguments that are passed directly to an argument of exactly the same type on another function with no modification, e.g.:

   void __bio_release_pages(char mark_dirty);
   void bio_release_pages(char mark_dirty)
   {
	__bio_release_pages(mark_dirty);
   }

get sign/zero-extended unnecessarily.  In the case of the above code, it compiles to:

   0:   40 0f be ff             movsbl %dil,%edi
   4:   e9 00 00 00 00          jmp    9 <bio_release_pages+0x9>

with "gcc -Os -c test.c".  Can the extension be optimised away?  Granted, the upper bits bits of RDI could contain rubbish on entry to bio_release_pages(), so sanitisation is not unreasonable - but on the other hand, __bio_release_pages() would surely have to assume the same and do the same sanitisation?

The toolchain used is the Fedora 37 system compiler:

gcc-12.2.1-4.fc37.x86_64
binutils-2.38-25.fc37.x86_64


---


### compiler : `gcc`
### title : `[12 Regression] false positive -Wfree-nonheap-object`
### open_at : `2023-01-12T15:27:53Z`
### last_modified_date : `2023-10-10T01:22:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108385
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Sorry I was not able to reduce this further. Changing almost anything makes the bug no-longer reproduce:


```

#include <vector>
#include <cstdint>
#include <cstring>

class DataType {
 public:
  DataType() {
   }

  DataType get() const;

 private:
  double v = 0.0;
  char values[41];
};

class ptrType {
 public:
  DataType someMethod() const {
    DataType t;
    t = t.get();
    return t;
  }
};

class AnotherDataType {
 public:
  typedef uint32_t size_type;

  AnotherDataType() : _size(0), _data(new double[0]) {}

  explicit AnotherDataType(size_type size) : _size(size), _data(new double[size]) {}

  virtual ~AnotherDataType() { delete[] _data; }

  uint32_t size() const { return _size; }

  double& operator()(size_type i) { return _data[i]; }

  AnotherDataType get(const AnotherDataType& b) const
  {
    AnotherDataType c(size());

    auto aItr = _data;
    auto cItr = c.begin();
    auto endp = _data + _size;

    for (; aItr != endp; ++aItr, ++cItr) {
      (*cItr) = (*aItr);
    }
    return c;
  }

  double sum() const {
    double sum = *_data;
    auto aItr = _data;
    for (; aItr != _data + _size; ++aItr) {
      sum = (*aItr);
    }
    return sum;
  }

  double* begin() { return _data; }

 private:
  size_type _size;
  double* _data;
};

AnotherDataType anotherMethod(const ptrType* ptrType1) {
  ptrType1->someMethod();
  return {};
}

struct otherStruct {
  const ptrType* ptrType1;
  std::vector<double> q1;
};

static double minF(otherStruct* params) {
  auto err = anotherMethod(params->ptrType1);

  return (err.get(err)).sum();
}

struct someStruct {
  double (*f)(otherStruct* params);
  otherStruct* params;
};

void foo(someStruct function) {
  std::vector<double> v;

  minF(function.params);
}

void why() {
  someStruct func;
  func.f = &minF;
  foo(func);
}
```

Godbolt link: https://godbolt.org/z/nqvsezj49


---


### compiler : `gcc`
### title : `Missed optimization with -fno-omit-frame-pointer on x86`
### open_at : `2023-01-12T17:25:36Z`
### last_modified_date : `2023-01-13T07:52:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108386
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
void bar (char *);

void
foo (void)
{
  char buf[384];
  bar (&buf[0]);
  bar (&buf[16]);
  bar (&buf[127]);
  bar (&buf[128]);
  bar (&buf[256]);
  bar (&buf[380]);
  bar (&buf[384]);
}

compiles with -O2 -fno-omit-frame-pointer to:
        pushq   %rbp
        movq    %rsp, %rbp
        subq    $384, %rsp
        leaq    -384(%rbp), %rdi
        call    bar
        leaq    -368(%rbp), %rdi
        call    bar
        leaq    -257(%rbp), %rdi
        call    bar
        leaq    -256(%rbp), %rdi
        call    bar
        leaq    -128(%rbp), %rdi
        call    bar
        leaq    -4(%rbp), %rdi
        call    bar
        movq    %rbp, %rdi
        call    bar
        leave
        ret
but this is unnecessarily large.  As frame pointer is here only because user asked for it, the compiler knows there is always constant difference between the stack pointer and frame pointer and perhaps in machine reorg could interchange those cases which would be smaller and not slower.
For these particular leaq/movq instructions, movq %r{sp,bp}, %rdi is 3 bytes,
leaq SIMM8(%rbp), %rdi 4 bytes, leaq SIMM8(%rsp), %rdi 5 bytes, leaq SIMM32(%rbp), %rdi 7 bytes and leaq SIMM32(%rsp), %rdi 8 bytes.
So at least from code size POV, movq %rsp, %rdi is smaller than any %rbp based leaq,
and similarly leaq SIMM8(%rbp), %rdi 2 bytes smaller than leaq SIMM32(%rbp), %rdi.
So, the mov would be a win always, and for frame sizes of more than 128 bytes %rsp up to 127 offset too.
Though, I think the aliasing code hardcodes frame pointer knowledge and ditto I think
the unwinder would be quite upset if we did such changes early.


---


### compiler : `gcc`
### title : `Missed optimization with [0, 0][-1U,-1U] range arithmetics`
### open_at : `2023-01-13T16:52:09Z`
### last_modified_date : `2023-08-31T19:53:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108397
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
+++ This bug was initially created as a clone of Bug #107131 +++

__attribute__((noipa)) unsigned long long                                                                                                                                            
foo (unsigned char o)                                                                                                                                                                
{                                                                                                                                                                                    
  unsigned long long t1 = -(long long) (o == 0);                                                                                                                                     
  unsigned long long t2 = -(long long) (t1 > 10439075533421201520ULL);                                                                                                               
  unsigned long long t3 = -(long long) (t1 <= t2);                                                                                                                                   
  return t3;                                                                                                                                                                         
}                                                                                                                                                                                    

from comment 8 of that PR could be optimized into return -1ULL.
t1 has range [0,0][-1ULL,-1ULL], t2 is set to 0 if t1 is 0 and to -1ULL if t1 is -1ULL
and thus t2 could be optimized to t2 = t1.  And t1 <= t2 can then trivially fold to
t1 <= t1 aka true.


---


### compiler : `gcc`
### title : `gcc defeats vector constant generation with intrinsics`
### open_at : `2023-01-14T00:24:09Z`
### last_modified_date : `2023-01-17T05:42:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108401
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.3.0`
### severity : `normal`
### contents :
Consider the following code:

#include <immintrin.h>

__m256i load_00FF()
{
    __m256i mm = _mm256_setzero_si256();
    return _mm256_srli_epi16(_mm256_cmpeq_epi64(mm, mm), 8);
}

This function generates a vector constant of alternating 0xFF and 0x00 bytes. The code is written this way to avoid a load from memory, which may cause a cache miss. The expected generated code is this:

        vpcmpeqq        ymm0, ymm0, ymm0
        vpsrlw  ymm0, ymm0, 8
        ret

which is almost exactly what gcc 8 generates (it uses vpcmpeqd instead of vpcmpeqq, which is fine). However, gcc 9 through 11 generates a memory load instead, defeating the attempt to avoid it:

        vmovdqa ymm0, YMMWORD PTR .LC0[rip]
        ret

and gcc 12 generates a worse code:

        movabs  rax, 71777214294589695
        vmovq   xmm1, rax
        vpbroadcastq    ymm0, xmm1
        ret

In all cases, the compiler flags are: -O3 -march=haswell

Code on godbolt.org: https://gcc.godbolt.org/z/sfT787PY9

I think the compiler should follow the code in intrinsics more closely since despite the apparent equivalence, the choice of instructions can have performance implications. The original code that is written by the developer is better anyway, so it's not clear why the compiler is being so creative in this case.


---


### compiler : `gcc`
### title : `Missed integer optimization on x86-64 unless -fwrapv is used`
### open_at : `2023-01-14T10:26:21Z`
### last_modified_date : `2023-01-16T07:54:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108406
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Consider this C++ code:

    #include <cstdint>

    // returns a if less than b or if b is INT32_MIN
    int32_t special_min(int32_t a, int32_t b)
    {
        return a < b || b == INT32_MIN ? a : b;
    }

GCC with -fwrapv correctly realizes that subtracting 1 from b can eliminate the special case, and it generates this code for x86-64:

    lea     edx, [rsi-1]
    mov     eax, edi
    cmp     edi, edx
    cmovg   eax, esi
    ret

But without -fwrapv it generates worse code:

    mov     eax, esi
    cmp     edi, esi
    jl      .L4
    cmp     esi, -2147483648
    je      .L4
    ret
    .L4:
    mov     eax, edi
    ret

If I wrote "hand optimized" C++ code trying to implement that optimization, I understand -fwrapv would be required, otherwise the compiler could decide the signed overflow is UB. But here the compiler is in control, it knows the behavior of integer overflow on x86-64, and so it should not matter whether -fwrapv is used.

Demo: https://godbolt.org/z/o881Mdqoa

Stack Overflow discussion: https://stackoverflow.com/questions/75110108/gcc-wont-use-its-own-optimization-trick-without-fwrapv

This is somewhat related to #102032 in the sense that it's an optimization missed without -fwrapv, but the type of optimization is different.  It is possible there's a single solution that would solve both problems (and others).


---


### compiler : `gcc`
### title : `x264 averaging loop not optimized well for avx512`
### open_at : `2023-01-14T20:55:39Z`
### last_modified_date : `2023-06-14T12:54:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108410
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
x264 benchmark has a loop averaging two unsigned char arrays that is executed with relatively low trip counts that does not play well with our vectorized code.  For AVX512 most time is spent in unvectorized variant since the average number of iterations is too small to reach the vector code.

This table shows runtimes of averaging given block size with scalar loop, vectorized loop for individual vector sizes and aocc codegen:

size   scalar     128     256     512    aocc
    2    8.13    9.49    9.49    9.49    9.49
    4    5.79    6.10    6.10    7.45    6.78
    6    5.44    5.43    5.42    6.78    5.87
    8    5.19    2.71    5.31    6.44    5.42
   12    5.14    3.17    5.33    6.10    4.97
   16    4.85    1.19    1.53    5.93    1.36
   20    4.82    2.03    1.90    6.10    1.90
   24    4.60    0.96    2.58    6.10    2.26
   28    4.51    1.55    2.97    6.00    2.55
   32    4.52    0.68    0.60    0.60    0.77
   34    4.77    0.96    0.88    0.80    0.96
   38    4.42    1.36    1.37    1.17    1.29
   42    4.40    0.84    1.82    1.73    1.63

So for sizes 2-8 scalar loop wins.
For sizes 12-16 128bit vectorization wins, 20-28 behaves funily.
However avx512 vectorization is a huge loss for all sizes up to 31 bytes.
aocc seems to win for 16 bytes.

Note that one problem is that for 256bit vector we peel the epilogue loop (since trip counts fits in max-completely-peeled-insns and max-completely-peel-times. Bumping both twice makes avx512 prologue unrolled too but it does not seem to help x264 benchmark itself.

bmk.c:
#include <stdlib.h>
unsigned char a[10000];
unsigned char b[10000];
unsigned char c[10000];

__attribute__ ((weak))
void
avg (unsigned char *a, unsigned char *b, unsigned char *c, int size)
{
  for (int i = 0; i <size; i++)
    {
      a[i] = (b[i] + c[i] + 1) >> 1;
    }
}
int
main(int argc, char**argv)
{
  int size = atoi (argv[1]);
  for (long i = 0 ; i < 10000000000/size; i++)
    {
      avg (a,b,c,size);
    }
  return 0;
}
#include <stdlib.h>
unsigned char a[10000];
unsigned char b[10000];
unsigned char c[10000];

__attribute__ ((weak))
void
avg (unsigned char *a, unsigned char *b, unsigned char *c, int size)
{
  for (int i = 0; i <size; i++)
    {
      a[i] = (b[i] + c[i] + 1) >> 1;
    }
}
int
main(int argc, char**argv)
{
  int size = atoi (argv[1]);
  for (long i = 0 ; i < 10000000000/size; i++)
    {
      avg (a,b,c,size);
    }
  return 0;
}

bmk.sh:
gcc -Ofast -march=native bmk.c -fno-tree-vectorize -o bmk.scalar
gcc -Ofast -march=native bmk.c -mprefer-vector-width=128 -o bmk.128
gcc -Ofast -march=native bmk.c -mprefer-vector-width=256 -o bmk.256
gcc -Ofast -march=native bmk.c -mprefer-vector-width=512 -o bmk.512
~/aocc-compiler-4.0.0//bin/clang -Ofast -march=native bmk.c -o bmk.aocc

echo "size   scalar     128     256     512    aocc"
for size in 2 4 6 8 12 16 20 24 28 32 34 38 42
do
  scalar=`time -f "%e" ./bmk.scalar $size 2>&1`
  v128=`time -f "%e" ./bmk.128 $size 2>&1`
  v256=`time -f "%e" ./bmk.256 $size 2>&1`
  v512=`time -f "%e" ./bmk.512 $size 2>&1`
  aocc=`time -f "%e" ./bmk.aocc $size 2>&1`
  printf "%5i %7.2f %7.2f %7.2f %7.2f %7.2f\n" $size $scalar $v128 $v256 $v512 $aocc
done


aocc codegen:
# %bb.0:                                # %entry
        pushq   %rbx
        .cfi_def_cfa_offset 16
        .cfi_offset %rbx, -16
        testl   %ecx, %ecx
        jle     .LBB0_15
# %bb.1:                                # %iter.check
        movl    %ecx, %r8d
        cmpl    $16, %ecx
        jae     .LBB0_3
# %bb.2:
        xorl    %eax, %eax
        jmp     .LBB0_14
.LBB0_3:                                # %vector.memcheck
        leaq    (%rsi,%r8), %r9
        leaq    (%rdi,%r8), %rax
        leaq    (%rdx,%r8), %r10
        cmpq    %rdi, %r9
        seta    %r11b
        cmpq    %rsi, %rax
        seta    %bl
        cmpq    %rdi, %r10
        seta    %r9b
        cmpq    %rdx, %rax
        seta    %r10b
        xorl    %eax, %eax
        testb   %bl, %r11b
        jne     .LBB0_14
# %bb.4:                                # %vector.memcheck
        andb    %r10b, %r9b
        jne     .LBB0_14
# %bb.5:                                # %vector.main.loop.iter.check
        cmpl    $128, %ecx
        jae     .LBB0_7
# %bb.6:
        xorl    %eax, %eax
        jmp     .LBB0_11
.LBB0_7:                                # %vector.ph
        movl    %r8d, %eax
        andl    $-128, %eax
        xorl    %ecx, %ecx
        .p2align        4, 0x90
.LBB0_8:                                # %vector.body
                                        # =>This Inner Loop Header: Depth=1
        vmovdqu (%rdx,%rcx), %ymm0
        vmovdqu 32(%rdx,%rcx), %ymm1
        vmovdqu 64(%rdx,%rcx), %ymm2
        vmovdqu 96(%rdx,%rcx), %ymm3
        vpavgb  (%rsi,%rcx), %ymm0, %ymm0
        vpavgb  32(%rsi,%rcx), %ymm1, %ymm1
        vpavgb  64(%rsi,%rcx), %ymm2, %ymm2
        vpavgb  96(%rsi,%rcx), %ymm3, %ymm3
        vmovdqu %ymm0, (%rdi,%rcx)
        vmovdqu %ymm1, 32(%rdi,%rcx)
        vmovdqu %ymm2, 64(%rdi,%rcx)
        vmovdqu %ymm3, 96(%rdi,%rcx)
        subq    $-128, %rcx
        cmpq    %rcx, %rax
        jne     .LBB0_8
# %bb.9:                                # %middle.block
        cmpq    %r8, %rax
        je      .LBB0_15
# %bb.10:                               # %vec.epilog.iter.check
        testb   $112, %r8b
        je      .LBB0_14
.LBB0_11:                               # %vec.epilog.ph
        movq    %rax, %rcx
        movl    %r8d, %eax
        andl    $-16, %eax
        .p2align        4, 0x90
.LBB0_12:                               # %vec.epilog.vector.body
                                        # =>This Inner Loop Header: Depth=1
        vmovdqu (%rdx,%rcx), %xmm0
        vpavgb  (%rsi,%rcx), %xmm0, %xmm0
        vmovdqu %xmm0, (%rdi,%rcx)
        addq    $16, %rcx
        cmpq    %rcx, %rax
        jne     .LBB0_12
# %bb.13:                               # %vec.epilog.middle.block
        cmpq    %r8, %rax
        je      .LBB0_15
        .p2align        4, 0x90
.LBB0_14:                               # %for.body
                                        # =>This Inner Loop Header: Depth=1
        movzbl  (%rsi,%rax), %ecx
        movzbl  (%rdx,%rax), %ebx
        leal    1(%rcx,%rbx), %ecx
        shrl    %ecx
        movb    %cl, (%rdi,%rax)
        incq    %rax
        cmpq    %rax, %r8
        jne     .LBB0_14
.LBB0_15:                               # %for.cond.cleanup
        popq    %rbx
        .cfi_def_cfa_offset 8
        vzeroupper
        retq


trunk does generate the following.
Prologue can be simplified (i.e. cml $30, %eax replaced by $cmp $31, %rcx)
and there is 256 move at L4 just not used for small block sizes becaue of the prologue check.
avg:
.LFB11:
        .cfi_startproc
        movq    %rdx, %r8
        movl    %ecx, %edx
        testl   %ecx, %ecx
        jle     .L27
        leal    -1(%rcx), %eax
        movl    %ecx, %r9d
        cmpl    $30, %eax
        jbe     .L3
        leaq    1(%rsi), %r10
        movq    %rdi, %rcx
        subq    %r10, %rcx
        cmpq    $62, %rcx
        jbe     .L3
        leaq    1(%r8), %r10
        movq    %rdi, %rcx
        subq    %r10, %rcx
        cmpq    $62, %rcx
        jbe     .L3
        cmpl    $62, %eax
        jbe     .L12
        movl    %edx, %ecx
        xorl    %eax, %eax
        shrl    $6, %ecx
        salq    $6, %rcx
        .p2align 4
        .p2align 3
.L5:
        vmovdqu8        (%rsi,%rax), %zmm1
        vpavgb  (%r8,%rax), %zmm1, %zmm0
        vmovdqu8        %zmm0, (%rdi,%rax)
        addq    $64, %rax
        cmpq    %rax, %rcx
        jne     .L5
        movl    %edx, %eax
        andl    $-64, %eax
        movl    %eax, %ecx
        cmpl    %eax, %edx
        je      .L26
        movl    %edx, %r9d
        subl    %eax, %r9d
        leal    -1(%r9), %r10d
        cmpl    $30, %r10d
        jbe     .L7
.L4:
        vmovdqu8        (%rsi,%rcx), %ymm2
        vpavgb  (%r8,%rcx), %ymm2, %ymm0
        vmovdqu8        %ymm0, (%rdi,%rcx)
        movl    %r9d, %ecx
        andl    $-32, %ecx
        addl    %ecx, %eax
        andl    $31, %r9d
        je      .L26
.L7:
        cltq
        .p2align 4
        .p2align 3
.L9:
        movzbl  (%rsi,%rax), %r9d
        movzbl  (%r8,%rax), %ecx
        leal    1(%r9,%rcx), %ecx
        sarl    %ecx
        movb    %cl, (%rdi,%rax)
        incq    %rax
        cmpl    %eax, %edx
        jg      .L9
.L26:
        vzeroupper
.L27:
        ret
        .p2align 4
        .p2align 3
.L3:
        movslq  %edx, %rcx
        xorl    %eax, %eax
        .p2align 4
        .p2align 3
.L10:
        movzbl  (%rsi,%rax), %r9d
        movzbl  (%r8,%rax), %edx
        leal    1(%r9,%rdx), %edx
        sarl    %edx
        movb    %dl, (%rdi,%rax)
        incq    %rax
        cmpq    %rcx, %rax
        jne     .L10
        ret
.L12:
        xorl    %ecx, %ecx
        xorl    %eax, %eax
        jmp     .L4
        .cfi_endproc
.LFE11:


---


### compiler : `gcc`
### title : `RISC-V: Negative optimization of GCSE && LOOP INVARIANTS`
### open_at : `2023-01-16T02:04:04Z`
### last_modified_date : `2023-08-24T09:28:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108412
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Consider such case:
void f (void * restrict in, void * restrict out, int l, int n, int m)
{
  int vl = 32;
  for (int i = 0; i < n; i++)
    {
      vint8mf8_t v1 = __riscv_vle8_v_i8mf8 (in + i + 1, vl);
      __riscv_vse8_v_i8mf8 (out + i + 1, v1, vl);

      vint8mf8_t v2 = __riscv_vle8_v_i8mf8 (in + i + 2, vl);
      __riscv_vse8_v_i8mf8 (out + i + 2, v2, vl);

      vint8mf8_t v3 = __riscv_vle8_v_i8mf8 (in + i + 3, vl);
      __riscv_vse8_v_i8mf8 (out + i + 3, v3, vl);

      vint8mf8_t v4 = __riscv_vle8_v_i8mf8 (in + i + 4, vl);
      __riscv_vse8_v_i8mf8 (out + i + 4, v4, vl);

      vint8mf8_t v5 = __riscv_vle8_v_i8mf8 (in + i + 5, vl);
      __riscv_vse8_v_i8mf8 (out + i + 5, v5, vl);

      vint8mf8_t v6 = __riscv_vle8_v_i8mf8 (in + i + 6, vl);
      __riscv_vse8_v_i8mf8 (out + i + 6, v6, vl);

      vint8mf8_t v7 = __riscv_vle8_v_i8mf8 (in + i + 7, vl);
      __riscv_vse8_v_i8mf8 (out + i + 7, v7, vl);

      vint8mf8_t v8 = __riscv_vle8_v_i8mf8 (in + i + 8, vl);
      __riscv_vse8_v_i8mf8 (out + i + 8, v8, vl);

      vint8mf8_t v9 = __riscv_vle8_v_i8mf8 (in + i + 9, vl);
      __riscv_vse8_v_i8mf8 (out + i + 9, v9, vl);

      vint8mf8_t v10 = __riscv_vle8_v_i8mf8 (in + i + 10, vl);
      __riscv_vse8_v_i8mf8 (out + i + 10, v10, vl);

      vint8mf8_t v11 = __riscv_vle8_v_i8mf8 (in + i + 11, vl);
      __riscv_vse8_v_i8mf8 (out + i + 11, v11, vl);

      vint8mf8_t v12 = __riscv_vle8_v_i8mf8 (in + i + 12, vl);
      __riscv_vse8_v_i8mf8 (out + i + 12, v12, vl);

      vint8mf8_t v13 = __riscv_vle8_v_i8mf8 (in + i + 13, vl);
      __riscv_vse8_v_i8mf8 (out + i + 13, v13, vl);

      vint8mf8_t v14 = __riscv_vle8_v_i8mf8 (in + i + 14, vl);
      __riscv_vse8_v_i8mf8 (out + i + 14, v14, vl);
    }
}

-O3 ASM:
f:
	addi	sp,sp,-192
	sw	a3,28(sp)
	ble	a3,zero,.L1
	addi	a5,a0,1
	sw	a5,76(sp)
	addi	a5,a1,1
	sw	a5,80(sp)
	addi	a5,a0,2
	sw	a5,84(sp)
	addi	a5,a1,2
	sw	a5,88(sp)
	addi	a5,a0,3
	sw	a5,92(sp)
	addi	a5,a1,3
	sw	a5,96(sp)
	addi	a5,a0,4
	sw	a5,100(sp)
	addi	a5,a1,4
	sw	a5,104(sp)
	addi	a5,a0,5
	sw	a5,108(sp)
	addi	a5,a1,5
	sw	a5,112(sp)
	addi	a5,a0,6
	sw	a5,116(sp)
	addi	a5,a1,6
	sw	a5,120(sp)
	addi	a5,a0,7
	sw	a5,124(sp)
	addi	a5,a1,7
	sw	a5,128(sp)
	addi	a5,a0,8
	sw	a5,132(sp)
	addi	a5,a1,8
	sw	a5,136(sp)
	addi	a5,a0,9
	sw	a5,140(sp)
	addi	a5,a1,9
	sw	a5,32(sp)
	addi	a5,a0,10
	sw	a5,36(sp)
	addi	a5,a1,10
	sw	a5,40(sp)
	sw	s0,188(sp)
	addi	a5,a0,11
	sw	s1,184(sp)
	sw	s2,180(sp)
	sw	s3,176(sp)
	sw	s4,172(sp)
	sw	s5,168(sp)
	sw	s6,164(sp)
	sw	s7,160(sp)
	sw	s8,156(sp)
	sw	s9,152(sp)
	sw	s10,148(sp)
	sw	s11,144(sp)
	sw	a5,44(sp)
	addi	a5,a1,11
	sw	a5,48(sp)
	addi	a5,a0,12
	sw	a5,52(sp)
	addi	a5,a1,12
	sw	a5,56(sp)
	addi	a5,a0,13
	sw	a5,60(sp)
	addi	a5,a1,13
	sw	a5,64(sp)
	addi	a5,a0,14
	sw	a5,68(sp)
	addi	a5,a1,14
	sw	a5,72(sp)
	li	a4,0
	li	a5,32
	vsetvli	zero,a5,e8,mf8,ta,ma
.L3:
	lw	a3,76(sp)
	lw	t2,60(sp)
	add	s11,a3,a4
	lw	a3,80(sp)
	add	t2,t2,a4
	add	t0,a3,a4
	lw	a3,84(sp)
	add	s10,a3,a4
	lw	a3,88(sp)
	add	t6,a3,a4
	lw	a3,92(sp)
	add	s9,a3,a4
	lw	a3,96(sp)
	add	t5,a3,a4
	lw	a3,100(sp)
	add	s8,a3,a4
	lw	a3,104(sp)
	add	t4,a3,a4
	lw	a3,108(sp)
	add	s7,a3,a4
	lw	a3,112(sp)
	add	t3,a3,a4
	lw	a3,116(sp)
	add	s6,a3,a4
	lw	a3,120(sp)
	add	t1,a3,a4
	lw	a3,124(sp)
	add	s5,a3,a4
	lw	a3,128(sp)
	add	a7,a3,a4
	lw	a3,132(sp)
	add	s4,a3,a4
	lw	a3,136(sp)
	add	a6,a3,a4
	lw	a3,140(sp)
	add	s3,a3,a4
	lw	a3,32(sp)
	add	a0,a3,a4
	lw	a3,36(sp)
	add	s2,a3,a4
	lw	a3,40(sp)
	add	a1,a3,a4
	lw	a3,44(sp)
	add	s1,a3,a4
	lw	a3,48(sp)
	add	a2,a3,a4
	lw	a3,52(sp)
	add	s0,a3,a4
	lw	a3,56(sp)
	add	a3,a3,a4
	sw	a3,12(sp)
	lw	a3,64(sp)
	add	a3,a3,a4
	sw	a3,16(sp)
	lw	a3,68(sp)
	add	a3,a3,a4
	sw	a3,20(sp)
	lw	a3,72(sp)
	add	a3,a3,a4
	sw	a3,24(sp)
	vle8.v	v24,0(s11)
	vse8.v	v24,0(t0)
	vle8.v	v24,0(s10)
	vse8.v	v24,0(t6)
	vle8.v	v24,0(s9)
	vse8.v	v24,0(t5)
	vle8.v	v24,0(s8)
	vse8.v	v24,0(t4)
	vle8.v	v24,0(s7)
	vse8.v	v24,0(t3)
	vle8.v	v24,0(s6)
	vse8.v	v24,0(t1)
	vle8.v	v24,0(s5)
	vse8.v	v24,0(a7)
	vle8.v	v24,0(s4)
	vse8.v	v24,0(a6)
	vle8.v	v24,0(s3)
	vse8.v	v24,0(a0)
	vle8.v	v24,0(s2)
	vse8.v	v24,0(a1)
	vle8.v	v24,0(s1)
	vse8.v	v24,0(a2)
	lw	a3,12(sp)
	vle8.v	v24,0(s0)
	addi	a4,a4,1
	vse8.v	v24,0(a3)
	lw	a3,16(sp)
	vle8.v	v24,0(t2)
	vse8.v	v24,0(a3)
	lw	a3,20(sp)
	vle8.v	v24,0(a3)
	lw	a3,24(sp)
	vse8.v	v24,0(a3)
	lw	a3,28(sp)
	bne	a3,a4,.L3
	lw	s0,188(sp)
	lw	s1,184(sp)
	lw	s2,180(sp)
	lw	s3,176(sp)
	lw	s4,172(sp)
	lw	s5,168(sp)
	lw	s6,164(sp)
	lw	s7,160(sp)
	lw	s8,156(sp)
	lw	s9,152(sp)
	lw	s10,148(sp)
	lw	s11,144(sp)
.L1:
	addi	sp,sp,192
	jr	ra

Codegen is quite ugly.

However, if we try -O3 -fno-gcse -fno-schedule-insns -fno-move-loop-invariants

ASM is much better, same as LLVM:
f:
	ble	a3,zero,.L1
	li	a5,0
	vsetvli	zero,a4,e8,mf8,ta,ma
.L3:
	addi	a2,a0,1
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,1
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,2
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,2
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,3
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,3
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,4
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,4
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,5
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,5
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,6
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,6
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,7
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,7
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,8
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,8
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,9
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,9
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,10
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,10
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,11
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,11
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,12
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,12
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,13
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,13
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a2,a0,14
	add	a2,a2,a5
	vle8.v	v24,0(a2)
	addi	a2,a1,14
	add	a2,a2,a5
	vse8.v	v24,0(a2)
	addi	a5,a5,1
	bne	a3,a5,.L3
.L1:
	ret

Currently, RVV support doesn't have any Cost model. I am not sure whether it's related to it. May need someone help me to fix it. Thanks.


---


### compiler : `gcc`
### title : `gcc does not optimize trivial code`
### open_at : `2023-01-16T14:29:26Z`
### last_modified_date : `2023-01-17T13:26:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108418
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
https://godbolt.org/z/s3j8jK6ca

```
#include <stdint.h>

int firewall1(const uint8_t *restrict data) {
    const uint8_t ip_proto = *data;
    const uint16_t dst_port = *((const uint16_t *)data + 32);
    const uint16_t qwe = *((const uint16_t *)data + 64);

    if (ip_proto == 17 && dst_port == 17 && qwe == 42) return 1;
    if (ip_proto == 17 && dst_port == 23 && qwe == 42) return 1;
    if (ip_proto == 17 && dst_port == 45 && qwe == 42) return 1;
    if (ip_proto == 17 && dst_port == 63 && qwe == 42) return 1;
    if (ip_proto == 17 && dst_port == 0 && qwe == 42) return 1;
    if (ip_proto == 17 && dst_port == 2 && qwe == 42) return 1;
    if (ip_proto == 17 && dst_port == 3 && qwe == 42) return 1;

    return 0;
}

int firewall2(const uint8_t *restrict data) {
    const uint8_t ip_proto = *data;
    const uint16_t dst_port = *((const uint16_t *)data + 32);
    const uint16_t qwe = *((const uint16_t *)data + 64);

    if (ip_proto == 17 && dst_port == 17) return 1;
    if (ip_proto == 17 && dst_port == 23) return 1;
    if (ip_proto == 17 && dst_port == 45) return 1;
    if (ip_proto == 17 && dst_port == 63) return 1;
    if (ip_proto == 17 && dst_port == 0) return 1;
    if (ip_proto == 17 && dst_port == 2) return 1;
    if (ip_proto == 17 && dst_port == 3) return 1;

    return 0;
}
```

It can't understand common condition (ip_proto == 17 && qwe == 42).

But it can for simpler case in firewall2.

See godbolt assembler output.


---


### compiler : `gcc`
### title : `[13/14/14 Regression] Dead Code Elimination Regression at -O2 since r13-440-g98e475a8f58`
### open_at : `2023-01-16T15:18:44Z`
### last_modified_date : `2023-08-09T01:52:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108419
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54281
case as file

cat case.c #1565
static int b = 6, c;
long d;
short h;
short i;
short j;
char k;
void foo();
short(a)(short l, short m) { return l + m; }
short f();
short g(unsigned short, int, char, long);
static signed char e() {
  unsigned n = -10;
  for (; n >= 14; n = a(n, 8)) {
    i = g(b, 0, c, b);
    j = f(i, b, d < j, 5, 7, 9, 5);
    k = 200 + n;
    h = k % 5;
    if (h)
      ;
    else
      foo();
  }
  return n;
}
int main() {
  b || e();
  b = 1;
}

`gcc-f99d7d669eaa2830eb5878df4da67e77ec791522 (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.2.0 -O2` can.

`gcc-f99d7d669eaa2830eb5878df4da67e77ec791522 (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movl	b(%rip), %esi
	testl	%esi, %esi
	je	.L12
	movl	$1, b(%rip)
	xorl	%eax, %eax
	ret
.L12:
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	movl	$-10, %r12d
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	movl	$2, %ebp
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movl	$5, %ebx
.L6:
	movslq	b(%rip), %rcx
	xorl	%edx, %edx
	xorl	%esi, %esi
	movzwl	%cx, %edi
	call	g
	movl	b(%rip), %esi
	movl	$5, %ecx
	movswq	j(%rip), %rdx
	movw	%ax, i(%rip)
	cmpq	d(%rip), %rdx
	movswl	%ax, %edi
	movl	$9, %r9d
	pushq	%rax
	.cfi_def_cfa_offset 40
	setg	%dl
	movl	$7, %r8d
	xorl	%eax, %eax
	pushq	$5
	.cfi_def_cfa_offset 48
	movzbl	%dl, %edx
	call	f
	movw	%ax, j(%rip)
	leal	-56(%r12), %eax
	movb	%al, k(%rip)
	cbtw
	idivb	%bl
	movl	%eax, %edx
	sarw	$8, %dx
	shrw	$8, %ax
	movw	%dx, h(%rip)
	popq	%rdx
	.cfi_def_cfa_offset 40
	popq	%rcx
	.cfi_def_cfa_offset 32
	je	.L13
.L5:
	addl	$8, %r12d
	subl	$1, %ebp
	movswl	%r12w, %r12d
	je	.L14
	movl	$1, %ebp
	jmp	.L6
.L14:
	movl	$1, b(%rip)
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 24
	xorl	%eax, %eax
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	ret
.L13:
	.cfi_restore_state
	xorl	%eax, %eax
	call	foo
	jmp	.L5
---------- END OUTPUT ---------


`gcc-releases/gcc-12.2.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movl	b(%rip), %r9d
	testl	%r9d, %r9d
	je	.L11
	movl	$1, b(%rip)
	xorl	%eax, %eax
	ret
.L11:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movl	$-66, %ebp
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	movl	$5, %ebx
	pushq	%r8
	.cfi_def_cfa_offset 32
.L5:
	movslq	b(%rip), %rcx
	xorl	%edx, %edx
	xorl	%esi, %esi
	movzwl	%cx, %edi
	call	g
	movswq	j(%rip), %rdx
	cmpq	d(%rip), %rdx
	movl	$9, %r9d
	pushq	%rcx
	.cfi_def_cfa_offset 40
	setg	%dl
	movl	b(%rip), %esi
	movswl	%ax, %edi
	pushq	$5
	.cfi_def_cfa_offset 48
	movzbl	%dl, %edx
	movl	$7, %r8d
	movl	$5, %ecx
	movw	%ax, i(%rip)
	xorl	%eax, %eax
	call	f
	popq	%rsi
	.cfi_def_cfa_offset 40
	popq	%rdi
	.cfi_def_cfa_offset 32
	movb	%bpl, k(%rip)
	movw	%ax, j(%rip)
	movsbw	%bpl, %ax
	idivb	%bl
	sarw	$8, %ax
	movw	%ax, h(%rip)
	cmpb	$-58, %bpl
	je	.L12
	movl	$-58, %ebp
	jmp	.L5
.L12:
	movl	$1, b(%rip)
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


Bisects to: r13-440-g98e475a8f58

commit 98e475a8f58ca3ba6e9bd5c9276efce4236f5d26
Author: Andrew MacLeod <amacleod@redhat.com>
Date:   Fri Mar 18 11:50:33 2022 -0400

    Fix return value in ranger_cache::get_global_range.
    
    The "is_current" status is returned by parameter, but was being returned by the
    function as well instead of true if NAME had a global range, and FALSE
    if it did not.
    
            * gimple-range-cache.cc (ranger_cache::get_global_range): Return the
            had_global value instead.


---


### compiler : `gcc`
### title : `[13 Regression] FAIL: gcc.target/i386/pr89618.c scan-tree-dump vect "LOOP VECTORIZED"`
### open_at : `2023-01-17T10:31:50Z`
### last_modified_date : `2023-03-14T08:48:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108429
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
The testcase FAILs for me since recently.


---


### compiler : `gcc`
### title : `GCC fails to elide udiv/msub when doing modulus by select of constants`
### open_at : `2023-01-18T15:11:08Z`
### last_modified_date : `2023-01-18T16:51:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108446
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
unsigned foo(int vl, unsigned len) {
  unsigned pad = vl <= 256 ? 128 : 256;
  return len % pad;
}

At -O2 aarch64 gcc generates:
foo:
        cmp     w0, 256
        mov     w2, 256
        mov     w0, 128
        csel    w2, w2, w0, gt
        udiv    w0, w1, w2
        msub    w0, w0, w2, w1
        ret

clang, for example can generate the cheaper:
foo:                                    // @foo
        cmp     w0, #256
        mov     w8, #127
        mov     w9, #255
        csel    w8, w9, w8, gt
        and     w0, w8, w1
        ret

Similar situation on x86.
I suppose this could be a match.pd fix or otherwise something during expand-time?


---


### compiler : `gcc`
### title : `[13 Regression] glibc math/test-*-iseqsig failures`
### open_at : `2023-01-18T17:37:11Z`
### last_modified_date : `2023-01-27T14:41:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108447
### status : `RESOLVED`
### tags : `missed-optimization, wrong-code`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #107608 +++

From https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107608#c32 :

FYI, most likely it's totally unrelated to this bug, for right now with latest gcc trunk and glibc trunk on x86-64, I still see the following iseqsig errors:

FAIL: math/test-double-iseqsig
FAIL: math/test-float-iseqsig
FAIL: math/test-float128-iseqsig
FAIL: math/test-float32-iseqsig
FAIL: math/test-float32x-iseqsig
FAIL: math/test-float64-iseqsig
FAIL: math/test-float64x-iseqsig
FAIL: math/test-ldouble-iseqsig

I think this can be reduced to e.g.
__attribute__((noipa)) int
foo (float x, float y)
{
  _Bool cmp1 = x <= y;
  _Bool cmp2 = x >= y;
  if (cmp1 && cmp2)
    return 1;
  else if (!cmp1 && !cmp2)
    return -1;
  return 0;
}

int
main ()
{
  if (foo (0.0f, __builtin_nanf ("")) != -1)
    __builtin_abort ();
  if (foo (__builtin_nanf (""), -42.0f) != -1)
    __builtin_abort ();
  if (foo (0.0f, -0.0f) != 1)
    __builtin_abort ();
  if (foo (42.0f, 42.0f) != 1)
    __builtin_abort ();
  if (foo (42.0f, -0.0f) != 0)
    __builtin_abort ();
  if (foo (0.0f, -42.0f) != 0)
    __builtin_abort ();
  return 0;
}
and is fairly different from the PR106805 bug, which is about iseqsig too, but in that case the ranges of the operands are singletons and there are issues not just on the trunk, but also on GCC 12 etc. (though more on the trunk).

In the above short testcase, it works fine in GCC 12 and only fails on the trunk.


---


### compiler : `gcc`
### title : `Optimize (a < b) == (b < a) to a == b`
### open_at : `2023-01-19T14:58:10Z`
### last_modified_date : `2023-09-12T22:32:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108465
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
For GCC 12 the following code with -O2:

int compare_eq(int a, int b) {
    return ((a < b) == (b < a));
}

compiles into the following assembly:

compare_eq(int, int):
        cmp     edi, esi
        setl    dl
        setg    al
        cmp     dl, al
        sete    al
        movzx   eax, al
        ret

Which is suboptimal. More optimal assembly would be:

compare_eq(int, int):
        xor     eax, eax
        cmp     edi, esi
        sete    al
        ret

Godbolt Playground: https://godbolt.org/z/4sfcTjjjb

Motivation: in generic C++ code the comparison is often done via a functor. The algorithm is only allowed to use that functor:

if (__comp(a, b) == __comp(b, a)) {
    return x;
} else if (__comp(b, a)) {
    return y;
}

Because of that, with the inlined functor the comparison becomes ((a < b) == (b < a))


---


### compiler : `gcc`
### title : `Suboptimal codegen for __int128 subtraction on x86_64`
### open_at : `2023-01-19T21:48:16Z`
### last_modified_date : `2023-01-19T23:17:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108471
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
On x86_64, GCC generates an excessive amount of redundant `mov` instructions for `__int128` subtraction in C/C++. Clicking through versions on godbolt shows 
that this started getting worse in GCC 9.1 and later versions. See also https://godbolt.org/z/86v6ar457

The code:
```
__int128 sub(__int128 a, __int128 b) { return a - b; }
```

At -O3 or -O2, GCC (trunk) generates:
```
sub:
        mov     r8, rdi
        mov     rax, rsi
        mov     rsi, r8
        mov     rdi, rax
        mov     r8, rdx
        mov     rax, rsi
        mov     rdx, rdi
        sub     rax, r8
        sbb     rdx, rcx
        ret
```
Interestingly, the use of `r8` in the first three instructions disappears when
compiling w/ -O1, and those instructions are folded into two `mov`s instead.

By contrast, Clang (also at -O3) generates:
```
sub:
        mov     rax, rdi
        sub     rax, rdx
        sbb     rsi, rcx
        mov     rdx, rsi
        ret
```

This is probably not a high-priority bug; I just wanted to bring attention to the fact that this is happening.


---


### compiler : `gcc`
### title : `fwprop over-optimizes conversion from + to |`
### open_at : `2023-01-20T10:59:39Z`
### last_modified_date : `2023-01-20T11:23:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108477
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
In the unsigned int case (baz) fwprop over-optimizes the addition to a logical or:

--cut here--
int lock;

int bar (int old)
{
  int val = (old >> 1) & 0x1;
  int new = (old & ~0x3) + 0x2 + val;

  lock = new;
  return val ? 0 : -1;
}

int ulock;

int baz (unsigned int old)
{
  unsigned int val = (old >> 1) & 0x1;
  unsigned int new = (old & ~0x3) + 0x2 + val;

  ulock = new;
  return val ? 0 : -1;
}
--cut here--

resulting in:

bar:
        movl    %edi, %eax
        andl    $-4, %edi
        sarl    %eax
        andl    $1, %eax
        leal    2(%rax,%rdi), %edx    <---- here
        subl    $1, %eax
        movl    %edx, lock(%rip)
        ret

baz:
        movl    %edi, %eax
        andl    $-4, %edi
        shrl    %eax
        andl    $1, %eax
        orl     %eax, %edi    <--- here ...
        subl    $1, %eax
        addl    $2, %edi      <--- ... and here
        movl    %edi, ulock(%rip)
        ret

Please note the three-operand addition, implemented with LEAL instruction in the signed case, which is not emitted in the unsigned case. The reason is fwprop pass that substitutes addition with the equivalent or operation.


---


### compiler : `gcc`
### title : `[13 Regression] ice in expand_LOOP_DIST_ALIAS with -O3 -ftrivial-auto-var-init=zero`
### open_at : `2023-01-20T16:24:44Z`
### last_modified_date : `2023-01-23T10:51:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108482
### status : `RESOLVED`
### tags : `ice-on-valid-code, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54317
C source code

For the attached C code, compiled as follows:

$ /home/dcb36/gcc/results/bin/gcc  -c -O3 -w -ftrivial-auto-var-init=zero  bug874.c
during RTL pass: expand
testFile.21840.c: In function ‘func_1.isra’:
testFile.21840.c:112:16: internal compiler error: in expand_LOOP_DIST_ALIAS, at internal-fn.cc:2737
0xb17ff4 expand_LOOP_DIST_ALIAS(internal_fn, gcall*)
	../../trunk.d1/gcc/internal-fn.cc:2737
0x86d30e expand_call_stmt(gcall*)
	../../trunk.d1/gcc/cfgexpand.cc:2737
0x86d30e expand_gimple_stmt_1(gimple*)
	../../trunk.d1/gcc/cfgexpand.cc:3880
0x86d30e expand_gimple_stmt(gimple*)
	../../trunk.d1/gcc/cfgexpand.cc:4044

The bug seems to exist since sometime before g:02c031088ac0bbf7,
dated 20221220.

I have a reduction running.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] ~20-30x slowdown in populating std::vector from std::ranges::iota_view`
### open_at : `2023-01-20T22:20:32Z`
### last_modified_date : `2023-07-07T10:44:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108487
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.2.0`
### severity : `normal`
### contents :
Using -std=c++20 -O3, comparing gcc 12.2 vs. gcc 10.3:
 * fn2 is 20-30x slower on gcc 12.2 (i.e. 2000-3000% more) 
 * fn1 is ~20% slower on gcc 12.2 

This test was run on an 52 core Intel Xeon Gold 6278C CPU.  Tests on www.godbolt.org directionally align with these findings.  It seems the slowdown was introduced in 10.4 & 11.1.  The trunk has identical performance to 12.2.

#include <vector>
#include <ranges>
#include <ctime>
#include <iostream>

__attribute__((noinline)) std::vector<int> fn1(int n)
{
    auto v = std::vector<int>(n);
    for(int i = 0; i != n; ++i)
        v[i] = i;
    return v;
}

__attribute__((noinline)) std::vector<int> fn2(int n)
{
    auto rng = std::ranges::iota_view{0, n};
    return std::vector<int>{rng.begin(), rng.end()};
}

int main() {
    int n = 100'000;
    int times = 100'000;

    auto t0 = std::clock();
    for (int i = 0; i < times; ++i)
        fn1(n);            
    auto t1 = std::clock();
    for (int i = 0; i < times; ++i)
        fn2(n);            
    auto t2 = std::clock();
    std::cout << t1 - t0 << '\n';
    std::cout << t2 - t1 << '\n';
    return 0;
}

P.S. 20% slowdown for a common vector population is still significant IMO.  I am not sure that qualifies as a bug.  I did not file one on account of the 'fn1' slowdown.


---


### compiler : `gcc`
### title : `bit_cast from 32-byte vector generates worse code than memcpy`
### open_at : `2023-01-23T22:01:58Z`
### last_modified_date : `2023-01-24T09:20:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108506
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
Gcc trunk on x86-64 produces much worse assembly for 'deserialize' func than for equivalent 'deserialize2'.
These two should be equivalent as bit_cast should be just a type-safe equivalent of memcpy (that is the only difference between the two funcs).

g++ -std=c++23 -O3 -mavx2

using v32uc = unsigned char __attribute((vector_size(32)));

constexpr auto N = 1024;

struct Foo
{
    int a[8];
};

static_assert(sizeof(Foo) == sizeof(v32uc));

void deserialize(const unsigned char* input, Foo* output)
{
    for (auto i = 0u; i != N; ++i)
    {
        v32uc vec;
        __builtin_memcpy(&vec, input, sizeof(vec));
        input += sizeof(vec);

        vec = __builtin_shuffle(vec,
            v32uc{
                3, 2, 1, 0,
                7, 6, 5, 4,
                11, 10, 9, 8,
                15, 14, 13, 12,
                19, 18, 17, 16,
                23, 22, 21, 20,
                27, 26, 25, 24,
                31, 30, 29, 28
                }
        );
        *output = __builtin_bit_cast(Foo, vec);
        output++;
    }
}

void deserialize2(const unsigned char* input, Foo* output)
{
    for (auto i = 0u; i != N; ++i)
    {
        v32uc vec;
        __builtin_memcpy(&vec, input, sizeof(vec));
        input += sizeof(vec);

        vec = __builtin_shuffle(vec,
            v32uc{
                3, 2, 1, 0,
                7, 6, 5, 4,
                11, 10, 9, 8,
                15, 14, 13, 12,
                19, 18, 17, 16,
                23, 22, 21, 20,
                27, 26, 25, 24,
                31, 30, 29, 28
                }
        );
        __builtin_memcpy(output, &vec, sizeof(vec));
        output++;
    }
}


Disassembly:

deserialize(unsigned char const*, Foo*):
  push rbp
  xor eax, eax
  mov rbp, rsp
  and rsp, -32
  vmovdqa ymm1, YMMWORD PTR .LC0[rip]
.L2:
  vmovdqu ymm3, YMMWORD PTR [rdi+rax]
  vpshufb ymm2, ymm3, ymm1
  vmovdqa YMMWORD PTR [rsp-32], ymm2
  mov rdx, QWORD PTR [rsp-32]
  mov rcx, QWORD PTR [rsp-24]
  vmovdqa xmm4, XMMWORD PTR [rsp-16]
  vmovq xmm0, rdx
  vpinsrq xmm0, xmm0, rcx, 1
  vmovdqu XMMWORD PTR [rsi+16+rax], xmm4
  vmovdqu XMMWORD PTR [rsi+rax], xmm0
  add rax, 32
  cmp rax, 32768
  jne .L2
  vzeroupper
  leave
  ret
deserialize2(unsigned char const*, Foo*):
  vmovdqa ymm1, YMMWORD PTR .LC0[rip]
  xor eax, eax
.L7:
  vmovdqu ymm2, YMMWORD PTR [rdi+rax]
  vpshufb ymm0, ymm2, ymm1
  vmovdqu YMMWORD PTR [rsi+rax], ymm0
  add rax, 32
  cmp rax, 32768
  jne .L7
  vzeroupper
  ret
.LC0:
  .byte 3
  .byte 2
  .byte 1
  .byte 0
  .byte 7
  .byte 6
  .byte 5
  .byte 4
  .byte 11
  .byte 10
  .byte 9
  .byte 8
  .byte 15
  .byte 14
  .byte 13
  .byte 12
  .byte 3
  .byte 2
  .byte 1
  .byte 0
  .byte 7
  .byte 6
  .byte 5
  .byte 4
  .byte 11
  .byte 10
  .byte 9
  .byte 8
  .byte 15
  .byte 14
  .byte 13
  .byte 12


---


### compiler : `gcc`
### title : `[11/12 Regression] Useless movzx instruction emitted when loading 8 bits from 24 bit struct`
### open_at : `2023-01-24T11:01:25Z`
### last_modified_date : `2023-02-13T20:04:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108516
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
This code produces an extra instruction in GCC 11 and 12 (but not 10 or before):

    #include <stdint.h>

    struct S
    {
        uint8_t e1;
        uint8_t e2;
        uint8_t e3;
    };

    uint32_t f2(S s) { return s.e2; }

The generated code is:

        mov     eax, edi
        movzx   eax, ah
        movzx   eax, al
        ret

The movzx from "al" is useless: it zeros the high 24 bits of eax which are known to be zero after the prior movzx.  GCC 10 and earlier do not emit the useless instruction, and neither do GCC 11 or 12 if the struct contains 4 bytes instead of 3.

Demo: https://godbolt.org/z/Wajo86GfM


---


### compiler : `gcc`
### title : `-Wuse-after-free false positive triggered by -O2 on a shared_ptr implementation`
### open_at : `2023-01-26T23:01:43Z`
### last_modified_date : `2023-01-28T23:49:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108565
### status : `NEW`
### tags : `diagnostic, EH, missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
May or may not be a duplicate of https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106119

Consider the following code, which is a subset of a shared_ptr's implementation:

struct shared_ptr {
    int *counter_ = nullptr;
    int *data_ = nullptr;
    shared_ptr(int *data)
        : counter_(data ? new int(1) : nullptr), data_(data) {
    }
    shared_ptr(const shared_ptr &other)
        : counter_(other.counter_), data_(other.data_) {
        if (counter_ != nullptr) {
            ++*counter_;
        }
    }
    ~shared_ptr() {
        if (counter_ != nullptr) {
            --*counter_;
            if (*counter_ == 0) {
                delete counter_;
                delete data_;
            }
        }
    }
};
void foo() {
    shared_ptr a(new int(10));  // should be non-nullptr
    shared_ptr b(a);
    shared_ptr c(new int(20));  // should be non-nullptr
}
int main() {
    foo();
}

`g++ -std=c++17 -Wuse-after-free -O1` compiles this code without any problems, and it runs without memory leaks or double-frees. However, `g++ -std=c++17 -Wuse-after-free -O2` generates a bogus warning:

In destructor 'shared_ptr::~shared_ptr()',
    inlined from 'void foo()' at x.cpp:27:1:
x.cpp:15:15: warning: pointer used after 'void operator delete(void*, long long unsigned int)' [-Wuse-after-free]
   15 |             --*counter_;
      |               ^~~~~~~~~
In destructor 'shared_ptr::~shared_ptr()',
    inlined from 'void foo()' at x.cpp:27:1:
x.cpp:17:24: note: call to 'void operator delete(void*, long long unsigned int)' here
   17 |                 delete counter_;
      |                        ^~~~~~~~

Seems like GCC is not smart enough to understand the underlying logic of 'shared_ptr'. Any of the following transformations remove the warning for me:

* Replacing `new int(10)` or `new int(20)` with nullptr
* Removing the ternary operator from `counter_`'s initialization.
* Replacing `-O2` with `-O1` or `-O0`

I've tested it both on trunk via Godbolt (https://godbolt.org/z/6ed1GY9Y7), and on my local GCC 12.2 (Windows, msys2) and GCC 12.1 (Ubuntu 22.04).


---


### compiler : `gcc`
### title : `Fix for PR96373 regresses fabd_1.c with -ftrapping-math`
### open_at : `2023-01-27T10:42:15Z`
### last_modified_date : `2023-04-03T08:58:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108571
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
The upcoming fix for PR96373 means that, without -fno-trapping-math,
we can no longer optimise abs(a - b) to absolute-difference(a, b)
for predicated SVE loops.  The subtraction becomes predicated,
and so we have the equivalent of:

  abs(IFN_COND_SUB(loopmask, a, b, c))

where c is the fallback/else value.  It isn't possible to tell from
this expression alone that the value of the abs result doesn't
matter for false loopmask elements (and that c will do just as
well as abs(c)).  It therefore isn't possible to fold this into
a conditional absolute difference without further information.

One possible fix would be to add a gimple value and RTL object that
represents an unknown/undefined/don't-care value, a bit like LLVM's
undef.  Another would be to make absolute difference a recognised
gimple operation (via ifns) and make the vectoriser produce it
directly.  Or we could add a pass that back-propagates information
about uses to discover which elements of a vector actually matter
(which I think would also require gimple-level absolute difference).


---


### compiler : `gcc`
### title : `memset uses SSE stores but afterwards does not but if used "" will use them`
### open_at : `2023-01-28T23:16:44Z`
### last_modified_date : `2023-01-30T15:21:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108585
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
This code, compiled with GCC 12.2 (-O2 -std=c++20)

#include <cstddef>

int foo(std::byte *arr);

int square(int num) {
    std::byte arr[96] = {};
    return foo(arr);
}

results in this:

square(int):
        sub     rsp, 104
        xor     eax, eax
        mov     ecx, 12
        mov     rdi, rsp
        rep stosq
        mov     rdi, rsp
        call    foo(std::byte*)
        add     rsp, 104
        ret

After swapping "std::byte" with "char" the result is:

square(int):
        sub     rsp, 104
        pxor    xmm0, xmm0
        mov     rdi, rsp
        movaps  XMMWORD PTR [rsp], xmm0
        movaps  XMMWORD PTR [rsp+16], xmm0
        movaps  XMMWORD PTR [rsp+32], xmm0
        movaps  XMMWORD PTR [rsp+48], xmm0
        movaps  XMMWORD PTR [rsp+64], xmm0
        movaps  XMMWORD PTR [rsp+80], xmm0
        call    foo(char*)
        add     rsp, 104
        ret

As you can see, SSE instructions are emitted only with an char array. BTW, in
clang, both versions are identical and use movaps.


---


### compiler : `gcc`
### title : `_subborrow_u32 generates suboptimal code when second subtraction operand is constant on x86 targets`
### open_at : `2023-01-31T12:39:41Z`
### last_modified_date : `2023-02-02T02:24:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108614
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
Here is some C++ code that generates suboptimal code with the -O2 -march=skylake-avx512 -m32 options with gcc 12.2.0:
#include <stdint.h>
#include <utility>
#include <x86intrin.h>
#include <immintrin.h>

std::pair<uint32_t, uint32_t> ComputeHiMaskAndHiZeroAmt(uint32_t len) {
    uint32_t hiMask;
    uint32_t hiZeroAmt;

    _addcarry_u32(_subborrow_u32(0, len, 32, &hiZeroAmt),
        uint32_t{0xFFFFFFFFu}, 0, &hiMask);
    
    hiMask = _bzhi_u32(hiMask, hiZeroAmt);

    return std::make_pair(hiMask, hiZeroAmt);
}

Here is the assembly code that is generated when the above code is compiled with gcc 12.2.0 with the -O2 -march=skylake-avx512 -m32 options:
_Z25ComputeHiMaskAndHiZeroAmtj:
        subl    $16, %esp
        movl    24(%esp), %eax
        movl    $32, %edx
        subl    %edx, %eax
        movl    20(%esp), %ecx
        movl    $-1, %edx
        adcl    $0, %edx
        movl    %eax, 4(%ecx)
        bzhi    %eax, %edx, %edx
        movl    %ecx, %eax
        movl    %edx, (%ecx)
        addl    $16, %esp
        ret     $4

Here is a more optimal version of the above code (for 32-bit x86):
_Z25ComputeHiMaskAndHiZeroAmtj:
        movl    8(%esp), %eax
        subl    $32, %eax
        movl    4(%esp), %ecx
        movl    $-1, %edx
        adcl    $0, %edx
        movl    %eax, 4(%ecx)
        bzhi    %eax, %edx, %edx
        movl    %ecx, %eax
        movl    %edx, (%ecx)
        ret     $4

Here is the assembly code that is generated when the above code is compiled with gcc 12.2.0 with the -O2 -march=skylake-avx512 options:
_Z25ComputeHiMaskAndHiZeroAmtj:
        movl    $32, %eax
        subl    %eax, %edi
        movl    $-1, %eax
        adcl    $0, %eax
        bzhi    %edi, %eax, %eax
        salq    $32, %rdi
        movl    %eax, %eax
        orq     %rdi, %rax
        ret

Here is a more optimal version of the above code (for 64-bit x86):
_Z25ComputeHiMaskAndHiZeroAmtj:
        subl    $32, %edi
        movl    $-1, %eax
        adcl    $0, %eax
        bzhi    %edi, %eax, %eax
        salq    $32, %rdi
        movl    %eax, %eax
        orq     %rdi, %rax
        ret


---


### compiler : `gcc`
### title : `GCC doesn't deduplicate string literals for const char*const and const char[]`
### open_at : `2023-02-01T09:50:53Z`
### last_modified_date : `2023-07-24T02:21:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108626
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.2.0`
### severity : `normal`
### contents :
Given the following code


----
#include <stdio.h>

static const char* const a = "bla";
static const char b[] = "bla";

int main() {
    puts(a);
    puts(b);
}
----

GCC 12.2 with -O2 produces

----
.LC0:
        .string "bla"
main:
        sub     rsp, 8
        mov     edi, OFFSET FLAT:.LC0
        call    puts
        mov     edi, OFFSET FLAT:_ZL1b
        call    puts
        xor     eax, eax
        add     rsp, 8
        ret
_ZL1b:
        .string "bla"
----

On the other hand, clang 15.0 with -O1 and higher produces

----
main:                                   # @main
        push    rbx
        lea     rbx, [rip + .L.str]
        mov     rdi, rbx
        call    puts@PLT
        mov     rdi, rbx
        call    puts@PLT
        xor     eax, eax
        pop     rbx
        ret
.L.str:
        .asciz  "bla"
----

For some reason, GCC doesn't want to combine readonly data for these two string literals.

Godbolt playground: https://godbolt.org/z/WaeazezE6


---


### compiler : `gcc`
### title : `Redundant calls to C++ spaceship operator<=> with attribute pure or const`
### open_at : `2023-02-02T08:00:40Z`
### last_modified_date : `2023-02-02T09:43:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108635
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
This code seems to have a missed optimization in GCC 10/11/12:

    #include <compare>

    struct S
    {
        std::weak_ordering operator<=>(const S&) const __attribute__((const));
    };

    int compare3way(S& a, S& b)
    {
        return (a < b) ? -1 : (a > b) ? 1 : 0;
    }

I expect operator<=> to be called once, but it is called twice.  This can be a major missed optimization if operator<=> is expensive.  It happens regardless of:

 1. Using attribute((const)) or attribute((pure)).
 2. Making operator<=> a free function or a member.
 3. Comparing (a > b) or (a < b) in the second ternary expression.  This is especially strange, because it's really calling the same pure function twice, and that's optimized correctly when the function being called is operator< instead of operator<=>.

Clang optimizes it as expected.

Demo: https://godbolt.org/z/jP51E6xaz


---


### compiler : `gcc`
### title : `no vectorization when copy constructor is present`
### open_at : `2023-02-05T14:38:37Z`
### last_modified_date : `2023-02-06T21:48:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108677
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
in this real life code

#include<cmath>

struct trig_pair {
   double CosPhi;
   double SinPhi;

   trig_pair() : CosPhi(1.), SinPhi(0.) {}
   trig_pair(const trig_pair &tp) : CosPhi(tp.CosPhi), SinPhi(tp.SinPhi) {}
   trig_pair(const double C, const double S) : CosPhi(C), SinPhi(S) {}
   trig_pair(const double phi) : CosPhi(cos(phi)), SinPhi(sin(phi)) {}
 
   //Return trig_pair fo angle increased by angle of tp.
   trig_pair Add(const trig_pair &tp) {
      return trig_pair(this->CosPhi*tp.CosPhi - this->SinPhi*tp.SinPhi,
                       this->SinPhi*tp.CosPhi + this->CosPhi*tp.SinPhi);
   }
};

trig_pair *TrigArr;

void FillTrigArr(trig_pair tp, unsigned MaxM)
{
//Fill TrigArr with trig_pair(jp*phi)
   if (!TrigArr) return;;
   TrigArr[1] = tp;
   for (unsigned jp = 2; jp <= MaxM; ++jp) TrigArr[jp] = TrigArr[jp-1].Add(tp);
}


gcc vectorize the loop even if a dependency is present...[1]
It will not if I comment out the copy contructor...[2]


[1]
https://godbolt.org/z/vhPeh35n5

[2]
https://godbolt.org/z/YPjdYdqG8


---


### compiler : `gcc`
### title : `suboptimal allocation with same memory op for many different instructions.`
### open_at : `2023-02-08T03:11:30Z`
### last_modified_date : `2023-04-23T01:57:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108707
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
#include<immintrin.h>

void
foo (__m512* pv, float* __restrict ps, int n, __m512* pdest,
__m512* p1, __m512* p2, __m512* p3)
{
    __m512 a = _mm512_setzero_ps ();
    __m512 b = a;
    __m512 c = a;
    for (int i = 0; i != n ;i++)
    {
        a = _mm512_fmadd_ps (p1[i], pv[i], a);
        b = _mm512_fmadd_ps (p2[i], pv[i], b);
        c = _mm512_fmadd_ps (p3[i], pv[i], c);
    }
    pdest[0] = a;
    pdest[1] = b;
    pdest[2] = c;
}

g++ -O2 -mavx512f -S

got 

.L3:
        vmovaps (%r8,%rax), %zmm3
        vmovaps (%r9,%rax), %zmm4
        vmovaps (%rsi,%rax), %zmm5
        vfmadd231ps     (%rdi,%rax), %zmm3, %zmm2
        vfmadd231ps     (%rdi,%rax), %zmm4, %zmm1
        vfmadd231ps     (%rdi,%rax), %zmm5, %zmm0
        addq    $64, %rax
        cmpq    %rax, %rdx
        jne     .L3

It would be better to load (%rdi, %rax) into a zmm then

.L3:
        vmovaps (%rdi,%rax), %zmm0
        vfmadd231ps     (%r8,%rax), %zmm0, %zmm3
        vfmadd231ps     (%r9,%rax), %zmm0, %zmm2
        vfmadd231ps     (%rsi,%rax), %zmm0, %zmm1
        addq    $64, %rax
        cmpq    %rax, %rdx
        jne     .L3


---


### compiler : `gcc`
### title : `Recognizing "rounding down to the nearest power of two"`
### open_at : `2023-02-08T06:59:39Z`
### last_modified_date : `2023-02-09T07:34:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108710
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
In the code

#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>

uint64_t foo (uint64_t x)
{
  x = x | (x >> 1);
  x = x | (x >> 2);
  x = x | (x >> 4);
  x = x | (x >> 8);
  x = x | (x >> 16);
  x = x | (x >> 32);
  return x - (x >> 1);
}

uint64_t bar (uint64_t x)
{
  if (x == 0)
    return 0;
  else
    return 1ul << (63 - __builtin_clzl(x));
}

void tst (uint64_t a)
{
  uint64_t r_foo, r_bar;
  r_foo = foo(a);
  r_bar = bar(a);
  printf ("%20lu %20lu %20lu\n", a, r_foo, r_bar);
  if (r_foo != r_bar)
    abort();
}

int main()
{
  tst(0ul);
  for (uint64_t i = 1; i<64; i++) {
    for (uint64_t j = 0; j<i; j++) {
      uint64_t a, b, c;
      b = 1ul << i;
      a = b - (1ul << j);
      c = b + (1ul << j);
      tst(a);
      tst(b);
      tst(c);
    }
  }
  return 0;
}

the nearest power of two, roundnihg down, is calculated, using the two
methods from chapter 3-2 in Hacker's Delight.  The method used in foo,
using only standard C, is used, for example, in the embench benchmkark.

Code for recent trunk is 

foo:
.LFB39:
        .cfi_startproc
        endbr64
        movq    %rdi, %rax
        shrq    %rax
        orq     %rax, %rdi
        movq    %rdi, %rax
        shrq    $2, %rax
        orq     %rdi, %rax
        movq    %rax, %rdi
        shrq    $4, %rdi
        orq     %rdi, %rax
        movq    %rax, %rdi
        shrq    $8, %rdi
        orq     %rax, %rdi
        movq    %rdi, %rax
        shrq    $16, %rax
        orq     %rax, %rdi
        movq    %rdi, %rax
        shrq    $32, %rax
        orq     %rdi, %rax
        movq    %rax, %rdx
        shrq    %rdx
        subq    %rdx, %rax
        ret

(same with -O3 and -Os) and for bar is

bar:
.LFB23:
        .cfi_startproc
        movq    %rdi, %rax
        testq   %rdi, %rdi
        je      .L4
        movabsq $-9223372036854775808, %rax
        bsrq    %rdi, %rcx
        xorq    $63, %rcx
        shrq    %cl, %rax
.L4:
        ret

which is more compact and should have a well-predicted branch.

It would be good for the idiom to be recognized (same as for the
round up to the nearest power of two).  Also, the number of register
moves in the original version could be removed by using more registers,
at least for -Os.


---


### compiler : `gcc`
### title : `Missing optimization with memory-barrier`
### open_at : `2023-02-08T08:33:00Z`
### last_modified_date : `2023-02-16T06:18:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108712
### status : `UNCONFIRMED`
### tags : `inline-asm, missed-optimization`
### component : `middle-end`
### version : `12.2.1`
### severity : `normal`
### contents :
In the following example the increments of `g` could be optimized to a `g+=20` equivalent.

But avr-gcc hoists the load of `g` outside the loop but the stores remain inside the loop.

That produces unneccessary overhead.

(I know that the idionatic solution would be to volatile-qualify the variable `flag` 
make a volatile-access like std::experimental::volatile_load()` or `ACCESS_ONCE()` (linux kernel).

https://godbolt.org/z/1b6xG5YP4

----
#include <stdint.h>
#include <util/atomic.h>
//#include <signal.h> // do not include that stub: wrong sig_atomic_t

typedef signed char sig_atomic_t; // __SIG_ATOMIC_TYPE__

#include <avr/interrupt.h>

static sig_atomic_t flag;
static uint8_t g; 

void func(void) {
    for(uint8_t i = 0; i < 20; i++) {
        __asm__ __volatile__ ("" : "+m" (flag));
        ++g;
        if (flag) {
            flag = 0;
        }
        __asm__ __volatile__ ("" : "+m" (flag));
     }
}

ISR(USART_RXC_vect) {
    __asm__ __volatile__ ("" : "+m" (flag));
    flag = 1;
}
----


---


### compiler : `gcc`
### title : `[11 Regression] Poor codegen when summing two arrays without AVX or SSE`
### open_at : `2023-02-08T19:17:27Z`
### last_modified_date : `2023-05-29T10:08:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108724
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
This program:

void foo(int *a, const int *__restrict b, const int *__restrict c)
{
  for (int i = 0; i < 16; i++) {
    a[i] = b[i] + c[i];
  }
}


When compiled for x86 by GCC 11.1+ with -O3 -mno-avx -mno-sse, produces:

foo:
        movq    %rdx, %rax
        subq    $8, %rsp
        movl    (%rsi), %edx
        movq    %rsi, %rcx
        addl    (%rax), %edx
        movl    4(%rax), %esi
        movq    $0, (%rsp)
        movl    %edx, (%rsp)
        movq    (%rsp), %rdx
        addl    4(%rcx), %esi
        movq    %rdx, -8(%rsp)
        movl    %esi, -4(%rsp)
        movq    -8(%rsp), %rdx
        movq    %rdx, (%rdi)
        movl    8(%rax), %edx
        addl    8(%rcx), %edx
        movq    $0, -16(%rsp)
        movl    %edx, -16(%rsp)
        movq    -16(%rsp), %rdx
        movl    12(%rcx), %esi
        addl    12(%rax), %esi
        movq    %rdx, -24(%rsp)
        movl    %esi, -20(%rsp)
        movq    -24(%rsp), %rdx
        movq    %rdx, 8(%rdi)
        [snip more of the same]
        movl    48(%rcx), %edx
        movq    $0, -96(%rsp)
        addl    48(%rax), %edx
        movl    %edx, -96(%rsp)
        movq    -96(%rsp), %rdx
        movl    52(%rcx), %esi
        addl    52(%rax), %esi
        movq    %rdx, -104(%rsp)
        movl    %esi, -100(%rsp)
        movq    -104(%rsp), %rdx
        movq    %rdx, 48(%rdi)
        movl    56(%rcx), %edx
        movq    $0, -112(%rsp)
        addl    56(%rax), %edx
        movl    %edx, -112(%rsp)
        movq    -112(%rsp), %rdx
        movl    60(%rcx), %ecx
        addl    60(%rax), %ecx
        movq    %rdx, -120(%rsp)
        movl    %ecx, -116(%rsp)
        movq    -120(%rsp), %rdx
        movq    %rdx, 56(%rdi)
        addq    $8, %rsp
        ret

(Godbolt link: https://godbolt.org/z/qq9dbP8ed)

This is bizarre - it's storing intermediate results on the stack, instead of keeping them in registers or writing them directly to *a, which is bound to be slow. (GCC 10.4, and Clang, produce more or less what I would expect, using only the provided arrays and a register.) I haven't done any benchmarking myself, but Jonathan Wakely's results (on list: https://gcc.gnu.org/pipermail/gcc-help/2023-February/142181.html) seem to bear this out.

From a bisect, this behavior seems to have been introduced by commit 33c0f246f799b7403171e97f31276a8feddd05c9 (tree-optimization/97626 - handle SCCs properly in SLP stmt analysis) from Oct 2020, and persists into GCC trunk.


---


### compiler : `gcc`
### title : `Removing dead code results in worse generated target code at -Os`
### open_at : `2023-02-10T10:25:04Z`
### last_modified_date : `2023-02-10T17:03:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108751
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
I found this case where slight changes in the program that, in theory, should not affect the output (or affect it trivially) cause the compiler to generate worse code: 

static int a = 0;
static int b = 1;
int main() {
  char c = 0;
  for (;;) {
    if (c)
      break;
    for (; a; a++) { // a is 0, this loop is dead
      if (b) // this is always true
        continue;
      else
        return 2; // this program will never return 2
    }
    c = 10;
  }
  return 3;
}

compiled with gcc-trunk -Os: 

main:
.L2:
        movl    a(%rip), %eax
        testl   %eax, %eax
        je      .L6
        incl    %eax
        movl    %eax, a(%rip)
        jmp     .L2
.L6:
        movl    $3, %eax
        ret

Clearly, the compiler has figured out that "return 2;" will never be executed. But if I remove it from the source:

static int a = 0;
static int b = 1;
int main() {
  char c = 0;
  for (;;) {
    if (c)
      break;
    for (; a; a++) {
      if (b)
        continue;
      //else
      // return 2;
    }
    c = 10;
  }
  return 3;
}

and compile with gcc-trunk -Os again:

main:
        movl    a(%rip), %eax
        xorl    %edx, %edx
.L2:
        testl   %eax, %eax
        jne     .L4
        testb   %dl, %dl
        je      .L7
        xorl    %eax, %eax
        movl    %eax, a(%rip)
        jmp     .L7
.L4:
        incl    %eax
        movb    $1, %dl
        jmp     .L2
.L7:
        movl    $3, %eax
        ret

the generated code is worse. 

The same thing happens if the return value is changed:

static int a = 0;
static int b = 1;
int main() {
  char c = 0;
  for (;;) {
    if (c)
      break;
    for (; a; a++) {
      if (b)
        continue;
      else
        return 2;
    }
    c = 10;
  }
  return 1; // changed from return 3
}

gcc-trunk -Os: 

main:
        movl    a(%rip), %eax
        xorl    %edx, %edx
.L2:
        testl   %eax, %eax
        jne     .L4
        testb   %dl, %dl
        je      .L7
        xorl    %eax, %eax
        movl    %eax, a(%rip)
        jmp     .L7
.L4:
        incl    %eax
        movb    $1, %dl
        jmp     .L2
.L7:
        movl    $1, %eax
        ret

and if we constant propagate b:

static int a = 0;
int main() {
  char c = 0;
  for (;;) {
    if (c)
      break;
    for (; a; a++) {
      if (1) // this was if (b) before
        continue;
      else
        return 2;
    }
    c = 10;
  }
  return 1;
}

gcc-trunk -Os:

main:
        movl    a(%rip), %eax
        xorl    %edx, %edx
.L2:
        testl   %eax, %eax
        jne     .L12
        testb   %dl, %dl
        je      .L7
        xorl    %eax, %eax
        movl    %eax, a(%rip)
        jmp     .L7
.L12:
        incl    %eax
        movb    $1, %dl
        jmp     .L2
.L7:
        movl    $1, %eax
        ret


---


### compiler : `gcc`
### title : `word_mode vectorization is pessimized by hard limit on nunits`
### open_at : `2023-02-10T11:24:30Z`
### last_modified_date : `2023-05-23T16:10:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108752
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
r13-5771-gdc87e1391c55c6 re-introduced a hard nunits limit to the vectorizer when using emulated vectors (aka word_mode vectorization).  That's because
this feature relies on vector lowering to implement plus, minus and negate
with bit operations and that has such limit in place for when dealing with
user written code that didn't have any cost modeling applied.

The fix is to emit supported operations from the vectorizer.


---


### compiler : `gcc`
### title : `Unnecessary instruction`
### open_at : `2023-02-10T22:00:10Z`
### last_modified_date : `2023-02-10T22:07:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108756
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `normal`
### contents :
You can test this on godbolt. Using gcc 12.2 on x86-64 linux

There appears to be an unnecessary instruction. I commented the assembly below

	struct T2 { bool a, b; };
	static T2 test();
	int myfunc() {
		auto [a, b] = test();
		return ((int)a<<1) + b;
	}

Result

	myfunc():
		sub     rsp, 8
		call    test()  <-- Result is b<<8 | a
		add     rsp, 8
		movzx   edx, al <--- moves a
		movzx   eax, ah <--- extends b into itself
		movzx   eax, al <--- Huh?
		lea     eax, [rax+rdx*2]
		ret


---
