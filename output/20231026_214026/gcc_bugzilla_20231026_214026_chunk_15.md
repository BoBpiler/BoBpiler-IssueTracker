### Total Bugs Detected: 4649
### Current Chunk: 15 of 30
### Bugs in this Chunk: 160 (From bug 2241 to 2400)
---


### compiler : `gcc`
### title : `[missed optimization] Missing optimization in unaliased pointers`
### open_at : `2019-07-05T05:03:51Z`
### last_modified_date : `2019-12-21T15:16:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91091
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `5.4.0`
### severity : `normal`
### contents :
Consider the following:

    struct s { int x; };
    struct t { int x; };

    void swap(struct s* p, struct t* q) {
        p->x = q->x;
        q->x = p->x;
    }


Aliasing rules forbid `p` and `q` to point to the same object; yet, GCC 5.4 and most subsequent versions produce (-O3):

    swap(s*, t*):
            mov     eax, DWORD PTR [rsi]
            mov     DWORD PTR [rdi], eax
            mov     DWORD PTR [rsi], eax // Possible alias between p and q
            ret

whereas GCC versions 4.5.3 to 5.3 and versions 8.1 to 8.2 correctly produce:

    swap(s*, t*):
            mov     eax, DWORD PTR [rsi]
            mov     DWORD PTR [rdi], eax
            ret

All versions produce the correct code if __restrict__ is used on any pointer.

This behavior can be verified on Godbolt: https://godbolt.org/z/WYMoFI


---


### compiler : `gcc`
### title : `BB vectorization is too quick to disable itself because of possible unrolling needed`
### open_at : `2019-07-05T11:56:54Z`
### last_modified_date : `2021-08-04T10:17:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91094
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
long long a[1024];
int b[1024];

void foo()
{
  a[0] = b[0] + b[2];
  a[1] = b[1] + b[3];
#if WORKS
  a[2] = b[4] + b[6];
  a[3] = b[5] + b[7];
#endif
}


The above is not vectorized fully (we vectorize the store) if !WORKS because

t2.c:12:1: missed:   Build SLP failed: unrolling required in basic block SLP

which checks group_size (2) against the vector(4) int number of elements.
It works fine with WORKS because then group_size is 4.

A similar issue prevents SPEC x264 from being vectorized optimally.
Testcase from that:

typedef unsigned int uint32_t;
typedef unsigned char uint8_t;
#define HADAMARD4(d0, d1, d2, d3, s0, s1, s2, s3) {\
    int t0 = s0 + s1;\
    int t1 = s0 - s1;\
    int t2 = s2 + s3;\
    int t3 = s2 - s3;\
    d0 = t0 + t2;\
    d2 = t0 - t2;\
    d1 = t1 + t3;\
    d3 = t1 - t3;\
}

uint32_t tmp[4][4];
__attribute__ ((noinline,noclone))
void x264_pixel_satd_8x4( uint8_t *pix1, int i_pix1, uint8_t *pix2, int i_pix2 )
{
    uint32_t a0, a1, a2, a3;
    for( int i = 0; i < 4; i++, pix1 += i_pix1, pix2 += i_pix2 )
    {
        a0 = (pix1[0] - pix2[0]) + ((pix1[4] - pix2[4]) << 16);
        a1 = (pix1[1] - pix2[1]) + ((pix1[5] - pix2[5]) << 16);
        a2 = (pix1[2] - pix2[2]) + ((pix1[6] - pix2[6]) << 16);
        a3 = (pix1[3] - pix2[3]) + ((pix1[7] - pix2[7]) << 16);
        HADAMARD4( tmp[i][0], tmp[i][1], tmp[i][2], tmp[i][3], a0,a1,a2,a3 );
    }
}


---


### compiler : `gcc`
### title : `AVX512 vector element extract uses more than 1 shuffle instruction; VALIGND can grab any element`
### open_at : `2019-07-07T05:27:37Z`
### last_modified_date : `2023-07-12T11:20:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91103
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
GCC9.1 and current trunk aren't good at extracting high elements, whether it's with GNU C native vector syntax, or when auto-vectorizing something that ends with the result in the high element.

Using VALIGND we can get any element with one immediate instruction, but its better to use AVX2 VPERMPD(immediate) when possible.  Or inside loops, VPERMPS(vector), or VPERMT2PS(vector).  Or of course vextractf32x4 if possible (element at the bottom of a 128-bit lane).

Or with only AVX2 available, VPERMPD(immediate) for high elements in __m256 and __m256d vectors is still a big win.

#include <immintrin.h>
float elem12(__m512 v) {  return v[12]; }
float elem15(__m512 v) {  return v[15]; }

gcc -Ofast -march=skylake-avx512
https://godbolt.org/z/241r8p

elem15:
        vextractf32x8   ymm0, zmm0, 0x1
        vextractf128    xmm0, ymm0, 0x1    # elem12 ends here, after these 2 insns
        vshufps xmm0, xmm0, xmm0, 255
         # no vzeroupper I guess because the caller must have __m512 vars too, recent optimization
        ret

But AVX512F has vextractf32x4 to extract a 128-bit lane, which would preclude the need for AVX2 vextractf128.  That's what clang does.

Obviously inside a loop it would be *much* better to use a single lane-crossing VPERMPS to also avoid the shufps.  Intel Skylake easily bottlenecks on shuffle throughput.  We'd need a 15 in an XMM register as a control vector, but loading it would be off the latency critical path.  (If we needed the scalar zero-extended instead of garbage in high elements, we could VPERMI2PS or VPERMT2PS with a zeroed vector and a shuffle-control.)

---

If the element we want is an even element in the low 256 bits, we can get it with a VPERMPD-immediate.  GCC does this:

elem6(float __vector(16)):         # GCC 10 trunk
        vextractf128    xmm0, ymm0, 0x1
        vunpckhps       xmm0, xmm0, xmm0
        ret

Instead it should be AVX2   vpermpd ymm0, ymm0, 3
This bug also applies to __m256, not just __m512

https://www.felixcloutier.com/x86/vpermpd
VPERMPD is a 64-bit granularity lane-crossing shuffle.  The AVX512F immediate version reuses the immediate for another 256-bit wide shuffle in the upper half; only the vector-control version can bring an element from the top half of a ZMM down to the bottom.  But if we're going to use a vector control, we might as well use VPERMPS.

For the integer version of this bug, use VPERMQ

------

But we can do even better by using an integer VALIGND (AVX512F) shuffle on FP data.  There unfortunately isn't an FP flavour of VALIGND, just integer.

AFAIK, Skylake-AVX512 still has no bypass-delay penalty for integer shuffles between FP math instructions, i.e. the shift unit is connected to both FP and integer forwarding networks.  Intel's optimization manual for Skylake (client) has a bypass-latency table that shows 0 extra latency cycles for SHUF/5/1,3 reading from anything, or anything reading from it.

https://www.felixcloutier.com/x86/valignd:valignq  It's a 4 or 8-byte granularity version of palignr, except that it's lane-crossing so the 256 and 512-bit versions are actually useful.  The immediate shift count can thus bring *any* element down to the bottom.  (Using the same input twice makes it a rotate).

VALIGND is good on Knight's Landing, too: unlike most 2-input shuffles, it has 1 per clock throughput.

For *any* compile-time-constant index, we can always compile v[i] to this:

extract15:
   valignd    zmm0, zmm0, zmm0, 15       # I think this is right.
   ret

The only downside I'm aware of is that some future AVX512 CPU might not run VALIGND as efficiently as SKX and KNL.


----

For vector elements narrower than 32 bits, we may need 2 shuffles even if we consider using a shuffle-control vector.  On Skylake-AVX512,  AVX512BW  vpermw  will get the job done, but costs 2 shuffle uops.  On CannonLake (and presumably other future Intel), it and  AVX512VBMI vpermb are only 1 uop, so it's definitely worth creating a shuffle-control vector if it can be reused.


Also worth considering instead of 2 shuffles: *unaligned* spill / reload like ICC does for GNU C native vector indexing.  Store-forwarding latency is only 6 or 7 cycles I think, and it avoids any port 5 pressure.  Not generally a good choice IMO when we can get the job done in one shuffle, but worth considering if we need multiple elements.  If the function doesn't need the stack aligned, an unaligned spill is generally cheapish, and store-forwarding still works efficiently.

IceLake is supposed to introduce a 2nd shuffle unit; that should help a lot to reduce shuffle port throughput bottlenecks.  So we don't want to get too aggressive tuning for store/reload, I don't think.

----

Semi-related for integer shuffles:

long long integer_extract(__m256i v) {
    return v[3];
}

uses a longer AVX512VL instruction instead of a shorter AVX2 vextracti128

integer_extract(long long __vector(4)):
        vextracti64x2   xmm0, ymm0, 0x1      # should be vextracti128
        vpextrq rax, xmm0, 1
        ret

Or store/reload instead of these 2 shuffles, depending on context.


---


### compiler : `gcc`
### title : `bad register choices for rs6000 -m32`
### open_at : `2019-07-08T20:48:01Z`
### last_modified_date : `2020-01-28T18:15:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91116
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
In the new testcase pr88233.c, which is

typedef struct { double a[2]; } A;
A
foo (const A *a)
{
  return *a;
}

we currently get as generated code for -m32

        addi 10,4,4
        lfiwzx 10,0,4
        addi 9,3,12
        lfiwzx 11,0,10
        addi 10,4,8
        lfiwzx 12,0,10
        addi 10,4,12
        stfiwx 10,0,3
        lfiwzx 0,0,10
        addi 10,3,4
        stfiwx 11,0,10
        addi 10,3,8
        stfiwx 12,0,10
        stfiwx 0,0,9
        blr


Expand decides to do this as four SImode copies, which isn't such a great
idea, of course; but RA thinks it is cost 0 to put a SImode in an FP or
altivec register.  That won't fly.


---


### compiler : `gcc`
### title : `_mm_movpi64_epi64/_mm_movepi64_pi64 generating store+load instead of using MOVQ2DQ/MOVDQ2Q`
### open_at : `2019-07-08T22:31:41Z`
### last_modified_date : `2020-01-27T13:14:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91117
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
_mm_movpi64_epi64 is never using MOVQ2DQ (and _mm_movepi64_pi64 never using MOVDQ2Q) despite documentation it should when used in mixed MMX -> SSE situations, and that these are in fact the intrinsics to use when desiring the Q2DQ/DQ2Q opcodes.

This appears to be due to the header defining them causing fallback memory write then read except in (technically invalid) SSE -> SSE cases where a MOVD is used.

Tested on GCC 7.4 + 9.1 locally, with additional testing on Godbolt all showing identical code being generated all the way back to 4.x series.

Compiled with -O1:

#include <emmintrin.h>

__m128i test( __m128i input ) {
	__m64 x = _mm_movepi64_pi64( input );
	return _mm_movpi64_epi64( _mm_mullo_pi16( x, x ) );
}

Generated assembly on GCC 9.1:

	movq    %xmm0, -16(%rsp)
	movq    -16(%rsp), %mm0
	movq    %mm0, %mm1
	pmullw  %mm0, %mm1
	movq    %mm1, -16(%rsp)
	movq    -16(%rsp), %xmm0
	ret

A version that makes explicit calls to movq2dq/movdq2q works and outputs the expected assembly sequence:

#include <emmintrin.h>

static inline __m64 _my_movepi64_pi64( __m128i input ) {
        __m64 result;
        asm( "movdq2q %1, %0" : "=y" (result) : "x" (input) : );
        return result;
}

static inline __m128i _my_movpi64_epi64( __m64 input ) {
        __m128i result;
        asm( "movq2dq %1, %0" : "=x" (result) : "y" (input) : );
        return result;
}

__m128i test( __m128i input ) {
	__m64 x = _my_movepi64_pi64( input );
	return _my_movpi64_epi64( _mm_mullo_pi16( x, x ) );
}

Generated assembly on GCC 7.4, 9.1, and others via Godbolt, again with -O1 (-O2 and -O3 make no difference):

	movdq2q %xmm0, %mm0
	pmullw  %mm0, %mm0
	movq2dq %mm0, %xmm0
	ret

For completeness, ICC generates the 'short' code form on all available versions without needing the inline assembly workaround.


---


### compiler : `gcc`
### title : `[10 Regression] no longer removes redundant store`
### open_at : `2019-07-09T11:56:51Z`
### last_modified_date : `2020-02-04T09:11:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91123
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Since r273135 FRE no longer removes the redundant store in

struct X { int i; int j; };

struct X x, y;
void foo ()
{
  x.i = 1;
  y = x;
  y.i = 1; // redundant
}

which is because of

static void *
vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *data_,
                       bool *disambiguate_only)
{     
...
  /* If we are looking for redundant stores do not create new hashtable
     entries from aliasing defs with made up alias-sets.  */
  if (*disambiguate_only || !data->tbaa_p)
    return (void *)-1;

I have added this check because we're doing

      val = vn_reference_lookup (lhs, gimple_vuse (stmt), VN_WALKREWRITE,
                                 &vnresult, false);
...
          /* We can only remove the later store if the former aliases
             at least all accesses the later one does or if the store
             was to readonly memory storing the same value.  */
          alias_set_type set = get_alias_set (lhs);
          if (! vnresult
              || vnresult->set == set
              || alias_set_subset_of (set, vnresult->set))

and vn_reference_lookup_3 inserts expressions (vnresult) using the
lookup alias-set but here we're looking for the dynamic type before
the store we're looking up.


---


### compiler : `gcc`
### title : `Bad bitfield coalescing`
### open_at : `2019-07-10T13:10:15Z`
### last_modified_date : `2023-07-19T04:05:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91131
### status : `RESOLVED`
### tags : `missed-optimization, wrong-code`
### component : `middle-end`
### version : `8.3.0`
### severity : `normal`
### contents :
In both C and Ada, bitfield coalescing does not work right, or at least not optimal. Since the issue manifests itself in the same way in both C and Ada front-ends, and on ARM, AVR and X86_64 back-ends, I presume that that the issue must lie somewhere in between. Please see past that C bitfields are defective by definition; Ada's definition is pretty well-defined.
Seen on GCC version 4.9.3, 8.3.0 and reportedly also on 9.1.

The key point below is that all assignments have a data-width of less or equal the machines data-width, and can be fully evaluated at compile time.

Pseudo-code:
type Rec_Type record
  A: 3 bits
  B: 1 bit
  C: 4 bits
end (packed, 8-bit wide)

volatile Rec_A : Rec_Type
volatile Rec_B : Rec_Type
volatile Rec_C : Rec_Type

Rec_A := (A => 0, B => 1, C => 0)
results in one pre-calculated constant byte being assigned directly. This is the way all such assignments should be.

Rec_B := (A => 1, B => 1, C => 1)
results in one pre-calculated value being stored in a memory location, which is then ĺoaded and assigned to Rec_B. Unnecessary memory consumption.

Rec_C := (A => 0, B => 0, C => 0)
results in Read, Modify, Write for each field (3 times). Horrible, and may cause defective execution if Reg_C is a peripheral register! Reading a register does not always give the last value written to it. Also Writing bits in "random" order may cause malfunction of the device.

In Ada the Reg_C problem can be mitigated by using pragma Atomic instead, this still causes a memory location to be used for the evaluated constant that could just as well be loaded as an immediate.

I've uploaded sources, preprocessed file and Makefile/GPR to this location: http://knaldgas.dk/~pdj/bitfields/ - The c-file is just compiled with gcc -O<whatever> -c main.c

Pretty odd that three similar assignments can give three different kinds of code generation...


---


### compiler : `gcc`
### title : `strlen of conditional plus index in known range not folded`
### open_at : `2019-07-11T23:21:59Z`
### last_modified_date : `2022-03-17T19:57:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91147
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The strlen optimization in get_range_strlen in gimple-fold.c successfully handles the case in f() below but misses the equivalent one in g().

$ cat b.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout b.c
const char a3[] = "123";
const char a5[] = "12345";

void f (int i, int j)
{
  if (j < 1)
    j = 1;

  unsigned n;
  if (i)
    n = __builtin_strlen (a3 + j);
  else
    n = __builtin_strlen (a5 + j);

  if (n > 4)                          // folded to false
    __builtin_abort ();
}

void g (int i, int j)
{
  if (j < 1)
    j = 1;

  const char *p = i ? a3 : a5;

  if (__builtin_strlen (p + j) > 4)   // not folded but could be
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1911, cgraph_uid=1, symbol_order=2)

f (int i, int j)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1916, cgraph_uid=2, symbol_order=3)

Removing basic block 3
g (int i, int j)
{
  sizetype _1;
  const char * _2;
  long unsigned int _3;
  const char * iftmp.4_4;

  <bb 2> [local count: 1073741824]:
  j_6 = MAX_EXPR <j_5(D), 1>;
  if (i_7(D) != 0)
    goto <bb 4>; [50.00%]
  else
    goto <bb 3>; [50.00%]

  <bb 3> [local count: 536870912]:

  <bb 4> [local count: 1073741824]:
  # iftmp.4_4 = PHI <&a3(2), &a5(3)>
  _1 = (sizetype) j_6;
  _2 = iftmp.4_4 + _1;
  _3 = __builtin_strlen (_2);
  if (_3 > 4)
    goto <bb 5>; [0.00%]
  else
    goto <bb 6>; [100.00%]

  <bb 5> [count: 0]:
  __builtin_abort ();

  <bb 6> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `[10 Regression] 456.hmmer regression on Haswell caused by r272922`
### open_at : `2019-07-12T18:12:11Z`
### last_modified_date : `2021-08-07T01:58:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91154
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
https://gcc.opensuse.org/gcc-old/SPEC/CINT/sb-czerny-head-64-2006/456_hmmer_big.png

shows two recent regressions, one around June 13th and one around Jul 3th.


---


### compiler : `gcc`
### title : `[SVE] Unfolded ZIPs of constants`
### open_at : `2019-07-15T08:53:53Z`
### last_modified_date : `2019-07-27T09:54:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91166
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Compiling this testcase with -O3 for -march=armv8.2-a+sve:

void
foo (double x[][4])
{
  for (int i = 0; i < 4; ++i)
    for (int j = 0; j < 4; ++j)
      x[i][j] = 0;
}

creates redundant ZIPs of constants:

foo:
.LFB0:
        .cfi_startproc
        addvl   x3, x0, #1
        mov     x1, 0
        mov     x2, 16
        mov     z0.b, #0
        mov     x4, x2
        zip1    z0.d, z0.d, z0.d     // Zero
        uqdecd  x4
        zip1    z1.d, z0.d, z0.d     // Zero
        whilelo p1.d, xzr, x2
        zip2    z0.d, z0.d, z0.d     // Zero
        whilelo p0.d, xzr, x4
.L2:
        st1d    z1.d, p1, [x0, x1, lsl 3]
        st1d    z0.d, p0, [x3, x1, lsl 3]
        incw    x1
        whilelo p0.d, x1, x4
        whilelo p1.d, x1, x2
        b.any   .L2
        ret
        .cfi_endproc

A similar problem occurs for variables:

void
foo (double x[][4], double y)
{
  for (int i = 0; i < 4; ++i)
    for (int j = 0; j < 4; ++j)
      x[i][j] = y;
}

This can be fixed by folding a VEC_PERM_EXPR in which the first two
inputs are both duplicates of the same value.


---


### compiler : `gcc`
### title : `Suboptimal code for arithmetic with bool and char`
### open_at : `2019-07-15T14:28:55Z`
### last_modified_date : `2019-07-16T09:20:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91174
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Consider the example:

int test (bool x) {
    return '0' + x;
}


For the above snippet the following suboptimal assembly is generated:


test(bool):
  movzx eax, dil
  add eax, 48
  ret


More efficient assembly would be:

test(bool):
  lea eax, [rdi + 48]
  ret


---


### compiler : `gcc`
### title : `strlen of a strcpy result with a conditional source not folded`
### open_at : `2019-07-16T17:54:43Z`
### last_modified_date : `2019-10-15T21:35:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91183
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
While testing a solutuion for pr91147 I noticed that in the following GCC folds the strlen call in f() into a constant but it doesn't know how to do that for the second strlen call.

$ cat a.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout a.c
void f (int i)
{
  if (__builtin_strlen (i ? "123" : "456") != 3)   // folded to false
    __builtin_abort ();
}

void g (int i)
{
  char a[4];
  __builtin_strcpy (a, i ? "123" : "456");   // transformed to MEM_REF
  if (3 != __builtin_strlen (a))             // not folded but could be
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1908, cgraph_uid=1, symbol_order=0)

f (int i)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1911, cgraph_uid=2, symbol_order=1)

Removing basic block 3
g (int i)
{
  char a[4];
  long unsigned int _1;
  unsigned int _4;

  <bb 2> [local count: 1073741824]:
  if (i_3(D) != 0)
    goto <bb 3>; [50.00%]
  else
    goto <bb 4>; [50.00%]

  <bb 3> [local count: 536870912]:

  <bb 4> [local count: 1073741824]:
  # _4 = PHI <3552564(2), 3355185(3)>
  MEM <unsigned int> [(char * {ref-all})&a] = _4;
  _1 = __builtin_strlen (&a);
  if (_1 != 3)
    goto <bb 5>; [0.00%]
  else
    goto <bb 6>; [100.00%]

  <bb 5> [count: 0]:
  __builtin_abort ();

  <bb 6> [local count: 1073741824]:
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `[10 regression] incorrect may be used uninitialized smw (272711, 273474]`
### open_at : `2019-07-18T08:03:50Z`
### last_modified_date : `2019-11-21T07:20:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91195
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
r272711 PASS
r273474 FAIL
r273563 FAIL

$ ~/arch-gcc/gcc_272711/bin/gcc -Werror=maybe-uninitialized -O2 -c x.c -DFOUR
$ ~/arch-gcc/gcc_273474/bin/gcc -Werror=maybe-uninitialized -O2 -c x.c
$ ~/arch-gcc/gcc_273474/bin/gcc -Werror=maybe-uninitialized -O2 -c x.c -DFOUR
x.c: In function ‘foo’:
x.c:15:13: error: ‘Msg[0]’ may be used uninitialized in this function [-Werror=maybe-uninitialized]
   15 |  Msg[num++] = m1;
      |  ~~~~~~~~~~~^~~~
cc1: some warnings being treated as errors

$ cat x.c
int f(char*);

#if defined FOUR
#define FOR_UP_LIMIT 4 // FAIL
#else
#define FOR_UP_LIMIT 3 // PASS
#endif

void foo(char *m1, char* m2)
{
    char* Msg[2];
    int num = 0;

    if(m1)
	Msg[num++] = m1;
    if(m2)
	Msg[num++] = m2;

    for (int j = 0; j < FOR_UP_LIMIT; j++) 
    switch (j) {
	case 0:
	    if(num == 0 || f(Msg[0]) )
		break;
    }
}


---


### compiler : `gcc`
### title : `GCC not generating AVX-512 compress/expand instructions`
### open_at : `2019-07-18T12:08:51Z`
### last_modified_date : `2021-12-20T05:16:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91198
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
We have a simple loop to select values based on a condition from one array and store the selected values contiguously in a second array:


https://godbolt.org/z/T7UXXD
================================
float const threshold = 0.5; 
int o = 0;
for (int i = 0; i < size; ++i) {
  if (input[i] < threshold) {
    output[o] = input[i];
    o++;
  } 
}
================================

It seems like GCC is not able to generate AVX-512 assembly using vcompressps instructions for this code. The same holds true for the orthogonal pattern (expansion using vexpandps). Is this a missed optimization in GCC or is there another issue in the example code which prevents vectorization?


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] SIMD not generated for horizontal sum of bytes in array`
### open_at : `2019-07-18T15:54:49Z`
### last_modified_date : `2023-07-07T10:35:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91201
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `normal`
### contents :
For this code —

    typedef unsigned long long E;
    const unsigned D = 2;
    E bytes[D];
    unsigned char sum() 
    {
        E b[D]{};
        //#pragma omp simd
        for(unsigned n=0; n<D; ++n)
        {
            E temp = bytes[n];
            temp += (temp >> 32);
            temp += (temp >> 16);
            temp += (temp >> 8);
            b[n] = temp;
        }
        E result = 0;
        //#pragma omp simd
        for(unsigned n=0; n<D; ++n) result += b[n];
        return result;
    }

GCC 6.4 generates the following neat assembler code, but all versions since GCC 7 (including GCC 9.1) fail to utilize SIMD instructions at all.

        vmovdqa xmm0, XMMWORD PTR bytes[rip]
        vpsrlq  xmm1, xmm0, 32
        vpaddq  xmm1, xmm1, xmm0
        vpsrlq  xmm0, xmm1, 16
        vpaddq  xmm1, xmm0, xmm1
        vpsrlq  xmm0, xmm1, 8
        vpaddq  xmm0, xmm0, xmm1
        vpsrldq xmm1, xmm0, 8
        vpaddq  xmm0, xmm0, xmm1
        vmovq   rax, xmm0
        ret

The code that GCC versions since 7.0, including and up to 9.1, generates, is:

        mov     rcx, QWORD PTR bytes[rip]
        mov     rdx, QWORD PTR bytes[rip+8]
        mov     rax, rcx
        shr     rax, 32
        add     rcx, rax
        mov     rax, rcx
        shr     rax, 16
        add     rcx, rax
        mov     rax, rdx
        shr     rax, 32
        add     rdx, rax
        mov     rax, rdx
        shr     rax, 16
        add     rdx, rax
        mov     rax, rcx
        shr     rax, 8
        add     rcx, rdx
        add     rcx, rax
        shr     rdx, 8
        lea     rax, [rcx+rdx]
        ret

Tested using compiler options -Ofast -std=c++17 -pedantic -Wall -Wextra -W -march=skylake. Tried also haswell, broadwell and znver1 for the -march option.

If I change the D constant to a larger one, such as 4 or 8, then SIMD instructions will begin appearing. Interestingly with D=4, it uses stack as a temporary, but with D=8, it manages without (on both AVX and non-AVX code).

If I uncomment the two OpenMP pragmas, then SIMD code will manifest, so it is clear that the compiler _can_ generate the optimal code, but for some reason chooses not to.

The testcase is a horizontal sum of all bytes in an array by the way.

Compiler Explorer link for quick testing: https://godbolt.org/z/azkXiL


---


### compiler : `gcc`
### title : `Unnecessary promotion of shift operands`
### open_at : `2019-07-18T17:58:15Z`
### last_modified_date : `2023-02-17T18:08:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91202
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.1.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
unsigned char
foo (unsigned char a, unsigned char b)
{
  return a >> b;
}
--cut here--

does not need its operand to be promoted to int on targets that have shift instruction in QI and HI mode.

Currently, gcc compiles the testcase (-O2) via _.original:

--cut here--
;; Function foo (null)
;; enabled by -tree-original


{
  return (unsigned char) ((int) a >> (int) b);
}
--cut here--

and ._optimized:

;; Function foo (foo, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=0)

--cut here--
foo (unsigned char a, unsigned char b)
{
  int _1;
  int _2;
  int _3;
  unsigned char _6;

  <bb 2> [local count: 1073741824]:
  _1 = (int) a_4(D);
  _2 = (int) b_5(D);
  _3 = _1 >> _2;
  _6 = (unsigned char) _3;
  return _6;

}
--cut here--

to:
        movzbl  %dil, %eax
        movl    %esi, %ecx
        sarl    %cl, %eax
        ret

However, on targets that provide QImode shift, the above code could be emitted as:

        movzbl  %dil, %eax
        movl    %esi, %ecx
        shrb    %cl, %al
        ret

This optimization would help the following testcase:

--cut here--
struct S1
{
  unsigned char val;
  unsigned char pad1;
  unsigned short pad2;
};

struct S1
test_shrb (struct S1 a, unsigned char b)
{
  a.val >>= b;

  return a;
}
--cut here--

that currently compiles to:

        movzbl  %dil, %edx
        movl    %esi, %ecx
        movl    %edi, %eax
        sarl    %cl, %edx
        movb    %dl, %al
        ret

but after PR91188 was fixed, the testcase could be compiled with the proposed optimization to:

        movl    %edi, %eax
        movl    %esi, %ecx
        shrb    %cl, %al
        ret

Please note that MSVC v19.21 implements the proposed optimization and generates:

        movzx   eax, cl
        movzx   ecx, dl
        shr     al, cl
        ret     0


---


### compiler : `gcc`
### title : `Missed optimization: (sub X Y) -> (xor X Y) when Y <= X and isPowerOf2(X + 1)`
### open_at : `2019-07-19T15:46:14Z`
### last_modified_date : `2022-08-31T09:36:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91213
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Proof https://rise4fun.com/Alive/Xr3

unsigned foo(unsigned x)
{
    if (x > 31) __builtin_unreachable();
    return 31 - x;
}

unsigned bar(unsigned x)
{
    if (x > 63) __builtin_unreachable();
    return 63 - x;
}

The optimization seems to be manually applied to __builtin_clz, and because
GCC cannot reverse it or apply again the:

unsigned bsrl(unsigned x)
{
    return 31 - __builtin_clz(x);
}

becomes (sub 31 (xor 31 (bsr x))):

bsrl(unsigned int):
  bsrl %edi, %edi
  movl $31, %eax
  xorl $31, %edi
  subl %edi, %eax
  ret


https://godbolt.org/z/3nzi0z


---


### compiler : `gcc`
### title : `[9 Regression] Returning std::array from lambda results in an extra copy step on return`
### open_at : `2019-07-20T10:06:39Z`
### last_modified_date : `2022-05-13T18:03:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91217
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.1.0`
### severity : `normal`
### contents :
It appears that for GCC versions later than or equal to version 8.1, an extra copy procedure when an std::array is being returned from a lambda. The problem does not exist for an immediately created function object.

Not affected: GCC 7.4
Affected: GCC 8.1 up to and including trunk.


---


### compiler : `gcc`
### title : `pointer relational expression not folded but equivalent inequality is`
### open_at : `2019-07-22T16:01:55Z`
### last_modified_date : `2021-11-24T05:39:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91227
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC folds the pointer inequality in function f below to false but it doesn't fold the equivalent relational expression in g.

$ cat b.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout b.c
void f (char *p)
{
  char a[3];
  if (p == a || p == a + 1 || p == a + 2 || p == a + 3)
    __builtin_abort ();
}


void g (char *p)
{
  char a[3];
  if (p >= a || p <= a + 3)
    __builtin_abort ();
} 

;; Function f (f, funcdef_no=0, decl_uid=1908, cgraph_uid=1, symbol_order=0)

f (char * p)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1912, cgraph_uid=2, symbol_order=1)

g (char * p)
{
  char a[3];
  _Bool _5;
  _Bool _6;
  _Bool _7;

  <bb 2> [local count: 1073741824]:
  _5 = &MEM <char[3]> [(void *)&a + 3B] >= p_1(D);
  _6 = &a <= p_1(D);
  _7 = _5 | _6;
  if (_7 != 0)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `Missed optimization: division and multiplying ops in ffast-math mode`
### open_at : `2019-07-24T12:12:55Z`
### last_modified_date : `2019-07-24T13:53:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91249
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
For this code:

float foo(float x, float y)
{
    return x * y/y;
}


gcc(trunk,9,8,7,6,5) with '-O3 -ffast-math' produces this:


foo(float, float):
        mulss   xmm0, xmm1
        divss   xmm0, xmm1
        ret


clang(trunk) with '-O3 -ffast-math' produces this:


foo(float, float):                               # @foo(float, float)
        ret


Notes: playground on godbolt - https://godbolt.org/z/Qjr3OD


---


### compiler : `gcc`
### title : `Missed optimization: is not used vfnmsub213ss`
### open_at : `2019-07-24T12:21:51Z`
### last_modified_date : `2021-08-10T23:13:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91250
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
For this code:

float foo(float a, float b, float c)
{
    return a * -b - c;
}


gcc(trunk) with '-O3 -ffast-math -march=haswell' produces this:


foo(float, float, float):
        vfmadd132ss     xmm0, xmm2, xmm1
        vxorps  xmm0, xmm0, XMMWORD PTR .LC0[rip]
        ret


clang (trunk) with '-O3 -ffast-math -march=haswell' produces this:


foo(float, float, float):
        vfnmsub213ss    xmm0, xmm1, xmm2 # xmm0 = -(xmm1 * xmm0) - xmm2
        ret


Note: playground on godbolt - https://godbolt.org/z/NTMVdg


---


### compiler : `gcc`
### title : `LTO disables linking with scalar MASS library (Fortran only)`
### open_at : `2019-07-29T12:58:19Z`
### last_modified_date : `2022-06-30T18:40:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91287
### status : `RESOLVED`
### tags : `lto, missed-optimization`
### component : `lto`
### version : `10.0`
### severity : `normal`
### contents :
The MASS (Mathematical Acceleration Subsystem) libraries provide alternatives to math functions in libm.  We've discovered that when compiling a Fortran program with -flto, the program is linked with libm rather than with the MASS functions, leading to a degradation in performance versus not using -flto.

luoxhu@genoa test_mass $ cat hellofortran.f90
recursive subroutine square_cube(i,isquare,icube)
  integer, intent(in)  :: i             ! input
  integer, intent(out) :: isquare,icube ! output
  !square = i**2
  isquare = atan2 (real(i), real(2))
  icube   = i**3
  if(i == 8) return
  call square_cube(i+1,isq,icub)
end subroutine square_cube

program xx
  implicit none
  integer :: i,isq,icub
  i = 1
  call square_cube(i,isq,icub)
  print*,"i,i^2,i^3=",i,isq,icub
end program xx

luoxhu@genoa test_mass $ cat build_fortran.sh
echo "mass without  lto"
~/local/gcc_t/bin/gfortran -O3 -mcpu=power9 hellofortran.f90 -mveclibabi=mass -L/opt/mass/8.1.3/Linux_LE/lib/ -lmass
nm a.out |grep atan2
echo "mass link with lto"
~/local/gcc_t/bin/gfortran -O3 -mcpu=power9 hellofortran.f90 -mveclibabi=mass -L/opt/mass/8.1.3/Linux_LE/lib/ -lmass -flto
nm a.out |grep atan2

luoxhu@genoa test_mass $ ./build_fortran.sh
mass without  lto
0000000010000a40 T atan2f
0000000010000a40 T _atan2f
mass link with lto
00000000100006f0 t 000001f8.plt_call.atan2f@@GLIBC_2.17
                 U atan2f@@GLIBC_2.17


The issue appears to occur during the "visibility" pass:

no-lto: (always __builtin_atan2f)
  hellofortran.f90.013t.ompexp: _3 = __builtin_atan2f (_2, 2.0e+0);
  hellofortran.f90.018t.fixup_cfg1: _3 = __builtin_atan2f (_2, 2.0e+0);
lto: (__builtin_atan2f -> atan2f )
  hellofortran.f90.013t.ompexp: _3 = __builtin_atan2f (_2, 2.0e+0);
  hellofortran.f90.016i.visibility:atan2/3 (atan2) @0x3fffb53e0708
  hellofortran.f90.018t.fixup_cfg1: _3 = atan2f (_2, 2.0e+0);

We've observed that the problem does not occur with a similar C/C++ test case.  It's not clear to me whether this is an LTO problem or a Fortran front end problem.

Thanks to Luo Xiong Hu for the test case and initial investigation.


---


### compiler : `gcc`
### title : `missing strlen lower bound of a string known to be at least N characters`
### open_at : `2019-07-31T19:48:01Z`
### last_modified_date : `2020-01-29T16:55:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91315
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
In the program below the strlen pass tracks the (minimum) length of the string formed by each of the functions, yet it doesn't expose that information to downstream passes that could fold each of the tests to false on that basis.

$ cat z.c && gcc -O2  -Wall -S -fdump-tree-optimized=/dev/stdout z.c
char a[8];

void f0 (void)
{
  a[0] = '1';
  a[1] = '2';

  if (__builtin_strlen (a) < 2)
    __builtin_abort ();
}

void f1 (void)
{
  __builtin_memcpy (a, "123", 3);
  if (__builtin_strlen (a) < 3)
    __builtin_abort ();
}

void f2 (void)
{
  *__builtin_stpcpy (a, "123") = '4';
  if (__builtin_strlen (a) < 4)
    __builtin_abort ();

}


;; Function f0 (f0, funcdef_no=0, decl_uid=1909, cgraph_uid=1, symbol_order=1)

f0 ()
{
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  MEM <unsigned short> [(char *)&a] = 12849;
  _1 = __builtin_strlen (&a);
  if (_1 <= 1)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=1912, cgraph_uid=2, symbol_order=2)

f1 ()
{
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "123", 3);
  _1 = __builtin_strlen (&a);
  if (_1 <= 2)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}



;; Function f2 (f2, funcdef_no=2, decl_uid=1915, cgraph_uid=3, symbol_order=3)

f2 ()
{
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "123", 3);
  MEM[(char *)&a + 3B] = 52;
  _1 = __builtin_strlen (&a);
  if (_1 <= 3)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `[9/10 Regression] x86-64 code generation / register allocation regressed.`
### open_at : `2019-08-01T15:00:24Z`
### last_modified_date : `2020-01-31T23:12:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91320
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
The following code:

    #include <boost/dynamic_bitset.hpp>
    
    size_t g(boost::dynamic_bitset<> const& a, bool value) {
        auto count = a.count();
        return value ? count : a.size() - count;
    }

When compiled with `gcc-8.3 -O3 -std=gnu++17 -mpopcnt` generates the following assembly:

    g(boost::dynamic_bitset<unsigned long, std::allocator<unsigned long> > const&, bool):
            mov     r9, QWORD PTR [rdi]
            mov     r8, QWORD PTR [rdi+8]
            sub     r8, r9
            sar     r8, 3
            je      .L5
            xor     edx, edx
            xor     eax, eax
    .L3:
            xor     ecx, ecx
            popcnt  rcx, QWORD PTR [r9+rdx*8]
            add     rdx, 1
            add     rax, rcx
            cmp     r8, rdx
            jne     .L3
    .L2:
            test    sil, sil
            jne     .L1
            mov     rdx, QWORD PTR [rdi+24]
            sub     rdx, rax
            mov     rax, rdx
    .L1:
            ret
    .L5:
            xor     eax, eax
            jmp     .L2


When compiled with `gcc-9.1 -O3 -std=gnu++17 -mpopcnt` generates the following assembly:

    g(boost::dynamic_bitset<unsigned long, std::allocator<unsigned long> > const&, bool):
            mov     r9, QWORD PTR [rdi]
            mov     rcx, QWORD PTR [rdi+8]
            sub     rcx, r9
            sar     rcx, 3
            je      .L5
            xor     eax, eax
            xor     r8d, r8d
    .L3:
            xor     edx, edx
            popcnt  rdx, QWORD PTR [r9+rax*8]
            add     rax, 1
            add     r8, rdx
            cmp     rcx, rax
            jne     .L3
    .L2:
            test    sil, sil
            jne     .L1
            mov     rax, QWORD PTR [rdi+24]
            sub     rax, r8
            mov     r8, rax
    .L1:
            mov     rax, r8
            ret
    .L5:
            xor     r8d, r8d
            jmp     .L2

Note the extra `mov rax, r8` instruction. gcc-8.3 better allocates registers, so that extra instruction is not necessary.


---


### compiler : `gcc`
### title : `[10 regression] g++.dg/lto/alias-4_0.C test failure`
### open_at : `2019-08-01T16:04:44Z`
### last_modified_date : `2020-04-09T12:18:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91322
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
The new alias-4_0.C fails on Arm. Without LTO it also fails with -O2 on AArch64, but it fails with -O2 and -O3 on Arm, so something must be different. Unfortunately even without -flto, the tree dumps don't show the inlining pass running so it's not obvious what is different.


---


### compiler : `gcc`
### title : `VRP does not handle array value range`
### open_at : `2019-08-02T12:40:56Z`
### last_modified_date : `2020-01-28T15:19:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91326
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 46659
testcase

For the attached code, gcc "10.0.0 20190730" -O3 generates:

foo:
        movsx   rdi, edi
        mov     eax, 1
        mov     edx, DWORD PTR arr[0+rdi*4]
        test    edx, edx
        je      .L1
        xor     eax, eax
        cmp     edx, 10
        setg    al
        add     eax, eax
.L1:
        ret

bar:
        xor     eax, eax
        cmp     edi, 2
        seta    al
        ret

baz:
        xor     eax, eax
        ret


clang "7.1.0" and "8.0.1" -O3 generate the optimal code for all 3 versions:

foo:
        xor     eax, eax
        ret


bar:
        xor     eax, eax
        ret


baz:
        xor     eax, eax
        ret


---


### compiler : `gcc`
### title : `Unnecessary call to __cxa_throw_bad_array_new_length`
### open_at : `2019-08-02T14:18:06Z`
### last_modified_date : `2019-08-02T17:11:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91329
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.0`
### severity : `normal`
### contents :
For the code

int* test(int i) {
    return new int[i];
}

The following assembly is generated:

test(int):
  movsx rdi, edi
  sub rsp, 8
  movabs rax, 2305843009213693950
  cmp rdi, rax
  ja .L2
  sal rdi, 2
  add rsp, 8
  jmp operator new[](unsigned long)
test(int) [clone .cold]:
.L2:
  call __cxa_throw_bad_array_new_length


However the `i * sizeof(int)` can not be greater than `2305843009213693950`. So the checks should be skipped.

Optimal assembly should look close to:

test(int):
  movsx rdi, edi
  sal rdi, 2
  jmp operator new[](unsigned long)


---


### compiler : `gcc`
### title : `[9/10 Regression] suboptimal register allocation for inline asm`
### open_at : `2019-08-02T20:44:05Z`
### last_modified_date : `2020-02-05T17:32:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91333
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
static double g(double x){
  asm volatile("":"+x"(x));
  return x;
}
double f(double a,double b){
  return g(g(a)+g(b));
}

(using -O1, -O2 or -O3)

With gcc-8, I get the sensible

	addsd	%xmm1, %xmm0

With gcc-9, I get

	movapd	%xmm0, %xmm2
	movapd	%xmm1, %xmm0
	movapd	%xmm2, %xmm1
	addsd	%xmm1, %xmm0

For some reason gcc decided to swap the 2 numbers?

-mavx helps remove all the moves, but on this slightly larger example:

double h(double a,double b){
  return f(f(a,a),f(b,b));
}

-mavx actually increases the number of moves, from 2 to 4... (and at -O1 gcc does this strange thing where it stores a double in %rax, good thing that disappears at -O2)


---


### compiler : `gcc`
### title : `Missed optimization: not passing hidden pointer but copying memory`
### open_at : `2019-08-04T21:24:24Z`
### last_modified_date : `2020-01-25T19:08:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91348
### status : `NEW`
### tags : `missed-optimization`
### component : `c`
### version : `9.1.0`
### severity : `normal`
### contents :
For the following example:

struct Vec3{
    double x, y, z;
};

void vadd_v2(struct Vec3* a, struct Vec3* out);

struct Vec3 use_v1(struct Vec3 *in){
    struct Vec3 out;
    vadd_v2(in, &out);
    return out;
}


the resulting assembler (-O2 -Wall) is:

use_v1:
        pushq   %r12
        movq    %rdi, %r12
        movq    %rsi, %rdi
        subq    $32, %rsp
        movq    %rsp, %rsi
        call    vadd_v2
        movq    16(%rsp), %rax
        movdqa  (%rsp), %xmm0
        movq    %rax, 16(%r12)
        movq    %r12, %rax
        movups  %xmm0, (%r12)
        addq    $32, %rsp
        popq    %r12
        ret

However, the hidden pointer could be passed directly into vadd_v2, which is what clang is doing:

use_v1:                                 # @use_v1
        pushq   %rbx
        movq    %rdi, %rbx
        movq    %rsi, %rdi
        movq    %rbx, %rsi
        callq   vadd_v2
        movq    %rbx, %rax
        popq    %rbx
        retq

See also https://godbolt.org/z/rT41Sj


---


### compiler : `gcc`
### title : `[9/10/11/12 Regression] Compare with negation is not eliminated`
### open_at : `2019-08-07T07:32:46Z`
### last_modified_date : `2022-03-02T12:07:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91384
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
void foo (void);
void bar (void);

int
test (int a)
{
  int r;

  if (r = -a)
    foo ();
  else
    bar ();

  return r;
}
--cut here--

results in (-O2):

        movl    %edi, %r12d
        negl    %r12d
        testl   %edi, %edi
        je      .L2

The combine pass can't do anything, since it sees:

(insn 6 3 7 2 (parallel [
            (set (reg/v:SI 82 [ <retval> ])
                (neg:SI (reg/v:SI 83 [ a ])))
            (clobber (reg:CC 17 flags))
        ]) "neg.c":9:9 464 {*negsi2_1}
     (expr_list:REG_UNUSED (reg:CC 17 flags)
        (nil)))
(insn 7 6 8 2 (set (reg:CCZ 17 flags)
        (compare:CCZ (reg/v:SI 83 [ a ])
            (const_int 0 [0]))) "neg.c":9:6 7 {*cmpsi_ccno_1}
     (expr_list:REG_DEAD (reg/v:SI 83 [ a ])
        (nil)))

Please note that the compare is with the original value (reg: 83), not with the result of the negation (reg: 82). Tree optimizers give us:

  r_3 = -a_2(D);
  if (a_2(D) != 0)
    goto <bb 3>; [50.00%]
  else
    goto <bb 4>; [50.00%]

This is a regression from 4.1.2, where the compiler is able to eliminate the compare:

        movl    %esi, %ebx
        negl    %ebx
        je      .L2


---


### compiler : `gcc`
### title : `Possible missed optimization: Can a pointer be passed as hidden pointer in x86-64 System V ABI`
### open_at : `2019-08-08T14:20:50Z`
### last_modified_date : `2023-01-28T20:38:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91398
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c`
### version : `9.1.0`
### severity : `normal`
### contents :
For the following example:

struct Vec3{
    double x, y, z;
};

struct Vec3 do_something(void);

void use(struct Vec3 *restrict out){
    *out = do_something();
}

The resulting assembly (-O2) is:

use:
        pushq   %rbx
        movq    %rdi, %rbx
        subq    $32, %rsp
        movq    %rsp, %rdi
        call    do_something
        movdqu  (%rsp), %xmm0
        movq    16(%rsp), %rax
        movups  %xmm0, (%rbx)
        movq    %rax, 16(%rbx)
        addq    $32, %rsp
        popq    %rbx
        ret

Here on godbolt: https://godbolt.org/z/kUPFox

However, as out is restrict, it could be passed as hidden pointer to do_something, which would lead to the following assembler:

use:
    jmp     do_something ; %rdi is now the hidden pointer

So is it a missed optimization, or is there something in x86-64 System V ABI that would forbid the above?


---


### compiler : `gcc`
### title : `__builtin_cpu_supports conjunction is optimized poorly`
### open_at : `2019-08-08T15:37:18Z`
### last_modified_date : `2021-05-06T13:40:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91400
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Clang 8 optimizes both f() and g() to the same code:

bool f()
{
    return __builtin_cpu_supports("popcnt") && __builtin_cpu_supports("ssse3");
}

bool g()
{
    extern unsigned int cpu_model;
    return (cpu_model & 64) && (cpu_model & 4);
}

f()/g():
        mov     eax, dword ptr [rip + cpu_model]
        and     eax, 68
        cmp     eax, 68
        sete    al
        ret

GCC generates this code only for g(). For f() GCC generates less optimal:

f():
        mov     edx, DWORD PTR __cpu_model[rip+12]
        mov     eax, edx
        shr     eax, 6
        and     eax, 1
        and     edx, 4
        mov     edx, 0
        cmove   eax, edx
        ret

I believe it would be great if GCC is able to generate the same code for f() too.


---


### compiler : `gcc`
### title : `Missed optimization on `labels as values` expression`
### open_at : `2019-08-09T11:04:54Z`
### last_modified_date : `2021-09-05T04:58:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91409
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.1.1`
### severity : `enhancement`
### contents :
I recently posted about this on SO, but it did not gain much traction: https://stackoverflow.com/questions/55987401/gcc-clang-labels-as-values-computing-offsets-at-runtime

The missed optimization is when using `labels as values` feature and computing address difference. The expression `&&label2 - &&label1` generates code that does the subtraction on runtime, while it might be possible to compute it on compile time.

Godbolt link: https://godbolt.org/z/zZdFYo


---


### compiler : `gcc`
### title : `Ordered compares aren't optimised`
### open_at : `2019-08-12T11:52:30Z`
### last_modified_date : `2023-08-05T15:36:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91425
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take this example:

===
#include <math.h>

_Bool lt(double a, double b) { return isless(a, b); }
_Bool lo(double a, double b) { return a < b; }
_Bool ll(double a, double b) { return lt(a,b) || lo(a,b); }
_Bool lx(double a, double b) { return lo(a,b) || lt(a,b); }
===

"lt" is an "unordered" comparison, that is, it does not raise an
exception if one (or both) of the inputs is a NaN, i.e. if the
comparison result is "unordered".  "lo" is an "ordered" comparison,
which means it does raise an exception if one of the inputs is a
NaN.

Doing both is the same as just doing the ordered comparison and
using the result for both.  "ll" should compile to the same as "lx"
does, and that should be the same as what is generated for "lo".

But it is not; not on rs6000 (with my cmpo patches), and not on x86
either: in both cases all three are different.  And in fact the
gimple output for all three is different; I think we need to fix
that before making RTL and the targets behave better.


---


### compiler : `gcc`
### title : `Performance Regression when upgrading from 8.3.0 to 9.0`
### open_at : `2019-08-13T08:40:46Z`
### last_modified_date : `2021-11-16T13:07:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91433
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
During the phoronix tests of botan-1.4.0-blowfish benchmark and crafty-1.4.4 benchmark, there are performance regressions in compilation process between version 8.3.0-433 and 9.0-454.


---


### compiler : `gcc`
### title : `Better induction variable for vectorization`
### open_at : `2019-08-13T09:39:58Z`
### last_modified_date : `2019-08-13T12:59:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91435
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
(from https://stackoverflow.com/q/57465290/1918193)
long RegularTest(int n) {
  long sum = 0;
  for (int i = 0; i < n; ++i)
    if (i % 2 != 0)
      sum += i + 1;
  return sum;
}

Compiling with -O3 -march=skylake, this gets vectorized, but the result has

  # vect_vec_iv_.14_60 = PHI <{ 0, 1, 2, 3, 4, 5, 6, 7 }(5), vect_vec_iv_.14_61(6)>
  vect_vec_iv_.14_61 = vect_vec_iv_.14_60 + { 8, 8, 8, 8, 8, 8, 8, 8 };
  vect__3.17_66 = vect_vec_iv_.14_60 + { 2, 2, 2, 2, 2, 2, 2, 2 };

(those are the only uses of vect_vec_iv_.14_6[01])

If we are only ever going to use x+2, why not use that instead, initialize with {2,3,4,...}, and skip the +2 at every iteration?

(there are other things to discuss about optimizing this testcase, for instance clang is clever enough to unroll by a factor of 2 and remove the condition, but let's stick to the induction variable for this PR)


---


### compiler : `gcc`
### title : `Tail-Call Optimization is not performed when return value is assumed.`
### open_at : `2019-08-15T15:50:44Z`
### last_modified_date : `2021-12-27T06:00:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91459
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
In situations where a function either returns a specific value or does not return at all, GCC fails to perform tail call optimizations. This appears to occur on all GCC versions with -O1, -O2, -O3, and -Os. It occurs with both the C and C++ front-ends.

Observe:

/* This function is guaranteed to only return the value '1', else it does not return.
// This is meant to emulate a function such as 'exec'.
*/
extern int function_returns_only_1_or_doesnt_return(int, int);

int foo1(int a, int b) {
    const int result = function_returns_only_1_or_doesnt_return(a, b);
    if (result == 1) {
        return result;
    }
    else {
        __builtin_unreachable();
    }
}

int foo2(int a, int b) {
    return function_returns_only_1_or_doesnt_return(a, b);
}


This results in the following output for -O3 on x86-64:

foo1(int, int):
  push rax
  call function_returns_only_1_or_doesnt_return(int, int)
  mov eax, 1
  pop rdx
  ret
foo3(int, int):
  jmp function_returns_only_1_or_doesnt_return(int, int)

While the behavior is correct, the tail-call optimization is far more optimal and preserves the same semantics.

The same behavior occurs with other architectures as well, so it does not appear to be a back-end issue.


---


### compiler : `gcc`
### title : `__builtin_assume_aligned should not break write combining`
### open_at : `2019-08-18T15:00:13Z`
### last_modified_date : `2019-08-21T11:46:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91482
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `normal`
### contents :
void write64 (void* p)
{
    unsigned* p1 = (unsigned*) __builtin_assume_aligned (p, 8);
    *p1++ = 0;
    unsigned* p2 = (unsigned*) __builtin_assume_aligned (p1, 4);
    *p2++ = 1;
}

When the two stores are written without __builtin_assume_aligned, they are coalesced into a single movq store. The code above, however, results in two movl stores, even though the new information provided by __builtin_assume_aligned does not prevent combination.


---


### compiler : `gcc`
### title : `[9 Regression] bogus argument missing terminating nul warning on strlen of a flexible array member`
### open_at : `2019-08-19T15:06:40Z`
### last_modified_date : `2022-05-27T08:37:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91490
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
The strlen call in f() below compiles with no warning and is successfully folded to a constant, but the equivalent call in g() triggers two spurious instances of the same warning and is not folded.

The warning is new in GCC 9 so GCC 8 compiles both functions without one, and folds neither call,

$ cat a.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout a.c
struct A { char n, s[]; };

const struct A a1 = { 3, "321" };

int f (void)
{
  return __builtin_strlen (a1.s);   // no warning, folded to 3
}

const struct A a2 = { 3, { 3, 2, 1, 0 } };
  
int g (void)
{
  return __builtin_strlen (a2.s);   // bogus warning, not folded
}

a.c: In function ‘g’:
a.c:14:30: warning: ‘strlen’ argument missing terminating nul [-Wstringop-overflow=]
   14 |   return __builtin_strlen (a2.s);   // bogus warning, not folded
      |                            ~~^~
a.c:10:16: note: referenced argument declared here
   10 | const struct A a2 = { 3, { 3, 2, 1, 0 } };
      |                ^~
a.c:14:30: warning: ‘strlen’ argument missing terminating nul [-Wstringop-overflow=]
   14 |   return __builtin_strlen (a2.s);   // bogus warning, not folded
      |                            ~~^~
a.c:10:16: note: referenced argument declared here
   10 | const struct A a2 = { 3, { 3, 2, 1, 0 } };
      |                ^~

;; Function f (f, funcdef_no=0, decl_uid=1910, cgraph_uid=1, symbol_order=1)

f ()
{
  <bb 2> [local count: 1073741824]:
  return 3;

}



;; Function g (g, funcdef_no=1, decl_uid=1914, cgraph_uid=2, symbol_order=3)

g ()
{
  long unsigned int _1;
  int _3;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strlen (&a2.s);
  _3 = (int) _1;
  return _3;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] STV change in r274481 causes 300.twolf regression on Haswell`
### open_at : `2019-08-20T08:03:26Z`
### last_modified_date : `2023-07-07T10:35:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91498
### status : `WAITING`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Split out from PR91154 comment#25

Biggest changes when benchmarking -mno-stv (base) against -mstv (peak):

   7.28%         37789  twolf_peak.none  twolf_peak.none   [.] ucxx2 
   4.21%         25709  twolf_base.none  twolf_base.none   [.] ucxx2        
   3.72%         22584  twolf_base.none  twolf_base.none   [.] new_dbox
   2.48%         22364  twolf_peak.none  twolf_peak.none   [.] new_dbox
   1.49%          8270  twolf_base.none  twolf_base.none   [.] sub_penal
   1.12%          7576  twolf_peak.none  twolf_peak.none   [.] sub_penal
   1.36%          9314  twolf_peak.none  twolf_peak.none   [.] old_assgnto_new2
   1.11%          5257  twolf_base.none  twolf_base.none   [.] old_assgnto_new2

and in ucxx2 I see

  0.17 │       mov    %eax,(%rsp)
  3.55 │       vpmins (%rsp),%xmm0,%xmm1   
       │       test   %eax,%eax
  0.22 │       vmovd  %xmm1,%r8d              
  0.80 │       cmovs  %esi,%r8d

...

Testcase:

extern int numBins;
extern int binOffst;
extern int binWidth;
extern int Trybin;
void foo (int);

void bar (int aleft, int axcenter)
{
  int a1LoBin = (((Trybin=((axcenter + aleft)-binOffst)/binWidth)<0)
                 ? 0 : ((Trybin>numBins) ? numBins : Trybin));
  foo (a1LoBin);
}

where combine eliminates the reg-reg copies STV adds to split live-ranges
between GPR and SSE uses (currently one plain move and one set via
vec_merge/duplicate).

Making STV of SI/DImode chains always happen after combine (in STV2) fixes
the testcase above but regresses gcc.target/i386/minmax-6.c which ran into
a very similar issue and was reduced from a SPEC CPU 2006 regression observed.

In the end the issue is that as soon as the RA decides it needs to spill
for a dual-use pseudo it ends up going through the stack because of, as
HJ notices, the minimum cost of moves between SSE and integer units is 8:

  /* Moves between SSE and integer units are expensive.  */
  if (SSE_CLASS_P (class1) != SSE_CLASS_P (class2))

    /* ??? By keeping returned value relatively high, we limit the number
       of moves between integer and SSE registers for all targets.
       Additionally, high value prevents problem with x86_modes_tieable_p(),
       where integer modes in SSE registers are not tieable
       because of missing QImode and HImode moves to, from or between
       MMX/SSE registers.  */
    return MAX (8, SSE_CLASS_P (class1)
                ? ix86_cost->hard_register.sse_to_integer
                : ix86_cost->hard_register.integer_to_sse);


---


### compiler : `gcc`
### title : `Stack Optimization bug on function and lambda return`
### open_at : `2019-08-20T14:31:14Z`
### last_modified_date : `2021-08-16T01:12:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91501
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `9.2.0`
### severity : `normal`
### contents :
Created attachment 46735
Preprocessed test file

When calling a function or a lambda to create function arguments, the stack is not reused properly.

This bug was discovered using the 8-2019-q3-update of ARM GCC, but it appears it also exists in GCC.

Discussion about this bug can be found here (for the ARM version of GCC) :
https://community.arm.com/developer/tools-software/tools/f/arm-compilers-forum/13366/arm-gcc-lambda-optimization/159463#159463

Link to online compiler test case : https://godbolt.org/z/hx2cEU

Attached is the preprocessed test case.

The problem with the following example code is :

wrapper2LAMBDA uses 32 bytes more of stack than wrapper1LAMBDA (ie the size of the structure), but it should not, the stack should be reused for further structures.

----------------------------------------
struct TestStruct
    {
        uint32_t field1;
        uint32_t field2;
        uint32_t field3;
        uint32_t field4;

        uint32_t field11;
        uint32_t field21;
        uint32_t field31;
        uint32_t field41;
    } ;

    struct TestStruct initStructure(uint32_t f1, uint32_t f2, uint32_t f3, uint32_t f4,uint32_t f11, uint32_t f21, uint32_t f31, uint32_t f41)
    {
        struct TestStruct myStruct;
        myStruct.field1 = f1;
        myStruct.field2 = f2;
        myStruct.field3 = f3;
        myStruct.field4 = f4;

        myStruct.field11 = f11;
        myStruct.field21 = f21;
        myStruct.field31 = f31;
        myStruct.field41 = f41;

        return myStruct;
    }

#define MACROLAMBDA(f1,f2,f3,f4,f5,f6,f7,f8) \
    [&]() -> struct TestStruct { struct TestStruct ${}; \
    $.field1 = f1; \
    $.field2 = f2; \
    $.field3 = f3; \
    $.field4 = f4; \
    $.field11 = f5; \
    $.field21 = f6; \
    $.field31 = f7; \
    $.field41 = f8; \
    return $; \
}()

    void __attribute__((noinline)) doStuff(struct TestStruct myStruct)
    {
        printf("f1 = %d, f2 = %d, f3 = %d, f4 = %d, f1 = %d, f2 = %d, f3 = %d, f4 = %d", myStruct.field1, myStruct.field2, myStruct.field3, myStruct.field4,myStruct.field11, myStruct.field21, myStruct.field31, myStruct.field41);
    }

    void __attribute__((noinline)) wrapper2LAMBDA(void)
    {
        doStuff(MACROLAMBDA(1,2,3,4,5,6,7,8));
        doStuff(MACROLAMBDA(11,22,33,44,55,66,77,88));
    }

    void __attribute__((noinline)) wrapper1LAMBDA(void)
    {
        doStuff(MACROLAMBDA(1,2,3,4,5,6,7,8));
    }

---------------------------

The assembly generated for both functions are :

----------------------------------------------
_Z14wrapper2LAMBDAv:
        movabs  rax, 8589934593
        sub     rsp, 72
        mov     QWORD PTR [rsp], rax
        movabs  rax, 17179869187
        mov     QWORD PTR [rsp+8], rax
        movabs  rax, 25769803781
        mov     QWORD PTR [rsp+16], rax
        movabs  rax, 34359738375
        mov     QWORD PTR [rsp+24], rax
        push    QWORD PTR [rsp+24]
        push    QWORD PTR [rsp+24]
        push    QWORD PTR [rsp+24]
        push    QWORD PTR [rsp+24]
        call    _Z7doStuff10TestStruct
        movabs  rax, 94489280523
        mov     QWORD PTR [rsp+64], rax
        movabs  rax, 188978561057
        mov     QWORD PTR [rsp+72], rax
        movabs  rax, 283467841591
        mov     QWORD PTR [rsp+80], rax
        movabs  rax, 377957122125
        mov     QWORD PTR [rsp+88], rax
        add     rsp, 32
        push    QWORD PTR [rsp+56]
        push    QWORD PTR [rsp+56]
        push    QWORD PTR [rsp+56]
        push    QWORD PTR [rsp+56]
        call    _Z7doStuff10TestStruct
        add     rsp, 104
        ret
_Z14wrapper1LAMBDAv:
        movabs  rax, 8589934593
        sub     rsp, 40
        mov     QWORD PTR [rsp], rax
        movabs  rax, 17179869187
        mov     QWORD PTR [rsp+8], rax
        movabs  rax, 25769803781
        mov     QWORD PTR [rsp+16], rax
        movabs  rax, 34359738375
        mov     QWORD PTR [rsp+24], rax
        push    QWORD PTR [rsp+24]
        push    QWORD PTR [rsp+24]
        push    QWORD PTR [rsp+24]
        push    QWORD PTR [rsp+24]
        call    _Z7doStuff10TestStruct
        add     rsp, 72
        ret
------------------------------------------

"sub     rsp, 72 " for wrapper2LAMBDA and "sub     rsp, 40" for wrapper1LAMBDA is what makes me believe stack is not reused, or that there is some dead stack.

The same behavior happen with ARM ASM

"sub	sp, sp, #80" for wrapper2LAMBDA and "sub	sp, sp, #52" for wrapper1LAMBDA.

This also happens with function returns.

Compiler versions : GCC 9.2.0 and 8-2019-q3-update
Compiled on a Windows 10 (with target STM32L4 in mind)
Command line :
-O2 -x c++ -std=c++17 -D"STM32" -D"STM32L4" -D"STM32L476xx" -D"STM32L476ZG" -D"ARM_MATH_CM4" -D"RELEASE" -D"NDEBUG" -D"USE_STM32L4XX_NUCLEO_64" -Wall -Wextra -Wno-comment -Wno-ignored-qualifiers -Wno-implicit-fallthrough -Wno-missing-field-initializers -Wno-overflow -Wno-sign-compare -Wno-type-limits -Wno-unused-parameter -Wpointer-arith -save-temps=obj -c -Wno-register -x c++ -fno-rtti -fno-exceptions -fpermissive -ggdb -fsingle-precision-constant -fmessage-length=0 -fvar-tracking -O2 -ffreestanding -fdata-sections -ffunction-sections -nostartfiles

with warning :
main.c:159:16: warning: format '%d' expects argument of type 'int', but argument 3 has type 'uint32_t' {aka 'long unsigned int'} [-Wformat=]

which is to be expected but should not be related to the bug.


---


### compiler : `gcc`
### title : `suboptimal atomic_fetch_sub and atomic_fetch_add`
### open_at : `2019-08-20T16:51:35Z`
### last_modified_date : `2021-12-21T16:47:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91502
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `enhancement`
### contents :
I have not specified the gcc version; it seems like it applies to any version.

I have noticed that if I write code:

#include <stdatomic.h>

int func(_Atomic(int) *a)
{
        return (atomic_fetch_sub(a, 1) - 1 == 0);
}

gcc generates optimized code (gcc -O2):
func:
.LFB0:
        .cfi_startproc
        xorl    %eax, %eax
        lock subl       $1, (%rdi)
        sete    %al
        ret

But when I change the condition to <= 0, it does not work. Correct me if I am wrong, but, I think, it should still be able to use sub:

#include <stdatomic.h>

int func(_Atomic(int) *a)
{
        return (atomic_fetch_sub(a, 1) - 1 <= 0);

}

func:
.LFB0:
        .cfi_startproc
        movl    $-1, %eax
        lock xaddl      %eax, (%rdi)
        cmpl    $1, %eax
        setle   %al
        movzbl  %al, %eax
        ret

Seems like the same problem exists for atomic_fetch_add as well.


---


### compiler : `gcc`
### title : `Inlining misses some logical operation folding`
### open_at : `2019-08-20T19:28:33Z`
### last_modified_date : `2019-09-03T20:15:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91504
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
In the following test case,

static inline unsigned deposit32(unsigned value, int start, int length,
                                 unsigned fieldval)
{
    unsigned mask = (~0U >> (32 - length)) << start;
    return (value & ~mask) | ((fieldval << start) & mask);
}

unsigned foo(unsigned value)
{
   return deposit32(value, 10, 1, 1);
}

unsigned bar(unsigned value)
{
    int start = 10;
    int length = 1;
    unsigned fieldval = 1;
    unsigned mask = (~0U >> (32 - length)) << start;
    return (value & ~mask) | ((fieldval << start) & mask);
}

One would expect FOO and BAR to compile to the same code, since
the latter is the trivial inlining of the former, but that does
not happen.

gcc -O2 -S z.c gives

foo:
	mvn	w1, w0
	and	w1, w1, 1024
	eor	w0, w1, w0
	ret

bar:
	orr	w0, w0, 1024
	ret

The problem appears independent of target, as it isvisible on
both x86 and aarch64 targets.


---


### compiler : `gcc`
### title : `optimization needs fictive memory allocation`
### open_at : `2019-08-21T20:39:56Z`
### last_modified_date : `2022-02-24T18:04:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91514
### status : `RESOLVED`
### tags : `missed-optimization, openmp`
### component : `tree-optimization`
### version : `8.3.1`
### severity : `normal`
### contents :
Created attachment 46738
Example project for optimized gemm, with optimization bug

In attachment(file gemm.h) exist optimization bug: we need to allocate fictive memory block for kc, without it we won't get vectorization in inner most loop, but kc is still dynamically traverse across big memory block for subdivided matrix C, so i'm suppose optimizer is too paranoid for possible data dependencies.


---


### compiler : `gcc`
### title : `missed optimization: no tailcall for types of class MEMORY`
### open_at : `2019-08-21T20:43:29Z`
### last_modified_date : `2021-08-10T23:23:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91515
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.1.0`
### severity : `normal`
### contents :
Produced assembler (-O2) for

   struct Vec3{
    double x, y, z;
   };

   struct Vec3 create(void);

   struct Vec3 use(){
    return create();
   }

looks as follows (live: https://godbolt.org/z/v-HjX0):

    use:
        pushq   %r12
        movq    %rdi, %r12
        call    create
        movq    %r12, %rax
        popq    %r12
        ret

Hower, I think that under System V AMD64 - ABI, the tailcall optimization:

    use:
        jmp    create

as create will move  %rdi-value to %rax anyway.


---


### compiler : `gcc`
### title : `Register allocation picks sub-optimal alternative with scratch registers`
### open_at : `2019-08-22T14:54:15Z`
### last_modified_date : `2019-08-22T14:54:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91523
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
consider this simple testcase:
_Bool f (unsigned a, unsigned b) { return a | b; }

when compiled for arm-eabi with options -O2 -march=armv7-a -mthumb, the compiler generates the following code sequence:

        orrs    r3, r0, r1      @ 8     [c=4 l=4]  *iorsi3_compare0_scratch/2
        ite     ne
        movne   r0, #1  @ 24    [c=8 l=6]  *p *thumb2_movsi_insn/1
        moveq   r0, #0  @ 25    [c=8 l=6]  *p *thumb2_movsi_insn/1
        bx      lr      @ 28    [c=8 l=4]  *thumb2_return

which is correct, but sub-optimal.  The pattern selected for the orrs has picked a sub-optimal alternative.

The pattern being matched is

(define_insn "*iorsi3_compare0_scratch"
  [(set (reg:CC_NOOV CC_REGNUM)
	(compare:CC_NOOV
	 (ior:SI (match_operand:SI 1 "s_register_operand" "%r,0,r")
		 (match_operand:SI 2 "arm_rhs_operand" "I,l,r"))
	 (const_int 0)))
   (clobber (match_scratch:SI 0 "=r,l,r"))]
  "TARGET_32BIT"
  "orrs%?\\t%0, %1, %2"
  [(set_attr "conds" "set")
   (set_attr "arch" "*,t2,*")
   (set_attr "length" "4,2,4")
   (set_attr "type" "logics_imm,logics_reg,logics_reg")]
)

and the insn matching it is

(insn 7 21 22 2 (parallel [
            (set (reg:CC_NOOV 100 cc)
                (compare:CC_NOOV (ior:SI (reg:SI 119)
                        (reg:SI 120))
                    (const_int 0 [0])))
            (clobber (scratch:SI))
        ]) "/home/rearnsha/work/pdtools/gcc-tests/cmpdi.c":3:35 96 {*iorsi3_compare0_scratch}
     (expr_list:REG_DEAD (reg:SI 120)
        (expr_list:REG_DEAD (reg:SI 119)
            (nil))))

Given that both input operands are dead, there is no reason why the compiler can't pick alternative 1 from the insn and use a scratch that clobbers one of the inputs (either directly, or via a commutative swap).  However it insists on allocating a different register (r3) and then using alternative 2 which is more expensive (32-bit insn instead of 16-bit insn).

Note the pattern shown above is updated in r274822 to add the thumb2 alternative that we want to match here.


---


### compiler : `gcc`
### title : `Unnecessary SSE and other instructions generated when compiling in C mode (vs. C++ mode)`
### open_at : `2019-08-23T06:50:12Z`
### last_modified_date : `2021-12-12T06:17:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91526
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Consider the following piece of code:

//--------------------------------------------------------------
struct Vec { float v[8]; };

struct Vec multiply(const struct Vec* v1, const struct Vec* v2)
{
    struct Vec result;
    for(unsigned i = 0; i < 8; ++i)
        result.v[i] = v1->v[i] * v2->v[i];
    return result;
}
//--------------------------------------------------------------

If this is compiled as C++, using g++ 9.2 with options -Ofast -march=skylake, the following result is produced:

_Z8multiplyPK3VecS1_:
  vmovups ymm0, YMMWORD PTR [rdx]
  mov rax, rdi
  vmulps ymm0, ymm0, YMMWORD PTR [rsi]
  vmovups YMMWORD PTR [rdi], ymm0
  vzeroupper
  ret

However, if it's compiled as C, using the same options, this is produced:

multiply:
  push rbp
  mov rax, rdi
  mov rbp, rsp
  and rsp, -32
  vmovups ymm0, YMMWORD PTR [rdx]
  vmulps ymm0, ymm0, YMMWORD PTR [rsi]
  vmovaps YMMWORD PTR [rsp-32], ymm0
  vmovdqa xmm2, XMMWORD PTR [rsp-16]
  vmovups XMMWORD PTR [rdi], xmm0
  vmovups XMMWORD PTR [rdi+16], xmm2
  vzeroupper
  leave
  ret

Not only are extra instructions surrounding the code, but moreover the assignment of the result into [rdi] has for some reason been split into two parts.

Both clang and icc produce the same result (very similar to the first result above) regardless of whether compiling as C or C++.


---


### compiler : `gcc`
### title : `[SVE] Redundant predicated store in gcc.target/aarch64/fmla_2.c`
### open_at : `2019-08-23T12:43:12Z`
### last_modified_date : `2019-10-21T10:23:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91532
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
In gcc.target/aarch64/fmla_2.c, we end up with two stores to the first array after if-conversion:

  _ifc__59 = *_55;
  _ifc__61 = _4 != 0 ? iftmp.0_31 : _ifc__59;
  *_55 = _ifc__61;
  iftmp.1_35 = __builtin_fma (_6, pretmp_53, pretmp_54);
  _ifc__64 = _4 == 0 ? pretmp_53 : _ifc__61;
  *_55 = _ifc__64;

instead of:

  iftmp.1_35 = __builtin_fma (_6, pretmp_53, pretmp_54);
  _ifc__64 = _4 == 0 ? pretmp_53 : iftmp.0_31;
  *_55 = _ifc__64;

We never recover from this and end up with the two stores to *_55 in the output:

        st1d    z2.d, p0, [x0, x6, lsl 3]
        ...
        st1d    z0.d, p0, [x0, x6, lsl 3]


---


### compiler : `gcc`
### title : `missed value-replacement in phiopt`
### open_at : `2019-08-24T18:58:35Z`
### last_modified_date : `2021-11-17T10:02:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91540
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
For the code below:

int Test(bool cond1, bool cond2)
{
    if (cond1)
    {
        if (cond2)
        {
            return 42;
        }
    }
    return 43;
}

gcc(trunk) with '-O3' produces:

Test(bool, bool):
  test dil, dil
  je .L3
  test sil, sil
  jne .L5
.L3:
  mov eax, 43
  ret
.L5:
  mov eax, 42
  ret

clang(trunk) with '-O3' produces:

Test(bool, bool): # @Test(bool, bool)
  mov eax, edi
  and eax, esi
  xor eax, 43
  ret

I think GCC can do it better.


---


### compiler : `gcc`
### title : `Try harder for AVX non-AVX2 cross-lane permutations`
### open_at : `2019-08-27T13:56:33Z`
### last_modified_date : `2019-08-29T09:51:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91560
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.1.1`
### severity : `normal`
### contents :
On:
typedef float __v8sf __attribute__((vector_size (32)));
typedef double __v4df __attribute__((vector_size (32)));
typedef int __v8si __attribute__((vector_size (32)));
typedef long long __v4di __attribute__((vector_size (32)));
#ifdef __clang__
#define S(x, y, t, ...) __builtin_shufflevector (x, y, __VA_ARGS__)
#else
#define S(x, y, t, ...) __builtin_shuffle (x, y, (t) { __VA_ARGS__ })
#endif

__v8sf f1 (__v8sf x, __v8sf y) { return S (x, y, __v8si, 0, 8, 9, 10, 11, 12, 13, 14 ); }
__v8sf f2 (__v8sf x, __v8sf y) { return S (x, y, __v8si, 0, 1, 8, 9, 10, 11, 12, 13 ); }
__v8sf f3 (__v8sf x, __v8sf y) { return S (x, y, __v8si, 0, 1, 2, 3, 8, 9, 10, 11 ); } 
__v8sf f4 (__v8sf x, __v8sf y) { return S (x, y, __v8si, 7, 7, 7, 7, 7, 7, 7, 7 ); } 
__v4df f5 (__v4df x, __v4df y) { return S (x, y, __v4di, 0, 4, 5, 6 ); } 
__v4df f6 (__v4df x, __v4df y) { return S (x, y, __v4di, 0, 1, 4, 5 ); } 
__v4df f7 (__v4df x, __v4df y) { return S (x, y, __v4di, 3, 3, 3, 3 ); }  

LLVM generates for -O2 -mavx -mno-avx2 shorter code for f1 and f2 (but worse code for f5), GCC simply gives up for f1/f2 and then expands the shuffle as lots of BIT_FIELD_REF extractions plus vector creation.
Wonder if ix86_expand_vec_perm_const_1 for the if (TARGET_AVX && !TARGET_AVX2) and 32-byte vectors shouldn't try harder (though, with rightly estimated costs).
The above permutations are what is used for OpenMP scans in scan-13.c (and variant thereof with double instead of float) and reason why we don't vectorize using 32-byte vectors.


---


### compiler : `gcc`
### title : `Optimisation test case and unnecessary XOR-OR pair instead of MOV.`
### open_at : `2019-08-27T19:22:07Z`
### last_modified_date : `2021-08-15T22:35:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91569
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
I wasn't entirely sure where to post this, but I have a very simple test 
problem that shows some missed optimisation potential. The task is to cast 
an integer to a long and replace the second lowest byte of the result with 
a constant (4). Below are three ways to achieve this:


long opt_test1(int num)             //  opt_test1:
{                                   //      movslq  %edi, %rax
    union {                         //      mmovb   $4, %ah
        long q;                     //      ret
        struct { char l,h; };
    } a;
    a.q = num;
    a.h = 4;
    return a.q;
}

The union here is modelled after the structure of a r?x register which 
contains the low and high byte registers: ?l and ?h. The cast and second 
byte assignment can be done in one instruction each. The optimiser manages 
to understand this and gives the optimal instructions.


long opt_test2(int num)             //  opt_test2:
{                                   //      movl    %edi, %eax
    long a = num;                   //      xor     %ah, %ah
    a &= (-1UL ^ 0xff00);           //      orb     $4, %ah
    a |= (4 << 8);                  //      cltq
    return a;                       //      ret
}

This solution, based on a bitwise AND and OR, is interesting. The optimiser 
recognised that I am interested in the second byte and makes use of the 'ah' 
register, but why is there a XOR and an OR rather than an a single, 
equivalent MOV? Similarly the (MOV + CLTQ) can be replaced outright with 
MOVSLQ. Notable here is that some older versions (such as "gcc-4.8.5 -O3") 
give results that correspond more to the C code:
    andl    $-65281, %edi
    orl     $1024, %edi
    movslq  %edi, %rax
    ret
which is actually better than the output for gcc-9.2.


long opt_test3(int num)             //  opt_test3:
{                                   //      movslq  %edi, %rdi
    long a = num;                   //      movq    %rdi, -8(%rsp)
    ((char*)&a)[1] = 4;             //      movb    $4, -7(%rsp)
    return a;                       //      movq    -8(%rsp), %rax
}                                   //      ret

This is the straightforwards approach, addressing the second byte in memory.
I am including this because LLVM manages to recognise that the stack is not 
actually necessary and goes for a register based solution.

As far as I could tell, these results seem quite consistent across most GCC 
versions and across all optimisation levels above -O0. However, I obtained 
the assembly code above using:

$ gcc-9.2 opt_tests.c -S -O3 -Wall -Wextra -pedantic


---


### compiler : `gcc`
### title : `Vectorization failure for a loop to do multiply-add because SLP loads unnecessarily require permutation`
### open_at : `2019-08-28T07:41:44Z`
### last_modified_date : `2019-12-12T14:20:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91573
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
The following code can not be vectorized ( compiling with gcc -O3 ):

=== begin code ===

char src[512];
char dst[512];

#define WIDTH 8

void foo(int height, int a, int b, int c, int d, int dst_stride) {
    char * ptr_src = src;
    char * ptr_dst = dst;

    for( int y = 0; y < height; y++ )
    {
        for( int x = 0; x < WIDTH; x++ )
            ptr_dst[x] = ( a*ptr_src[x] + b*ptr_src[x+1] + c*ptr_src[x] + d*ptr_src[x+1]) >> 6;
        ptr_dst += dst_stride;
        ptr_src += 32;
    }
}

=== end code ===

However, the case can be vectorized with either following modifications:
1) If the calculation is simpler, e.g.
     ptr_dst[x] = ( a*ptr_src[x] + c*ptr_src[x] ) >> 6;

2) If WIDTH is larger. e.g.
     #define WIDTH 16

This case is a hot loop from real application. It can be exposed on both AArch64 and X86-64 platform.


---


### compiler : `gcc`
### title : `tailr1 pass creates redundant phi nodes`
### open_at : `2019-08-28T13:41:58Z`
### last_modified_date : `2020-01-02T16:17:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91579
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
When compiling the following testcase (at least at -O2 and higher),
the early tail-call pass creates redundant PHI nodes.  The problem
with them is that ipa-prop then cannot see through them and does not
discover that an unchanged scalar argument is passed to another call.
As a consequence we lose information in jump functions and may not
clone the function when we should.

typedef long unsigned int size_t;
typedef int (*compare_t)(const void *, const void *);
int partition (void *base, size_t nmemb, size_t size, compare_t cmp);
void
my_qsort (void *base, size_t nmemb, size_t size, compare_t cmp)
{
  int pt;
  if (nmemb > 1)
    {
      pt = partition (base, nmemb, size, cmp);
      my_qsort (base, pt + 1, size, cmp);
      my_qsort ((void*)((char*) base + (pt + 1) * size),
		nmemb - pt - 1, size, cmp);
    }
}

Results into:
  <bb 2> :
  # base_13 = PHI <base_6(D)(0), _9(3)>
  # nmemb_11 = PHI <nmemb_7(D)(0), _5(3)>
  # size_14 = PHI <size_20(D)(0), size_14(3)>
  # cmp_15 = PHI <cmp_21(D)(0), cmp_15(3)>

where the last two PHI nodes are superfluous.

(I think I have a patch to address it, let me test it.)


---


### compiler : `gcc`
### title : `Missing horizontal addition auto-vectorization`
### open_at : `2019-08-29T03:54:50Z`
### last_modified_date : `2021-12-12T13:03:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91594
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
The next code (with -O3 -ffast-math -msse3):

float a2[4], b2[4], c2[4];

void hadd2() {
    c2[0] = a2[0] + a2[1];
    c2[1] = a2[2] + a2[3];
    c2[2] = b2[0] + b2[1];
    c2[3] = b2[2] + b2[3];
}

Compiles without auto-vectorization:

hadd2():
        movss   xmm0, DWORD PTR a2[rip]
        addss   xmm0, DWORD PTR a2[rip+4]
        movss   DWORD PTR c2[rip], xmm0
        movss   xmm0, DWORD PTR a2[rip+8]
        addss   xmm0, DWORD PTR a2[rip+12]
        movss   DWORD PTR c2[rip+4], xmm0
        movss   xmm0, DWORD PTR b2[rip]
        addss   xmm0, DWORD PTR b2[rip+4]
        movss   DWORD PTR c2[rip+8], xmm0
        movss   xmm0, DWORD PTR b2[rip+8]
        addss   xmm0, DWORD PTR b2[rip+12]
        movss   DWORD PTR c2[rip+12], xmm0
        ret

The expected code with HADDPS instruction (which does not compile):

hadd2():
        movaps  xmm0, XMMWORD PTR a1[rip]
        haddps  xmm0, XMMWORD PTR b1[rip]
        movaps  XMMWORD PTR c1[rip], xmm0
        ret

In contrast, the normal addition code:

void add2() {
    c2[0] = a2[0] + b2[0];
    c2[1] = a2[1] + b2[1];
    c2[2] = a2[2] + b2[2];
    c2[3] = a2[3] + b2[3];
}

Compiles with auto-vectorization:

add2():
        movaps  xmm0, XMMWORD PTR a2[rip]
        addps   xmm0, XMMWORD PTR b2[rip]
        movaps  XMMWORD PTR c2[rip], xmm0
        ret

Compiler Explorer Code: https://gcc.godbolt.org/z/9Hs9su


---


### compiler : `gcc`
### title : `[9 regression] 60% speed drop on neon intrinsic loop`
### open_at : `2019-08-29T15:13:35Z`
### last_modified_date : `2021-08-17T10:13:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91598
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Performance of the attached neon loop drops on Cortex-A53 by about 60% between GCC 7 and GCC 8.  Performance of trunk is the same as GCC 8.

There are two separate changes, both related to instruction scheduler that cause the regression.  The first change in r253235 is responsible for 70% of the regression.
===
    haifa-sched: fix autopref_rank_for_schedule qsort comparator
    
            * haifa-sched.c (autopref_rank_for_schedule): Order 'irrelevant' insns
            first, always call autopref_rank_data otherwise.
    
    
    
    git-svn-id: svn+ssh://gcc.gnu.org/svn/gcc/trunk@253235 138bc75d-0d04-0410-961f-82ee72b054a4
===

After this change instead of
r1 = [rb + 0]
r2 = [rb + 8]
r3 = [rb + 16]
r4 = <math with r1>
r5 = <math with r2>
r6 = <math with r3>

we got
r1 = [rb + 0]
<math with r1>
r2 = [rb + 8]
<math with r2>
r3 = [rb + 16]
<math with r3>

which, apparently, cortex-a53 autoprefetcher doesn't recognize.  This schedule happens because r2= load gets lower priority than the "irrelevant" <math with r1> due to the above patch.

If we think about it, the fact that "r1 = [rb + 0]" can be scheduled means that true dependencies of all similar base+offset loads are resolved.  Therefore, for autoprefetcher-friendly schedule we should prioritize memory reads before "irrelevant" instructions.

On the other hand, following similar logic, we want to delay memory stores as much as possible to start scheduling them only after all potential producers are scheduled.  I.e., for autoprefetcher-friendly schedule we should prioritize "irrelevant" instructions before memory writes.

Obvious patch to implement the above is attached.  It brings 70% of regressed performance on this testcase back.

The second part of the regression is due to compiler getting lucky with scheduling inline-asms representing the intrinsics.  After 
===
    Set default sched pressure algorithm
    
    The Arm backend sets the default sched-pressure algorithm to SCHED_PRESSURE_MODEL.
    Benchmarking on AArch64 shows this speeds up floating point performance on SPEC -
    eg. CactusBSSN improves by ~16%.  The gains are mostly due to less spilling,
    so enable this on AArch64 by default.
    
        gcc/
            * config/aarch64/aarch64.c (aarch64_override_options_internal):
            Set PARAM_SCHED_PRESSURE_ALGORITHM to SCHED_PRESSURE_MODEL.
    
    
    git-svn-id: svn+ssh://gcc.gnu.org/svn/gcc/trunk@254378 138bc75d-0d04-0410-961f-82ee72b054a4
===
the compiler no longer gets lucky on this testcase.

The solution here is to convert intrinsics in arm-neon.h to builtins/UNSPECs and attach scheduler descriptions to the UNSPECs.


---


### compiler : `gcc`
### title : `Missed optimization with sqrt(x*x)`
### open_at : `2019-09-03T14:39:11Z`
### last_modified_date : `2023-03-31T11:43:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91645
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
Based on a discussion on stackoverflow: https://stackoverflow.com/questions/57673825/how-to-force-gcc-to-assume-that-a-floating-point-expression-is-non-negative.

With gcc-trunk and '-std=c++17 -O3', the function 

float test (float x) 
{
    return std::sqrt(x*x);
}

produces the following assembly:

test(float):
        mulss   xmm0, xmm0
        pxor    xmm2, xmm2
        ucomiss xmm2, xmm0
        movaps  xmm1, xmm0
        sqrtss  xmm1, xmm1
        ja      .L8
        movaps  xmm0, xmm1
        ret
.L8:
        sub     rsp, 24
        movss   DWORD PTR [rsp+12], xmm1
        call    sqrtf
        movss   xmm1, DWORD PTR [rsp+12]
        add     rsp, 24
        movaps  xmm0, xmm1
        ret


As far as I can tell, it calls sqrtf, unless the argument to sqrt is >= 0, to check for negatives/NaN's and set the appropriate errno. The behavior is reasonable, as expected.

Adding '-fno-math-errno', '-ffast-math', or '-ffinite-math-only' removes all the clutter and compiles the same code into the neat

test(float):
        mulss   xmm0, xmm0
        sqrtss  xmm0, xmm0
        ret


Now, the problem is that GCC doesn't seem to optimize away the call to sqrtf based on some surrounding code. As an example, it would be neat to have this (or something similar) to get compiled into the same mulss-sqrtss-ret:

float test (float x) 
{
    float y = x*x;
    if (y >= 0.f)
        return std::sqrt(y);
    __builtin_unreachable();
}

If I understand it correctly, the 'y >= 0.f' excludes 'y' being NaN and 'y' being negative (though this is excluded by 'y = x*x'), so there is no need to check if the argument to `std::sqrt` is any bad, enabling to just do 'sqrtss' and return.

Furthemore, adding e.g. '#pragma GCC optimize ("no-math-errno")' before the 'test' function doesn't lead to optimizing it either, though I'm not sure whether this is expected to work and/or requires a separate bugtracker issue.


---


### compiler : `gcc`
### title : `Regressions of SPEC2017 rate caused by r274994`
### open_at : `2019-09-04T02:40:57Z`
### last_modified_date : `2019-09-11T06:06:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91654
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
compiled with -march=skylake-avx512 -Ofast -funroll-loops -flto

--------------------------------
531.deepsjeng_r  -7.18%
548.exchange_r  -6.70%
557.xz_r -6.74%
508.namd_r -2.81%
527.cam4_r -6.48%
544.nab_r -3.99%
Tested on skylake server.
-------------------------------------

Refer to
https://gcc.gnu.org/ml/gcc-patches/2019-08/msg02129.html


---


### compiler : `gcc`
### title : `[ARM/thumb] redundant memcpy does not get optimized away on thumb`
### open_at : `2019-09-05T17:08:59Z`
### last_modified_date : `2022-08-03T17:25:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91674
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
consider this c++ function

#include <cstring>
#include <array>
#include <cstdint>
auto to_bytes(uint32_t arg){
    std::array<uint8_t, sizeof(arg)> out{};
    std::memcpy(out.data(), &arg, sizeof(arg));
    return out;
}

on a little endian arch this function could be no-op. 
compiled with g++ -Os we get:
to_bytes(unsigned int):
        mov     eax, edi
        ret 

on arm this somewhat works:
compiled with arm-none-eabi-g++ -Os
to_bytes(unsigned int):
        sub     sp, sp, #8
        add     sp, sp, #8
        bx      lr

notice the redundant sub followed by an add

but if if thumb is forced, the full optimization is not performed
compiled with arm-none-eabi-g++ -Os -march=armv7-m -mtune=cortex-m3
to_bytes(unsigned int):
        mov     r3, r0
        movs    r0, #0
        uxtb    r2, r3
        bfi     r0, r2, #0, #8
        ubfx    r2, r3, #8, #8
        bfi     r0, r2, #8, #8
        ubfx    r2, r3, #16, #8
        bfi     r0, r2, #16, #8
        lsrs    r3, r3, #24
        sub     sp, sp, #8
        bfi     r0, r3, #24, #8
        add     sp, sp, #8
        bx      lr

in contrast, cross compiling with clang7 produces the desired optimization:
compiled with clang++7 --target=arm-none-eabi -march=armv7-m -mtune=cortex-m3
to_bytes(unsigned int):
        bx      lr

notice also how there is no redundant stack pointer manipulation


---


### compiler : `gcc`
### title : `Integer promotion quirk prevents efficient power of 2 division`
### open_at : `2019-09-06T09:57:19Z`
### last_modified_date : `2019-09-10T08:50:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91680
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `middle-end`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Created attachment 46841
Preprocessed source

For the function,

uint_fast32_t foo_a(uint_fast8_t x) {
    const uint_fast32_t q = 1 << x;
    return 256 / q;
}

GCC-{8.*,9.*} generates inefficient division instructions for both x86_64 and ARM. Since 'q' is a power of 2, the result should be a single right shift. Indeed, if the function is changed to,

uint_fast32_t foo_b(uint_fast8_t x) {
    return 256 / (1 << x); // This is, of course, equivalent to (256 >> x)
},

the expected instructions are generated. Counter-intuitively, if the original function is instead changed slightly to,

uint_fast32_t foo_c(uint_fast8_t x) {
    const uint_fast32_t q = (uint_fast32_t)1 << x;
    return 256 / q;
},

the expected instructions are once again generated.

See https://godbolt.org/z/37K0Ig for examples.


---


### compiler : `gcc`
### title : `Missed optimization for 128 bit arithmetic operations`
### open_at : `2019-09-06T10:11:38Z`
### last_modified_date : `2023-07-12T13:46:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91681
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the function:

void multiply128x64x2_3 ( 
    const unsigned long a, 
    const unsigned long b, 
    const unsigned long c, 
    const unsigned long d, 
    __uint128_t o[2]
  ) noexcept
{
    __uint128_t B0 = __uint128_t{ b } * c;
    __uint128_t B2 = __uint128_t{ a } * c;
    __uint128_t B1 = __uint128_t{ b } * d;
    __uint128_t B3 = __uint128_t{ a } * d;

    o[0] = B2 + (B0 >> 64);
    o[1] = B3 + (B1 >> 64);
}


With compilation flags "-O2 -std=c++17 -mavx" the following assembly is produced:

multiply128x64x2_3(unsigned long, unsigned long, unsigned long, unsigned long, unsigned __int128*):
  mov rax, rdx
  push rbx
  mov rbx, rdx
  mov r9, rdi
  mul rsi
  mov rax, rdx
  xor edx, edx
  mov r10, rax
  mov rax, rbx
  mov r11, rdx
  pop rbx
  mul rdi
  add rax, r10
  adc rdx, r11
  mov QWORD PTR [r8], rax
  mov rax, rsi
  xor edi, edi
  mov QWORD PTR [r8+8], rdx
  mul rcx
  mov rax, rcx
  mov rsi, rdx
  mul r9
  add rsi, rax
  adc rdi, rdx
  mov QWORD PTR [r8+16], rsi
  mov QWORD PTR [r8+24], rdi
  ret

However, it is sub-optimal. Touching the stack is not necessary and the same result could be achieved with less instructions:

multiply128x64x2_3(unsigned long, unsigned long, unsigned long, unsigned long, unsigned __int128*):
  mov r9, r8
  mov r8, rdx
  mov rax, rsi
  mul r8
  mov rax, r8
  mov r10, rdx
  mul rdi
  add r10, rax
  mov rax, rsi
  mov QWORD PTR [r9], r10
  adc rdx, 0
  mov QWORD PTR [8+r9], rdx
  mul rcx
  mov rax, rdi
  mov r11, rdx
  mul rcx
  add r11, rax
  mov QWORD PTR [16+r9], r11
  adc rdx, 0
  mov QWORD PTR [24+r9], rdx
  ret


---


### compiler : `gcc`
### title : `Fused multiply subtract not generated when same operand appears in multiplication and subtraction and -ffast-math`
### open_at : `2019-09-06T15:50:06Z`
### last_modified_date : `2019-09-09T07:37:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91687
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Some architectures have instructions that allow the expressions of the form
"(x * y) - z" to be done in one instruction (for example the FNMSUB instruction in AArch64).

For example

float f (float x, float y, float z) {
  return (y * x) - z;
}

Will generate

f:
  fnmsub  s0, s0, s1, s2
  ret

This is done with -O2/-O3 with GCC but if -funsafe-math-optimizations are enabled
it will convert expressions of the form
  (x * y) - x
to
  (x - 1) * y

which then means that the FNMSUB instruction is not generated.

For example on AArch64 trunk (with -O2 -funsafe-math-optimizations)

float f (float x, float y) {
    return (y * x) - y;
}

will generate

f:
  fmov    s2, 1.0e+0
  fsub    s0, s0, s2
  fmul    s0, s0, s1
  ret

whereas if you just compile with (-funsafe-math-optimizations) then the correct
code will be generated.

f:
  fnmsub  s0, s1, s0, s1
  ret

This also happens on x86-64.

Godbolt demonstrating the problem.
https://godbolt.org/z/HuU5LO


---


### compiler : `gcc`
### title : `Missed optimization for multiplication on 1.5 and 1.25`
### open_at : `2019-09-09T13:03:49Z`
### last_modified_date : `2021-12-13T04:48:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91709
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
On x86_32 for any number X of type (unsigned, unsigned short, unsigned char) multiplication by 1.5 with a conversion back to unsigned with any rounding mode produces the exactly same result as if X + (X >> 1).

Same holds for 1.25:
unsigned(X * 1.25) == unsigned(X + (X >> 2))

The above transformation allows to emit a short code without floating point computations:

test2(unsigned int):
  mov eax, edi
  shr eax
  add eax, edi
  ret


Instead of:
test(unsigned int):
  movl %edi, %edi
  pxor %xmm0, %xmm0
  cvtsi2sdq %rdi, %xmm0
  mulsd .LC0(%rip), %xmm0
  cvttsd2siq %xmm0, %rax
  ret
.LC0:
  .long 0
  .long 1073217536


---


### compiler : `gcc`
### title : `Missed optimization for checking nan and comparison`
### open_at : `2019-09-10T13:07:03Z`
### last_modified_date : `2021-12-25T05:22:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91721
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the example:

int doubleToString_0(double a) {
    if ( __builtin_isnan( a ) )
        return 1;
    else if ( a == 0. )
        return 2;

    return 3;
}

A suboptimal assembly with two `ucomisd` is generated for the above sample:

doubleToString_0(double):
  ucomisd xmm0, xmm0
  jp .L4
  ucomisd xmm0, QWORD PTR .LC0[rip]
  jnp .L8
.L5:
  mov eax, 3
  ret
.L8:
  jne .L5
  mov eax, 2
  ret
.L4:
  mov eax, 1
  ret
.LC0:
  .long 0
  .long 0


More optimal solution would be to do only the second `ucomisd` and check flags for a NaN:

doubleToString_0(double):
  pxor xmm1, xmm1
  ucomisd xmm0, xmm1
  jp .L4
  je .L8
.L5:
  mov eax, 3
  ret
.L8:
  mov eax, 2
  ret
.L4:
  mov eax, 1
  ret


---


### compiler : `gcc`
### title : `gcc generates sub-optimal assembly when AVX instructions are used.`
### open_at : `2019-09-10T14:45:37Z`
### last_modified_date : `2019-09-11T09:03:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91722
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
The following code:

    #include <immintrin.h>
    
    __m256 copysign_ps(__m256 from, __m256 to) {
        constexpr float signbit = -0.f;
        auto const avx_sigbit = _mm256_broadcast_ss(&signbit);
        return _mm256_or_ps(_mm256_and_ps(avx_sigbit, from), _mm256_andnot_ps(avx_sigbit, to));
    }

When compiled with `g++-9.2 -O2 -mavx -std=c++11` produces the following assembly:

    copysign_ps(float __vector(8), float __vector(8)):
            push    rbp
            vmovaps ymm2, ymm0
            mov     rbp, rsp
            and     rsp, -32
            vbroadcastss    ymm0, DWORD PTR .LC0[rip]
            vandnps ymm1, ymm0, ymm1
            vandps  ymm0, ymm0, ymm2
            vorps   ymm0, ymm0, ymm1
            leave
            ret
    .LC0:
            .long   2147483648

The 4 instructions involving rbp, rsp and leave do not seem to be necessary at all.

When compiled with `clang++-8.0 -O2 -mavx -std=c++11` it produces assembly with only expected instructions:

    .LCPI0_0:
            .long   2147483648              # 0x80000000
            .long   2147483648              # 0x80000000
            .long   2147483648              # 0x80000000
            .long   2147483648              # 0x80000000
            .long   2147483648              # 0x80000000
            .long   2147483648              # 0x80000000
            .long   2147483648              # 0x80000000
            .long   2147483648              # 0x80000000
    .LCPI0_1:
            .long   2147483647              # 0x7fffffff
            .long   2147483647              # 0x7fffffff
            .long   2147483647              # 0x7fffffff
            .long   2147483647              # 0x7fffffff
            .long   2147483647              # 0x7fffffff
            .long   2147483647              # 0x7fffffff
            .long   2147483647              # 0x7fffffff
            .long   2147483647              # 0x7fffffff
    copysign_ps(float __vector(8), float __vector(8)):                 # @copysign_ps(float __vector(8), float __vector(8))
            vandps  ymm0, ymm0, ymmword ptr [rip + .LCPI0_0]
            vandps  ymm1, ymm1, ymmword ptr [rip + .LCPI0_1]
            vorps   ymm0, ymm1, ymm0
            ret


---


### compiler : `gcc`
### title : `[9 Regression] builtin fma is not optimized or vectorized as *+`
### open_at : `2019-09-10T16:11:51Z`
### last_modified_date : `2019-10-21T11:58:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91723
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
i'd expect a*b+c to generate the same code as __builtin_fmaf(a,b,c)
when hw instruction is available for fmaf, but the later generates
significantly worst code in some cases, e.g. when vectorization
is involved.

consider:

void
foo (float *restrict r, const float *restrict a,
     const float *restrict b, const float *restrict c)
{
	for (int i=0; i < 4; i++) {
		float x;
#ifdef BUILTIN
		x = __builtin_fmaf(a[i],b[i],c[i]);
		x = __builtin_fmaf(a[i],b[i],x);
#else
		x = a[i]*b[i]+c[i];
		x = a[i]*b[i]+x;
#endif
		r[i] = x;
	}
}

with gcc -O3 -mfma -mavx -ffp-contract=fast -fno-math-errno i get good code:

foo:
        vmovups (%rdx), %xmm0
        vmovups (%rsi), %xmm1
        vmovaps %xmm0, %xmm2
        vfmadd213ps     (%rcx), %xmm1, %xmm2
        vfmadd132ps     %xmm1, %xmm2, %xmm0
        vmovups %xmm0, (%rdi)
        ret

but if i add -DBUILTIN i get

foo:
        vmovss  (%rsi), %xmm0
        vmovss  (%rdx), %xmm1
        vmovaps %xmm0, %xmm2
        vfmadd213ss     (%rcx), %xmm1, %xmm2
        vfmadd132ss     %xmm1, %xmm2, %xmm0
        vmovss  4(%rdx), %xmm1
        vmovss  %xmm0, (%rdi)
        vmovss  4(%rsi), %xmm0
        vmovaps %xmm0, %xmm2
        vfmadd213ss     4(%rcx), %xmm1, %xmm2
        vfmadd132ss     %xmm1, %xmm2, %xmm0
        vmovss  8(%rdx), %xmm1
        vmovss  %xmm0, 4(%rdi)
        vmovss  8(%rsi), %xmm0
        vmovaps %xmm0, %xmm2
        vfmadd213ss     8(%rcx), %xmm1, %xmm2
        vfmadd132ss     %xmm1, %xmm2, %xmm0
        vmovss  12(%rdx), %xmm1
        vmovss  %xmm0, 8(%rdi)
        vmovss  12(%rsi), %xmm0
        vmovaps %xmm0, %xmm2
        vfmadd213ss     12(%rcx), %xmm1, %xmm2
        vfmadd132ss     %xmm1, %xmm2, %xmm0
        vmovss  %xmm0, 12(%rdi)
        ret


i expected identical results, the same happens on other targets.


---


### compiler : `gcc`
### title : `Adding omp simd pragma prevents vectorization`
### open_at : `2019-09-11T04:21:26Z`
### last_modified_date : `2019-09-11T12:32:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91732
### status : `NEW`
### tags : `missed-optimization, openmp`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
omp-simd.c:
void poisson(int Q, const double *restrict gsym, const double *restrict du, double *restrict dv) {
#pragma omp simd
  for (int i=0; i<Q; i++) {
    const double g[2][2] = {{gsym[Q*0+i], gsym[Q*2+i]},
                            {gsym[Q*2+i], gsym[Q*1+i]}};
    for (int j=0; j<2; j++)
      dv[Q*j+i] = g[j][0] * du[Q*0+i] + g[j][1] * du[Q*1+i];
  }
}

The above fails to vectorize despite unrolling the inner loop.

$ gcc -Ofast -march=skylake-avx512 -fopenmp -fopt-info -fopt-info-missed -c omp-simd.c
omp-simd.c:6:5: optimized: loop with 2 iterations completely unrolled (header execution count 357878152)
omp-simd.c:4:38: missed: couldn't vectorize loop
omp-simd.c:4:18: missed: not vectorized: not suitable for scatter store D.4095[_37][0][0] = _4;

If I remove the "#pragma omp simd", it vectorizes:

$ gcc -Ofast -march=skylake-avx512 -fopenmp -fopt-info -fopt-info-missed -c omp-simd.c
omp-simd.c:5:5: optimized: loop with 2 iterations completely unrolled (header execution count 357878152)
omp-simd.c:2:3: optimized: loop vectorized using 32 byte vectors
omp-simd.c:2:3: optimized:  loop versioned for vectorization because of possible aliasing
omp-simd.c:2:3: optimized: loop with 2 iterations completely unrolled (header execution count 18709371)

If instead, I replace "#pragma omp simd" with "#pragma GCC ivdep", it vectorizes without possible aliasing.

$ gcc -Ofast -march=skylake-avx512 -fopenmp -fopt-info -fopt-info-missed -c omp-simd.c
omp-simd.c:6:5: optimized: loop with 2 iterations completely unrolled (header execution count 357878152)
omp-simd.c:3:3: optimized: loop vectorized using 32 byte vectors
omp-simd.c:3:3: optimized: loop with 2 iterations completely unrolled (header execution count 24166268)

I think aliasing should not be a concern due to use of restrict.  Also, if I manually unroll the inner loop (which the compiler is unrolling for me), the original "omp simd" version vectorizes nicely.

Reproduced on trunk: https://gcc.godbolt.org/z/wKdHg0


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Runtime regression for SPEC2000 177.mesa on Haswell around the end of August 2018`
### open_at : `2019-09-11T08:55:50Z`
### last_modified_date : `2023-07-07T10:36:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91735
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
The SUSE SPECfp2000 continuous regression tester shows a ~20% runtime regression on Haswell around the end of August 2018.

[1] https://gcc.opensuse.org/gcc-old/SPEC/CFP/sb-czerny-head-64/


---


### compiler : `gcc`
### title : `Runtime regression for SPEC2000 252.eon on Haswell around beginning of February 2019`
### open_at : `2019-09-11T09:50:58Z`
### last_modified_date : `2019-09-11T13:37:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91736
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
The SUSE SPECint2000 continuous regression tester shows a ~8% runtime
regression on Haswell around the beginning of February 2019.

[1] https://gcc.opensuse.org/gcc-old/SPEC/CINT/sb-czerny-head-64/252_eon_big.png


---


### compiler : `gcc`
### title : `Missed optimization for arithmetic operations of integers and floating point constants`
### open_at : `2019-09-11T14:16:51Z`
### last_modified_date : `2021-12-13T04:47:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91739
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the example:

double foo(unsigned i, unsigned j) {
  return i * 4.0 + j * 7.0;
}

Right now GCC emits code that converts integers to a floating points and does the multiplications:

foo(unsigned int, unsigned int): # @foo(unsigned int, unsigned int)
  mov eax, edi
  cvtsi2sd xmm1, rax
  mulsd xmm1, qword ptr [rip + .LCPI0_0]
  mov eax, esi
  cvtsi2sd xmm0, rax
  mulsd xmm0, qword ptr [rip + .LCPI0_1]
  addsd xmm0, xmm1
  ret

However it is possible to do better. If the max value of integer multiplied by the floating point constant fits into the mantissa and there is an integral type that could also hold the value then do the multiplication using integers:

double foo2(unsigned i, unsigned j) {
  return i * 4ull + j * 7ull;
}

This results in a much better code:

foo2(unsigned int, unsigned int): # @foo2(unsigned int, unsigned int)
  mov eax, edi
  mov ecx, esi
  lea rdx, [8*rcx]
  sub rdx, rcx
  lea rax, [rdx + 4*rax]
  cvtsi2sd xmm0, rax
  ret


---


### compiler : `gcc`
### title : `Bad register allocation of multi-register types`
### open_at : `2019-09-12T12:57:50Z`
### last_modified_date : `2021-08-12T08:09:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91753
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
The following example shows that register allocation of types which require multiple registers is quite non-optimal:

#include <stdint.h>

#include <arm_neon.h>
void neon_transform_nada(const uint8x16x4_t table, uint8_t * values, int volume) {
  uint8x16_t x1 = vld1q_u8(values + 0);
  uint8x16_t x2 = vld1q_u8(values + 16);
  uint8x16_t x3 = vld1q_u8(values + 16*2);
  uint8x16_t x4 = vld1q_u8(values + 16*3);
  for(int i = 0; i  <  volume; i++) {
          x1 = vqtbx4q_u8(x1, table,x1);
          x2 = vqtbx4q_u8(x2, table,x2);
          x3 = vqtbx4q_u8(x3, table,x3);
          x4 = vqtbx4q_u8(x4, table,x4);
   }
  vst1q_u8(values + 0,    x1);
  vst1q_u8(values + 16,   x2);
  vst1q_u8(values + 16*2, x3);
  vst1q_u8(values + 16*3, x4);
}

With -O2/O3:

neon_transform_nada:
	cmp	w1, 0
	ldp	q31, q30, [x0]
	ldp	q29, q28, [x0, 32]
	ble	.L2
	mov	v27.16b, v1.16b
	mov	w2, 0
	mov	v26.16b, v3.16b
	mov	v25.16b, v0.16b
	mov	v24.16b, v2.16b
	.p2align 3,,7
.L3:
	mov	v0.16b, v25.16b
	add	w2, w2, 1
	mov	v20.16b, v25.16b
	cmp	w1, w2
	mov	v16.16b, v25.16b
	mov	v4.16b, v25.16b
	mov	v1.16b, v27.16b
	mov	v21.16b, v27.16b
	mov	v17.16b, v27.16b
	mov	v5.16b, v27.16b
	mov	v2.16b, v24.16b
	mov	v22.16b, v24.16b
	mov	v18.16b, v24.16b
	mov	v6.16b, v24.16b
	mov	v3.16b, v26.16b
	mov	v23.16b, v26.16b
	mov	v19.16b, v26.16b
	mov	v7.16b, v26.16b
	tbx	v31.16b, {v0.16b - v3.16b}, v31.16b
	tbx	v30.16b, {v20.16b - v23.16b}, v30.16b
	tbx	v29.16b, {v16.16b - v19.16b}, v29.16b
	tbx	v28.16b, {v4.16b - v7.16b}, v28.16b
	bne	.L3
.L2:
	stp	q31, q30, [x0]
	stp	q29, q28, [x0, 32]
	ret

With -O1 it looks a lot better but there are still 4 redundant moves:

neon_transform_nada:
	ldr	q19, [x0]
	ldr	q18, [x0, 16]
	ldr	q17, [x0, 32]
	ldr	q16, [x0, 48]
	cmp	w1, 0
	ble	.L2
	mov	w2, 0
.L3:
	mov	v4.16b, v0.16b
	mov	v5.16b, v1.16b
	mov	v6.16b, v2.16b
	mov	v7.16b, v3.16b
	tbx	v19.16b, {v4.16b - v7.16b}, v19.16b
	tbx	v18.16b, {v4.16b - v7.16b}, v18.16b
	tbx	v17.16b, {v4.16b - v7.16b}, v17.16b
	tbx	v16.16b, {v4.16b - v7.16b}, v16.16b
	add	w2, w2, 1
	cmp	w1, w2
	bne	.L3
.L2:
	str	q19, [x0]
	str	q18, [x0, 16]
	str	q17, [x0, 32]
	str	q16, [x0, 48]
	ret


---


### compiler : `gcc`
### title : `[10 regression] g++.dg/lto/alias-3 FAILs`
### open_at : `2019-09-12T14:55:06Z`
### last_modified_date : `2019-09-16T13:13:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91756
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Between 20190910 (r275594) and 20190911 (r275651), g++.dg/lto/alias-3 regressed
on both Solaris/SPARC and Solaris/x86:

+FAIL: g++.dg/lto/alias-3 cp_lto_alias-3_0.o-cp_lto_alias-3_1.o execute  -O3 -flto -fno-early-inlining 

Thread 2 received signal SIGABRT, Aborted.
[Switching to Thread 1 (LWP 1)]
0xfe289715 in __lwp_sigqueue () from /lib/libc.so.1
(gdb) where
#0  0xfe289715 in __lwp_sigqueue () from /lib/libc.so.1
#1  0xfe281fbf in thr_kill () from /lib/libc.so.1
#2  0xfe1c92fa in raise () from /lib/libc.so.1
#3  0xfe19b29e in abort () from /lib/libc.so.1
#4  0x08050e29 in main ()
    at /vol/gcc/src/hg/trunk/local/gcc/testsuite/g++.dg/lto/alias-3_0.C:27

A reghunt identified the following patch as the culprit:

2019-09-11  Richard Biener  <rguenther@suse.de>

        PR tree-optimization/90387
        * vr-values.c (vr_values::extract_range_basic): After inlining
        simplify non-constant __builtin_constant_p to false.

        * gcc.dg/Warray-bounds-44.c: New testcase.


---


### compiler : `gcc`
### title : `-fvisibility=hidden during -fpic still uses GOT indirection on arm64`
### open_at : `2019-09-13T16:58:21Z`
### last_modified_date : `2021-09-17T00:20:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91766
### status : `RESOLVED`
### tags : `missed-optimization, patch, visibility`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
Passing -fvisibility=hidden does not stop GOT round-trips for global symbols during -fpic. Example:

#if DO_HIDE
#define HIDE __attribute__ ((visibility("hidden")))
#else
#define HIDE
#endif

int room[2] HIDE;

int* foo()
{
    int* p = room;
    *p = 0;
    return p;
}

$ gcc-8.2 -Ofast -fvisibility=hidden -fpic

foo:
        adrp    x1, _GLOBAL_OFFSET_TABLE_
        ldr     x1, [x1, #:gotpage_lo15:room]
        mov     x0, x1
        str     wzr, [x1]
        ret

Explicitly hiding it via DO_HIDE works as expected:

$ gcc-8.2 -Ofast -fvisibility=hidden -fpic -DDO_HIDE

foo:
        adrp    x1, room
        add     x0, x1, :lo12:room
        str     wzr, [x1, #:lo12:room]
        ret

Here's the godbolt link: https://godbolt.org/z/HCd2LT


---


### compiler : `gcc`
### title : `Can eliminate compare from loop with known number of iterations`
### open_at : `2019-09-16T09:58:00Z`
### last_modified_date : `2021-05-04T12:32:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91775
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
The compiler can eliminate compare for targets, where operation sets flags (e.g x86).  Following example:

--cut here--
#define N 1024

int a[N], b[N], c[N];

void
foo (void)
{
  int i;

  for (i = 0; i < N; i++)
    a[i] = b[i] + c[i];
}
--cut here--

compiles to (-O2):

        xorl    %eax, %eax
.L2:
        movl    c(%rax), %edx
        addl    b(%rax), %edx
        addq    $4, %rax
        movl    %edx, a-4(%rax)
(*)     cmpq    $4096, %rax
        jne     .L2
        ret


clang generates (-O2 -fno-vectorize -fno-unroll-loops):

        movq    $-4096, %rax            # imm = 0xF000
.LBB0_1:                                # =>This Inner Loop Header: Depth=1
        movl    c+4096(%rax), %ecx
        addl    b+4096(%rax), %ecx
        movl    %ecx, a+4096(%rax)
        addq    $4, %rax
        jne     .LBB0_1
        retq

Please note that clang rewrites loop to eliminate compare (*).


---


### compiler : `gcc`
### title : ``-fsplit-paths` generates slower code on arm`
### open_at : `2019-09-16T11:00:54Z`
### last_modified_date : `2019-09-18T11:47:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91776
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `normal`
### contents :
I'm doing this test on a Raspberry Pi Model 3B+. The CPU is BCM2835 ARMv7.
Writing a silly program calculating the cycle length of Fibonacci sequence modulo n.

version: gcc (Raspbian 8.3.0-6+rpi1) 8.3.0

#include <stdio.h>
#include <time.h>
typedef unsigned int uint;
typedef unsigned long long ullong;
int main(){
	uint m;
	ullong cyc=0,lastcyc=0;
	clock_t lastclock=0;
	for(m=2;;m++){
		uint
			a=0,
			b=1,
			n=0;
		do{
			b+=a;
			a=b-a;
			n++;
			if(b>=m)
				b-=m;
		}while(
			a!=0||
			b!=1
		);
		cyc+=n;
		//if(n>=4*m)
		//	printf("%u: %u %.2f\n",m,n,(double)n/m);
		if(cyc-lastcyc>100000000){
			clock_t now=clock();
			printf("~ %.0f loop/s\n",(double)(cyc-lastcyc)/(now-lastclock)*CLOCKS_PER_SEC);
			lastclock=now;
			lastcyc=cyc;
		}
	}
}

(1)
pi@rpi:~/Desktop $ gcc -Wall -march=native -mtune=native -o fibmod -O2  fibmod.c 
pi@rpi:~/Desktop $ ./fibmod
~ 240755135 loop/s
~ 277965738 loop/s
~ 276675919 loop/s
~ 277244469 loop/s
~ 277207289 loop/s
~ 277303633 loop/s
^C

(2)
pi@rpi:~/Desktop $ gcc -Wall -march=native -mtune=native -o fibmod -O2 -fsplit-paths fibmod.c 
pi@rpi:~/Desktop $ ./fibmod
~ 137691044 loop/s
~ 144593838 loop/s
~ 144397428 loop/s
~ 144519131 loop/s
~ 144392500 loop/s
^C

Also tested with `-Ofast -nofsplit-paths`, the speed measured is almost same as (1).

On other hardware with x86_64 arch, this option doesn't seem to make observable difference in running time.

btw, clang without `-march=mative -mtune-native` also produces the same speed as (1), but with these two options, the speed is even higher.

(3)
pi@rpi:~/Desktop $ clang -Wall -march=native -mtune=native -o fibmodclang -Ofast fibmod.c 
pi@rpi:~/Desktop $ ./fibmodclang 
~ 291343047 loop/s
~ 347350967 loop/s
~ 349217005 loop/s
~ 349320149 loop/s
~ 349367926 loop/s
~ 349372536 loop/s
^C


---


### compiler : `gcc`
### title : `std::variant index +1-1`
### open_at : `2019-09-16T22:49:19Z`
### last_modified_date : `2021-12-02T11:38:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91788
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.0`
### severity : `normal`
### contents :
(this is a detail, it probably has a negligible impact)

      constexpr size_t index() const noexcept
      { return size_t(typename _Base::__index_type(this->_M_index + 1)) - 1; }

IIUC, the whole +1-1 is here so that for a valueless variant, index_type(-1) becomes size_t(-1). I think there are cases where we could do better. For instance, for a never valueless type, we could just return _M_index. If there are fewer than 128 alternatives, we could use a sign extension: "return (signed char)_M_index;". Maybe some well-placed __builtin_unreachable to specify the range of _M_index would work as well.


---


### compiler : `gcc`
### title : `Value ranges determined from comparisons not used transitively`
### open_at : `2019-09-17T02:49:58Z`
### last_modified_date : `2021-12-16T03:04:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91789
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take the following code:

int foo(int a, int b)
{
  if (b < a)
    __builtin_unreachable();
  if (a < 0)
    return -1;
  if (b < 0)
    return 0;
  return 1;
}

The compiler should be able to determine that the b < 0 can never be true.  At that point in the code a >= 0 and b >= a, therefore transitively b >= 0.


The problem is not tied to __builtin_unreachable as can be seen by changing the code slightly:

int foo(int a, int b)
{
  if (b < a)
    return 2;
  if (a < 0)
    return -1;
  if (b < 0)
    return 0;
  return 1;
}

After the initial test b < a is handled there is still a threeway comparison.

The problem can be seen with 9.2.1 as well as the current trunk version.  clang 8.0.0 generates pretty much the same code as gcc.


---


### compiler : `gcc`
### title : `Sub-optimal YMM register allocation.`
### open_at : `2019-09-17T13:03:31Z`
### last_modified_date : `2021-08-20T01:14:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91796
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
The following code when compiled with `g++ -O3 -mavx2 -std=c++11`

    __m256d copysign2_pd(__m256d from, __m256d to) {
        auto a = _mm256_castpd_si256(from);
        auto avx_signbit = _mm256_castsi256_pd(_mm256_slli_epi64(_mm256_cmpeq_epi64(a, a), 63));
        return _mm256_or_pd(_mm256_and_pd(avx_signbit, from), _mm256_andnot_pd(avx_signbit, to)); // (avx_signbit & from) | (~avx_signbit & to)
    }

Generates the following assembly:

    copysign2_pd(double __vector(4), double __vector(4)):
            vmovapd ymm2, ymm0
            vmovapd ymm0, YMMWORD PTR .LC3[rip]
            vandnpd ymm1, ymm0, ymm1
            vandpd  ymm0, ymm0, ymm2
            vorpd   ymm0, ymm0, ymm1
            ret
    .LC3:
            .long   0
            .long   -2147483648
            .long   0
            .long   -2147483648
            .long   0
            .long   -2147483648
            .long   0
            .long   -2147483648

In the assembly instruction `vmovapd ymm2, ymm0` is unnecessary. It can instead load constant .LC3 directly into ymm2. The expected code is:

    copysign2_pd(double __vector(4), double __vector(4)):
            vmovapd ymm2, YMMWORD PTR .LC3[rip]
            vandnpd ymm1, ymm2, ymm1
            vandpd  ymm0, ymm2, ymm0
            vorpd   ymm0, ymm0, ymm1
            ret


---


### compiler : `gcc`
### title : `[10/11/12/13/14 regression] r265398 breaks gcc.target/powerpc/vec-rlmi-rlnm.c`
### open_at : `2019-09-17T21:21:20Z`
### last_modified_date : `2023-06-20T19:37:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91804
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Executing on host: /home/seurer/gcc/build/gcc-test2/gcc/xgcc -B/home/seurer/gcc/build/gcc-test2/gcc/ /home/seurer/gcc/gcc-test2/gcc/testsuite/gcc.target/powerpc/vec-rlmi-rlnm.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -O2 -mcpu=power9 -ffat-lto-objects -fno-ident -S -o vec-rlmi-rlnm.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/build/gcc-test2/gcc/xgcc -B/home/seurer/gcc/build/gcc-test2/gcc/ /home/seurer/gcc/gcc-test2/gcc/testsuite/gcc.target/powerpc/vec-rlmi-rlnm.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -O2 -mcpu=power9 -ffat-lto-objects -fno-ident -S -o vec-rlmi-rlnm.s
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c (test for excess errors)
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times vrlwmi 1
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times vrldmi 1
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times splt 2
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times vextsb2d 1
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times vslw 1
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times vsld 1
gcc.target/powerpc/vec-rlmi-rlnm.c: xxlor found 3 times
FAIL: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times xxlor 2
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times vrlwnm 2
PASS: gcc.target/powerpc/vec-rlmi-rlnm.c scan-assembler-times vrldnm 2

Note the extra xxlor

with r265397:
rlnm_test_2:
.LFB5:
	.cfi_startproc
	xxspltib 32,8
	vextsb2d 0,0
	vsld 3,3,0
	xxlor 35,35,36
	vrldnm 2,2,3
	blr

with r265398:
rlnm_test_2:
.LFB5:
	.cfi_startproc
	xxspltib 0,8
	xxlor 32,0,0
	vextsb2d 0,0
	vsld 3,3,0
	xxlor 35,35,36
	vrldnm 2,2,3
	blr


---


### compiler : `gcc`
### title : `256-bit vector store isn't used`
### open_at : `2019-09-18T20:09:50Z`
### last_modified_date : `2021-08-25T06:55:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91811
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-1 xxx]$ cat y.i
typedef struct
{
  long long width, height;
  long long x, y;
} info;

extern void bar (info *);

void
foo (long long width, long long height,
     long long x, long long y)
{
  info t;
  t.width = width;
  t.height = height;
  t.x = x;
  t.y = y;
  bar (&t);
}
[hjl@gnu-cfl-1 xxx]$  /export/build/gnu/tools-build/gcc-debug/build-x86_64-linux/gcc/xgcc -B/export/build/gnu/tools-build/gcc-debug/build-x86_64-linux/gcc/ -O2 -march=skylake -ftree-slp-vectorize -mtune-ctrl=^sse_typeless_stores -mprefer-vector-width=256  -S y.i
[hjl@gnu-cfl-1 xxx]$ cat y.s
	.file	"y.i"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	vmovq	%rdi, %xmm1
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	vpinsrq	$1, %rsi, %xmm1, %xmm0
	vmovq	%rdx, %xmm2
	vmovdqa	%xmm0, (%rsp)
	movq	%rsp, %rdi
	vpinsrq	$1, %rcx, %xmm2, %xmm0
	vmovdqa	%xmm0, 16(%rsp)
	call	bar
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 10.0.0 20190918 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 xxx]$ 

Is it possible to use 256-bit YMM register store?


---


### compiler : `gcc`
### title : `SSE optimization flaw with float vs. double`
### open_at : `2019-09-19T11:33:22Z`
### last_modified_date : `2019-09-19T12:58:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91818
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
Consider the following code:

//-------------------------------------------------
#include <cmath>
#include <array>

using Float = std::array<double, 4>;

Float p(Float a, Float b)
{
    Float result;
    for(unsigned i = 0; i < result.size(); ++i)
        result[i] = std::sqrt(a[i]*a[i] + b[i]*b[i]);
    return result;
}
//-------------------------------------------------

When compiled with gcc 9.2, using -Ofast -march=skylake, it produces the following result:

//-------------------------------------------------
        push    rbp
        mov     rax, rdi
        mov     rbp, rsp
        vmovupd ymm1, YMMWORD PTR [rbp+48]
        vmovupd ymm0, YMMWORD PTR [rbp+16]
        vmulpd  ymm1, ymm1, ymm1
        vfmadd132pd     ymm0, ymm1, ymm0
        vsqrtpd ymm0, ymm0
        vmovupd YMMWORD PTR [rdi], ymm0
        vzeroupper
        pop     rbp
        ret
//-------------------------------------------------

Besides the surrounding boilerplate (which might or might not be necessary, I'm not knowledgeable enough to fully understand this), the actual operations are sensible.

However, consider what happens if we change the type alias to:

using Float = std::array<float, 8>;

One would think the result would be almost identical, yet this is produced:

//-------------------------------------------------
        push    rbp
        vxorps  xmm2, xmm2, xmm2
        mov     rax, rdi
        mov     rbp, rsp
        vmovups ymm1, YMMWORD PTR [rbp+48]
        vmovups ymm0, YMMWORD PTR [rbp+16]
        vmulps  ymm1, ymm1, ymm1
        vfmadd132ps     ymm0, ymm1, ymm0
        vrsqrtps        ymm1, ymm0
        vcmpneqps       ymm2, ymm2, ymm0
        vandps  ymm1, ymm1, ymm2
        vmulps  ymm0, ymm1, ymm0
        vmulps  ymm1, ymm0, ymm1
        vmulps  ymm0, ymm0, YMMWORD PTR .LC1[rip]
        vaddps  ymm1, ymm1, YMMWORD PTR .LC0[rip]
        vmulps  ymm0, ymm1, ymm0
        vmovups YMMWORD PTR [rdi], ymm0
        vzeroupper
        pop     rbp
        ret
.LC0:
        .long   3225419776
        .long   3225419776
        .long   3225419776
        .long   3225419776
        .long   3225419776
        .long   3225419776
        .long   3225419776
        .long   3225419776
.LC1:
        .long   3204448256
        .long   3204448256
        .long   3204448256
        .long   3204448256
        .long   3204448256
        .long   3204448256
        .long   3204448256
        .long   3204448256
//-------------------------------------------------

This is not a question of the number of loops being 8, as

using Float = std::array<float, 4>;

produces a very similar result.

Note that clang 8.0 produces this (from the <float, 8> version of the code):

//-------------------------------------------------
        mov     rax, rdi
        vmovups ymm0, ymmword ptr [rsp + 8]
        vmulps  ymm0, ymm0, ymm0
        vmovups ymm1, ymmword ptr [rsp + 40]
        vfmadd213ps     ymm1, ymm1, ymm0
        vsqrtps ymm0, ymm1
        vmovups ymmword ptr [rdi], ymm0
        vzeroupper
        ret
//-------------------------------------------------


---


### compiler : `gcc`
### title : `unnecessary sign-extension after _mm_movemask_epi8 or __builtin_popcount`
### open_at : `2019-09-19T17:39:52Z`
### last_modified_date : `2020-02-01T13:04:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91824
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
gcc -O2 -mpopcnt leaves unnecessary cdqe:

#include <cstdint>
#include <emmintrin.h>

void f(uint64_t& val, __m128i mask)
{
    val += __builtin_popcount(_mm_movemask_epi8(mask));
}

void g(uint64_t& val, __m128i mask)
{
    val += __builtin_popcountll(_mm_movemask_epi8(mask));
}

f:
  pmovmskb eax, xmm0
  popcnt eax, eax
  cdqe
  add QWORD PTR [rdi], rax
  ret
g:
  pmovmskb eax, xmm0
  cdqe
  popcnt rax, rax
  add QWORD PTR [rdi], rax
  ret

Both cdqe are unnecessary, because the results of both pmovmskb and __builtin_popcount can not be negative.

Only lower 16 bits of pmovmskb can be non-zero. And the image of popcnt is either [0..32] or [0..64] depending on the argument.


---


### compiler : `gcc`
### title : `[8/9 Regression] incorrect use of shr and shrx to shift by 64, missed optimization of vector shift`
### open_at : `2019-09-20T12:42:58Z`
### last_modified_date : `2023-07-27T13:57:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91838
### status : `RESOLVED`
### tags : `missed-optimization, wrong-code`
### component : `rtl-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
Test case:

using T = unsigned char; // or ushort, or uint
using V [[gnu::vector_size(8)]] = T;
V f(V x) { return x >> 8 * sizeof(T); }

GCC 10 compiles to either xor or shift (which should better be xor, as well)

GCC 9.2 compiles to:
  vmovq rax, xmm0
  mov ecx, 64
  shr rax, cl
  sal rax, (64 - 8*sizeof(T))
  vmovq xmm0, rax

The `shr rax, cl`, where cl == 64 is a nop, because shr (and shrx, which is used when BMI2 is enabled) mask the count with 0x3f. Consequently the last element of the input vector is unchanged in the output.

In any case, the use of shr/shrx with shifts > 64 (or 32 in case of the 32-bit variant) should not occur.


---


### compiler : `gcc`
### title : `invalid vectorization of isless, islessequal, etc.`
### open_at : `2019-09-23T10:46:07Z`
### last_modified_date : `2021-11-20T22:39:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91861
### status : `UNCONFIRMED`
### tags : `missed-optimization, wrong-code`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
Test case (cf. https://godbolt.org/z/z3TH9F):

#include <cmath>

using V [[gnu::vector_size(16)]] = float;

V f(V x, V y) {
    int r [[gnu::vector_size(16)]];
    for (int i = 0; i < 4; ++i) {
        r[i] = -std::isless(x[i], y[i]);
    }
    return reinterpret_cast<V>(r);
}

Using `-O3`, the `std::isless` calls are vectorized to a cmpnltps instruction which will raise FE_INVALID if one of the arguments is NaN. However, the math.h compare functions are not allowed to raise FP exceptions.

There's also a missed optimization here:

Starting with AVX, one of the quiet compare instructions can be used. E.g. translate isless to cmpp[sd] with predicate LT_OQ (0x11).

Without AVX, it's possible to use CMPORDPS and PCMPGTD:

isless(x, y) =>
xi = reinterpret as int vector(x)
yi = reinterpret as int vector(y)
xp = xi < 0 ? -(xi & 0x7fffffff) : xi
yp = yi < 0 ? -(yi & 0x7fffffff) : yi
cmpord(x, y) && (xp < yp)

Whether that's faster than a loop over ucomiss is still to be shown.


---


### compiler : `gcc`
### title : `Sign extend of an int is not recognized`
### open_at : `2019-09-23T18:07:33Z`
### last_modified_date : `2019-09-24T12:57:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91866
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Consider the example:


using size_t = unsigned long long;

size_t index0(int i) {
    return size_t(i + 1) - 1;
}


GCC generates the following assembly:

index0(int):
  lea eax, [rdi+1]
  cdqe
  sub rax, 1
  ret


However a more optimal assembly is possible:

index0(int): # @index0(int)
  movsxd rax, edi
  ret

Godbolt playground: https://godbolt.org/z/3j7_SE


---


### compiler : `gcc`
### title : `Constant bitfield assignment causes unnecessary use of memory and instructions`
### open_at : `2019-09-23T19:35:00Z`
### last_modified_date : `2021-08-01T23:24:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91869
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.2.1`
### severity : `enhancement`
### contents :
Created attachment 46914
MWE in C, compile with gcc -O2

Assigning constant bitfields to variables cause unnecessary use of memory, instructions and time.

Having a byte consisting of multiple bitfields, setting more than one bitfield causes gcc to place the constant in .rodata instead of immediate loading.
When all fields are zero, or setting only one of the bitfields, immediate loading is used as expected.

On microcontrollers with limited memory this is a problem. Bitfields (representation clauses) in Ada are well-defined, but I've written the MWE in C as C's bitfields exhibit the same (mis-)behaviour.

The attached code should be generated with:
$ gcc -O2 main.c -o main
$ objdump -h -S main > main.lss

Tried with Targets being x86_64 (9.2.1), arm-linux-gnueabihf (9.2.1) and avr (9.2.0), same result.


---


### compiler : `gcc`
### title : `Value range knowledge of higher bits not used in optimizations`
### open_at : `2019-09-24T14:12:13Z`
### last_modified_date : `2021-08-01T17:37:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91881
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the example:

unsigned long long sample2(unsigned long long m) {
    if (m >= 100) __builtin_unreachable();
    m *= 16;
    return m >> 3;
}

After the `if` statement we do know that the higher bits are set to 0. So instead of generating the following assembly:

sample2(unsigned long long):
  mov rax, rdi
  sal rax, 4
  shr rax, 3
  ret

A more optimal assembly could be generated:

sample2(unsigned long long):
  lea rax, [rdi + rdi]
  ret


Godbolt playground: https://godbolt.org/z/1iSpTh

P.S.: that optimization is important for std::to_chars(..., double) like functions, where a significant of a double is extracted into an unsigned long long variable, so its upper bits are always zero.


---


### compiler : `gcc`
### title : `boolean XOR tautology missed optimisation`
### open_at : `2019-09-24T14:18:57Z`
### last_modified_date : `2023-08-02T14:03:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91882
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
This functions is always return true regardless of the value of the arguments:

bool bool_xor_test(bool a, bool b)
{
  return (a != b) == ((a || b) && !(a && b));
}

bool bool_xor_test2(bool a, bool b)
{
  return (a ^ b) == ((a || b) && !(a && b));
}

but compiler does not simplify to "return true;". Expression ((a || b) && !(a && b)) poorly optimized


BUT! In this case:
bool xor_test_unsigned_int(unsigned int a, unsigned int b)
{
  return (a ^ b) == ((a | b) & ~(a & b));
}

It actually simlified to "return true;" :

        movl    $1, %eax
        ret


---


### compiler : `gcc`
### title : `Division by a constant could be optimized for known variables value range`
### open_at : `2019-09-24T14:25:17Z`
### last_modified_date : `2021-08-15T01:35:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91883
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the example:

unsigned long long kBorder = (1ull<<62);

unsigned long long sample(unsigned long long m) {
    if (m >= kBorder) __builtin_unreachable();
    return m / 10;
}

It produces the following assembly:

sample(unsigned long long):
  movabs rdx, -3689348814741910323
  mov rax, rdi
  mul rdx
  mov rax, rdx
  shr rax, 3
  ret

However, knowing that the higher bits are always 0, the constant could be adjusted to avoid the `shr rax, 3`:

sample(unsigned long long):
  movabs rax, 1844674407370955162
  mul rdi
  mov rax, rdx
  ret

Godbolt playground: https://godbolt.org/z/YU2yAC

This issue is probably related to PR 91881

P.S.: that optimization is important for std::to_chars(..., double) like functions, where a significant of a double is extracted into an unsigned long long variable, so its upper bits are always zero.


---


### compiler : `gcc`
### title : `Very poor optimization on large attribute vector_size`
### open_at : `2019-09-25T11:59:02Z`
### last_modified_date : `2021-08-25T06:38:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91897
### status : `RESOLVED`
### tags : `ABI, missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Consider the following code:

//-----------------------------------------------------------
typedef double Double16 __attribute__((vector_size(8*16)));

Double16 mult(const Double16& v1, const Double16& v2)
{
    return v1 * v2;
}
//-----------------------------------------------------------

Using the compiler options "-Ofast -march=skylake", clang 9.0.0 produces this output from it:

//-----------------------------------------------------------
  vmovapd ymm0, ymmword ptr [rsi]
  vmovapd ymm1, ymmword ptr [rsi + 32]
  vmovapd ymm2, ymmword ptr [rsi + 64]
  vmovapd ymm3, ymmword ptr [rsi + 96]
  vmulpd ymm0, ymm0, ymmword ptr [rdi]
  vmulpd ymm1, ymm1, ymmword ptr [rdi + 32]
  vmulpd ymm2, ymm2, ymmword ptr [rdi + 64]
  vmulpd ymm3, ymm3, ymmword ptr [rdi + 96]
  ret
//-----------------------------------------------------------

However, gcc 9.2 produces the following output:

//-----------------------------------------------------------
  push rbp
  mov rax, rdi
  mov rbp, rsp
  and rsp, -128
  sub rsp, 392
  vmovdqa xmm5, XMMWORD PTR [rsi]
  vmovdqa xmm6, XMMWORD PTR [rsi+16]
  vmovdqa xmm7, XMMWORD PTR [rsi+32]
  vmovdqa xmm1, XMMWORD PTR [rsi+48]
  vmovdqa xmm2, XMMWORD PTR [rsi+64]
  vmovdqa xmm3, XMMWORD PTR [rsi+80]
  vmovdqa xmm4, XMMWORD PTR [rsi+96]
  vmovaps XMMWORD PTR [rsp+8], xmm5
  vmovaps XMMWORD PTR [rsp+24], xmm6
  vmovdqa xmm5, XMMWORD PTR [rsi+112]
  vmovdqa xmm6, XMMWORD PTR [rdx]
  vmovaps XMMWORD PTR [rsp+40], xmm7
  vmovaps XMMWORD PTR [rsp+56], xmm1
  vmovdqa xmm7, XMMWORD PTR [rdx+16]
  vmovdqa xmm1, XMMWORD PTR [rdx+32]
  vmovaps XMMWORD PTR [rsp+72], xmm2
  vmovaps XMMWORD PTR [rsp+88], xmm3
  vmovdqa xmm2, XMMWORD PTR [rdx+48]
  vmovdqa xmm3, XMMWORD PTR [rdx+64]
  vmovaps XMMWORD PTR [rsp+104], xmm4
  vmovdqa xmm4, XMMWORD PTR [rdx+80]
  vmovaps XMMWORD PTR [rsp+136], xmm6
  vmovaps XMMWORD PTR [rsp+152], xmm7
  vmovaps XMMWORD PTR [rsp+168], xmm1
  vmovaps XMMWORD PTR [rsp+184], xmm2
  vmovaps XMMWORD PTR [rsp+200], xmm3
  vmovaps XMMWORD PTR [rsp+216], xmm4
  vmovaps XMMWORD PTR [rsp+120], xmm5
  vmovdqa xmm5, XMMWORD PTR [rdx+96]
  vmovapd ymm7, YMMWORD PTR [rsp+8]
  vmovapd ymm1, YMMWORD PTR [rsp+40]
  vmulpd ymm0, ymm7, YMMWORD PTR [rsp+136]
  vmovapd ymm2, YMMWORD PTR [rsp+72]
  vmovdqa xmm6, XMMWORD PTR [rdx+112]
  vmovaps XMMWORD PTR [rsp+232], xmm5
  vmovapd ymm5, YMMWORD PTR [rsp+104]
  vmovdqa xmm4, xmm0
  vmovapd YMMWORD PTR [rsp-120], ymm0
  vmulpd ymm0, ymm1, YMMWORD PTR [rsp+168]
  vmovaps XMMWORD PTR [rsp+248], xmm6
  vmovaps XMMWORD PTR [rdi], xmm4
  vmovdqa xmm4, XMMWORD PTR [rsp-104]
  vmovdqa xmm3, xmm0
  vmovapd YMMWORD PTR [rsp-88], ymm0
  vmulpd ymm0, ymm2, YMMWORD PTR [rsp+200]
  vmovaps XMMWORD PTR [rdi+32], xmm3
  vmovdqa xmm3, XMMWORD PTR [rsp-72]
  vmovaps XMMWORD PTR [rdi+16], xmm4
  vmovaps XMMWORD PTR [rdi+48], xmm3
  vmovdqa xmm2, xmm0
  vmovapd YMMWORD PTR [rsp-56], ymm0
  vmulpd ymm0, ymm5, YMMWORD PTR [rsp+232]
  vmovdqa xmm6, XMMWORD PTR [rsp-40]
  vmovaps XMMWORD PTR [rdi+64], xmm2
  vmovaps XMMWORD PTR [rdi+80], xmm6
  vmovapd YMMWORD PTR [rsp-24], ymm0
  vmovdqa xmm7, XMMWORD PTR [rsp-8]
  vmovaps XMMWORD PTR [rdi+96], xmm0
  vmovaps XMMWORD PTR [rdi+112], xmm7
  vzeroupper
  leave
  ret
//-----------------------------------------------------------

Curiously, the current trunk version of gcc available at godbolt as of writing this produces this instead:

//-----------------------------------------------------------
  push rbp
  mov rax, rdi
  mov rbp, rsp
  and rsp, -32
  sub rsp, 8
  vmovapd ymm0, YMMWORD PTR [rsi]
  vmovapd ymm2, YMMWORD PTR [rsi+32]
  vmulpd ymm7, ymm0, YMMWORD PTR [rdx]
  vmulpd ymm1, ymm2, YMMWORD PTR [rdx+32]
  vmovapd ymm4, YMMWORD PTR [rsi+64]
  vmovapd ymm6, YMMWORD PTR [rsi+96]
  vmulpd ymm3, ymm4, YMMWORD PTR [rdx+64]
  vmovapd YMMWORD PTR [rsp-120], ymm7
  mov rcx, QWORD PTR [rsp-112]
  vmovdqa xmm0, XMMWORD PTR [rsp-120]
  mov QWORD PTR [rdi+8], rcx
  mov rcx, QWORD PTR [rsp-104]
  vmulpd ymm5, ymm6, YMMWORD PTR [rdx+96]
  vmovapd YMMWORD PTR [rsp-24], ymm1
  mov QWORD PTR [rdi+16], rcx
  vmovq QWORD PTR [rdi], xmm0
  mov rcx, QWORD PTR [rsp-16]
  mov rdi, QWORD PTR [rsp-96]
  mov QWORD PTR [rax+40], rcx
  mov QWORD PTR [rax+24], rdi
  mov rcx, QWORD PTR [rsp]
  mov rdi, QWORD PTR [rsp-8]
  vmovdqa xmm0, XMMWORD PTR [rsp-24]
  vmovapd YMMWORD PTR [rsp-56], ymm3
  mov QWORD PTR [rax+48], rdi
  mov QWORD PTR [rax+56], rcx
  vmovapd YMMWORD PTR [rsp-88], ymm5
  vmovq QWORD PTR [rax+32], xmm0
  vmovdqa xmm0, XMMWORD PTR [rsp-56]
  mov rdi, QWORD PTR [rsp-48]
  mov rdx, QWORD PTR [rsp-40]
  mov QWORD PTR [rax+72], rdi
  mov QWORD PTR [rax+80], rdx
  mov rcx, QWORD PTR [rsp-32]
  mov rsi, QWORD PTR [rsp-80]
  mov rdi, QWORD PTR [rsp-72]
  mov rdx, QWORD PTR [rsp-64]
  vmovq QWORD PTR [rax+64], xmm0
  vmovdqa xmm0, XMMWORD PTR [rsp-88]
  mov QWORD PTR [rax+88], rcx
  mov QWORD PTR [rax+104], rsi
  mov QWORD PTR [rax+112], rdi
  mov QWORD PTR [rax+120], rdx
  vmovq QWORD PTR [rax+96], xmm0
  vzeroupper
  leave
  ret
//-----------------------------------------------------------


---


### compiler : `gcc`
### title : `[optimization] switch statements for state machines could be better`
### open_at : `2019-09-25T12:50:26Z`
### last_modified_date : `2021-08-14T09:37:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91898
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Interesting talk at cppcon'19 by Miro Knejp 'Non-conforming C++: the Secrets the Committee Is Hiding From You'  It'll be on youtube at some point.


This PR concerns his observations about a common VM implementation technique.

We have an array of instructions, and we execute these one after eachother.  The natural way to write this would be:

for (int index = 0; ; index++)
  switch (insns[index]) {
    case ADD: ... break;
    case SUB: ... break;
    ...
    case END: goto end;
 } end:;

this leads to code gen that looks like:

loop:
  r = insn[index];
  index = index + 1;
  [range check on r]
  addr = jmptable[r];
  jmp [addr];
ADD: ...
  jmp loop

etc.  This performs poorly because all the dispatches go through the single jmp[addr] instruction, and the CPU's indirect branch predictor has no idea where it'll go.  But there are patterns in the sequence of virtual instructions, just as there are in real code!

Hence, people use the GCC label-address extension and write:
static void *const dispatch[] = {&&ADD, &&SUB, ... };

index = 0; goto *dispatch[insn[index++]];

ADD: ... goto *dispatch[insn[index++]];
SUB: ... goto *dispatch[insn[index++]];

code generation looks like:

ADD: ...
   r = insn[index];
   index++;
   addr = dispatch[r];
   jmp [addr];

This is faster because there are per-virtual-insn indirect jumps, and the branch predictor does much better.

(We're also eliding the range check, I do not know how much that would cost if it was added).

Hence, it might be worthwhile duplicating the switch's dispatch logic at the end of every case block.


---


### compiler : `gcc`
### title : `Merge constant literals`
### open_at : `2019-09-25T13:01:19Z`
### last_modified_date : `2023-02-01T18:58:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91899
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Consider the example:

static const char data1[] = {'t','e','s','t'};
static const char data2[] = "test test";

bool index0(const char* cmp) {
    return cmp == data1 || cmp == data2;
}

Right now GCC generates suboptimal assembly:

index0(char const*):
  mov eax, offset data1
  cmp rdi, rax
  sete cl
  mov eax, offset data2
  cmp rdi, rax
  sete al
  or al, cl
  ret
data1:
  .ascii "test"

data2:
  .asciz "test test"

A more efficient way to generate the code is to merge `data1` and `data2`:

index0(char const*):
  mov eax, offset data
  cmp rdi, rax
  sete al
  ret
data:
  .ascii "test test"


Constant literals merging significantly reduces binary size and cache misses.


---


### compiler : `gcc`
### title : `unchanged stack array not optimized away in favor off constant pool entry`
### open_at : `2019-09-25T14:01:19Z`
### last_modified_date : `2019-09-26T10:17:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91901
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 46940
full test code

#include <https://github.com/Neargye/magic_enum/raw/master/include/magic_enum.hpp>

enum class E
{
  E00, E01, E02, E03, E04, E05, E06, E07, E08, E09,
  E10, E11, E12, E13, E14, E15, E16, E17, E18, E19,
  E20, E21, E22, E23, E24, E25, E26, E27, E28, E29,
  E30, E31, E32, E33, E34, E35, E36, E37, E38, E39,
  E40, E41, E42, E43, E44, E45, E46, E47, E48, E49,
  E50, E51, E52, E53, E54, E55, E56, E57, E58, E59,
  E60, E61, E62, E63,
};

#include <cstdio>
void test(Color c)
{
    auto name = magic_enum::enum_name(c);
    printf(name.data());
}

$ g++ -O2 -std=c++2a
It generates the lookup table in .rodata but then still copies it to the stack inside name<E>(E): https://godbolt.org/z/cDp30l

Remove E63 to see the initialization being completely inlined, potentially resulting in even larger code.


---


### compiler : `gcc`
### title : `[8 Regression] Performance regression on 8.3.0 with -O3 and avx`
### open_at : `2019-09-30T09:27:53Z`
### last_modified_date : `2021-05-14T13:18:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91934
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `normal`
### contents :
Created attachment 46980
Function where problem occurs

GCC version is 8.3.0 building on Gentoo.
Configured with: /var/tmp/portage/sys-devel/gcc-8.3.0-r1/work/gcc-8.3.0/configure --host=x86_64-pc-linux-gnu --build=x86_64-pc-linux-gnu --prefix=/usr --bindir=/usr/x86_64-pc-linux-gnu/gcc-bin/8.3.0 --includedir=/usr/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include --datadir=/usr/share/gcc-data/x86_64-pc-linux-gnu/8.3.0 --mandir=/usr/share/gcc-data/x86_64-pc-linux-gnu/8.3.0/man --infodir=/usr/share/gcc-data/x86_64-pc-linux-gnu/8.3.0/info --with-gxx-include-dir=/usr/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include/g++-v8 --with-python-dir=/share/gcc-data/x86_64-pc-linux-gnu/8.3.0/python --enable-languages=c,c++,go,fortran --enable-obsolete --enable-secureplt --disable-werror --with-system-zlib --enable-nls --without-included-gettext --enable-checking=release --with-bugurl=https://bugs.gentoo.org/ --with-pkgversion='Gentoo 8.3.0-r1 p1.1' --disable-esp --enable-libstdcxx-time --enable-shared --enable-threads=posix --enable-__cxa_atexit --enable-clocale=gnu --enable-multilib --with-multilib-list=m32,m64 --disable-altivec --disable-fixed-point --enable-targets=all --enable-libgomp --disable-libmudflap --disable-libssp --disable-libmpx --disable-systemtap --enable-vtable-verify --enable-lto --without-isl --enable-default-pie --enable-default-ssp
Thread model: posix
gcc version 8.3.0 (Gentoo 8.3.0-r1 p1.1) 


Building my source with:
-Wall -Wextra -Wshadow -pedantic -march=native -mavx -mavx2 -O3 -g -save-temps   -std=gnu99

resulted .i file is attached. I got similar results on Debian 10 and godbolt.org (https://godbolt.org/z/-0cGgu)

The problem is low performance when building with 8.3.0. I tested this code on 7.4.0, 8.3.0, 9.1.0 and generated assembler in 8.3.0 contains some strange SIMD instructions before main work. Problem occurs only with -O3 flag.


---


### compiler : `gcc`
### title : `__builtin_bswap16 loop optimization`
### open_at : `2019-09-30T16:22:11Z`
### last_modified_date : `2019-10-21T08:55:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91940
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 46984
code snippet

Hello,

I am using "gcc (SUSE Linux) 9.2.1 20190903 [gcc-9-branch revision 275330]" and I see the following performance issue with the __builtin_bswap16() on x86_64 platform.

Attached here is code sample implementing byte swapping for arrays of 2-byte words.
I see that the following code (when compiled with -O3)

inline void swab_bi(const void* from, void* to, std::size_t size) {
        const auto begin = reinterpret_cast<const std::uint16_t*>(from);
        const auto end = reinterpret_cast<const std::uint16_t*>(reinterpret_cast<const std::uint8_t*>(from) + size);
        auto out = reinterpret_cast<std::uint16_t*>(to);

        for(auto it = begin; it != end; ++it) {
                *(out++) = __builtin_bswap16(*it);
        }
}

takes 0.023 sec. on average to execute on my hardware (Intel Core-i5).
While the following code

inline void swab(const void* from, void* to, std::size_t size) {
        const auto begin = reinterpret_cast<const std::uint16_t*>(from);
        const auto end = reinterpret_cast<const std::uint16_t*>(reinterpret_cast<const std::uint8_t*>(from) + size);
        auto out = reinterpret_cast<std::uint16_t*>(to);

        for(auto it = begin; it != end; ++it) {
                *(out++) = ((*it & 0xFF) << 8) | ((*it & 0xFF00) >> 8);
        }
}

is *more* efficiently. It takes only 0.011 sec.

When I try to dump assembler output for both function I see that packed instructions are used for the latter case:

        movdqu  0(%rbp,%rax), %xmm0
        movdqa  %xmm0, %xmm1
        psllw   $8, %xmm0
        psrlw   $8, %xmm1
        por     %xmm1, %xmm0
        movups  %xmm0, (%r12,%rax)
        addq    $16, %rax

while rolw is used for the former case:

        movzwl  0(%rbp,%rax), %edx
        rolw    $8, %dx
        movw    %dx, (%r12,%rax)
        addq    $2, %rax


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] gcc.dg/vect/pr66142.c should not need early inlining to be vectorized since r10-3311-gff6686d2e5f797d6`
### open_at : `2019-10-01T17:01:34Z`
### last_modified_date : `2023-07-07T10:36:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91954
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Currently we need early inlinining to vectorize the testcase even though late inlining does all necessary job. This seems like pass ordering problem.


---


### compiler : `gcc`
### title : `missing simplification for (C - a) << N`
### open_at : `2019-10-02T12:32:51Z`
### last_modified_date : `2021-08-10T19:01:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91965
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Noticed this issue when preparing a testcase for PR 87047.  We do not simplify
(1048575ull - x) << 44 on GIMPLE:

unsigned long long foo(unsigned long long a)
{
    return (1048575 - a) << 44;
}
void bar(unsigned long long *a)
{
    for (int i = 0; i < 1024; i++)
        a[i] = foo(a[i]);
}

we use (~a)<<44 for 'foo', but that is thanks to combine, in 'bar' combine doesn't make the same transform (the 1048575 constant is moved into a pseudo in another BB, and combine doesn't know that's the only use):

foo:
        movq    %rdi, %rax
        notq    %rax
        salq    $44, %rax
        ret
bar:
        leaq    8192(%rdi), %rcx
        movl    $1048575, %edx
.L4:
        movq    %rdx, %rax
        subq    (%rdi), %rax
        addq    $8, %rdi
        salq    $44, %rax
        movq    %rax, -8(%rdi)
        cmpq    %rcx, %rdi
        jne     .L4
        ret

Do we want to handle this early on via match.pd? Perhaps also applies to simplifying (a +- C) << N.


---


### compiler : `gcc`
### title : `worse code for small array copy using pointer arithmetic than array indexing`
### open_at : `2019-10-02T18:38:11Z`
### last_modified_date : `2023-08-15T09:09:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91975
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
When the contents of one small array is copied into another, GCC with -O2 generates more or less optimal code depending on whether the source array is statically initialized, and also depending on whether the copying takes place using pointer arithmetic and dereferencing or using array indexing.  Some copies are transformed into series of assignments while others remain as loops, while others still into a single MEM_REF statement.

The emitted assembly is also more or less efficient.

-O3 is required to get the same optimally efficient code.

Clang emits optimally efficient code for all forms at -O2.

It seems that since GCC can make some of the functions below optimally efficient it should be able to do it for all of them.

$ cat b.c && gcc -DT=int -O2 -S -fdump-tree-optimized=/dev/stdout b.c
const T a[] = { 0, 1, 2, 3, 4, 5, 6, 7 };
T b[sizeof a / sizeof *a];

void f0 (void)   // near optimal
{
  const T *s = a;
  T *d = b;
  for (unsigned i = 0; i != sizeof a / sizeof *a; ++i)
    d[i] = s[i];
}

void g0 (void)   // suboptimal
{
  const T *s = a;
  T *d = b;
  for (unsigned i = 0; i != sizeof a / sizeof *a; ++i)
    *d++ = *s++;
}

extern const T c[sizeof a / sizeof *a];

void f1 (void)   // optimal
{
  const T *s = c;
  T *d = b;
  for (unsigned i = 0; i != sizeof a / sizeof *a; ++i)
    d[i] = s[i];
}

void g1 (void)   // optimal
{
  const T *s = c;
  T *d = b;
  for (unsigned i = 0; i != sizeof a / sizeof *a; ++i)
    *d++ = *s++;
}


;; Function f0 (f0, funcdef_no=0, decl_uid=1926, cgraph_uid=1, symbol_order=2)

f0 ()
{
  <bb 2> [local count: 119292717]:
  MEM <unsigned long> [(int *)&b] = 4294967296;
  MEM <unsigned long> [(int *)&b + 8B] = 12884901890;
  MEM <unsigned long> [(int *)&b + 16B] = 21474836484;
  MEM <unsigned long> [(int *)&b + 24B] = 30064771078;
  return;

}



;; Function g0 (g0, funcdef_no=1, decl_uid=1935, cgraph_uid=2, symbol_order=3)

g0 ()
{
  sizetype ivtmp.25;
  int prephitmp_4;
  int pretmp_5;

  <bb 2> [local count: 119292716]:

  <bb 3> [local count: 954449108]:
  # prephitmp_4 = PHI <0(2), pretmp_5(4)>
  # ivtmp.25_2 = PHI <0(2), ivtmp.25_15(4)>
  MEM[symbol: b, index: ivtmp.25_2, offset: 0B] = prephitmp_4;
  ivtmp.25_15 = ivtmp.25_2 + 4;
  if (ivtmp.25_15 != 32)
    goto <bb 4>; [87.50%]
  else
    goto <bb 5>; [12.50%]

  <bb 4> [local count: 835156388]:
  pretmp_5 = MEM[symbol: a, index: ivtmp.25_15, offset: 0B];
  goto <bb 3>; [100.00%]

  <bb 5> [local count: 119292717]:
  return;

}



;; Function f1 (f1, funcdef_no=2, decl_uid=1945, cgraph_uid=3, symbol_order=4)

f1 ()
{
  <bb 2> [local count: 119292717]:
  MEM <unsigned char[32]> [(char * {ref-all})&b] = MEM <unsigned char[32]> [(char * {ref-all})&c];
  return;

}



;; Function g1 (g1, funcdef_no=3, decl_uid=1954, cgraph_uid=4, symbol_order=5)

g1 ()
{
  <bb 2> [local count: 119292717]:
  MEM <unsigned char[32]> [(char * {ref-all})&b] = MEM <unsigned char[32]> [(char * {ref-all})&c];
  return;

}


---


### compiler : `gcc`
### title : `Speed degradation because of inlining a register clobbering function`
### open_at : `2019-10-03T09:27:46Z`
### last_modified_date : `2020-01-10T06:48:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91981
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Consider the example that is a simplified version of boost::container::small_vector:


#define MAKE_INLINING_BAD 1

struct vector {
    int* data_;
    int* capacity_;
    int* size_;
    
    void push_back(int v) {
        if (capacity_ > size_) {
            *size_ = v;
            ++size_;
        } else {
            reallocate_and_push(v);
        }
    }

    void reallocate_and_push(int v)
#if MAKE_INLINING_BAD
    {
        // Just some code that clobbers many registers.
        // You may skip reading it
        const auto old_cap = capacity_ - data_; 
        const auto old_size = capacity_ - size_; 
        const auto new_cap = old_cap * 2 + 1;

        auto new_data_1 = new int[new_cap];
        auto new_data = new_data_1;
        for (int* old_data = data_; old_data != size_; ++old_data, ++new_data) {
            *new_data = *old_data;
        }

        delete[] data_;
        data_ = new_data_1;
        size_ = new_data_1 + old_size;
        capacity_ = new_data_1 + new_cap;

        *size_ = v;
        ++size_;
    }
#else
    ;
#endif
};

void bad_inlining(vector& v) {
    v.push_back(42);
}


With `#define MAKE_INLINING_BAD 0` the generated code is quite good:

bad_inlining(vector&):
  mov rax, QWORD PTR [rdi+16]
  cmp QWORD PTR [rdi+8], rax
  jbe .L2
  mov DWORD PTR [rax], 42
  add rax, 4
  mov QWORD PTR [rdi+16], rax
  ret
.L2:
  mov esi, 42
  jmp vector::reallocate_and_push(int)

However, with `#define MAKE_INLINING_BAD 1` the compiler decides to inline the `reallocate_and_push` function that clobbers many registers. So the compiler stores the values of those registers on the stack before doing the cmp+jbe:

bad_inlining(vector&):
  push r13 ; don't need those for the `(capacity_ > size_)` case
  push r12     ; likewise
  push rbp     ; likewise
  push rbx     ; likewise
  mov rbx, rdi ; likewise
  sub rsp, 8   ; likewise
  mov rdx, QWORD PTR [rdi+8]
  mov rax, QWORD PTR [rdi+16]
  cmp rdx, rax
  jbe .L2
  mov DWORD PTR [rax], 42
  add rax, 4
  mov QWORD PTR [rdi+16], rax
  add rsp, 8 ; don't need those for the `(capacity_ > size_)` case
  pop rbx     ; likewise
  pop rbp     ; likewise
  pop r12     ; likewise
  pop r13     ; likewise
  ret
.L2: 
  ; vector::reallocate_and_push(int) implementation goes here

This greatly degrades the performance of the first branch (more than x3 degradation in real code).


The possible fix would be to place all the push/pop operations near the inlined `reallocate_and_push`:

bad_inlining(vector&):
  mov rax, QWORD PTR [rdi+16]
  cmp QWORD PTR [rdi+8], rax
  jbe .L2
  mov DWORD PTR [rax], 42
  add rax, 4
  mov QWORD PTR [rdi+16], rax
  ret
.L2: 
  push r13
  push r12
  push rbp
  push rbx
  mov rbx, rdi
  sub rsp, 8
  ; vector::reallocate_and_push(int) implementation goes here
  add rsp, 8
  pop rbx
  pop rbp
  pop r12
  pop r13
  ret

Godbolt playground: https://godbolt.org/z/oDutOd


---


### compiler : `gcc`
### title : `missed loop unrolling optimization`
### open_at : `2019-10-03T23:25:39Z`
### last_modified_date : `2019-10-04T08:09:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91986
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `normal`
### contents :
#include <cstdint>
#include <utility>
const int N = 4;
void bitreverse(int* data)
{
    uint32_t j = 0;
    for (uint32_t i = 0; i < N; ++i)
    {
        if (j > i)
            std::swap(data[i], data[j]);

        uint32_t k = N/2;
        while (k <= j)
        {
            j -= k;
            k /= 2;
        }
        j += k;
    }
}

Even with -O3 gcc doesn't fully unroll the loop but still there are quite some redundant instructions: https://godbolt.org/z/cA-S8J
It's similar with N=8: https://godbolt.org/z/niZPeS

Interestingly if you change the swap line slightly clang suddenly starts using pshufd which seems quite clever: https://godbolt.org/z/5YJnJ1


---


### compiler : `gcc`
### title : `fold non-constant strlen relational expressions`
### open_at : `2019-10-04T20:20:13Z`
### last_modified_date : `2019-10-16T17:38:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91996
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Now that the strlen pass is integrated with EVRP it's become possible to make use of the EVRP ranges to fold even some relational expressions involving non-constant strlen values.  For example, in the test case below, since a's length is known to be at least 7, copies of its first N characters must be at least as long as MIN (N, strlen (a)].

$ cat b.c && gcc -S -O2 -Wall -fdump-tree-optimized=/dev/stdout b.c
extern char a[32], b[32];

void f (void)
{
  if (__builtin_strlen (a) < 7)
    return;

  __builtin_memcpy (b, a, sizeof b);

  if (__builtin_strlen (b) < 7)   // can be folded to false
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1926, cgraph_uid=1, symbol_order=0)

Removing basic block 6
Removing basic block 7
f ()
{
  long unsigned int _1;
  long unsigned int _2;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strlen (&a);
  if (_1 <= 6)
    goto <bb 5>; [34.00%]
  else
    goto <bb 3>; [66.00%]

  <bb 3> [local count: 708669605]:
  MEM <unsigned char[32]> [(char * {ref-all})&b] = MEM <unsigned char[32]> [(char * {ref-all})&a];
  _2 = __builtin_strlen (&b);
  if (_2 <= 6)
    goto <bb 4>; [0.00%]
  else
    goto <bb 5>; [100.00%]

  <bb 4> [count: 0]:
  __builtin_abort ();

  <bb 5> [local count: 1073741829]:
  return;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] switch code generation regression`
### open_at : `2019-10-06T12:35:07Z`
### last_modified_date : `2023-09-21T14:01:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92005
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
The following code:

```
template<class... Ts> struct overloaded : Ts... { using Ts::operator()...; };
template<class... Ts> overloaded(Ts...) -> overloaded<Ts...>;

struct T0 {};
struct T1 {};
struct T2 {};
struct T3 {};
struct T4 {};

struct variant
{
    unsigned index_;

    union
    {
        T0 t0_;
        T1 t1_;
        T2 t2_;
        T3 t3_;
        T4 t4_;
    };
};

template<class F> int visit( F f, variant const& v )
{
    switch( v.index_ )
    {
        case 0: return f( v.t0_ );
        case 1: return f( v.t1_ );
        case 2: return f( v.t2_ );
        case 3: return f( v.t3_ );
        case 4: return f( v.t4_ );
        default: __builtin_unreachable();
    }
}

int do_visit(variant const& v) {
     return visit(overloaded{
        [](T0 val) { return 3; },
        [](T1 val) { return 5; },
        [](T2 val) { return 8; },
        [](T3 val) { return 9; },
        [](T4 val) { return 10; }
    }, v);
}
```

(https://godbolt.org/z/uxQ6KF)

generates

```
do_visit(variant const&):
        mov     eax, DWORD PTR [rdi]
        jmp     [QWORD PTR .L4[0+rax*8]]
.L4:
        .quad   .L8
        .quad   .L7
        .quad   .L9
        .quad   .L5
        .quad   .L3
.L9:
        mov     eax, 8
        ret
.L7:
        mov     eax, 5
        ret
.L8:
        mov     eax, 3
        ret
.L5:
        mov     eax, 9
        ret
.L3:
        mov     eax, 10
        ret
```

with the current gcc trunk on godbolt.org (g++ (Compiler-Explorer-Build) 10.0.0 20191005 (experimental)) and

```
do_visit(variant const&):
        mov     eax, DWORD PTR [rdi]
        mov     eax, DWORD PTR CSWTCH.7[0+rax*4]
        ret
CSWTCH.7:
        .long   3
        .long   5
        .long   8
        .long   9
        .long   10
```

with gcc 9.2.


---


### compiler : `gcc`
### title : `Extremely inefficient x86_64 code for trivally copyable types passed in registers.`
### open_at : `2019-10-09T16:17:22Z`
### last_modified_date : `2021-08-23T07:16:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92038
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
The following code:

    #include <variant>
    void f(std::variant<int, int>);
    void g() { f({}); }

When compiled with `g++-9.2 -std=gnu++17 -O3 -march=skylake` generates the following assembly:

    g():
        mov     DWORD PTR [rsp-16], 0
        mov     BYTE PTR [rsp-12], 0
        mov     rdi, QWORD PTR [rsp-16]
        jmp     f(std::variant<int, int>)

Which is rather poor: unnecessary memory stores; dependency of rdi on the value of 3 bytes of padding at [rsp-11], [rsp-10], [rsp-9] which are unset, which may prevent store-to-load forwarding.

`clang++-8.0 -std=gnu++17 -O3 -march=skylake` generates the expected assembly:

    g():
        xor     edi, edi
        jmp     f(std::variant<int, int>)


---


### compiler : `gcc`
### title : ``final` does not cause devirtualization of nested calls`
### open_at : `2019-10-10T16:33:51Z`
### last_modified_date : `2020-01-29T13:35:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92054
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.0`
### severity : `normal`
### contents :
Consider the example:


struct A {
    virtual int f() { return 0; }
    virtual int g() { return f() + 40; } 
};

struct B2 final : A {
    int f() override { return 42; }
};

int test(B2& b) {
    return b.g();
}


GCC-10 generates the assembly that does a fair vptr call. However, `B2` is final, so any call to the virtual functions of `A` end up with a call to the same function in `B2`. So `B2::g()` should inline the `A::g()` and get optimized to:

int test(B2& b) { return B2::f() + 40; }


Which is just 82, because `B2::f()` always returns 42.

Godbolt playground: https://godbolt.org/z/PJ4nL-


---


### compiler : `gcc`
### title : `extracting element from NEON float-vector moves to/from integer register`
### open_at : `2019-10-12T09:50:55Z`
### last_modified_date : `2019-10-14T13:55:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92075
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
On ARM, when extracting an element from a float32x2_t expression, i.e.:

   float32x2_t v = (...);
   float v0 = v[0];

In most cases, gcc moves the element to an general-purpose register and back to a VFP/Neon register, even at -Ofast. It doesn't seem to happen when v is an argument or the return value of a non-inline function, but it does happen e.g. when v is an arithmetic expression or produced by a NEON intrinsic or inline asm. It doesn't seem to matter how v0 is consumed (i.e. by returning it, passing it as argument to a function, or consuming it by inline asm).

Some test-cases:

#include <arm_neon.h>

float test1( float32x2_t v ) {
        return (v + v)[0];
}

void test2() {
        float32x2_t v;
        asm( "" : "=w"(v) );
        float v0 = v[0];
        asm( "" :: "w"(v0) );
}

void foo( float );
void test3( uint32x2_t v ) {
        foo( vcvt_n_f32_u32( v, 32 )[0] );
}

output produced by "arm-linux-gnueabihf-gcc-9 (Debian 9.2.1-8) 9.2.1 20190909" with -Ofast -mcpu=cortex-a8 -mfpu=neon, reformatted for readability:

test1:
        vadd.f32  d0, d0, d0
        vmov.32   r3, d0[0]
        vmov      s0, r3
        bx        lr
test2:
        vmov.32  r3, d16[0]
        vmov     s15, r3
        bx       lr
test3:
        vcvt.f32.u32  d0, d0, #32
        vmov.32       r3, d0[0]
        vmov          s0, r3
        b             foo(PLT)

This is especially bad on the cortex-A8, where moving from a VFP/Neon register to an general purpose register causes a severe pipeline stall.

Note btw how in test1 and test3 no move is needed at all: the final move destination is the register it originally came from, and a different choice of register allocation can make this also true in test2.


---


### compiler : `gcc`
### title : `Missed CSE of _mm512_set1_epi8(c) with _mm256_set1_epi8(c)`
### open_at : `2019-10-13T14:01:40Z`
### last_modified_date : `2023-06-13T07:43:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92080
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
As a workaround for PR 82887 some code (e.g. a memset) uses

__m512i zmm = _mm512_set1_epi8((char)c);
__m256i ymm = _mm256_set1_epi8((char)c);

instead of 

  ymm = _mm512_castsi512_si256(zmm);

(found in the persistent-memory library https://github.com/pmem/pmdk/blob/a6031710f7c102c6b8b6b19dc9708a3b7d43e87b/src/libpmem/x86_64/memset/memset_nt_avx512f.h#L193 )

Obviously we'd like to CSE that instead of actually broadcasting twice.  MVCE:

#include <immintrin.h>

__m512i sinkz;
__m256i sinky;
void foo(char c) {
    sinkz = _mm512_set1_epi8(c);
    sinky = _mm256_set1_epi8(c);
}

https://godbolt.org/z/CeXhi8  g++ (Compiler-Explorer-Build) 10.0.0 20191012

# g++ -O3 -march=skylake-avx512  (AVX512BW + AVX512VL are the relevant ones)
foo(char):
        vpbroadcastb    %edi, %zmm0
        vmovdqa64       %zmm0, sinkz(%rip)
        vpbroadcastb    %edi, %ymm0          # wasted insn
        vmovdqa64       %ymm0, sinky(%rip)   # wasted EVEX prefix
        vzeroupper
        ret

Without AVX512VL it wastes even more instructions (vmovd + AVX2 vpbroadcastb xmm,ymm), even though AVX512BW vpbroadcastb zmm does set the YMM register.  (There are no CPUs with AVX512BW but not AVX512VL; if people compile that way it's their own fault.  But this might be relevant for set1_epi32() on KNL).

Clang finds this optimization, and uses a shorter vmovdqa for the YMM store saving another 2 bytes of code size:

        vpbroadcastb    %edi, %zmm0
        vmovdqa64       %zmm0, sinkz(%rip)
        vmovdqa         %ymm0, sinky(%rip)
        vzeroupper
        ret


---


### compiler : `gcc`
### title : `Provide way to avoid saving callee-saved registers in functions without callers`
### open_at : `2019-10-14T12:00:31Z`
### last_modified_date : `2022-10-01T11:12:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92086
### status : `NEW`
### tags : `missed-optimization`
### component : `c`
### version : `10.0`
### severity : `enhancement`
### contents :
In some cases, it is desirable as an optimization not to save any callee-saved registers in the function prologue.  This is common for functions which are at the lowest frame, where there is nothing to return to, and unwinding cannot proceed, either.  However, GCC seems to generate code for saving registers even for such functions, for example:

int f1 (int);

__attribute__ ((noreturn, nothrow))
void
f2 (void)
{
  int x1 = f1 (1);
  int x2 = f1 (2);
  int x3 = f1 (3);
  int x4 = f1 (4);
  f1 (x1);
  f1 (x2);
  f1 (x3);
  f1 (x4);
  __builtin_unreachable ();
}

yields this on x86-64 (with GCC 9):

f2:
	pushq	%r14
	movl	$1, %edi
	pushq	%r13
	pushq	%r12
	pushq	%rbp
	subq	$8, %rsp
	call	f1@PLT
	movl	$2, %edi
	movl	%eax, %r14d
	call	f1@PLT
	movl	$3, %edi
	movl	%eax, %r13d
	call	f1@PLT
	movl	$4, %edi
	movl	%eax, %r12d
	call	f1@PLT
	movl	%r14d, %edi
	movl	%eax, %ebp
	call	f1@PLT
	movl	%r13d, %edi
	call	f1@PLT
	movl	%r12d, %edi
	call	f1@PLT
	movl	%ebp, %edi
	call	f1@PLT

If it is not possible to unwind into the caller of f2 (say because it does not exist), there is no impact on debugging experience because the saved values are useless even for debugging.

I've reported this bug against the C front end because we may need a new attribute for this.  (If noreturn+nothrown cannot be repurposed.)

Mailing list discussion: https://gcc.gnu.org/ml/gcc-help/2019-10/msg00052.html


---


### compiler : `gcc`
### title : `[9 Regression] After r262333, the following code cannot be vectorized on powerpc64le.`
### open_at : `2019-10-15T03:29:08Z`
### last_modified_date : `2019-12-02T08:14:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92098
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 47035
dump file(Includes dump files that can be vectorized and not vectorized)

For the following code
---
#define NIL 0

typedef struct {
  unsigned int hash_size;
  unsigned short * head, * prev;
  unsigned int w_size;
} deflate_state;

void slide_hash(deflate_state *s)
{
    unsigned n, m;
    unsigned short *p;
    unsigned int wsize = s->w_size;

    n = s->hash_size;
    p = &s->head[n];
    do {
        m = *--p;
        *p = (unsigned short)(m >= wsize ? m - wsize : NIL);
    } while (--n);
}
---

The compile command I used is 
cc1 a.c -Ofast  -fdump-tree-vect-details-all -fdump-tree-slp-details-all

we found r262333 will cause it can not be vectorized.  Because

a.c:20:5: note:   vect_is_simple_use: vectype vector(4) unsigned intD.4
a.c:20:5: note:   not vectorized: relevant stmt not supported: patt_37 = wsize_12 <= m_16;
a.c:20:5: note:  bad operation or unsupported loop bound.

But before the commit this code can be vectorized.

Attachment is the file I dumped


---


### compiler : `gcc`
### title : `fold strlen after strcmp(a, b) == 0`
### open_at : `2019-10-15T21:11:41Z`
### last_modified_date : `2020-01-29T13:42:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92112
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
After a strcmp call evaluates to zero the lengths of the two arguments can be assumed to be equal.  GCC could make use of that to fold strlen expressions such as those in the test case below but doesn't (yet).

$ cat a.c && gcc -S -O2 -Wall -Wextra -fdump-tree-optimized=/dev/stdout a.c
extern char a[], b[];

void f (void)
{
  if (__builtin_strcmp (a, b) != 0)
    return;

  if (__builtin_strlen (a) != __builtin_strlen (b))   // can be folded to false
    __builtin_abort ();
}


void g (void)
{
  if (__builtin_strlen (a) < 7)
    return;
  if (__builtin_strcmp (a, b) != 0)
    return;

  if (__builtin_strlen (b) < 7)   // can be folded to false
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=0)

Removing basic block 6
Removing basic block 7
f ()
{
  int _1;
  long unsigned int _2;
  long unsigned int _3;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strcmp (&a, &b);
  if (_1 != 0)
    goto <bb 5>; [34.00%]
  else
    goto <bb 3>; [66.00%]

  <bb 3> [local count: 708669605]:
  _2 = __builtin_strlen (&a);
  _3 = __builtin_strlen (&b);
  if (_2 != _3)
    goto <bb 4>; [0.00%]
  else
    goto <bb 5>; [100.00%]

  <bb 4> [count: 0]:
  __builtin_abort ();

  <bb 5> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1935, cgraph_uid=2, symbol_order=1)

Removing basic block 7
Removing basic block 8
Removing basic block 9
g ()
{
  long unsigned int _1;
  int _2;
  long unsigned int _3;

  <bb 2> [local count: 1073741823]:
  _1 = __builtin_strlen (&a);
  if (_1 <= 6)
    goto <bb 6>; [34.00%]
  else
    goto <bb 3>; [66.00%]

  <bb 3> [local count: 708669604]:
  _2 = __builtin_strcmp (&a, &b);
  if (_2 != 0)
    goto <bb 6>; [34.00%]
  else
    goto <bb 4>; [66.00%]

  <bb 4> [local count: 467721938]:
  _3 = __builtin_strlen (&b);
  if (_3 <= 6)
    goto <bb 5>; [0.00%]
  else
    goto <bb 6>; [100.00%]

  <bb 5> [count: 0]:
  __builtin_abort ();

  <bb 6> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `Missed vectorization for iteration dependent loads and simple multiplicative accumulators`
### open_at : `2019-10-16T18:51:29Z`
### last_modified_date : `2019-10-17T17:20:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92130
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 47051
Perlin2D noise mesh generation

So,

I do have pretty complex multi level loop spread across many functions, but it can be all vectorized, but under certain scenarios gcc does not vectorize it with gcc 9.2.1

I am attaching somehow simplified code with few defines inside to play with it.

The one exposed by default present the biggest challenge to gcc, despite me able to vectorize it manually.

I tested this on SSE2, AVX2 (cascadelake and znver2), AVX512 (-march=knm and -march=skylake-avx512) and ARM SVE, with all same effects. I am using associative math and other flags mentioned in the sourcefile at the top.

The high level overview is like this:

input: A, F, W, maxO, sufficiently aligned d.

foreach y:
  foreach x:
    float v = 0.0
    float a = 1.0
    float f = 1.0
    foreach o in [0, maxO):
      v += a * g(f * x, f * y, o, h(o, p))
      a *= A
      f *= F
    d[y*W + x] = v

where both g and h are pure functions (relatively complex tho) with no control flow or data dependent flow.

In some situations if a and f are replaced by a precomputed table of coefficient for every o, and then used as v += a[o] * g(f[o] * x, f[o] * y, h(o, p)), it does vectorize, but not always. h(o, p) could also be precomputed, but I didn't bother as it appears to not have any bad effect on vectorizer.

Vectorizater should vectorize along the 'foreach x', and compute multiple x-s per-lane completely independently. It is true that when updating a and f, each lane need to be duplicated, but that can be done by computing it scalarly, and then broadcasting, or by repeating same constants updates in each lane.


---


### compiler : `gcc`
### title : `new test case gcc.dg/vect/vect-cond-reduc-4.c fails with its introduction in r277067`
### open_at : `2019-10-16T19:38:42Z`
### last_modified_date : `2019-12-02T06:24:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92132
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
This fails on powerpc64 both BE and LE

spawn -ignore SIGHUP /home/seurer/gcc/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/build/gcc-test/gcc/ /home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/vect-cond-reduc-4.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -flto -ffat-lto-objects -maltivec -mvsx -mno-allow-movmisalign -ftree-vectorize -fno-tree-loop-distribute-patterns -fno-vect-cost-model -fno-common -O2 -fdump-tree-vect-details -lm -o ./vect-cond-reduc-4.exe
PASS: gcc.dg/vect/vect-cond-reduc-4.c -flto -ffat-lto-objects (test for excess errors)
Setting LD_LIBRARY_PATH to :/home/seurer/gcc/build/gcc-test/gcc::/home/seurer/gcc/build/gcc-test/gcc:/home/seurer/gcc/build/gcc-test/./gmp/.libs:/home/seurer/gcc/build/gcc-test/./prev-gmp/.libs:/home/seurer/gcc/build/gcc-test/./mpfr/src/.libs:/home/seurer/gcc/build/gcc-test/./prev-mpfr/src/.libs:/home/seurer/gcc/build/gcc-test/./mpc/src/.libs:/home/seurer/gcc/build/gcc-test/./prev-mpc/src/.libs:/home/seurer/gcc/build/gcc-test/./isl/.libs:/home/seurer/gcc/build/gcc-test/./prev-isl/.libs
Execution timeout is: 300
spawn [open ...]
PASS: gcc.dg/vect/vect-cond-reduc-4.c -flto -ffat-lto-objects execution test
gcc.dg/vect/vect-cond-reduc-4.c -flto -ffat-lto-objects : pattern found 0 times
FAIL: gcc.dg/vect/vect-cond-reduc-4.c -flto -ffat-lto-objects  scan-tree-dump-times vect "LOOP VECTORIZED" 2
PASS: gcc.dg/vect/vect-cond-reduc-4.c -flto -ffat-lto-objects  scan-tree-dump-times vect "condition expression based on integer induction." 2
testcase /home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/vect.exp completed in 2 seconds

		=== gcc Summary ===

# of expected passes		6
# of unexpected failures	2


---


### compiler : `gcc`
### title : `Implement popcountsi expansion for arm`
### open_at : `2019-10-17T09:52:00Z`
### last_modified_date : `2021-12-21T11:29:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92135
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
On aarch64 we can do SImode and DImode popcount operations using the AdvSIMD CNT instruction. As the comment in aarch64.md says:
;; Pop count be done via the "CNT" instruction in AdvSIMD.
;;
;; MOV	v.1d, x0
;; CNT	v1.8b, v.8b
;; ADDV b2, v1.8b
;; MOV	w0, v2.b[0]

We should be able to do a similar thing on arm, using the VCNT instruction. This just needs implementing as an expansion in arm.md.
int
foocnt (int a)
{
  return __builtin_popcount (a);
}

is the trivial testcase.
Haven't thought too much about the DImode case, but perhaps that can also be accelerated in similar ways


---


### compiler : `gcc`
### title : `clang vs gcc optimizing with adc/sbb`
### open_at : `2019-10-17T15:27:48Z`
### last_modified_date : `2023-05-07T16:15:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92140
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
The following code:

extern char table[];
extern int c, v;

void tst1 (void) { v += table[c] != 0; }
void tst2 (void) { v -= table[c] != 0; }
unsigned int tst3 (unsigned int n) { return n ? 2 : 1; }

compiled with gcc and clang see some optimizing opportunities for gcc.
Table with instruction generated:
        gcc    clang
tst1:     5        3
tst2:     5        3
tst3:     4        3

This is with:
gcc (GCC) 10.0.0 20191017 (experimental)
clang version 10.0.0 (trunk 373843)


---


### compiler : `gcc`
### title : `Enefficient x86_64 code`
### open_at : `2019-10-18T10:43:03Z`
### last_modified_date : `2021-08-22T00:56:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92149
### status : `NEW`
### tags : `ABI, missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
The following code:

    #include <array>
    #include <cstring>
    #include <cstdint>
    
    using std::uint64_t;
    using A = std::array<unsigned char, 6>;
    
    template<class T>
    constexpr A pack(T v) {
        using C = unsigned char;
        return {C(v), C(v >> 8), C(v >> 16), C(v >> 24), C(v >> 32), C(v >> 40)};
    }

    template<class T>
    constexpr A pack2(T v) {
        A r{};
        std::memcpy(r.data(), &v, 6);
        return r;
    }
    
    A f(uint64_t a) { return pack(a); }
    A f2(uint64_t a) { return pack2(a); }

When compiled with `clang++-9.0 -std=gnu++17 -O3 -march=skylake` produces the most efficient assembly:

    f(unsigned long):
            mov     rax, rdi
            ret
    f2(unsigned long):
            mov     rax, rdi
            ret

Whereas when compiled with `g++-9.2 -std=gnu++17 -O3 -march=skylake` the assembly is inefficient:

    f(unsigned long):
            mov     rax, rdi
            shr     rax, 32
            mov     BYTE PTR [rsp-2], al
            mov     rax, rdi
            shr     rax, 40
            mov     BYTE PTR [rsp-1], al
            movzx   eax, WORD PTR [rsp-2]
            sal     rax, 32
            mov     rdx, rax
            mov     eax, edi
            or      rax, rdx
            ret
    f2(unsigned long):
            movabs  rax, 281474976710655
            and     rax, rdi
            ret

Why does g++ emit such verbose code please?


---


### compiler : `gcc`
### title : `Spurious register copying`
### open_at : `2019-10-18T13:34:24Z`
### last_modified_date : `2023-06-27T03:54:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92151
### status : `UNCONFIRMED`
### tags : `inline-asm, missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.1.0`
### severity : `normal`
### contents :
Created attachment 47066
Code to demonstrate the issue.

The attached code is a fragment of a larger function.  The larger function is a string copy which takes string 's', destination-buffer 'd' and end-of-destination-buffer 'e'.  The body of the function is inline asm, 

The problem I have tripped over is the compiler makes spurious copies of registers.  In some cases I have seen it PUSH %RBX in order to do so.

Compiling the enclosed with gcc 9.1:

  1) with "#define TWITCH 1" produces spurious copying of %rdi

  2) with "#define TWITCH 0" produces no spurious copying

The results for the two cases are given below, marked up to try to show what is going on.

The extra shuffling of registers is ugly as sin, but not I suppose a big overhead, at least until PUSH/POP get added to make a register available for this nonsense :-(

For completeness, I tried gcc 8.1 which does some similar (but different) spurious copying -- see Result 3, below.

_____________________________________________________________
Result 1 -- gcc 9.1 -O3 -- #define TWITCH 1

qstpxcpy_asm0:
// Arguments: d -- %rdi, s -- %rsi, e -- %rdx
//    Locals: w, t

              movq    %rdi, %rcx    // inserted by compiler -- gcc 9.1

    mov  (%rsi), %rax   // w  = *s
    lea  -8(%rdx), %rdx // e -= 8
    mov  %eax, %r11d    // t  = w
.L2:
              movq    %rcx, %rdi    // inserted by compiler

    mov  %rax, (%rcx)   // *d = w
    lea  8(%rdi), %rdi  // d += 8

              movq    %rdi, %rcx    // inserted by compiler

    cmp   %rdx, %rdi    // "d - e"   -- __asm__ goto
    jae   .L3           // quit if d >= e

    mov   %eax, %r11d   // t = w

    cmp   $-1, %r11     // check 't' -- __asm__ goto
    jnz   .L4           // j if at end

    mov   %rax, (%rdi)  // *d = w
    lea   8(%rdi), %rdi // d += 8

              movq    %rdi, %rcx    // inserted by compiler

    cmp   %rdx, %rdi    // "d - e"   -- __asm__ goto
    jae   .L3           // quit if d >= e

    mov   %eax, %r11d   // t = w

    cmp   $-1, %r11     // check 't' -- __asm__ goto
    jz    .L2           // j if not at end

.L4:
    lea   (%rcx, %r11), %rax    // return d + t
    ret
.L3:
    lea   (%rdx, %r11), %rax    // return e + t
    ret

_____________________________________________________________
Result 2 -- gcc 9.1 -O3 -- #define TWITCH 0

qstpxcpy_asm0:
// Arguments: d -- %rdi, s -- %rsi, e -- %rdx
//    Locals: w, t

    mov  (%rsi), %rax   // w  = *s
    lea  -8(%rdx), %rdx // e -= 8
    mov  %eax, %r11d    // t  = w
.L2:
    mov  %rax, (%rdi)   // *d = w
    lea  8(%rdi), %rdi  // d += 8

    cmp   %rdx, %rdi    // "d - e"   -- __asm__ goto
    jae   .L5           // quit if d >= e

    mov   %eax, %r11d   // t = w

    cmp   $-1, %r11     // check 't' -- __asm__ goto
    jnz   .L5           // j if at end

    mov   %rax, (%rdi)  // *d = w
    lea   8(%rdi), %rdi // d += 8

    cmp   %rdx, %rdi    // "d - e"   -- __asm__ goto
    jae   .L5           // quit if d >= e

    mov   %eax, %r11d   // t = w

    cmp   $-1, %r11     // check 't' -- __asm__ goto
    jz    .L5           // j if not at end

.L5:
    lea   (%rdx, %r11), %rax    // return e + t
    ret

_____________________________________________________________
Result 3 -- gcc 8.1 -O3 -- #define TWITCH 1

qstpxcpy_asm0:
// Arguments: d -- %rdi, s -- %rsi, e -- %rdx
//    Locals: w, t

    mov  (%rsi), %rax   // w  = *s
    lea  -8(%rdx), %rdx // e -= 8
    mov  %eax, %r11d    // t  = w
.L2:
              movq    %rdi, %rcx    // inserted by compiler

    mov  %rax, (%rdi)   // *d = w
    lea  8(%rcx), %rcx  // d += 8

              movq    %rcx, %rdi    // inserted by compiler

    cmp   %rdx, %rcx    // "d - e"   -- __asm__ goto
    jae   .L3           // quit if d >= e

    mov   %eax, %r11d   // t = w

    cmp   $-1, %r11     // check 't' -- __asm__ goto
    jnz   .L4           // j if at end

    mov   %rax, (%rcx)  // *d = w
    lea   8(%rcx), %rcx // d += 8

              movq    %rcx, %rdi    // inserted by compiler

    cmp   %rdx, %rcx        // "d - e"   -- __asm__ goto
    jae   .L3               // quit if d >= e

    mov   %eax, %r11d   // t = w

    cmp   $-1, %r11     // check 't' -- __asm__ goto
    jz    .L2           // j if not at end

.L4:
    lea   (%rdi, %r11), %rax    // return d + t
    ret
.L3:
    lea   (%rdx, %r11), %rax    // return e + t
    ret


---


### compiler : `gcc`
### title : `strlen(a) not folded after memset(a, 0, sizeof a)`
### open_at : `2019-10-18T19:31:37Z`
### last_modified_date : `2020-01-29T13:51:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92155
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Now that GCC unrolls memset-like loops with small numbers of iterations (pr91975) and transforms some of them into MEM_REF, the strlen pass can also determine the lengths of zeroed-out arrays to be zero.  This can be seen in function f below.

But GCC doesn't yet transform memset calls into the equivalent MEM_REFs, and the strlen pass for some reason can't figure out that the length of an array that's been zeroed-out by memset is also zero.  This missed optimization can be seen in function g below.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
extern char a4[4];
extern char b4[4];

void f (void)
{
  for (int i = 0; i != sizeof a4; ++i)
    a4[i] = 0;
  for (int i = 0; i != sizeof b4; ++i)
    b4[i] = 0;

  if (__builtin_strlen (a4) != __builtin_strlen (b4))
    __builtin_abort ();
}

void g (void)
{
  __builtin_memset (a4, 0, sizeof a4);
  __builtin_memset (b4, 0, sizeof b4);

  if (__builtin_strlen (a4) != __builtin_strlen (b4))
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=0)

f ()
{
  <bb 2> [local count: 214748369]:
  MEM <unsigned int> [(char *)&a4] = 0;
  MEM <unsigned int> [(char *)&b4] = 0;
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1943, cgraph_uid=2, symbol_order=1)

g ()
{
  long unsigned int _1;
  long unsigned int _2;

  <bb 2> [local count: 1073741824]:
  __builtin_memset (&a4, 0, 4);
  __builtin_memset (&b4, 0, 4);
  _1 = __builtin_strlen (&a4);
  _2 = __builtin_strlen (&b4);
  if (_1 != _2)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `Poor code generation for addcarry / subborrow`
### open_at : `2019-10-21T18:59:48Z`
### last_modified_date : `2021-02-02T16:58:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92168
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Real World Technology reader reports poor GCC code generation relative to Clang and MSVC for source code involving carry / borrow.

https://www.realworldtech.com/forum/?threadid=188061&curpostid=188061

with example at Compiler Explorer

https://godbolt.org/z/YYq6ou


---


### compiler : `gcc`
### title : `x86 backend claims V4SI multiplication support, preventing more optimal pattern`
### open_at : `2019-10-22T08:44:49Z`
### last_modified_date : `2020-01-29T14:06:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92175
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Costing has

19010         /* Without sse4.1, we don't have PMULLD; it's emulated with 7
19011            insns, including two PMULUDQ.  */
19012         else if (mode == V4SImode && !(TARGET_SSE4_1 || TARGET_AVX))
19013           return ix86_vec_cost (mode, cost->mulss * 2 + cost->sse_op * 5);

but for a testcase doing just x * 2 that is excessive.  The vectorizer
would change that to x << 1 via vect_recog_mult_pattern (yeah, oddly
not to x + x ...).

This causes SSE vectorization to be disregarded easily, falling back to
MMX "emulation" mode which doesn't claim V4SImode multiplication support
producing essentially SSE code but with only half of the lanes doing useful
work.

I'm not sure if pattern recog should try costing here.  Certainly the
vectorizer won't try the PMULUDQ variant if the backend would claim to
not support V4SImode mult.

Noticed for the testcase in PR92173.


---


### compiler : `gcc`
### title : `Missed optimization on casting __builtin_ia32_rdtsc result to int32`
### open_at : `2019-10-22T15:55:51Z`
### last_modified_date : `2023-09-28T10:39:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92180
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
Link: https://godbolt.org/z/wcIN0a

#include <stdint.h>

uint32_t foo1() {
    return __builtin_ia32_rdtsc();
}

uint64_t foo2() {
    return __builtin_ia32_rdtsc();
}

Generates assembly:
foo1():
        rdtsc
        sal     rdx, 32
        or      rax, rdx
        ret
foo2():
        rdtsc
        sal     rdx, 32
        or      rax, rdx
        ret

While clang generates better code for foo1

foo1():                               # @foo1()
        rdtsc
        ret


---


### compiler : `gcc`
### title : `gcc tries to create a relocation in a mergeable section`
### open_at : `2019-10-23T02:27:25Z`
### last_modified_date : `2021-12-22T09:49:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92183
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Given

struct foo {
  const char *bar;
  const char *zed;
};
void g(struct foo *r);
void f() {
  struct foo t = {"bar", "zed"};
  g(&t);
}


gcc -O3 produces

        .section        .rodata.str1.1,"aMS",@progbits,1
.LC0:
        .string "bar"
.LC1:
        .string "zed"

....

        .section        .rodata.cst8,"aM",@progbits,8
        .align 8
.LC2:
        .quad   .LC1


While it is not illegal to have a relocation in a mergeable section, I don't know of any linker that supports that, so gcc is being over aggressive.


---


### compiler : `gcc`
### title : `Cannot merge memory write for _mm_cvtps_ph/_mm256_cvtps_ph and x86-64`
### open_at : `2019-10-23T08:15:11Z`
### last_modified_date : `2020-01-29T14:10:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92188
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
Created attachment 47089
Test code

For this code, the memory write cannot be merged with vcvtps2ph.

void test1(__m128i *x, const __m256 *y)
{
    // Cannot merge memory write
    *x = _mm256_cvtps_ph(*y, _MM_FROUND_CUR_DIRECTION);
}

  ...
  vcvtps2ph $4, %ymm0, %xmm0
  vmovaps %xmm0, (%rdi)
  ...

A workaround is to change the output type to __v8hi as.

void test2(__v8hi *x, const __m256 *y)
{
    // Memory write merged
    *x = (__v8hi)_mm256_cvtps_ph(*y, _MM_FROUND_CUR_DIRECTION);
}

  ...
  vcvtps2ph $4, %ymm0, (%rdi)
  ...

However it does not work for the 128 bit variant of vcvtps2ph.

void test4(__v4hi *x, const __m128 *y)
{
    // Cannot merge memory write
    *x = (__v4hi)(((__v2di)_mm_cvtps_ph(*y, _MM_FROUND_CUR_DIRECTION))[0]);
}

  ...
  vcvtps2ph $4, %xmm0, %xmm0
  vmovq %xmm0, (%rdi)
  ...

The opposite problem exists for e.g. _mm256_extracti128_si256, which normally merges the memory write but not for output type __v8hi.

void test6(__v8hi *x, const __m256i *y)
{
    // Cannot merge memory write
    *x = (__v8hi)_mm256_extracti128_si256(*y, 1);
}

  ...
  vextracti128 $0x1, %ymm0, %xmm0
  vmovaps %xmm0, (%rdi)
  ...

It would be good if all variants behave the same, with memory write merged.

I use "-O3 -march=core-avx2" when compiling (using compiler explorer).


---


### compiler : `gcc`
### title : `missed optimisation for multiplication when it's known that at least one of the arguments is 0`
### open_at : `2019-10-26T05:41:08Z`
### last_modified_date : `2023-08-22T05:05:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92233
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `enhancement`
### contents :
testcase:

unsigned test_mult(unsigned a, unsigned b)
{
  if ((a == 0) || (b == 0))
  {
    return a*b; // here a*0 or 0*b or 0*0 - always 0
  }
  return 0;
}

So this function should always return 0 no matter what, but GCC generate comparisons and imul instruction, even with -O3


---


### compiler : `gcc`
### title : `[x86] Missed optimisation opportunity with bit tests`
### open_at : `2019-10-26T21:13:35Z`
### last_modified_date : `2021-11-28T06:54:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92237
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
See https://godbolt.org/z/mP-8Y7

An expression such as:

bool foo(uint64_t val)
{
    return (val & 0x120) == 0x20;
}

gets assembled to:
<foo>:
   0:	81 e7 20 01 00 00    	and    $0x120,%edi
   6:	48 83 ff 20          	cmp    $0x20,%rdi
   a:	0f 94 c0             	sete   %al
   d:	c3                   	retq


Some part of optimisation has noticed that, due to the 32bit constant, the AND can be performed on %edi, but hasn't spotted that the same is true for the following CMP.

In this example, the CMP could use %edi as well, and save emitting the REX prefix into the instruction stream.


---


### compiler : `gcc`
### title : `Missing "auto-vectorization" of char array reversal using x86 scalar bswap when SIMD pshufb isn't available`
### open_at : `2019-10-27T22:22:17Z`
### last_modified_date : `2021-12-29T05:57:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92243
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
We could use integer bswap to speed up an in-place byte-reverse loop by a factor of probably 8, the same way we uses SIMD shuffles.

Consider this loop which reverses an explicit-length char array: https://godbolt.org/z/ujXq_J

typedef char swapt; // int can auto-vectorize with just SSE2
void strrev_explicit(swapt *head, long len)
{
  swapt *tail = head + len - 1;
  for( ; head < tail; ++head, --tail) {
      swapt h = *head, t = *tail;
      *head = t;
      *tail = h;
  }
}

gcc -O3 (including current trunk) targeting x86-64 makes naive scalar byte-at-a-time code, even though bswap r64 is available to byte-reverse a uint64 in 1 or 2 uops (AMD and Intel, respectively).

With -mssse3, we do see auto-vectorization using SIMD pshufb (after checking lengths and calculating how many 16-byte chunks can be done before bloated fully-unrolled cleanup).  Doing the same thing with 64-bit integer registers would be very much worth it (for code where a loop like this was a bottleneck).

----

With `swapt = short`, vectorizing with SSE2 pshuflw / pshufhw / pshufd is probably worth it, but GCC chooses not to do that either.  Or working in 8-byte chunks just using movq + pshuflw, so we only have 1 shuffle per 8-byte load/store instead of 3 per 16-byte store.  That's a good balance for modern Intel (Haswell, Skylake, and I think IceLake), although some AMD and earlier Intel with more integer shuffle throughput (e.g. Sandybridge) might do better with 3x shuffles per 16-byte load/store.


---


### compiler : `gcc`
### title : `vectorized loop updating 2 copies of the same pointer (for in-place reversal cross in the middle)`
### open_at : `2019-10-27T23:37:15Z`
### last_modified_date : `2021-05-04T12:31:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92244
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
We get a redundant instruction inside the vectorized loop here.  But it's not a separate *counter*, it's a duplicate of the tail pointer.

It goes away if we find tail with while(*tail++); instead of calculating it from head+length.

Only happens with vectorization, not pure scalar (bug 92243 is about the fact that -O3 fails to use bswap as a GP-integer shuffle to auto-vectorize without x86 SSSE3).

typedef char swapt;
void strrev_explicit(swapt *head, long len)
{
  swapt *tail = head + len - 1;
  for( ; head < tail; ++head, --tail) {
      swapt h = *head, t = *tail;
      *head = t;
      *tail = h;
  }
}
https://godbolt.org/z/wdGv4S

compiled with g++ -O3 -march=sandybridge gives us a main loop of

        ...
        movq    %rcx, %rsi         # RSI = RCX before entering the loop
        addq    %rdi, %r8
.L4:
        vmovdqu (%rcx), %xmm3       # tail load from RCX
        addq    $16, %rax        # head
        subq    $16, %rcx        # tail
        subq    $16, %rsi        # 2nd tail?
        vmovdqu -16(%rax), %xmm0
        vpshufb %xmm2, %xmm3, %xmm1
        vmovups %xmm1, -16(%rax)
        vpshufb %xmm2, %xmm0, %xmm0
        vmovups %xmm0, 16(%rsi)     # tail store to RSI
        cmpq    %r8, %rax           # } while(head != end_head)
        jne     .L4

RSI = RCX before and after the loop.  This is obviously pointless.
head uses the same register for loads and stores.

 Then we have bloated fully-unrolled scalar cleanup, instead of using the shuffle control for 8-byte vectors -> movhps.  Or scalar bswap.  Ideally we'd do something clever at the overlap like one load + shuffle + store, but we might have to load the next vector before storing the current to make this work at the overlap.  That would presumably require more special-casing this kind of meet-in-the-middle loop.


----

The implicit-length version doesn't have this extra sub in the main loop.

void strrev_implicit(swapt *head)
{
  swapt *tail = head;
  while(*tail) ++tail;    // find the 0 terminator, like head+strlen
  --tail;                 // tail points to the last real char
  for( ; head < tail; ++head, --tail) {
      swapt h = *head, t = *tail;
      *head = t;
      *tail = h;
  }
}

.L22:
        vmovdqu (%rcx), %xmm3
        addq    $16, %rdx           # head
        subq    $16, %rcx           # tail
        vmovdqu -16(%rdx), %xmm0
        vpshufb %xmm2, %xmm3, %xmm1
        vmovups %xmm1, -16(%rdx)
        vpshufb %xmm2, %xmm0, %xmm0
        vmovups %xmm0, 16(%rcx)
        cmpq    %rsi, %rdx          # } while(head != end_head)
        jne     .L22


---


### compiler : `gcc`
### title : `strncpy followed by nul store not folded into memcpy`
### open_at : `2019-10-27T23:54:21Z`
### last_modified_date : `2020-01-29T14:47:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92245
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Because the strlen pass has no support for strncpy beyond warnings, GCC emits suboptimal code for all functions below except f0.  In f1(), the strncpy call and nul store can be merged into a single memcpy.  The same can happen in f2(), f3(), and f4().  In addition, the strlen calls can then be folded into constants.  Since strncpy followed by a nul store to terminate the string is a common idiom this could would not only lead to better code but also help expose buffer overflows when inappropriately using the result.

With the strlen calls removed, Clang emits the same optimal code for f0() and f1(), but does just as poorly on the rest of the functions.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
extern char a[8];

void f0 (void)   // optimal
{
  __builtin_memcpy (a, "123", 4);
  if (__builtin_strlen (a) != 3)
    __builtin_abort ();
}

void f1 (void)
{
  __builtin_strncpy (a, "123", 3);
  a[3] = 0;                          // can be merged with the above

  if (__builtin_strlen (a) != 3)
    __builtin_abort ();
}

void f2 (void)
{
  const char s[] = "123";

  __builtin_strncpy (a, s, 3);
  a[3] = 0;                          // can be merged with the above

  if (__builtin_strlen (a) != 3)     // can be folded to false
    __builtin_abort ();
}

void f3 (const char *s)
{
  if (__builtin_strlen (s) != 3)
    return;

  __builtin_strncpy (a, s, 3);
  a[3] = 0;                          // can be merged with the above

  if (__builtin_strlen (a) != 3)     // can be folded to false
    __builtin_abort ();
}

void f4 (const char *s)
{
  if (__builtin_strlen (s) < 3)
    return;

  __builtin_strncpy (a, s, 3);
  a[3] = 0;                          // can be merged with the above

  if (__builtin_strlen (a) != 3)     // can be folded to false
    __builtin_abort ();
}


;; Function f0 (f0, funcdef_no=0, decl_uid=1931, cgraph_uid=1, symbol_order=0)

f0 ()
{
  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "123", 4); [tail call]
  return;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=1934, cgraph_uid=2, symbol_order=1)

f1 ()
{
  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "123", 3);
  a[3] = 0;
  return;

}



;; Function f2 (f2, funcdef_no=2, decl_uid=1937, cgraph_uid=3, symbol_order=2)

f2 ()
{
  const char s[4];
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  s = "123";
  __builtin_strncpy (&a, &s, 3);
  a[3] = 0;
  _1 = __builtin_strlen (&a);
  if (_1 != 3)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  s ={v} {CLOBBER};
  return;

}



;; Function f3 (f3, funcdef_no=3, decl_uid=1941, cgraph_uid=4, symbol_order=3)

Removing basic block 6
Removing basic block 7
f3 (const char * s)
{
  long unsigned int _1;
  long unsigned int _2;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strlen (s_5(D));
  if (_1 != 3)
    goto <bb 5>; [67.00%]
  else
    goto <bb 3>; [33.00%]

  <bb 3> [local count: 354334802]:
  __builtin_strncpy (&a, s_5(D), 3);
  a[3] = 0;
  _2 = __builtin_strlen (&a);
  if (_2 != 3)
    goto <bb 4>; [0.00%]
  else
    goto <bb 5>; [100.00%]

  <bb 4> [count: 0]:
  __builtin_abort ();

  <bb 5> [local count: 1073741826]:
  return;

}



;; Function f4 (f4, funcdef_no=4, decl_uid=1944, cgraph_uid=5, symbol_order=4)

Removing basic block 6
Removing basic block 7
f4 (const char * s)
{
  long unsigned int _1;
  long unsigned int _2;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strlen (s_5(D));
  if (_1 <= 2)
    goto <bb 5>; [51.12%]
  else
    goto <bb 3>; [48.88%]

  <bb 3> [local count: 524845004]:
  __builtin_strncpy (&a, s_5(D), 3);
  a[3] = 0;
  _2 = __builtin_strlen (&a);
  if (_2 != 3)
    goto <bb 4>; [0.00%]
  else
    goto <bb 5>; [100.00%]

  <bb 4> [count: 0]:
  __builtin_abort ();

  <bb 5> [local count: 1073741828]:
  return;

}


---


### compiler : `gcc`
### title : `Byte or short array reverse loop auto-vectorized with 3-uop vpermt2w instead of 1 or 2-uop vpermw (AVX512)`
### open_at : `2019-10-28T00:10:29Z`
### last_modified_date : `2020-01-29T14:47:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92246
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
typedef short swapt;
void strrev_explicit(swapt *head, long len)
{
  swapt *tail = head + len - 1;
  for( ; head < tail; ++head, --tail) {
      swapt h = *head, t = *tail;
      *head = t;
      *tail = h;
  }
}

g++ -O3 -march=skylake-avx512
  (Compiler-Explorer-Build) 10.0.0 20191022 (experimental)

https://godbolt.org/z/LS34w9

        ...
.L4:
        vmovdqu16       (%rdx), %ymm1
        vmovdqu16       (%rax), %ymm0
        vmovdqa64       %ymm1, %ymm3        # useless copy
        vpermt2w        %ymm1, %ymm2, %ymm3
        vmovdqu16       %ymm3, (%rax)
        vpermt2w        %ymm0, %ymm2, %ymm0
        addq    $32, %rax
        vmovdqu16       %ymm0, (%rcx)
        subq    $32, %rdx
        subq    $32, %rcx       # two tail pointers, PR 92244 is unrelated to this
        cmpq    %rsi, %rax
        jne     .L4

vpermt2w ymm is 3 uops on SKX and CannonLake:  2p5 + p015 (https://www.uops.info/table.html)

Obviously better would be  vpermw (%rax), %ymm2, %ymm0.

vpermw apparently can't micro-micro-fuse a load, but it's only 2 ALU uops plus a load if we use a memory source.  SKX still bottlenecks on 2p5 for vpermw, losing only the p015 uop, but in general fewer uops is better.

But on CannonLake it runs on p01 + p5 (plus p23 with a memory source).

uops.info doesn't have IceLake-client data yet but vpermw throughput on IceLake is 1/clock, vs 1 / 2 clocks for vpermt2w, so this could double throughput on CNL and ICL.

We have exactly the same problem with AVX512VBMI vpermt2b over vpermb with ICL
g++ -O3 -march=icelake-client -mprefer-vector-width=512


---


### compiler : `gcc`
### title : `Inconsistent canonicalization of (minus (minus A B) C)`
### open_at : `2019-10-30T11:35:19Z`
### last_modified_date : `2020-01-29T15:07:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92281
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Here are two combine attempts from a simple testcase:

arm-none-eabi-gcc -O2 -marm -mcpu=arm7tdmi

typedef unsigned long long t64;

t64 f1(t64 a, t64 b) { return a + ~b; }

Trying 19 -> 8:
   19: r119:SI=r127:SI
      REG_DEAD r127:SI
    8: r125:SI=r119:SI-ltu(cc:CC,0)-r121:SI
      REG_DEAD r121:SI
      REG_DEAD r119:SI
      REG_DEAD cc:CC
Failed to match this instruction:
(set (reg:SI 125 [+4 ])
    (minus:SI (minus:SI (reg:SI 127)
            (reg:SI 121 [ b+4 ]))
        (ltu:SI (reg:CC 100 cc)
            (const_int 0 [0]))))

Trying 21 -> 8:
   21: r121:SI=r129:SI
      REG_DEAD r129:SI
    8: r125:SI=r119:SI-ltu(cc:CC,0)-r121:SI
      REG_DEAD r121:SI
      REG_DEAD r119:SI
      REG_DEAD cc:CC
Successfully matched this instruction:
(set (reg:SI 125 [+4 ])
    (minus:SI (minus:SI (reg:SI 119 [ a+4 ])
            (ltu:SI (reg:CC 100 cc)
                (const_int 0 [0])))
        (reg:SI 129)))

These are mathematically equivalent, but because we do not produce consistent RTL for them we need two patterns if we are to match both alternatives.

I think both should be canonicalized with the LTU inside the inner MINUS expression, but I wouldn't mind if the other were chosen, as long as we were consistent.


---


### compiler : `gcc`
### title : `gimple for (a + ~b) is harder to optimize in RTL when types are unsigned`
### open_at : `2019-10-30T11:50:05Z`
### last_modified_date : `2023-08-05T17:15:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92282
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Given:

t f1(t a, t b) { return a + ~b; }

if t is of type int64_t, then the gimple produced is


  _1 = ~b_2(D);
  _4 = _1 + a_3(D);
 
Which on Arm can then easily optimize into a 3 instruction sequence

MVN  R2, R2
ADDS R0, R0, R2
SBC  R1, R1, R3

(because on Arm, SBC = Rn - Rm - ~C == Rn + ~Rm + C)

But if the type is changed to uint64_t, then the gimple is transformed into

  _1 = a_2(D) - b_3(D);
  _4 = _1 + 18446744073709551615;

Which is almost impossible for the back-end to optimize back into the optimal sequence.  The result is that we end up with two carry-propagating subtract operations instead of one and less parallelism in the overall sequence as the bit-wise invert can operate in parallel on any super-scalar architecture.

Note that the same problem likely exists on 64-bit architectures if t is uint128_t.


---


### compiler : `gcc`
### title : `Non-optimal code generated for H8`
### open_at : `2019-10-30T16:01:28Z`
### last_modified_date : `2020-02-03T21:50:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92291
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
I am using a cross compiler for Renesas H8S. In a few places it generates really bad code. Given the following program:


struct s {
    char a, b;
    char c[11];
} x[2];

void test(int n)
{
    struct s *sp = &x[n];

    sp->a = 1;
    sp->b = 1;
}

I would expect that the pointer "sp" is calculated once and reused to access the fields "a" and "b". But instead the pointer is recalculated for each access. This generates a lot of extra code, including calls to __mulhi3. I have tested with gcc 8.2 and 9.2 and with different optimization levels (-O1, -O2, -Os) all with the same result. With -O0 "sp" is only calculated once and kept as a variable on the stack but the rest of the code is not as good as it could be.
---
Using built-in specs.
COLLECT_GCC=h8300-none-elf-gcc
Target: h8300-none-elf
Configured with: /home/mti/abs/arm-none-eabi-gcc/h8/src/gcc-9.2.0/configure --target=h8300-none-elf --prefix=/usr --with-native-system-header-dir=/include --libexecdir=/usr/lib --enable-languages=c,c++ --enable-plugins --disable-decimal-float --disable-libffi --disable-libgomp --disable-libmudflap --disable-libquadmath --disable-libssp --disable-libstdcxx-pch --disable-libstdcxx --disable-nls --disable-shared --disable-threads --disable-tls --with-gnu-as --with-gnu-ld --with-system-zlib --without-headers --with-python-dir=share/gcc-arm-none-eabi --with-gmp --with-mpfr --with-mpc --with-isl --with-libelf --enable-gnu-indirect-function --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' --with-pkgversion='Arch Repository' --with-bugurl=https://bugs.archlinux.org/ --with-multilib-list=rmprofile
Thread model: single
gcc version 9.2.0 (Arch Repository) 
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O1' '-S' '-Wall'
 /usr/lib/gcc/h8300-none-elf/9.2.0/cc1 -E -quiet -v test.c -Wall -O1 -fpch-preprocess -o test.i
ignoring nonexistent directory "/usr/lib/gcc/h8300-none-elf/9.2.0/../../../../h8300-none-elf/sys-include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/lib/gcc/h8300-none-elf/9.2.0/include
 /usr/lib/gcc/h8300-none-elf/9.2.0/include-fixed
 /usr/lib/gcc/h8300-none-elf/9.2.0/../../../../h8300-none-elf/include
End of search list.
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O1' '-S' '-Wall'
 /usr/lib/gcc/h8300-none-elf/9.2.0/cc1 -fpreprocessed test.i -quiet -dumpbase test.c -auxbase test -O1 -Wall -version -o test.s
GNU C17 (Arch Repository) version 9.2.0 (h8300-none-elf)
	compiled by GNU C version 9.2.0, GMP version 6.1.2, MPFR version 4.0.2, MPC version 1.1.0, isl version isl-0.19-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
GNU C17 (Arch Repository) version 9.2.0 (h8300-none-elf)
	compiled by GNU C version 9.2.0, GMP version 6.1.2, MPFR version 4.0.2, MPC version 1.1.0, isl version isl-0.19-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 67bb4ca8e2b97056926c3ecedb8a3eae
COMPILER_PATH=/usr/lib/gcc/h8300-none-elf/9.2.0/:/usr/lib/gcc/h8300-none-elf/9.2.0/:/usr/lib/gcc/h8300-none-elf/:/usr/lib/gcc/h8300-none-elf/9.2.0/:/usr/lib/gcc/h8300-none-elf/:/usr/lib/gcc/h8300-none-elf/9.2.0/../../../../h8300-none-elf/bin/
LIBRARY_PATH=/usr/lib/gcc/h8300-none-elf/9.2.0/:/usr/lib/gcc/h8300-none-elf/9.2.0/../../../../h8300-none-elf/lib/
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O1' '-S' '-Wall'


---


### compiler : `gcc`
### title : `Inefficient vector constructor`
### open_at : `2019-10-31T00:02:09Z`
### last_modified_date : `2020-02-04T08:46:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92295
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
[hjl@gnu-skx-1 microbenchmark]$ cat dup.c
typedef int X __attribute__((vector_size (32)));

X
foo (int x, int z)
{
  X y = { x, x, x, x, z, z, z, z };
  return y;
}

[hjl@gnu-skx-1 microbenchmark]$ gcc -S -O2 -march=skylake-avx512 dup.c
[hjl@gnu-skx-1 microbenchmark]$ cat dup.s
	.file	"dup.c"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	vmovd	%esi, %xmm2
	vmovd	%edi, %xmm3
	vpinsrd	$1, %esi, %xmm2, %xmm1
	vpinsrd	$1, %edi, %xmm3, %xmm0
	vpunpcklqdq	%xmm1, %xmm1, %xmm1
	vpunpcklqdq	%xmm0, %xmm0, %xmm0
	vinserti128	$0x1, %xmm1, %ymm0, %ymm0
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 9.2.1 20190827 (Red Hat 9.2.1-1)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-skx-1 microbenchmark]$ 

We can generate:

	vpbroadcastd	%edi, %xmm0
	vpbroadcastd	%esi, %xmm1
	vinserti128	$1, %xmm1, %ymm0, %ymm0
	retq


---


### compiler : `gcc`
### title : `Gimple passes could do a better job of forming address CSEs`
### open_at : `2019-10-31T16:44:16Z`
### last_modified_date : `2020-01-29T15:14:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92308
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Consider this testcase which was mentioned in https://gcc.gnu.org/ml/gcc-help/2019-10/msg00122.html.  

#define BB_ADDRESS 0x43fe1800

void test1(void) {
  volatile uint32_t * const p = (uint32_t *) BB_ADDRESS;

  p[3] = 1;
  p[4] = 2;
  p[1] = 3;
  p[7] = 4;
  p[0] = 6;
}

The gimple generated for this is

test1 ()
{
;;   basic block 2, loop depth 0
;;    pred:       ENTRY
  MEM[(volatile uint32_t *)1140725772B] ={v} 1;
  MEM[(volatile uint32_t *)1140725776B] ={v} 2;
  MEM[(volatile uint32_t *)1140725764B] ={v} 3;
  MEM[(volatile uint32_t *)1140725788B] ={v} 4;
  MEM[(volatile uint32_t *)1140725760B] ={v} 6;
  return;
;;    succ:       EXIT

}

However, it's very unlikely on any RISC type architecture that addresses of this form will be valid.  The TARGET_LEGITIMIZE_ADDRESS hook can help here, but that has to guess how to split the address and it has no idea what, for each call, the best base that should be chosen.  In this case the best base is likely to be the lowest addressed object in the sequence, so that all other objects can use a small positive offset from that.

The GIMPLE passes have a much broader view on the code being optimized, so forming a common base for all these addresses should be straight forward and much more likely to lead to better code than having to use a heuristic in the back-end.


---


### compiler : `gcc`
### title : `[11/12/13 Regression] sinking of loads happen too early which causes vectorization not to be done`
### open_at : `2019-11-03T10:19:19Z`
### last_modified_date : `2023-08-04T17:26:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92335
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
in the following code (compiled with -O2 or -O3 and even with -march=haswell)
gcc will use a branchless construct in foo but not in bar (changing from float to int does not modify the behavior)
(see https://godbolt.org/z/0ZWKb5 )

with -Ofast they will compile in the same vectorized branchless loop, still I do not see why the branch shall be retained at -O2 in bar

for random "x" the branchless version is 6 times faster on any out-of-order cpu

float foo(float const * __restrict__ x, 
float const * __restrict__ y) {
  float ret=0.f;
  for (int i=0;i<1024;++i) {
    auto k = y[i];
    ret += x[i]>0.f ? k : 0.f;
  }
    return ret;
}



float bar(float const * __restrict__ x, 
float const * __restrict__ y) {
  float ret=0.f;
  for (int i=0;i<1024;++i) {
    auto k = y[i];
    if(x[i]>0.f) ret += k;
  }
    return ret;
}


---


### compiler : `gcc`
### title : `[10/11/12 Regression] a small missed transformation into x?b:0`
### open_at : `2019-11-04T01:22:27Z`
### last_modified_date : `2023-03-10T19:25:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92342
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Take these two functions:
int f(int a,int b, int c)
{
  return a==c?b:0;
}

int g(int a, int b, int c)
{
  return b & -(a==c);
}
---- CUT ----
We used to produce the same code generation for both of them:
        cmp     w0, w2
        csel    w0, w1, wzr, eq
        ret


But in GCC 10 we produce for g:
        cmp     w0, w2
        csetm   w0, eq
        and     w0, w0, w1
        ret

I think this was introduced by:
2019-05-09  Segher Boessenkool  <segher@kernel.crashing.org>

       * combine.c (combine_simplify_rtx): Don't make IF_THEN_ELSE RTL.


---


### compiler : `gcc`
### title : `Missing considering fre optimization of vector load in auto-vectorization`
### open_at : `2019-11-04T06:55:14Z`
### last_modified_date : `2019-11-04T12:06:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92344
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
For testcase loop.c
-------------------
cat loop.c:

int loop
(unsigned char * input1, unsigned char * input2, int stride1, int stride2)
{
    unsigned int tmp[4][4];
    unsigned int var0, var1, var2, var3;
    int sum = 0;
    for (int i = 0; i < 4; i++, input1 += stride1, input2 += stride2) {
	var0 = (input1[0] + input2[0]) + (input1[4] + input2[4]);
	var1 = (input1[1] + input2[1]) + (input1[5] + input2[5]);
	var2 = (input1[2] + input2[2]) + (input1[6] + input2[6]);
	var3 = (input1[3] + input2[3]) + (input1[7] + input2[7]);
	int inter0 = var0 + var1;
	int inter1 = var0 + var1;
	int inter2 = var2 + var3;
	int inter3 = var2 + var3;
	tmp[i][0] = inter0 + inter2;
	tmp[i][2] = inter0 + inter2;
	tmp[i][1] = inter1 + inter3;
	tmp[i][3] = inter1 + inter3;
    }
    for (int i = 0; i < 4; i++) {
	int inter0 = tmp[0][i] + tmp[1][i];
	int inter1 = tmp[0][i] + tmp[1][i];
	int inter2 = tmp[2][i] + tmp[3][i];
	int inter3 = tmp[2][i] + tmp[3][i];
	var0 = inter0 + inter2;
	var2 = inter0 + inter2;
	var1 = inter1 + inter3;
	var3 = inter1 + inter3;
	sum += var0 + var1 + var2 + var3;
    }

    return sum;
}
---------------

Command line:
--------------------------
/usr/gcc10_20191101/bin/gcc -Ofast -march=skylake loop.c -S
--------------------------

before slp1, we have:
----------
bb2:
  ...
  tmp[0][0] = _95;
  tmp[0][2] = _95;
  tmp[0][1] = _95;
  tmp[0][3] = _95;
  ...
  i_168 = 1;
  tmp[i_168][0] = _168;
  tmp[i_168][2] = _168;
  tmp[i_168][1] = _168;
  tmp[i_168][3] = _168;
  ...
  i_238 = i_168 + 1;
  tmp[i_238][0] = _238;
  tmp[i_238][2] = _238;
  tmp[i_238][1] = _238;
  tmp[i_238][3] = _238;
  ...
  i_309 = i_238 + 1;
  tmp[i_309][0] = _48;
  tmp[i_309][2] = _48;
  tmp[i_309][1] = _48;
  tmp[i_309][3] = _48;
  ...

  vectp_tmp.9_284 = &tmp + 16; ------ &tmp[1][0]
  vectp_tmp.14_276 = &tmp + 32; ----- &tmp[2][0]
  vectp_tmp.17_272 = &tmp + 48; ----- &tmp[1][0]
  vect__51.7_285 = MEM <vector(4) unsigned int> [(unsigned int *)&tmp];
  vect__52.10_281 = MEM <vector(4) unsigned int> [(unsigned int *)vectp_tmp.9_284];
  vect__55.15_273 = MEM <vector(4) unsigned int> [(unsigned int *)vectp_tmp.14_276];
  vect__56.18_269 = MEM <vector(4) unsigned int> [(unsigned int *)vectp_tmp.17_272];
  ..........
-------------

in slp1 we have 256bit vector generated since 256bit vector_cost less than 128bits vector cost:

256bits vectorization
--------
   _540 = {_238, _238, _238, _238, _48, _48, _48, _48};
  vect_cst__541 = _540;
  _542 = {_95, _95, _95, _95, _168, _168, _168, _168};
  vect_cst__543 = _542;
  MEM <vector(8) unsigned int> [(unsigned int *)&tmp] = vect_cst__543;
  _545 = &tmp[0][0] + 32;
  MEM <vector(8) unsigned int> [(unsigned int *)_545] = vect_cst__541;
-----------

256bits vectorization cost
------------------------------------------
2 times 256bits vector_store costs 48 in body
(2 256bits vector store costs)
---------------------------------------

128bits vectorization
------------------
  _540 = {_95, _95, _95, _95};
  vect_cst__541 = _540;
 _543 = {_48, _48, _48, _48};
  vect_cst__544 = _543;
  _545 = {_238, _238, _238, _238};
  vect_cst__546 = _545;
  _547 = {_168, _168, _168, _168};
  vect_cst__548 = _547;
  MEM <vector(4) unsigned int> [(unsigned int *)&tmp] = vect_cst__541;
  vectp.32_549 = &tmp[i_168][0];
  MEM <vector(4) unsigned int> [(unsigned int *)vectp.32_549] = vect_cst__548;
  vectp.32_551 = vectp.32_549 + 16;
  MEM <vector(4) unsigned int> [(unsigned int *)vectp.32_551] = vect_cst__546;
  vectp.32_553 = vectp.32_551 + 16;
  MEM <vector(4) unsigned int> [(unsigned int *)vectp.32_553] = vect_cst__544;
------------------

128bits vectorization cost:

----------------------------
4 times 128bit vector_store costs 64 in body
(4 128bit vector store costs)
---------------------------

But since there's 128bit loads after these stores, using 128bit stores can enable full redudant elimation of the folowing 128bits vector loads, it should be more accurate to have vectorization cost like:

128bit vectorization cost:
-------------------------------
4 times 128bit vector_store costs 64 in body **minus 4 times 128bit vector_load cost 48 in body**

totally cost 64 - 48 = 16 inside loop body.
-------------------------------

256 bit vectorization cost:
-----------------
2 times 256bit vector_store costs 64 in body 

totally cost 48 inside loop body.
----------------

Then 128bit vectorization will be generated.


---


### compiler : `gcc`
### title : `Missed optimization of std::find looking for item in array of items [0..n]`
### open_at : `2019-11-04T14:58:53Z`
### last_modified_date : `2020-10-06T16:42:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92356
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.2.0`
### severity : `normal`
### contents :
Performing a std::find on an array with elements [0, 1, 2,..., n] does not produce similar optimizations as implementing the find using a raw for loop.  This is true when using clang 9.0 or gcc 9.2 with flags `-O2 -std=c++17`.  Using libc++ with clang produces the same result with std::find as the raw loop version.

Here is a link to compiler explore demonstrating the difference in generated assembly.
https://godbolt.org/z/pM1FQQ

One item I just discovered that I did not expect is that a range-for loop in gcc also has inefficiencies.  That might be a separate issue, and I can file one if needed.

I understand this looks like a contrived example, but I feel like this scenario does come up sometimes with enums.  People will have a large enum list where they want to check if a value is in a small subset of the enum and that subset just happens to be the first few items in the enum.


---


### compiler : `gcc`
### title : `strlen(s) != 0 not folded into *s`
### open_at : `2019-11-07T16:41:48Z`
### last_modified_date : `2023-05-02T05:10:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92408
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
In simple expressions, GCC folds strlen(*s) calls whose result is only used in a test for equality with zero to tests for the first character being nul.  But it does only a superficial job and doesn't also do the same folding when the result is stored in a variable that is then tested for the same equality.

Clang folds both.

$ cat z.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout z.c
void f (void);

void g (const char *s)
{
  if (__builtin_strlen (s))   // folded to if (*s)
    f ();
}

void h (const char *s)
{
  __SIZE_TYPE__ n = __builtin_strlen (s);
  if (n)                      // not folded into if (*s) but could be
    f ();
}

;; Function g (g, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=0)

Removing basic block 5
g (const char * s)
{
  char _1;

  <bb 2> [local count: 1073741824]:
  _1 = *s_4(D);
  if (_1 != 0)
    goto <bb 3>; [33.00%]
  else
    goto <bb 4>; [67.00%]

  <bb 3> [local count: 354334802]:
  f (); [tail call]

  <bb 4> [local count: 1073741824]:
  return;

}



;; Function h (h, funcdef_no=1, decl_uid=1935, cgraph_uid=2, symbol_order=1)

Removing basic block 5
h (const char * s)
{
  long unsigned int n;

  <bb 2> [local count: 1073741824]:
  n_4 = __builtin_strlen (s_3(D));
  if (n_4 != 0)
    goto <bb 3>; [33.00%]
  else
    goto <bb 4>; [67.00%]

  <bb 3> [local count: 354334802]:
  f (); [tail call]

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `excessive errno aliasing assumption defeats optimization`
### open_at : `2019-11-07T18:13:37Z`
### last_modified_date : `2019-11-12T18:52:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92412
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
GCC folds to false the test for a's contents in f() below but it doesn't do the same in g().  It appears to be caused by the call to strlen which, when being optimized by the strlen pass, triggers a call to GCC's default_ref_may_alias_errno().  Although the default_ref_may_alias_errno() function called to determine whether errno can alias a variable returns false for static variables defined in the same translation unit it returns true for local variables like a.

Other compilers (Clang and Visual C/C++) fold the tests in both functions to false.

$ cat b.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout b.c
void* f (void)
{ 
  const char a[] = "123";

  void *p = __builtin_malloc (sizeof a);

  if (a[0] != '1' || a[1] != '2' || a[2] != '3' || a[3] != '\0')   // folded to false
    __builtin_abort ();

  return p;
}

void* g (void)
{
  const char a[] = "123";

  void *p = __builtin_malloc (__builtin_strlen (a) + 1);

  if (a[0] != '1' || a[1] != '2' || a[2] != '3' || a[3] != '\0')   // not folded
    __builtin_abort ();

  return p;
}


;; Function f (f, funcdef_no=0, decl_uid=1930, cgraph_uid=1, symbol_order=0)

f ()
{
  void * p;

  <bb 2> [local count: 1073741824]:
  p_3 = __builtin_malloc (4); [tail call]
  return p_3;

}



;; Function g (g, funcdef_no=1, decl_uid=1935, cgraph_uid=2, symbol_order=1)

Removing basic block 8
Removing basic block 9
Removing basic block 10
Removing basic block 11
g ()
{
  void * p;
  const char a[4];
  char _3;
  char _4;
  char _5;
  char _6;

  <bb 2> [local count: 1073741824]:
  a = "123";
  p_10 = __builtin_malloc (4);
  _3 = a[0];
  if (_3 != 49)
    goto <bb 6>; [0.00%]
  else
    goto <bb 3>; [100.00%]

  <bb 3> [local count: 1073741824]:
  _4 = a[1];
  if (_4 != 50)
    goto <bb 6>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 4> [local count: 1073741824]:
  _5 = a[2];
  if (_5 != 51)
    goto <bb 6>; [0.00%]
  else
    goto <bb 5>; [100.00%]

  <bb 5> [local count: 1073741824]:
  _6 = a[3];
  if (_6 != 0)
    goto <bb 6>; [0.00%]
  else
    goto <bb 7>; [100.00%]

  <bb 6> [count: 0]:
  __builtin_abort ();

  <bb 7> [local count: 1073741824]:
  a ={v} {CLOBBER};
  return p_10;

}


---


### compiler : `gcc`
### title : `SIMD integer subtract with constant always becomes add`
### open_at : `2019-11-10T04:54:43Z`
### last_modified_date : `2021-08-21T23:14:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92436
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Firstly, this isn't a bug, rather a missed optimization opportunity (I presume this is the place to post these?).


With the optimizer enabled, it seems like SIMD integer subtract, with a constant, always gets turned into a SIMD integer add with the constant negated.

For a target like x86 SSE, I suppose this may make sense, as the commutative property of addition gives more flexibility around register placement, but it isn't always beneficial - for example, if the constant could be re-used elsewhere.

Example (x86):

	_mm_or_si128(
		_mm_sub_epi8(a, _mm_set1_epi8(99)),
		_mm_set1_epi8(99)
	);

In this case, the '99' constant can be used in both the subtract and or, but GCC will always convert the first use to a '-99' constant, meaning that it now has to deal with two constants: https://godbolt.org/z/gaKAkA

This can have a greater effect when the constants are held in registers, as the negated constant wastes a register, which can sometimes cause otherwise unnecessary register spilling elsewhere.

The behavior persists with AVX enabled, and I've even seen it for ARM NEON: https://godbolt.org/z/z3b5mq

---

Perhaps a different issue, but maybe related: I noticed that switching the order of the arguments for subtract, GCC seems to think the two constants are different, even though this is not the case: https://godbolt.org/z/6fGhGd

For this second example ((99-a)|99), I'd have thought the more appropriate assembly to be something like:

	vmovdqa xmm1, XMMWORD PTR .LC0[rip]
	vpsubb  xmm0, xmm1, xmm0
	vpor    xmm0, xmm0, xmm1


---


### compiler : `gcc`
### title : `Unnecessary register duplication of vector constant in x86`
### open_at : `2019-11-10T09:02:56Z`
### last_modified_date : `2021-08-21T23:24:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92437
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Consider the following code example:

	#include <x86intrin.h>
	void fn(__m128i* in, __m128i* out) {
		int i=0;
		const __m128i num = _mm_set1_epi8(99);
		while(i<100) {
			__m128i a = in[i];
			__m128i b = _mm_add_epi8(a, num);
			if(_mm_movemask_epi8(b))
				a = _mm_or_si128(a, num);
			if(_mm_movemask_epi8(a))
				a = _mm_or_si128(a, num);
			out[i] = a;
			i++;
		}
	}

The vector `num` is referenced 3 times in the loop, and GCC seems to load it into 3 separate registers, when 1 would suffice: https://godbolt.org/z/mP22ez (in this link, the `99` vector is held in xmm2, xmm3 and xmm4).

This seems to be the case regardless of AVX being enabled or not.

I don't really get what a possible cause for this is, but it seems that the `if` conditions are necessary to trigger this effect.


---


### compiler : `gcc`
### title : `Unnecessary memory read in a loop`
### open_at : `2019-11-11T13:39:50Z`
### last_modified_date : `2022-01-06T04:03:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92455
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the example:

typedef struct {
    int* ptr_; 
} int_ptr;  

int_ptr f1(int_ptr* x) {
    int_ptr* max = x;
    for (int i =0 ; i < 5; ++ i) {
        ++ x;
        if (*max->ptr_ < *x->ptr_) {
            max = x;
        }
    }
    return *max;
}

GCC with -O2 generates the following assembly:

f1(int_ptr*):
  lea rsi, [rdi+40]
  mov rax, rdi
.L3:
  mov rcx, QWORD PTR [rax]  ; <== This could be removed from the loop
  mov rdx, QWORD PTR [rdi+8]
  add rdi, 8
  mov edx, DWORD PTR [rdx]
  cmp DWORD PTR [rcx], edx
  cmovl rax, rdi
  cmp rsi, rdi
  jne .L3
  mov rax, QWORD PTR [rax]
  ret


If we rewrite the example to avoid int_ptr:

int* f2(int** x) {
    int** max = x;
    for (int i =0 ; i < 5; ++ i) {
        ++ x;
        if (**max < **x) {
            max = x;
        }
    }
    return *max;
}

Then there'll be less memory accesses in a loop:
f2(int**):
  mov rax, QWORD PTR [rdi] ; <=== Not in a loop any more
  lea rcx, [rdi+40]
.L8:
  mov rdx, QWORD PTR [rdi+8]
  add rdi, 8
  mov esi, DWORD PTR [rdx]
  cmp DWORD PTR [rax], esi
  cmovl rax, rdx
  cmp rcx, rdi
  jne .L8
  ret


Please improve the memory accesses for the first case

Godbolt playground: https://godbolt.org/z/CaGbT2


---


### compiler : `gcc`
### title : `GCC generates calls to __dpd_trunctdsd2 with -mhard-dfp`
### open_at : `2019-11-12T22:10:41Z`
### last_modified_date : `2020-07-21T21:23:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92488
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.1.1`
### severity : `normal`
### contents :
While cleaning up libdfp, I noticed GCC still leans on libgcc to perform _Decimal128 to _Decimal32 truncation.

This can be inlined with 4-5 instructions depending on ISA using the "round to prepare for shorter precision" rounding mode supported on all PPC DFP targets:

    mffs    fp4     /* Save current rounding mode.  */
    mtfsfi  7, 7, 1 /* Set round to prepare for shorter.  */
    drdpq   fp0,fp2 /* Initial round to _Decimal64.  */
    mtfsf   0xff,fp4,1,0 /* Restore previous rounding mode.  */
    drsp    fp1,fp0 /* Round result to _Decimal32.  */

mffs/mtsfi could be substituted for "mffscdrni fp4, 7" on ISA 3.0 and above.

e.g the following trivial example:

_Decimal32
truncd128(_Decimal128 d)
{
  return d;
}


---


### compiler : `gcc`
### title : `AVX512: Missed vectorization opportunity`
### open_at : `2019-11-13T08:35:55Z`
### last_modified_date : `2021-12-22T10:41:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92492
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Created attachment 47231
testcase cut from 525.x264_r

for simple testcase
cat test.c:

typedef unsigned char uint8_t;

static inline uint8_t x264_clip_uint8( int x )
{
  return x&(~63) ? (-x)>>7 : x;
}


void mc_weight( uint8_t *dst, uint8_t *src, int i_width, int i_height )
{
	for( int x = 0; x < i_width; x++ )
	    dst[x] = x264_clip_uint8(src[x]);
}

Refer to

https://godbolt.org/z/TnACA-

Icc generate much better code by using vptestmd and maskmov

gcc loop:
------------
        vmovdqu8        (%rsi,%rax), %ymm6
        vpmovzxbw       %xmm6, %ymm3
        vpmovzxwd       %xmm3, %ymm0
        vextracti128    $0x1, %ymm3, %xmm3
        vpand   %ymm5, %ymm0, %ymm9
        vpmovzxwd       %xmm3, %ymm3
        vextracti128    $0x1, %ymm6, %xmm2
        vpcmpd  $0, %ymm4, %ymm9, %k0
        vpmovzxbw       %xmm2, %ymm2
        vpand   %ymm5, %ymm3, %ymm9
        vpcmpd  $0, %ymm4, %ymm9, %k1
        vpmovzxwd       %xmm2, %ymm1
        vextracti128    $0x1, %ymm2, %xmm2
        vpand   %ymm5, %ymm1, %ymm9
        vpmovzxwd       %xmm2, %ymm2
        vpcmpd  $0, %ymm4, %ymm9, %k2
        vpand   %ymm5, %ymm2, %ymm9
        kunpckbw        %k0, %k1, %k0
        vpcmpd  $0, %ymm4, %ymm9, %k1
        vpsubd  %ymm0, %ymm4, %ymm0
        vpsubd  %ymm3, %ymm4, %ymm3
        vpsubd  %ymm1, %ymm4, %ymm1
        vpsubd  %ymm2, %ymm4, %ymm2
        kunpckbw        %k2, %k1, %k1
        kunpckwd        %k0, %k1, %k1
        vpermt2w        %ymm3, %ymm8, %ymm0
        vpermt2w        %ymm2, %ymm8, %ymm1
        vpsraw  $7, %ymm0, %ymm0
        vpsraw  $7, %ymm1, %ymm1
        vpand   %ymm0, %ymm7, %ymm0
        vpand   %ymm1, %ymm7, %ymm1
        vpackuswb       %ymm1, %ymm0, %ymm0
        vpermq  $216, %ymm0, %ymm0
        vmovdqu8        %ymm6, %ymm0{%k1}
        vmovdqu8        %ymm0, (%rdi,%rax)
        addq    $32, %rax
        cmpq    %rcx, %rax
------------

icc loop:
----------
        vpmovzxbd (%rsi,%r8), %ymm3                             #12.31
        vptestmd  %ymm1, %ymm3, %k1                             #5.12
        vpsubd    %ymm3, %ymm0, %ymm2                           #12.31
        vpsrad    $7, %ymm2, %ymm3{%k1}                         #12.31
        vpmovdb   %ymm3, (%r8,%rdi)                             #12.6
        addq      $8, %r8                                       #11.2
        cmpq      %rcx, %r8                                     #11.2
        jb        ..B1.7        # Prob 82%                      #11.2
----------


origin case cut from SPEC2017 525.x264_r, refer to attachment


---


### compiler : `gcc`
### title : `strncmp with constant unterminated arrays not folded`
### open_at : `2019-11-13T17:49:06Z`
### last_modified_date : `2019-11-22T17:02:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92501
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
GCC 9 and later fold strncmp calls with constant strings as arguments but it doesn't optimize equivalent calls when one or both arguments is an unterminated array.  The example below shows the difference.

Clang folds both.

$ cat z.c && gcc -O2 -c -Wall -Wextra -fdump-tree-optimized=/dev/stdout z.c
const char a[] = { '1', '2', '3', '\0' };
const char b[] = { '1', '2', '3' };

void f (void)
{
  if (__builtin_strncmp (a, "123", 3))
    __builtin_abort ();
}

void g (void)
{
  if (__builtin_strncmp (b, "123", 3))
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=2)

f ()
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1935, cgraph_uid=2, symbol_order=3)

g ()
{
  int _1;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strncmp (&b, "123", 3);
  if (_1 != 0)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `Use x86 xchg instruction more`
### open_at : `2019-11-17T16:59:33Z`
### last_modified_date : `2021-12-21T11:36:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92549
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Take this code

__attribute__((noinline))
int f(int a, int b)
{
  return b - a + 5;
}
int foo(int a, int b)
{
  return 1 + f(b, a);
}
int main()
{
  return foo(39, 3);
}

gcc 9.2.1 generates for foo on x86-64 this code:

        movl    %edi, %r8d
        movl    %esi, %edi
        movl    %r8d, %esi
        call    f
        addl    $1, %eax
        ret

This could be better:

        xchgl   %edi, %esi
        call    f
        addl    $1, %eax
        ret

Switching parameter location is not a uncommon pattern.

If the regparm is used on x86-32 the same likely applies there.


---


### compiler : `gcc`
### title : `Returning std∷map breaks tail-recursion optimization`
### open_at : `2019-11-18T13:07:52Z`
### last_modified_date : `2021-07-24T15:11:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92559
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `9.2.0`
### severity : `normal`
### contents :
The following code, exploiting tail-recursion, does not get optimized into a loop with -O3 option, so it crashes if you try to run it.

    #include <map>

    using MyMap = std::map<unsigned, unsigned>;

    MyMap foo(MyMap m) {
        if (m[0] == 0)
            return m;
        else {
            m[0] -= 1;
            return foo(m);
        }
    }

    int main() {
        MyMap m = {{0, 999999}};
        foo(m);
    }

While debugging GCC, I found that `find_tail_calls()` function quits at this code¹:

          /* If the statement references memory or volatile operands, fail.  */
          if (gimple_references_memory_p (stmt)
          || gimple_has_volatile_ops (stmt))
        return;


1: https://github.com/gcc-mirror/gcc/blob/38f05dc5db9241d3de8041a683972f086edce561/gcc/tree-tailcall.c#L464

I haven't found any duplicates of this so far.


---


### compiler : `gcc`
### title : `Inefficient code for multidimensional array assess`
### open_at : `2019-11-19T05:53:56Z`
### last_modified_date : `2023-06-25T03:43:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92574
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
For

[hjl@gnu-skx-1 tmp]$ cat foo3.c 
extern long bar[25][10];
long
foo (long ip, long itp)
{
  long ip1 = ip + 1;
  long itp1 = itp + 1;

  return (bar[ip][itp]
	  + bar[ip][itp1]
	  + bar[ip1][itp]
	  + bar[ip1][itp1]);
}

On x86-64 GCC -O2 generate:

	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	leaq	(%rdi,%rdi,4), %rax
	leaq	1(%rsi), %rcx
	addq	%rax, %rax
	leaq	(%rax,%rsi), %rdx
	addq	%rcx, %rax
	movq	bar(,%rax,8), %rax
	addq	bar(,%rdx,8), %rax
	leaq	5(%rdi,%rdi,4), %rdx
	addq	%rdx, %rdx
	addq	%rdx, %rsi
	addq	%rcx, %rdx
	addq	bar(,%rsi,8), %rax
	addq	bar(,%rdx,8), %rax
	ret
	.cfi_endproc

Clang 8 generates:

	.type	foo,@function
foo:                                    # @foo
	.cfi_startproc
# %bb.0:
	shlq	$4, %rdi
	leaq	(%rdi,%rdi,4), %rcx
	movq	bar+8(%rcx,%rsi,8), %rax
	addq	bar(%rcx,%rsi,8), %rax
	addq	bar+80(%rcx,%rsi,8), %rax
	addq	bar+88(%rcx,%rsi,8), %rax
	retq
.Lfunc_end0:
	.size	foo, .Lfunc_end0-foo
	.cfi_endproc


---


### compiler : `gcc`
### title : `[i386] cmov not generated`
### open_at : `2019-11-19T09:30:41Z`
### last_modified_date : `2019-12-09T10:03:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92578
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Cat test.c

int foo(int moves, int movecnt, int komove) {
    int newcnt = movecnt;
    if (moves == komove)
        newcnt--;
    return newcnt;
}

gcc10 -O2 test.c -S

        cmpl    %edx, %edi
        movl    %esi, %eax
        sete    %dl
        movzbl  %dl, %edx
        subl    %edx, %eax
        ret

It could be better like

        cmpl      %edx, %edi                                    #6.12
        lea       -1(%rsi), %eax                                #5.9
        cmovne    %esi, %eax                                    #6.12
        ret      

Just like icc did, refer to https://godbolt.org/z/6mqkt8


---


### compiler : `gcc`
### title : `A 454.calculix optimization opportunity`
### open_at : `2019-11-19T13:48:58Z`
### last_modified_date : `2019-11-25T08:40:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92584
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Richi, I noticed the following graph:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=8.170.0

I noticed that the speed drop back on the trunk happened since r278281.
Would you be interested in what loop optimization made the difference?


---


### compiler : `gcc`
### title : `Redundant comparison after subtraction on x86`
### open_at : `2019-11-20T07:32:26Z`
### last_modified_date : `2020-03-18T07:09:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92592
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Consider the example:

int sample(int a, int b) {    
    unsigned diff = (unsigned)b - (unsigned)a;
    unsigned sign_bit = b < a;
    return diff + sign_bit;
}

With -O2 and -O3 GCC produces the assembly:

sample(int, int):
  mov eax, esi  ; <=== not required
  xor edx, edx
  sub eax, edi
  cmp esi, edi  ; <=== not required
  setl dl
  add eax, edx
  ret

However, `sub` changes the status flags and there's no need to call `cmp`:

sample(int, int):
  xor eax, eax
  sub esi, edi
  setl al
  add eax, esi
  ret


The above sample is a minimized version of std::midpoint.

Godbolt playground: https://godbolt.org/z/j6FGq4


---


### compiler : `gcc`
### title : `Failure in gcc.target/powerpc/bswap64-2.c`
### open_at : `2019-11-20T22:34:16Z`
### last_modified_date : `2019-11-28T22:32:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92602
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
r278509 exposes this problem (the -fno-common patch).  It causes global
variables to be accessed via an anchor.

But now fwprop1 does:

In insn 8, replacing
 (mem/c:DI (reg/f:DI 119) [1 ul+0 S8 A64])
 with (mem/c:DI (unspec:DI [
                (symbol_ref:DI ("*.LANCHOR0") [flags 0x182])
                (reg:DI 2 2)
            ] UNSPEC_TOCREL) [1 ul+0 S8 A64])

after which we have

(insn 6 3 8 2 (parallel [
            (set (reg:DI 117 [ _1 ])
                (bswap:DI (reg/v:DI 118 [ a ])))
            (clobber (scratch:DI))
            (clobber (scratch:DI))
        ]) "bswap64-2.c":10:45 156 {bswapdi2_reg}
     (expr_list:REG_DEAD (reg/v:DI 118 [ a ])
        (nil)))
(insn 8 6 0 2 (set (mem/c:DI (unspec:DI [
                    (symbol_ref:DI ("*.LANCHOR0") [flags 0x182])
                    (reg:DI 2 2)
                ] UNSPEC_TOCREL) [1 ul+0 S8 A64])
        (reg:DI 117 [ _1 ])) "bswap64-2.c":10:43 622 {*movdi_internal64}
     (expr_list:REG_DEAD (reg/f:DI 119)
        (expr_list:REG_DEAD (reg:DI 117 [ _1 ])
            (nil))))

and those can no longer be combined.  Before the change, combine would
combine the reg-reg bswap and the simple indirect store to a bswapping
store.


---


### compiler : `gcc`
### title : `auto vectorization failed for type promotation`
### open_at : `2019-11-21T09:20:05Z`
### last_modified_date : `2020-05-15T08:19:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92611
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Cat test.c

void foo(int *__restrict dst, char *__restrict src)
{
	for(int x = 0; x < 4; x++ )
	    dst[x] = src[x];
}

Clang generate

---------------
        vpmovsxbd       (%rsi), %xmm0
        vmovdqu %xmm0, (%rdi)
        retq
---------------

while GCC generate

----------------
        movsbl  (%rsi), %eax
        movl    %eax, (%rdi)
        movsbl  1(%rsi), %eax
        movl    %eax, 4(%rdi)
        movsbl  2(%rsi), %eax
        movl    %eax, 8(%rdi)
        movsbl  3(%rsi), %eax
        movl    %eax, 12(%rdi)
        ret
---------------


Refer to https://godbolt.org/z/ckmXm_


---


### compiler : `gcc`
### title : `[9/10 Regression] Performance regression in compress-rar on CLX server`
### open_at : `2019-11-22T08:56:44Z`
### last_modified_date : `2020-03-31T10:07:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92626
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 47329
rar source code

We found 15% regression in clear 31520 compared to clear 31380 on compress-rar-1.1.0. The root cause is the update of gcc upstream version from gcc-9-branch@277298 to gcc-9-branch@277869(clear gcc 9.2.1-691 to 9.2.1-716). 

How to run,
phoronix install compress-rar-1.1.0
phoronix run compress-rar

The source code of compress-rar is also attached.
clear CFlags: -g -O3 -feliminate-unused-debug-types -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=32 -Wformat -Wformat-security -m64 -fasynchronous-unwind-tables -Wp,-D_REENTRANT -ftree-loop-distribute-patterns -Wl,-z -Wl,now -Wl,-z -Wl,relro -fno-semantic-interposition -ffat-lto-objects -fno-trapping-math -Wl,-sort-common -Wl,--enable-new-dtags -mtune=skylake


---


### compiler : `gcc`
### title : `Make use of TYPE_RESTRICT for function-call pointer-escape analysis – especially for Fortran`
### open_at : `2019-11-22T10:05:00Z`
### last_modified_date : `2021-12-22T10:14:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92628
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
As discussed, TYPE_RESTRICT should also be used for of pointer-escape analysis.

In Fortran, assume the following program. When honoring the Fortran semantics – passed to the ME via the TYPE_RESTRICT for 'n' – the loop can be optimized away:

  subroutine test(n)  ! n is passed by reference
    integer :: n, i
    n = 0
    call do_something() ! ME assumes that callee might modify 'n'
    ! the following loop could be optimized way as still  n == 0
    do i = 1, n
      call bar()
    end do
  end subroutine
    
A full test case and a *patch* to actually *set* TYPE_RESTRICT in the Fortran FE can be found at https://gcc.gnu.org/ml/fortran/2019-11/msg00127.html

Some more discussion and references to the Fortran standard can be found at
https://gcc.gnu.org/ml/fortran/2019-11/msg00126.html

 * * *

A similar rule applies to other variables in Fortran (in principle also to static variables). Example - the same but not using a PARM_DECL:
[Will surely require changes to the FE and not only to the ME.]

subroutine foo()
    integer :: n, i ! Both local variables
    call my_print(n)  ! n passed by reference, regarded as pointer escape
    n = 0
    call bar() ! ME assumes that the callee might modify 'n'
    ! the following loop could be optimized way as  n == 0
    do i = 1, n
      call bar()
    end d
end

[The Fortran programs become nicer by providing an 'interface', i.e. an explicit function declaration (cf. test case in the link above). But in terms of the pointer-escape analysis, this has no effect.]

 * * *

Fortran standard wise

Within the language, a pointer escape is only possible by associating a pointer to a variable – which is only permitted for variables with POINTER or TARGET attribute.
This can be locally removed (e.g. passing a TARGET variable as argument to a function whose dummy argument declaration does not have it. Then, within that function one can assume no aliasing is done [restriction on the programmer].)

Additionally, ASYNCHRONOUS ("call start_do(var); … ; call wait()") (implies default ME behavior, i.e. pointer address escapes) and VOLATILE have an effect. – As well as concurrency (coarrays, asynchronous I/O, thread parallelization, e.g. via OpenMP or OpenACC).

For a non-pointer, non-target variable, there remains one way to get two identifiers pointing to the same memory: Passing it as argument to a function (multiple times – or once and accessing it directly from the other scope).
In order to avoid problems here, Fortran requires (from the programmer) that those variables are either only accessed for reading – or that only one identifier is used for modification and all others are not not used for read access (neither before nor after the modification).

See https://gcc.gnu.org/ml/fortran/2019-11/msg00126.html for some language quotes.

Otherwise, see Fortran 2018, https://j3-fortran.org/doc/year/18/18-007r1.pdf for the definitions of TARGET, POINTER, ASYNCHRONOUS and VOLATILE.
For argument association, see Section 15.5.2 in general and 15.5.2.13 in particular. (An in-depth description of data association/defining/undefining is in Chapter 9)


---


### compiler : `gcc`
### title : `gcc unable to remove empty loop after loop body is removed after malloc/free pair was removed`
### open_at : `2019-11-23T09:29:01Z`
### last_modified_date : `2021-08-29T00:30:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92638
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
$cat test.c

#include<stdio.h>
#include<string.h>
#include<stdlib.h>
#include<time.h>

char* get(const char* value, const char separator) {
  int separator_index = strchr(value, separator) - value;
  char* result = (char*)malloc(separator_index);
  memcpy(result, value, separator_index);
  result[separator_index] = '\0';

  return result;
}

int main() {
  const char separator = ',';
  clock_t t = clock();

  for (size_t i = 0; i < 100000000; ++i) {
    free(get("127.0.0.1, 127.0.0.2:1111", separator));
  }

  float elapsed_seconds = (((double)(clock() - t)) / CLOCKS_PER_SEC);
  printf("%f seconds.\n", elapsed_seconds);

  return 0;
}


$ gcc -O3 -S -o-

get(char const*, char):
        push    rbp
        movsx   esi, sil
        mov     rbp, rdi
        push    rbx
        sub     rsp, 8
        call    strchr
        sub     rax, rbp
        movsx   rbx, eax
        mov     rdi, rbx
        call    malloc
        mov     rdx, rbx
        mov     rsi, rbp
        mov     rdi, rax
        call    memcpy
        mov     BYTE PTR [rax+rbx], 0
        add     rsp, 8
        pop     rbx
        pop     rbp
        ret
.LC1:
        .string "%f seconds.\n"
main:
        push    rbx
        call    clock
        mov     rbx, rax
        mov     eax, 100000000
.L5:
        sub     rax, 1 // <------------------ Loop body still there.
        jne     .L5
        call    clock
        pxor    xmm0, xmm0
        mov     edi, OFFSET FLAT:.LC1
        sub     rax, rbx
        cvtsi2sd        xmm0, rax
        mov     eax, 1
        divsd   xmm0, QWORD PTR .LC0[rip]
        cvtsd2ss        xmm0, xmm0
        cvtss2sd        xmm0, xmm0
        call    printf
        xor     eax, eax
        pop     rbx
        ret
.LC0:
        .long   0
        .long   1093567616


---


### compiler : `gcc`
### title : `Hand written vector code is 450 times slower when compiled with GCC compared to Clang`
### open_at : `2019-11-24T09:47:20Z`
### last_modified_date : `2023-05-23T02:23:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92645
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Hi,
the attached are preprocessed files for Skia where Clang ifdefs was removed so we get roughly same file for GCC and Clang. The internal loop of _ZN3hsw16blit_row_color32EPjPKjij, _ZN3hsw16blit_row_color32EPjPKjij, _ZN3hsw16blit_row_color32EPjPKjij and _ZN3hsw16blit_row_color32EPjPKjij
looks a lot worse when compiled by GCC then by clang. 

I also added flatten to eliminate the inlining difference. Clang has heuristics that makes functions with hand written vector code hot.

GCC code packs via stack:
  0.43 â       mov          %ax,0xae(%rsp)
  0.03 â       movzbl       0x78(%rsp),%eax
  0.02 â       mov          %cx,0xd8(%rsp)
  0.02 â       mov          %ax,0xb0(%rsp)
  0.54 â       vpextrb      $0x9,%xmm5,%eax
  0.16 â       mov          %ax,0xb2(%rsp)
  0.51 â       vpextrb      $0xa,%xmm5,%eax
  0.21 â       mov          %ax,0xb4(%rsp)
  0.16 â       vpextrb      $0xb,%xmm5,%eax
  0.46 â       mov          %ax,0xb6(%rsp)
  0.24 â       vpextrb      $0xc,%xmm5,%eax
  0.28 â       mov          %ax,0xb8(%rsp)
  0.41 â       vpextrb      $0xd,%xmm5,%eax
  0.20 â       mov          %ax,0xba(%rsp)
  0.47 â       vpextrb      $0xe,%xmm5,%eax
  0.92 â       mov          %ax,0xbc(%rsp)
  0.72 â       vpextrb      $0xf,%xmm5,%eax
  1.24 â       mov          %ax,0xbe(%rsp)
 10.94 â       vmovdqa      0xa0(%rsp),%ymm4
  0.02 â       mov          %cx,0xda(%rsp)
  0.00 â       mov          %cx,0xdc(%rsp)
       â       mov          %cx,0xde(%rsp)
 10.34 â       vpmullw      0xc0(%rsp),%ymm4,%ymm0
  2.05 â       vpaddw       %ymm1,%ymm0,%ymm0
  0.50 â       vpaddw       %ymm3,%ymm0,%ymm0
  0.00 â       mov          %r9,0x58(%rsp)
  0.52 â       vpsrlw       $0x8,%ymm0,%ymm0
  0.39 â       vpextrw      $0x0,%xmm0,%eax
  0.69 â       mov          %al,%r8b
  0.17 â       vpextrw      $0x1,%xmm0,%eax
  0.51 â       mov          %r8,0x50(%rsp)
  6.87 â       vmovdqa      0x50(%rsp),%xmm5
  1.08 â       vpinsrb      $0x1,%eax,%xmm5,%xmm1
  0.00 â       vpextrw      $0x2,%xmm0,%eax
  0.73 â       vpinsrb      $0x2,%eax,%xmm1,%xmm1
  0.02 â       vpextrw      $0x3,%xmm0,%eax

  0.75 â       vpinsrb      $0x3,%eax,%xmm1,%xmm1
  0.10 â       vpextrw      $0x4,%xmm0,%eax
  0.98 â       vpinsrb      $0x4,%eax,%xmm1,%xmm1
  0.16 â       vpextrw      $0x5,%xmm0,%eax
  1.00 â       vpinsrb      $0x5,%eax,%xmm1,%xmm1
  0.22 â       vpextrw      $0x6,%xmm0,%eax
  1.10 â       vpinsrb      $0x6,%eax,%xmm1,%xmm1
  0.30 â       vpextrw      $0x7,%xmm0,%eax
  0.31 â       vextracti128 $0x1,%ymm0,%xmm0
  0.90 â       vpinsrb      $0x7,%eax,%xmm1,%xmm6
  0.21 â       vpextrw      $0x0,%xmm0,%eax
  0.35 â       vmovaps      %xmm6,0x50(%rsp)
  1.15 â       mov          0x58(%rsp),%r9
  0.13 â       mov          0x50(%rsp),%r8
  0.29 â       mov          %al,%r9b
  0.49 â       mov          %r8,0x50(%rsp)
  0.07 â       vpextrw      $0x1,%xmm0,%eax
  0.45 â       mov          %r9,0x58(%rsp)
  7.08 â       vmovdqa      0x50(%rsp),%xmm7
  1.19 â       vpinsrb      $0x9,%eax,%xmm7,%xmm1
  0.00 â       vpextrw      $0x2,%xmm0,%eax
  0.78 â       vpinsrb      $0xa,%eax,%xmm1,%xmm1
  0.00 â       vpextrw      $0x3,%xmm0,%eax
  0.77 â       vpinsrb      $0xb,%eax,%xmm1,%xmm1
  0.01 â       vpextrw      $0x4,%xmm0,%eax
  0.86 â       vpinsrb      $0xc,%eax,%xmm1,%xmm1
  0.03 â       vpextrw      $0x5,%xmm0,%eax
  0.88 â       vpinsrb      $0xd,%eax,%xmm1,%xmm1
  0.04 â       vpextrw      $0x6,%xmm0,%eax
  0.97 â       vpinsrb      $0xe,%eax,%xmm1,%xmm1
  0.08 â       vpextrw      $0x7,%xmm0,%eax
  1.44 â       vpinsrb      $0xf,%eax,%xmm1,%xmm0
  1.37 â       vpextrd      $0x1,%xmm0,%eax
  0.13 â       vinsertps    $0xe,%xmm0,%xmm0,%xmm1
  0.02 â       vmovaps      %xmm0,0x50(%rsp)
  2.17 â       vpinsrd      $0x1,%eax,%xmm1,%xmm1

....

Clang code:

Percentâ       vpmullw      %ymm0,%ymm2,%ymm2
       â       vpaddw       %ymm1,%ymm2,%ymm2
       â       vpsrlw       $0x8,%ymm2,%ymm2
       â       vextracti128 $0x1,%ymm2,%xmm3
       â       vpackuswb    %xmm3,%xmm2,%xmm2
       â       vmovdqu      %xmm2,(%rdi)
       â       add          $0x10,%rsi
       â       add          $0x10,%rdi
       â       mov          %r9d,%eax
       â       cmp          $0x4,%r9d
       â     â jae          39179b0 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0xa0>
       â     â jmp          3917a02 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0xf2>
       â       mov          %edx,%eax
  0.29 â       cmp          $0x4,%r9d
  0.00 â     â jb           3917a02 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0xf2>
  0.07 â       nop
  3.95 â       vpmovzxbw    (%rsi),%ymm2
 13.41 â       vpmullw      %ymm0,%ymm2,%ymm2
 13.87 â       vpaddw       %ymm1,%ymm2,%ymm2
  2.93 â       vpsrlw       $0x8,%ymm2,%ymm2
  0.84 â       vextracti128 $0x1,%ymm2,%xmm3
  9.98 â       vpackuswb    %xmm3,%xmm2,%xmm2
  6.89 â       vmovdqu      %xmm2,(%rdi)
  0.57 â       vpmovzxbw    0x10(%rsi),%ymm2
  4.02 â       vpmullw      %ymm0,%ymm2,%ymm2
 12.15 â       vpaddw       %ymm1,%ymm2,%ymm2
  2.02 â       vpsrlw       $0x8,%ymm2,%ymm2
  1.22 â       vextracti128 $0x1,%ymm2,%xmm3
  8.04 â       vpackuswb    %xmm3,%xmm2,%xmm2
  7.09 â       vmovdqu      %xmm2,0x10(%rdi)
  0.19 â       add          $0x20,%rsi
  0.19 â       add          $0x20,%rdi
  0.57 â       add          $0xfffffff8,%eax
  7.26 â       cmp          $0x3,%eax
  0.29 â     â jg           39179b0 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0xa0>
  0.02 â       and          $0x3,%edx
Percentâ       test         %edx,%edx
  0.04 â     â jle          3917af1 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0x1e1>
       â       mov          %ecx,%eax
       â       shr          $0x18,%eax
       â       shr          $0x1f,%ecx
       â       add          %eax,%ecx
       â       vmovd        %ecx,%xmm0
       â       vpbroadcastb %xmm0,%xmm0
       â       mov          %r8,%rax
       â       shl          $0x20,%rax
       â       or           %r8,%rax
       â       vmovq        %rax,%xmm1
       â       vpbroadcastq %xmm1,%xmm1
       â       vpmovzxbw    %xmm1,%ymm1
       â       vpmovzxbw    %xmm0,%ymm0
       â       vpsllw       $0x8,%ymm1,%ymm1
       â       vpor         0x1f3eb35(%rip),%ymm1,%ymm1        # 5856580 <SkNamedGamut::kAdobeRGB+0x858>
       â       test         $0x1,%dl
       â     â jne          3917a5c <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0x14c>
       â       mov          %edx,%eax
       â       cmp          $0x1,%edx
       â     â jne          3917a90 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0x180>
       â     â jmpq         3917af1 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0x1e1>
       â       lea          -0x1(%rdx),%eax
       â       mov          (%rsi),%ecx
       â       add          $0x4,%rsi
       â       vmovq        %rcx,%xmm2
       â       vpmovzxbw    %xmm2,%xmm2
       â       vpmullw      %xmm0,%xmm2,%xmm2
       â       vpaddw       %xmm1,%xmm2,%xmm2
       â       vpsrlw       $0x8,%xmm2,%xmm2
       â       vpackuswb    %xmm0,%xmm2,%xmm2
       â       vmovq        %xmm2,%rcx
       â       mov          %ecx,(%rdi)
       â       add          $0x4,%rdi
       â       cmp          $0x1,%edx
       â       xor          %ecx,%ecx
       â       nop
       â       nop
       â       mov          (%rsi,%rcx,1),%edx
       â       vmovq        %rdx,%xmm2
       â       vpmovzxbw    %xmm2,%xmm2
       â       vpmullw      %xmm0,%xmm2,%xmm2
       â       vpaddw       %xmm1,%xmm2,%xmm2
       â       vpsrlw       $0x8,%xmm2,%xmm2
       â       vpackuswb    %xmm0,%xmm2,%xmm2
       â       vmovd        %xmm2,(%rdi,%rcx,1)
       â       mov          0x4(%rsi,%rcx,1),%edx
       â       vmovq        %rdx,%xmm2
       â       vpmovzxbw    %xmm2,%xmm2
       â       vpmullw      %xmm0,%xmm2,%xmm2
       â       vpaddw       %xmm1,%xmm2,%xmm2
       â       vpsrlw       $0x8,%xmm2,%xmm2
       â       vpackuswb    %xmm0,%xmm2,%xmm2
       â       vmovd        %xmm2,0x4(%rdi,%rcx,1)
       â       add          $0x8,%rcx
       â       add          $0xfffffffe,%eax
       â     â jg           3917aa0 <hsw::blit_row_color32(unsigned int*, unsigned int const*, int, unsigned int)+0x190>
  0.38 â       vzeroupper
  0.26 â     â retq


---


### compiler : `gcc`
### title : `dead store elimination by iteration domain pruning`
### open_at : `2019-11-25T02:28:16Z`
### last_modified_date : `2020-03-04T07:56:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92649
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
For this small case,

int f(void)
{
        int i, a[1024];

        for (i=0; i<1024; i++)
                a[i] = 5;
        return a[0];
}

"gcc -O3" can't figure out the memory stores from a[1] to a[1023] all can be eliminated. The assembly code for aarch64 is as below.

        movi    v0.4s, 0x5
        sub     sp, sp, #4096
        mov     x0, sp
        add     x1, sp, 4096
.L2:
        str     q0, [x0], 16
        cmp     x0, x1
        bne     .L2
        ldr     w0, [sp]
        add     sp, sp, 4096
        ret


---


### compiler : `gcc`
### title : `[10 Regression] Unnecessary stv transform in some x86 backend`
### open_at : `2019-11-25T05:36:22Z`
### last_modified_date : `2021-07-26T23:44:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92651
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
For test case

#include <math.h>

int foo(unsigned char a, unsigned char b)
{
    int isum=abs(a - b);
    return isum;
}

Using -O2 -march=corei7 GCC generates:

        movzx   edi, dil
        movzx   esi, sil
        movd    xmm1, edi
        movd    xmm0, esi
        movdqa  xmm3, xmm1
        psubd   xmm3, xmm0
        psubd   xmm0, xmm1
        pmaxsd  xmm0, xmm3
        movd    eax, xmm0
        ret

while on -O2 -march=x86-64 it will be:

        movzx   eax, dil
        movzx   esi, sil
        sub     eax, esi
        cdq
        xor     eax, edx
        sub     eax, edx
        ret

On other case using -O2 -march=corei7 -mtune=generic:

        movzx   edi, dil
        movzx   esi, sil
        mov     eax, edi
        sub     eax, esi
        sub     esi, edi
        cmp     eax, esi
        cmovl   eax, esi
        ret

This happens since r277481. (Refers to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91154). In STV2 the transform was executed since the  sse_to_integer RTL cost for corei7 is 2, which made the conversion worthwhile for some cmove instructions. I think it affects most IA processors with such kind of cost.

The stv conversion results in about 7% regression on 525.x264_r. I wonder if the conversion is designed on purpose to handle cmove, if not I think it is better to adjust sse_to_integer RTL cost to avoid such issue. According to my experiment, 6 would be a proper value.


---


### compiler : `gcc`
### title : `Suboptimal vectorization of variable shift`
### open_at : `2019-11-25T09:00:49Z`
### last_modified_date : `2021-08-25T03:24:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92655
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
For the following testcase vect_recog_vector_vector_shift_pattern isn't able
to fully elide the use of 'int' and thus we fall back to vectorization with
SSE instead of AVX with -O3 -march=core-avx2

#define MULSEQ {32,34,35,38}
#define STRIDE 4
#define M61 2305843009213693951ULL
#define BITS 61
typedef unsigned long uint64_t;
typedef struct myvec_t { uint64_t __attribute__ ((aligned (32))) val[STRIDE]; } __attribute__ ((aligned (32))) myvec_t;
inline uint64_t MULWU(uint64_t k, uint64_t m)
{
  return (( (k)<<(m) & M61) + ( (k) >> (BITS-m))  )  ;
}
myvec_t MULWU(myvec_t x)
{
  myvec_t __attribute__ ((aligned (32))) v;
  myvec_t __attribute__ ((aligned (32))) SPECIALMUL=MULSEQ;
  for(int j=0;j<STRIDE;j++)
    v.val[j] = MULWU(x.val[j], SPECIALMUL.val[j]);
  return v;
}


---


### compiler : `gcc`
### title : `The zero_extend insn can't be eliminated in the combine pass`
### open_at : `2019-11-25T11:35:29Z`
### last_modified_date : `2022-10-16T18:10:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92656
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
I compiled the C function with "-march=rv32imafc -mabi=ilp32f -mtune=sifive-7-series -O2 -funroll-loops", and there are more slli/ srli instructions than GCC 8.3.

==========
 C Source
==========
unsigned short foo(unsigned short val, unsigned short result) {
  unsigned char i = 0;
  unsigned char data = (unsigned char)(val >> 8);

  for (i = 0; i < 3; i++) {
    data >>= 1;
    if (data & 1)
      result ^= 0x4002;
    result >>= 1;
  }

  return result;
}

==========
 Assembly
 GCC 9.2
=========
foo:
	li	a5,16384
	srli	a4,a0,9
	addi	t0,a5,2
	andi	t1,a4,1
	xor	a3,a1,t0
	srli	a2,a0,10
	bne t1,zero,1f; mv a3,a1; 1: # movcc
	srli	t2,a3,1
	andi	a6,a2,1
	xor	a1,t2,t0
	srli	a0,a0,11
	slli	a7,a1,16  ###
	andi	t5,a0,1
	srli	t3,a7,16  ###
	bne a6,zero,1f; mv t3,t2; 1: # movcc
	srli	t4,t3,1
	slli	t6,t4,16  ###
	srli	a5,t6,16  ###
	xor	t0,a5,t03
	slli	a4,t0,16  ###
	srli	t1,a4,16  ###
	bne t5,zero,1f; mv t1,a5; 1: # movcc
	srli	a0,t1,1
	ret

==========
 Assembly
 GCC 8.3
==========
foo:
	srli	a0,a0,8
	li	a5,16384
	addi	t0,a5,2
	srli	a2,a0,1
	xor	a3,a1,t0
	andi	t1,a2,1
	bne t1,zero,1f; mv a3,a1; 1: # movcc
	srli	a4,a0,2
	srli	t2,a3,1
	andi	a1,a4,1
	xor	a6,t2,t0
	bne a1,zero,1f; mv a6,t2; 1: # movcc
	srli	a7,a0,3
	srli	t3,a6,1
	andi	t4,a7,1
	xor	t5,t3,t0
	bne t4,zero,1f; mv t5,t3; 1: # movcc
	srli	a0,t5,1
	ret

When combiner try to combine zero_extend insn and another insn, the subst pattern can not simplify according rule below because the last condition (nonzero_bits) can not be met. 
In simplify-rtx.c:
/* (zero_extend:M (subreg:N <X:O>)) is <X:O> (for M == O) or
   (zero_extend:M <X:O>), if X doesn't have any non-zero bits outside
   of mode N.  E.g.
   (zero_extend:SI (subreg:QI (and:SI (reg:SI) (const_int 63)) 0)) is
   (and:SI (reg:SI) (const_int 63)).  */
if (partial_subreg_p (op)
    && is_a <scalar_int_mode> (mode, &int_mode)
    && is_a <scalar_int_mode> (GET_MODE (SUBREG_REG (op)), &op0_mode)
    && GET_MODE_PRECISION (op0_mode) <= HOST_BITS_PER_WIDE_INT
    && GET_MODE_PRECISION (int_mode) >= GET_MODE_PRECISION (op0_mode)
    && subreg_lowpart_p (op)
    && (nonzero_bits (SUBREG_REG (op), op0_mode)
    & ~GET_MODE_MASK (GET_MODE (op))) == 0)
{
    if (GET_MODE_PRECISION (int_mode) == GET_MODE_PRECISION (op0_mode))
      return SUBREG_REG (op);
    return simplify_gen_unary (ZERO_EXTEND, int_mode, SUBREG_REG (op),
                               op0_mode);
}

By the way, I also noticed this issue could be caused by 2-to-2 combination (https://gcc.gnu.org/viewcvs/gcc?view=revision&revision=263067).


---


### compiler : `gcc`
### title : `High stack usage due ftree-ch`
### open_at : `2019-11-25T13:00:15Z`
### last_modified_date : `2020-01-05T11:15:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92657
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 47351
High stack usage due ftree-ch

The code snippet (gcc_free_ch_stack.c) shows a high stack usage.  With GCC 9.2.1 I see the resulting stack usage using -fstack-usage along with -O2:

arm                     632
aarch64                 448
powerpc                 912
powerpc64le             560
s390                    600
s390x                   632
i386                    1376
x86_64                  784

The same issue also shows in master branch. It seems that it is due -ftree-ch pass with feeds -ftree-loop-im, -ftree-pre, -fmove-loop-invariants, and -fgcse. Andrew Pinski suggested is mostly due lack of a good estimate register pressure for loop invariant code motion.

Andrew also suggested to use -fno-tree-loop-im -fno-tree-pre -fno-gcse, however even with this options the resulting stack usage does not get in par with -Os option (which disables -ftree-ch).  On powerpc64le:

$ ./gcc/xgcc -v 2>&1 | grep 'gcc version'
gcc version 10.0.0 20191121 (experimental) (GCC) 

$ ./gcc/xgcc -B gcc -O2 stack_usage.c -fstack-usage -c; cat stack_usage.su
stack_usage.c:157:6:mlx5e_grp_sw_update_stats	496	static

$ ./gcc/xgcc -B gcc -O2 stack_usage.c -fstack-usage -c -fno-tree-loop-im -fno-tree-pre -fno-move-loop-invariants -fno-gcse; cat stack_usage.su
stack_usage.c:157:6:mlx5e_grp_sw_update_stats	176	static$ ./gcc/xgcc -B gcc -Os stack_usage.c -fstack-usage -c; cat stack_usage.su

$ ./gcc/xgcc -B gcc -Os stack_usage.c -fstack-usage -c; cat stack_usage.su
stack_usage.c:157:6:mlx5e_grp_sw_update_stats	32	static


---
