### Total Bugs Detected: 4649
### Current Chunk: 22 of 30
### Bugs in this Chunk: 160 (From bug 3361 to 3520)
---


### compiler : `gcc`
### title : `(bit_and (negate (convert thurth_value@1)) integer_onep) is not optimized to just @1`
### open_at : `2021-07-06T08:43:21Z`
### last_modified_date : `2023-05-01T20:34:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101339
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(int a, int b)
{
  _Bool c = a == b;
  int t = c;
  int t1 = -t;
  return t1 & 1;
}
---- CUT ---
This is not optimized to just "return (int)(a == b)" at the gimple level


---


### compiler : `gcc`
### title : `SLP discovery via vect_slp_linearize_chain is imperfect`
### open_at : `2021-07-06T10:52:05Z`
### last_modified_date : `2022-02-03T11:57:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101340
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
For polyhedron capacita we have the hot fourir routine which with -ffast-math
suffers from reassoc perturbing the SLP lanes to not match.  SLP discovery
reassociation could help here but it's limited by the single_use check.

For loop vect we could allow uses outside of the chain but of course we
do not want to expand multi-uses inside.  That doesn't fit well with the
simple worklist approach but would need sth more elaborate.


---


### compiler : `gcc`
### title : `missed tail call with constructor to global data and RSO`
### open_at : `2021-07-06T21:56:25Z`
### last_modified_date : `2022-10-28T09:12:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101353
### status : `NEW`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
// https://godbolt.org/z/9zP5dP3sE
#include <new>
struct T {
    int x;
    T(int) noexcept;
    ~T();
};
T factory(int) noexcept;
alignas(T) char buffer[sizeof(T)];
void placement_new() {
    ::new ((void*)buffer) T(42);
}
void placement_call() {
    ::new ((void*)buffer) T(factory(42));
}

g++ -O2 -std=c++20 test.cpp

Somehow the compiler is failing to generate an actual `jmp` tail call for `placement_call` the way it's able to for `placement_new`.

  _Z13placement_newv:
    movl $42, %esi
    movl $buffer, %edi
    jmp _ZN1TC1Ei
  _Z14placement_callv:
    subq $8, %rsp
    movl $42, %esi
    movl $buffer, %edi
    call _Z7factoryi
    addq $8, %rsp
    ret

Note that right now Clang has the same symptom; but ICC and MSVC both get this right and generate tail-calls appropriately in both cases. So this isn't any obscure C++ corner case AFAICT; seems it's truly just a missed optimization.

Clang's missed-optimization bug is filed as https://bugs.llvm.org/show_bug.cgi?id=51000


---


### compiler : `gcc`
### title : `memset codegen for constant sized does not use SSE instructions`
### open_at : `2021-07-07T14:21:25Z`
### last_modified_date : `2021-07-08T13:03:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101366
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
https://gcc.godbolt.org/z/hP99MYMEG

void Test(char* dst) {
    __m128i pattern = _mm_set1_epi8(dst[-1]);
    for (int i = 0; i < 4; i++) {
        _mm_loadu_si128(reinterpret_cast<__m128i*>(dst + 16 * i), pattern);
    }
}

vs

void TestStd(char* s) {
    memset(s, s[-1], 64);
}


-O3 -msse4.2

Test(char*):
        movzbl  -1(%rdi), %eax
        pxor    %xmm1, %xmm1
        movd    %eax, %xmm0
        pshufb  %xmm1, %xmm0
        movups  %xmm0, (%rdi)
        movups  %xmm0, 16(%rdi)
        movups  %xmm0, 32(%rdi)
        movups  %xmm0, 48(%rdi)
        ret
TestStd(char*):
        movabsq $72340172838076673, %rdx
        movzbl  -1(%rdi), %eax
        movq    %rax, %rcx
        imulq   %rdx, %rcx
        mulq    %rdx
        addq    %rcx, %rdx
        movq    %rax, (%rdi)
        movq    %rdx, 8(%rdi)
        movq    %rax, 16(%rdi)
        movq    %rdx, 24(%rdi)
        movq    %rax, 32(%rdi)
        movq    %rdx, 40(%rdi)
        movq    %rax, 48(%rdi)
        movq    %rdx, 56(%rdi)
        ret


---


### compiler : `gcc`
### title : `Expand vector mod as vector div + multiply-subtract`
### open_at : `2021-07-09T12:21:28Z`
### last_modified_date : `2021-07-13T09:43:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101390
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
When the target supports an sdiv/udiv pattern for vector modes we could synthesise a vector modulus operation using the division and a multiply-subtract operation.
#define N 128

extern signed int si_a[N], si_b[N], si_c[N];

void
test_si ()
{
  for (int i = 0; i < N; i++)
    si_c[i] = si_a[i] % si_b[i];
}

On AArch64 SVE (but not Neon) has vector SDIV/UDIV instructions and so could generate:
.L2:
        ld1w    z2.s, p0/z, [x4, x0, lsl 2]
        ld1w    z1.s, p0/z, [x3, x0, lsl 2]
        movprfx z0, z2
        sdiv    z0.s, p1/m, z0.s, z1.s
        msb     z0.s, p1/m, z1.s, z2.s
        st1w    z0.s, p0, [x1, x0, lsl 2]
        incw    x0
        whilelo p0.s, w0, w2
        b.any   .L2

This can be achieved by implementing the smod and mod optabs in the aarch64 backend for SVE, but this is a generic transformation, so could be handled more generally in vect_recog_divmod_pattern and/or the vector lowering code so that more targets can benefit.


---


### compiler : `gcc`
### title : `strlen of a constant char vector not folded`
### open_at : `2021-07-09T21:47:07Z`
### last_modified_date : `2021-07-09T21:47:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101401
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
The strlen pass only folds the result in f() but not the equivalent in g().  It should be able to handle both.

$ cat a.c && gcc -O2 -S -fdump-tree-optimized=/dev/stdout a.c
void f (void)
{
  char c8[8] = { 1, 2, 3 };
  if (__builtin_strlen (&c8[0]) != 3)
    __builtin_abort ();
}

void g (void)
{
  __attribute__ ((vector_size (8))) char c8 = { 1, 2, 3 };
  if (__builtin_strlen (&c8[0]) != 3)
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1943, cgraph_uid=1, symbol_order=0)

void f ()
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1947, cgraph_uid=2, symbol_order=1)

void g ()
{
  vector(8) char c8;
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  c8 = { 1, 2, 3, 0, 0, 0, 0, 0 };
  _1 = __builtin_strlen (&VIEW_CONVERT_EXPR<char[8]>(c8)[0]);
  if (_1 != 3)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  c8 ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `cond_removal_in_popcount_clz_ctz_pattern and factor_out_conditional_conversion do a similar transformation`
### open_at : `2021-07-10T13:43:55Z`
### last_modified_date : `2023-04-11T02:16:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101404
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Looking into what code could be simplified, I noticed cond_removal_in_popcount_clz_ctz_pattern and factor_out_conditional_conversion are really doing a similar transformation and could even be combined.

Yes cond_removal_in_popcount_clz_ctz_pattern checks the conditional part of the if statement but I don't think it needs to really.


---


### compiler : `gcc`
### title : `NULL dereferences aren't assumed to be unreachable even with -fdelete-null-pointer-checks -fno-isolate-erroneous-paths-dereference`
### open_at : `2021-07-13T05:25:28Z`
### last_modified_date : `2021-07-13T07:44:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101432
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
Consider this C code:

#include <stddef.h>
int f(_Bool x) {
    if(x) {
        int *null = NULL;
        return *null;
    } else {
        return 42;
    }
}

I want this assembly:

f:
        movl    $42, %eax
        ret

But even with -O3 -fdelete-null-pointer-checks -fno-isolate-erroneous-paths-dereference, I get this assembly:

f:
        movl    $42, %eax
        testb   %dil, %dil
        je      .L1
        movl    0, %eax
.L1:
        ret


---


### compiler : `gcc`
### title : `vector-by-vector left shift expansion for char/short is not optimal`
### open_at : `2021-07-13T10:55:08Z`
### last_modified_date : `2021-08-25T03:27:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101434
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
short r[8], a[8], b[8];

void f1 (void)
{
  int i;

  for (i = 0; i < 8; i++)
    r[i] = a[i] << b[i];
}
--cut here--

compiles with -O2 -ftree-vectorize -mxop to:

        vmovdqa a(%rip), %xmm0
        vmovdqa b(%rip), %xmm1
        vpmovsxwd       %xmm0, %xmm2
        vpsrldq $8, %xmm0, %xmm0
        vpmovsxwd       %xmm1, %xmm3
        vpsrldq $8, %xmm1, %xmm1
        vpshad  %xmm3, %xmm2, %xmm2
        vpmovsxwd       %xmm0, %xmm0
        vpmovsxwd       %xmm1, %xmm1
        vpshad  %xmm1, %xmm0, %xmm0
        vpperm  .LC0(%rip), %xmm0, %xmm2, %xmm2
        vmovdqa %xmm2, r(%rip)
        ret

SImode vpshad is used together with lots of other instructions, but a HImode vpshaw should be emitted instead.

Similar testcase:

--cut here--
short r[8], a[8], b[8];

void f2 (void)
{
  int i;

  for (i = 0; i < 8; i++)
    r[i] = a[i] >> b[i];
}
--cut here--

results in expected HImode vect-by-vect shift insn:

        vpxor   %xmm0, %xmm0, %xmm0
        vpsubw  b(%rip), %xmm0, %xmm0
        vpshaw  %xmm0, a(%rip), %xmm0
        vmovdqa %xmm0, r(%rip)
        ret

(do not bother with vpxor and vpsubw, these are just one of XOP peculiarities.)


---


### compiler : `gcc`
### title : `Unnecessary vzeroupper when upper bits of YMM registers already zero`
### open_at : `2021-07-14T22:39:23Z`
### last_modified_date : `2023-05-08T12:22:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101456
### status : `REOPENED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Unnecessary vzeroupper:

[hjl@gnu-cfl-2 tmp]$ cat x.c 
#include <x86intrin.h>

extern __m256d x;

void
foo (void)
{
  x = _mm256_setzero_pd ();
}
[hjl@gnu-cfl-2 tmp]$ gcc -S -O2 x.c -mavx2 
c[hjl@gnu-cfl-2 tmp]$ cat x.s 
	.file	"x.c"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB5667:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovapd	%ymm0, x(%rip)
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	vzeroupper  <<<<<< Not needed since upper bits of YMM0 are zero.
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE5667:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 11.1.1 20210531 (Red Hat 11.1.1-3)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-2 tmp]$


---


### compiler : `gcc`
### title : `Optimizers should fold bounds checking for -D_GLIBCXX_ASSERTIONS=1`
### open_at : `2021-07-15T19:18:13Z`
### last_modified_date : `2022-06-02T00:46:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101466
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
https://godbolt.org/z/fPdGeKMYM

Should fold together and optimize to something like this

https://godbolt.org/z/6cooP81xc


---


### compiler : `gcc`
### title : `SRA sometimes produces worse code with inline functions (seen with -fipa-icf sometimes)`
### open_at : `2021-07-16T16:33:54Z`
### last_modified_date : `2021-11-22T07:03:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101474
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 51166
Test case

I've come across a weird behaviour when using -fipa-icf, could maybe be related to bug 80277.

Build with -O1 -fipa-icf and the second version of the identical function actually has worse codegen than if you have just one of them there or than if you hadn't passed -fipa-icf at all.

See example: https://godbolt.org/z/n8zz947aK


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] -ftree-loop-distribute-patterns can slow down and increases size of code`
### open_at : `2021-07-17T02:59:16Z`
### last_modified_date : `2023-05-29T10:05:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101481
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.1`
### severity : `normal`
### contents :
Created attachment 51168
simplified example reproducing problem

Hi,

I found -ftree-loop-distribute-patterns to be far too aggressive in replacing code, leading to increased code size and substantial slowdowns (12% in the program I just hit this).

The code size increase & slowdown are partially caused by the function call itself, and partially due to the spilling necessary to make that function call. Worsened by the PLT call to memmove().

A very simplified example (also attached) is this:

typedef struct node
{
    unsigned char chunks[4];
    unsigned char count;
} node;

void
foo(node *a, unsigned char newchunk, unsigned char off)
{
    if (a->count > 3)
        __builtin_unreachable();

    for (int i = a->count - 1; i >= off; i--)
        a->chunks[i + 1] = a->chunks[i];
    a->chunks[off] = newchunk;
}

which with `-O2 -fPIC` boils down to:
foo(node*, unsigned char, unsigned char):
        pushq   %r12
        movl    %edx, %r8d
        movl    %esi, %r12d
        pushq   %rbp
        movq    %rdi, %rbp
        pushq   %rbx
        movzbl  4(%rdi), %ecx
        movzbl  %r8b, %ebx
        leal    -1(%rcx), %edx
        cmpl    %ebx, %edx
        jl      .L2
        movl    %ecx, %eax
        movslq  %edx, %rsi
        subl    %ebx, %ecx
        subl    $1, %ecx
        movq    %rsi, %rdx
        subq    %rcx, %rdx
        leaq    1(%rcx), %r8
        leaq    (%rdi,%rdx), %rsi
        movzbl  %al, %edi
        movq    %r8, %rdx
        movq    %rdi, %rax
        subq    %rcx, %rax
        leaq    0(%rbp,%rax), %rdi
        call    memmove@PLT
.L2:
        movb    %r12b, 0(%rbp,%rbx)
        popq    %rbx
        popq    %rbp
        popq    %r12
        ret

compare to `-O2 -fPIC -fno-tree-loop-distribute-patterns`

foo(node*, unsigned char, unsigned char):
        movzbl  4(%rdi), %eax
        movzbl  %dl, %edx
        subl    $1, %eax
        cmpl    %edx, %eax
        jl      .L2
        cltq
.L3:
        movzbl  (%rdi,%rax), %ecx
        movb    %cl, 1(%rdi,%rax)
        subq    $1, %rax
        cmpl    %eax, %edx
        jle     .L3
.L2:
        movb    %sil, (%rdi,%rdx)
        ret

Which I think makes the problem apparent.

Regards,

Andres Freund


---


### compiler : `gcc`
### title : `Calling std::equal with std::byte* does not use memcmp`
### open_at : `2021-07-17T11:56:20Z`
### last_modified_date : `2021-07-18T21:14:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101485
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.0`
### severity : `normal`
### contents :
See https://godbolt.org/z/fKbhqe1sz

```
#include <algorithm>

auto equal_with_bytes(std::byte* b1, std::byte* e1, std::byte* b2) {
  return std::equal(b1, e1, b2);
}

auto equal_with_chars(unsigned char* b1, unsigned char* e1, unsigned char* b2) {
  return std::equal(b1, e1, b2);
}
```

In the above code `equal_with_chars` invokes `memcmp` while `equal_with_bytes` does not, although it would be eligble.

The problematic code seems to be in `stl_algobase.h`

```
  template<typename _II1, typename _II2>
    _GLIBCXX20_CONSTEXPR
    inline bool
    __equal_aux1(_II1 __first1, _II1 __last1, _II2 __first2)
    {
      typedef typename iterator_traits<_II1>::value_type _ValueType1;
      const bool __simple = ((__is_integer<_ValueType1>::__value
			      || __is_pointer<_ValueType1>::__value)
			     && __memcmpable<_II1, _II2>::__value);
      return std::__equal<__simple>::equal(__first1, __last1, __first2);
    } 
```

since `std::byte` is neither an integer nor a pointer.

Suggested fix: Extend the condition with a check for `std::byte`

```
__is_integer<_ValueType1>::__value || __is_pointer<_ValueType1>::__value || __is_byte<_ValueType1>::__value
```


---


### compiler : `gcc`
### title : `CONSTRUCTOR for vector(1) should just be VCE`
### open_at : `2021-07-20T22:01:04Z`
### last_modified_date : `2022-06-21T01:52:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101540
### status : `ASSIGNED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
I noticed with the following source:
typedef unsigned char __attribute__((__vector_size__ (1))) W;
typedef unsigned char __attribute__((__vector_size__ (8))) V;
typedef unsigned short __attribute__((__vector_size__ (16))) U;

unsigned short us;

W
foo (unsigned char uc)
{
  V v = __builtin_convertvector ((U){ } >= us, V);
  return __builtin_shufflevector ((W){ }, v, 4) & uc;
}

int
main (void)
{
  W x = foo (5);
  if (x[0] != 5)
    __builtin_abort();
  return 0;
}
----- CUT ----
We produce
_6 = {uc_10(D)};
But this should be the same as
_6 = VIEW_CONVERT_EXPR<uc_10>

--- CUT ---
This is what we get in the IR:
  _6 = {uc_10(D)};
  _14 = BIT_FIELD_REF <_4, 8, 0>;
  _15 = VIEW_CONVERT_EXPR<unsigned char>(_6);
  _16 = _14 & _15;
  _17 = VIEW_CONVERT_EXPR<W>(_16);

We do change the BIT_FIELD_REF that was assigned to _15 to a VCE though.


---


### compiler : `gcc`
### title : `extra zeroing of empty struct argument/return value`
### open_at : `2021-07-21T05:37:26Z`
### last_modified_date : `2022-10-27T03:11:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101543
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
struct Tag {
    friend Tag make_tag();
private:
    Tag() {}
};

Tag make_tag() { 
    return Tag{}; 
};

void needs_tag(Tag);

void foo1(void) {
    Tag t = make_tag();
    needs_tag(t);
}


struct Tag1 {};
struct Tag2 {};
struct Tag3 {};
struct Tag4 {};
struct Tag5 {};

void needs_tags(int x, Tag1 t1, Tag2 t2, Tag3 t3, Tag4 t4, Tag5 t5, int y);

void foo2(Tag1 t1, Tag2 t2, Tag3 t3, Tag4 t4, Tag5 t5)
{
needs_tags(12345, t1, t2, t3, t4, t5, t6, t7,t8, 200);
}

---- CUT ---
In make_tag the return register does not need to be zerod.
In foo1, the argument x0 does not need to be zeroed before passing to needs_tag.
In foo2, argument registers x1-x5 don't need to have been zeroed before passing to needs_tags.


---


### compiler : `gcc`
### title : `Suboptimal codegen for __builtin_shufflevector`
### open_at : `2021-07-22T12:41:37Z`
### last_modified_date : `2021-08-25T07:28:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101579
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
For

---
typedef unsigned int __attribute__((__vector_size__ (32))) U;
typedef unsigned char __attribute__((__vector_size__ (64))) V;

V g;

U
foo (void)
{
  V v = __builtin_shufflevector (g, g,
				 0, 1, 2, 0, 5, 1, 0, 1, 3, 2, 3, 0, 4, 3, 1, 2,
				 2, 0, 4, 2, 3, 1, 1, 2, 3, 4, 1, 1, 0, 0, 5, 2,
				 0, 3, 3, 3, 3, 4, 5, 0, 1, 5, 2, 1, 0, 1, 1, 2,
				 3, 2, 0, 5, 4, 5, 1, 0, 1, 4, 4, 3, 4, 5, 2, 0)
;
  v ^= 255;
  V w = v + g;
  U u = ((union { V a; U b; }) w).b + ((union { V a; U b; }) w).b[1];
  return u;
}
---

GCC 12 -march=skylake -O2 generates

	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	vpcmpeqd	%ymm4, %ymm4, %ymm4
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	andq	$-64, %rsp
	subq	$72, %rsp
	movzbl	g+2(%rip), %ecx
	movzbl	g+1(%rip), %edx
	movzbl	g(%rip), %eax
	movzbl	g+3(%rip), %edi
	movzbl	g+5(%rip), %esi
	movzbl	g+4(%rip), %r8d
	vmovd	%ecx, %xmm7
	vmovd	%edi, %xmm0
	vmovd	%edx, %xmm1
	vmovd	%eax, %xmm5
	vmovdqa	%xmm7, -72(%rsp)
	vpinsrb	$1, %eax, %xmm7, %xmm11
	vmovd	%r8d, %xmm2
	vmovd	%esi, %xmm7
	vpinsrb	$1, %edx, %xmm5, %xmm6
	vpinsrb	$1, %edi, %xmm2, %xmm14
	vpinsrb	$1, %ecx, %xmm1, %xmm12
	vpinsrb	$1, %ecx, %xmm0, %xmm15
	vpinsrb	$1, %edx, %xmm7, %xmm8
	vpinsrb	$1, %eax, %xmm0, %xmm3
	vpunpcklwd	%xmm11, %xmm6, %xmm13
	vpunpcklwd	%xmm12, %xmm14, %xmm9
	vpunpcklwd	%xmm6, %xmm8, %xmm8
	vpunpcklwd	%xmm3, %xmm15, %xmm3
	vpunpckldq	%xmm8, %xmm13, %xmm8
	vpunpckldq	%xmm9, %xmm3, %xmm3
	vpunpcklqdq	%xmm3, %xmm8, %xmm3
	vpaddb	g(%rip), %ymm4, %ymm10
	vmovdqa	%xmm14, -88(%rsp)
	vmovdqa	%xmm3, -104(%rsp)
	vpinsrb	$1, %r8d, %xmm0, %xmm9
	vpinsrb	$1, %ecx, %xmm7, %xmm4
	vpinsrb	$1, %ecx, %xmm2, %xmm3
	vpinsrb	$1, %edx, %xmm0, %xmm14
	vpinsrb	$1, %edx, %xmm1, %xmm8
	vpinsrb	$1, %eax, %xmm5, %xmm13
	vpunpcklwd	%xmm4, %xmm13, %xmm13
	vpunpcklwd	%xmm8, %xmm9, %xmm8
	vpunpcklwd	%xmm3, %xmm11, %xmm3
	vpunpcklwd	%xmm12, %xmm14, %xmm14
	vmovdqa	-104(%rsp), %xmm4
	vpunpckldq	%xmm13, %xmm8, %xmm8
	vpunpckldq	%xmm14, %xmm3, %xmm3
	vpunpcklqdq	%xmm8, %xmm3, %xmm3
	vmovdqa	-72(%rsp), %xmm13
	vinserti128	$0x1, %xmm3, %ymm4, %ymm3
	vpsubb	%ymm3, %ymm10, %ymm10
	vmovdqa	%ymm10, -56(%rsp)
	vmovdqa	%xmm10, %xmm8
	vpinsrb	$1, %esi, %xmm1, %xmm3
	vpinsrb	$1, %edi, %xmm5, %xmm10
	vpinsrb	$1, %edi, %xmm0, %xmm0
	vpinsrb	$1, %eax, %xmm7, %xmm7
	vpinsrb	$1, %edx, %xmm13, %xmm13
	vpunpcklwd	%xmm13, %xmm3, %xmm3
	vpunpcklwd	%xmm0, %xmm10, %xmm0
	vpunpcklwd	%xmm7, %xmm9, %xmm9
	vpunpcklwd	%xmm12, %xmm6, %xmm6
	vpunpckldq	%xmm6, %xmm3, %xmm6
	vpunpckldq	%xmm9, %xmm0, %xmm0
	vpunpcklqdq	%xmm6, %xmm0, %xmm0
	vpinsrb	$1, %eax, %xmm1, %xmm6
	vpinsrb	$1, %r8d, %xmm1, %xmm1
	vpunpcklwd	-88(%rsp), %xmm1, %xmm1
	vpinsrb	$1, %esi, %xmm5, %xmm5
	vpinsrb	$1, %esi, %xmm2, %xmm2
	vpunpcklwd	%xmm5, %xmm15, %xmm3
	vpunpcklwd	%xmm6, %xmm2, %xmm5
	vpunpcklwd	%xmm11, %xmm2, %xmm2
	vpcmpeqd	%ymm4, %ymm4, %ymm4
	vpunpckldq	%xmm5, %xmm3, %xmm3
	vpunpckldq	%xmm2, %xmm1, %xmm1
	vpaddb	g+32(%rip), %ymm4, %ymm4
	vpunpcklqdq	%xmm1, %xmm3, %xmm1
	vinserti128	$0x1, %xmm1, %ymm0, %ymm0
	vpsubb	%ymm0, %ymm4, %ymm4
	vmovdqa	%xmm8, 8(%rsp)
	vmovdqa	%ymm4, -24(%rsp)
	vmovdqa	-40(%rsp), %xmm4
	vpbroadcastd	12(%rsp), %ymm0
	vmovdqa	%xmm4, 24(%rsp)
	vpaddd	8(%rsp), %ymm0, %ymm0
	leave
	.cfi_def_cfa 7, 8
	ret

clang 12 generates

foo:                                    # @foo
	.cfi_startproc
# %bb.0:
	vmovdqa	g(%rip), %ymm0
	vpcmpeqd	%ymm1, %ymm1, %ymm1
	vpxor	%ymm1, %ymm0, %ymm1
	vpermq	$68, %ymm1, %ymm1               # ymm1 = ymm1[0,1,0,1]
	vpshufb	.LCPI0_0(%rip), %ymm1, %ymm1    # ymm1 = ymm1[0,1,2,0,5,1,0,1,3,
2,3,0,4,3,1,2,18,16,20,18,19,17,17,18,19,20,17,17,16,16,21,18]
	vpaddb	%ymm0, %ymm1, %ymm0
	vpbroadcastd	.LCPI0_1(%rip), %ymm1   # ymm1 = [1,1,1,1,1,1,1,1]
	vpermd	%ymm0, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	retq
.Lfunc_end0:
	.size	foo, .Lfunc_end0-foo
	.cfi_endproc


---


### compiler : `gcc`
### title : `(a|b) ==/!= a -> (b & ~a) ==/!= 0 on gimple`
### open_at : `2021-07-23T04:02:26Z`
### last_modified_date : `2021-07-27T20:02:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101591
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f(unsigned a, unsigned b)
{
    return (a|b) == a;
}

This should be optimized to:
bool f(unsigned a, unsigned b)
{
    return (b & ~a) ==/!= 0;
}

It might be only best if you we do for constant b's though.


---


### compiler : `gcc`
### title : `no right shift pattern for vector(2) long long on aarch64`
### open_at : `2021-07-24T05:31:14Z`
### last_modified_date : `2021-08-25T02:32:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101609
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
#define SIZE 16
typedef long long veci __attribute__((vector_size(SIZE)));

void f(veci &a, veci &b, veci &c){
  c= a>>b;
}
----- CUT ----
We currently produce:
        ldr     q1, [x1]
        ldr     q0, [x0]
        fmov    d3, d1
        dup     d1, v1.d[1]
        fmov    d2, d0
        dup     d0, v0.d[1]
        neg     d4, d3
        sshl    d4, d2, d4
        neg     d2, d1
        sshl    d2, d0, d2
        fmov    x1, d4
        fmov    x0, d2
        stp     x1, x0, [x2]
        ret
----- CUT ----
But we should be able to do:
        ldr     q0, [x1]
        ldr     q1, [x0]
        neg     v0.2d, v0.2d
        sshl    v0.2d, v1.2d, v0.2d
        str     q0, [x2]


---


### compiler : `gcc`
### title : `CST - (x ^ (CST-1)) can be optimized to x + 1 if x < CST and CST is a power of 2`
### open_at : `2021-07-24T06:20:15Z`
### last_modified_date : `2021-08-03T23:16:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101610
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
unsigned long f(unsigned long x)
{
    if (x >= 64)__builtin_unreachable();
    x = x ^ 63;
unsigned long y = (unsigned long )x;
unsigned long z = 64 - y;
return z;
}
This should just be:
unsigned long long f1(unsigned long x)
{
  return x + 1;
}


---


### compiler : `gcc`
### title : `AVX2 vector arithmetic shift lowered to scalar unnecessarily`
### open_at : `2021-07-24T08:17:58Z`
### last_modified_date : `2021-08-04T22:01:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101611
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Stealing the example from PR 56873

#define SIZE 32
typedef long long veci __attribute__((vector_size(SIZE)));

veci f(veci a, veci b){
  return a>>b;
}

but compiling with -O3 -mavx2 this time, gcc produces scalar code

	vmovq	%xmm1, %rcx
	vmovq	%xmm0, %rax
	vpextrq	$1, %xmm0, %rsi
	sarq	%cl, %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrq	$1, %xmm1, %rcx
	vextracti128	$0x1, %ymm1, %xmm1
	movq	%rax, %rdx
	sarq	%cl, %rsi
	vmovq	%xmm0, %rax
	vmovq	%xmm1, %rcx
	vmovq	%rdx, %xmm5
	sarq	%cl, %rax
	vpextrq	$1, %xmm1, %rcx
	movq	%rax, %rdi
	vpextrq	$1, %xmm0, %rax
	vpinsrq	$1, %rsi, %xmm5, %xmm0
	sarq	%cl, %rax
	vmovq	%rdi, %xmm4
	vpinsrq	$1, %rax, %xmm4, %xmm1
	vinserti128	$0x1, %xmm1, %ymm0, %ymm0
	ret

while clang outputs much shorter vector code

	vpbroadcastq	.LCPI0_0(%rip), %ymm2   # ymm2 = [9223372036854775808,9223372036854775808,9223372036854775808,9223372036854775808]
	vpsrlvq	%ymm1, %ymm2, %ymm2
	vpsrlvq	%ymm1, %ymm0, %ymm0
	vpxor	%ymm2, %ymm0, %ymm0
	vpsubq	%ymm2, %ymm0, %ymm0
	retq


---


### compiler : `gcc`
### title : `_Complex float multiply expansion does not allow for a tail call to __mulsc3`
### open_at : `2021-07-24T08:48:11Z`
### last_modified_date : `2021-07-27T10:47:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101612
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
_Complex float f(_Complex float x, _Complex float y) {
  return x*y;
}
----- CUT -----
At -O2, we produce during complex lowering:
  if (_13 unord _14)
    goto <bb 4>; [0.05%]
  else
    goto <bb 3>; [99.95%]

  <bb 4> [local count: 536864]:
  _15 = __mulsc3 (x$real_5, x$imag_6, y$real_7, y$imag_8);
  _16 = REALPART_EXPR <_15>;
  _17 = IMAGPART_EXPR <_15>;
  GIMPLE_NOP

  <bb 3> [local count: 1073741824]:
  # _18 = PHI <_13(2), _16(4)>
  # _19 = PHI <_14(2), _17(4)>
  _3 = COMPLEX_EXPR <_18, _19>;

This does allow __mulsc3 to be tailed call as the tail call coding does not know anything about COMPLEX_EXPR and REAL/IMAGPART_EXPR.


---


### compiler : `gcc`
### title : `a ? -1 : 1 -> (-(type)a) | 1`
### open_at : `2021-07-25T05:47:46Z`
### last_modified_date : `2022-06-04T09:14:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101617
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
I noticed this while looking through a few bug reports but it deals with <=> too.
Take:
int f(int i)
{
  int t = i ? -1 : 0;
  return t | 1;
}

int f1(int i)
{
  return i ? -1 : 1;
}

------- CUT ------
These two should produce the same code generation.

Filing so I don't lose this while I am going through bug reports.


---


### compiler : `gcc`
### title : `gcc cannot optimize int8_t vector assign with subscription to shuffle`
### open_at : `2021-07-26T01:45:37Z`
### last_modified_date : `2021-07-27T11:01:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101621
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.1`
### severity : `enhancement`
### contents :
https://gcc.godbolt.org/z/91cqenf99

typedef char v16b __attribute__((vector_size(16)));

To summary it up, regarding optimizing v = { v[n] ...} into shuffle, targeting Intel x86(x86_64):
These is a lack of optimization when there is a zero
There is some regression starting from gcc9.
so this might be 2 issues. But I think a proper fix could resolve both.


* gcc can optimize int8_t vector assign with subscription of the same vector to shuffle, like this:
v16b gcc_can_shuffle(v16b b) {
    return (v16b) {b[0], b[0], b[0], b[0], b[4], b[4], b[4], b[4], b[8], b[8], b[8], b[8], b[12], b[12], b[12], b[12]};
}

* However, if there is a zero, gcc can't handle this. Actually this is supported on Intel x86, with a negative subscription indicating the 'zero value'.
Clang can do the optimization starting with clang 5.

* Furthermore, there is a regression:
gcc < 8 can always optimize it, but starting with gcc9, if there is a cast, then the optimization fails:
typedef long v2si64 __attribute__((vector_size(16)));
v16b gcc_cannot_shuffle_with_cast(v2si64 x) {
    v16b b = (v16b)x;
    v16b b0 = {b[0], b[0], b[0], b[0], b[4], b[4], b[4], b[4], b[8], b[8], b[8], b[8], b[12], b[12], b[12], b[12]};
    return b0;
}
gcc 11 can optimize it on -O3, but not on -O1 or -O2.


---


### compiler : `gcc`
### title : `#pragma omp for simd defeats VECT_COMPARE_COSTS optimisations`
### open_at : `2021-07-27T10:02:59Z`
### last_modified_date : `2021-07-27T11:09:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101637
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Compiling this with -O3 -march=armv8.2-a+sve:

void
foo (__INT64_TYPE__ *a, __INT32_TYPE__ *b, __INT16_TYPE__ *c)
{
//#pragma omp for simd
  for (int i = 0; i < 100; ++i)
    a[i] = b[i] + c[i];
}

gives:

.L2:
        ld1w    z1.s, p0/z, [x1, x3, lsl 2]
        ld1sh   z0.s, p0/z, [x2, x3, lsl 1]
        punpklo p1.h, p0.b
        add     z0.s, z0.s, z1.s
        punpkhi p0.h, p0.b
        sunpklo z1.d, z0.s
        sunpkhi z0.d, z0.s
        st1d    z1.d, p1, [x0, x3, lsl 3]
        st1d    z0.d, p0, [x5, x3, lsl 3]
        add     x3, x3, x6
        whilelo p0.s, w3, w4
        b.any   .L2

whereas uncommenting the pragma gives the considerably uglier:

.L2:
        ld1h    z0.h, p0/z, [x2, x3, lsl 1]
        punpklo p1.h, p0.b
        punpkhi p0.h, p0.b
        ld1w    z2.s, p1/z, [x1, x3, lsl 2]
        ld1w    z3.s, p0/z, [x7, x3, lsl 2]
        punpklo p2.h, p1.b
        punpkhi p1.h, p1.b
        sunpklo z1.s, z0.h
        sunpkhi z0.s, z0.h
        add     z1.s, z1.s, z2.s
        add     z0.s, z0.s, z3.s
        sunpklo z2.d, z1.s
        sunpklo z3.d, z0.s
        sunpkhi z1.d, z1.s
        sunpkhi z0.d, z0.s
        st1d    z1.d, p1, [x0, #1, mul vl]
        punpklo p1.h, p0.b
        punpkhi p0.h, p0.b
        st1d    z3.d, p1, [x0, #2, mul vl]
        st1d    z0.d, p0, [x0, #3, mul vl]
        st1d    z2.d, p2, [x0]
        add     x3, x3, x6
        add     x0, x0, x5
        whilelo p0.h, w3, w4
        b.any   .L2

For VECT_COMPARE_COSTS targets, we should probably still consider all
possibilities and pick the “best” vector implementation (ignoring the
comparison with scalar code).


---


### compiler : `gcc`
### title : `vectorization with bool reduction`
### open_at : `2021-07-27T10:58:49Z`
### last_modified_date : `2023-08-27T05:57:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101639
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
bool f(char* p, long n)
{
  bool r = true;
  for(long i = 0; i < n; ++i)
    r &= (p[i] != 0);
  return r;
}

is not vectorized, while if I simply declare r as char instead of bool, it is (not quite optimal since it fails to pull &1 out of the loop, but that's a separate issue).


---


### compiler : `gcc`
### title : `(len | N) == len is transformed to len & N != 0`
### open_at : `2021-07-27T19:55:47Z`
### last_modified_date : `2023-08-23T01:22:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101650
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
While looking at PR 101590, I noticed this one.
#include <stddef.h>

const size_t N = 4;

bool foo(size_t len) {
	size_t newlen = len | N;
	return newlen == len;
}


(for cmp  (NE EQ)
     icmp (EQ NE)
 (simplify
  (cmp (bit_ior @0 INTEGER_CST@1) @0)
  (icmp (bit_and @0 @1) ({ build_zero_cst (TREE_TYPE (@0)); })
 )
)

This is already done for bit_and.


---


### compiler : `gcc`
### title : `BB vectorizer doesn't handle lowpart of existing vector`
### open_at : `2021-07-29T01:47:47Z`
### last_modified_date : `2022-06-02T06:47:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101668
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c

typedef int v16si __attribute__((vector_size (64)));
typedef long long v8di __attribute__((vector_size (64)));

void
bar_s32_s64 (v8di * dst, v16si src)
{
  long long tem[8];
  tem[0] = src[0];
  tem[1] = src[1];
  tem[2] = src[2];
  tem[3] = src[3];
  tem[4] = src[4];
  tem[5] = src[5];
  tem[6] = src[6];
  tem[7] = src[7];
  dst[0] = *(v8di *) tem;
}

gcc -O3 -march=skylake-avx512 will fail to vectorize the case after my r12-2549 because i've increased vec_construct cost for SKX/CLX. Here's dump for slp2

  <bb 2> [local count: 1073741824]:
  _1 = BIT_FIELD_REF <src_18(D), 32, 0>;
  _2 = (long long int) _1;
  _3 = BIT_FIELD_REF <src_18(D), 32, 32>;
  _4 = (long long int) _3;
  _5 = BIT_FIELD_REF <src_18(D), 32, 64>;
  _6 = (long long int) _5;
  _7 = BIT_FIELD_REF <src_18(D), 32, 96>;
  _8 = (long long int) _7;
  _9 = BIT_FIELD_REF <src_18(D), 32, 128>;
  _10 = (long long int) _9;
  _11 = BIT_FIELD_REF <src_18(D), 32, 160>;
  _12 = (long long int) _11;
  _13 = BIT_FIELD_REF <src_18(D), 32, 192>;
  _14 = (long long int) _13;
  _15 = BIT_FIELD_REF <src_18(D), 32, 224>;
  _31 = {_1, _3, _5, _7, _9, _11, _13, _15};
  vect__2.4_32 = (vector(8) long long int) _31;
  _16 = (long long int) _15;
  MEM <vector(8) long long int> [(long long int *)&tem] = vect__2.4_32;
  _17 = MEM[(v8di *)&tem];
  *dst_28(D) = _17;
  tem ={v} {CLOBBER};
  return;

But actually, there's no need for vec_contruct from each element, it will be optimized to

   <bb 2> [local count: 1073741824]:
  _2 = BIT_FIELD_REF <src_18(D), 256, 0>;
  vect__2.4_32 = (vector(8) long long int) _2;
  *dst_28(D) = vect__2.4_32;
  return;

So at the time slp2 can realize the optimization and categorize vec_contruct cost more accurately, we can avoid this regression.


---


### compiler : `gcc`
### title : `^ not changed to | if the non-zero don't overlap`
### open_at : `2021-07-29T13:11:04Z`
### last_modified_date : `2023-08-28T20:21:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101676
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Take:
int f(int a, int b)
{
    if (a & ~8) __builtin_unreachable();
    if (b & 8) __builtin_unreachable();
    return a ^ b;
}
----- CUT -----
The xor here can converted into ior as a and b non-zero bits don't overlap.
Note clang does this optimization.

But clang does not do the following (which we should be able to do too).
Take:
int f(int a, int b)
{
    if (a & 8) __builtin_unreachable();
    if (b & ~1) __builtin_unreachable();
    return (a) ^ (b<<3);
}
This should be able to turn into a bit insert of b into a.
Right now both GCC (and really clang) turn this on aarch64:
        eor     w0, w0, w1, lsl 3 ;;;clang uses orr

Both can handle the following though:
int f(int a, int b)
{
 //   if (a & 8) __builtin_unreachable();
  //  if (b & ~1) __builtin_unreachable();
    return (a&~8) ^ ((b&1)<<3);
}
Unless you uncomment out the condtionals :).


---


### compiler : `gcc`
### title : `failure to shrink wrap simple loop with more aggressive jump threading`
### open_at : `2021-07-30T10:26:54Z`
### last_modified_date : `2022-12-28T20:52:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101690
### status : `UNCONFIRMED`
### tags : `missed-optimization, testsuite-fail, xfail`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
gcc.dg/shrink-wrap-loop.c is failing after the jump threader rewrite.  The test is:

int foo (int *p1, int *p2);

int
test (int *p1, int *p2)
{
  int *p;

  for (p = p2; p != 0; p++)
    {
      if (!foo (p, p1))
        return 0;
    }

  return 1;
}

Compile with -O2 -fdump-rtl-pro_and_epilogue.

The dump file is missing "Performing shrink-wrapping".

What happens is that the new threader is threading things a bit too early.

The gist is this BB inside a loop:

  <bb 6> :
  # p_2 = PHI <p2_6(D)(2), p_12(5)>
  if (p_2 != 0B)
    goto <bb 3>; [INV]
  else
    goto <bb 7>; [INV]

Our threader can move this check outside of the loop (good).  This is
done before branch probabilities are calculated and causes the probs
to be calculated as:

<bb 2> [local count: 216361238]:
  if (p2_6(D) != 0B)
    goto <bb 7>; [54.59%]
  else
    goto <bb 6>; [45.41%]

Logically this seems correct to me.  A simple check outside of a loop
should slightly but not overwhelmingly favor a non-zero value.

Interestingly however, the old threader couldn't get this, but the IL
ended up identical, albeit with different probabilities.  What happens
is that, because the old code could not thread this, the p2 != 0 check
would remain inside the loop and probs would be calculated thusly:

  <bb 6> [local count: 1073741824]:
  # p_2 = PHI <p2_6(D)(2), p_12(5)>
  if (p_2 != 0B)
    goto <bb 3>; [94.50%]
  else
    goto <bb 7>; [5.50%]

Then when the loop header copying pass ("ch") shuffled things around,
the IL would end up identical to my early threader code, but with the
probabilities would remain as 94.5/5.5.

The above discrepancy causes the RTL ifcvt pass to generate different
code, and by the time we get to the shrink wrapping pass, things look
sufficiently different such that the legacy code can actually shrink
wrap, whereas our new code does not.

IMO, if the loop-ch pass moves conditionals outside of a loop, the
probabilities should be adjusted, but that does mean the shrink wrap
won't happen for this contrived testcase.

Perhaps someone with more knowledge of loop-ch or the shrink wrapping code could help here.


---


### compiler : `gcc`
### title : `Terrible SIMD register allocation with a tight loop operating on 8 registers.`
### open_at : `2021-07-30T14:04:23Z`
### last_modified_date : `2023-10-16T17:19:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101693
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
There are a few issues regarding unnecessary register spilling, but this also exhibits a lot of unnecessary juggling between registers.

See https://godbolt.org/z/da76fY1n7 and https://www.reddit.com/r/cpp_questions/comments/oui5tc/simd_what_to_do_when_your_compiler_forgets_how_to/

The gist is that there's a tight loop, executed a constant number of times (~64 times) where accumulation happens to 8 ymm registers, and only those 8 registers are used from outside of the loop. Before the loop zeros are assinged, and after the loop horizontal addition is performed. GCC generates suboptimal code, whereas clang gets it right. It seems to perform unnecessary movs in a pattern following a -> b -> vpdpbusd to b -> a. All versions on godbolt >=8.1 seem to exhibit the issue, including trunk.


---


### compiler : `gcc`
### title : `GCC optimization and code generation for if-else chains vs ternary chains vs a switch`
### open_at : `2021-07-30T23:37:26Z`
### last_modified_date : `2021-08-03T14:07:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101701
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
I'm looking at an example of three equivalent functions implemented with if-else chains, ternary chains, and a switch. Gcc is not compiling them equivalently: https://godbolt.org/z/8cjGr7M7W.

For the if-else chain, gcc does not optimize away the jumps.
For the ternary chain, gcc does its codegen well.
For the switch, gcc also does its codegen well but there is an extra mov instruction compared to the ternary chain.

I don't think it's idealistic to say these should compile equivalently - if someone told me to prefer one over the other for performance reasons I'd dismiss it as a micro-optimization.

Clang does not do this perfectly either at the moment.




This bug is probably miscategorized. I am not sure the correct category for it.


---


### compiler : `gcc`
### title : `(bool0 + bool1) & 1 and (bool0 + bool1) == 1 can be optimized to bool0 ^ bool1`
### open_at : `2021-07-31T07:05:01Z`
### last_modified_date : `2023-06-23T05:31:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101703
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f(bool a, bool b)
{
    int t = a;
    int t1 = b;
    return (t + t1) & 1;
}
bool fa(bool a, bool b)
{
    int t = a;
    int t1 = b;
    return (t + t1)==1;
}
bool fb(bool a, bool b)
{
    return a!=b;
}
bool fc(bool a, bool b)
{
    return a^b;
}

These three should produce the same code gen.  Right now fb and fc do but f and fa needs to handled.

the for fa, == 1 can be converted into & 1 as the range is [0,2]:
  # RANGE [0, 2] NONZERO 3
  _1 = t_3 + t1_5;
  _6 = _1 == 1;

and only 1 can be if & 1 is true. And the rest just follows through.


---


### compiler : `gcc`
### title : `Missed optimization opportunity when copying lots of bitfields`
### open_at : `2021-07-31T13:25:15Z`
### last_modified_date : `2023-07-19T04:10:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101705
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.1`
### severity : `normal`
### contents :
Using gcc-11.1.1 [revision 62bbb113ae68a7e724255e17143520735bcb9ec9], I observe that gcc is able to recognize and combine lots of "structb->member = structa->member" assignments into SIMD instructions. However, this only works as long as "member" has exactly 8*n bits. It would appear gcc is not smart enough to consider smaller entities (if and when they add up to multiples of 8 or 64).

Observed
========
[On x86_64]

»gcc -O3 -c 1.c; gcc -O3 -c 2.c
»objdump -d 1.o
   0:   f3 0f 6f 3e             movdqu (%rsi),%xmm7
...
»objdump -d 2.o
   0:   0f b6 16                movzbl (%rsi),%edx
   3:   0f b6 07                movzbl (%rdi),%eax
   6:   83 e2 01                and    $0x1,%edx
   9:   83 e0 fe                and    $0xfffffffe,%eax
   c:   09 d0                   or     %edx,%eax
   e:   88 07                   mov    %al,(%rdi)
  10:   0f b6 16                movzbl (%rsi),%edx
  13:   83 e0 fd                and    $0xfffffffd,%eax
  16:   83 e2 02                and    $0x2,%edx
  19:   09 d0                   or     %edx,%eax
  1b:   88 07                   mov    %al,(%rdi)
...

Expected
========
Emit some movdqu/movups even for 2.c.

Other info
==========
gcc version 11.1.1 20210625 [revision 62bbb113ae68a7e724255e17143520735bcb9ec9] (SUSE Linux) &
gcc version 7.5.0 (SUSE Linux)


---


### compiler : `gcc`
### title : `bool0^bool1^1 -> bool0 == bool1`
### open_at : `2021-07-31T19:44:55Z`
### last_modified_date : `2022-11-26T16:47:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101706
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
void fa(bool &a, bool &b, bool &c)
{
    c= a==b;
}
void fb(bool &a, bool &b, bool &c)
{
    int t = (a^b);
    c= (t^1);
}

I don't know how often this shows up but I am putting this here for completeness.


---


### compiler : `gcc`
### title : `Suboptimal codegen when clearing struct fields`
### open_at : `2021-07-31T23:58:13Z`
### last_modified_date : `2021-08-03T12:00:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101708
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC trunk (as well as 11.2) currently seem to be performing suboptimal codegen in reset2 and reset3, that in theory should yield the same (or very similar to the) result of reset1:

struct example {
    unsigned a;
    unsigned char b;
    unsigned c:1;
    unsigned d:2;
    unsigned char f; 
    unsigned char g;
    unsigned e;
};

void reset1(struct example *e) {
    e->b = 0,
    e->d = 0,
    e->e = 0,
    e->f = 0,
    e->g = 0;
}

void reset2(struct example *e) {
    *e = (struct example) {
        .a = e->a,
        .c = e->c
    };
}

void reset3(struct example *e) {
    struct example tmp = {
        .a = e->a,
        .c = e->c
    };
    *e = tmp;
}

compiled with -O2 -Wall yields:

reset1:
        and     QWORD PTR [rdi+4], 63744
        ret
reset2:
        movzx   eax, BYTE PTR [rdi+5]
        mov     edx, DWORD PTR [rdi]
        mov     DWORD PTR [rdi+8], 0
        mov     QWORD PTR [rdi], 0
        and     eax, 1
        mov     DWORD PTR [rdi], edx
        mov     BYTE PTR [rdi+5], al
        ret
reset3:
        mov     QWORD PTR [rsp-8], 0
        mov     eax, DWORD PTR [rdi]
        mov     DWORD PTR [rsp-12], eax
        movzx   eax, BYTE PTR [rdi+5]
        and     eax, 1
        mov     BYTE PTR [rsp-7], al
        mov     rax, QWORD PTR [rsp-12]
        mov     QWORD PTR [rdi], rax
        mov     eax, DWORD PTR [rsp-4]
        mov     DWORD PTR [rdi+8], eax
        ret

Godbolt link: https://godbolt.org/z/asr57TWqq

Bug reported at the request of @gnutools: https://twitter.com/gnutools/status/1421525698515251207

I tentatively selected rtl-optimization as the component, but I'm not familiar at all with the internals of GCC: feel free to update as needed.


---


### compiler : `gcc`
### title : `lea does not have a zero_extend version of it`
### open_at : `2021-08-01T17:52:11Z`
### last_modified_date : `2021-10-03T00:43:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101716
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
unsigned long long sample1(unsigned long long m) {
    unsigned int t = -1;
    return (m << 1) & t;
}

---- CUT ----
Currently we produce:
        lea     rax, [rdi+rdi]
        mov     eax, eax

But really we should just produce:
        lea     eax, [rdi + rdi]
        ret

Note I noticed a few other related addressing modes missing too.


---


### compiler : `gcc`
### title : `Missed fold for a/b*b`
### open_at : `2021-08-03T14:00:06Z`
### last_modified_date : `2021-12-16T00:19:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101754
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC is not optimizing the following (contrived) divisibility check https://godbolt.org/z/1rEhYa8Ts.

bool foo(int a, int b) {
    return a/b*b == a;
}
bool bar(int a, int b) {
    return a%b == 0;
}

If GCC can fold a/b*b into a-a%b, it will be able to optimize properly the rest of the way.


---


### compiler : `gcc`
### title : `Inconsistent optimizations with UBSan`
### open_at : `2021-08-03T17:03:45Z`
### last_modified_date : `2021-08-04T05:57:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101758
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Input:

gcc -fsanitize=undefined -O2 -fno-strict-aliasing -Wall

float b(unsigned a) { return *(float*)&a; }
float c(unsigned a) { return *(float*)&a; }
float d(unsigned a) { return *(float*)&a; }


Output:

b:
        mov     DWORD PTR [rsp-4], edi
        movss   xmm0, DWORD PTR [rsp-4]
        ret
c:
        movd    xmm0, edi
        ret
d:
        movd    xmm0, edi
        ret

or, on ARM

b:
        sub     sp, sp, #8
        str     r0, [sp, #4]
        vldr.32 s0, [sp, #4]
        add     sp, sp, #8
        bx      lr
c:
        sub     sp, sp, #8
        vmov    s0, r0
        add     sp, sp, #8
        bx      lr
d:
        sub     sp, sp, #8
        vmov    s0, r0
        add     sp, sp, #8
        bx      lr


Expected output:

Same assembly for all three. And maybe don't set up an unused stack frame on ARM (it's correctly optimized out without UBSan).


Compiler Explorer: https://godbolt.org/z/Tfs5qazqM


(Found it while trying to determine if that function is a strict aliasing violation (it is), and whether it can be fixed with memcpy or a union (both work).)


---


### compiler : `gcc`
### title : `loop->finite_p is not always true for some loops even with -ffinite-loops being used`
### open_at : `2021-08-03T22:14:08Z`
### last_modified_date : `2021-08-06T19:44:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101769
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take the following C++ testcase:
struct Node
{
	Node*	right;
	Node*	down;
};

inline
void free_node(Node*)
{
}

void free_all(Node* n_)
{
	if (n_ == nullptr) {
		return;
	}
	free_all(n_->right);
	do {
		Node* t = n_->down;
		free_node(n_);
		n_ = t;
	} while (n_);
}

void free_all2_r(Node* n_)
{
	if (n_->right) {
		free_all2_r(n_->right);
	}
	do {
		Node* t = n_->down;
		free_node(n_);
		n_ = t;
	} while (n_);
}

void free_all2(Node* n_)
{
	if (n_) {
		free_all2_r(n_);
	}
}

void loop(Node* n_)
{
	do {
		n_ = n_->down;
	} while (n_);
}

int main()
{
	return 0;
}
---- CUT -----
These all should be empty functions at -O2 -ffinite-loops but currently free_all, free_all2_r, and free_all2 produce empty loops around the element.

loop is able to do the right thing.
What seems to be happening is loop->finite_p being lost somewhere.


---


### compiler : `gcc`
### title : `P1143R2 constinit implementation is incomplete (joining with thread_local)`
### open_at : `2021-08-05T03:47:41Z`
### last_modified_date : `2021-08-11T20:02:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101786
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.0`
### severity : `minor`
### contents :
The paper says:

http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1143r2.html

> constinit can also be useful to compilers for non-initializing declarations
> of thread_local variables:
> 
> extern thread_local constinit x;
> int f() { return x; }
> 
> Without constinit, runtime code must be executed to perform a check of a
> guard variable and conditionally initialize x each time it is used. (Other
> techniques exist, but this approach is common.) If the variable is known to
> have constant initialization, this can be avoided.

Let's fix the missing type for x and try:

extern thread_local constinit int x;
int f() { return x; }

In case of compilation, GCC does not remove the TLS wrapper function as it should according to this paper:

_ZTW1x:
        push    rbp
        mov     rbp, rsp
        mov     eax, OFFSET FLAT:_ZTH1x
        test    rax, rax
        je      .L2
        call    _ZTH1x
.L2:
        mov     rdx, QWORD PTR fs:0
        mov     rax, QWORD PTR x@gottpoff[rip]
        add     rax, rdx
        pop     rbp
        ret
_Z1fv:
        push    rbp
        mov     rbp, rsp
        call    _ZTW1x
        mov     eax, DWORD PTR [rax]
        pop     rbp
        ret

The code it should produce should look like this: 

_Z1fv:
        push    rbp
        mov     rbp, rsp
        mov     rax, QWORD PTR x@gottpoff[rip]
        mov     eax, DWORD PTR fs:[rax]
        pop     rbp
        ret

What I can get now is only by replacing "thread_local constinit" with "__thread".

Clang implements this feature.


---


### compiler : `gcc`
### title : `Miss optimization to optimized (vashl op0, (op1: const_duplicate_vector)) to (ashl op0 op1_inner)`
### open_at : `2021-08-06T03:22:06Z`
### last_modified_date : `2021-12-15T08:27:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101796
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <immintrin.h>

__m512i
foo (__m512i a, short b)
{
  return _mm512_srlv_epi16 (a, _mm512_set1_epi16 (3));
}

llvm generate 

vpsrlw  zmm0, zmm0, 3

but gcc generate

foo(long long __vector(8), short):
        movl    $3, %eax
        vpbroadcastw    %eax, %zmm31
        vpsrlvw %zmm31, %zmm0, %zmm0
        ret


---


### compiler : `gcc`
### title : `vect_worthwhile_without_simd_p is broken`
### open_at : `2021-08-06T10:26:09Z`
### last_modified_date : `2023-02-10T10:00:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101801
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
vect_worthwhile_without_simd_p is currently

bool
vect_worthwhile_without_simd_p (vec_info *vinfo, tree_code code)
{
  loop_vec_info loop_vinfo = dyn_cast <loop_vec_info> (vinfo);
  unsigned HOST_WIDE_INT value;
  return (loop_vinfo
          && LOOP_VINFO_VECT_FACTOR (loop_vinfo).is_constant (&value)
          && value >= vect_min_worthwhile_factor (code));
}

which means it's never worthwhile to BB vectorize.  Also the VF check
doesn't honor SLP so that a fully SLPed loop with VF == 1 is never
considered worthwhile to vectorize.

I ran into this beast when looking at vectorization of mask condition
operations like cond_mask1 & cond_mask2 which, for AVX512, have
integer mode but vectorizable_operation does

  /* Worthwhile without SIMD support?  Check only during analysis.  */
  if (!VECTOR_MODE_P (vec_mode)
      && !vec_stmt
      && !vect_worthwhile_without_simd_p (vinfo, code))
    {
      if (dump_enabled_p ())
        dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,
                         "not worthwhile without SIMD support.\n");
      return false;
    }

and in my case with SLP the VF was indeed one and vectorization failed.
I think the code should not look at the vectorization factor but instead
at the vector type (and its number of components).


---


### compiler : `gcc`
### title : `Vectorization can end up creating vector bool CTORs`
### open_at : `2021-08-06T10:31:14Z`
### last_modified_date : `2021-08-06T10:32:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101802
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
When SLP vectorization creates code for external bool defs that are used in
condition composition we can end up combining them with vector booleans
and thus push types like vector(16) <signed-boolean:1> on them.

vect_create_constant_vectors then converts the components to signed-boolean:1
using _3 = _2 ? -1 : 0 and builds a CTOR with signed-boolean:1 components.
It's probably better to code-generate the "conversion" to vector bool by
using a CTOR with the original bools and then producing the vector bool mask
by a comparison against zero (if supported?).


---


### compiler : `gcc`
### title : `Max<bool0,bool1> -> bool0 | bool1 Min<bool0, bool1> -> a & b`
### open_at : `2021-08-06T20:11:07Z`
### last_modified_date : `2023-10-25T22:45:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101805
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int maxbool(bool ab, bool bb)
{
  int a = ab;
  int b = bb;
  int c;
  c = (a > b)?a : b;
  return c;
}
int minbool(bool ab, bool bb)
{
  int a = ab;
  int b = bb;
  int c;
  c = (a < b)?a : b;
  return c;
}
---- CUT ----
These two should be optimized to just:
int maxbool_or(bool ab, bool bb)
{
  int c = ab | bb;
  return c;
}
int minbool_and(bool ab, bool bb)
{
  int c = ab & bb;
  return c;
}
------ CUT----

GCC, ICC, clang nor MSVC do this optimization.


---


### compiler : `gcc`
### title : `Extra zero extends for some arguments in some cases`
### open_at : `2021-08-06T21:10:50Z`
### last_modified_date : `2023-05-19T17:21:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101806
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool g(bool a, bool b)
{
  return ~a & b;
}
---- CUT ---
Currently we produce:
        and     w1, w1, 255
        and     w0, w0, 255
        bic     w0, w1, w0
        and     w0, w0, 1

---- CUT ---
But we should produce:
        bic     w0, w1, w0
        and     w0, w0, 1

The zero extends are not needed.
This happens because combine does the correct thing until it tries to figure out the cutting point:Trying 2, 8 -> 16:
    2: r98:SI=zero_extend(x0:QI)
      REG_DEAD x0:QI
    8: r102:SI=~r98:SI&r99:SI
      REG_DEAD r98:SI
      REG_DEAD r99:SI
   16: x0:SI=r102:SI&0x1
      REG_DEAD r102:SI
Failed to match this instruction:
(set (reg:SI 0 x0)
    (and:SI (and:SI (not:SI (reg:SI 0 x0 [ a ]))
            (reg/v:SI 99 [ b ]))
        (const_int 1 [0x1])))
Successfully matched this instruction:
(set (reg:SI 102)
    (not:SI (reg:SI 0 x0 [ a ])))
Failed to match this instruction:
(set (reg:SI 0 x0)
    (and:SI (and:SI (reg:SI 102)
            (reg/v:SI 99 [ b ]))
        (const_int 1 [0x1])))

If we had chose (and:SI (not:SI (reg:SI 0 x0 [ a ])) (reg/v:SI 99 [ b ])) instead, we would have gotten the correct thing.


---


### compiler : `gcc`
### title : `bool0 < bool1 Should expand as !bool0 &bool1 and bool0 <= bool1 as !bool0 | bool1`
### open_at : `2021-08-06T21:22:19Z`
### last_modified_date : `2023-08-02T13:21:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101807
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f(bool a, bool b)
{
    return a<b;
}
bool g(bool a, bool b)
{
  return !a & b;
}
These two should produce similar code.
<= is similar.


---


### compiler : `gcc`
### title : `comparison0 < comparison1 should be transformed into comparison0` & comparison1; likewise for <=`
### open_at : `2021-08-06T21:27:26Z`
### last_modified_date : `2023-08-28T21:33:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101808
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:

bool f(int ai, int bi)
{
    bool a = ai, b = bi;
    return a<b;
}
bool g(int ai, int bi)
{
    bool a = ai, b = bi;
  return !a & b;
}

bool fe(int ai, int bi)
{
    bool a = ai, b = bi;
    return a<=b;
}
bool ge(int ai, int bi)
{
    bool a = ai, b = bi;
  return !a | b;
}
---- CUT----
f and g should produce the same code fe and ge should too.

So for aarch64, we should get cmp followed by a ccmp for all 4 functions.


---


### compiler : `gcc`
### title : `unsigned + -1u > 3 is not optimzied to unsigned > 4 if unsigned range is known not to include 0`
### open_at : `2021-08-07T22:33:47Z`
### last_modified_date : `2021-08-07T22:38:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101815
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(unsigned int a)
{
  if (a == 0) return 0;
  unsigned int t = a;
  a += 4294967295;
  return a > 3u;
}
int g(unsigned int a)
{
  if (a == 0) __builtin_unreachable();
  unsigned int t = a;
  a += -1;
  return a > 3u;
}

Both of these should produce:
  _7 = a_3(D) > 4;
  # RANGE [0, 1] NONZERO 1
  _4 = (intD.6) _7;

But only f does.
f is able to be optimized in reassoc1.


---


### compiler : `gcc`
### title : `slight missed optimization with __builtin_add_overflow with ^1`
### open_at : `2021-08-08T02:29:47Z`
### last_modified_date : `2023-06-02T14:23:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101816
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
void f(unsigned long long *x, unsigned long long y, unsigned long long z)
{
  int d = __builtin_add_overflow (*x,y,x);
  x[1] += z + !d;
}
Right now we produce:
        xorl    %eax, %eax
        addq    (%rdi), %rsi
        setc    %al
        movq    %rsi, (%rdi)
        xorq    $1, %rax
        addq    %rdx, %rax
        addq    %rax, 8(%rdi)
        ret

But the setc and xor could be combined into:
setnc   %al


---


### compiler : `gcc`
### title : `Codegen bug for popcount`
### open_at : `2021-08-08T22:59:19Z`
### last_modified_date : `2023-10-24T11:20:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101822
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC cleverly optimizes the following loop into a popcount intrinsic:

uint32_t foo(uint32_t n) {
    uint32_t count = 0;
    while(n) {
        n &= n - 1;
        count++;
    }
    return count;
}

But the generated assembly is highly redundant https://godbolt.org/z/nbGb13G5W:

foo(unsigned int):
        xor     eax, eax
        xor     edx, edx
        popcnt  eax, edi
        test    edi, edi
        cmove   eax, edx
        ret

if(n == 0) __builtin_unreachable(); does seem to help the compiler's analysis.

It seems here the compiler is not realizing both the loop and popcnt intrinsic are well-defined for n == 0. This is closely related to another bug: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101821.


---


### compiler : `gcc`
### title : `Vectorizer doesn't vectorize when loop bound depends on two independent variables that are unknown`
### open_at : `2021-08-10T09:41:11Z`
### last_modified_date : `2021-08-10T11:16:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101842
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The following example

float f(float *p, float d, int len, float lim)
{
  float m[4];
  for (int i = 0; i < len && d >= lim; i += 4)
  {
    m[0] = p[0] * p[0];
    m[1] = p[1] * p[1];
    m[2] = p[2] * p[2];
    m[3] = p[3] * p[3];
    d = d - m[0];
    d = d - m[1];
    d = d - m[2];
    d = d - m[3];
    p += 4;
  }

  return d;
}

isn't vectorized at -Ofast because

```
missed: not vectorized: number of iterations cannot be computed.
```

which seems odd because I would expect that it would be treated as just any other loop with unbounded iterations.  Commenting out this check results in it bailing out because of it not knowing how to deal with the reduction.

This loop should be easy to vectorize with vectorizing the multiplications of m and then reducing the changes of `d - sum (m[0..3])`.


---


### compiler : `gcc`
### title : `Improve __builtin_shufflevector emitted code`
### open_at : `2021-08-10T13:14:55Z`
### last_modified_date : `2021-12-15T11:26:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101846
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
typedef short v16hi __attribute__((vector_size (32)));
typedef short v32hi __attribute__((vector_size (64)));

v32hi
foo (v16hi x)
{
  return __builtin_shufflevector (x, (v16hi) {}, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23,
						 8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
}

v16hi
bar (v32hi x)
{
  return __builtin_shufflevector (x, x, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
}

shows two cases, where we should be emitting just
vpmovzxwd       %ymm0, %zmm0
and
vpmovdw %zmm0, %ymm0
but we actually emit
        vmovdqa %ymm0, %ymm0
        vpmovzxwd       %ymm0, %zmm0
where the vmovdqa is unnecessary - the permutation doesn't care about the elements at or above 32-bytes - and
        vmovdqa64       %zmm0, %zmm1
        vmovdqa64       .LC0(%rip), %zmm0
        vpermi2w        %zmm1, %zmm1, %zmm0
Similarly for permutations matching other vpmovxz* or vpmov* instructions.


---


### compiler : `gcc`
### title : `match_arith_overflow checks only mulv4_optab/umulv4_optab  tables when smul_highpart_optab/umul_highpart_optab can produce decent code too`
### open_at : `2021-08-11T07:22:15Z`
### last_modified_date : `2023-05-19T11:00:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101856
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
While looking at 30314, I Noticed aarch64 was not producing MUL_OVERFLOW for that test. I tested __builtin_umulll_overflow manually and got:
        umulh   x8, x1, x0
        cbz     x8, 

Which is what I had expected.

Original testcase:
typedef __SIZE_TYPE__ size_t;
extern void *malloc(size_t), abort(void);
void *allocate(size_t num, size_t size) {
    size_t total = num * size;

    if (total / size != num) abort();
    /* call malloc, whatever */
    return 0;
}

The code does:

      || (code == MULT_EXPR
	  && optab_handler (cast_stmt ? mulv4_optab : umulv4_optab,
			    TYPE_MODE (type)) == CODE_FOR_nothing))

Oh it seems like there could be caching of reading optab_handler in match_arith_overflow too.


---


### compiler : `gcc`
### title : `[11 Regression] SLP Vectorizer change pushes VEC_PERM_EXPR into bad location spoiling further optimization opportunities`
### open_at : `2021-08-13T05:01:45Z`
### last_modified_date : `2023-05-29T10:05:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101895
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Consider this code:


void foo(int * restrict a, int b, int *c) {
  a[0] = c[0]*b + a[0];
  a[1] = c[2]*b + a[1];
  a[2] = c[1]*b + a[2];
  a[3] = c[3]*b + a[3];
}

Prior to this commit:

commit 126ed72b9f48f8530b194532cc281fb761690435
Author: Richard Biener <rguenther@suse.de>
Date:   Wed Sep 30 17:08:01 2020 +0200

    optimize permutes in SLP, remove vect_attempt_slp_rearrange_stmts

    This introduces a permute optimization phase for SLP which is
    intended to cover the existing permute eliding for SLP reductions
    plus handling commonizing the easy cases.

    It currently uses graphds to compute a postorder on the reverse
    SLP graph and it handles all cases vect_attempt_slp_rearrange_stmts
    did (hopefully - I've adjusted most testcases that triggered it
    a few days ago).  It restricts itself to move around bijective
    permutations to simplify things for now, mainly around constant nodes.

    As a prerequesite it makes the SLP graph cyclic (ugh).  It looks
    like it would pay off to compute a PRE/POST order visit array
    once and elide all the recursive SLP graph walks and their
    visited hash-set.  At least for the time where we do not change
    the SLP graph during such walk.

    I do not like using graphds too much but at least I don't have to
    re-implement yet another RPO walk, so maybe it isn't too bad.

    It now computes permute placement during iteration and thus should
    get cycles more obviously correct.

[ ... ]

GCC would generate this (x86_64 -O3 -march=native):

  vect__1.6_27 = VEC_PERM_EXPR <vect__1.5_25, vect__1.5_25, { 0, 2, 1, 3 }>;
  vect__2.7_29 = vect__1.6_27 * _28;
  _1 = *c_18(D);
  _2 = _1 * b_19(D);
  vectp.9_30 = a_20(D);
  vect__3.10_31 = MEM <vector(4) int> [(int *)vectp.9_30];
  vect__4.11_32 = vect__2.7_29 + vect__3.10_31;



This is good.  Note how the VEC_PERM_EXPR happens before the vector multiply and how the vector multiply directly feeds the vector add.  On our target we have a vector multiply-add which would be generated and all is good.


After the above change we generate this:

  vect__2.6_28 = vect__1.5_25 * _27;
  _29 = VEC_PERM_EXPR <vect__2.6_28, vect__2.6_28, { 0, 2, 1, 3 }>;
  _1 = *c_18(D);
  _2 = _1 * b_19(D);
  vectp.8_30 = a_20(D);
  vect__3.9_31 = MEM <vector(4) int> [(int *)vectp.8_30];
  vect__4.10_32 = _29 + vect__3.9_31;


Note how we have the vmul, then permute, then vadd.  This spoils our ability to generate a vmadd.  This behavior is still seen on the trunk as well.

Conceptually it seems to me that having a permute at the start or end of a chain of vector operations is better than moving the permute into the middle of a chain of dependent vector operations.

We could probably fix this in the backend with some special patterns, but ISTM that getting it right in SLP would be better.


---


### compiler : `gcc`
### title : `[12 regression] cray regression with -O2 -ftree-slp-vectorize compared to -O2`
### open_at : `2021-08-14T14:27:34Z`
### last_modified_date : `2022-04-05T06:24:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101908
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
This should be easy to track since cray is stupid benchmark.

According to run
https://lnt.opensuse.org/db_default/v4/CPP/latest_runs_report?younger_in_days=14&older_in_days=0&all_changes=on&min_percentage_change=0.001&revisions=e54acea9e5a821448af97c66e94a1e4c4f977d5d%2Ce87209a1269622017bf3d98bf71502dcb0f893aa%2C73474527aaa24d9236aca074c5494a07f40ce058&include_user_branches=on

slp-vectorize seems to cause 101%-124% regression on zen and 96% on kabylake.ve


---


### compiler : `gcc`
### title : `[12/13/14 Regression] 73% regression on tfft benchmark for -O2 -ftree-loop-vectorize compared to -O2 on zen hardware`
### open_at : `2021-08-14T14:29:52Z`
### last_modified_date : `2023-05-08T12:22:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101909
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
https://lnt.opensuse.org/db_default/v4/CPP/latest_runs_report?younger_in_days=14&older_in_days=0&all_changes=on&min_percentage_change=0.001&revisions=e54acea9e5a821448af97c66e94a1e4c4f977d5d%2Ce87209a1269622017bf3d98bf71502dcb0f893aa%2C73474527aaa24d9236aca074c5494a07f40ce058&include_user_branches=on

This regression seems to be limited to zenith tester and does not reproduce on benzen


---


### compiler : `gcc`
### title : `[12 Regression] tsvc regressions for -O2 -ftree-loop-vectorize at zen hardware`
### open_at : `2021-08-14T14:33:14Z`
### last_modified_date : `2022-03-08T15:06:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101910
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
https://lnt.opensuse.org/db_default/v4/CPP/latest_runs_report?younger_in_days=14&older_in_days=0&all_changes=on&min_percentage_change=0.001&revisions=e54acea9e5a821448af97c66e94a1e4c4f977d5d%2Ce87209a1269622017bf3d98bf71502dcb0f893aa%2C73474527aaa24d9236aca074c5494a07f40ce058&include_user_branches=on

shows large regressions on tsvc for benchmarks s253, s271, s318, s3110, s281, s292, s443

Maybe generic model needs retuning here.


---


### compiler : `gcc`
### title : `std::function's move ctor is slower than the copy one for empty source objects`
### open_at : `2021-08-15T13:17:25Z`
### last_modified_date : `2022-12-29T22:14:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101923
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `normal`
### contents :
std::function's move constructor calls swap() irrespective of whether the source object is empty or not. In contrast, the copy constructor first checks if the source object is empty and if it is, nothing is being done as the `this` object is constructed in an empty state by _Function_base().

Calling swap() on an empty source requires more work, because some data needs to be copied - for example, the POD data cannot be moved.

Could the move constructor check if the source is empty too, as the copy one does? Please let me know if I am missing a rule that prevents that.

I have noticed that on version 9.3.0, but I see the code is the same in current master at:
https://gcc.gnu.org/git/?p=gcc.git;a=blob;f=libstdc%2B%2B-v3/include/bits/std_function.h;hb=c22bcfd2f7dc9bb5ad394720f4a612327dc898ba#l391

I have tested on a MacBook M1 and the copy ctor for empty sources is almost 2x faster than the move ctor:

-----------------------------------------------------
Benchmark           Time             CPU   Iterations
-----------------------------------------------------
copy            0.945 ns        0.945 ns    555789159
move             1.83 ns         1.83 ns    382183169

I have made an YouTube video for describing my findings and the benchmark results:
https://www.youtube.com/watch?v=WA3mKab-tn8


---


### compiler : `gcc`
### title : `[meta-bug] struct/complex/other argument passing and return should be improved`
### open_at : `2021-08-16T00:53:21Z`
### last_modified_date : `2023-08-28T12:37:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101926
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
There are many of these bugs, for x86_64, aarch64, powerpc, etc.


---


### compiler : `gcc`
### title : `There is no vector mode popcount for aarch64`
### open_at : `2021-08-16T05:02:09Z`
### last_modified_date : `2022-02-01T05:04:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101927
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:

#include <stdlib.h>
#include <stdint.h>

size_t hd (const uint8_t *restrict a, const uint8_t *restrict b, size_t l) {
  size_t r = 0, x;
  for (x = 0; x < l; x++)
    r += __builtin_popcount (a[x] ^ b[x]);

  return r;
}

at -O3 we don't vectorize this.
Clang/LLVM does:
.LBB0_5:                                // =>This Inner Loop Header: Depth=1
        ld1     { v3.b }[0], [x8]
        sub     x12, x8, #2
        ld1     { v5.b }[0], [x10]
        ld1     { v4.b }[0], [x12]
        sub     x12, x10, #2
        ld1     { v6.b }[0], [x12]
        add     x12, x8, #1
        ld1     { v3.b }[4], [x12]
        add     x12, x10, #1
        ld1     { v5.b }[4], [x12]
        sub     x12, x8, #1
        ld1     { v4.b }[4], [x12]
        sub     x12, x10, #1
        ld1     { v6.b }[4], [x12]
        eor     v3.8b, v5.8b, v3.8b
        ushll   v3.2d, v3.2s, #0
        and     v3.16b, v3.16b, v1.16b
        eor     v4.8b, v6.8b, v4.8b
        ushll   v4.2d, v4.2s, #0
        and     v4.16b, v4.16b, v1.16b
        cnt     v3.16b, v3.16b
        cnt     v4.16b, v4.16b
        uaddlp  v3.8h, v3.16b
        uaddlp  v4.8h, v4.16b
        uaddlp  v3.4s, v3.8h
        uaddlp  v4.4s, v4.8h
        add     x8, x8, #4
        subs    x11, x11, #4
        uadalp  v2.2d, v3.4s
        uadalp  v0.2d, v4.4s
        add     x10, x10, #4
        b.ne    .LBB0_5

------ CUT ----
Note I think we could be better.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] r12-7319 regress x264_r by 4% on CLX.`
### open_at : `2021-08-16T05:08:55Z`
### last_modified_date : `2023-04-17T13:14:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101929
### status : `REOPENED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
The regression is in x264_pixel_satd_8x4

typedef unsigned char uint8_t;
typedef unsigned int uint32_t;
typedef unsigned short uint16_t;

// in: a pseudo-simd number of the form x+(y<<16)
// return: abs(x)+(abs(y)<<16)
static inline
uint32_t abs2( uint32_t a )
{
    uint32_t s = ((a>>15)&0x10001)*0xffff;
    return (a+s)^s;
}

#define HADAMARD4(d0, d1, d2, d3, s0, s1, s2, s3) {\
    int t0 = s0 + s1;\
    int t1 = s0 - s1;\
    int t2 = s2 + s3;\
    int t3 = s2 - s3;\
    d0 = t0 + t2;\
    d2 = t0 - t2;\
    d1 = t1 + t3;\
    d3 = t1 - t3;\
}

int
x264_pixel_satd_8x4( uint8_t *pix1, int i_pix1, uint8_t *pix2, int i_pix2 )
{
    uint32_t tmp[4][4];
    uint32_t a0, a1, a2, a3;
    int sum = 0;
    for( int i = 0; i < 4; i++, pix1 += i_pix1, pix2 += i_pix2 )
    {
        a0 = (pix1[0] - pix2[0]) + ((pix1[4] - pix2[4]) << 16);
        a1 = (pix1[1] - pix2[1]) + ((pix1[5] - pix2[5]) << 16);
        a2 = (pix1[2] - pix2[2]) + ((pix1[6] - pix2[6]) << 16);
        a3 = (pix1[3] - pix2[3]) + ((pix1[7] - pix2[7]) << 16);
        HADAMARD4( tmp[i][0], tmp[i][1], tmp[i][2], tmp[i][3], a0,a1,a2,a3 );
    }
    for( int i = 0; i < 4; i++ )
    {
        HADAMARD4( a0, a1, a2, a3, tmp[0][i], tmp[1][i], tmp[2][i], tmp[3][i] );
        sum += abs2(a0) + abs2(a1) + abs2(a2) + abs2(a3);
    }
    return (((uint16_t)sum) + ((uint32_t)sum>>16)) >> 1;
}

after increase cost of vector CTOR, slp1 won't vector for below
git diff my.slp1 original.slp1

-  _820 = {_187, _189, _187, _189};
-  vect_t2_188.65_821 = VIEW_CONVERT_EXPR<vector(4) int>(_820);
-  vect__200.67_823 = vect_t0_184.64_819 - vect_t2_188.65_821;
-  vect__191.66_822 = vect_t0_184.64_819 + vect_t2_188.65_821;
-  _824 = VEC_PERM_EXPR <vect__191.66_822, vect__200.67_823, { 0, 1, 6, 7 }>;
-  vect__192.68_825 = VIEW_CONVERT_EXPR<vector(4) unsigned int>(_824);
   t3_190 = (int) _189;
   _191 = t0_184 + t2_188;
   _192 = (unsigned int) _191;
+  tmp[0][0] = _192;
   _194 = t0_184 - t2_188;
   _195 = (unsigned int) _194;
+  tmp[0][2] = _195;
   _197 = t1_186 + t3_190;
   _198 = (unsigned int) _197;
+  tmp[0][1] = _198;
   _200 = t1_186 - t3_190;
   _201 = (unsigned int) _200;
-  MEM <vector(4) unsigned int> [(unsigned int *)&tmp] = vect__192.68_825;
+  tmp[0][3] = _201;

but the vectorized version can somehow help fre to eliminate redundant vector load and then got even better performace.

git diff dump.veclower21 dump.fre5

   MEM <vector(4) unsigned int> [(unsigned int *)&tmp + 48B] = vect__54.89_852;
-  vect__63.9_482 = MEM <vector(4) unsigned int> [(unsigned int *)&tmp];
-  vect__64.12_478 = MEM <vector(4) unsigned int> [(unsigned int *)&tmp + 16B];
-  vect__65.13_477 = vect__63.9_482 + vect__64.12_478;
+  vect__65.13_477 = vect__192.68_825 + vect__273.75_834;
   vect_t0_100.14_476 = VIEW_CONVERT_EXPR<vector(4) int>(vect__65.13_477);
-  vect__67.15_475 = vect__63.9_482 - vect__64.12_478;
+  vect__67.15_475 = vect__192.68_825 - vect__273.75_834;
   vect_t1_101.16_474 = VIEW_CONVERT_EXPR<vector(4) int>(vect__67.15_475);
-  vect__68.19_470 = MEM <vector(4) unsigned int> [(unsigned int *)&tmp + 32B];
-  vect__69.22_466 = MEM <vector(4) unsigned int> [(unsigned int *)&tmp + 48B];
-  vect__70.23_465 = vect__68.19_470 + vect__69.22_466;
+  vect__70.23_465 = vect__354.82_843 + vect__54.89_852;

If slp1 can realize this and add the upper part to comparison of scalar cost vs vector cost, gcc should do vectorization, but currently it doesn't.


---


### compiler : `gcc`
### title : `Vectorizer failed due to argument types differ for ldexp`
### open_at : `2021-08-16T09:56:52Z`
### last_modified_date : `2021-08-29T08:35:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101932
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
This is from PR98309

--- Comment #2 from Hongtao.liu <crazylht at gmail dot com> ---
Although avx512 have vscalefps/d, vectorizer failed at
```
      /* We can only handle calls with arguments of the same type.  */
      if (rhs_type
          && !types_compatible_p (rhs_type, TREE_TYPE (op)))
        {
          if (dump_enabled_p ())
            dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,
                             "argument types differ.\n");
          return false;
        }
```

since __builtin_ldexp is defined as BT_FN_DOUBLE_DOUBLE_INT with 2 different
type arguments.


---


### compiler : `gcc`
### title : `[12 Regression] 538.imagick_r LTO -Ofast regression on Zen2 and Kabylake caused by r12-2666-g29f0e955c97`
### open_at : `2021-08-16T12:42:33Z`
### last_modified_date : `2021-09-15T15:57:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101935
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
When compiled with -flto -Ofast -march=native, the benchmark
538.imagick_r is about 28% slower on AMD zen2 machines (see
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=287.507.0 )
and about 18% slower on Intel Kabylake (see
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=11.507.0 )

I have independently reproduced the regression on a zen2-based desktop
and bisected it down to commit r12-2666-g29f0e955c97 (x86: Update
piecewise move and store).


---


### compiler : `gcc`
### title : `[12 Regression] 538.imagick_r -Ofast regression on Kabylake since g:872da9a6f664a06d73c987aa0cb2e5b830158a10`
### open_at : `2021-08-16T12:48:48Z`
### last_modified_date : `2021-09-15T15:58:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101936
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
When compiling with -Ofast -march=native, the benchmark is ~27% slower on -march=skylake machine:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=32.507.0

I'm bisecting that right now on a Znver1 machine with -march=skylake set.


---


### compiler : `gcc`
### title : `Register spilling when zero initialization of ymm array is defered past the declaration point.`
### open_at : `2021-08-16T19:48:21Z`
### last_modified_date : `2021-08-16T20:13:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101939
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `11.2.0`
### severity : `normal`
### contents :
using

    __m256i r[8] = {_mm256_setzero_si256()};

as opposed to

    __m256i r[8];
    for (int i = 0; i < 8; ++i) r[i] = _mm256_setzero_si256();

before a hot loop results in MUCH better codegen. See godbolt below.

https://godbolt.org/z/sbscKzso3


---


### compiler : `gcc`
### title : `suboptimal SLP for reduced case from namd_r`
### open_at : `2021-08-17T09:26:48Z`
### last_modified_date : `2022-03-08T08:00:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101944
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
For SPEC2017 bmk 508.namd_r, it's observed that it degraded by -3.73% 
at -O2 -ftree-slp-vectorize vs baseline -O2 on Power9 with either default cost model or very cheap cost model. By isolating functions, several functions are responsible for it.  One typical case is the below reduced one:

------------- TEST CASE 

typedef double BigReal;

extern BigReal table_four_i[8];
extern void func1(BigReal *);
extern void func2(BigReal *);
extern void func3(BigReal, BigReal, BigReal, BigReal);

void foo(BigReal scaling, BigReal *A1, BigReal *B1, BigReal diffa) {

  BigReal vdwEnergy = 0;
  func1(&vdwEnergy);

  const BigReal A = scaling * *A1;
  const BigReal B = scaling * *B1;

  BigReal vdw_d = A * table_four_i[0] - B * table_four_i[2];
  BigReal vdw_c = A * table_four_i[1] - B * table_four_i[3];
  BigReal vdw_b = A * table_four_i[4] - B * table_four_i[6];
  BigReal vdw_a = A * table_four_i[5] - B * table_four_i[7];

  register BigReal vdw_val =
      ((diffa * vdw_d * (1 / 6.) + vdw_c * (1 / 4.)) * diffa +
       vdw_b * (1 / 2.)) * diffa + vdw_a;
  vdwEnergy -= vdw_val;

  func2 (&vdwEnergy);
  func3 (vdw_a, vdw_b, vdw_c, vdw_d);
}

-------------

Options: -O2 -ffast-math -ftree-slp-vectorize -mcpu=power9

Scalar version at optimized dumping:

  func1 (&vdwEnergy);
  _1 = *A1_32(D);
  A_34 = _1 * scaling_33(D);
  _2 = *B1_35(D);
  B_36 = _2 * scaling_33(D);
  _3 = table_four_i[0];
  _5 = table_four_i[2];
  _6 = _5 * B_36;
  vdw_d_37 = .FMS (_3, A_34, _6);
  _7 = table_four_i[1];
  _9 = table_four_i[3];
  _10 = _9 * B_36;
  vdw_c_38 = .FMS (_7, A_34, _10);
  _11 = table_four_i[4];
  _13 = table_four_i[6];
  _14 = _13 * B_36;
  vdw_b_39 = .FMS (_11, A_34, _14);
  _15 = table_four_i[5];
  _17 = table_four_i[7];
  _18 = _17 * B_36;
  vdw_a_40 = .FMS (_15, A_34, _18);
  _51 = diffa_41(D) * 1.666666666666666574148081281236954964697360992431640625e-1;
  _21 = vdw_c_38 * 2.5e-1;
  _22 = .FMA (vdw_d_37, _51, _21);
  _24 = vdw_b_39 * 5.0e-1;
  _25 = .FMA (_22, diffa_41(D), _24);
  _26 = _25 * diffa_41(D);
  vdwEnergy.0_27 = vdwEnergy;
  _49 = _18 + vdwEnergy.0_27;
  _52 = .FMA (_15, A_34, _26);
  _28 = _49 - _52;
  vdwEnergy = _28;
  func2 (&vdwEnergy);


Vector version at optimized dumping:

  func1 (&vdwEnergy);
  _1 = *A1_32(D);
  A_34 = _1 * scaling_33(D);
  _49 = {A_34, A_34};
  _2 = *B1_35(D);
  B_36 = _2 * scaling_33(D);
  _54 = {B_36, B_36};
  vect__3.6_48 = MEM <vector(2) double> [(double *)&table_four_i];
  vect__5.10_52 = MEM <vector(2) double> [(double *)&table_four_i + 16B];
  vect__6.11_55 = vect__5.10_52 * _54;
  vect_vdw_d_37.12_56 = .FMS (vect__3.6_48, _49, vect__6.11_55);
  _58 = BIT_FIELD_REF <vect_vdw_d_37.12_56, 64, 64>;
  _57 = BIT_FIELD_REF <vect_vdw_d_37.12_56, 64, 0>;
  _11 = table_four_i[4];
  _13 = table_four_i[6];
  _14 = _13 * B_36;
  vdw_b_39 = .FMS (_11, A_34, _14);
  _15 = table_four_i[5];
  _17 = table_four_i[7];
  _18 = _17 * B_36;
  vdw_a_40 = .FMS (_15, A_34, _18);
  _51 = diffa_41(D) * 1.666666666666666574148081281236954964697360992431640625e-1;
  _59 = {_51, 2.5e-1};
  vect__20.13_60 = vect_vdw_d_37.12_56 * _59;
  _61 = .REDUC_PLUS (vect__20.13_60);
  _24 = vdw_b_39 * 5.0e-1;
  _25 = .FMA (diffa_41(D), _61, _24);
  _26 = _25 * diffa_41(D);
  vdwEnergy.0_27 = vdwEnergy;
  _66 = _18 + vdwEnergy.0_27;
  _68 = .FMA (_15, A_34, _26);
  _28 = _66 - _68;
  vdwEnergy = _28;
  func2 (&vdwEnergy);

reduc.c:24:34: note: Cost model analysis for part in loop 0:
  Vector cost: 16
  Scalar cost: 17


---


### compiler : `gcc`
### title : `__builtin_clrsb is never inlined`
### open_at : `2021-08-17T16:13:21Z`
### last_modified_date : `2021-08-25T02:45:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101950
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.1.0`
### severity : `normal`
### contents :
With gcc 11.1 on ARM 32-bit and Intel, I don't see that __builtin_clrsb is inlined. On AARCH64 it is inlined and the cls instruction is used, as expected. I use the C-code below to compare the assembly generated. For ARM, I use -O3 -mcpu=cortex-a53 -marm and for Intel I just use -O3.


On ARM 32-bit, clrsb1 seems to be the fastest code (see below for the assembly code) since clz handles zero correctly. On Intel, bsr does not handle zero, hence the workaround of setting the lsb before calling __builtin_clzl (see below for the assembly code). On Intel, clrsb1 is slighly longer and uses a jump to handle the zero case. clang apparently uses variant clrsb1 on ARM and Intel, and it's inlined on both architectures when using -O3.





#define SHIFT (sizeof(x)*8-1)

int clz(unsigned long x) {
    if (x == 0) {
        return sizeof(x)*8;
    }
    return __builtin_clzl(x);
}

int clsb(long x) {
    return clz(x ^ (x >> SHIFT));
}

int clrsb1(long x) {
    return clsb(x)-1;
}

int clrsb2(long x) {
    x = ((x << 1) ^ (x >> SHIFT)) | 1;
    return __builtin_clzl(x);
}

int clrsb3(long x) {
    return __builtin_clrsbl(x);
}



on ARM 32-bit:
clrsb1:
        eor     x0, x0, x0, asr 63
        clz     x0, x0
        sub     w0, w0, #1
        ret

on Intel:
clrsb2:
        lea     rax, [rdi+rdi]
        sar     rdi, 63
        xor     rax, rdi
        or      rax, 1
        bsr     rax, rax
        xor     eax, 63
        ret


---


### compiler : `gcc`
### title : `(signed<<31)>>31 should become -(signed&1)`
### open_at : `2021-08-18T05:47:59Z`
### last_modified_date : `2023-10-20T23:08:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101955
### status : `NEW`
### tags : `easyhack, missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(int b)
{
    return (((b)<<31)>>31);
}
int f1(int b)
{
    return -(b&1);
}
These two should produce the same code.


---


### compiler : `gcc`
### title : `Miss vectorization from v4hi to v4df`
### open_at : `2021-08-18T06:00:02Z`
### last_modified_date : `2022-03-07T21:42:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101956
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
void
foo (double* p, short* q)
{
   p[0] = q[0];
   p[1] = q[1];
   p[2] = q[2];
   p[3] = q[3];
}

clang generates 

        vpmovsxwd       xmm0, qword ptr [rsi]
        vcvtdq2pd       ymm0, xmm0
        vmovups ymmword ptr [rdi], ymm0
        vzeroupper

gcc generates

foo(double*, short*):
  movswl (%rsi), %eax
  vxorps %xmm0, %xmm0, %xmm0
  vcvtsi2sdl %eax, %xmm0, %xmm1
  movswl 2(%rsi), %eax
  vcvtsi2sdl %eax, %xmm0, %xmm2
  movswl 4(%rsi), %eax
  vmovsd %xmm2, %xmm2, %xmm3
  vcvtsi2sdl %eax, %xmm0, %xmm2
  movswl 6(%rsi), %eax
  vcvtsi2sdl %eax, %xmm0, %xmm0
  vunpcklpd %xmm0, %xmm2, %xmm2
  vunpcklpd %xmm3, %xmm1, %xmm0
  vinsertf128 $0x1, %xmm2, %ymm0, %ymm0
  vmovupd %ymm0, (%rdi)
  vzeroupper
  ret


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] compiling with "-O2 -fselective-scheduling2 -fno-tree-ch" produce bad result`
### open_at : `2021-08-18T10:52:19Z`
### last_modified_date : `2023-07-07T10:40:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101958
### status : `NEW`
### tags : `missed-optimization, wrong-code`
### component : `rtl-optimization`
### version : `11.2.0`
### severity : `normal`
### contents :
gcc-11.2.0, x86_64-pc-linux-gnu, compiling with the following options:
"-O2 -fselective-scheduling2 -fno-tree-ch -lm"
The source code is for counting the number of primes up to 10000.

#include <stdio.h>
#include <math.h>

unsigned long  max_prime =10000;
int cpu_execute_event()
{
  unsigned long c,l;
  int n = 0;
  double t;

  for(c = 2; c < max_prime; c++)
  {
    t = sqrt((double)c);
    for(l = 2; l <= t; l++)
      if (c % l == 0)
        break;
    if (l > t )
      n++;
  }
  return n;                                                                                                                                                                                 
}

int main ()
{
	int n;
	n = cpu_execute_event();
        printf("n:%d\n",n);
	return 0;
}

The real return value of cpu_execute_event() is 2 where it should be 1229.
It works correctly when I compiled with "-O2 -fselective-scheduling2" or "-O2 -fno-tree-ch".I suspect that instruction scheduling changes the meaning of the code.And gcc10.2 also produces the same phenomenon.

The command line is the following:
gcc-11.2 cpu_execute_event.c -O2 -fselective-scheduling2 -fno-tree-ch -lm

There is no output message, no error, no warning even compiled with the options "-Wall -Wextra".

I use gcc 11.2.0, configured with:
../gcc-11.2.0/configure --prefix=/home/chann/gcc/gcc11_2 --enable-threads=posix --disable-checking --disable-multilib --enable-languages=c,c++


---


### compiler : `gcc`
### title : `GCC10 produces bigger asm for simple switch than GCC7 - cortexM4 since r8-2701-g9dc3d6a96167b4c8`
### open_at : `2021-08-19T14:55:28Z`
### last_modified_date : `2021-11-04T12:53:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101981
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.3.1`
### severity : `normal`
### contents :
For cortex-m4 -Os, GCC10 produces bigger assembly code than GCC7 for very simple switch statements.

Here is the C code example to trigger the regression:

*file: switch.c*
```C
int switchFunction(int foo) {
  switch (foo) {
  case 0:
    return 0;
  case 1:
    return 1;
  default:
    return -1;
  }
}

int main() {
  return 0;
}
```
To reproduce, I downloaded the toolchain from here https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm/downloads
 
Compile command: `arm-none-eabi-gcc -Os -mcpu=cortex-m4 ./switch.c`

For GCC7 (arm-none-eabi-gcc (GNU Tools for Arm Embedded Processors 7-2018-q2-update) 7.3.1 20180622 (release) [ARM/embedded-7-branch revision 261907]) it produces this assembly code:
```asm
000080f8 <switchFunction>:
    80f8:       2801            cmp     r0, #1
    80fa:       bf88            it      hi
    80fc:       f04f 30ff       movhi.w r0, #4294967295 ; 0xffffffff
    8100:       4770            bx      lr
```

While GCC10 (arm-none-eabi-gcc (GNU Arm Embedded Toolchain 10.3-2021.07) 10.3.1 20210621 (release)) produces one more line (25% bigger):
```asm
00008100 <switchFunction>:
    8100:       b118            cbz     r0, 810a <switchFunction+0xa>
    8102:       2801            cmp     r0, #1
    8104:       bf18            it      ne
    8106:       f04f 30ff       movne.w r0, #4294967295 ; 0xffffffff
    810a:       4770            bx      lr
```

Note: GCC10 produces the same code as if it was a simple `if`:
```C
int ifFunction(int foo) {
  if (foo == 0) {
    return 0;
  } else if (foo == 1) {
    return 1;
  } else {
    return -1;
  }
}
```

Shouldn't GCC10 produces the same code as GCC7?


---


### compiler : `gcc`
### title : `Fail to optimize (a & b) | (c & ~b) to vpternlog instruction.`
### open_at : `2021-08-20T01:46:13Z`
### last_modified_date : `2022-09-25T19:35:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101989
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
cat test.c

__m128i foo (__m128i src1, __m128i src2, __m128i src3)
{
  return (src2 & ~src1) | (src3 & src1);
}

__m128i foo1 (__m128i src1, __m128i src2, __m128i src3)
{
  return (src2 & src1) | (src3 & ~src1);
}

gcc -O2 -march=cascadelake  generates

foo(long long __vector(2), long long __vector(2), long long __vector(2)):
        vpxor   %xmm2, %xmm1, %xmm2
        vpand   %xmm0, %xmm2, %xmm2
        vpxor   %xmm1, %xmm2, %xmm0
        ret
foo1(long long __vector(2), long long __vector(2), long long __vector(2)):
        vpxor   %xmm2, %xmm1, %xmm1
        vpand   %xmm0, %xmm1, %xmm1
        vpxor   %xmm2, %xmm1, %xmm0
        ret


icx generates

foo(long long __vector(2), long long __vector(2), long long __vector(2)):                        # 
        vpternlogq      xmm0, xmm2, xmm1, 202
        ret
foo1(long long __vector(2), long long __vector(2), long long __vector(2)):                       # 
        vpternlogq      xmm0, xmm1, xmm2, 202
        ret

Guess we need a post_reload splitter to match

Failed to match this instruction:
(set (reg:V2DI 88)
    (xor:V2DI (and:V2DI (xor:V2DI (reg:V2DI 92)
                (reg/v:V2DI 87 [ src3 ]))
            (reg:V2DI 91))
        (reg/v:V2DI 87 [ src3 ])))
Failed to match this instruction:


---


### compiler : `gcc`
### title : `bit_and or bit_ior with an invariant inside loop is not pulled out of the loop`
### open_at : `2021-08-20T02:35:48Z`
### last_modified_date : `2022-10-22T22:32:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101991
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take these two functions:
int f(int t, int d, int e)
{
  int r = d;
  for(int i = 0; i < t; i++)
    r &= e;
  return r;
}

int f1(int t, int d, int e)
{
  int r = d;
  if (0 < t)
    r &= e;
  return r;
}
They should produce the same code.  Right now f still has a loop in it.
The same is true with bit_ior.

Note the code is really bad when the vectorizer comes around really.
I forgot how I found this either.
Note clang/ICC also vectorize this code crappily.
clang on the trunk almost gets there but still has a loop:

        movl    %esi, %eax
        xorl    %esi, %esi
        testl   %edi, %edi
        cmovgl  %edi, %esi
        jle     .LBB0_6
# %bb.1:
        leal    -1(%rsi), %edi
        movl    %esi, %ecx
        andl    $7, %ecx
        cmpl    $7, %edi
        jb      .LBB0_4
# %bb.2:
        andl    $-8, %esi
        negl    %esi
        .p2align        4, 0x90
.LBB0_3:                                # =>This Inner Loop Header: Depth=1
        addl    $8, %esi
        jne     .LBB0_3
.LBB0_4:
        andl    %edx, %eax
        testl   %ecx, %ecx
        je      .LBB0_6
        .p2align        4, 0x90
.LBB0_5:                                # =>This Inner Loop Header: Depth=1
        addl    $-1, %ecx
        jne     .LBB0_5
.LBB0_6:
        retq


---


### compiler : `gcc`
### title : `Potential vectorization opportunity when condition checks array address`
### open_at : `2021-08-20T03:36:42Z`
### last_modified_date : `2021-08-24T23:22:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101993
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
For

float foo(int * restrict a, int * restrict res, int n)
{
  int i;
  for (i = 0; i < 8; i++)
  {
    if (a + i)
      res[i] = *(a + i) * 2;
  }
}

Compile with -O3

Clang generates

foo:                                    # @foo
        testq   %rdi, %rdi
        je      .LBB0_2
        movdqu  (%rdi), %xmm0
        paddd   %xmm0, %xmm0
        movdqu  %xmm0, (%rsi)
.LBB0_2:
        retq

While GCC generates

foo:
        testq   %rdi, %rdi
        je      .L5
        movl    (%rdi), %eax
        leaq    8(%rdi), %rdx
        addl    %eax, %eax
        movl    %eax, (%rsi)
        movl    4(%rdi), %eax
        addl    %eax, %eax
.L3:
        movl    %eax, 4(%rsi)
        movl    (%rdx), %eax
        addl    %eax, %eax
        movl    %eax, 8(%rsi)
        movl    12(%rdi), %eax
        addl    %eax, %eax
        movl    %eax, 12(%rsi)
        ret
.L5:
        movl    4, %eax
        movl    $8, %edx
        addl    %eax, %eax
        jmp     .L3

If a is 0 or negative then it should be an invalid pointer. It seems clang have such assumption and test a first then optimize loop body.

Is it possible for GCC to do such optimization?


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] regression built-in memset missed-optimization arm -Os since r9-3594`
### open_at : `2021-08-20T09:35:53Z`
### last_modified_date : `2023-07-07T10:40:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101995
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `10.3.1`
### severity : `normal`
### contents :
For cortex-m4 -Os, GCC10 produces bigger assembly code than GCC7 when memset is called.

Here is the C code example to trigger the regression:

```C
#include <stdio.h>
#include <string.h>

struct foo_t {
  int a;
  int b;
  int c;
  int d;
};

/* Random function modifying foo with another value than 0 */
void doStuff(struct foo_t *foo) {
  foo->b = foo->a + foo->c;
}

void twoLinesFunction(struct foo_t *foo) {
  /* R0 is saved in GCC10 but not in GCC7 */
  memset(foo, 0x00, sizeof(struct foo_t));
  doStuff(foo);
}

int main(void) {
  struct foo_t foo;
  twoLinesFunction(&foo);
  return 0;
}
```

compile command: `gcc -Os -mcpu=cortex-m4`

GCC7.3.1 produces:
```asm
<twoLinesFunction>:
    push    {r3, lr}
    movs    r2, #16
    movs    r1, #0
    bl      8168 <memset>
    ldmia.w sp!, {r3, lr}
    b.w     8104 <doStuff>
```

While GCC10.3.0 produces:
```asm
<twoLinesFunction>:
    push    {r4, lr}
    movs    r2, #16
    mov     r4, r0        --> backup r0
    movs    r1, #0
    bl      8174 <memset>
    mov     r0, r4        --> restore r0
    ldmia.w sp!, {r4, lr}
    b.w     810c <doStuff>
```

Main function remains the same.

The builtin memset function does not change R0 so there is no need to save it and restore it later. GCC7 is more efficient.
GCC10 should not backup R0 for this builtin function in this case, it produces slower code.

There is this PR https://gcc.gnu.org/bugzilla/show_bug.cgi?id=61241 which is also referring to this behavior with a patch to implement the optimization but I'm not sure when this optimization has been wiped out.


---


### compiler : `gcc`
### title : `[12 Regression] no cmov generated for loads next to each other`
### open_at : `2021-08-21T20:00:24Z`
### last_modified_date : `2022-03-16T13:01:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102008
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Take:
struct Foo {  int a;  int b; };

int test(int side, const Foo *foo) {
  if (side == 1) return foo->a;
  return foo->b;
}
----- CUT ----
Before r12-897, GCC was able to produce a cmov for this case but now we don't.
Note for aarch64 we produce now:
        cmp     w0, 1
        add     x0, x1, 4
        csel    x0, x0, x1, ne
        ldr     w0, [x0]
Which is actually reasonible still.

Note I noticed this while looking into PR 68274.


---


### compiler : `gcc`
### title : `Redudant mov instruction for broadcast.`
### open_at : `2021-08-23T09:22:47Z`
### last_modified_date : `2022-02-03T09:07:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102021
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
#include<immintrin.h>

__m256i
foo ()
{
  return _mm256_set1_epi16 (12);
}


foo():
        movabsq $3377751260921868, %rax
        vpbroadcastq    %rax, %ymm31
        vmovdqa64       %ymm31, %ymm0
        ret

I guess scratch sse register somehow prevent LRA to merge move instructions.

Maybe we should add define_peephole2 for those if we still want to use ix86_gen_scratch_sse_rtx.


---


### compiler : `gcc`
### title : `Unnecessary duplication of mtcrf instruction`
### open_at : `2021-08-23T11:27:45Z`
### last_modified_date : `2021-08-23T13:21:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102023
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Building Linux Kernel, mpc885_ads_defconfig (powerpc), several places we get multiple mtcrf whereas a single mtcrf instruction would be enough:

Exemple: arch/powerpc/perf/8xx-pmu.o, function mpc8xx_pmu_read()

At two places we get:
 258:	7d 82 01 20 	mtcrf   32,r12
 25c:	7d 81 01 20 	mtcrf   16,r12
 260:	7d 80 81 20 	mtcrf   8,r12

The above should be replaced by the following:

	mtcrf	56,r12

56 being the or-ing of 32|16|8

Dump of the complete function is below:

000000e0 <mpc8xx_pmu_read>:
  e0:	94 21 ff b0 	stwu    r1,-80(r1)
  e4:	7c 08 02 a6 	mflr    r0
  e8:	7d 80 00 26 	mfcr    r12
  ec:	93 21 00 34 	stw     r25,52(r1)
  f0:	90 01 00 54 	stw     r0,84(r1)
  f4:	91 81 00 14 	stw     r12,20(r1)
  f8:	7c 79 1b 78 	mr      r25,r3
  fc:	4b ff ff 05 	bl      0 <event_type>
 100:	2c 03 00 00 	cmpwi   r3,0
 104:	41 80 01 f4 	blt     2f8 <mpc8xx_pmu_read+0x218>
 108:	2c 03 00 04 	cmpwi   r3,4
 10c:	2e 03 00 03 	cmpwi   cr4,r3,3
 110:	2d 03 00 01 	cmpwi   cr2,r3,1
 114:	2d 83 00 02 	cmpwi   cr3,r3,2
 118:	93 81 00 40 	stw     r28,64(r1)
 11c:	93 a1 00 44 	stw     r29,68(r1)
 120:	92 41 00 18 	stw     r18,24(r1)
 124:	92 61 00 1c 	stw     r19,28(r1)
 128:	92 81 00 20 	stw     r20,32(r1)
 12c:	92 a1 00 24 	stw     r21,36(r1)
 130:	92 c1 00 28 	stw     r22,40(r1)
 134:	92 e1 00 2c 	stw     r23,44(r1)
 138:	93 01 00 30 	stw     r24,48(r1)
 13c:	93 41 00 38 	stw     r26,56(r1)
 140:	93 61 00 3c 	stw     r27,60(r1)
 144:	93 c1 00 48 	stw     r30,72(r1)
 148:	93 e1 00 4c 	stw     r31,76(r1)
 14c:	7e 80 00 26 	mfcr    r20
 150:	3b 79 01 88 	addi    r27,r25,392
 154:	3a 40 00 00 	li      r18,0
 158:	3a 60 00 00 	li      r19,0
 15c:	3b c0 00 00 	li      r30,0
 160:	3b e0 00 00 	li      r31,0
 164:	3e e0 00 00 	lis     r23,0
			166: R_PPC_ADDR16_HA	dtlb_miss_counter
 168:	3a c0 00 00 	li      r22,0
 16c:	3f 40 00 00 	lis     r26,0
			16e: R_PPC_ADDR16_HA	instruction_counter
 170:	3e a0 00 01 	lis     r21,1
 174:	3f 00 00 00 	lis     r24,0
			176: R_PPC_ADDR16_HA	itlb_miss_counter
 178:	7f 63 db 78 	mr      r3,r27
 17c:	48 00 00 01 	bl      17c <mpc8xx_pmu_read+0x9c>
			17c: R_PPC_REL24	generic_atomic64_read
 180:	7c 9c 23 78 	mr      r28,r4
 184:	7c 7d 1b 78 	mr      r29,r3
 188:	41 92 01 40 	beq     cr4,2c8 <mpc8xx_pmu_read+0x1e8>
 18c:	41 91 01 50 	bgt     cr4,2dc <mpc8xx_pmu_read+0x1fc>
 190:	41 8a 00 dc 	beq     cr2,26c <mpc8xx_pmu_read+0x18c>
 194:	39 3a 00 00 	addi    r9,r26,0
			196: R_PPC_ADDR16_LO	instruction_counter
 198:	40 8e 00 fc 	bne     cr3,294 <mpc8xx_pmu_read+0x1b4>
 19c:	81 49 00 00 	lwz     r10,0(r9)
 1a0:	7f f6 22 a6 	mfspr   r31,150
 1a4:	81 09 00 00 	lwz     r8,0(r9)
 1a8:	7c 0a 40 00 	cmpw    r10,r8
 1ac:	40 82 ff f0 	bne     19c <mpc8xx_pmu_read+0xbc>
 1b0:	57 ff 84 3e 	rlwinm  r31,r31,16,16,31
 1b4:	7d 49 fe 70 	srawi   r9,r10,31
 1b8:	55 47 84 3e 	rlwinm  r7,r10,16,16,31
 1bc:	51 5f 80 1e 	rlwimi  r31,r10,16,0,15
 1c0:	51 27 80 1e 	rlwimi  r7,r9,16,0,15
 1c4:	7e 7f e0 10 	subfc   r19,r31,r28
 1c8:	7e 47 e9 10 	subfe   r18,r7,r29
 1cc:	2c 12 00 00 	cmpwi   r18,0
 1d0:	7c fe 3b 78 	mr      r30,r7
 1d4:	40 a0 00 c0 	bge     294 <mpc8xx_pmu_read+0x1b4>
 1d8:	7e 73 b0 14 	addc    r19,r19,r22
 1dc:	7f c7 f3 78 	mr      r7,r30
 1e0:	7f e8 fb 78 	mr      r8,r31
 1e4:	7f a5 eb 78 	mr      r5,r29
 1e8:	7f 86 e3 78 	mr      r6,r28
 1ec:	7f 63 db 78 	mr      r3,r27
 1f0:	7e 52 a9 14 	adde    r18,r18,r21
 1f4:	48 00 00 01 	bl      1f4 <mpc8xx_pmu_read+0x114>
			1f4: R_PPC_REL24	generic_atomic64_cmpxchg
 1f8:	7c 1d 18 00 	cmpw    r29,r3
 1fc:	40 82 ff 7c 	bne     178 <mpc8xx_pmu_read+0x98>
 200:	7c 1c 20 40 	cmplw   r28,r4
 204:	40 82 ff 74 	bne     178 <mpc8xx_pmu_read+0x98>
 208:	81 81 00 14 	lwz     r12,20(r1)
 20c:	80 01 00 54 	lwz     r0,84(r1)
 210:	82 81 00 20 	lwz     r20,32(r1)
 214:	82 a1 00 24 	lwz     r21,36(r1)
 218:	82 c1 00 28 	lwz     r22,40(r1)
 21c:	82 e1 00 2c 	lwz     r23,44(r1)
 220:	83 01 00 30 	lwz     r24,48(r1)
 224:	83 41 00 38 	lwz     r26,56(r1)
 228:	83 61 00 3c 	lwz     r27,60(r1)
 22c:	83 81 00 40 	lwz     r28,64(r1)
 230:	83 a1 00 44 	lwz     r29,68(r1)
 234:	83 c1 00 48 	lwz     r30,72(r1)
 238:	83 e1 00 4c 	lwz     r31,76(r1)
 23c:	38 b9 00 68 	addi    r5,r25,104
 240:	7e 43 93 78 	mr      r3,r18
 244:	83 21 00 34 	lwz     r25,52(r1)
 248:	82 41 00 18 	lwz     r18,24(r1)
 24c:	7e 64 9b 78 	mr      r4,r19
 250:	82 61 00 1c 	lwz     r19,28(r1)
 254:	7c 08 03 a6 	mtlr    r0
 258:	7d 82 01 20 	mtcrf   32,r12
 25c:	7d 81 01 20 	mtcrf   16,r12
 260:	7d 80 81 20 	mtcrf   8,r12
 264:	38 21 00 50 	addi    r1,r1,80
 268:	48 00 00 00 	b       268 <mpc8xx_pmu_read+0x188>
			268: R_PPC_REL24	generic_atomic64_add
 26c:	7f cd 42 e6 	mftbu   r30
 270:	7f ec 42 e6 	mftb    r31
 274:	7d 2d 42 e6 	mftbu   r9
 278:	7c 1e 48 40 	cmplw   r30,r9
 27c:	40 82 ff f0 	bne     26c <mpc8xx_pmu_read+0x18c>
 280:	7e 7c f8 10 	subfc   r19,r28,r31
 284:	56 72 27 3e 	rlwinm  r18,r19,4,28,31
 288:	7d 3d f1 10 	subfe   r9,r29,r30
 28c:	51 32 20 36 	rlwimi  r18,r9,4,0,27
 290:	56 73 20 36 	rlwinm  r19,r19,4,0,27
 294:	7f c7 f3 78 	mr      r7,r30
 298:	7f e8 fb 78 	mr      r8,r31
 29c:	7f a5 eb 78 	mr      r5,r29
 2a0:	7f 86 e3 78 	mr      r6,r28
 2a4:	7f 63 db 78 	mr      r3,r27
 2a8:	48 00 00 01 	bl      2a8 <mpc8xx_pmu_read+0x1c8>
			2a8: R_PPC_REL24	generic_atomic64_cmpxchg
 2ac:	7c 1d 18 00 	cmpw    r29,r3
 2b0:	41 a2 ff 50 	beq     200 <mpc8xx_pmu_read+0x120>
 2b4:	7f 63 db 78 	mr      r3,r27
 2b8:	48 00 00 01 	bl      2b8 <mpc8xx_pmu_read+0x1d8>
			2b8: R_PPC_REL24	generic_atomic64_read
 2bc:	7c 9c 23 78 	mr      r28,r4
 2c0:	7c 7d 1b 78 	mr      r29,r3
 2c4:	40 92 fe c8 	bne     cr4,18c <mpc8xx_pmu_read+0xac>
 2c8:	83 f8 00 00 	lwz     r31,0(r24)
			2ca: R_PPC_ADDR16_LO	itlb_miss_counter
 2cc:	3b c0 00 00 	li      r30,0
 2d0:	7e 64 f8 50 	subf    r19,r4,r31
 2d4:	7e 72 fe 70 	srawi   r18,r19,31
 2d8:	4b ff ff bc 	b       294 <mpc8xx_pmu_read+0x1b4>
 2dc:	7e 88 01 20 	mtcrf   128,r20
 2e0:	40 a2 ff b4 	bne     294 <mpc8xx_pmu_read+0x1b4>
 2e4:	83 f7 00 00 	lwz     r31,0(r23)
			2e6: R_PPC_ADDR16_LO	dtlb_miss_counter
 2e8:	3b c0 00 00 	li      r30,0
 2ec:	7e 64 f8 50 	subf    r19,r4,r31
 2f0:	7e 72 fe 70 	srawi   r18,r19,31
 2f4:	4b ff ff a0 	b       294 <mpc8xx_pmu_read+0x1b4>
 2f8:	81 81 00 14 	lwz     r12,20(r1)
 2fc:	80 01 00 54 	lwz     r0,84(r1)
 300:	83 21 00 34 	lwz     r25,52(r1)
 304:	7c 08 03 a6 	mtlr    r0
 308:	7d 82 01 20 	mtcrf   32,r12
 30c:	7d 81 01 20 	mtcrf   16,r12
 310:	7d 80 81 20 	mtcrf   8,r12
 314:	38 21 00 50 	addi    r1,r1,80
 318:	4e 80 00 20 	blr


---


### compiler : `gcc`
### title : `missed optimization on 2 equivalent expressions when -fwrapv is not used`
### open_at : `2021-08-24T08:43:48Z`
### last_modified_date : `2023-09-24T09:07:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102032
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `enhancement`
### contents :
Consider the following code from bug 30484:

int f (int a, int b, int c)
{
  if (b < 0)
    return a + b + c;
  else
    return a + c + b;
}

On x86_64 with -O3, GCC 11.2.0 gives

        leal    (%rdi,%rdx), %eax
        addl    %esi, %edi
        addl    %edx, %edi
        addl    %esi, %eax
        testl   %esi, %esi
        cmovs   %edi, %eax

But x86_64 processors (and most processors, AFAIK) do not care about overflows, so that a + b + c and a + c + b are actually equivalent for the processor and one should get only 2 instructions, corresponding to the 2 additions. Surprisingly, -fwrapv has the effect to allow this optimization, with the following generated code:

        addl    %edx, %edi
        leal    (%rdi,%rsi), %eax


---


### compiler : `gcc`
### title : `vectorizer costing is off because of if-conversion`
### open_at : `2021-08-24T13:03:59Z`
### last_modified_date : `2021-08-24T13:04:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102040
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
void foo (double * __restrict x, double *y, double *z, int n)
{
  for (int i = 0; i < n; ++i)
    {
      double tem;
      if (i & 8)
        tem = y[i] * 3.;
      else
        tem = z[i] + 5.;
      x[i] = tem;
    }
}

is vectorized using masked loads with -Ofast -mavx2 on x86_64 and the
cost model says

t3.c:3:21: note:  Cost model analysis:
  Vector inside of loop cost: 252
  Vector prologue cost: 56
  Vector epilogue cost: 352
  Scalar iteration cost: 84
  Scalar outside cost: 8
  Vector outside cost: 408
  prologue iterations: 0
  epilogue iterations: 4
  Calculated minimum iters for profitability: 6

in particular the scalar iteration cost is based on unconditionally
executing

0x3907200 i_26 & 8 1 times scalar_stmt costs 4 in prologue
0x3907200 _1 != 0 1 times scalar_stmt costs 4 in prologue
0x3907200 .MASK_LOAD (_4, 64B, _39) 1 times scalar_load costs 12 in prologue
0x3907200 _5 * 3.0e+0 1 times scalar_stmt costs 20 in prologue
0x3907200 _1 == 0 1 times scalar_stmt costs 4 in prologue
0x3907200 .MASK_LOAD (_8, 64B, _41) 1 times scalar_load costs 12 in prologue
0x3907200 _9 + 5.0e+0 1 times scalar_stmt costs 12 in prologue
0x3907200 _1 == 0 ? tem_19 : tem_21 1 times scalar_stmt costs 4 in prologue
0x3907200 tem_14 1 times scalar_store costs 12 in prologue

each scalar iteration will only execute one arm of the condition and
given the condition contiguous memory blocks of size 8 * sizeof (double)
are loaded either from x or y.  It's not clear how the vector code
behaves when using masked loads but when you use non-possibly-trapping
accesses we'd not use masked loads anyway.

With -mprefer-vector-width=128 and thus V2DFmode vectors the conditional
code is hardly going to be profitable given for each useful lane we're
computing one not useful one (consider the condition being & 1 here
for a similarly extreme example).

Besides these statically visible partitions which eventually could see
an optimal solution (unroll to their period for example) in the dynamic
case the scalar costing would need to consider branch prediction and
branch mispredict costs.  An available profile could provide a hint
at whether we're dealing with a known well-predicted branch or whether
we don't know.

double x[1024], y[1024], z[1024], a[1024];
void foo ()
{
  for (int i = 0; i < 1024; ++i)
    {
      double tem;
      if (a[i] > 0.)
        tem = y[i];
      else
        tem = z[i];
      x[i] = tem;
    }
}

is vectorized with SSE to

.L2:
        movapd  %xmm2, %xmm0
        movapd  y(%rax), %xmm1
        addq    $16, %rax
        cmpltpd a-16(%rax), %xmm0
        andpd   %xmm0, %xmm1
        andnpd  z-16(%rax), %xmm0
        orpd    %xmm1, %xmm0
        movaps  %xmm0, x-16(%rax)
        cmpq    $8192, %rax
        jne     .L2

we manage to obfuscate the non-vectorized variant to

        pxor    %xmm1, %xmm1
        jmp     .L5
        .p2align 4,,10
        .p2align 3
.L11:
        movsd   y(%rax), %xmm0
.L4:
        movsd   %xmm0, x(%rax)
        addq    $8, %rax
        cmpq    $8192, %rax
        je      .L10
.L5:
        movsd   a(%rax), %xmm0
        comisd  %xmm1, %xmm0
        ja      .L11
        movsd   z(%rax), %xmm0
        jmp     .L4
.L10:
        ret

so I'm not actually sure it will run faster even when we make the
a[] compares all the same (the vector variant runs almost 3 times faster).

clang ends up using integer moves and cmov here (and it unrolls) but
it does not vectorize.  GCCs vector variant is nearly two times faster.

So intuition when comparing branchy scalar and straight-line vector code
can prove wrong.


---


### compiler : `gcc`
### title : `slightly worse code as PRE on some code got disabled for loop vectorization`
### open_at : `2021-08-25T07:14:27Z`
### last_modified_date : `2021-11-22T08:54:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102054
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
This is a test case reduced from SPEC2017 bmk 541.leela_r source FastBoard.cpp, when I was investigating the O2 vectorization degradation on SPEC2017 run. It's an issue similar to PR100794, but which is only applied at O2 and fixed by re-running pcom at O2. This one is applied for O3 vectorization as well.

TEST CASE:

class FastBoard {
public:
    static const int NBR_SHIFT = 4;
    static const int MAXBOARDSIZE = 19;
    static const int MAXSQ = ((MAXBOARDSIZE + 2) * (MAXBOARDSIZE + 2));
    enum square_t {
        BLACK = 0, WHITE = 1, EMPTY = 2, INVAL = 3
    };

    bool self_atari(int color, int vertex);

protected:
    int m_dirs[4];
    square_t m_square[MAXSQ];
    int nbr_libs[20];
};

bool FastBoard::self_atari(int color, int vertex) {
  int nbr_libs_cnt = 0;
  nbr_libs[nbr_libs_cnt++] = vertex;

  for (int k = 0; k < 20; k++) {
    int ai = vertex + m_dirs[k];

    if (m_square[ai] == FastBoard::EMPTY) {
      bool found = false;

      for (int i = 0; i < nbr_libs_cnt; i++) {
        if (nbr_libs[i] == ai) {
          found = true;
          break;
        }
      }

      if (!found) {
        if (nbr_libs_cnt > 1)
          return false;
        nbr_libs[nbr_libs_cnt++] = ai;
      }
    }
  }

  return true;
}

Options: -mcpu=power9 -Ofast (or -O2 -ftree-vectorize) etc.

With -fno-tree-loop-vectorize, it passes down the vertex_11 for nbr_libs[0].

  <bb 3> [local count: 1014686026]:
  # prephitmp_26 = PHI <pretmp_28(5), vertex_11(D)(10)>
  # ivtmp.17_27 = PHI <ivtmp.17_3(5), ivtmp.17_8(10)>
  if (ai_15 == prephitmp_26)
    goto <bb 8>; [5.50%]
  else
    goto <bb 4>; [94.50%]

  <bb 4> [local count: 958878295]:
  if (ivtmp.17_27 != _31)
    goto <bb 5>; [93.84%]
  else
    goto <bb 11>; [6.16%]

  <bb 5> [local count: 899822494]:
  ivtmp.17_3 = ivtmp.17_27 + 4;
  _21 = (void *) ivtmp.17_3;
  pretmp_28 = MEM[(int *)_21];
  goto <bb 3>; [100.00%]


Without -fno-tree-loop-vectorize, it has the below IRs instead, always do the load before ai comparison.

  <bb 4> [local count: 1014686026]:
  # ivtmp.12_27 = PHI <ivtmp.12_28(5), ivtmp.12_26(3)>
  ivtmp.12_28 = ivtmp.12_27 + 4;
  _22 = (void *) ivtmp.12_28;
  _3 = MEM[(int *)_22];
  if (_3 == ai_15)
    goto <bb 8>; [5.50%]
  else
    goto <bb 5>; [94.50%]


  <bb 5> [local count: 958878295]:
  if (ivtmp.12_28 != _30)
    goto <bb 4>; [93.84%]
  else
    goto <bb 10>; [6.16%]


---


### compiler : `gcc`
### title : `full 128byte swap using __builtin_shuffle should produce rev64 followed by ext`
### open_at : `2021-08-25T07:16:31Z`
### last_modified_date : `2021-08-25T07:20:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102055
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
#define vector __attribute__((vector_size(16)))

vector char g(vector char a)
{
    return __builtin_shuffle(a,(vector char){15,14,13,12,11,10,9,8,7,6,5,4,3,2,1, 0});
}

vector char g1(vector char a)
{
    vector char t= __builtin_shuffle(a,(vector char){7,6,5,4,3,2,1,0,15,14,13,12,11,10,9,8,});
    vector long long t1 = (vector long long)t;
    t1 = __builtin_shuffle(t1, (vector long long){1,0});
    return (vector char)t1;
}

The first case uses ldr/tlb but really it can be done in two steps as rev64 followed by ext.
        rev64   v0.16b, v0.16b
        ext     v0.16b, v0.16b, v0.16b, #8


---


### compiler : `gcc`
### title : `VEC_PERM_EXPR of different sizes are not combined`
### open_at : `2021-08-25T07:19:44Z`
### last_modified_date : `2021-08-25T07:26:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102056
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
take this on x86_64 with -mavx2:
#define vector __attribute__((vector_size(16)))

vector char g(vector char a)
{
    return __builtin_shuffle(a,(vector char){15,14,13,12,11,10,9,8,7,6,5,4,3,2,1, 0});
}

vector char g1(vector char a)
{
    vector char t= __builtin_shuffle(a,(vector char){7,6,5,4,3,2,1,0,15,14,13,12,11,10,9,8,});
    vector long long t1 = (vector long long)t;
    t1 = __builtin_shuffle(t1, (vector long long){1,0});
    return (vector char)t1;
}
--- CUT ---
We get:
g:
        .cfi_startproc
        vpshufb .LC0(%rip), %xmm0, %xmm0
        ret
g1:
        vpshufb .LC1(%rip), %xmm0, %xmm0
        vpalignr        $8, %xmm0, %xmm0, %xmm0
        ret

But really these two functions are the same.


---


### compiler : `gcc`
### title : `[12/13/14 regression] 450.soplex regressed on x86_64 with -Ofast -march=generic (by 8-15%)`
### open_at : `2021-08-25T08:41:49Z`
### last_modified_date : `2023-05-08T12:22:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102058
### status : `SUSPENDED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
All three LNT x86_64 testers have experienced a regression when
running SPECFP 2006 benchmark 450.soplex compiled with -Ofast
-march=generic (as opposed to -march=native builds which seem not to
be affected).

A znver2 machine regressed by 15%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=414.150.0&plot.1=300.150.0&

A znver1 machine regressed by 8%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=451.150.0&plot.1=27.150.0&

An Intel Kabylake machine regressed by 9%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=429.150.0&plot.1=25.150.0&

I have bisected the regression (on another znver2 machine) to revision
r12-2733-g31855ba6b16:

31855ba6b16cd138d7484076a08cd40d609654b8 is the first bad commit
commit 31855ba6b16cd138d7484076a08cd40d609654b8
Author: Richard Biener <rguenther@suse.de>
Date:   Thu Jul 29 14:14:48 2021 +0200

    Add emulated gather capability to the vectorizer
    
    This adds a gather vectorization capability to the vectorizer
    without target support by decomposing the offset vector, doing
    sclar loads and then building a vector from the result.  This
    is aimed mainly at cases where vectorizing the rest of the loop
    offsets the cost of vectorizing the gather.
    [...]


---


### compiler : `gcc`
### title : `powerpc suboptimal unrolling simple array sum`
### open_at : `2021-08-25T11:27:40Z`
### last_modified_date : `2021-09-22T13:53:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102062
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.2.0`
### severity : `normal`
### contents :
--- test.c ---
int test(int *arr, int sz)
{
        int ret = 0;
        int i;

        if (sz < 1)
                __builtin_unreachable();

        for (i = 0; i < sz*2; i++)
                ret += arr[i];

        return ret;
}
---

gcc-11 compiles this to:
test:
        rldic 4,4,1,32
        addi 10,3,-4
        rldicl 9,4,63,33
        li 3,0
        mtctr 9
.L2:
        addi 8,10,4
        lwz 9,4(10)
        addi 10,10,8
        lwz 8,4(8)
        add 9,9,3
        add 9,9,8
        extsw 3,9
        bdnz .L2
        blr

I may be unaware of a constraint of C standard here, but maintaining the two base addresses seems pointless, so is beginning the first at offset -4.

The bigger problem is keeping a single sum. Keeping two sums and adding them at the end reduces critical latency of the loop from 6 to 2, which brings throughput on large loops from 6 cycles per iteration down to about 2.2 on POWER9 without harming short loops:

test:
        rldic 4,4,1,32
        rldicl 9,4,63,33
        mtctr 9
        li 8,0
        li 9,0
.L2:
        lwz 6,0(3)
        lwz 7,4(3)
        addi 3,3,8
        add  8,8,6
        add  9,9,7
        bdnz .L2
        add 9,9,8
        extsw 3,9
        blr

Any reason this can't be done?


---


### compiler : `gcc`
### title : `aarch64: Suboptimal addressing modes for SVE LD1W, ST1W`
### open_at : `2021-08-25T14:46:53Z`
### last_modified_date : `2021-08-25T15:19:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102066
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
For the code:
#include <arm_sve.h>

void foo(int n, float *x, float *y) {
    for (unsigned i=0; i<n; i+=svcntw()) {
        svfloat32_t val = svld1_f32(svptrue_b8(), &x[i]);
        svst1_f32(svptrue_b8(), &y[i], val);
    }
}

at -O3 -march=armv8.2-a+sve GCC generates:
foo:
        cbz     w0, .L1
        mov     w4, 0
        cntw    x6
        ptrue   p0.b, all
.L3:
        ubfiz   x3, x4, 2, 32
        add     w4, w4, w6
        add     x5, x1, x3
        add     x3, x2, x3
        ld1w    z0.s, p0/z, [x5]
        st1w    z0.s, p0, [x3]
        cmp     w4, w0
        bcc     .L3
.L1:
        ret

but it could be making better use of the addressing modes:
foo:                                    // @foo
        cbz     w0, .LBB0_3
        mov     w8, wzr
        ptrue   p0.b
        cntw    x9
.LBB0_2:                                // %for.body
        mov     w10, w8
        ld1w    { z0.s }, p0/z, [x1, x10, lsl #2]
        add     w8, w8, w9
        cmp     w8, w0
        st1w    { z0.s }, p0, [x2, x10, lsl #2]
        b.lo    .LBB0_2
.LBB0_3:                                // %for.cond.cleanup
        ret

I guess the predicates and constraints in @aarch64_pred_mov<mode> in aarch64-sve.md should allow for the scaled address modes


---


### compiler : `gcc`
### title : `fill_always_executed_in_1 incomplete computation`
### open_at : `2021-08-26T09:29:21Z`
### last_modified_date : `2021-09-14T01:20:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102075
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
ALWAYS_EXECUTED_IN is not computed completely for nested loops.  Current design will exit if an inner loop doesn't dominate outer loop's latch or exit after exiting from inner loop, which caused early return from outer loop, then ALWAYS EXECUTED blocks after inner loops are skipped.

For example, x->k should be move out of outer loop but doesn't.

struct X { int i; int j; int k;};

void foo(struct X *x, int n, int l)
{
  for (int j = 0; j < l; j++)
    {
      for (int i = 0; i < n; ++i)
        {
          int *p = &x->j;
          int tem = *p;
          x->j += tem * i;
        }
      int *r = &x->k;
      int tem2 = *r;
      x->k += tem2 * j;
    }
}


Discussion lists:

https://gcc.gnu.org/pipermail/gcc-patches/2021-August/577444.html


---


### compiler : `gcc`
### title : `powerpc: mflr called twice when using __builtin_return_address(0)`
### open_at : `2021-08-26T11:38:33Z`
### last_modified_date : `2021-08-26T18:21:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102076
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Fonction:

long f(void *);

long g(void)
{
	return 3 + f(__builtin_return_address(0));
}


Builds into:

00000000 <g>:
   0:	7c 08 02 a6 	mflr    r0
   4:	7c 68 02 a6 	mflr    r3
   8:	94 21 ff f0 	stwu    r1,-16(r1)
   c:	90 01 00 14 	stw     r0,20(r1)
  10:	48 00 00 01 	bl      10 <g+0x10>
  14:	80 01 00 14 	lwz     r0,20(r1)
  18:	38 63 00 03 	addi    r3,r3,3
  1c:	7c 08 03 a6 	mtlr    r0
  20:	38 21 00 10 	addi    r1,r1,16
  24:	4e 80 00 20 	blr



The 'mflr r0' should be avoided by using the value in r3


---


### compiler : `gcc`
### title : `Gcc unnecessarily initializes indeterminate variables passed across function boundaries`
### open_at : `2021-08-27T12:28:01Z`
### last_modified_date : `2021-08-30T07:36:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102096
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Compared to clang where:

long ret_unspec(void){ auto long rv; return rv; }

void take6(long,long,long,long,long,long);

void call_take6(void)
{
    //6 unnecessary XORs on GCC
    auto long id0; //indeterminate
    auto long id1; //indeterminate
    auto long id2; //indeterminate
    auto long id3; //indeterminate
    auto long id4; //indeterminate
    auto long id5; //indeterminate
    take6(id0,id1,id2,id3,id4,id5);
}

yields (x86_64):

ret_unspec:                            # @ret_unspec2
        retq
call_take6:                             # @call_take6
        jmp     take6        

(1+5 bytes), GCC compiles the above to
ret_unspec2:
        xorl    %eax, %eax
        ret
call_take6:
        xorl    %r9d, %r9d
        xorl    %r8d, %r8d
        xorl    %ecx, %ecx
        xorl    %edx, %edx
        xorl    %esi, %esi
        xorl    %edi, %edi
        jmp     take6

(3+19 bytes), unnecessarily 0-initializing  the indeterminate return-value/arguments.

Type casting the called function can often be hackishly used to get the same assembly but doing so is technically UB and not as generic as supporting the passing of unspecified arguments/return values, which can be used to omit argument register initializations not just for arguments at the end of an argument pack but also in the middle.

TL;DR: Allowing to passing/return indeterminate variables without generating initializing code for them would be nice. Clang already does it.


---


### compiler : `gcc`
### title : `s390: Inefficient code for 64x64=128 signed multiply for <= z13`
### open_at : `2021-08-29T12:30:48Z`
### last_modified_date : `2021-11-25T19:17:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102117
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `normal`
### contents :
__int128 imul128(long long a, long long b)
{
   return (__int128)a * (__int128)b;
}

creates sequence with 3 multiplies:

_Z7imul128xx:
.LFB0:
        .cfi_startproc
        ldgr    %f2,%r12
        .cfi_register 12, 17
        ldgr    %f0,%r13
        .cfi_register 13, 16
        lgr     %r13,%r3
        mlgr    %r12,%r4
        srag    %r1,%r3,63
        msgr    %r1,%r4
        srag    %r4,%r4,63
        msgr    %r4,%r3
        agr     %r4,%r1
        agr     %r12,%r4
        stmg    %r12,%r13,0(%r2)
        lgdr    %r13,%f0
        .cfi_restore 13
        lgdr    %r12,%f2
        .cfi_restore 12
        br      %r14
        .cfi_endproc


The following sequence only requires 1 multiply:

__int128 imul128_opt(long long a, long long b)
{
   unsigned __int128 x = (unsigned __int128)(unsigned long long)a;
   unsigned __int128 y = (unsigned __int128)(unsigned long long)b;
   unsigned long long t1 = (a >> 63) & a;
   unsigned long long t2 = (b >> 63) & b;
   unsigned __int128 u128 = x * y;
   unsigned long long hi = (u128 >> 64) - (t1 + t2);
   unsigned long long lo = (unsigned long long)u128;
   unsigned __int128 res = hi;
   res <<= 64;
   res |= lo;
   return (__int128)res;
}

_Z11imul128_optxx:
.LFB1:
        .cfi_startproc
        ldgr    %f2,%r12
        .cfi_register 12, 17
        ldgr    %f0,%r13
        .cfi_register 13, 16
        lgr     %r13,%r3
        mlgr    %r12,%r4
        lgr     %r1,%r3
        srag    %r3,%r3,63
        ngr     %r3,%r1
        srag    %r1,%r4,63
        ngr     %r4,%r1
        agr     %r3,%r4
        sgrk    %r3,%r12,%r3
        stg     %r13,8(%r2)
        lgdr    %r12,%f2
        .cfi_restore 12
        lgdr    %r13,%f0
        .cfi_restore 13
        stg     %r3,0(%r2)
        br      %r14
        .cfi_endproc


---


### compiler : `gcc`
### title : `switch conversion to load table should do integer compression`
### open_at : `2021-08-29T20:43:26Z`
### last_modified_date : `2021-08-30T07:36:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102121
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(int a)
{
switch (a)
{
    case 0:
      return 100;
    case 1:
      return 200;
    case 3:
      return 250;
    case 4:
      return 0;
    case 5:
      return 7;
    case 6:
      return 3;
}
return 0;
}

int f1(int a)
{
unsigned char t = 0;
switch (a)
{
    case 0:
      t=100;
      break;
    case 1:
      t = 200u;
      break;
    case 3:
      t = 250u;
      break;
    case 4:
      t = 0;
      break;
    case 5:
      t = 7;
      break;
    case 6:
      t = 3;
      break;
}
asm("":"+r"(t));
return t;
}

These two functions should have the same assembly and the load table should be using byte loads for both.


---


### compiler : `gcc`
### title : `(ARM Cortex-M3 and newer) missed optimization. memcpy not needed operations`
### open_at : `2021-08-30T08:27:44Z`
### last_modified_date : `2022-03-23T14:57:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102125
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `normal`
### contents :
uint64_t bar64(const uint8_t *rData1)
{
    uint64_t buffer;
    memcpy(&buffer, rData1, sizeof(buffer));
    return buffer;
}

compiler options: 
-Ox -mthumb -mcpu=cortex-my

where x : 2,3,s   y:3,4,7

```
bar64:
        sub     sp, sp, #8
        mov     r2, r0
        ldr     r0, [r0]  @ unaligned
        ldr     r1, [r2, #4]      @ unaligned
        mov     r3, sp
        stmia   r3!, {r0, r1}
        ldrd    r0, [sp]
        add     sp, sp, #8
        bx      lr
```

it is enough to:

```
        mov     r3, r0
        ldr     r0, [r0]  @ unaligned
        ldr     r1, [r3, #4]      @ unaligned
        bx      lr
```

32 bit memcpy is optimized correctly:

Full example code:

```
uint64_t foo64(const uint8_t *rData1)
{
    uint64_t buffer;
    buffer =  (((uint64_t)rData1[7]) << 56)|((uint64_t)(rData1[6]) << 48)|((uint64_t)(rData1[5]) << 40)|(((uint64_t)rData1[4]) << 32)|
                            (((uint64_t)rData1[3]) << 24)|(((uint64_t)rData1[2]) << 16)|((uint64_t)(rData1[1]) << 8)|rData1[0];
    return buffer;
}

uint64_t bar64(const uint8_t *rData1)
{
    uint64_t buffer;
    memcpy(&buffer, rData1, sizeof(buffer));
    return buffer;
}

uint32_t foo32(const uint8_t *rData1)
{
    uint32_t buffer;
    buffer = (((uint32_t)rData1[3]) << 24)|(((uint32_t)rData1[2]) << 16)|((uint32_t)(rData1[1]) << 8)|rData1[0];
    return buffer;
}

uint32_t bar32(const uint8_t *rData1)
{
    uint32_t buffer;
    memcpy(&buffer, rData1, sizeof(buffer));
    return buffer;
}
```

compiler output:
```
foo64:
        mov     r3, r0
        ldr     r0, [r0]  @ unaligned
        ldr     r1, [r3, #4]      @ unaligned
        bx      lr
bar64:
        sub     sp, sp, #8
        mov     r2, r0
        ldr     r0, [r0]  @ unaligned
        ldr     r1, [r2, #4]      @ unaligned
        mov     r3, sp
        stmia   r3!, {r0, r1}
        ldrd    r0, [sp]
        add     sp, sp, #8
        bx      lr
foo32:
        ldr     r0, [r0]  @ unaligned
        bx      lr
bar32:
        ldr     r0, [r0]  @ unaligned
        bx      lr
```

Clang compiles without overhead:

https://godbolt.org/z/P7G7Whxqz


---


### compiler : `gcc`
### title : `(ARM Cortex-M3 and newer) changing operation order  may reduce number of instructions needed`
### open_at : `2021-08-30T20:24:13Z`
### last_modified_date : `2021-08-31T13:27:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102135
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `normal`
### contents :
uint64_t foo64(const uint8_t *rData1)
{
    uint64_t buffer;
    buffer =  (((uint64_t)rData1[7]) << 56)|((uint64_t)(rData1[6]) << 48)|((uint64_t)(rData1[5]) << 40)|(((uint64_t)rData1[4]) << 32)|
                            (((uint64_t)rData1[3]) << 24)|(((uint64_t)rData1[2]) << 16)|((uint64_t)(rData1[1]) << 8)|rData1[0];
   
----
foo64:
        mov     r3, r0
        ldr     r0, [r0]  @ unaligned
        ldr     r1, [r3, #4]      @ unaligned
        bx      lr

Only 3 instructions are needed:

        ldr     r1, [r0, #4]      @ unaligned
        ldr     r0, [r0]  @ unaligned
        bx      lr

Options:
-O3 -mthumb -mcpu=cortex-M4


---


### compiler : `gcc`
### title : `t = a==0 and a = PHI<0, t> should be done earlier than PRE`
### open_at : `2021-08-30T23:52:38Z`
### last_modified_date : `2023-09-21T14:02:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102138
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int len(int f, int l) {
 return l < f ? 0 : l - f + 1;
}

int lenzero(int f, int l) {
 return len(f, l) == 0;
}

int g(); int h();
int lenzero1(int f, int l) {
 if ( len(f, l) == 0)
   return g();
  return h();
}

we should be able to optimize lenzero at -O1 but don't.  lenzero1 is optimized at -O1 because dom is able to jump thread.
The difference in IR is:
  # iftmp.0_9 = PHI <0(2), iftmp.0_8(3)>
  _2 = iftmp.0_9 == 0;

vs:
  # iftmp.0_13 = PHI <0(2), iftmp.0_12(3)>
  if (iftmp.0_13 == 0)

at -O2 PRE is enabled which does the optimization.
This is similar to spaceship_replacement in tree-ssa-phiopt but a more general statement of the problem.


---


### compiler : `gcc`
### title : `Better expansion of __builtin_*_overflow should be done`
### open_at : `2021-09-01T05:34:57Z`
### last_modified_date : `2023-06-02T14:23:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102153
### status : `ASSIGNED`
### tags : `internal-improvement, missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
One thing I noticed is that __builtin_*_overflow is always expanded as:
result = 0;
if (overflow)
  result = 1;

Which then later on get changed to result = overflow during ifcvt or jump threaded.  Why not instead just use cstore instead.


---


### compiler : `gcc`
### title : `Too many runtime alias checks when vectorizing`
### open_at : `2021-09-01T12:15:00Z`
### last_modified_date : `2022-01-05T13:29:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102160
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The following is reduced from (or rather "inspired") 507.cactuBSSN_r ML_BSSN_Advect_Body where, when one works around other issues by editing the
source, the vectorizer intends to create > 8000 runtime alias checks (and refuses).

void foo (double *a, double *b, int off, int n, int m)
{
  for (int j = 0; j < m; ++j)
    for (int i = 0; i < n; ++i)
      a[j*n+i] = b[j*n+i] + b[(j+1)*n+i] + b[(j-1)*n+i];
}

this small example iterates over a 2d array in a linearized way
(and a way that as written does not actually guarantee that each
a[j*n + i] is only written once, that is, the 2 dimensions do not "overlap").

The interesting bit is that the kernel offsets the accesses in the outer loop
iteration direction and thus when analyzing the refs in the innermost loop
we have three unknown non-constant offsets to b[] and we will create three
runtime alias checks that fail to merge (obviously).

We need to do better by formulating the alias checks with respect to the
outermost [interesting] iteration where we should be able to merge the
checks into one, obviously making it less precise by computing the access
extent of the whole loop nest.

As additional benefit the runtime alias check can be hoisted and thus
versioning applied to the outer loop.  That might already magically
work even.


---


### compiler : `gcc`
### title : `powerpc64 int memory operations using FP instructions`
### open_at : `2021-09-02T02:44:19Z`
### last_modified_date : `2022-01-05T18:18:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102169
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
--- test.c ---
int foo, bar;
  
void test(void)
{
        foo = bar;
}
---

Using Debian gcc 10.2 with -O2 flag, this compiles to:

     addis   r2,r12,0
     addi    r2,r2,0
     addis   r9,r2,0
     addi    r9,r9,0
     lfiwzx  f0,0,r9
     addis   r9,r2,0
     addi    r9,r9,0
     stfiwx  f0,0,r9
     blr

Peter confirmed it also uses FP registers with trunk (but I don't have the asm output at hand).

This can be suboptimal on some processors, e.g., on POWER9 lfiwzx is "Tuple Restricted (R)" which reduces dispatch throughput on the cycle it is dispatched. And generally just seems like a surprising thing to do with no shortage of GPRs.


---


### compiler : `gcc`
### title : `vget_low_*/vget_high_* intrinsics should become BIT_FIELD_REF during gimple`
### open_at : `2021-09-02T07:59:58Z`
### last_modified_date : `2021-10-22T10:10:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102171
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <arm_neon.h>
float16x4_t f0(float16x8_t b)
{
  return vget_low_f16 (b);
}
float16x4_t f(float16x8_t b)
{
  return vget_high_f16 (b);
}
float16x4_t f1(float16x8_t b)
{
  return ((float16x4_t*)&b)[0];
}
float16x4_t f2(float16x8_t b)
{
  return ((float16x4_t*)&b)[1];
}
---- CUT ----
For little-endian f0 and f1 are the same, and f and f2 are the same.
For big-endian f0 and f2 are the same, and f0 and f1 are the same.

The reason why we should simplify them at the gimple level is to allow the gimple optimizers do the work for us.


---


### compiler : `gcc`
### title : `BB SLP scalar costing is off with extern promoted nodes`
### open_at : `2021-09-02T12:18:07Z`
### last_modified_date : `2021-09-06T06:56:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102176
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
On aarch64 we can see

int foo(long *restrict res, long *restrict foo, long a, long b)
{
  res[0] = ((foo[0] * a) >> 1) + foo[0];
  res[1] = ((foo[1] * b) >> 1) + foo[1];
}

being vectorized as

t.c:3:10: note: Costing subgraph:
t.c:3:10: note: node 0x35f03b0 (max_nunits=2, refcnt=1)
t.c:3:10: note: op template: *res_12(D) = _4;
t.c:3:10: note:         stmt 0 *res_12(D) = _4;
t.c:3:10: note:         stmt 1 MEM[(long int *)res_12(D) + 8B] = _8;
t.c:3:10: note:         children 0x35f0440
t.c:3:10: note: node 0x35f0440 (max_nunits=2, refcnt=1)
t.c:3:10: note: op template: _4 = _1 + _3;
t.c:3:10: note:         stmt 0 _4 = _1 + _3;
t.c:3:10: note:         stmt 1 _8 = _5 + _7;
t.c:3:10: note:         children 0x35f04d0 0x35f0560
t.c:3:10: note: node 0x35f04d0 (max_nunits=2, refcnt=2)
t.c:3:10: note: op template: _1 = *foo_10(D);
t.c:3:10: note:         stmt 0 _1 = *foo_10(D);
t.c:3:10: note:         stmt 1 _5 = MEM[(long int *)foo_10(D) + 8B];
t.c:3:10: note: node 0x35f0560 (max_nunits=2, refcnt=1)
t.c:3:10: note: op template: _3 = _2 >> 1;
t.c:3:10: note:         stmt 0 _3 = _2 >> 1;
t.c:3:10: note:         stmt 1 _7 = _6 >> 1;
t.c:3:10: note:         children 0x35f05f0 0x35f0710
t.c:3:10: note: node (external) 0x35f05f0 (max_nunits=2, refcnt=1)
t.c:3:10: note:         stmt 0 _2 = _1 * a_11(D);
t.c:3:10: note:         stmt 1 _6 = _5 * b_14(D);
t.c:3:10: note:         children 0x35f04d0 0x35f0680
t.c:3:10: note: node (external) 0x35f0680 (max_nunits=1, refcnt=1)
t.c:3:10: note:         { a_11(D), b_14(D) }
t.c:3:10: note: node (constant) 0x35f0710 (max_nunits=1, refcnt=1)
t.c:3:10: note:         { 1, 1 }

so the promoted external node 0x35f05f0 should keep the load live.
vect_bb_slp_scalar_cost relies on PURE_SLP_STMT but
that's unreliable here since the per-stmt setting cannot capture the
different uses.  The code shares intend (and some bugs) with
vect_bb_slp_mark_live_stmts and the problem in general is a bit
difficult given the lack of back-mapping from stmt to SLP nodes
referencing it.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] SPECFP 2006 470.lbm regressions on AMD Zen CPUs after r12-897-gde56f95afaaa22`
### open_at : `2021-09-02T15:38:07Z`
### last_modified_date : `2023-07-27T09:22:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102178
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
LNT has detected an 18% regression of SPECFP 2006 benchmark 470.lbm
when it is compiled with -Ofast -march=native on a Zen2 machine:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=421.240.0&plot.1=301.240.0&

...and similarly a 6% regression when it is run on the same machine
with -Ofast:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=450.240.0&plot.1=24.240.0&

I have bisected both on another zen2 machine to commit
r12-897-gde56f95afaaa22 (Run pass_sink_code once more before
store_merging).

Zen1 machine has also seen a similar -march=native regression in the
same time frame:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=450.240.0&plot.1=24.240.0&

Zen1 -march=generic seems to be unaffected, which is also the case for
the Intel machines we track.

Although lbm has been known to have weird regressions caused entirely
by code layout where the compiler was not really at fault, the fact
that both generic code-gen and Zen1 are affected seems to indicate this
is not the case.


---


### compiler : `gcc`
### title : `sccvn compare predicated result issue in vn_nary_op_insert_into`
### open_at : `2021-09-03T06:38:45Z`
### last_modified_date : `2021-09-11T06:41:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102183
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
In tree-ssa-sccvn.c, function vn_nary_op_insert_into, the following code might lead to inserting entra vn_pval entry:

  vn_pval *nval = vno->u.values;
  vn_pval **next = &vno->u.values;
  bool found = false;
  for (vn_pval *val = (*slot)->u.values; val; val = val->next)
    {
      if (expressions_equal_p (val->result, vno->u.values->result)) // Here
	{
	  ...
	  /* Append value.  */
	  *next = (vn_pval *) obstack_alloc (&vn_tables_obstack,
					     sizeof (vn_pval)
					     + val->n * sizeof (int));
	  (*next)->next = NULL;
	  (*next)->result = val->result;
	  ...
	  next = &(*next)->next;
	  if (dump_file && (dump_flags & TDF_DETAILS))
	    fprintf (dump_file, "Appending predicate to value.\n");
	  continue;
	}
      /* Copy other predicated values.  */
      *next = (vn_pval *) obstack_alloc (&vn_tables_obstack,
					 sizeof (vn_pval)
					 + (val->n-1) * sizeof (int));
      memcpy (*next, val, sizeof (vn_pval) + (val->n-1) * sizeof (int));
      (*next)->next = NULL;
      next = &(*next)->next;
    }

I think 'expressions_equal_p' here should check if current val->result equals to the result in original vno? But vno->u.values will be changed if the "Copy other predicated values" branch is taken first (by setting *next). Here's an example test code:

    #include <stdio.h>

    unsigned int c = 1;

    void
    f (unsigned int a, unsigned int b)
    {
      if (a >= b)
        c = 2;
      else
        c = 9;

      for (unsigned i = 0; i < 100; i++)
        {
          if (a < b)
	    {
	      printf ("AAAA");
	      if (a >= b)
	        printf ("XXXXX");
	    }
          else
	    printf ("BBBB");
        }
    }

In detailed fre1 dump file, the new predicate value is not appended to old entry:

Processing block 0: BB2
Value numbering stmt = if (a_5(D) >= b_6(D))
Recording on edge 2->3 a_5(D) ge_expr b_6(D) == true
Recording on edge 2->4 a_5(D) ge_expr b_6(D) == false
Recording on edge 2->3 a_5(D) lt_expr b_6(D) == false   //===>old one
Recording on edge 2->4 a_5(D) lt_expr b_6(D) == true   //===>old one  
...
Processing block 5: BB6
Value numbering stmt = if (a_5(D) < b_6(D))
Recording on edge 6->7 a_5(D) lt_expr b_6(D) == true   //===>new one
Recording on edge 6->9 a_5(D) lt_expr b_6(D) == false
Appending predicate to value.

After changed to 'if (expressions_equal_p (val->result, nval->result))', the new predicate value can be appended to old one:

Processing block 0: BB2
Value numbering stmt = if (a_5(D) >= b_6(D))
Recording on edge 2->3 a_5(D) ge_expr b_6(D) == true
Recording on edge 2->4 a_5(D) ge_expr b_6(D) == false
Recording on edge 2->3 a_5(D) lt_expr b_6(D) == false     //===>old one
Recording on edge 2->4 a_5(D) lt_expr b_6(D) == true      //===>old one
...
Processing block 5: BB6
Value numbering stmt = if (a_5(D) < b_6(D))
Recording on edge 6->7 a_5(D) lt_expr b_6(D) == true    //===>new one
Appending predicate to value.
Recording on edge 6->9 a_5(D) lt_expr b_6(D) == false
Appending predicate to value.

(I think inserting extra vn_pval entries might cause losing optimize opportunities.)


---


### compiler : `gcc`
### title : `Over widening detection doesn't work when no range information when doing bitwise operations (|, ^, and &)`
### open_at : `2021-09-03T08:31:20Z`
### last_modified_date : `2023-07-13T18:30:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102188
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following example:

#include <stdint.h>

int f( int16_t a[16] )
{
    int res = 0;
    for ( int i = 0; i < 16; i++ )
      res |= (a[i]);

    return res;
}

gets vectorized with the ORs happening as int32, but since | can't overflow or underflow the ORs could have been done as int16.  This saves 2 ORs and 4 widenings.

I would have expected overwidening detection to handle this, however that fails because VRP has no range information for `a` but in this case it's safe to do based on the type alone.


---


### compiler : `gcc`
### title : `Inefficent expansion of memset when range is [0,1]`
### open_at : `2021-09-05T01:19:41Z`
### last_modified_date : `2023-09-12T05:52:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102202
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
void g(int a, char *d)
{
  if (a < 0 || a > 1) __builtin_unreachable();
  __builtin_memset(d, 0, a);
}

----- CUT -----
GCC compiles on x86_64 to:
g(int, char*):
        .cfi_startproc
        testl   %edi, %edi
        je      .L1
        xorl    %eax, %eax
.L2:
        movl    %eax, %edx
        addl    $1, %eax
        movb    $0, (%rsi,%rdx)
        cmpl    %edi, %eax
        jb      .L2
.L1:
        ret

Which is better than clang/LLVM/ICC does but the loop is not needed as a will either be 0 or 1 and we already jump around the loop.

Here is another example not using __builtin_unreachable:
void g(int a, char *d)
{
  __builtin_memset(d, 0, a&1);
}


---


### compiler : `gcc`
### title : `__builtin_memset and __builtin_memcpy could be expanded inline if range is known to be small`
### open_at : `2021-09-05T01:28:54Z`
### last_modified_date : `2021-09-05T01:28:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102203
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
typedef decltype(sizeof(0)) size_t;
void g(size_t a, char *d, char *e)
{
  if (a>16)__builtin_unreachable();
  __builtin_memcpy(d, e, a);
}

----- CUT ----
This could be inlined like it is on x86_64.


---


### compiler : `gcc`
### title : `vec<int> + 1 could be done as vec<int> - (-1)`
### open_at : `2021-09-05T03:39:51Z`
### last_modified_date : `2021-09-06T08:35:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102205
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
template <class T> using V [[gnu::vector_size(16)]] = T;
auto a1(V<  int> b) { return 1 + b; }

---- CUT ----
Currently GCC produces:
a1(int __vector(4)):
        paddd   .LC0(%rip), %xmm0
        ret
        .cfi_endproc
.LFE0:
        .size   a1(int __vector(4)), .-a1(int __vector(4))
        .section        .rodata.cst16,"aM",@progbits,16
        .align 16
.LC0:
        .long   1
        .long   1
        .long   1
        .long   1

----------------------------------------
But it might be best if GCC produces (like LLVM):
a1(int __vector(4)):
        pcmpeqd %xmm1, %xmm1
        psubd   %xmm1, %xmm0
        retq


---


### compiler : `gcc`
### title : `NRVO for function parameters`
### open_at : `2021-09-05T19:55:09Z`
### last_modified_date : `2022-03-08T20:23:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102209
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
I believe this is a missed optimization opportunity.

Given following class and functions:


----
struct s{ 
   s();
   s(const s&);
   s(s&&);
   void doit();
};

s bar0(){ 
  s v = s();
  v.doit();
  return v;
}
s bar1(s v = s()){
 v.doit(); 
 return v;
}


void foo0(){ auto v = bar0(); }
void foo1(){ auto v = bar1(); }
----

I can see that in bar0, the returned s is copy (and move) elided.
But for bar1, this is not the case.

I've tried different things, like changing function signature, adding std::forward, change optimization level, disable exceptions, but I was never able to obtain the desired result.
As in bar1 `v` is actually built a level higher on the stack, for this code-snippet I would even have expected GCC to have less issue optimizing the copy/move away.


For reference: https://godbolt.org/z/oe7W3nvcP

foo0():
        sub     rsp, 24
        call    construct()
        lea     rdi, [rsp+15]
        call    s::doit()
        call    destroy()
        add     rsp, 24
        ret
foo1():
        sub     rsp, 24
        call    construct()
        lea     rdi, [rsp+15]
        call    s::doit()
        call    move()
        call    destroy()
        add     rsp, 24
        jmp     destroy()

In both cases, bar* has been inlined.
It can be easily verified that in the case of bar0 everything is inlined, and no copies are made.
In the case of foo1, a temporary is created, passed to bar without moving or copying, but when bar1 returns, a move is made.

As s is passed by value, I do not think the move is necessary.
So either my assumption is wrong, or GCC is playing safe.

I would like to know if it is a missed optimization opportunity on the side of the programmer or compiler (for those classes where even an unnecessary move might be costly).


---


### compiler : `gcc`
### title : `[12 Regression] missed optimization causing Warray-bounds`
### open_at : `2021-09-06T10:14:15Z`
### last_modified_date : `2021-11-23T10:01:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102216
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch, TREE`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
With a gcc build from commit 637dfcf43cf, I run into an incorrect Warray-bounds (which causes a buildbreaker when building gdb, as reported here: https://sourceware.org/pipermail/gdb/2021-September/049687.html ).

Reproducer minimized from gdb/language.c:
...
$ cat language.c
#include <algorithm>

static inline bool
compare_cstrings (const char *str1, const char *str2)
{
  return str1 < str2;
}

void
add_set_language_command ()
{
  static const char **language_names;

  language_names = new const char *[6];

  language_names[0] = "auto";
  language_names[1] = "local";
  language_names[2] = "unknown";

  const char **language_names_p = language_names;
  /* language_names_p == &language_names[0].  */
  language_names_p++;
  /* language_names_p == &language_names[1].  */
  language_names_p++;
  /* language_names_p == &language_names[2].  */
  language_names_p++;
  /* language_names_p == &language_names[3].  */

  const char **sort_begin;

  if (0)
    sort_begin = &language_names[3];
  else
    sort_begin = language_names_p;

  language_names[3] = "";
  language_names[4] = "";
  language_names[5] = nullptr;

  std::sort (sort_begin, &language_names[5], compare_cstrings);
}
...

First with gcc-11:
...
$ g++-11 -x c++ -Werror -Wall -O2 -S language.c -Warray-bounds=1
$ 
...

Now with trunk:
...
$ g++ -x c++ -Werror -Wall -O2 -S language.c -Warray-bounds=1 
In file included from /home/vries/gcc_versions/devel/install/include/c++/12.0.0/algorithm:61,
                 from language.c:1:
In function ‘void std::__final_insertion_sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = const char**; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<bool (*)(const char*, const char*)>]’,
    inlined from ‘void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = const char**; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<bool (*)(const char*, const char*)>]’ at /home/vries/gcc_versions/devel/install/include/c++/12.0.0/bits/stl_algo.h:1960:31,
    inlined from ‘void std::sort(_RAIter, _RAIter, _Compare) [with _RAIter = const char**; _Compare = bool (*)(const char*, const char*)]’ at /home/vries/gcc_versions/devel/install/include/c++/12.0.0/bits/stl_algo.h:4868:18,
    inlined from ‘void add_set_language_command()’ at language.c:40:13:
/home/vries/gcc_versions/devel/install/include/c++/12.0.0/bits/stl_algo.h:1869:32: error: array subscript 19 is outside array bounds of ‘void [48]’ [-Werror=array-bounds]
 1869 |           std::__insertion_sort(__first, __first + int(_S_threshold), __comp);
      |           ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
language.c: In function ‘void add_set_language_command()’:
language.c:14:38: note: at offset 152 into object of size 48 allocated by ‘operator new []’
   14 |   language_names = new const char *[6];
      |                                      ^
cc1plus: all warnings being treated as errors
...

Now set 'if (0)' to 'if (1)':
...
$ g++ -x c++ -Werror -Wall -O2 -S language.c -Warray-bounds=1 
$
...

So either the warning is incorrect (for this fairly trivial example), or there's a compiler/std::sort bug and the warning is letting us known.


---


### compiler : `gcc`
### title : `fast-math inhibits fp contraction for a + b * a`
### open_at : `2021-09-06T16:04:43Z`
### last_modified_date : `2021-09-06T18:37:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102219
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Test case (compiled with -O2 -mfma):

[[gnu::optimize("fast-math")]]
float f(float a, float b)
{
    return a + b * a;
}

float g(float a, float b)
{
    return a + b * a;
}

f is not contracted to an fma, while g is.

Wild guess: GCC considers a transformation to (1 + b) * a and determines too late that (1 + b) * a is better transformed to a + b * a (note that `return (1 + b) * a` also emits vmulss, vaddss like f above and also doesn't contract to an FMA even though it could/should). Maybe (1 + b) * a should be contracted to fma(a, b, a) with fast-math, in general?


---


### compiler : `gcc`
### title : `Missed optimizations for algorithms over std::unique_ptr`
### open_at : `2021-09-06T17:34:15Z`
### last_modified_date : `2023-04-19T21:03:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102221
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Sorting/heaping/etc. an array/vector of unique_ptr generates worse codegen than the equivalent code on an array of raw pointers. This is a missed optimization, as the operations involved should just be mere swaps.

For instance:


#include <memory>
#include <algorithm>

#if defined(SMART)
using ptr = std::unique_ptr<int>;
#else
using ptr = int*;
#endif

void f()
{
    extern ptr *data;
    const auto cmp = [](const auto &a, const auto &b) { return *a < *b; };
    std::sort(data, data+1000, cmp);
}


https://gcc.godbolt.org/z/7zPYxr9q7


This generates quite some extra code (including calls to operator delete); the result is that such a sort is slower than the countepart with a raw pointer.


---


### compiler : `gcc`
### title : `Missed arithmetic fold`
### open_at : `2021-09-07T15:22:54Z`
### last_modified_date : `2022-02-23T21:26:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102232
### status : `RESOLVED`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
LLVM optimizes bar into tgt here but not foo.

https://godbolt.org/z/nhEjaoanx

int foo(int a, int b) {
    return b * (1 + a / b) - a;
}
int bar(int a, int b) {
    return b * (a / b) + b - a;
}
int tgt(int a, int b) {
    return b - a % b;
}

LLVM appears to miss this too.


---


### compiler : `gcc`
### title : `powerpc suboptimal boolean test of contiguous bits`
### open_at : `2021-09-08T11:55:20Z`
### last_modified_date : `2022-01-12T00:27:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102239
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.1`
### severity : `normal`
### contents :
gcc version 11.2.1 20210815 (Debian 11.2.0-2) 

Build flags -O2

--- test.c ---
void foo(long arg)
{
        if (arg & ((1UL << 33) | (1UL << 34)))
                asm volatile("# if");
        else
                asm volatile("# else");
}
---

generates:

foo:
        rldicr 3,3,29,1
        srdi. 3,3,29
        beq 0,.L6
        # if
        blr
.L6:
        # else
        blr

This test of multiple contiguous bits could be tested with a single instruction with rldicr. or rldicl.

Those instructions do tend to be more expensive than the Rc=0 form (C2 cracked on POWER9, slower pipe class on POWER10), but I think they should be better than the current 2-instruction sequence.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] extra spilling when using inline-asm and all registers`
### open_at : `2021-09-09T20:43:34Z`
### last_modified_date : `2023-07-07T10:40:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102264
### status : `WAITING`
### tags : `inline-asm, missed-optimization, ra`
### component : `target`
### version : `9.1.0`
### severity : `normal`
### contents :
I am trying to use custom intrinsics in order to have more control over the assembly that the compiler is generating. The concept of these custom intrinsics comes from http://users.ece.cmu.edu/~franzf/papers/wpmvp16.pdf.

For performance reasons, my code requires me to use all the available SIMD registers on the machine, but when I use my custom intrinsics, I am only getting half of the SIMD registers which leads to register spilling.

This is the code and generated assembly in question: https://godbolt.org/z/fqn53G9qT

Any help would be greatly appericated.


---


### compiler : `gcc`
### title : `s390: Inefficient code for __builtin_ctzll`
### open_at : `2021-09-09T21:04:41Z`
### last_modified_date : `2021-09-09T22:09:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102265
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `enhancement`
### contents :
unsigned long long ctzll(unsigned long long x)
{
   return __builtin_ctzll(x);
}

creates:
        lcgr    %r1,%r2
        ngr     %r2,%r1
        lghi    %r1,63
        flogr   %r2,%r2
        sgrk    %r2,%r1,%r2
        lgfr    %r2,%r2
        br      %r14


Optimal sequence for z15 uses population count, for all others use ^ 63 instead of 63 -.

unsigned long long ctzll_opt(unsigned long long x)
{
#if __ARCH__ >= 13
   return __builtin_popcountll((x-1) & ~x);
#else
   return __builtin_clzll(x & -x) ^ 63;
#endif
}

< z15:
        lcgr    %r1,%r2
        ngr     %r2,%r1
        flogr   %r2,%r2
        xilf    %r2,63
        lgfr    %r2,%r2
        br      %r14

=> 1 instruction saved.

z15:
        .cfi_startproc
        lay     %r1,-1(%r2)
        ncgrk   %r2,%r1,%r2
        popcnt  %r2,%r2,8
        br      %r14
        .cfi_endproc

=> On z15 only 3 instructions required.


---


### compiler : `gcc`
### title : `memset expansion is sometimes slow for small sizes`
### open_at : `2021-09-12T21:59:18Z`
### last_modified_date : `2021-09-14T02:04:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102294
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.1`
### severity : `normal`
### contents :
Created attachment 51444
Test program that illustrates the issue

The output of the attached test program is as follows for an Intel Core i7-4790 CPU (3.6 GHz) when compiled with -O2:
$ ~/test/bio_init 
Elapsed time: 0.874763 s
Elapsed time: 0.480335 s
Elapsed time: 0.733273 s

The above output shows that bio_init2() runs faster than bio_init3() and that bio_init3() runs faster than bio_init1(). bio_init3() uses structure assignment to initialize struct bio while bio_init2() uses memberwise initialization. bio_init1() uses memset(). To me it was a big surprise to see that bio_init3() is slower than bio_init2(). Apparently clang generates better code:

$ clang -O2 -o bio_init-clang bio_init.c
$ ./bio_init-clang 

Elapsed time: 0.446804 s
Elapsed time: 0.455009 s
Elapsed time: 0.407392 s

Can gcc be modified such that bio_init3() runs at least as fast as bio_init2()?

The bio_init[123]() source code comes from the Linux kernel. Optimization level -O2 has been chosen because that is what the Linux kernel uses.


---


### compiler : `gcc`
### title : `excessive stack usage`
### open_at : `2021-09-16T01:02:41Z`
### last_modified_date : `2021-11-15T01:11:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102355
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
void escape(unsigned long long& a);

void foobar()
{
    unsigned long long local;
    escape(local);
}

For the function "foobar" GCC allocates excessive stack space:

foobar():
        sub     rsp, 24
        lea     rdi, [rsp+8]
        call    escape(unsigned long long&)
        add     rsp, 24
        ret

The function "foobar" only needs 8 bytes of stack space, but GCC allocates 24. Please note, that this excessive allocation isn't needed for stack alignment: 8 bytes of local variables are enough to keep the stack aligned. I also tested Clang and it allocates 8 bytes.

GCC makes this stack layout:
    8 bytes padding
    8 bytes variable "local"
    8 bytes padding
    8 bytes return address

I believe the problem is related to the fact that GCC aligns the stack twice: the first time after the return address placement and the second time after the local variables are placed. Playing with -mpreferred-stack-boundary confirms this:

-mpreferred-stack-boundary | stack usage
                         3             8
               4 (default)            24
                         5            56
                         6           120

https://godbolt.org/z/h56aoKvvh

In all cases the stack usage is twice as much (minus 8 bytes for return address) as the required alignment. I believe stack space can be conserved by doing alignment only once.


---


### compiler : `gcc`
### title : `Missing optimization for strlen after enable O2 vectorization`
### open_at : `2021-09-17T02:13:35Z`
### last_modified_date : `2021-09-17T09:22:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102382
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
testcase is from strlenopt-85.c

void* elim_strlen_calloc_store_memset_1 (unsigned a, unsigned b)
{
  char *p = __builtin_calloc (a, 1);

  p[0] = '\0';
  p[1] = '\0';
  p[2] = '\0';
  p[3] = '\0';

  __builtin_memset (p, 0, b);

  n0 = __builtin_strlen (p);

  return p;
}

diff --git a/strlenopt-85.c.193t.strlen1 b/../vectorize/strlenopt-85.c.193t.strlen1
index 2c8bc4a..6591d9e 100644
--- a/strlenopt-85.c.193t.strlen1
+++ b/../vectorize/strlenopt-85.c.193t.strlen1
@@ -10,6 +10,8 @@
 ;; 2 succs { 1 }
 void * elim_strlen_calloc_store_memset_1 (unsigned int a, unsigned int b)
 {
+  vector(4) char * vectp.5;
+  vector(4) char * vectp_p.4;
   char * p;
   long unsigned int _1;
   long unsigned int _2;
@@ -19,11 +21,10 @@ void * elim_strlen_calloc_store_memset_1 (unsigned int a, unsigned int b)
   <bb 2> [local count: 1073741824]:
   _1 = (long unsigned int) a_5(D);
   p_8 = __builtin_calloc (_1, 1);
-  MEM[(char *)p_8 + 1B] = 0;
-  MEM[(char *)p_8 + 2B] = 0;
-  MEM[(char *)p_8 + 3B] = 0;
+  MEM <vector(4) char> [(char *)p_8] = { 0, 0, 0, 0 };
   _2 = (long unsigned int) b_13(D);
-  _3 = 0;
+  __builtin_memset (p_8, 0, _2);
+  _3 = __builtin_strlen (p_8);
   _4 = (unsigned int) _3;
   n0 = _4;
   return p_8;

It seems to be a missing optimization here.


---


### compiler : `gcc`
### title : `Missing optimization for PRE after enable O2 vectorization`
### open_at : `2021-09-17T02:35:02Z`
### last_modified_date : `2021-11-22T08:54:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102383
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
testcase is form gcc.dg/tree-ssa/predcom-1.c

void abort (void);

unsigned fib[1000];

__attribute__ ((noinline))
void count_fib(void)
{
  int i;

  fib[0] = 0;
  fib[1] = 1;
  for (i = 2; i < 1000; i++)
    fib[i] = (fib[i-1] + fib[i - 2]) & 0xffff;
}

git diff novectorize vectorize dump

diff --git a/../novectorize/predcom-1.c.248t.optimized b/./predcom-1.c.248t.optimized
index 9e4783d..7846af6 100644
--- a/../novectorize/predcom-1.c.248t.optimized
+++ b/./predcom-1.c.248t.optimized
@@ -5,53 +5,57 @@ Removing basic block 5
 __attribute__((noinline))
 void count_fib ()
 {
-  sizetype ivtmp.13;
+  sizetype ivtmp.16;
+  unsigned int fib_I_lsm1.6;
   unsigned int fib_I_lsm0.5;
   int i;
-  unsigned int _2;
-  unsigned int _4;
   unsigned int _5;
   unsigned int _6;
-  unsigned int prephitmp_21;
-  unsigned int prephitmp_24;
-  unsigned int _41;
+  unsigned int _19;
+  unsigned int _20;
+  unsigned int _21;
+  unsigned int _37;
+  int _38;
+  unsigned int _46;
   unsigned int _47;
-  unsigned int _48;
-  unsigned int _59;
-  int _65;
-  unsigned int pretmp_66;
+  int _54;
+  unsigned int _55;
+  unsigned int _56;
+  unsigned int _57;
 
   <bb 2> [local count: 10737416]:
-  MEM <unsigned long> [(unsigned int *)&fib] = 4294967296;
+  MEM <vector(2) unsigned int> [(unsigned int *)&fib] = { 0, 1 };
 
   <bb 3> [local count: 10737417]:
-  # prephitmp_21 = PHI <1(2), _48(3)>
-  # prephitmp_24 = PHI <0(2), _6(3)>
-  # fib_I_lsm0.5_38 = PHI <1(2), _48(3)>
-  # ivtmp.13_7 = PHI <4(2), ivtmp.13_8(3)>
-  _5 = prephitmp_21 + prephitmp_24;
+  # fib_I_lsm0.5_32 = PHI <0(2), _6(3)>
+  # fib_I_lsm1.6_33 = PHI <1(2), _47(3)>
+  # ivtmp.16_11 = PHI <4(2), ivtmp.16_10(3)>
+  _5 = fib_I_lsm0.5_32 + fib_I_lsm1.6_33;
   _6 = _5 & 65535;
-  MEM[(unsigned int *)&fib + -8B + ivtmp.13_7 * 4] = _6;
-  _47 = _6 + fib_I_lsm0.5_38;
-  _48 = _47 & 65535;
-  MEM[(unsigned int *)&fib + -4B + ivtmp.13_7 * 4] = _48;
-  ivtmp.13_8 = ivtmp.13_7 + 2;
-  if (ivtmp.13_8 != 1000)
+  MEM[(unsigned int *)&fib + -8B + ivtmp.16_11 * 4] = _6;
+  _46 = _6 + fib_I_lsm1.6_33;
+  _47 = _46 & 65535;
+  MEM[(unsigned int *)&fib + -4B + ivtmp.16_11 * 4] = _47;
+  ivtmp.16_10 = ivtmp.16_11 + 2;
+  if (ivtmp.16_10 != 1000)
     goto <bb 3>; [98.00%]
   else
     goto <bb 4>; [2.00%]
 
   <bb 4> [local count: 10737416]:
-  i_51 = (int) ivtmp.13_7;
-  _41 = _6 + _48;
-  _59 = _41 & 65535;
-  fib[i_51] = _59;
-  i_61 = i_51 + 1;
-  _65 = i_51 + -1;
-  pretmp_66 = fib[_65];
-  _2 = _59 + pretmp_66;
-  _4 = _2 & 65535;
-  fib[i_61] = _4;
+  i_50 = (int) ivtmp.16_11;
+  _38 = i_50 + -1;
+  _37 = fib[_38]; ----- missing optimization here
+  _54 = i_50 + -2;
+  _55 = fib[_54]; ----- and here.
+  _56 = _37 + _55;
+  _57 = _56 & 65535;
+  fib[i_50] = _57;
+  i_59 = i_50 + 1;
+  _19 = fib[_38];
+  _20 = _19 + _57;
+  _21 = _20 & 65535;
+  fib[i_59] = _21;
   return;
 
 }


---


### compiler : `gcc`
### title : `Missing optimization for pcom after enable O2 vectorization`
### open_at : `2021-09-17T02:48:49Z`
### last_modified_date : `2021-09-17T07:58:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102384
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
testcase is from gcc.dg/tree-ssa/predcom-dse-3.c

void __attribute__((noinline)) foo4 (int *a)
{
  int i;
  for (i = 0; i < 100; i++)
    {
      a[i] = 0;
      a[i + 3] = -1;
    }
}

novectorize vs vectorize dump

@@ -1576,111 +1576,86 @@ void foo3 (int * a)
 ;; Function foo4 (foo4, funcdef_no=4, decl_uid=2022, cgraph_uid=5, symbol_order=12)
 
 Processing loop 1
-Creating dr for *_3
...skipping...
-  _5 = _4 * 4;
-  _6 = a_10(D) + _5;
-  i_13 = i_17 + 1;
-  ivtmp_15 = ivtmp_16 - 1;
-  if (ivtmp_15 != 0)
-    goto <bb 5>; [98.99%]
+  vect_cst__14 = { 0, 0 };
+  vect_cst__22 = { -1, -1 };
+  vectp_a.59_23 = a_10(D) + 12;
+
+  <bb 3> [local count: 536870800]:
+  # vectp_a.56_19 = PHI <vectp_a.56_20(5), a_10(D)(2)>
+  # vectp_a.58_24 = PHI <vectp_a.58_25(5), vectp_a.59_23(2)>
+  # ivtmp_27 = PHI <ivtmp_28(5), 0(2)>
+  MEM <vector(2) int> [(int *)vectp_a.56_19] = vect_cst__14;
+  MEM <vector(2) int> [(int *)vectp_a.58_24] = vect_cst__22;---- this is bad
+  vectp_a.56_20 = vectp_a.56_19 + 8;
+  vectp_a.58_25 = vectp_a.58_24 + 8;
+  ivtmp_28 = ivtmp_27 + 1;
+  if (ivtmp_28 < 50)
+    goto <bb 5>; [98.00%]
   else
-    goto <bb 4>; [1.01%]
+    goto <bb 4>; [2.00%]
 
-  <bb 5> [local count: 1052266996]:
+  <bb 5> [local count: 526133384]:
   goto <bb 3>; [100.00%]
 
   <bb 4> [local count: 10737416]:
-  MEM[(int *)a_10(D) + 400B] = -1; ---- this is good
-  MEM[(int *)a_10(D) + 404B] = -1;
-  MEM[(int *)a_10(D) + 408B] = -1;
   return;


---


### compiler : `gcc`
### title : `Failure to optimize adjacent 8-bit loads into a single bigger load`
### open_at : `2021-09-17T22:32:34Z`
### last_modified_date : `2023-09-21T08:27:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102391
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

uint16_t HeaderReadU16LE(int offset, uint8_t *RomHeader)
{
    return RomHeader[offset] |
        (RomHeader[offset + 1] << 8);
}

This can be optimized into a single 16-bit load. On -O3, this optimization is done by LLVM, but not by GCC.

This winds up affecting the resulting assembly quite a bit:

AMD64 GCC:

HeaderReadU16LE:
  movsx rdi, edi
  movzx edx, BYTE PTR [rsi+1+rdi]
  movzx eax, BYTE PTR [rsi+rdi]
  sal edx, 8
  or eax, edx
  ret

AMD64 LLVM:

HeaderReadU16LE:
  movsxd rax, edi
  movzx eax, word ptr [rsi + rax]
  ret


---


### compiler : `gcc`
### title : `Failure to optimize a sign extension to a zero extension`
### open_at : `2021-09-17T23:44:42Z`
### last_modified_date : `2023-09-21T08:27:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102392
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

void f(int64_t x);

void g(int32_t x)
{
    if (x < 0)
        __builtin_unreachable();
    f(x);
}

This can be optimized to avoid the sign extension since x can't be under 0. This optimization is done by LLVM, but not by GCC.

Sample resulting assembly from GCC:

g:
  movsx rdi, edi
  jmp f

from LLVM:

g:
  mov edi, edi
  jmp f

(PS: I originally found this while looking at the code that led me to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102391: an error check earlier in the code (not in the example cited there) wound up making this assumption possible, and slightly changed the assembly code emitted by LLVM there to be even more efficient)


---


### compiler : `gcc`
### title : `Failure to optimize 2 8-bit stores into a single 16-bit store`
### open_at : `2021-09-18T00:14:10Z`
### last_modified_date : `2023-09-21T08:26:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102393
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

void HeaderWriteU16LE(int offset, uint16_t value, uint8_t *RomHeader)
{
    RomHeader[offset] = value;
    RomHeader[offset + 1] = value >> 8;
}

Non-withstanding aliasing, this can be optimized to `*(uint16_t *)(RomHeader + offset) = value`. This transformation is done by LLVM, but not by GCC.

Sample AMD64 output for this from GCC:

HeaderWriteU16LE:
  movsx rdi, edi
  mov eax, esi
  mov BYTE PTR [rdx+rdi], sil
  mov BYTE PTR [rdx+1+rdi], ah
  ret

And from LLVM:

HeaderWriteU16LE:
  movsxd rax, edi
  mov word ptr [rdx + rax], si
  ret

PS: The equivalent pattern for 4 8-bit stores gets optimized into a single 32-bit store.


---


### compiler : `gcc`
### title : `Seemingly suboptimal optimization of jmp/cmovcc for conditionally loading constants`
### open_at : `2021-09-18T18:17:08Z`
### last_modified_date : `2023-09-21T08:25:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102402
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
#include <stdint.h>

struct MusicPlayerTrack
{
    uint8_t flags;
    uint8_t modT;
};

void ClearModM(struct MusicPlayerTrack *track, uint8_t modT)
{
    if (track->modT == 0)
        track->flags |= 3;
    else
        track->flags |= 12;
}

This is optimized weirdly by GCC. Leaving it as-is gives this AMD64 assembly:

ClearModM:
  movzx edx, BYTE PTR [rdi]
  mov eax, edx
  or eax, 12
  cmp BYTE PTR [rdi+1], 0
  jne .L3
  mov eax, edx
  or eax, 3
.L3:
  mov BYTE PTR [rdi], al
  ret

Whereas changing the `if` to `if (modT == 0)` gives this:

ClearModM:
  movzx eax, BYTE PTR [rdi]
  mov edx, eax
  or eax, 12
  or edx, 3
  test sil, sil
  cmove eax, edx
  mov BYTE PTR [rdi], al
  ret

It seems to me that this should be better than the first output, though of course this could be the other way considering how finicky cmovcc seems to be, but it seems to me like at least one should be preferred above the other.

Note that this also occurs on IA-32, so the issue seems unrelated to whether modT is in a register or in memory. Perhaps it's about whether it's a function argument ?


---


### compiler : `gcc`
### title : `Loop vectorized with 32 byte vectors actually uses 16 byte vectors`
### open_at : `2021-09-18T22:17:29Z`
### last_modified_date : `2021-09-22T02:40:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102404
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
Created attachment 51480
Test case

Consider the loop on L11 of the attached file.  Compiling as:

❯ gcc -march=tigerlake -Ofast -mprefer-vector-width=512 -S -fopenmp test.c -fopt-info
test.c:25:37: optimized: loop vectorized using 32 byte vectors
test.c:4:6: optimized: loop turned into non-loop; it never loops

which notes that (as requested) the loop has been vectorized using 32-byte (zmm) vectors.  Inspecting the resulting assembly (also attached) we observe that has actually ben unrolled by a factor of two and then vectorized using 16-byte (ymm) vectors.

As a point of comparison recent versions of Clang use 32-byte vectors for this loop, resulting in code which is half the size.


---


### compiler : `gcc`
### title : `[11 Regression] Lost Load/Store Motion`
### open_at : `2021-09-21T20:09:03Z`
### last_modified_date : `2023-05-29T10:05:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102436
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 51492
Testcase

So consider this loop (-O2, lim2 dump, trunk, x86_64):



;;   basic block 3, loop depth 1
;;    pred:       2
;;                10
  # target_8 = PHI <target_13(D)(2), target_17(10)>
  _4 = board[target_8];
  if (_4 == 13)
    goto <bb 4>; [94.50%]
  else
    goto <bb 7>; [5.50%]
;;    succ:       4
;;                7

;;   basic block 4, loop depth 1
;;    pred:       3
  if (captures.32_5 == 0)
    goto <bb 5>; [33.00%]
  else
    goto <bb 6>; [67.00%]
;;    succ:       5
;;                6

;;   basic block 5, loop depth 1
;;    pred:       4
  numb_moves.1_21 = numb_moves;
  _22 = (long unsigned int) numb_moves.1_21;
  _23 = _22 * 24;
  _24 = (struct move_s *) _23;
  _24->from = gfrom.30_1;
  _24->target = target_8;
  _24->captured = 13;
  _24->castled = 0;
  _24->promoted = 0;
  _24->ep = 0;
  _26 = numb_moves.1_21 + 1;
  numb_moves = _26;
;;    succ:       6

;;   basic block 6, loop depth 1
;;    pred:       4
;;                5
  target_17 = target_8 + offset_14;
  _7 = board[target_17];
  if (_7 != 0)
    goto <bb 10>; [94.50%]
  else
    goto <bb 9>; [5.50%]
;;    succ:       10
;;                9

;;   basic block 10, loop depth 1
;;    pred:       6
  goto <bb 3>; [100.00%]
;;    succ:       3

In particular note the load from and store to numb_moves in block #5 within the loop.  I don't immediately see an aliasing issue that would prevent LSM.  The bigger problem is control flow, obviously the load/store may not be executed, but I thought our LIM/LSM code handled that correctly.

If we look at gcc-10 we get something like this:


;;   basic block 3, loop depth 1
;;    pred:       2
;;                10
  # target_9 = PHI <target_14(D)(2), target_19(10)>
  # numb_moves_lsm.43_6 = PHI <numb_moves_lsm.43_34(2), numb_moves_lsm.43_2(10)>
  # numb_moves_lsm_flag.44_20 = PHI <numb_moves_lsm_flag.44_35(2), numb_moves_lsm_flag.44_18(10)>
  _4 = board[target_9];
  if (_4 == 13)
    goto <bb 4>; [94.50%]
  else
    goto <bb 7>; [5.50%]
;;    succ:       4
;;                7

;;   basic block 4, loop depth 1
;;    pred:       3
  if (captures.32_5 == 0)
    goto <bb 5>; [33.00%]
  else
    goto <bb 6>; [67.00%]
;;    succ:       5
;;                6

;;   basic block 5, loop depth 1
;;    pred:       4
  numb_moves.1_21 = numb_moves_lsm.43_6;
  _22 = (long unsigned int) numb_moves.1_21;
  _23 = _22 * 24;
  _24 = (struct move_s *) _23;
  _24->from = gfrom.30_1;
  _24->target = target_9;
  _24->captured = 13;
  _24->castled = 0;
  _24->promoted = 0;
  _24->ep = 0;
  _26 = numb_moves.1_21 + 1;
  numb_moves_lsm.43_37 = _26;
  numb_moves_lsm_flag.44_38 = 1;
;;    succ:       6

;;   basic block 6, loop depth 1
;;    pred:       4
;;                5
  # numb_moves_lsm.43_2 = PHI <numb_moves_lsm.43_6(4), numb_moves_lsm.43_37(5)>
  # numb_moves_lsm_flag.44_18 = PHI <numb_moves_lsm_flag.44_20(4), numb_moves_lsm_flag.44_38(5)>
  target_19 = target_9 + offset_15;
  _8 = board[target_19];
  if (_8 != 0)
    goto <bb 10>; [94.50%]
  else
    goto <bb 11>; [5.50%]
;;    succ:       10
;;                11

[ ... ]
;;   basic block 10, loop depth 1
;;    pred:       6
  goto <bb 3>; [100.00%]
;;    succ:       3


Obviously with the load before the loop and the store after.


---


### compiler : `gcc`
### title : `[x86-64] Failure to optimize out spill in vector code when a cast is used`
### open_at : `2021-09-21T22:37:35Z`
### last_modified_date : `2023-09-21T08:23:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102438
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stddef.h>

typedef double simde_float64x1_t __attribute__((__vector_size__(8)));

simde_float64x1_t simde_vabs_f64(simde_float64x1_t a) {
    simde_float64x1_t r;
    r[0] = -a[0];
    return (simde_float64x1_t)r;
}

On AMD64 with -O3, this is outputted:

simde_vabs_f64(double __vector(1)):
        movsd   xmm0, QWORD PTR [rsp+8]
        xorpd   xmm0, XMMWORD PTR .LC0[rip]
        mov     rax, rdi
        movsd   QWORD PTR [rsp-24], xmm0
        mov     rdx, QWORD PTR [rsp-24]
        mov     QWORD PTR [rdi], rdx
        ret

If we instead just return `r` (without the cast) this is instead outputted:

simde_vabs_f64(double __vector(1)):
        movsd   xmm0, QWORD PTR [rsp+8]
        xorpd   xmm0, XMMWORD PTR .LC0[rip]
        mov     rax, rdi
        movsd   QWORD PTR [rdi], xmm0
        ret

It seems as though the presence of a cast (to the same type, no less) confuses GCC into spilling the result into memory.

The GIMPLE optimized output is different for the two, so idk how much this target-specific to x86, but I haven't been able to reproduce it anywhere else, so ¯\_(ツ)_/¯. 

PS: The same bug can also be reproduced with -m32


---


### compiler : `gcc`
### title : `SRA don't handle Structs with flexible array members even though they are not used`
### open_at : `2021-09-22T13:22:07Z`
### last_modified_date : `2021-09-27T22:08:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102452
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `enhancement`
### contents :
Created attachment 51494
test.c

Structs with flexible array members are not optimized on the stack. Testing with the test.c file shown below (which outputs the runtime in seconds), we see that the compiled program is significantly slower when the struct has a flexible array member than when it does not. This was tested on GCC 9.3.0 and 10.3.0 on Ubuntu 20.04.

$ gcc -O3 test.c
$ ./a.out
0.302769
$ gcc -O3 -DUSE_FLEX_ARR=1 test.c
$ ./a.out
0.728760

clang does not have this issue.

$ clang -O3 test.c
$ ./a.out
0.312194
$ clang -O3 -DUSE_FLEX_ARR=1 test.c
$ ./a.out
0.301175

This is what test.c looks like:

#include <stdlib.h>
#include <stdio.h>
#include <time.h>
#include <string.h>

struct Test {
    long is_a;
    union {
        struct {
            long one;
            long two;
            long three;
        } a;
        struct {
            int one;
            int two;
            int three;
            int four;
#if USE_FLEX_ARR
            char arr[];
#endif
        } b;
    } as;
};

#define COUNT 100000000

static inline struct Test make_test_a(struct Test *test)
{
    if (test->is_a) {
        return *test;
    } else {
        struct Test ret;
        ret.as.a.one = test->as.b.one;
        ret.as.a.two = test->as.b.two;
        ret.as.a.three = test->as.b.three;
        return ret;
    }
}

/* This function should be optimized to not allocate struct Test on the stack
 * since it only uses attribute "three". */
static inline long get_three(struct Test *test)
{
    return make_test_a(test).as.a.three;
}

int main(int argc, char *argv[])
{
    struct timespec start, end;

    struct Test *mem = malloc(sizeof(struct Test) * COUNT);
    memset(mem, 0, sizeof(struct Test) * COUNT);

    clock_gettime(CLOCK_MONOTONIC, &start);
    {
        for (int i = 0; i < COUNT; i++) {
            long three = get_three(&mem[i]);
            if (three) {
                /* Impossible case. */
                printf("what\n");
            }
        }
    }
    clock_gettime(CLOCK_MONOTONIC, &end);

    double time = (end.tv_sec - start.tv_sec) + (end.tv_nsec - start.tv_nsec) / 1000000000.0;
    printf("%f\n", time);

    return 0;
}


---


### compiler : `gcc`
### title : `Miss optimization for (_Float16) sqrtf ((float) f16)`
### open_at : `2021-09-23T05:55:25Z`
### last_modified_date : `2022-06-27T13:35:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102464
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Related thread
[1] https://gcc.gnu.org/pipermail/gcc-patches/2021-July/574216.html
[2] https://gcc.gnu.org/pipermail/gcc-patches/2021-July/574330.html


---


### compiler : `gcc`
### title : `Missed SLP discovery for gathers`
### open_at : `2021-09-23T11:17:57Z`
### last_modified_date : `2021-12-02T17:21:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102467
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
void foo (double * __restrict a, long * __restrict b, double * c)
{
  for (int i = 0; i < 1024; ++i)
    {
      a[2*i] = c[b[2*i]];
      a[2*i + 1] = c[b[2*i + 1]];
    }
}

is not SLP vectorized.


---


### compiler : `gcc`
### title : `[12/13 Regression] 521.wrf_r 5% slower at -Ofast and generic x86_64 tuning after r12-3426-g8f323c712ea76c`
### open_at : `2021-09-23T16:45:16Z`
### last_modified_date : `2022-07-26T13:27:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102473
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
All three x86_64 LNT machines have detected a 4.5-5.2% performance
regression of SPEC FPrate 2017 benchmarks 521.wrf_r when compiled with
-Ofast and the default (generic) march and mtune.

Zen2 based machine regressed by 5%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=294.548.0
Zen1 based machine regressed by 5.2%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=35.548.0
Kabylake based machine regressed by 4.5%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=34.548.0

On an AMD zen2 based machine I have bisected the regression to commit
r12-3426-g8f323c712ea76c:

8f323c712ea76cc4506b03895e9b991e4e4b2baf is the first bad commit
commit 8f323c712ea76cc4506b03895e9b991e4e4b2baf
Author: liuhongt <hongtao.liu@intel.com>
Date:   Tue Sep 7 12:39:04 2021 +0800

    Optimize v4sf reduction.

    gcc/ChangeLog:
            
            PR target/101059
            * config/i386/sse.md (reduc_plus_scal_<mode>): Split to ..
            (reduc_plus_scal_v4sf): .. this, New define_expand.
            (reduc_plus_scal_v2df): .. and this, New define_expand.


I have confirmed that the commit causes a similar regression on
another Intel Skylake server.

On the Zen2 machine, this is the difference in samples collected by
perf for different symbols (before is commit 60eec23b5ed, after commit
8f323c712ea):

| Symbol                                      | sys lib | Before | After |  diff |     % |
|---------------------------------------------+---------+--------+-------+-------+-------|
| __logf_fma                                  | yes     |  68882 | 68940 |   +58 | +0.08 |
| __atanf                                     | yes     |  66664 | 66196 |  -468 | -0.70 |
| __module_advect_em_MOD_advect_scalar_pd     | no      |  62286 | 62348 |   +62 | +0.10 |
| __powf_fma                                  | yes     |  56213 | 56127 |   -86 | -0.15 |
| __module_mp_wsm5_MOD_nislfv_rain_plm        | no      |  46990 | 48340 | +1350 | +2.87 |
| __module_mp_wsm5_MOD_wsm52d                 | no      |  41031 | 40968 |   -63 | -0.15 |
| __module_small_step_em_MOD_advance_uv       | no      |  30908 | 30909 |    +1 | +0.00 |
| __module_small_step_em_MOD_advance_w        | no      |  28738 | 28600 |  -138 | -0.48 |
| __module_advect_em_MOD_advect_scalar        | no      |  28400 | 28429 |   +29 | +0.10 |
| __expf_fma                                  | yes     |  26702 | 26516 |  -186 | -0.70 |
| __module_big_step_utilities_em_MOD_phy_prep | no      |  25878 | 25816 |   -62 | -0.24 |
| psim_unstable_                              | no      |  24994 | 25106 |  +112 | +0.45 |
| __module_bl_ysu_MOD_ysu2d                   | no      |  24799 | 25251 |  +452 | +1.82 |
| psih_unstable_                              | no      |  22600 | 23139 |  +539 | +2.38 |
| __module_small_step_em_MOD_advance_mu_t     | no      |  22250 | 22232 |   -18 | -0.08 |
| __memset_avx2_unaligned_erms                | yes     |  21748 | 21613 |  -135 | -0.62 |
| _ZGVbN4vv_powf_sse4                         | yes     |  21206 | 21355 |  +149 | +0.70 |


---


### compiler : `gcc`
### title : `Reduction of 4 chars can be improved`
### open_at : `2021-09-25T15:22:53Z`
### last_modified_date : `2021-10-12T07:25:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102483
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
char foo (char* p)
 {
   char sum = 0;
    for (int i = 0; i != 4; i++)
    sum += p[i];
     return sum;
  }

-O3 -march=x86-64


GCC trunk:

foo:
        mov     edx, DWORD PTR [rdi]
        movzx   eax, dh
        mov     ecx, edx
        add     eax, edx
        shr     ecx, 16
        add     eax, ecx
        shr     edx, 24
        add     eax, edx
        ret


GCC 11 (much better):
foo:
        movzx   eax, BYTE PTR [rdi+1]
        add     al, BYTE PTR [rdi]
        add     al, BYTE PTR [rdi+2]
        add     al, BYTE PTR [rdi+3]
        ret


Best? llvm-mca says so..

foo:                                    # @foo
        movd    xmm0, dword ptr [rdi]           # xmm0 = mem[0],zero,zero,zero
        pxor    xmm1, xmm1
        psadbw  xmm1, xmm0
        movd    eax, xmm1
        ret


https://godbolt.org/z/sT9svvj7W


---


### compiler : `gcc`
### title : `__builtin_popcount(y&3) is not optimized to (y&1)+((y&2)>>1) if don't have popcount optab (or expensive one)`
### open_at : `2021-09-26T08:36:12Z`
### last_modified_date : `2021-09-27T08:38:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102487
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(unsigned y)
{
  return __builtin_popcount(y&3);
}

On x86_64 (without popcount optab enabled) this should be optimized just:
        movl    %edi, %eax
        shrl    %edi
        andl    $1, %edi
        andl    $1, %eax
        addl    %edi, %eax
        ret

But we currently get:

        .cfi_startproc
        subq    $8, %rsp
        .cfi_def_cfa_offset 16
        andl    $3, %edi
        call    __popcountdi2
        addq    $8, %rsp
        .cfi_def_cfa_offset 8
        ret

For aarch64 we currently get:

        and     x0, x0, 3
        fmov    d0, x0
        cnt     v0.8b, v0.8b
        addv    b0, v0.8b
        fmov    w0, s0
        ret

vs:

        and     w1, w0, 1
        ubfx    x0, x0, 1, 1
        add     w0, w0, w1
        ret

The second one is much much cheaper as you don't need to move between register sets.


---


### compiler : `gcc`
### title : `Failure to optimize vector reduction properly especially when using OpenMP`
### open_at : `2021-09-27T00:52:19Z`
### last_modified_date : `2023-09-21T08:22:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102494
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
#include <stdint.h>
#include <stddef.h>

typedef int8_t simde_int8x8_t __attribute__((__vector_size__(8)));

int16_t
simde_vaddlv_s8(simde_int8x8_t a) {
    int16_t r = 0;

#pragma omp simd reduction(+:r)
    for (size_t i = 0 ; i < (sizeof(a) / sizeof(a[0])) ; i++) {
      r += a[i];
    }

    return r;
}

Compiled with -O3 -fopenmp-simd, this is the output on AMD64:

simde_vaddlv_s8(signed char __vector(8)):
        pxor    xmm1, xmm1
        movdqa  xmm2, xmm0
        pcmpgtb xmm1, xmm0
        punpcklbw       xmm0, xmm1
        punpcklbw       xmm2, xmm1
        pshufd  xmm0, xmm0, 78
        movq    QWORD PTR [rsp-24], xmm2
        movq    QWORD PTR [rsp-16], xmm0
        movdqa  xmm0, XMMWORD PTR [rsp-24]
        psrldq  xmm0, 8
        paddw   xmm0, XMMWORD PTR [rsp-24]
        movdqa  xmm1, xmm0
        psrldq  xmm1, 4
        paddw   xmm0, xmm1
        movdqa  xmm1, xmm0
        psrldq  xmm1, 2
        paddw   xmm0, xmm1
        pextrw  eax, xmm0, 0
        ret

This is what Clang manages:

simde_vaddlv_s8(signed char __vector(8)):
        punpcklbw       xmm0, xmm0              # xmm0 = xmm0[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7]
        psraw   xmm0, 8
        pshufd  xmm1, xmm0, 238                 # xmm1 = xmm0[2,3,2,3]
        paddw   xmm1, xmm0
        pshufd  xmm0, xmm1, 85                  # xmm0 = xmm1[1,1,1,1]
        paddw   xmm0, xmm1
        movdqa  xmm1, xmm0
        psrld   xmm1, 16
        paddw   xmm1, xmm0
        movd    eax, xmm1
        ret

Weirdly enough, removing the `#pragma omp simd reduction(+r)` slightly improves   GCC's output to this:

simde_vaddlv_s8(signed char __vector(8)):
        pxor    xmm1, xmm1
        movdqa  xmm2, xmm0
        pcmpgtb xmm1, xmm0
        punpcklbw       xmm0, xmm1
        punpcklbw       xmm2, xmm1
        pshufd  xmm0, xmm0, 78
        paddw   xmm0, xmm2
        pextrw  edx, xmm0, 1
        pextrw  eax, xmm0, 0
        add     eax, edx
        pextrw  edx, xmm0, 2
        add     eax, edx
        pextrw  edx, xmm0, 3
        add     eax, edx
        ret


---


### compiler : `gcc`
### title : `optimize some consecutive byte load pattern to word load`
### open_at : `2021-09-27T05:31:12Z`
### last_modified_date : `2023-09-21T14:09:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102495
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `enhancement`
### contents :
I use the following code get a 32-bit word from a byte array by loading each byte and shifting them, but GCC doesn't optimize the code to a single word load when I put the byte load in a loop.

Clang trunk can optimize all of the follows: https://gcc.godbolt.org/z/KfWE67K5c


```
#define SHL(a,b) ((uint32_t)(a) << (b))

// both GCC and Clang optimize to *(uint32_t*)(vv)
uint32_t getword_b(const uint8_t *vv)
{
	return SHL(vv[3], 24) | SHL(vv[2], 16) | SHL(vv[1], 8) | SHL(vv[0], 0);
}

// GCC cannot optimize this, Clang can
uint32_t getword_forloop(const uint8_t *vv)
{
	uint32_t res = 0;
	for (size_t i = 0; i < 4; i++) {
		res |= SHL(vv[i], (i * 8));
	}
	return res;
}

// both GCC and Clang optimize to ((uint32_t*)(vec))[word_idx]
uint32_t getword_from_vec(const uint8_t *vec, size_t word_idx)
{
	size_t byte_idx = word_idx * 4;
	const uint8_t *vv = vec + byte_idx;
	return SHL(vv[3], 24) | SHL(vv[2], 16) | SHL(vv[1], 8) | SHL(vv[0], 0);
}

// neither GCC nor Clang 12.0.1 can optimize this, Clang trunk can
uint32_t getword_from_vec_forloop(const uint8_t *vec, size_t word_idx)
{
	size_t byte_idx = word_idx * 4;
	uint32_t res = 0;
	for (size_t i = 0; i < 4; i++) {
		res |= SHL(vec[byte_idx + i], (i * 8));
	}
	return res;
}
```


---


### compiler : `gcc`
### title : `Redundant max/min operation before vector reduction`
### open_at : `2021-09-28T07:28:16Z`
### last_modified_date : `2021-12-29T06:05:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102512
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
cat test.c

#define MAX(a, b) ((a) > (b) ? (a) : (b))

short
foo1 (short* p)
{
  short max = p[0];
  for (int i = 0; i != 8; i++)
    max = MAX(max, p[i]);
  return max;
}

short
foo2 (short* p)
{
  short max = p[0];
  for (int i = 1; i != 8; i++)
    max = MAX(max, p[i]);
  return max;
}

gcc -O3 -mavx2 -S 



in foo1 the first MAX_EXPR <_10, vect__4.7_13> is redundant since it's contained by the latter .REDUC_MAX.
in foo2 vectorizer failed to recognize .REDUC_MAX pattern. 

;; Function foo1 (foo1, funcdef_no=0, decl_uid=2991, cgraph_uid=1, symbol_order=0)

.248t.optimized
short int foo1 (short int * p)
{
  vector(8) short int vect_max_11.8;
  vector(8) short int vect__4.7;
  short int max;
  vector(8) short int _10;
  short int _20;

  <bb 2> [local count: 119292720]:
  max_9 = *p_8(D);
  _10 = {max_9, max_9, max_9, max_9, max_9, max_9, max_9, max_9};
  vect__4.7_13 = MEM <vector(8) short int> [(short int *)p_8(D)];
  vect_max_11.8_14 = MAX_EXPR <_10, vect__4.7_13>;
  _20 = .REDUC_MAX (vect_max_11.8_14); [tail call]
  return _20;

}



;; Function foo2 (foo2, funcdef_no=1, decl_uid=3000, cgraph_uid=2, symbol_order=1)

short int foo2 (short int * p)
{
  short int stmp_max_11.21;
  vector(4) short int vect_max_11.20;
  vector(4) short int vect__4.19;
  short int max;
  short int _4;
  short int _25;
  vector(4) short int _30;
  short int _34;
  vector(4) short int _38;
  vector(4) short int _39;
  vector(4) short int _40;
  vector(4) short int _41;
  short int _44;
  short int _46;

  <bb 2> [local count: 268435454]:
  max_9 = *p_8(D);
  _30 = {max_9, max_9, max_9, max_9};
  vect__4.19_35 = MEM <vector(4) short int> [(short int *)p_8(D) + 2B];
  vect_max_11.20_36 = MAX_EXPR <_30, vect__4.19_35>;
  _38 = VEC_PERM_EXPR <vect_max_11.20_36, { 0, 0, 0, 0 }, { 2, 3, 4, 5 }>;
  _39 = MAX_EXPR <vect_max_11.20_36, _38>;
  _40 = VEC_PERM_EXPR <_39, { 0, 0, 0, 0 }, { 1, 2, 3, 4 }>;
  _41 = MAX_EXPR <_39, _40>;
  stmp_max_11.21_42 = BIT_FIELD_REF <_41, 16, 0>;
  _4 = MEM[(short int *)p_8(D) + 10B];
  _46 = MEM[(short int *)p_8(D) + 12B];
  _34 = MAX_EXPR <_4, _46>;
  _25 = MEM[(short int *)p_8(D) + 14B];
  _44 = MAX_EXPR <_25, stmp_max_11.21_42>;
  max_26 = MAX_EXPR <_34, _44>;
  return max_26;

}


---


### compiler : `gcc`
### title : `[12 regression] regressions on aarch64 since r12-3899`
### open_at : `2021-09-28T15:05:46Z`
### last_modified_date : `2022-01-20T15:49:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102517
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Since r12-3899 (g:d06dc8a2c73735e9496f434787ba4c93ceee5eea), I have noticed regressions on aarch64:

FAIL: gcc:gcc.dg/dg.exp=gcc.dg/out-of-bounds-1.c  (test for warnings, line 12)

gcc.dg/pr78408-1.c: pattern found 15 times
FAIL: gcc:gcc.dg/dg.exp=gcc.dg/pr78408-1.c scan-tree-dump-times fab1 "after previous" 17

FAIL: gcc:gcc.target/aarch64/aarch64.exp=gcc.target/aarch64/cpymem-q-reg_1.c scan-assembler ldp\\tq[0-9]*
FAIL: gcc:gcc.target/aarch64/aarch64.exp=gcc.target/aarch64/cpymem-q-reg_1.c scan-assembler stp\\tq[0-9]*


---


### compiler : `gcc`
### title : `[12/13 Regression] Dead Code Elimination Regression at -O3 since r12-476-gd846f225c25c5885`
### open_at : `2021-09-30T09:27:06Z`
### last_modified_date : `2022-10-13T17:03:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102540
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c
static long a;
static unsigned b;
void foo(void);
int main() {
    long c, e;
    c = b = a;
    e = c ? 2 / (c + 1) : 0;
    if (e && !b)
        foo();
    a = 0;
}

11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11 -v
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (GCC)

gcc-11 test.c -S -O3
cat test.s
	.file	"test.c"
	.text
	.section	.text.startup,"ax",@progbits
	.p2align 4
	.globl	main
	.type	main, @function
main:
.LFB0:
	.cfi_startproc
	movq	$0, a(%rip)
	xorl	%eax, %eax
	ret
	.cfi_endproc
.LFE0:
	.size	main, .-main
	.local	a
	.comm	a,8,8
	.ident	"GCC: (GNU) 11.2.0"
	.section	.note.GNU-stack,"",@progbits


gcc-trunk -v
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ 
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20210930 (experimental) (GCC)

gcc-trunk test.c -S -O3
cat test.s
	.file	"test.c"
	.text
	.section	.text.startup,"ax",@progbits
	.p2align 4
	.globl	main
	.type	main, @function
main:
.LFB0:
	.cfi_startproc
	movq	a(%rip), %rcx
	movq	%rcx, %rsi
	andl	$4294967295, %esi
	je	.L13
	movl	$2, %eax
	addq	$1, %rsi
	cqto
	idivq	%rsi
	testb	$1, %al
	je	.L13
	testl	%ecx, %ecx
	je	.L17
.L13:
	movq	$0, a(%rip)
	xorl	%eax, %eax
	ret
.L17:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rdx, a(%rip)
	popq	%rcx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:
	.size	main, .-main
	.local	a
	.comm	a,8,8
	.ident	"GCC: (GNU) 12.0.0 20210930 (experimental)"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `[12 Regregression] Missed Dead Code Elimination regression (trunk vs 11.2.0) at -O3`
### open_at : `2021-09-30T13:22:17Z`
### last_modified_date : `2021-10-12T13:35:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102546
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat missed_case.c
static int a;
static char b, c, d;
void bar(void);
void foo(void);

int main() {
    int f = 0;
    for (; f <= 5; f++) {
        bar();
        b = b && f;
        d = f << f;
        if (!(a >= d || f))
            foo();
        c = 1;
        for (; c; c = 0)
            ;
    }
}


11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-trunk -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ 
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20210930 (experimental) (GCC)

gcc-trunk -O3 -S missed_case.c

main:
.LFB0:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	xorl	%ebx, %ebx
.L6:
	call	bar
	cmpb	$0, b(%rip)
	je	.L2
	testl	%ebx, %ebx
	jne	.L5
	movb	$0, b(%rip)
	movl	$1, %ebx
	movb	$0, c(%rip)
	call	bar
	cmpb	$0, b(%rip)
	je	.L4
.L5:
	movb	$1, b(%rip)
.L4:
	addl	$1, %ebx
	movb	$0, c(%rip)
	cmpl	$6, %ebx
	jne	.L6
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L2:
	.cfi_restore_state
	movl	%ebx, %eax
	movl	%ebx, %ecx
	sall	%cl, %eax
	testb	%al, %al
	jle	.L4
	testl	%ebx, %ebx
	jne	.L4
	call	foo
	movb	$0, c(%rip)
	movl	$1, %ebx
	call	bar
	cmpb	$0, b(%rip)
	jne	.L5
	jmp	.L2
	.cfi_endproc


gcc-11 -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ 
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (GCC)

cat missed_case.s

main:
.LFB0:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$5, %ebx
	call	bar
	movb	$0, b(%rip)
	movb	$0, c(%rip)
	.p2align 4,,10
	.p2align 3
.L2:
	call	bar
	cmpb	$0, b(%rip)
	movb	$0, c(%rip)
	setne	b(%rip)
	subl	$1, %ebx
	jne	.L2
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc


---


### compiler : `gcc`
### title : `equality comparison of a [static N] parameter to null not folded`
### open_at : `2021-10-01T16:38:04Z`
### last_modified_date : `2023-08-03T21:04:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102556
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The C99 [static N] array notation in a function parameter indicates that the caller must provide as an argument an array with at least N element.  Therefore, in the body of the function, the parameter may be assumed to be nonnull, the same way as if it had been declared with attribute nonnull.

The test case below shows that GCC fails to take advantage of this to fold pointless comparisons of such parameters to null, even though it does make use of the equivalent guarantee provided by the attribute.

In contrast, Clang folds the expression in both functions to false.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
__attribute__ ((nonnull)) int f (int *a)
{
  return a == 0;   // folded to false with a warning (good)
}

int g (int a[static 1])
{
  return a == 0;   // not folded, missing warning
}
z.c: In function ‘f’:
z.c:3:12: warning: ‘nonnull’ argument ‘a’ compared to NULL [-Wnonnull-compare]
    3 |   return a == 0;   // folded to false with a warning (good)
      |          ~~^~~~

;; Function f (f, funcdef_no=0, decl_uid=1978, cgraph_uid=1, symbol_order=0)

__attribute__((nonnull))
int f (int * a)
{
  <bb 2> [local count: 1073741824]:
  return 0;

}



;; Function g (g, funcdef_no=1, decl_uid=1981, cgraph_uid=2, symbol_order=1)

__attribute__((access ("^0[s1]", )))
int g (int * a)
{
  _Bool _1;
  int _3;

  <bb 2> [local count: 1073741824]:
  _1 = a_2(D) == 0B;
  _3 = (int) _1;
  return _3;

}


---


### compiler : `gcc`
### title : `Missed loop vectorization with reduction and ptr load/store inside loop`
### open_at : `2021-10-02T10:07:39Z`
### last_modified_date : `2021-10-04T07:06:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102564
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
void test1(int *p, int *t, int N) {
    for (int i = 0; i != N; i++) *t += p[i];
}

void test2(int *p, int *t, int N) {
    if (N > 1024) // hint, N is not small
        for (int i = 0; i != N; i++) *t += p[i];
}

void test3(int *p, int *t, int N) {
    if (N > 1024) { // hint, N is not small
        int s = 0;
        for (int i = 0; i != N; i++) s += p[i];
        *t += s;
    }
}

test3 is successfully vectorized with LLVM, GCC, ICC. Sadly, only ICC can catch test1 and test2.

https://godbolt.org/z/PzoYd4eEK


---


### compiler : `gcc`
### title : `[i386] GCC should emit LOCK BTS for simple bit-test-and-set operations with std::atomic`
### open_at : `2021-10-02T16:08:46Z`
### last_modified_date : `2023-01-18T22:30:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102566
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Simple test:

$ cat test.cpp
#include <atomic>
bool tbit(std::atomic<int> &i)
{
    return i.fetch_or(1, std::memory_order_relaxed) & 1;
}

The sequence x.fetch_or(singlebit_constant) & singlebit_constant can be implemented by a LOCK BTS sequence. The above should emit:

    lock bts $1, (%rdi)
    setb %al
    ret

But instead it emits a cmpxchg loop - see https://gcc.godbolt.org/z/99enKaffa.

This was found reviewing MariaDB lightweight-mutex code, which uses the sign bit to indicate a contended mutex. See this commit[1] by one of their maintainers for the removal of fetch_or because it emits an extra loop.

Bonus: LOCK BTR can be used in the sequence x.fetch_and(~single_bit_constant) & single_bit_constant

[1] https://github.com/dr-m/atomic_sync/commit/d5e22b2d42cdbac7a15d242bf1446377555c4041


---


### compiler : `gcc`
### title : `Missed redudant add with add_overflow on the tree level`
### open_at : `2021-10-02T22:22:18Z`
### last_modified_date : `2021-10-04T07:11:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102569
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f1(unsigned x, unsigned y, unsigned *res)
{
    bool t = __builtin_add_overflow(x, y, res);
    *res -= (x+y);
    return t;
}
----- CUT ----
We currently get:
  _7 = .ADD_OVERFLOW (x_5(D), y_6(D));
  _1 = REALPART_EXPR <_7>;
  _2 = IMAGPART_EXPR <_7>;
  t_10 = (bool) _2;
  _3 = x_5(D) + y_6(D);
  _4 = _1 - _3;

But _3 and _1 are the same _4 should be optimized to 0 as it is on the RTL level.

The way I see this being done is during the late fre add x+y in the table for REALPART_EXPR<_7> and it will be done correctly.


---


### compiler : `gcc`
### title : `missed fully redudant with internal function of add_overflow in FRE`
### open_at : `2021-10-02T22:53:46Z`
### last_modified_date : `2021-10-04T14:54:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102570
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f1(unsigned x, unsigned y, unsigned *res)
{
    bool t = __builtin_add_overflow(x, y, res);
    unsigned res1;
    bool t1 = __builtin_add_overflow(x, y, &res1);
    *res -= res1;
    return t==t1;
}

This should be optimized at fre1 but currently takes into DOM to optimize it to just *res = 0; return 1;

FRE does not handle some of the internal functions.

Note this is different from PR 102569 but related.


---


### compiler : `gcc`
### title : `Failure to optimize double _Complex stores to use largest loads/stores possible`
### open_at : `2021-10-03T14:01:34Z`
### last_modified_date : `2023-09-21T08:08:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102575
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
void test(double _Complex *a)
{
    a[0] = 1;
    a[1] = 1;
}

With -O3, on AMD64 GCC outputs this:

test(double _Complex*):
        movsd   xmm1, QWORD PTR .LC0[rip]
        movsd   xmm0, QWORD PTR .LC0[rip+8]
        movsd   QWORD PTR [rdi], xmm1
        movsd   QWORD PTR [rdi+8], xmm0
        movsd   QWORD PTR [rdi+16], xmm1
        movsd   QWORD PTR [rdi+24], xmm0
        ret

Clang instead outputs this:

test(double _Complex*):
        movsd   xmm0, qword ptr [rip + .LCPI0_0] # xmm0 = mem[0],zero
        movups  xmmword ptr [rdi], xmm0
        movups  xmmword ptr [rdi + 16], xmm0
        ret

It seems to me like the second output should always be faster.

PS: The difference is even larger with `-mavx2`.


---


### compiler : `gcc`
### title : `Failure to optimize out allocation if volatile read to a local is present in the middle`
### open_at : `2021-10-03T17:58:05Z`
### last_modified_date : `2023-09-21T08:05:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102579
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
void test_unused() {
  volatile int d;
  int *p = new int;
  d;
  delete p;
}

This can be optimized to just reading `d` once. LLVM does this optimization, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to optimize signed division to unsigned division when dividend can't be negative`
### open_at : `2021-10-03T20:13:56Z`
### last_modified_date : `2023-09-21T08:05:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102580
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
int f(int x) {
  if (x < 0) 
    __builtin_abort();
  return x/3;
}

The `return` statement can be optimized to `return (unsigned)x/3;`. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[x86] Failure to optimize 32-byte integer vector conversion to 16-byte float vector properly when converting upper part with -mavx2`
### open_at : `2021-10-04T02:57:04Z`
### last_modified_date : `2023-09-21T07:58:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102583
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
typedef int v8si __attribute__((vector_size(32)));
typedef float v4sf __attribute__((vector_size(16)));

v4sf high (v8si *srcp)
{
  v8si src = *srcp;
  return (v4sf) { (float)src[4], (float)src[5], (float)src[6], (float)src[7] };
}

With -O3 -mavx2, GCC outputs this:

high(int __vector(8)*):
        vmovdqa ymm0, YMMWORD PTR [rdi]
        vperm2i128      ymm0, ymm0, ymm0, 17
        vcvtdq2ps       xmm0, xmm0
        vzeroupper
        ret

LLVM instead outputs this:

high(int __vector(8)*):
        vcvtdq2ps       xmm0, xmmword ptr [rdi + 16]
        ret

And GCC outputs the equivalent code if -mavx2 is removed:

high(int __vector(8)*):
        cvtdq2ps        xmm0, XMMWORD PTR [rdi+16]
        ret


---


### compiler : `gcc`
### title : `Failure to optimize search for value in vector-sized area to use SIMD`
### open_at : `2021-10-04T12:12:24Z`
### last_modified_date : `2023-09-21T07:56:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102591
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
bool match8(char *tpl) 
{
    int found = 0;
    for (int at = 0; at < 16; at++)
        if (tpl[at] == 0)
            found = 1;
    return found;
}

This function can be greatly optimized by using SIMD. It can be optimized to something like this:

typedef char v16i8 __attribute__((vector_size(16)));

bool match8v2(char *tpl)
{
    v16i8 values;
    __builtin_memcpy(&values, tpl, 16);
    v16i8 compared = (values == 0);
    return _mm_movemask_epi8((__m128i)compared) != 0;
}

This optimization is done by LLVM, but not by GCC.

PS: I've marked this as an x86 bug, but only because I could not find a portable way of expressing `_mm_movemask_epi8((__m128i)compared)`, I would assume other architectures have similar ways of expressing the same thing cheaply.

(For example, Altivec should be able to implement that operation with a `vec_extract(vec_vbpermq((__vector unsigned char)compared, perm), 1)` with `perm` looking like this: `{120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 0}` and the 1 replaced with 14 on big-endian)


---


### compiler : `gcc`
### title : `[12 Regression] large performance changes between 1932e1169a236849f5e7f1cd386da100d9af470f and 9cfb95f9b92326e86e99b50350ebf04fa9cd2477 (probably jump threading)`
### open_at : `2021-10-08T11:01:49Z`
### last_modified_date : `2021-10-16T08:24:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102646
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
There are many regressions and also some improvements especialy in -O2 tsvc (which is simple benchmark) in this range
See https://lnt.opensuse.org/db_default/v4/CPP/latest_runs_report?younger_in_days=14&older_in_days=0&min_percentage_change=0.02&revisions=1932e1169a236849f5e7f1cd386da100d9af470f%2C9cfb95f9b92326e86e99b50350ebf04fa9cd2477

and also https://lnt.opensuse.org/db_default/v4/SPEC/latest_runs_report?younger_in_days=14&older_in_days=0&min_percentage_change=0.02&revisions=1932e1169a236849f5e7f1cd386da100d9af470f%2C9cfb95f9b92326e86e99b50350ebf04fa9cd2477

We get:

+2021-09-27  Aldy Hernandez  <aldyh@redhat.com>
+
+       * gimple-range-path.cc
+       (path_range_query::precompute_ranges_in_block): Rename to...
+       (path_range_query::compute_ranges_in_block): ...this.
+       (path_range_query::precompute_ranges): Rename to...
+       (path_range_query::compute_ranges): ...this.
+       (path_range_query::precompute_relations): Rename to...
+       (path_range_query::compute_relations): ...this.
+       (path_range_query::precompute_phi_relations): Rename to...
+       (path_range_query::compute_phi_relations): ...this.
+       * gimple-range-path.h: Rename precompute* to compute*.
+       * tree-ssa-threadbackward.c
+       (back_threader::find_taken_edge_switch): Same.
+       (back_threader::find_taken_edge_cond): Same.
+       * tree-ssa-threadedge.c
+       (hybrid_jt_simplifier::compute_ranges_from_state): Same.
+       (hybrid_jt_state::register_equivs_stmt): Inline...
+       * tree-ssa-threadedge.h: ...here.
+
+2021-09-27  Aldy Hernandez  <aldyh@redhat.com>
+
+       * tree-vrp.c (lhs_of_dominating_assert): Remove.
+       (class vrp_jt_state): Remove.
+       (class vrp_jt_simplifier): Remove.
+       (vrp_jt_simplifier::simplify): Remove.
+       (class vrp_jump_threader): Remove.
+       (vrp_jump_threader::vrp_jump_threader): Remove.
+       (vrp_jump_threader::~vrp_jump_threader): Remove.
+       (vrp_jump_threader::before_dom_children): Remove.
+       (vrp_jump_threader::after_dom_children): Remove.
+
+2021-09-27  Aldy Hernandez  <aldyh@redhat.com>
+
+       * passes.def (pass_vrp_threader): New.
+       * tree-pass.h (make_pass_vrp_threader): Add make_pass_vrp_threader.
+       * tree-ssa-threadedge.c (hybrid_jt_state::register_equivs_stmt): New.
+       (hybrid_jt_simplifier::hybrid_jt_simplifier): New.
+       (hybrid_jt_simplifier::simplify): New.
+       (hybrid_jt_simplifier::compute_ranges_from_state): New.
+       * tree-ssa-threadedge.h (class hybrid_jt_state): New.
+       (class hybrid_jt_simplifier): New.
+       * tree-vrp.c (execute_vrp): Remove ASSERT_EXPR based jump
+       threader.
+       (class hybrid_threader): New.
+       (hybrid_threader::hybrid_threader): New.
+       (hybrid_threader::~hybrid_threader): New.
+       (hybrid_threader::before_dom_children): New.
+       (hybrid_threader::after_dom_children): New.
+       (execute_vrp_threader): New.
+       (class pass_vrp_threader): New.
+       (make_pass_vrp_threader): New.
+
+2021-09-27  Martin Liska  <mliska@suse.cz>
+
+       * output.h (enum section_flag): New.
+       (SECTION_FORGET): Remove.
+       (SECTION_ENTSIZE): Make it (1UL << 8) - 1.
+       (SECTION_STYLE_MASK): Define it based on other enum
+       values.
+       * varasm.c (switch_to_section): Remove unused handling of
+       SECTION_FORGET.
+
+2021-09-27  Martin Liska  <mliska@suse.cz>
+
+       * common.opt: Add new variable flag_default_complex_method.
+       * opts.c (finish_options): Handle flags related to
+         x_flag_complex_method.
+       * toplev.c (process_options): Remove option handling related
+       to flag_complex_method.
+
+2021-09-27  Richard Biener  <rguenther@suse.de>
+
+       PR middle-end/102450
+       * gimple-fold.c (gimple_fold_builtin_memory_op): Avoid using
+       type_for_size, instead use int_mode_for_size.
+
+2021-09-27  Andrew Pinski  <apinski@marvell.com>
+
+       PR c/94726
+       * gimplify.c (gimplify_save_expr): Return early
+       if the type of val is error_mark_node.
+
+2021-09-27  Aldy Hernandez  <aldyh@redhat.com>
+
+       * tree-ssanames.c (ssa_name_has_boolean_range): Use
+       get_range_query.
+
+2021-09-27  Aldy Hernandez  <aldyh@redhat.com>
+
+       * gimple-ssa-evrp-analyze.h (class evrp_range_analyzer): Remove
+       vrp_visit_cond_stmt.
+       * tree-ssa-dom.c (cprop_operand): Convert to range_query API.
+       (cprop_into_stmt): Same.
+       (dom_opt_dom_walker::optimize_stmt): Same.
+
+2021-09-27  Richard Biener  <rguenther@suse.de>
+
+       PR tree-optimization/97351
+       PR tree-optimization/97352
+       PR tree-optimization/82426
+       * tree-vectorizer.h (dr_misalignment): Add vector type
+       argument.
+       (aligned_access_p): Likewise.
+       (known_alignment_for_access_p): Likewise.
+       (vect_supportable_dr_alignment): Likewise.
+       (vect_known_alignment_in_bytes): Likewise.  Refactor.
+       (DR_MISALIGNMENT): Remove.
+       (vect_update_shared_vectype): Likewise.
+       * tree-vect-data-refs.c (dr_misalignment): Refactor, handle
+       a vector type with larger alignment requirement and apply
+       the negative step adjustment here.
+       (vect_calculate_target_alignment): Remove.
+       * tree-vect-stmts.c (vect_analyze_stmt): Push/pop the
+       vector type of an SLP node to the representative stmt-info.
+       (vect_transform_stmt): Likewise.
+
+2021-09-27  liuhongt  <hongtao.liu@intel.com>
+
+       Revert:
+       2021-09-09  liuhongt  <hongtao.liu@intel.com>
+
+       PR target/101059
+       * config/i386/sse.md (reduc_plus_scal_<mode>): Split to ..
+       (reduc_plus_scal_v4sf): .. this, New define_expand.
+       (reduc_plus_scal_v2df): .. and this, New define_expand.
+

From which (since it reproduces at -O2 and thus is not vectorization related) it seems that it can be attributed to the new threader.

Honza


---


### compiler : `gcc`
### title : `[12 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0) since r12-2381-g704e8a825c78b9a8`
### open_at : `2021-10-08T13:35:44Z`
### last_modified_date : `2021-11-25T14:03:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102648
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat case.c   
void foo(void);
static int b, c, d = 40000;
static short e;
unsigned char f = 1;
static short a(short f, short g) { return f * g; }
int main() {
    for (; e; e++) {
        c = 0;
        while (c - 1)
            if (a(d, !f) > 1)
                foo();
    }
    return 0;
}

11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11 -v
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (GCC)


gcc-11 -O3 case.c -S -o /dev/stdout
...
main:
.LFB1:
	.cfi_startproc
	cmpw	$0, e(%rip)
	je	.L2
	movl	$0, c(%rip)
.L3:
	jmp	.L3
	.p2align 4,,10
	.p2align 3
.L2:
	xorl	%eax, %eax
	ret

gcc-trunk -v
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ 
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211008 (experimental) (GCC)

gcc-trunk -O3 case.c -S -o /dev/stdout
...
main:
.LFB1:
	.cfi_startproc
	cmpw	$0, e(%rip)
	movzbl	f(%rip), %edx
	je	.L12
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	.p2align 4,,10
	.p2align 3
.L5:
	movl	$0, c(%rip)
	xorl	%eax, %eax
	testb	%dl, %dl
	sete	%al
	imulw	$-25536, %ax, %ax
	cmpw	$1, %ax
	jg	.L15
.L3:
.L10:
	jmp	.L10
	.p2align 4,,10
	.p2align 3
.L15:
	call	foo
	cmpl	$1, c(%rip)
	movzbl	f(%rip), %edx
	jne	.L3
	addw	$1, e(%rip)
	jne	.L5
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
.L12:
	xorl	%eax, %eax
	ret


704e8a825c78b9a8424c291509413bbb48e602c7 introduced this regression


---


### compiler : `gcc`
### title : `[12/13 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-10-08T13:47:15Z`
### last_modified_date : `2022-11-03T19:33:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102650
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat case.c                                                                            bisections
static int a = 2, b, c, d;
void foo(void);
int main() {
    short e;
    int f = -1;
    if (b)
        c = 0;
    c || (f = 2);
    for (; d < 1; d++)
        e = f + a;
    if (!e)
        foo();
    return 0;
}


11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11 -v
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (GCC)

gcc-11 -O3 case.c -S -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	movl	d(%rip), %eax
	testl	%eax, %eax
	jg	.L2
	movl	$1, d(%rip)
.L2:
	xorl	%eax, %eax
	ret

gcc-trunk -v
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ 
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211008 (experimental) (GCC)

gcc-trunk -O3 case.c -S -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	movl	d(%rip), %ecx
	testl	%ecx, %ecx
	jg	.L3
	movl	$1, d(%rip)
	xorl	%eax, %eax
	ret
.L3:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret


18b88412069f51433e1b4f440d3c035bfc7b5cca (https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=18b88412069f51433e1b4f440d3c035bfc7b5cca) introduced this regression


---


### compiler : `gcc`
### title : `Unnecessary zeroing out of local ARM NEON arrays`
### open_at : `2021-10-08T15:18:46Z`
### last_modified_date : `2021-10-08T20:59:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102652
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.2.0`
### severity : `enhancement`
### contents :
Created attachment 51567
Testcase to reproduce the bug. Sorry for gzipping it, but if uncompressed, it exceeds the 1 MB file size limit.

This is my first time reporting a compiler bug, so please be kind to me if I made any mistakes. In particular, I'm not sure if tree-optimization is the correct component.

Consider the attached code, briefly reproduced next, which is a minimal testcase obtained from many instances of more complex code in use in an application of mine:

/* START CODE */
#include <arm_neon.h>

void bug(int8_t *out, const int8_t *in) {
    for (int i = 0; i < 2; i++) {
        int8x16x4_t x;

        x.val[0] = vld1q_s8(&in[16 * i]);
        x.val[1] = x.val[2] = x.val[3] = vshrq_n_s8(x.val[0], 7);

        vst4q_s8(&out[64 * i], x);
    }
}
/* END CODE */

This is the assembly output of this code:

0000000000000000 <bug>:
   0:	d10203ff 	sub	sp, sp, #0x80
   4:	d2800009 	mov	x9, #0x0                   	// #0
   8:	d2800008 	mov	x8, #0x0                   	// #0
   c:	d2800007 	mov	x7, #0x0                   	// #0
  10:	d2800006 	mov	x6, #0x0                   	// #0
  14:	d2800005 	mov	x5, #0x0                   	// #0
  18:	d2800004 	mov	x4, #0x0                   	// #0
  1c:	d2800003 	mov	x3, #0x0                   	// #0
  20:	a90023e9 	stp	x9, x8, [sp]
  24:	d2800002 	mov	x2, #0x0                   	// #0
  28:	a9011be7 	stp	x7, x6, [sp, #16]
  2c:	a90213e5 	stp	x5, x4, [sp, #32]
  30:	f9001be3 	str	x3, [sp, #48]
  34:	3dc00020 	ldr	q0, [x1]
  38:	a903a7e2 	stp	x2, x9, [sp, #56]
  3c:	a9049fe8 	stp	x8, x7, [sp, #72]
  40:	4f090404 	sshr	v4.16b, v0.16b, #7
  44:	3d8003e0 	str	q0, [sp]
  48:	4c4023e0 	ld1	{v0.16b-v3.16b}, [sp]
  4c:	a90597e6 	stp	x6, x5, [sp, #88]
  50:	4ea41c81 	mov	v1.16b, v4.16b
  54:	a9068fe4 	stp	x4, x3, [sp, #104]
  58:	4ea41c82 	mov	v2.16b, v4.16b
  5c:	f9003fe2 	str	x2, [sp, #120]
  60:	4ea41c83 	mov	v3.16b, v4.16b
  64:	4c9f0000 	st4	{v0.16b-v3.16b}, [x0], #64
  68:	3dc00424 	ldr	q4, [x1, #16]
  6c:	910103e1 	add	x1, sp, #0x40
  70:	3d8013e4 	str	q4, [sp, #64]
  74:	4f090484 	sshr	v4.16b, v4.16b, #7
  78:	4c402020 	ld1	{v0.16b-v3.16b}, [x1]
  7c:	4ea41c81 	mov	v1.16b, v4.16b
  80:	4ea41c82 	mov	v2.16b, v4.16b
  84:	4ea41c83 	mov	v3.16b, v4.16b
  88:	4c000000 	st4	{v0.16b-v3.16b}, [x0]
  8c:	910203ff 	add	sp, sp, #0x80
  90:	d65f03c0 	ret

It can be seen that the generated code attemps to zero out the variable "x", which I understand is, first of all, uncalled for (seeing as it's local to function bug and not in the global scope), and even if it were necessary, it has no effect anyway since these variables are initialized later.

Many registers are redundantly zeroed (at addresses 4-1c and 24) which are then stored in the stack (at addresses 20, 28-30, 38, 3c, 4c, 54 and 5c). None of these instructions were required to be generated. The zeroed out values are loaded in addresses 48 and 78, but 3 out of the 4 registers (v1, v2, v3) are immediately overwritten, in addresses 50, 58 and 60 for the first load, and 7c-84 for the second load. For the remaining register that is loaded (v0), an unnecessary and redundant trip to memory is performed: for the first iteration of the loop, q0 is loaded at address 34, stored at address 44 and reloaded with the same value in address 48. The second and third instructions could just be removed. For the second iteration, a a load is performed in address 68, followed by a store in address 70 and another load in address 78. Again, the second and third instructions could be removed, so long as the destination register of the instruction in address 68, and the source register of the instruction in address 74, were both changed to q0.

In total, it appears that 24 out of 37 instructions could be removed from the generated code without any change of behavior, many of which are fairly expensive as they involve trips to memory. Thus, I estimate a speedup on the order of 3x if this issue were fixed.

Note that the "-mcpu=native" and "-mtune=native" do not make the issue go away.

This issue only appears to happen for small loops that can be fully unrolled. If the loop iteration count is unknown at compile-time, or if a larger iteration count is used such as 32, the issue goes away, as seen in the following assembly output:

0000000000000000 <bug>:
   0:	91080022 	add	x2, x1, #0x200
   4:	d503201f 	nop
   8:	3cc10424 	ldr	q4, [x1], #16
   c:	4ea41c80 	mov	v0.16b, v4.16b
  10:	4f090484 	sshr	v4.16b, v4.16b, #7
  14:	4ea41c81 	mov	v1.16b, v4.16b
  18:	4ea41c82 	mov	v2.16b, v4.16b
  1c:	4ea41c83 	mov	v3.16b, v4.16b
  20:	4c9f0000 	st4	{v0.16b-v3.16b}, [x0], #64
  24:	eb02003f 	cmp	x1, x2
  28:	54ffff01 	b.ne	8 <bug+0x8>  // b.any
  2c:	d65f03c0 	ret
        
However, even this code could be improved to something like this (manually written, untested modification):

0000000000000000 <bug>:
   0:               add	x2, x1, #0x200
   4:               nop
   8:               ldr	q0, [x1], #16
   c:               sshr	v1.16b, v0.16b, #7
  10:               mov	v2.16b, v1.16b
  14:               mov	v3.16b, v1.16b
  18:               st4	{v0.16b-v3.16b}, [x0], #64
  1c:               cmp	x1, x2
  20:               b.ne	8 <bug+0x8>  // b.any
  24:               ret

It appears gcc is trying to avoid using the v0-v3 registers elsewhere, i.e. in the load and shift instructions.

For completeness, here is the output of "gcc-11 -v -save-temps -O3 -c -o bug.o bug.c":

Using built-in specs.
COLLECT_GCC=gcc
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 10.3.0-1ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-10/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-10 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 10.3.0 (Ubuntu 10.3.0-1ubuntu1) 
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-c' '-o' 'bug.o' '-mlittle-endian' '-mabi=lp64'
 /usr/lib/gcc/aarch64-linux-gnu/10/cc1 -E -quiet -v -imultiarch aarch64-linux-gnu bug.c -mlittle-endian -mabi=lp64 -O3 -fpch-preprocess -fasynchronous-unwind-tables -fstack-protector-strong -Wformat -Wformat-security -fstack-clash-protection -o bug.i
ignoring nonexistent directory "/usr/local/include/aarch64-linux-gnu"
ignoring nonexistent directory "/usr/lib/gcc/aarch64-linux-gnu/10/include-fixed"
ignoring nonexistent directory "/usr/lib/gcc/aarch64-linux-gnu/10/../../../../aarch64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/lib/gcc/aarch64-linux-gnu/10/include
 /usr/local/include
 /usr/include/aarch64-linux-gnu
 /usr/include
End of search list.
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-c' '-o' 'bug.o' '-mlittle-endian' '-mabi=lp64'
 /usr/lib/gcc/aarch64-linux-gnu/10/cc1 -fpreprocessed bug.i -quiet -dumpbase bug.c -mlittle-endian -mabi=lp64 -auxbase-strip bug.o -O3 -version -fasynchronous-unwind-tables -fstack-protector-strong -Wformat -Wformat-security -fstack-clash-protection -o bug.s
GNU C17 (Ubuntu 10.3.0-1ubuntu1) version 10.3.0 (aarch64-linux-gnu)
	compiled by GNU C version 10.3.0, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.0, isl version isl-0.23-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
GNU C17 (Ubuntu 10.3.0-1ubuntu1) version 10.3.0 (aarch64-linux-gnu)
	compiled by GNU C version 10.3.0, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.0, isl version isl-0.23-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: af83b0a86657149dda0e3a20e47571e2
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-c' '-o' 'bug.o' '-mlittle-endian' '-mabi=lp64'
 as -v -EL -mabi=lp64 -o bug.o bug.s
GNU assembler version 2.37 (aarch64-linux-gnu) using BFD version (GNU Binutils for Ubuntu) 2.37
COMPILER_PATH=/usr/lib/gcc/aarch64-linux-gnu/10/:/usr/lib/gcc/aarch64-linux-gnu/10/:/usr/lib/gcc/aarch64-linux-gnu/:/usr/lib/gcc/aarch64-linux-gnu/10/:/usr/lib/gcc/aarch64-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/aarch64-linux-gnu/10/:/usr/lib/gcc/aarch64-linux-gnu/10/../../../aarch64-linux-gnu/:/usr/lib/gcc/aarch64-linux-gnu/10/../../../../lib/:/lib/aarch64-linux-gnu/:/lib/../lib/:/usr/lib/aarch64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/aarch64-linux-gnu/10/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-c' '-o' 'bug.o' '-mlittle-endian' '-mabi=lp64'



System information:

Raspberry Pi 4 Model B board with 4 GB of RAM. CPU: Broadcom BCM2711 with 4 x Cortex-A72 CPUs. Output of "uname -a": Linux rpi4 5.11.0-1019-raspi #20-Ubuntu SMP PREEMPT Tue Sep 21 15:23:42 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux

Jetson Nano 2 GB board. CPU: Nvidia Tegra X1 with 4 x Cortex-A57 CPUs. Output of "uname -a": Linux jetson-nano 4.9.140-tegra #1 SMP PREEMPT Tue Oct 27 21:02:37 PDT 2020 aarch64 aarch64 aarch64 GNU/Linux



Versions of gcc in which I tried this in the Raspberry Pi 4:

Using built-in specs.
COLLECT_GCC=gcc-7
COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/7/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 7.5.0-6ubuntu4' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu
Thread model: posix
gcc version 7.5.0 (Ubuntu/Linaro 7.5.0-6ubuntu4) 

Using built-in specs.
COLLECT_GCC=gcc-9
COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/9/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.3.0-23ubuntu2' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu
Thread model: posix
gcc version 9.3.0 (Ubuntu 9.3.0-23ubuntu2)

Using built-in specs.
COLLECT_GCC=gcc-10
COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/10/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 10.3.0-1ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-10/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-10 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 10.3.0 (Ubuntu 10.3.0-1ubuntu1) 

Using built-in specs.
COLLECT_GCC=gcc-11
COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 11.2.0-7ubuntu2' --with-bugurl=file:///usr/share/doc/gcc-11/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-11 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-serialization=2
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (Ubuntu 11.2.0-7ubuntu2) 



Versions of gcc in which I tried this in the Jetson Nano:

Using built-in specs.
COLLECT_GCC=gcc-7
COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/7/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 7.5.0-3ubuntu1~18.04' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu
Thread model: posix
gcc version 7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 

Using built-in specs.
COLLECT_GCC=gcc-8
COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/8/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 8.4.0-1ubuntu1~18.04' --with-bugurl=file:///usr/share/doc/gcc-8/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-8 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --disable-libphobos --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu
Thread model: posix
gcc version 8.4.0 (Ubuntu/Linaro 8.4.0-1ubuntu1~18.04)


---


### compiler : `gcc`
### title : `[AArch64] Failure to optimize to using stp instead of 2 strs when possible`
### open_at : `2021-10-09T21:01:34Z`
### last_modified_date : `2023-09-21T07:52:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102672
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
struct X {
    int i;
    void *p;
};

void foo(struct X *q, void *p)
{
    struct X b{};
    b.p = p;
    *q = b;
}

With -O3, GCC outputs this:

foo(X*, void*):
        str     wzr, [x0]
        str     x1, [x0, 8]
        ret

LLVM instead outputs this:

foo(X*, void*):
        stp     xzr, x1, [x0]
        ret


---


### compiler : `gcc`
### title : `Failure to optimize out malloc/nothrow allocation that's only used for bool checking`
### open_at : `2021-10-10T03:07:24Z`
### last_modified_date : `2023-09-21T07:51:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102676
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
#include <new>
#include <stdlib.h>

bool f()
{
  return new(std::nothrow) int;
}

bool g()
{
    return malloc(1);
}

Both these functions can be optimized to `return true;`. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize out 64-bit multiplication to 32-bit multiplication when possible in circumstances involving modifying a 64-bit variable that gets converted to 32-bit`
### open_at : `2021-10-10T16:59:47Z`
### last_modified_date : `2023-09-21T07:50:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102679
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

int32_t mac(int32_t *b, int64_t sqr)
{
    sqr += (int64_t)*b * *b;
    return sqr;
}

This can be optimized to remove the `(int64_t)` cast. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `std::min_element / ranges::min_element does not get optimized away with std::gcd`
### open_at : `2021-10-11T14:39:46Z`
### last_modified_date : `2021-10-12T07:19:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102684
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.1`
### severity : `enhancement`
### contents :
This code which calculates the gcd of the minimum and maximum element of a range of elements gets optimized away entirely with clang to just a `return` statement with `-O3` while gcc (trunk) produces up to 232 lines of assembly depending on the optimization level (https://compiler-explorer.com/z/TYrx5exP3).

#include <algorithm>
#include <numeric>
#include <span>
#include <vector>

namespace r = std::ranges;

template <typename T>
constexpr auto find_gcd(std::span<T> container) {
    return std::gcd(*r::min_element(container),
                    *r::max_element(container));
}

int main() {
    std::vector<int> ints = {2, 4, 6, 10};
    return find_gcd<int>(ints);
}

The same happens when `std::min_element` is used for example. I found one other post regarding `std::min_element` from 2018 about custom predicates but I wasn't sure if it might have the same underlying problem.


---


### compiler : `gcc`
### title : `[12 regression] gcc.dg/ipa/iinline-attr.c fails after r12-4288`
### open_at : `2021-10-11T18:53:24Z`
### last_modified_date : `2021-10-13T13:03:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102688
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
g:177b800f5fc861af1bf8700f050de28dd47ee1aa, r12-4288

FAIL: gcc.dg/ipa/iinline-attr.c scan-ipa-dump inline "hooray[^\\n]*inline copy in test"

Executing on host: /home/seurer/gcc/git/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-test/gcc/ /home/seurer/gcc/git/gcc-test/gcc/testsuite/gcc.dg/ipa/iinline-attr.c    -fdiagnostics-plain-output   -O2 -fdump-ipa-inline -S -o iinline-attr.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/git/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-test/gcc/ /home/seurer/gcc/git/gcc-test/gcc/testsuite/gcc.dg/ipa/iinline-attr.c -fdiagnostics-plain-output -O2 -fdump-ipa-inline -S -o iinline-attr.s^M
PASS: gcc.dg/ipa/iinline-attr.c (test for excess errors)
FAIL: gcc.dg/ipa/iinline-attr.c scan-ipa-dump inline "hooray[^\\n]*inline copy in test"


---


### compiler : `gcc`
### title : `[12 Regression] Dead Code Elimination Regression at -O3 since r12-2591-g2e96b5f14e402569`
### open_at : `2021-10-12T13:34:37Z`
### last_modified_date : `2021-10-21T08:30:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102703
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c
void foo(void);

static int a, b;
static short c;
int main() {
  for (; a; ++a) {
    unsigned short d = a;
    c = d >= 2 ? 0 : 2;
    if (!(b | d) && d)
      foo();
  }
}

gcc-11.2.0 test.c -S -O3 -o /dev/stdout
main:
.LFB0:
	.cfi_startproc
.L2:
	movl	a(%rip), %eax
	testl	%eax, %eax
	je	.L12
	incl	a(%rip)
	jmp	.L2
.L12:
	ret
	.cfi_endproc


gcc-trunk test.c -S -O3 -o /dev/stdout
main:
.LFB0:
	.cfi_startproc
	movl	a(%rip), %eax
	testl	%eax, %eax
	je	.L25
	.p2align 4,,10
	.p2align 3
.L24:
	movzwl	%ax, %edx
	cmpw	$1, %ax
	jbe	.L27
	testl	%edx, %edx
	jne	.L35
.L33:
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
.L4:
	call	foo
	movl	a(%rip), %eax
	addl	$1, %eax
	movl	%eax, a(%rip)
	je	.L36
.L2:
	movzwl	%ax, %edx
	cmpw	$1, %ax
	jbe	.L3
	testl	%edx, %edx
	je	.L4
	addl	$1, %eax
	movl	%eax, a(%rip)
	jne	.L2
.L36:
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L35:
	addl	$1, %eax
	movl	%eax, a(%rip)
	jne	.L24
.L25:
	xorl	%eax, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L27:
	testl	%edx, %edx
	jne	.L30
	testb	$1, %al
	jne	.L33
.L30:
	addl	$1, %eax
	movl	%eax, a(%rip)
	jmp	.L24
	.p2align 4,,10
	.p2align 3
.L3:
	.cfi_def_cfa_offset 16
	testl	%edx, %edx
	jne	.L6
	testb	$1, %al
	jne	.L4
.L6:
	addl	$1, %eax
	movl	%eax, a(%rip)
	jmp	.L2
	.cfi_endproc


gcc-trunk -v   
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211012 (experimental) (GCC)

Started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=2e96b5f14e4025691b57d2301d71aa6092ed44bc (same as https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102546)


---


### compiler : `gcc`
### title : `NRVO for throw expression`
### open_at : `2021-10-12T13:39:18Z`
### last_modified_date : `2021-10-13T08:47:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102704
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `enhancement`
### contents :
Consider this code:

struct mytype
{
    mytype();
    mytype(mytype const&);
    mytype(mytype&&);
};

void test()
{
    mytype e;
    throw e;
}

Currently for function test() GCC generates the following sequence of calls (pseudocode):

    char e[sizeof(mytype)];
    mytype_default_ctor(e);
    p = __cxa_allocate_exception();
    mytype_move_ctor(p, e);
    __cxa_throw(p);

I believe a trick similar to NRVO for returns can be made here. When a variable meets NRVO criteria, compiler can remove the local variable and replace it with a storage allocated by __cxa_allocate_exception. Here what I believe can be generated:

    p = __cxa_allocate_exception();
    mytype_default_ctor(p);
    __cxa_throw(p);


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Dead Code Elimination Regression at -O3 since r12-2637-g145bc41dae7c7bfa093d61e77346f98e6a595a0e`
### open_at : `2021-10-12T13:41:56Z`
### last_modified_date : `2023-07-31T18:12:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102705
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c

void foo(void);
static int b;
static char c;
static short a(short d, short e) { return e == 0 || d && e == 1 ? 0 : d % e; }
int main() {
  b = c = b >= 2 ? 0 : 1 >> b;
  short f = a(0 >= 0 ^ c, 5);
  if (f == c)
    foo();
  a(0, 9);
}

11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11.2.0 test.c -S -O3 -o /dev/stdout
main:
.LFB1:
	.cfi_startproc
	movl	b(%rip), %ecx
	movl	$1, %eax
	sarl	%cl, %eax
	movl	%eax, %edx
	xorl	%eax, %eax
	cmpl	$2, %ecx
	cmovl	%edx, %eax
	movsbl	%al, %eax
	movl	%eax, b(%rip)
	xorl	%eax, %eax
	ret
	.cfi_endproc


gcc-trunk test.c -S -O3 -o /dev/stdout
main:
.LFB1:
	.cfi_startproc
	movl	b(%rip), %ecx
	cmpl	$1, %ecx
	jle	.L2
	xorl	%ecx, %ecx
	movl	%ecx, b(%rip)
.L5:
	xorl	%eax, %eax
	ret
.L2:
	movl	$1, %eax
	sarl	%cl, %eax
	movl	%eax, %edx
	movl	%eax, b(%rip)
	xorl	$1, %edx
	movsbw	%dl, %dx
	cmpw	%ax, %dx
	jne	.L5
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc

gcc-trunk -v   
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211012 (experimental) (GCC)

It started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=145bc41dae7c7bfa093d61e77346f98e6a595a0e


---
