### Total Bugs Detected: 4649
### Current Chunk: 24 of 30
### Bugs in this Chunk: 160 (From bug 3681 to 3840)
---


### compiler : `gcc`
### title : `AArch64 unnecessary use of call-preserved register`
### open_at : `2022-01-19T06:56:54Z`
### last_modified_date : `2022-01-19T09:24:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104110
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
gcc misses an optimization (or in some sense deoptimizes) by using a call-preserved register to save a trivial constant across a function call.

Source code:

void bar(unsigned);
unsigned foo(unsigned c) {
    bar(1U << c);
    return 1;
}

Output from gcc -O3 on AArch64:

foo:
        stp     x29, x30, [sp, -32]!
        mov     x29, sp
        str     x19, [sp, 16]
        mov     w19, 1
        lsl     w0, w19, w0
        bl      bar
        mov     w0, w19
        ldr     x19, [sp, 16]
        ldp     x29, x30, [sp], 32
        ret

Note that x19 is used unnecessarily to save the constant 1 across the function call, causing an unnecessary push and pop.  It would have been better to just use some call-clobbered register for the constant 1 before the function call, and then a simple `mov w0, 1` afterward.\

Same behavior with -O, -O2, -Os.  Tested on godbolt, affects yesterday's trunk and all the way back to 5.4.

Might be related to bug 70801 or bug 71768 but I am not sure.


---


### compiler : `gcc`
### title : `Optimize {FLOOR,CEIL,ROUND}_{DIV,MOD}_EXPR in tree-vect-patterns.cc`
### open_at : `2022-01-19T14:12:59Z`
### last_modified_date : `2022-03-08T16:20:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104116
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #102860 +++

As mentioned in https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102860#c7, we should
try to pattern recognize these operations and turn them into TRUNC_{DIV,MOD}_EXPR
if the optab is supported (note, for constant last operand build on top of the current vect_recog_divmod_pattern, so that we handle it even if the optab isn't supported etc.).
For signed operands,
    r = x %[fl] y;                                                                                                                                                                    
    is                                                                                                                                                                                
    r = x % y; if (r && (x ^ y) < 0) r += y;                                                                                                                                          
    and                                                                                                                                                                               
    d = x /[fl] y;                                                                                                                                                                    
    is                                                                                                                                                                                
    r = x % y; d = x / y; if (r && (x ^ y) < 0) --d;                                                                                                                                  
    and                                                                                                                                                                               
    r = x %[cl] y;                                                                                                                                                                    
    is                                                                                                                                                                                
    r = x % y; if (r && (x ^ y) >= 0) r -= y;                                                                                                                                         
    and                                                                                                                                                                               
    d = /[cl] y;                                                                                                                                                                      
    is                                                                                                                                                                                
    r = x % y; d = x / y; if (r && (x ^ y) >= 0) ++d;                                                                                                                                 
(too lazy to write rounding case or do the unsigned case of %[cl] or /[cl] - ([fl] isn't needed for unsigned)).  Of course, in vect-patterns the ifs should be written as COND_EXPRs and probably the conditional increments or decrements should be done using corresponding unsigned type with casts.


---


### compiler : `gcc`
### title : `On Zen3, 510.parest_r (built with -Ofast) is faster with generic than with native ISA`
### open_at : `2022-01-19T17:20:21Z`
### last_modified_date : `2023-01-18T17:41:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104122
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
On Zen3 based CPUs, benchmark 510.parest_r from the SPEC 2017 FPrate is faster with -march=generic than with -march=native.  LNT reports 11% regression:

  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=463.457.0&plot.1=471.457.0&

However, my own measurements on a different but similar EPYC machine suggest it can be as high as 26%.  On a yet another Ryzen machine I can see almost 10% too.  I only have older-than-LNT data from the Ryzen machine and we did not see the regression when gcc 11 was released.  However it seems that the generic tuning improved while the native one did not.


---


### compiler : `gcc`
### title : `Poor optimization for vector splat DW with small consts`
### open_at : `2022-01-19T17:40:36Z`
### last_modified_date : `2023-07-13T07:22:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104124
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.1.1`
### severity : `normal`
### contents :
It looks to me like the compiler is seeing register pressure caused by loading all the vector long long constants I need in my code. This is leaf code of a size it can run out of volatilizes (no stack-frame). But this puts more pressure on volatile VRs, VSRs, and GPRs. Especially GPRs because it loading from .rodata when it could (and should) use a vector immediate.

For example:

vui64_t
__test_splatudi_0_V0 (void)
{
  return vec_splats ((unsigned long long) 0);
}

vi64_t
__test_splatudi_1_V0 (void)
{
  return vec_splats ((signed long long) -1);
}

Generate:
00000000000001a0 <__test_splatudi_0_V0>:
     1a0:       8c 03 40 10     vspltisw v2,0
     1a4:       20 00 80 4e     blr

00000000000001c0 <__test_splatudi_1_V0>:
     1c0:       8c 03 5f 10     vspltisw v2,-1
     1c4:       20 00 80 4e     blr
        ...

But other cases that could use immedates like:

vui64_t
__test_splatudi_12_V0 (void)
{
  return vec_splats ((unsigned long long) 12);
}

GCC 9/10/11 Generates for power8:

0000000000000170 <__test_splatudi_12_V0>:
     170:       00 00 4c 3c     addis   r2,r12,0
                        170: R_PPC64_REL16_HA   .TOC.
     174:       00 00 42 38     addi    r2,r2,0
                        174: R_PPC64_REL16_LO   .TOC.+0x4
     178:       00 00 22 3d     addis   r9,r2,0
                        178: R_PPC64_TOC16_HA   .rodata.cst16+0x20
     17c:       00 00 29 39     addi    r9,r9,0
                        17c: R_PPC64_TOC16_LO   .rodata.cst16+0x20
     180:       ce 48 40 7c     lvx     v2,0,r9
     184:       20 00 80 4e     blr

and for Power9:
0000000000000000 <__test_splatisd_12_PWR9>:
       0:       d1 62 40 f0     xxspltib vs34,12
       4:       02 16 58 10     vextsb2d v2,v2
       8:       20 00 80 4e     blr

So why can't the power8 target generate:

00000000000000f0 <__test_splatudi_12_V1>:
      f0:       8c 03 4c 10     vspltisw v2,12
      f4:       4e 16 40 10     vupkhsw v2,v2
      f8:       20 00 80 4e     blr

This is 4 cycles vs 9 ((best case) and it is always 9 cycles because GCC does not exploit immediate fusion).
In fact GCC 8 (AT12) does this.

So I tried defining my own vec_splatudi:

vi64_t
__test_splatudi_12_V1 (void)
{
  vi32_t vwi = vec_splat_s32 (12);
  return vec_unpackl (vwi);
}

Which generates the <__test_splatudi_12_V1> sequence above for GCC 8. But for GCC 9/10/11 it generates:

0000000000000110 <__test_splatudi_12_V1>:
     110:       00 00 4c 3c     addis   r2,r12,0
                        110: R_PPC64_REL16_HA   .TOC.
     114:       00 00 42 38     addi    r2,r2,0
                        114: R_PPC64_REL16_LO   .TOC.+0x4
     118:       00 00 22 3d     addis   r9,r2,0
                        118: R_PPC64_TOC16_HA   .rodata.cst16+0x20
     11c:       00 00 29 39     addi    r9,r9,0
                        11c: R_PPC64_TOC16_LO   .rodata.cst16+0x20
     120:       ce 48 40 7c     lvx     v2,0,r9
     124:       20 00 80 4e     blr

Again! GCC has gone out of its way to be this clever! Badly! While it can be appropriately clever for power9!

I have tried many permutations of this and the only way I have found to prevent this (GCC 9/10/11) cleverness is to use inline __asm (which has other bad side effects).


---


### compiler : `gcc`
### title : `Extra instructions generated for dual float return on ARM64.`
### open_at : `2022-01-20T13:05:43Z`
### last_modified_date : `2022-01-21T07:54:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104145
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
In the following code snippet, inefficient code is generated when returning 2 floats on ARM64/AArch64: https://godbolt.org/z/3G8nMT8W4

```
typedef float f32;
typedef double f64;

template <class A, class B = A>
struct duo
{
    A a;
    B b;
};

duo<f32> stream_load2(const f32* p)
{
    f32 a, b;
    asm("ldnp %s0, %s1, %2" : "=w"(a), "=w"(b) : "m"(*(const f32(*)[2])p));
    return {a, b}; // NOTE: many extra instuctions are generated!
}

duo<f32> stream_load2_ldp(const f32* p)
{
    return {p[0], p[1]}; // NOTE: inefficient code is generated for this!
}

duo<f64> stream_load2(const f64* p)
{
    f64 a, b;
    asm("ldnp %d0, %d1, %2" : "=w"(a), "=w"(b) : "m"(*(const f64(*)[2])p));
    return {a, b}; // NOTE: works as expected!
}
```

GCC output (v6.4+):
```
stream_load2(float const*):
        ldnp s1, s0, [x0]
        fmov    w2, s1
        fmov    w0, s0
        mov     x1, 0
        bfi     x1, x2, 0, 32
        bfi     x1, x0, 32, 32
        lsr     x0, x1, 32
        lsr     w1, w1, 0
        fmov    s1, w0
        fmov    s0, w1
        ret

stream_load2_ldp(float const*):
        ldr     d0, [x0]
        fmov    x1, d0
        lsr     x0, x1, 32
        fmov    s1, w0
        lsr     w0, w1, 0
        fmov    s0, w0
        ret

stream_load2(double const*):
        ldnp d0, d1, [x0]
        ret
```

Clang output:
```
stream_load2(float const*):
        ldnp    s0, s1, [x0]
        ret

stream_load2_ldp(float const*):
        ldp     s0, s1, [x0]
        ret

stream_load2(double const*):
        ldnp    d0, d1, [x0]
        ret
```


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] x86: excessive code generated for 128-bit byteswap`
### open_at : `2022-01-20T23:24:36Z`
### last_modified_date : `2023-07-07T10:24:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104151
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Hello, noticed that gcc generates redundant sequence of instructions for code that does 128-bit byteswap implemented with 2 64-bit byteswap intrinsics. I narrowed it to something like this:

__uint128_t bswap(__uint128_t a)
{
    std::uint64_t x[2];
    memcpy(x, &a, 16);
    std::uint64_t y[2];
    y[0] = __builtin_bswap64(x[1]);
    y[1] = __builtin_bswap64(x[0]);
    memcpy(&a, y, 16);
    return a;
}

Produces:
https://godbolt.org/z/hEsPqvhv3

        mov     QWORD PTR [rsp-24], rdi
        mov     QWORD PTR [rsp-16], rsi
        movdqa  xmm0, XMMWORD PTR [rsp-24]
        palignr xmm0, xmm0, 8
        movdqa  xmm1, xmm0
        pshufb  xmm1, XMMWORD PTR .LC0[rip]
        movaps  XMMWORD PTR [rsp-24], xmm1
        mov     rax, QWORD PTR [rsp-24]
        mov     rdx, QWORD PTR [rsp-16]
        ret

Expected (alternatively for simd types - single pshufb, clang can do it):

        mov     rdx, rdi
        mov     rax, rsi
        bswap   rdx
        bswap   rax
        ret


---


### compiler : `gcc`
### title : `[12 Regression] Missed CSE after lowering of &MEM[ptr_1 + CST]`
### open_at : `2022-01-21T13:01:20Z`
### last_modified_date : `2023-07-27T09:22:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104162
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
In the testcase of PR99673 we can see

  <bb 2> [local count: 1073741824]:
  _13 = &MEM[(struct B *)pc_2(D) + 1B].i;
  .ASAN_CHECK (6, _13, 4, 4);
  _1 = MEM[(struct B *)pc_2(D) + 1B].i;
  _14 = &pd_4(D)->i;
  .ASAN_CHECK (7, _14, 4, 4);
  pd_4(D)->i = _1;
  _9 = (sizetype) i_6(D);
  _10 = _9 * 16;
  _11 = _10 + 4;
  _12 = pc_2(D) + 1;
  psa_7 = _12 + _11;
  f (psa_7);

and FRE5 is not CSEing _12 to _13.


---


### compiler : `gcc`
### title : `[12 Regression] -Warray-bounds for unreachable code inlined from std::sort()`
### open_at : `2022-01-21T15:40:24Z`
### last_modified_date : `2023-10-25T22:03:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104165
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
In file included from /home/install/include/c++/12.0.1/algorithm:61,
                 from repro.cpp:1:
In function ‘void std::__final_insertion_sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = int*; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<bar(int, int)::<lambda(int, int)> >]’,
    inlined from ‘void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = int*; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<bar(int, int)::<lambda(int, int)> >]’ at /home/install/include/c++/12.0.1/bits/stl_algo.h:1940:31,
    inlined from ‘void std::sort(_RAIter, _RAIter, _Compare) [with _RAIter = int*; _Compare = bar(int, int)::<lambda(int, int)>]’ at
/home/install/include/c++/12.0.1/bits/stl_algo.h:4853:18,
    inlined from ‘int bar(int, int)’ at repro.cpp:17:14,
    inlined from ‘int foo(int)’ at repro.cpp:25:13:
/home/install/include/c++/12.0.1/bits/stl_algo.h:1849:32: error: array subscript 16 is outside array bounds of ‘unsigned char [16]’ [-Werror=array-bounds]
 1849 |           std::__insertion_sort(__first, __first + int(_S_threshold), __comp);
      |           ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In function ‘int bar(int, int)’,
    inlined from ‘int foo(int)’ at repro.cpp:25:13:
repro.cpp:4:7: note: at offset 64 into object ‘f.140’ of size 16
    4 |   int f[l];
      |       ^
cc1plus: all warnings being treated as errors
make: *** [Makefile:5: all] Error 1

Getting the above bogus warning when compiling following reproducer

#include <algorithm>

static int bar(int n, int l) { // make function non-static and warning goes away
	int f[l];
	int x = 0;
	int r = n;

	for (; x < l;) {
		if (r) {
			x = l;
		} else { // Take out this else and the warning goes away
			r = 1;
		}
	}

	if (r == 1) { // Take out this branch and the warning goes away
		std::sort(f, f + x,
				[](int a, int b) { return a > b; });
	}
	return 1;

}

int foo(int n) {
	return bar(n, 4);
}


Compiled with HEAD of GCC (as of 20/01/2021)
g++ -c -march=armv8-a -Werror -Wall -O2  repro.cpp

The compiler sees that the size of the array is 4 ints (16-bytes), but doesn't see that x <= 4, so that no array out of bounds should occur. Workaround is to use malloc for the array.


---


### compiler : `gcc`
### title : `Fails to optimize nested array indexing p[i/N].data[i%N]`
### open_at : `2022-01-23T16:51:56Z`
### last_modified_date : `2022-01-24T09:34:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104195
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC seems to be unable to optimize some nested array accesses of the form p[i/N].data[i%N] into a simple ((T*)p)[i]. The C test case is

    struct CChunk { int data[4]; };
    int f(struct CChunk *p, unsigned long long i) { return p[i/4].data[i%4]; }

gcc -O2 currently produces:

    movq %rsi, %rax
    andl $3, %esi
    shrq $2, %rax
    salq $4, %rax
    addq %rax, %rdi
    movl (%rdi,%rsi,4), %eax
    ret

but I would prefer it to produce:

    movl (%rdi,%rsi,4), %eax
    retq

A more exhaustive C++ test follows — GCC can optimize a few of these, but not all. (Clang can't optimize any of these; I've just filed https://github.com/llvm/llvm-project/issues/53367 about that.) https://godbolt.org/z/3E1e6c5e3

template<class T, int N>
struct Chunk {
    T data[N];
};

template<class T, int N, class IndexType>
int f(Chunk<T, N> *p, IndexType i) {
    return p[i/N].data[i%N];
}

template int f(Chunk<char,2>*, unsigned long long);  // GCC wins
template int f(Chunk<char,4>*, unsigned long long);  // GCC wins
template int f(Chunk<char,2>*, unsigned);
template int f(Chunk<char,4>*, unsigned);
template int f(Chunk<int,2>*, unsigned long long);  // GCC wins
template int f(Chunk<int,4>*, unsigned long long);
template int f(Chunk<int,2>*, unsigned);
template int f(Chunk<int,4>*, unsigned);


---


### compiler : `gcc`
### title : `[12 regression] gcc.dg/vect/pr81196.c  fails after r12-6844`
### open_at : `2022-01-24T20:15:12Z`
### last_modified_date : `2022-01-26T13:12:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104214
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
g:f1af8528d34418bc874ae9d993ee0dc3559972d2, r12-6844
make  -k check-gcc RUNTESTFLAGS="vect.exp=gcc.dg/vect/pr81196.c"
FAIL: gcc.dg/vect/pr81196.c scan-tree-dump-times vect "vectorized 1 loops" 1
FAIL: gcc.dg/vect/pr81196.c -flto -ffat-lto-objects  scan-tree-dump-times vect "vectorized 1 loops" 1
# of expected passes		2
# of unexpected failures	2

commit f1af8528d34418bc874ae9d993ee0dc3559972d2 (HEAD, refs/bisect/bad)
Author: Richard Biener <rguenther@suse.de>
Date:   Mon Jan 24 11:50:06 2022 +0100

    tree-optimization/102131 - fix niter analysis wrt overflow


---


### compiler : `gcc`
### title : `SLP discovery does not consider swapping comparisons`
### open_at : `2022-01-26T12:43:36Z`
### last_modified_date : `2022-05-02T11:17:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104240
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The following testcase is not vectorized because of a < vs > operation mismatch.

void foo (int *c, float *x, float *y)
{
  c[0] = x[0] < y[0];
  c[1] = y[1] > x[1];
  c[2] = x[2] < y[2];
  c[3] = x[3] < y[3];
}


---


### compiler : `gcc`
### title : `[i386] GCC may want to use 32-bit (I)DIV if it can for 64-bit operands`
### open_at : `2022-01-26T18:30:09Z`
### last_modified_date : `2022-05-08T04:39:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104250
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
In
long long f1(long long n, long long d)
{
    return n / d;
}

GCC generates:

        movq    %rdi, %rax
        cqto
        idivq   %rsi
        ret

Which is fine, except that the 64-bit IDIV instruction is significantly slower than the 32-bit (I)DIV. In recent CPUs (such as PMC, SNC, WLC, GLC), that's 18 vs 14 cycles, but it was much worse in older CPUs. There's still a significant difference for Atom cores, such as used in Alder Lake-E.

Clang generates:
        movq    %rdi, %rax
        movq    %rdi, %rcx
        orq     %rsi, %rcx
        shrq    $32, %rcx
        je      .LBB0_1
        cqto
        idivq   %rsi
        retq
.LBB0_1:
        xorl    %edx, %edx
        divl    %esi
        retq

That is, it ORs the two operands and checks if any bit in the upper half is set. If so, it performs the 64-bit division; otherwise, it performs the 32-bit one.

References:
https://gcc.godbolt.org/z/385a3da8q
https://uops.info/html-instr/IDIV_R32.html
https://uops.info/html-instr/IDIV_R64.html


---


### compiler : `gcc`
### title : `[10 Regression] '-fcompare-debug' failure (length) w/ -O2 -fnon-call-exceptions -fno-inline-small-functions since r10-3575-g629387a6586a7531`
### open_at : `2022-01-28T02:51:04Z`
### last_modified_date : `2022-05-10T10:19:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104263
### status : `RESOLVED`
### tags : `compare-debug-failure, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
gcc 12.0.1 20220123 snapshot (g:2da90ad39bf8fa9ee287e040d1f4411cb7a2e7ed) fails -fcompare-debug check when compiling the following testcase w/ -O2 -fnon-call-exceptions -fno-inline-small-functions:

int n;

int
bar (void)
{
  int a;

  n = 0;
  a = 0;

  return n;
}

__attribute__ ((pure, returns_twice)) int
foo (void)
{
  n = bar () + 1;
  foo ();

  return 0;
}

% gcc-12.0.1 -O2 -fcompare-debug -fnon-call-exceptions -fno-inline-small-functions -gno-statement-frontiers -c ovslsrlb.c
x86_64-pc-linux-gnu-gcc-12.0.1: error: ovslsrlb.c: '-fcompare-debug' failure (length)

For x86_64 the difference looks like this, though the failure is not target-specific:

--- ovslsrlb.c.gkd	2022-01-28 09:42:43.969435346 +0700
+++ ovslsrlb.gk.c.gkd	2022-01-28 09:42:43.987435394 +0700
@@ -40,8 +40,8 @@
 (note # 0 0 [bb 2] NOTE_INSN_BASIC_BLOCK)
 (note # 0 0 NOTE_INSN_PROLOGUE_END)
 (note # 0 0 NOTE_INSN_FUNCTION_BEG)
-(insn:TI # 0 0 2 (set (mem/c:SI (symbol_ref:DI ("n") [flags 0x2]  <var_decl # n>) [ MEM[(int *)&n]+0 S4 A32])
-        (const_int 1 [0x1])) "ovslsrlb.c":8:5# {*movsi_internal}
+(insn:TI # 0 0 2 (set (mem/c:SI (symbol_ref:DI ("n") [flags 0x2]  <var_decl # n>) [ n+0 S4 A32])
+        (const_int 1 [0x1])) "ovslsrlb.c":17:5# {*movsi_internal}
      (nil))
 (insn # 0 0 2 (parallel [
             (set (reg:DI 0 ax)


---


### compiler : `gcc`
### title : `Missed vectorization in 526.blender_r`
### open_at : `2022-01-28T07:05:11Z`
### last_modified_date : `2022-08-30T21:09:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104265
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Blenders hottest spot is rayobject_bb_intersect_test which can be summarized as

struct Isect {
    float start[3];
    float dir[3];
    float dist;
    float origstart[3];
    float origdir[3];
    int bv_index[6];
    float idot_axis[3];
    /* More stuff.  */
    void *userdata; /* aligns it to 8 */
};

int rayobject_bb_intersect_test (struct Isect *isec, const float *bb)
{
  float t1x = (bb[isec->bv_index[0]] - isec->start[0]) * isec->idot_axis[0];
  float t2x = (bb[isec->bv_index[1]] - isec->start[0]) * isec->idot_axis[0];
  float t1y = (bb[isec->bv_index[2]] - isec->start[1]) * isec->idot_axis[1];
  float t2y = (bb[isec->bv_index[3]] - isec->start[1]) * isec->idot_axis[1];
  float t1z = (bb[isec->bv_index[4]] - isec->start[2]) * isec->idot_axis[2];
  float t2z = (bb[isec->bv_index[5]] - isec->start[2]) * isec->idot_axis[2];

  if (t1x > t2y  || t2x < t1y  || t1x > t2z || t2x < t1z || t1y > t2z || t2y < t1z) return 0;
  if (t2x < 0.0f || t2y < 0.0f || t2z < 0.0f) return 0;
  if (t1x > isec->dist || t1y > isec->dist || t1z > isec->dist) return 0;
  return 1;
}

at least on x86 with SSE4 (for some extra permutes) we can vectorize the
comparisons, carefully filling extra lanes of V4SF vectors with redundant
data from another lane starting with {t1x,t1y,t1z,} {t2x,t2y,t2z,}
vectors.  AOCC does this kind of vectorization and receives a nice 25%
uplift in performance from it.


---


### compiler : `gcc`
### title : `z13: inefficient vec_popcnt for 16-bit for z13`
### open_at : `2022-01-28T10:22:50Z`
### last_modified_date : `2022-01-29T03:38:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104268
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `enhancement`
### contents :
#include <vecintrin.h>

vector unsigned short popcnt(vector unsigned short a)
{
   return vec_popcnt(a);
}

Generates with -march=z13

_Z6popcntDv8_t:
.LFB1:
        .cfi_startproc
        vzero   %v0
        vpopct  %v24,%v24,0
        vleib   %v0,8,7
        vsrlb   %v0,%v24,%v0
        vab     %v24,%v24,%v0
        vgbm    %v0,21845
        vn      %v24,%v24,%v0
        br      %r14
        .cfi_endproc


Optimal sequence would be:
vector unsigned short popcnt_opt(vector unsigned short a)
{
   vector unsigned short r = (vector unsigned short)vec_popcnt((vector unsigned char)a);
   vector unsigned short b = vec_rli(r, 8);
   r = r + b;
   r = r >> 8;
   return r;
}

_Z10popcnt_optDv8_t:
.LFB3:
        .cfi_startproc
        vpopct  %v24,%v24,0
        verllh  %v0,%v24,8
        vah     %v24,%v0,%v24
        vesrlh  %v24,%v24,8
        br      %r14
        .cfi_endproc


---


### compiler : `gcc`
### title : `[12 Regression] 538.imagick_r run-time at -Ofast -march=native regressed by 26% on Intel Cascade Lake server CPU`
### open_at : `2022-01-28T17:12:15Z`
### last_modified_date : `2023-06-07T07:14:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104271
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
On our Intel Cascade Lake server CPU, the benchmark 538.imagick_r from
the SPEC 2017 FPrate suite is 26% slower when built with GCC 12 using
options -Ofast -march=native than when built with GCC 11 with the same
options.

I have bisected the issue to r12-2549-g872da9a6f664a0:

  872da9a6f664a06d73c987aa0cb2e5b830158a10 is the first bad commit
  commit 872da9a6f664a06d73c987aa0cb2e5b830158a10
  Author: liuhongt <hongtao.liu@intel.com>
  Date:   Fri Mar 26 10:56:47 2021 +0800
                   
    Add the member integer_to_sse to processor_cost as a cost simulation for movd/pinsrd. It will be used to calculate the cost of vec_construct.
                   
    gcc/ChangeLog: 
                   
            PR target/99881
            * config/i386/i386.h (processor_costs): Add new member
            integer_to_sse.
            * config/i386/x86-tune-costs.h (ix86_size_cost, i386_cost,
            i486_cost, pentium_cost, lakemont_cost, pentiumpro_cost,
            geode_cost, k6_cost, athlon_cost, k8_cost, amdfam10_cost,
            bdver_cost, znver1_cost, znver2_cost, znver3_cost,
            btver1_cost, btver2_cost, btver3_cost, pentium4_cost,
            nocona_cost, atom_cost, atom_cost, slm_cost, intel_cost,
            generic_cost, core_cost): Initialize integer_to_sse same value
            as sse_op.
            (skylake_cost): Initialize integer_to_sse twice as much as sse_op.
            * config/i386/i386.c (ix86_builtin_vectorization_cost):
            Use integer_to_sse instead of sse_op to calculate the cost of
            vec_construct.


It is very likely that the problem is a load-to-store-forwarding stall
issue and so related to PR 80689 but the commit makes it either more
frequent or much worse.

Sorry for reporting this so late, unfortunately we do not benchmark
this class of CPUs periodically.  We do not see the problem on any of
the AMD Zens or Intel Kabylake (client) CPUs.


---


### compiler : `gcc`
### title : `memset is not elimited when followed by a store loop writing to that memory location`
### open_at : `2022-01-28T23:16:21Z`
### last_modified_date : `2022-07-15T19:14:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104276
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
clang is unable to remove the memset in code like 

std::vector<int> foo() {
  auto result = std::vector<int>(SZ);
  int *ptr = result.data();
  for (std::size_t n = 0; n < SZ; ++n) {
		ptr[n] = static_cast<int>( n );
  }
  return result;
}
https://gcc.godbolt.org/z/5cbKejfqr

This is unaffected if the value is set during resize.  That may result in inlining of the memset but does not eliminate it.

However for code that uses blessed methods like memset subsequently

std::vector<int> foo() {
  auto result = std::vector<int>(SZ);
  std::memset(result.data(), 5, sizeof(int) * SZ);
  return result;
}

https://gcc.godbolt.org/z/Kfs9x8Pe9

It is.  This seems to be the usecase of resize_and_overwrite in future string.  Is there a way to formulate the code to do this.  Or, and I think a better way, is there a builtin or could there be that lets the compiler assume that the memory will be subsequently written to?  e.g. `__bultin_assume_set( void * dest, size_t count )` ?


---


### compiler : `gcc`
### title : `(bool)(bool0 + bool1) should be simplified into bool0 | bool1`
### open_at : `2022-01-30T18:34:24Z`
### last_modified_date : `2023-06-23T00:45:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104292
### status : `ASSIGNED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
bool bool_or(bool b1, bool b2) {
    return b1 + b2;
}

generates

bool_or(bool, bool):
        movzbl  %dil, %edi
        movzbl  %sil, %esi
        addl    %esi, %edi
        setne   %al
        ret

Whereas it could generate

bool_or(bool, bool):
        or   %edi, %esi
        mov  %esi, %eax
        ret

For a net gain of two instructions (and the final mov can often be elided, so up to three).

I encountered this while using std::plus<> with boolean types. It could be optimized as a specialization of std::plus, but I think a magic transformation in the optimizer would make it apply in more places.


---


### compiler : `gcc`
### title : `MIN<unsigned, 1> should simplify to unsigned != 0`
### open_at : `2022-01-31T05:26:54Z`
### last_modified_date : `2023-09-21T14:16:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104296
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
unsigned t(unsigned a)
{
    unsigned b = 1;
    return a >= b ? b : a;
}

unsigned t1(unsigned a)
{
    return a >= 1 ? 1 : a;
}

These two functions are the same but currently they do not optimize to the same thing.


---


### compiler : `gcc`
### title : `MIN_EXPR is not detected for a >= -__INT_MAX__  ? -__INT_MAX__  : a`
### open_at : `2022-01-31T07:04:28Z`
### last_modified_date : `2023-06-09T16:26:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104297
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int t(int a)
{
    int b = -__INT_MAX__ ;
    int c = a >= b ? b : a;
    return c;
}

int t1(int a)
{
    int c = a >= -__INT_MAX__  ? -__INT_MAX__  : a;
    return c;
}

--- CUT ---
t1 should be detected to MIN_EXPR <a_2(D), -2147483647>;


---


### compiler : `gcc`
### title : `[AArch64] Failure to optimize 8-bit bitreverse pattern`
### open_at : `2022-01-31T22:23:49Z`
### last_modified_date : `2023-09-21T07:43:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104315
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
unsigned int stb_bitreverse8(unsigned char n)
{
   n = ((n & 0xAA) >> 1) + ((n & 0x55) << 1);
   n = ((n & 0xCC) >> 2) + ((n & 0x33) << 2);
   return (unsigned char) ((n >> 4) + (n << 4));
}

On AArch64, with -O3, GCC currently outputs this:

stb_bitreverse8(unsigned char):
  mov w2, 170
  mov w1, 85
  and w1, w1, w0, lsr 1
  and w0, w2, w0, lsl 1
  orr w0, w1, w0
  mov w1, -52
  mov w2, 51
  and w1, w1, w0, lsl 2
  and w0, w2, w0, lsr 2
  and w1, w1, 255
  orr w0, w0, w1
  lsr w1, w0, 4
  orr w0, w1, w0, lsl 4
  and w0, w0, 255
  ret

LLVM instead outputs this:

stb_bitreverse8(unsigned char):
  rbit w8, w0
  lsr w0, w8, #24
  ret

This optimization should be faster and quite useful, especially as there does not seem to be any way to use `rbit` manually with intrinsics in GCC.


---


### compiler : `gcc`
### title : `RSO is not used when doing assignment rather than initialization and address taken afterwards`
### open_at : `2022-01-31T22:34:39Z`
### last_modified_date : `2022-01-31T22:50:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104316
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
~~~c
#include <stdlib.h>

typedef struct Buffer {
    char *buf;
    size_t len;
    size_t cap;
} Buffer;

Buffer Buffer_Init_Return(void);
void Buffer_Init_Ref(Buffer *buf);
void Buffer_Add(Buffer *buf, const char *);

void
Use_Buffers(void)
{
    Buffer buf1 = Buffer_Init_Return();

    Buffer buf2;
    buf2 = Buffer_Init_Return();

    Buffer_Add(&buf1, "1");
    Buffer_Add(&buf2, "2");
}
~~~

https://godbolt.org/z/qTz36qP9r

The code snippets for initializing buf1 and buf2 are almost the same. From an as-if perspective, they are identical.

GCC 12 generates several copy instructions when initializing buf2, while Clang and ICC generate the straight-forward code.

Would it be difficult to tell GCC to optimize the above initialization for buf2, for the case that buf2 is not accessed between the declaration and the assignment?


---


### compiler : `gcc`
### title : `RISC-V: Subword atomics result in library calls`
### open_at : `2022-02-01T23:36:46Z`
### last_modified_date : `2023-10-05T08:47:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104338
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
There's a handful of bugs sort of related to this one, but nothing specific.  This has been a long-standing issue and I think folks are generally familiar with it, but just to summarize: we don't have sub-word atomic instruction in RISC-V, so we just call out to the libatomic routines.  This causes fallout in a handful of places (see 86005 and 81358, for example) and there's been some attempts to resolve it but nothing appears to have stuck.

I figured it'd be a good starter project for Patrick, as he's yet to do any GCC stuff.  He's working through it and doesn't have anything to post yet, but figured I'd just open the bug now so folks knew what was going on from our end.


---


### compiler : `gcc`
### title : `Missing phiopt due to cast`
### open_at : `2022-02-02T00:27:52Z`
### last_modified_date : `2023-09-21T13:55:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104339
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(unsigned short y)
{
    unsigned short t =y;
   int tt = y;
   y = -y;
   int yy = y;
   if (t) tt = y;
   return tt;
}

--- CUT ---
We should be able to optimize this to:
  y_3 = -y_2(D);
  tt_1 = (int) y_3;

But currently don't do a few things.


---


### compiler : `gcc`
### title : `Suboptimal -Os code for manually unrolled loop`
### open_at : `2022-02-02T13:59:27Z`
### last_modified_date : `2022-02-02T22:41:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104344
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Manually copying the byte representation of a float to a uint32_t emits optimal code with -Os when the copy is performed in a loop. When the copy is hand-unrolled , -Os generates much larger code than -O3.

It's unclear to me why -Os doesn't generate the same code; to my eye they are both equivalent and well-formed, and -O3 seems to agree. Apologies in advance if I'm simply misunderstanding the C11 Standard and what transformations are legal here.

This happens at least on x64 on gcc 12.0.1, and on ARMv7-M on armgcc 10.2. Various architecture-specific flags like "-mcpu" and "-march" do not appear to make a difference.

Code:
=====
#include <stdint.h>

_Static_assert(sizeof(uint32_t) == sizeof(float), "");
_Static_assert(sizeof(uint32_t) == 4, "");

uint32_t cast_through_char_unrolled(float f) {
  uint32_t u;
  char const *src = (char const *)&f;
  char *dst = (char *)&u;
  *dst++ = *src++;
  *dst++ = *src++;
  *dst++ = *src++;
  *dst++ = *src++;
  return u;
}

uint32_t cast_through_char_loop(float f) {
  uint32_t u;
  char const *src = (char const *)&f;
  char *dst = (char *)&u;
  for (int i = 0; i < 4; ++i) {
    *dst++ = *src++;
  }
  return u;
}

-Os output (flags: "--std=c11 -Wall -Wextra -Os")
=======================================
cast_through_char_unrolled:
  movd eax, xmm0
  xor edx, edx
  mov dl, al
  mov dh, ah
  xor ax, ax
  movzx edx, dx
  or eax, edx
  ret

cast_through_char_loop:
  movd eax, xmm0
  ret


-O3 output (flags: "--std=c11 -Wall -Wextra -O3")
=======================================
cast_through_char_unrolled:
  movd eax, xmm0
  ret

cast_through_char_loop:
  movd eax, xmm0
  ret

Godbolt example:
================
https://gcc.godbolt.org/z/bn9xq1b56

Gcc details follow, captured by adding "-v" to my command-line:
===============================================================
Using built-in specs.
COLLECT_GCC=/opt/compiler-explorer/gcc-snapshot/bin/gcc
Target: x86_64-linux-gnu
Configured with: ../gcc-trunk-20220202/configure --prefix=/opt/compiler-explorer/gcc-build/staging --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --disable-bootstrap --enable-multiarch --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --enable-clocale=gnu --enable-languages=c,c++,fortran,ada,d --enable-ld=yes --enable-gold=yes --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-linker-build-id --enable-lto --enable-plugins --enable-threads=posix --with-pkgversion=Compiler-Explorer-Build-gcc-756eabacfcd767e39eea63257a026f61a4c4e661-binutils-2.36.1
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 12.0.1 20220202 (experimental) (Compiler-Explorer-Build-gcc-756eabacfcd767e39eea63257a026f61a4c4e661-binutils-2.36.1) 
COLLECT_GCC_OPTIONS='-fdiagnostics-color=always' '-g' '-o' '/app/output.s' '-masm=intel' '-S' '-std=c11' '-O3' '-Wall' '-Wextra' '-v' '-mtune=generic' '-march=x86-64' '-dumpdir' '/app/'
 /opt/compiler-explorer/gcc-trunk-20220202/bin/../libexec/gcc/x86_64-linux-gnu/12.0.1/cc1 -quiet -v -imultiarch x86_64-linux-gnu -iprefix /opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/ <source> -quiet -dumpdir /app/ -dumpbase output.c -dumpbase-ext .c -masm=intel -mtune=generic -march=x86-64 -g -O3 -Wall -Wextra -std=c11 -version -fdiagnostics-color=always -o /app/output.s
GNU C11 (Compiler-Explorer-Build-gcc-756eabacfcd767e39eea63257a026f61a4c4e661-binutils-2.36.1) version 12.0.1 20220202 (experimental) (x86_64-linux-gnu)
	compiled by GNU C version 7.5.0, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.24-GMP

GGC heuristics: --param ggc-min-expand=30 --param ggc-min-heapsize=4096
ignoring nonexistent directory "/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/../../../../x86_64-linux-gnu/include"
ignoring duplicate directory "/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/12.0.1/include"
ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
ignoring duplicate directory "/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/12.0.1/include-fixed"
ignoring nonexistent directory "/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/12.0.1/../../../../x86_64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/include
 /opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/include-fixed
 /usr/local/include
 /opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/../../include
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
GNU C11 (Compiler-Explorer-Build-gcc-756eabacfcd767e39eea63257a026f61a4c4e661-binutils-2.36.1) version 12.0.1 20220202 (experimental) (x86_64-linux-gnu)
	compiled by GNU C version 7.5.0, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.24-GMP

GGC heuristics: --param ggc-min-expand=30 --param ggc-min-heapsize=4096
Compiler executable checksum: 85eab4743b9643508f1adb2d853127bf
COMPILER_PATH=/opt/compiler-explorer/gcc-trunk-20220202/bin/../libexec/gcc/x86_64-linux-gnu/12.0.1/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../libexec/gcc/x86_64-linux-gnu/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../libexec/gcc/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/../../../../x86_64-linux-gnu/bin/
LIBRARY_PATH=/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/../../../../lib64/:/lib/x86_64-linux-gnu/:/lib/../lib64/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib64/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/../../../../x86_64-linux-gnu/lib/:/opt/compiler-explorer/gcc-trunk-20220202/bin/../lib/gcc/x86_64-linux-gnu/12.0.1/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-fdiagnostics-color=always' '-g' '-o' '/app/output.s' '-masm=intel' '-S' '-std=c11' '-O3' '-Wall' '-Wextra' '-v' '-mtune=generic' '-march=x86-64' '-dumpdir' '/app/output.'
Compiler returned: 0


---


### compiler : `gcc`
### title : `[12 Regression] "nvptx: Transition nvptx backend to STORE_FLAG_VALUE = 1" patch made some code generation worse`
### open_at : `2022-02-02T14:10:08Z`
### last_modified_date : `2022-03-02T12:10:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104345
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `minor`
### contents :
First, I do acknowledge that commit beed3f8f60492289ca6211d86c54a2254a642035 "nvptx: Transition nvptx backend to STORE_FLAG_VALUE = 1" generally does improve nvptx code generation -- thanks!

I've however run into one case where it causes a regression:

    PASS: libgomp.oacc-c/../libgomp.oacc-c-c++-common/reduction-cplx-dbl.c -DACC_DEVICE_TYPE_nvidia=1 -DACC_MEM_SHARED=0 -foffload=nvptx-none  -O0  (test for excess errors)
    [-PASS:-]{+FAIL:+} libgomp.oacc-c/../libgomp.oacc-c-c++-common/reduction-cplx-dbl.c -DACC_DEVICE_TYPE_nvidia=1 -DACC_MEM_SHARED=0 -foffload=nvptx-none  -O0  execution test
    PASS: libgomp.oacc-c/../libgomp.oacc-c-c++-common/reduction-cplx-dbl.c -DACC_DEVICE_TYPE_nvidia=1 -DACC_MEM_SHARED=0 -foffload=nvptx-none  -O2  (test for excess errors)
    [-PASS:-]{+FAIL:+} libgomp.oacc-c/../libgomp.oacc-c-c++-common/reduction-cplx-dbl.c -DACC_DEVICE_TYPE_nvidia=1 -DACC_MEM_SHARED=0 -foffload=nvptx-none  -O2  execution test

    libgomp: The Nvidia accelerator has insufficient resources to launch 'worker$_omp_fn$0' with num_workers = 32 and vector_length = 32; recompile the program with 'num_workers = x and vector_length = y' on that offloaded region or '-fopenacc-dim=:x:y' where x * y <= 896.

Same for C++.

That's with a Nvidia Tesla K20c, Driver Version: 346.46 -- so, rather old.

By the way: the subsequent commit 659f8161f61d3f75c3a47cf646147e8f7b4dcb34 "nvptx: Add support for PTX's cnot instruction" is not helpful or even relevant here; there are no 'cnot's appearing in the PTX code loaded to the GPU (per 'GOMP_DEBUG=1' execution).

Per 'diff' of  'GOMP_DEBUG=1' execution we indeed see *more* registers used after "nvptx: Transition nvptx backend to STORE_FLAG_VALUE = 1" than before.  For '-O0':

    [...]
     Link log info    : 4 bytes gmem
     info    : Function properties for 'gang$_omp_fn$0':
    -info    : used 51 registers, 112 stack, 0 bytes smem, 328 bytes cmem[0], 16 bytes cmem[2], 0 bytes lmem
    +info    : used 68 registers, 112 stack, 0 bytes smem, 328 bytes cmem[0], 8 bytes cmem[2], 0 bytes lmem
     info    : Function properties for 'worker$_omp_fn$0':
    -info    : used 51 registers, 112 stack, 136 bytes smem, 328 bytes cmem[0], 16 bytes cmem[2], 0 bytes lmem
    +info    : used 68 registers, 112 stack, 136 bytes smem, 328 bytes cmem[0], 8 bytes cmem[2], 0 bytes lmem
     info    : Function properties for 'vector$_omp_fn$0':
    -info    : used 51 registers, 112 stack, 0 bytes smem, 328 bytes cmem[0], 16 bytes cmem[2], 0 bytes lmem
    +info    : used 68 registers, 112 stack, 0 bytes smem, 328 bytes cmem[0], 8 bytes cmem[2], 0 bytes lmem
       GOMP_OFFLOAD_openacc_exec: prepare mappings
       nvptx_exec: kernel vector$_omp_fn$0: launch gangs=1, workers=1, vectors=32
       nvptx_exec: kernel vector$_omp_fn$0: finished
    -GOACC_parallel_keyed: mapnum=3, hostaddrs=0x7ffc760394a0, size=0x60bb30, kinds=0x60bb48
    +GOACC_parallel_keyed: mapnum=3, hostaddrs=0x7fff99653530, size=0x60bad0, kinds=0x60bae8
       GOMP_OFFLOAD_openacc_exec: prepare mappings
    +
    +libgomp: The Nvidia accelerator has insufficient resources to launch 'worker$_omp_fn$0' with num_workers = 32 and vector_length = 32; recompile the program with 'num_workers = x and vector_length = y' on that offloaded region or '-fopenacc-dim=:x:y' where x * y <= 896.
    -  nvptx_exec: kernel worker$_omp_fn$0: launch gangs=1, workers=32, vectors=32
    -  nvptx_exec: kernel worker$_omp_fn$0: finished
    -GOACC_parallel_keyed: mapnum=3, hostaddrs=0x7ffc760394a0, size=0x60bb50, kinds=0x60bb68
    -  GOMP_OFFLOAD_openacc_exec: prepare mappings
    -  nvptx_exec: kernel gang$_omp_fn$0: launch gangs=32, workers=1, vectors=32
    -  nvptx_exec: kernel gang$_omp_fn$0: finished

Similar for '-O2', just with less stack usage.

Cross-checking with a more recent Driver Version: 450.119.03, I'm only seeing slightly increased register usage; 52 registers after "nvptx: Transition nvptx backend to STORE_FLAG_VALUE = 1" compared to 51 registers before:

    [...]
    Link log info    : 4 bytes gmem
    info    : Function properties for 'vector$_omp_fn$0':
    info    : used [-51-]{+52+} registers, 112 stack, 0 bytes smem, 328 bytes cmem[0], 16 bytes cmem[2], 0 bytes lmem
    info    : Function properties for 'worker$_omp_fn$0':
    info    : used [-51-]{+52+} registers, 112 stack, 136 bytes smem, 328 bytes cmem[0], 16 bytes cmem[2], 0 bytes lmem
    info    : Function properties for 'gang$_omp_fn$0':
    info    : used [-51-]{+52+} registers, 112 stack, 0 bytes smem, 328 bytes cmem[0], 16 bytes cmem[2], 0 bytes lmem
    [...]

This suggests that compared to before, after "nvptx: Transition nvptx backend to STORE_FLAG_VALUE = 1" GCC is generating certain PTX code sequences that the Driver/JIT fails to understand/optimize?  While not ideal, the code still executes fine (with newish Driver/JIT), and I'm thus OK if we classify that as not worth looking into -- but I at least wanted to report my findings: maybe there's a way to tune the GCC/nvptx code generation to the PTX -> SASS compiler's liking?

Possibly (but that's just guessing!), the reason might be around the following PTX code change:

    [...]
    -setp.leu.f64 %r82,%r25,0d7fefffffffffffff;
    -@ ! %r82 bra $L3;
    +@ %r78 bra $L20;
    +setp.leu.f64 %r138,%r57,0d7fefffffffffffff;
    +bra $L3;
    +$L20:
     .loc 2 1976 21
    -setp.leu.f64 %r83,%r57,0d7fefffffffffffff;
    -@ %r83 bra $L19;
    +setp.leu.f64 %r138,%r57,0d7fefffffffffffff;
    +@ %r138 bra $L19;
     $L3:
    [...]

From a quick look, I read this to mean that the originally ("before") unconditional 'setp.leu.f64 %r82,%r25,0d7fefffffffffffff;' is now ("after") done conditionally.

Maybe related, maybe not: when curiously 'diff'ing the before vs. after nvptx-none target libraries, I noticed amongst all the "noise" (improved code generation):

'nvptx-none/libatomic/gcas.o':

    [...]
     atom.cas.b32 %r137,[%r34],%r136,%r139;
     setp.eq.u32 %r140,%r137,%r136;
     selp.u32 %r138,1,0,%r140;
    -setp.ne.u32 %r141,%r138,0;
    -@ %r141 bra $L21;
    +@ %r140 bra $L18;
     st.u32 [%r201],%r137;
    -bra $L19;
    +$L18:
    +setp.eq.u32 %r142,%r138,0;
    +@ %r142 bra $L19;
     $L21:
    [...]

... which again looks like a pattern where an originally ("before") unconditional 'setp.ne.u32 %r141,%r138,0;' is now ("after") done conditionally.

Similar in other files -- but I certainly didn't look in detail, and I'm certainly not claiming this to be/cause any actual problem.


And, I've spotted a few cases where we're generating "maybe worse" code:

'nvptx-none/libgomp/openacc.o' (complete 'diff'):

    @@ -25,6 +25,7 @@
     .reg .u64 %r28;
     .reg .u32 %r29;
     .reg .u32 %r30;
    +.reg .pred %r31;
     mov.u64 %r27,%ar0;
     st.u64 [%frame+16],%r27;
     ld.u64 %r28,[%frame+16];
    @@ -38,8 +39,8 @@
     ld.param.u32 %r30,[%value_in];
     }
     mov.u32 %r23,%r30;
    -set.u32.ne.u32 %r24,%r23,0;
    -neg.s32 %r24,%r24;
    +setp.ne.u32 %r31,%r23,0;
    +selp.u32 %r24,1,0,%r31;
     st.u32 [%frame],%r24;
     ld.u32 %r25,[%frame];
     mov.u32 %r26,%r25;

'nvptx-none/newlib/libc/reent/lib_a-renamer.o' (complete 'diff'):

    @@ -28,6 +28,8 @@
     .reg .u32 %r32;
     .reg .pred %r33;
     .reg .u32 %r36;
    +.reg .u32 %r38;
    +.reg .pred %r39;
     mov.u64 %r26,%ar0;
     mov.u64 %r27,%ar1;
     mov.u64 %r28,%ar2;
    @@ -58,7 +60,9 @@
     ld.param.u32 %r36,[%value_in];
     }
     .loc 2 57 6
    -set.u32.eq.u32 %r25,%r36,-1;
    +setp.eq.u32 %r39,%r36,-1;
    +selp.u32 %r38,1,0,%r39;
    +neg.s32 %r25,%r38;
     $L1:
     .loc 2 64 1
     mov.u32 %value,%r25;

'nvptx-none/newlib/libc/stdio/lib_a-remove.o' (complete 'diff'):

    @@ -24,6 +24,8 @@
     .reg .u64 %r26;
     .reg .u64 %r27;
     .reg .u32 %r30;
    +.reg .u32 %r33;
    +.reg .pred %r34;
     mov.u64 %r26,%ar0;
     mov.u64 %r27,%ar1;
     .loc 2 65 7
    @@ -37,7 +39,9 @@
     ld.param.u32 %r30,[%value_in];
     }
     .loc 2 65 6
    -set.u32.eq.u32 %value,%r30,-1;
    +setp.eq.u32 %r34,%r30,-1;
    +selp.u32 %r33,1,0,%r34;
    +neg.s32 %value,%r33;
     .loc 2 69 1
     st.param.u32 [%value_out],%value;
     ret;
    @@ -51,6 +55,8 @@
     .reg .u64 %r27;
     .reg .u64 %r30;
     .reg .u32 %r31;
    +.reg .u32 %r34;
    +.reg .pred %r35;
     mov.u64 %r27,%ar0;
     .loc 2 65 7
     ld.global.u64 %r30,[_impure_ptr];
    @@ -64,7 +70,9 @@
     ld.param.u32 %r31,[%value_in];
     }
     .loc 2 65 6
    -set.u32.eq.u32 %value,%r31,-1;
    +setp.eq.u32 %r35,%r31,-1;
    +selp.u32 %r34,1,0,%r35;
    +neg.s32 %value,%r34;
     .loc 2 77 1
     st.param.u32 [%value_out],%value;
     ret;

'nvptx-none/newlib/libm/common/lib_a-s_rint.o' (complete 'diff'):

    @@ -80,6 +80,7 @@
     .reg .u32 %r119;
     .reg .pred %r120;
     .reg .u32 %r122;
    +.reg .pred %r123;
     .reg .u32 %r124;
     .reg .u32 %r125;
     .reg .u32 %r126;
    @@ -197,7 +198,8 @@
     setp.eq.u32 %r120,%r41,0;
     @ %r120 bra $L5;
     .loc 2 114 9
    -set.u32.eq.u32 %r122,%r58,19;
    +setp.eq.u32 %r123,%r58,19;
    +selp.u32 %r122,1,0,%r123;
     shl.b32 %r41,%r122,31;
     .loc 2 115 17
     not.b32 %r124,%r64;

I haven't looked if that's "actually worse" in SASS, or just "maybe worse" in the intermediate PTX representation.  (... and is most certainly not related to the regression mentioned before.)


It seems unlikely, but I'll report in case anything here changes due to Tom's several GCC/nvptx back end commits yesterday.


So, please close this PR as "won't fix" unless you see something here that you'd like to look into.


---


### compiler : `gcc`
### title : `Failure to use csinv instead of mvn+csel where possible`
### open_at : `2022-02-02T23:40:40Z`
### last_modified_date : `2023-09-21T07:43:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104357
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
unsigned char stbi__clamp(int x)
{
   if ((unsigned)x > 255) {
      if (x < 0) return 0;
      if (x > 255) return 255;
   }
   return x;
}

With -O3, GCC outputs this (on aarch64):

stbi__clamp(int):
  mvn w1, w0
  cmp w0, 256
  and w0, w0, 255
  asr w1, w1, 31
  and w1, w1, 255
  csel w0, w0, w1, cc
  ret

LLVM instead outputs this:

stbi__clamp(int):
  asr w8, w0, #31
  cmp w0, #255
  csinv w0, w0, w8, ls
  ret

I don't know if the `and`s are there because of ABI differences, but it seems to me like the `mvn` can definitely be replaced by using `csinv` instead of `csel`.


---


### compiler : `gcc`
### title : `Failure to optimize abs pattern (x^(x<0?-1:0)) - (x<0?-1:0)`
### open_at : `2022-02-03T02:29:50Z`
### last_modified_date : `2023-09-21T07:42:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104360
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

typedef int16_t v8i16 __attribute__((vector_size(16)));

v8i16 abs_i16(v8i16 x)
{
    auto isN = x < v8i16{};

    x ^= isN;
    return x - isN;
}

This (although I think v8i16 could be replaced with any integer vector type and it still would work) can be optimized to using an abs instruction where possible (such as `pabsw` on x86-64, or `abs` on aarch64)

PS: this doesn't even necessarily require an abs instruction. on standard x86-64 with -O3, GCC manages just this:

abs_i16(short __vector(8)):
  pxor xmm1, xmm1
  pcmpgtw xmm1, xmm0
  pxor xmm0, xmm1
  psubw xmm0, xmm1
  ret

whereas LLVM outputs this:

abs_i16(short __vector(8)):
  pxor xmm1, xmm1
  psubw xmm1, xmm0
  pmaxsw xmm0, xmm1
  ret

which I'm pretty sure is better.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Failure to vectorise conditional grouped accesses after PR102659`
### open_at : `2022-02-03T14:51:41Z`
### last_modified_date : `2023-05-08T12:23:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104368
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following test regressed with PR102659, compiled with
-O3 -march=armv8.2-a+sve:

void f(int *restrict x, int *restrict y, int n)
{
  for (int i = 0; i < n; ++i)
    if (x[i] > 0)
      x[i] = y[i * 2] + y[i * 2 + 1];
}

Previously we treated the y[] accesses as a linear group
and so could use LD2W.  Now we treat them as individual
gather loads instead:

.L3:
        ld1w    z1.s, p0/z, [x0, x3, lsl 2]
        lsl     z0.s, z2.s, #1
        cmpgt   p0.s, p0/z, z1.s, #0
        ld1w    z1.s, p0/z, [x1, z0.s, sxtw 2]   // Gather
        ld1w    z0.s, p0/z, [x5, z0.s, sxtw 2]   // Gather
        add     z0.s, z1.s, z0.s
        st1w    z0.s, p0, [x0, x3, lsl 2]
        incw    z2.s
        add     x3, x3, x4
        whilelo p0.s, w3, w2
        b.any   .L3


---


### compiler : `gcc`
### title : `[x86] Failure to use optimize pxor+pcmpeqb+pmovmskb+cmp 0xFFFF pattern to ptest`
### open_at : `2022-02-03T18:08:42Z`
### last_modified_date : `2023-09-21T13:35:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104371
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
bool is_zero(__m128i x)
{
    return _mm_movemask_epi8(_mm_cmpeq_epi8(x, _mm_setzero_si128())) == 0xffff;
}

This can be optimized to `return _mm_testz_si128(x, x);`. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[ARM] Unnecessary writes to stack when passing aggregate in registers`
### open_at : `2022-02-03T19:20:41Z`
### last_modified_date : `2022-02-04T00:50:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104372
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Missed optimization when an aggregate is passed by value entirely in registers:

struct Bar {};

struct Foo {
    Bar *addr;
    int size;
};

Bar* MakeBar1(Bar *addr, int) noexcept {
    return addr;
}

Bar* MakeBar2(Foo foo) noexcept {
    return foo.addr;
}


When compiled with '-O2' using 'arm-unknown-linux-gnueabihf-g++ (GCC) 12.0.0' generates:

MakeBar1(Bar*, int):
        bx      lr
MakeBar2(Foo):
        sub     sp, sp, #8
        add     r3, sp, #8
        stmdb   r3, {r0, r1}
        add     sp, sp, #8
        bx      lr


The creation of a stack frame in MakeBar2 is completely unnecessary. For comparison, Clang 11.0.1 generates identical code for both functions that matches MakeBar1 shown here.


---


### compiler : `gcc`
### title : `[x86] Failure to recognize bzhi pattern when shr is present`
### open_at : `2022-02-03T23:17:59Z`
### last_modified_date : `2023-09-21T13:00:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104375
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

uint64_t bextr_u64(uint64_t w, unsigned off, unsigned int len)
{
        return (w >> off) & ((1U << len) - 1U);
}

With -mbmi2, this can be optimized to using shrx followed by bzhi. This transformation is done by LLVM, but not by GCC.


PS: Even in the case where the shr is removed and thus the bzhi pattern is recognized (e.g. `return w & ((1U << len) - 1U);`), it is still not compiled optimally as it for some reason decides to put the result of the bzhi in an intermediary register before moving it to eax.


---


### compiler : `gcc`
### title : `Failure to optimize clz equivalent to clz`
### open_at : `2022-02-04T01:10:30Z`
### last_modified_date : `2023-10-24T11:17:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104376
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

uint32_t countLeadingZeros32(uint32_t x)
{
    if (x == 0)
        return 32;
    return (31 - __builtin_clz(x)) ^ 31;
}

On x86, with `-mlzcnt`, GCC outputs this:

countLeadingZeros32(unsigned int):
  mov eax, 32
  test edi, edi
  je .L1
  mov eax, 31
  lzcnt edi, edi
  sub eax, edi
  xor eax, 31
.L1:
  ret

LLVM instead outputs this:

countLeadingZeros32(unsigned int):
  lzcnt eax, edi
  ret


---


### compiler : `gcc`
### title : `(N - x) ^ N should be optimized to x if x <= N (unsigned) and N is a pow2 - 1`
### open_at : `2022-02-04T07:01:04Z`
### last_modified_date : `2022-02-04T18:36:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104378
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
#define n 8
#define N ((1u<<n)-1)

unsigned countLeadingZeros32(unsigned x)
{
   if (x > N) __builtin_unreachable();
   return (N - x) ^ N;
}

This should be optimized to just
return x;

Like it is done by LLVM.


---


### compiler : `gcc`
### title : `aarch64: Redundant SXTH for “bag of bits” moves`
### open_at : `2022-02-04T18:14:05Z`
### last_modified_date : `2022-10-16T17:21:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104387
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
This PR is about another case in which we generate a redundant SXTH.
Specifically:

void f(short *x, short y)
{
  x[0] = y;
  x[1] = y;
}

generates:

        sxth    w1, w1
        strh    w1, [x0]
        strh    w1, [x0, 2]

even though the RTL makes it clear that the upper 16 bits of the
promoted “y” are unused.

This is related to PR64537, but I think it's worth keeping as a
separate PR since the fix isn't necessarily the same.


---


### compiler : `gcc`
### title : `Failure to optimize vector pattern for x < 0`
### open_at : `2022-02-05T00:08:09Z`
### last_modified_date : `2023-09-21T12:55:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104394
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

typedef int32_t v4i32 __attribute__((vector_size(16)));

v4i32 get_cmpmask(v4i32 mask)
{
    v4i32 signmask{(int32_t)0x80000000, (int32_t)0x80000000, (int32_t)0x80000000, (int32_t)0x80000000};
    return ((signmask & mask) == signmask);
}

This can be optimized to `return mask < 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[x86] Failure to recognize min/max pattern using pcmp+pblendv`
### open_at : `2022-02-05T21:32:27Z`
### last_modified_date : `2023-09-21T12:54:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104401
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <smmintrin.h>

__m128i min32(__m128i value, __m128i input)
{
  return _mm_blendv_epi8(input, value, _mm_cmplt_epi32(value, input));
}

With -O3 -msse4.1, GCC outputs this:

min32(long long __vector(2), long long __vector(2)):
  movdqa xmm2, xmm0
  movdqa xmm0, xmm1
  movdqa xmm3, xmm1
  pcmpgtd xmm0, xmm2
  pblendvb xmm3, xmm2, xmm0
  movdqa xmm0, xmm3
  ret

LLVM instead outputs this:

min32(long long __vector(2), long long __vector(2)):
  pminsd xmm0, xmm1
  ret

The equivalent code with cmpgt used instead of cmplt can be optimized to pmaxsd.


---


### compiler : `gcc`
### title : `Inefficient register allocation on complex returns`
### open_at : `2022-02-06T07:20:34Z`
### last_modified_date : `2023-06-18T21:11:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104405
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following testcase

#include <complex.h>

complex double f (complex double a, complex double b)
{
    return a * b * I;
}

compiled at -Ofast has unneeded copies of input

i.e.

f:
        fmov    d4, d1
        fmov    d1, d0
        fmul    d0, d4, d2
        fmul    d4, d4, d3
        fnmadd  d0, d1, d3, d0
        fnmsub  d1, d1, d2, d4
        ret

the first two moves are unneeded and looks to be an artifact of how IMAGPART_EXPR and REALPART_EXPR are expanded.  This seems to be a generic issue as both x86 and Arm targets seem to have the same problem.


---


### compiler : `gcc`
### title : `SLP discovery doesn't use TWO_OPERAND nodes as seeds`
### open_at : `2022-02-06T07:24:57Z`
### last_modified_date : `2022-02-07T08:48:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104406
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
the following example

#include <complex.h>

complex double f (complex double a, complex double b)
{
    return a * b;
}

compiled at -Ofast fails to SLP because at tree level it generates

  a$real_5 = REALPART_EXPR <a_1(D)>;
  a$imag_6 = IMAGPART_EXPR <a_1(D)>;
  b$real_7 = REALPART_EXPR <b_2(D)>;
  b$imag_8 = IMAGPART_EXPR <b_2(D)>;
  _9 = a$real_5 * b$real_7;
  _10 = a$imag_6 * b$imag_8;
  _11 = a$real_5 * b$imag_8;
  _12 = a$imag_6 * b$real_7;
  _13 = _9 - _10;
  _14 = _11 + _12;
  _3 = COMPLEX_EXPR <_13, _14>;

But SLP discovery does not attempt to start at _13 and _14, instead starts at the multiplies:

 note:   Starting SLP discovery for
 note:     _11 = a$real_5 * b$imag_8;
 note:     _12 = a$imag_6 * b$real_7;

And ultimately fails because it doesn't know what to do with REALPART_EXPR and IMAGPART_EXPR.


---


### compiler : `gcc`
### title : `SLP discovery fails due to -Ofast rewriting`
### open_at : `2022-02-06T09:13:14Z`
### last_modified_date : `2022-02-07T10:55:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104408
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The following testcase:

typedef struct { float r, i; } cf;
void
f (cf *restrict a, cf *restrict b, cf *restrict c, cf *restrict d, cf e)
{
  for (int i = 0; i < 100; ++i)
    {
      b[i].r = e.r * (c[i].r - d[i].r) - e.i * (c[i].i - d[i].i);
      b[i].i = e.r * (c[i].i - d[i].i) + e.i * (c[i].r - d[i].r);
    }
}

when compiled at -O3 forms an SLP tree but fails at -Ofast because match.pd rewrites the expression into 

      b[i].r = e.r * (c[i].r - d[i].r) + e.i * (d[i].i - c[i].i);
      b[i].i = e.r * (c[i].i - d[i].i) + e.i * (c[i].r - d[i].r);

and so introduces a different interleaving in the second multiply operation.

It's unclear to me what the gain of actually doing this is as it results in worse vector and scalar code due to you losing the sharing of the computed value of the nodes.

Without the rewriting the first code can re-use the load from the first vector and just reverse the elements:

.L2:
        ldr     q1, [x3, x0]
        ldr     q0, [x2, x0]
        fsub    v0.4s, v0.4s, v1.4s
        fmul    v1.4s, v2.4s, v0.4s
        fmul    v0.4s, v3.4s, v0.4s
        rev64   v1.4s, v1.4s
        fneg    v0.2d, v0.2d
        fadd    v0.4s, v0.4s, v1.4s
        str     q0, [x1, x0]
        add     x0, x0, 16
        cmp     x0, 800
        bne     .L2

While with the rewrite it forces an increase in VF to be able to handle the interleaving

.L2:
        ld2     {v0.4s - v1.4s}, [x3], 32
        ld2     {v4.4s - v5.4s}, [x2], 32
        fsub    v2.4s, v1.4s, v5.4s
        fsub    v3.4s, v4.4s, v0.4s
        fsub    v5.4s, v5.4s, v1.4s
        fmul    v2.4s, v2.4s, v6.4s
        fmul    v4.4s, v6.4s, v3.4s
        fmla    v2.4s, v7.4s, v3.4s
        fmla    v4.4s, v5.4s, v7.4s
        mov     v0.16b, v2.16b
        mov     v1.16b, v4.16b
        st2     {v0.4s - v1.4s}, [x1], 32
        cmp     x5, x1
        bne     .L2

in scalar you lose the ability to re-use the subtract so you get an extra sub.


---


### compiler : `gcc`
### title : `union initialization of a vector is not optimized due to vector cost model`
### open_at : `2022-02-06T22:31:15Z`
### last_modified_date : `2023-09-21T12:52:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104412
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

typedef int64_t v2i64 __attribute__((vector_size(16)));

v2i64 _mm_set_epi64x(int64_t i1, int64_t i2)
{
    union {
        int64_t data[2];
        v2i64 v;
    } d = {.data = {i2, i1}};
    return d.v;
}

With -O3, AArch64 GCC outputs this:

_mm_set_epi64x(long, long):
  sub sp, sp, #16
  stp x1, x0, [sp]
  ldr q0, [sp]
  add sp, sp, 16
  ret

LLVM instead outputs this:

_mm_set_epi64x(long, long):
  fmov d0, x1
  mov v0.d[1], x0
  ret


---


### compiler : `gcc`
### title : `_mm_set1_epi8 isn't optimized for SSE2`
### open_at : `2022-02-07T05:18:30Z`
### last_modified_date : `2022-02-07T12:57:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104413
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
[hjl@gnu-tgl-3 tmp]$ cat foo.c
#include <x86intrin.h>

__m128i
foo (char c)
{
  return _mm_set1_epi8 (c);
}
[hjl@gnu-tgl-3 tmp]$ cat foo.s
	.file	"foo.c"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB5670:
	.cfi_startproc
	movd	%edi, %xmm0
	punpcklbw	%xmm0, %xmm0
	punpcklwd	%xmm0, %xmm0
	pshufd	$0, %xmm0, %xmm0
	ret
	.cfi_endproc
.LFE5670:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 11.2.1 20220127 (Red Hat 11.2.1-9)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-tgl-3 tmp]$ 

GCC should generate

	movd	%edi, %xmm0
	pxor %xmm1, %xmm1;
	pshufb %xmm1, %xmm0
	ret


---


### compiler : `gcc`
### title : `Combine optimization opportunity exposed after pro_and_epilogue`
### open_at : `2022-02-08T04:29:08Z`
### last_modified_date : `2022-02-25T01:06:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104438
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
#include<stdint.h>
#include<immintrin.h>

static __m256i __attribute__((always_inline)) load8bit_4x4_avx2(const uint8_t *const src,
    const uint32_t stride)
{
    __m128i src01, src23;
    src01 = _mm_cvtsi32_si128(*(int32_t*)(src + 0 * stride));
    src01 = _mm_insert_epi32(src01, *(int32_t *)(src + 1 * stride), 1);
    src23 = _mm_cvtsi32_si128(*(int32_t*)(src + 2 * stride));
    src23 = _mm_insert_epi32(src23, *(int32_t *)(src + 3 * stride), 1);
    return _mm256_setr_m128i(src01, src23);
}

uint32_t  compute4x_m_sad_avx2_intrin(
    uint8_t  *src,         // input parameter, source samples Ptr
    uint32_t  src_stride,  // input parameter, source stride
    uint8_t  *ref,         // input parameter, reference samples Ptr
    uint32_t  ref_stride,  // input parameter, reference stride
    uint32_t  height,      // input parameter, block height (M)
    uint32_t  width)       // input parameter, block width (N)
{
    __m128i xmm0;
    __m256i ymm = _mm256_setzero_si256();
    uint32_t y;
    (void)width;

    for (y = 0; y < height; y += 4) {
        const __m256i src0123 = load8bit_4x4_avx2(src, src_stride);
        const __m256i ref0123 = load8bit_4x4_avx2(ref, ref_stride);
        ymm = _mm256_add_epi32(ymm, _mm256_sad_epu8(src0123, ref0123));
        src += src_stride << 2;
        ref += ref_stride << 2;
    }

    xmm0 = _mm_add_epi32(_mm256_castsi256_si128(ymm),
        _mm256_extracti128_si256(ymm, 1));

    return (uint32_t)_mm_cvtsi128_si32(xmm0);
}  




gcc -O2 -mavx2 -S



suboptimal asm

.L4:
        vpxor   xmm3, xmm3, xmm3      # 12        [c=4 l=4]  movv4di_internal/0
        vpxor   xmm0, xmm0, xmm0      # 11        [c=4 l=4]  movv8si_internal/0
        vextracti128    xmm3, ymm3, 0x1 # 409     [c=4 l=6]  vec_extract_hi_v4di
        vpaddd  xmm0, xmm0, xmm3    # 429   [c=4 l=4]  *addv4si3/1
        vmovd   eax, xmm0     # 430     [c=4 l=4]  *movsi_internal/12
        ret       # 437       [c=0 l=1]  simple_return_internal

It can be optimized to just

        xor eax, eax

Before pro_and_epilogue, cfg is like

.L2
...asm...
jmp .L4

.L3:
        vpxor   xmm3, xmm3, xmm3      # 12        [c=4 l=4]  movv4di_internal/0
        vpxor   xmm0, xmm0, xmm0      # 11        [c=4 l=4]  movv8si_internal/0

.L4:

        vextracti128    xmm3, ymm3, 0x1 # 409     [c=4 l=6]  vec_extract_hi_v4di
        vpaddd  xmm0, xmm0, xmm3    # 429   [c=4 l=4]  *addv4si3/1
        vmovd   eax, xmm0     # 430     [c=4 l=4]  *movsi_internal/12
        ret       # 437       [c=0 l=1]  simple_return_internal


And Since there're 2 predecessor bbs for .L4, it can't be optimized off, but after pro_and_epilogue, GCC copy .L4 to .L2 and merge .L4 with .L3, and exposed the opportunity.


---


### compiler : `gcc`
### title : `Missing constant folding in shift expression.`
### open_at : `2022-02-08T19:35:44Z`
### last_modified_date : `2023-10-25T22:06:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104444
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `enhancement`
### contents :
#include <cstdint>

inline bool f(uint32_t m, int n) {
  return (m >> n) != 0;
}

bool g(int n) {
  return f(1 << 24, n);
}

g can be optimised to "return n <= 24". LLVM does that but gcc doesn't.

The example above drove me to another missing optimisation opportunity based on undefined behaviour. (Perhaps a matter for other report?)

bool h(uint32_t m, int n) {
  return (n >= 0 && n < 32) || (m >> n) != 0;
}

If (n >= 0 && n < 32) is false, then (m >> n) is UB (in C++, probably also in C). Therefore, h can be optimised to "return true" but gcc doesn't do that (neither does LLVM).

See here: https://godbolt.org/z/hx9vGe6Kj

If confirmed, these bugs could be added to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=19987

Potentially related:

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95817
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94789#c1


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Wstringop-overflow + atomics incorrect warning on dynamic object`
### open_at : `2022-02-09T19:37:19Z`
### last_modified_date : `2023-09-18T11:03:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104475
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 52399
qfutureinterface.cpp preprocessed

In:
static inline int switch_on(QAtomicInt &a, int which)
{
    return a.fetchAndOrRelaxed(which) | which;
}

static inline int switch_off(QAtomicInt &a, int which)
{
    return a.fetchAndAndRelaxed(~which) & ~which;
}

void QFutureInterfaceBase::setThrottled(bool enable)
{
    QMutexLocker lock(&d->m_mutex);
    if (enable) {
        switch_on(d->state, Throttled);
    } else {
        switch_off(d->state, Throttled);
        if (!(d->state.loadRelaxed() & suspendingOrSuspended))
            d->pausedWaitCondition.wakeAll();
    }
}

Compiling the attached preprocessed sources with:

g++ -Wall -Wextra -march=haswell -O2 -c -o /dev/null qfutureinterface.cpp.ii

Produces:

In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_or(__int_type, std::memory_order) [with _ITp = int]’,
    inlined from ‘static T QAtomicOps<X>::fetchAndOrRelaxed(std::atomic<T>&, typename QAtomicAdditiveType<T>::AdditiveT) [with T = int; X = int]’ at /home/tjmaciei/obj/qt/qt6/qtbase/include/QtCore/../../../../../../src/qt/qt6/qtbase/src/corelib/thread/qatomic_cxx11.h:449:33,
    inlined from ‘T QBasicAtomicInteger<T>::fetchAndOrRelaxed(T) [with T = int]’ at /home/tjmaciei/obj/qt/qt6/qtbase/include/QtCore/../../../../../../src/qt/qt6/qtbase/src/corelib/thread/qbasicatomic.h:168:36,
    inlined from ‘int switch_on(QAtomicInt&, int)’ at /home/tjmaciei/src/qt/qt6/qtbase/src/corelib/thread/qfutureinterface.cpp:59:31,
    inlined from ‘void QFutureInterfaceBase::setThrottled(bool)’ at /home/tjmaciei/src/qt/qt6/qtbase/src/corelib/thread/qfutureinterface.cpp:71:18:
/home/tjmaciei/dev/gcc/include/c++/12.0.1/bits/atomic_base.h:648:33: warning: ‘unsigned int __atomic_or_fetch_4(volatile void*, unsigned int, int)’ writing 4 bytes into a region of size 0 overflows the destination [-Wstringop-overflow=]
  648 |       { return __atomic_fetch_or(&_M_i, __i, int(__m)); }
      |                ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
In member function ‘std::__atomic_base<_IntTp>::__int_type std::__atomic_base<_IntTp>::fetch_and(__int_type, std::memory_order) [with _ITp = int]’,
    inlined from ‘static T QAtomicOps<X>::fetchAndAndRelaxed(std::atomic<T>&, typename QAtomicAdditiveType<T>::AdditiveT) [with T = int; X = int]’ at /home/tjmaciei/obj/qt/qt6/qtbase/include/QtCore/../../../../../../src/qt/qt6/qtbase/src/corelib/thread/qatomic_cxx11.h:425:34,
    inlined from ‘T QBasicAtomicInteger<T>::fetchAndAndRelaxed(T) [with T = int]’ at /home/tjmaciei/obj/qt/qt6/qtbase/include/QtCore/../../../../../../src/qt/qt6/qtbase/src/corelib/thread/qbasicatomic.h:159:37,
    inlined from ‘int switch_off(QAtomicInt&, int)’ at /home/tjmaciei/src/qt/qt6/qtbase/src/corelib/thread/qfutureinterface.cpp:64:32,
    inlined from ‘void QFutureInterfaceBase::setThrottled(bool)’ at /home/tjmaciei/src/qt/qt6/qtbase/src/corelib/thread/qfutureinterface.cpp:73:19:
/home/tjmaciei/dev/gcc/include/c++/12.0.1/bits/atomic_base.h:638:34: warning: ‘unsigned int __atomic_fetch_and_4(volatile void*, unsigned int, int)’ writing 4 bytes into a region of size 0 overflows the destination [-Wstringop-overflow=]
  638 |       { return __atomic_fetch_and(&_M_i, __i, int(__m)); }
      |                ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~

GCC Git commit 1ce5395977f37e8d0c03394f7b932a584ce85cc7, built today.


---


### compiler : `gcc`
### title : `[12 Regression] cond_op is combined without considering single_use`
### open_at : `2022-02-10T06:45:02Z`
### last_modified_date : `2022-02-11T07:52:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104479
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c

void
mc_weight (unsigned int* __restrict dst, unsigned int* __restrict src,
	   int i_width,int i_scale, unsigned int* __restrict y)
{
  for(int x = 0; x < i_width; x++)
    dst[x] =  src[x] >> 3 > 255 ? src[x] >> 3 : y[x];
}

gcc -march=icelake-server -O3


gcc11.2 

        vpsrld  ymm0, YMMWORD PTR [rsi+rax], 3
        vpcmpud k1, ymm0, ymm2, 2
        vmovdqu32       ymm1{k1}, YMMWORD PTR [r8+rax]
        vpcmpud k1, ymm0, ymm2, 6
        vpblendmd       ymm0{k1}, ymm1, ymm0
        vmovdqu YMMWORD PTR [rcx+rax], ymm0

gcc 12

        vmovdqu ymm1, YMMWORD PTR [rsi+rax]
        vpsrld  ymm2, ymm1, 3
        vpcmpud k1, ymm2, ymm3, 2
        vmovdqu32       ymm0{k1}, YMMWORD PTR [r8+rax]
        vpcmpud k1, ymm2, ymm3, 6
        vmovdqa ymm2, ymm0
        vpsrld  ymm2{k1}, ymm1, 3
        vmovdqu YMMWORD PTR [rcx+rax], ymm2

It's because in match.pd

---------------cut----------------
(for uncond_op (UNCOND_BINARY)
     cond_op (COND_BINARY)
 (simplify
  (vec_cond @0 (view_convert? (uncond_op@4 @1 @2)) @3)
  (with { tree op_type = TREE_TYPE (@4); }
   (if (vectorized_internal_fn_supported_p (as_internal_fn (cond_op), op_type)
	&& is_truth_type_for (op_type, TREE_TYPE (@0)))
    (view_convert (cond_op @0 @1 @2 (view_convert:op_type @3))))))
 (simplify
  (vec_cond @0 @1 (view_convert? (uncond_op@4 @2 @3)))
  (with { tree op_type = TREE_TYPE (@4); }
   (if (vectorized_internal_fn_supported_p (as_internal_fn (cond_op), op_type)
	&& is_truth_type_for (op_type, TREE_TYPE (@0)))
    (view_convert (cond_op (bit_not @0) @2 @3 (view_convert:op_type @1)))))))
---------------end-------------------

uncond_op + vec_cond is combined to cond_op w/o considering uncond_op result could be used by others, which caused unoptimal codegen.


---


### compiler : `gcc`
### title : `-freorder-block-and-partition not splitting into sections with __builin_expect()`
### open_at : `2022-02-10T11:22:23Z`
### last_modified_date : `2022-02-24T11:04:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104484
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.2.1`
### severity : `normal`
### contents :
I expected code guarded by __builtin_expect() to be pushed into .text.cold, but that's not happening:

int f1, f2;

inline int is() {
    return __builtin_expect(f1, 0) && f2;
}

void heavy() {
    extern void very_heavy();
    if (is()) {
        very_heavy();
    }
}


void fun();

void light() {
    fun();
    heavy();
    fun();
}


with -O3 -freorder-blocks-and-partition:


light:
.LFB2:
        .cfi_startproc
        subq    $8, %rsp
        .cfi_def_cfa_offset 16
        xorl    %eax, %eax
        call    fun
        movl    f1(%rip), %edx
        testl   %edx, %edx
        jne     .L8        .p2align 4,,10
        .p2align 3
.L8:
        .cfi_restore_state
        movl    f2(%rip), %eax
        testl   %eax, %eax
        je      .L6
        xorl    %eax, %eax
        call    very_heavy
        jmp     .L6
        .cfi_endproc

.L6:
        xorl    %eax, %eax
        addq    $8, %rsp
        .cfi_remember_state
        .cfi_def_cfa_offset 8
        jmp     fun


I expected .L8 to be in another section, but it isn't.


---


### compiler : `gcc`
### title : `x387 fmod inline code is slow`
### open_at : `2022-02-10T13:47:46Z`
### last_modified_date : `2022-02-10T14:18:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104485
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
In 526.blender_r one can see us expanding fmod as

        fld1
        fldl    (%rsi)
.L2:
        fprem
        fnstsw  %ax
        testb   $4, %ah
        jne     .L2
        fstp    %st(1)
...

which is quite a bit slower than just calling into libm.  The case in
question is actually special and can be approximated by

void foo (double * __restrict s, double *d)
{
  s[0] = fmod(d[0], 1.0f);
  s[1] = fmod(d[1], 1.0f);
}

where obtaining the fractional part of {d[0], d[1]} might even be vectorizable.

Building 526.blender_r with -fno-builtin-fmod (-mno-fancy-math-387 doesn't do
the trick here) speeds it up by 1% on Zen2.


---


### compiler : `gcc`
### title : `vector lowering for VEC_COND_EXPR does not consider smaller vectors`
### open_at : `2022-02-11T12:58:38Z`
### last_modified_date : `2022-02-14T07:14:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104501
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following C++ testcase is decomposed to scalar operations by vector lowering when compiling with just SSE2 support.  But when using a vector size of 16 we
can produce optimized code.

typedef int vsi __attribute__((vector_size(32)));

vsi res, v1, v2, v3, v4;

void foo ()
{
  res = v1 == v2 ? v3 : v4;
}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] trivially-destructible destructors interfere with loop optimization - maybe related to lifetime-dse.`
### open_at : `2022-02-12T20:16:20Z`
### last_modified_date : `2023-05-29T10:06:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104515
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
This issue started in GCC-9.1, but a change in GCC-11 made it worse.

It didn't exist in GCC-7.1-GCC-8.5

Short description:
-----------------

When we have a loop that can be optimized out, calling the destructor for a trivially-destructible type will prevent the optimization starting from GCC-9.1

These are loops that correctly optimized out in GCC-7.1 to GCC-8.5

This bug doesn't happen if we set -fno-lifetime-dse

Interestingly enough - a non-trivially-destructible destructor doesn't necessarily prevent the optimization.

How this became worse in GCC-11:
-------------------------------

In GCC-11 this also applies to calling the destructor of basic types (int, long etc.)

So loops that optimized in GCC-7.1 to GCC-10.3 no longer optimize.

Short reproducing example:
-------------------------

NOTE: No `include`s are needed

```
using T = int;
struct Vec {
  T* end;
};
void pop_back_many(Vec& v, unsigned n) {
  for (unsigned i = 0; i < n; ++i) {
    --v.end;
    v.end->~T();
  }
}
```
compiled with `-O3 -Wall`

In GCC-7 to GCC-10, `pop_back_many` optimizes out the loop (becomes `v.end-=n`).
In GCC-11, the loop remains.

See https://godbolt.org/z/vTexxhxP9

NOTE that adding `-fno-lifetime-dse` will re-enable the loop optimization.

Why this matters
----------------
This prevents optimization of a loop over `std::vector<int>::pop_back()`, which is a very common usecase!

Loops that optimize out in GCC-7.1 to GCC-10.3 will suddenly not optimize in GCC-11.1/2, making existing code run MUCH slower! (O(n) instead of O(1))

NOTE: std::vector<int>::resize is a lot slower than loop over pop_back. A loop over pop_back is currently the most efficient way to do pop_back_many!

More complete reproducing example:
---------------------------------

- We can replace the type `T` with a class that is trivially destructible.
**In that case, the problem exists in previous versions of GCC as well**

- We can replace the type `T` with a class that had user-supplied destructor.
**In that case, the loop correctly optimizes out if possible**

Actual examples:
https://godbolt.org/z/7WqTPq3cE

compiled with `-O3 -Wall`
```
template <typename T>
struct Vec {
  T* end;
};

template <typename T>
void pop_back_many(Vec<T>& v, unsigned n) {
  for (unsigned i = 0; i < n; ++i) {
    --v.end;
    v.end->~T();
  }
}

struct TrivialDestruct {
    ~TrivialDestruct()=default;
};

struct NoopDestruct {
    ~NoopDestruct(){}
};

unsigned count=0;
struct CountDestruct {
    ~CountDestruct(){++count;}
};

// Here loop optimization fails in GCC-11.1-11.2
// But succeeds in GCC 7.1-10.3
//
// NOTE that adding -fno-lifetime-dse re-enabled the optimization
template void pop_back_many(Vec<int>&, unsigned);
// Here loop optimization fails in GCC-9.1-11.2
// But succeeds in GCC 7.1-8.5
//
// NOTE that adding -fno-lifetime-dse re-enabled the optimization
template void pop_back_many(Vec<TrivialDestruct>&, unsigned);
// Here loop optimization succeeds in all versions
//
// NOTE that it's surprising that a no-op destructor can be optimized
// but a trivial destructor can't
template void pop_back_many(Vec<NoopDestruct>&, unsigned);
// Here loop optimization succeeds in all version
//
// NOTE that it's surprising that a destructor with an action
// can be optimized, but a trivial destructor can't
template void pop_back_many(Vec<CountDestruct>&, unsigned);
```


---


### compiler : `gcc`
### title : `[12 Regression] Dead Code Elimination Regression at -O3 (trunk vs. 11.2.0)`
### open_at : `2022-02-14T10:07:43Z`
### last_modified_date : `2022-02-16T14:07:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104526
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat case.c #138750
void foo(void);

static int a, b = 1, *c = &b;
int main() {
  for (; a; a--) {
    int d = 2 >> (1 / *c);
    if (!d)
      foo();
  }
}

gcc-58aeb75d4097010ad9bb72b964265b18ab284f93 (trunk) -O3 can not eliminate foo but gcc-11.2.0 -O3 can.

gcc-58aeb75d4097010ad9bb72b964265b18ab284f93 (trunk) -O3 -S -o /dev/stdout case.c
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movl	a(%rip), %eax
	testl	%eax, %eax
	je	.L12
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	xorl	%ebx, %ebx
.L2:
	movl	b(%rip), %ecx
	leal	1(%rcx), %eax
	cmpl	$2, %eax
	movl	$2, %eax
	cmova	%ebx, %ecx
	sarl	%cl, %eax
	testl	%eax, %eax
	je	.L3
	movl	$0, a(%rip)
.L10:
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L3:
	.cfi_restore_state
	call	foo
	subl	$1, a(%rip)
	jne	.L2
	jmp	.L10
.L12:
	.cfi_def_cfa_offset 8
	.cfi_restore 3
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


gcc-11.2.0 -O3 -S -o /dev/stdout case.c
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movl	a(%rip), %eax
	testl	%eax, %eax
	je	.L2
	movl	$0, a(%rip)
.L2:
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=c2b610e7c6c89fd422c5c31f01023bcddf3cf4a5

----- Build information -----
----- 58aeb75d4097010ad9bb72b964265b18ab284f93 (trunk)
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ 
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.1 20220213 (experimental) (GCC)

----- releases/gcc-11.2.0
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ 
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (GCC)


---


### compiler : `gcc`
### title : `[12 Regression] inefficient codegen around new/delete`
### open_at : `2022-02-14T15:51:55Z`
### last_modified_date : `2022-03-04T14:35:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104529
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Consider the following example

#include <cstdint>
#include <cstdlib>
#include <vector>

struct param {
  uint32_t k;
  std::vector<uint8_t> src;
  std::vector<uint8_t> ref0;
};

size_t foo() {
    param test[] = {
    {48, {255, 0, 0, 0, 0, 0}
    }};
    return sizeof(test);
}

where the entire thing should have been elided, but that is already reported in #94294.

Instead this code also shows that we are generating quite inefficient code (even at -Ofast)

on AArch64 we generate:

foo():
        stp     x29, x30, [sp, -32]!
        mov     w1, 255 <-- 1
        mov     x0, 6
        mov     x29, sp
        str     w1, [sp, 24] <-- 1
        strh    wzr, [sp, 28] <-- 2
        bl      operator new(unsigned long)
        ldrh    w3, [sp, 28] <-- 2
        mov     x1, 6
        ldr     w4, [sp, 24] <-- 1
        str     w4, [x0]
        strh    w3, [x0, 4]
        bl      operator delete(void*, unsigned long)
        mov     x0, 56
        ldp     x29, x30, [sp], 32
        ret

There's no reason to spill and rematerialize a constant when the constant is representable in a single move.

It's also unclear to me why it things the 255 and 0 need to be before the call to new.  But even if it did need it, it's better to re-create the constants rather than materializing them again.

However x86 gets this right, which is why I've opened this as a target bug:

foo():
        sub     rsp, 8
        mov     edi, 6
        call    operator new(unsigned long)
        mov     esi, 6
        mov     DWORD PTR [rax], 255
        mov     rdi, rax
        xor     eax, eax
        mov     WORD PTR [rdi+4], ax
        call    operator delete(void*, unsigned long)
        mov     eax, 56
        add     rsp, 8
        ret


---


### compiler : `gcc`
### title : `[12/13 Regression] Dead Code Elimination Regression at -O3 (trunk vs. 11.2.0)`
### open_at : `2022-02-14T16:40:13Z`
### last_modified_date : `2022-11-08T00:23:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104530
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Dead Code Elimination Regression at -O3 (trunk vs. a7fede6704dd207fb32b97bc30f945acc7b953c5) 138751
---------------
cat case.c #138751
void foo(void);

static int a, *b = &a, c, d = 1;

int main() {
    c = 0 == b;
    a = *b;
    if (c % d)
        for (; d; --d)
            foo();
    b = 0;
}

gcc-58aeb75d4097010ad9bb72b964265b18ab284f93 (trunk) -O3 can not eliminate foo but gcc-11.2.0 -O3 can.

gcc-58aeb75d4097010ad9bb72b964265b18ab284f93 (trunk) -O3 -S -o /dev/stdout case.c
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movq	b(%rip), %rax
	testq	%rax, %rax
	movl	(%rax), %edx
	sete	%al
	movzbl	%al, %eax
	movl	%edx, a(%rip)
	cltd
	idivl	d(%rip)
	testl	%edx, %edx
	je	.L9
	pushq	%rax
	.cfi_def_cfa_offset 16
	.p2align 4,,10
	.p2align 3
.L3:
	call	foo
	subl	$1, d(%rip)
	jne	.L3
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rcx, b(%rip)
	popq	%rsi
	.cfi_def_cfa_offset 8
	ret
.L9:
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rdx, b(%rip)
	ret
---------- END OUTPUT ---------


gcc-11.2.0 -O3 -S -o /dev/stdout case.c
--------- OUTPUT ---------

main:
.LFB0:
	.cfi_startproc
	movq	b(%rip), %rax
	movq	$0, b(%rip)
	movl	(%rax), %eax
	movl	%eax, a(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=c6bb1db76b3ac127aff7dacf391fc1798a94bb7d

----- Build information -----
----- 58aeb75d4097010ad9bb72b964265b18ab284f93 (trunk)
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ --prefix=/zdata/compiler_cache/gcc-58aeb75d4097010ad9bb72b964265b18ab284f93
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.1 20220213 (experimental) (GCC)

----- releases/gcc-11.2.0
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0   (GCC)


---


### compiler : `gcc`
### title : `Failed to inline a very simple template function when it's explicit instantiated.`
### open_at : `2022-02-15T04:34:33Z`
### last_modified_date : `2022-02-17T22:52:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104539
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.2.0`
### severity : `normal`
### contents :
Consider:

    template <int>
    //inline
    int f() {
        return 0;
    }
    
    int g() {
        return f<0>() + 1;
    }

Using -O3, I'd expect f to be inlined in g and this is indeed the case:

    g():
      mov eax, 1
      ret

However, if f is explicit instantiated:

    template unsigned f<0>();

then we get a function call (or a jmp if tail call optimisation is possible)

    g():
      sub rsp, 8
      call int f<0>()
      add rsp, 8
      add eax, 1
      ret
    
A (quite unusual, IMHO) workaround is declaring f as inline:

    template <unsigned n>
    inline
    unsigned f() {
        return n;
    }

https://godbolt.org/z/TarsTY3zb


---


### compiler : `gcc`
### title : `std::vector::resize(v.size() - n) produces poor code`
### open_at : `2022-02-15T11:28:33Z`
### last_modified_date : `2022-11-28T23:01:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104547
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.1`
### severity : `normal`
### contents :
The codegen for this is pretty bad:

#include <vector>

void shrink(std::vector<int>& v, unsigned n) {
    v.resize(v.size() - n);
}

_Z6shrinkRSt6vectorIiSaIiEEj:
.LFB865:
        .cfi_startproc
        movq    8(%rdi), %rdx
        movq    (%rdi), %rcx
        movl    %esi, %esi
        movq    %rdx, %rax
        subq    %rcx, %rax
        sarq    $2, %rax
        movq    %rax, %r8
        subq    %rsi, %r8
        jb      .L3
        cmpq    %rax, %r8
        jnb     .L1
        leaq    (%rcx,%r8,4), %rax
        cmpq    %rax, %rdx
        je      .L1
        movq    %rax, 8(%rdi)
.L1:
        ret
.L3:
        pushq   %rax
        .cfi_def_cfa_offset 16
        movl    $.LC0, %edi
        call    _ZSt20__throw_length_errorPKc
        .cfi_endproc



Telling the compiler that v.size() - n doesn't wrap doesn't help at all:

#include <vector>

void shrink(std::vector<int>& v, unsigned n) {
    if (v.size() < n)
      __builtin_unreachable();
    v.resize(v.size() - n);
}

Why do we still have the call to __throw_length_error() there? It's unreachable, because we're only shrinking, not growing.


This is better:

void shrink_pop(std::vector<int>& v, unsigned n) {
    while (n--)
      v.pop_back();
}

_Z10shrink_popRSt6vectorIiSaIiEEj:
.LFB866:
        .cfi_startproc
        testl   %esi, %esi
        je      .L10
        movq    8(%rdi), %rax
        movl    %esi, %esi
        negq    %rsi
        leaq    (%rax,%rsi,4), %rdx
        .p2align 4,,10
        .p2align 3
.L12:
        subq    $4, %rax
        movq    %rax, 8(%rdi)
        cmpq    %rax, %rdx
        jne     .L12
.L10:
        ret
        .cfi_endproc


And this:

void shrink_min(std::vector<int>& v, unsigned n) {
    v.resize(std::min<std::size_t>(v.size(), n));
}

_Z10shrink_minRSt6vectorIiSaIiEEj:
.LFB867:
        .cfi_startproc
        movq    8(%rdi), %rdx
        movq    (%rdi), %rcx
        movl    %esi, %esi
        movq    %rdx, %rax
        subq    %rcx, %rax
        sarq    $2, %rax
        cmpq    %rax, %rsi
        jb      .L54
.L52:
        ret
        .p2align 4,,10
        .p2align 3
.L54:
        leaq    (%rcx,%rsi,4), %rax
        cmpq    %rax, %rdx
        je      .L52
        movq    %rax, 8(%rdi)
        ret
        .cfi_endproc


---


### compiler : `gcc`
### title : `GCC misses basic optimization for restricted pointers`
### open_at : `2022-02-16T18:54:59Z`
### last_modified_date : `2022-02-16T21:37:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104574
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following TU

void g (void);

int 
f (int *restrict p)
{
  p[0] = 7;
  g ();
  return p[0];
}

is compiled by GCC at -O3 to

f:
        pushq   %rbx
        movq    %rdi, %rbx
        movl    $7, (%rdi)
        call    g
        movl    (%rbx), %eax
        popq    %rbx
        ret

Obviously, GCC seems to think that g may modify the data reachable through p. However, if g did that it would cause undefined behavior anyway.

So GCC misses a simple optimization opportunity here; it doesn't have to reload the memory contents after the call to g; in fact, the function will always return 7.

For comparison, Clang compiles the TU to

f:
        pushq   %rax
        movl    $7, (%rdi)
        callq   g
        movl    $7, %eax
        popq    %rcx
        retq


---


### compiler : `gcc`
### title : `vectorizer failed to reduce max & index search together`
### open_at : `2022-02-17T07:02:10Z`
### last_modified_date : `2022-02-18T00:48:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104579
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
int max (int *src, int n, int *position)
{
  int maxInt;
  int maxIndex;
  int i;

  maxInt = src[0];
  maxIndex = 0;
  for (i = 1; i < n; i++) {
    if (maxInt < src[i]) {
      maxInt = src[i];
      maxIndex = i;
    }
  }
  *position = maxIndex;
  return (maxInt);
}

GCC can reduce max and index search separately, but not together.


---


### compiler : `gcc`
### title : `[11 Regression] Unoptimal code for __negdi2 (and others) from libgcc2 due to unwanted vectorization`
### open_at : `2022-02-17T12:30:35Z`
### last_modified_date : `2023-05-29T10:06:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104582
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Following testcase (taken from libgcc):

--cut here--
typedef		 int DItype	__attribute__ ((mode (DI)));
typedef unsigned int UDItype	__attribute__ ((mode (DI)));
typedef		 int TItype	__attribute__ ((mode (TI)));

#define Wtype	DItype
#define UWtype	UDItype
#define DWtype	TItype

#if __BYTE_ORDER__ != __ORDER_LITTLE_ENDIAN__
  struct DWstruct {Wtype high, low;};
#else
  struct DWstruct {Wtype low, high;};
#endif

typedef union
{
  struct DWstruct s;
  DWtype ll;
} DWunion;

DWtype
__negdi2 (DWtype u)
{
  const DWunion uu = {.ll = u};
  const DWunion w = { {.low = -uu.s.low,
		       .high = -uu.s.high - ((UWtype) -uu.s.low > 0) } };

  return w.ll;
}
--cut here--

compiles with -O2 on x86_64 to:

__negdi2:
        movq    %rdi, %rax
        negq    %rsi
        negq    %rax
        cmpq    $1, %rdi
        adcq    $-1, %rsi
        movq    %rax, %xmm0
        movq    %rsi, %xmm1
        punpcklqdq      %xmm1, %xmm0
        movaps  %xmm0, -24(%rsp)
        movq    -24(%rsp), %rax
        movq    -16(%rsp), %rdx
        ret

Please note the convoluted sequence to move the value at the end.

gcc-10 compiles the code to:

__negdi2:
        negq    %rsi
        movq    %rdi, %rax
        negq    %rax
        movq    %rsi, %rdx
        cmpq    $1, %rdi
        adcq    $-1, %rdx
        ret


---


### compiler : `gcc`
### title : `memset loses alignment infomation in some cases`
### open_at : `2022-02-18T05:24:30Z`
### last_modified_date : `2022-02-20T16:19:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104588
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `11.2.0`
### severity : `normal`
### contents :
https://gcc.godbolt.org/z/M3YoaYeEf

```c++
struct alignas(16) foo
  {
    unsigned char a[32];
    foo() : a() { }
  };t

struct alignas(16) bar
  {
    foo f;
    long m,n,p,q;
    bar() : f(),m(),n(),p(),q() { }
  };

constexpr void* operator new(unsigned long, void* p) noexcept { return p;  }
inline void operator delete(void*, void*) noexcept { }

bar* construct(bar* p)
  {
    return new(p) bar();
  }
```


For `bar::bar()` with -O3, GCC generates two `MOVUPS` followed by two `MOVAPS`, while Clang generates four `MOVAPS`. `MOVUPS` was slow on some older CPUs.


GCC output:

```
construct(bar*):
        pxor    xmm0, xmm0
        mov     rax, rdi
        movups  XMMWORD PTR [rdi], xmm0
        movups  XMMWORD PTR [rdi+16], xmm0
        pxor    xmm0, xmm0
        movaps  XMMWORD PTR [rdi+32], xmm0
        movaps  XMMWORD PTR [rdi+48], xmm0
        ret
```

Clang output:

```
construct(bar*):                      # @construct(bar*)
        mov     rax, rdi
        xorps   xmm0, xmm0
        movaps  xmmword ptr [rdi + 48], xmm0
        movaps  xmmword ptr [rdi + 32], xmm0
        movaps  xmmword ptr [rdi + 16], xmm0
        movaps  xmmword ptr [rdi], xmm0
        ret
```


---


### compiler : `gcc`
### title : `ppc64: even/odd permutation for VSX 64-bit to 32-bit conversions is no longer necessary.`
### open_at : `2022-02-18T08:35:23Z`
### last_modified_date : `2022-03-08T16:20:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104590
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Created attachment 52473
proposal patch remove the extra shuffles

The following VSX intrinsics did not need to exist in the first place, and they must be replaced or renamed by convinced names suitable to the mapped instructions or at least modified by removing the unnecessary shuffles:

- vector unsigned int vec_unsignede(vector double) -> xvcvdpuxws
- vector unsigned int vec_unsignedo(vector double) -> xvcvdpuxws
- vector float vec_floate(vector double) -> xvcvdpsp
- vector float vec_floato(vector double) -> xvcvdpsp
- vector float vec_floate(vector signed long long) -> xvcvsxdsp
- vector float vec_floato(vector signed long long) -> xvcvsxdsp
- vector float vec_floate(vector unsigned long long) -> xvcvuxdsp
- vector float vec_floato(vector unsigned long long) -> xvcvuxdsp

According to the latest update of ISA 3.1:

Previous versions of the architecture allowed the contents of bits 32:63 of each doubleword in the result register to be undefined, however, all processors that support this instruction write the result into bits 32:63 of each doubleword in the result register
as well as into bits 0:31, as is required by this version of the architecture.


---


### compiler : `gcc`
### title : `unvectorized loop due to bool condition loaded from memory`
### open_at : `2022-02-18T10:05:59Z`
### last_modified_date : `2022-05-05T08:39:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104595
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
For the case:

#include "stdbool.h"
#define N 256
typedef char T;
extern T a[N];
extern T b[N];
extern T c[N];
extern bool pb[N];
extern char pc[N];

void predicate_by_bool() {
  for (int i = 0; i < N; i++)
    c[i] = pb[i] ? a[i] : b[i];
}

void predicate_by_char() {
  for (int i = 0; i < N; i++)
    c[i] = pc[i] ? a[i] : b[i];
}

Simply compiled with -Ofast -mcpu=power10, vectorizer can vectorize the 2nd function predicate_by_char but can't vectorize the first. It seems currently GCC just supports very limited case with bool types such as some patterns in vect_recog_bool_pattern.

I guess here the size of bool seems to be a problem, for the size of bool, C says "An object declared as type _Bool is large enough to store the values 0 and 1.", C++ says "The value of sizeof(bool) is implementation defined and might differ from 1.". But the "implementation defined" looks to be compiler defined? then compiler should be aware of it when compiling. If so, we can use the equivalent size type for the load instead and make it compare with zero to get the predicate just like the char variant, I think the expectation to see both these loops vectorized is reasonable then?


---


### compiler : `gcc`
### title : `VCE<integer_type>(vector){} should be converted (or expanded) into BIT_INSERT_EXPR`
### open_at : `2022-02-18T21:15:31Z`
### last_modified_date : `2022-02-21T08:45:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104600
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
When I looked at PR 104582, I Noticed that we had:

 _14 = {_1, _5};
  _8 = VIEW_CONVERT_EXPR<__int128>(_14);

Which can be converted into (with the ordering corrected for endianness):
t1 = (__128)_1
_8 = BIT_INSERT_EXPR(t1, 64, _5);

You can see this by taking the following testcases:

#define vector __attribute__((vector_size(16)))

__int128 f(long a, long b)
{
  vector long t = {a, b};
  return (__int128)t;
}

void f1(__int128 *t1, long a, long b)
{
  vector long t = {a, b};
  *t1 =  (__int128)t;
}

void f2(__int128 *t1, long a, long b)
{
  vector long t = {a, b};
  *t1 =  ((__int128)t) + 1;
}


f2 is really bad for x86_64 as GCC does a store to the stack and then loads back.

Note if you use | instead of +, GCC does the right thing even.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] wrong detection of  -Warray-bounds for interesting tail resusive case`
### open_at : `2022-02-19T05:21:21Z`
### last_modified_date : `2023-07-07T10:42:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104603
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.3.0`
### severity : `normal`
### contents :
Created attachment 52477
a minimal sample of the bug

g++-10.3.0 and g++-11.2 -O2 -Warray-bounds on Ubuntu 20.04.3 LTS show wrong warnings to the attachment code.

g++ -O2 -Warray-bounds -DA t.cpp does not show the warnings.
g++-9 -O2 -Warray-bounds does not, too.

---
>cat t.cpp
struct Base {
  bool isX_;
  Base(bool isX = false) : isX_(isX) { }
  bool isX() const { return isX_; }
  bool operator==(const Base& rhs) const;
};

struct X : public Base {
  X(const Base& b) : Base(true), b_(b) { }
  bool operator==(const X& rhs) const { return b_ == rhs.b_; }
  Base b_;
};

inline bool Base::operator==(const Base& rhs) const
{
    return isX() && rhs.isX() && static_cast<const X&>(*this) == static_cast<const X&>(rhs);
}

Base base;

#ifndef A
void f()
{
  X(base) == X(base);
}
#endif

int main()
{
#ifdef A
  X(base) == X(base);
#endif
}
---

---
% g++-10 --version
g++-10 (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0
% g++-10 -O2 -Warray-bounds array-bounds-bug.cpp
array-bounds-bug.cpp: In function 'void f()':
array-bounds-bug.cpp:18:29: warning: array subscript 2 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:9: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |         ^
array-bounds-bug.cpp:18:29: warning: array subscript 2 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:20: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |                    ^
array-bounds-bug.cpp:18:29: warning: array subscript 3 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:9: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |         ^
array-bounds-bug.cpp:18:29: warning: array subscript 3 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:20: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |                    ^
array-bounds-bug.cpp:18:29: warning: array subscript 4 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:9: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |         ^
array-bounds-bug.cpp:18:29: warning: array subscript 4 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:20: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |                    ^
array-bounds-bug.cpp:18:29: warning: array subscript 5 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:9: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |         ^
array-bounds-bug.cpp:18:29: warning: array subscript 5 is outside array bounds of 'X [1]' [-Warray-bounds]
   18 |   bool isX() const { return isX_; }
      |                             ^~~~
array-bounds-bug.cpp:38:20: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |                    ^
array-bounds-bug.cpp:24:51: warning: array subscript 2 is outside array bounds of 'X [1]' [-Warray-bounds]
   24 |   bool operator==(const X& rhs) const { return b_ == rhs.b_; }
      |                                                ~~~^~~~~~~~~
array-bounds-bug.cpp:38:9: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
      |         ^
array-bounds-bug.cpp:24:58: warning: array subscript 2 is outside array bounds of 'X [1]' [-Warray-bounds]
   24 |   bool operator==(const X& rhs) const { return b_ == rhs.b_; }
      |                                                      ~~~~^~
array-bounds-bug.cpp:38:20: note: while referencing '<anonymous>'
   38 |   X(base) == X(base);
---


---


### compiler : `gcc`
### title : `memcmp () == 0 can be optimized better for avx512f`
### open_at : `2022-02-21T11:00:42Z`
### last_modified_date : `2023-10-10T07:37:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104610
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f(char *a)
{
    char t[] = "0123456789012345678901234567890";
    return __builtin_memcmp(a, &t[0], sizeof(t)) == 0;
}

----- CUT ----
GCC does this via branches and compares but clang/LLVM does:

        vmovdqu (%rdi), %ymm0
        vpxor   .LCPI0_0(%rip), %ymm0, %ymm0
        vptest  %ymm0, %ymm0
        sete    %al
        vzeroupper


---


### compiler : `gcc`
### title : `memcmp/strcmp/strncmp can be optimized when the result is tested for [in]equality with 0 on aarch64`
### open_at : `2022-02-21T11:06:04Z`
### last_modified_date : `2023-09-28T11:35:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104611
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:

bool f(char *a)
{
    char t[] = "0123456789012345678901234567890";
    return __builtin_memcmp(a, &t[0], sizeof(t)) == 0;
}

Right now GCC uses branches to optimize this but this could be done via a few loads followed by xor (eor) of the two sides and then oring the results of xor
and then umavx and then comparing that to 0. This can be done for the middle-end code too if there is a max reduction opcode.


---


### compiler : `gcc`
### title : `Missed optimization about reading backwards`
### open_at : `2022-02-22T08:36:04Z`
### last_modified_date : `2023-09-21T14:09:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104632
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `enhancement`
### contents :
This is a piece of code that has been simplified from a Boyer-Moore-Horspool implementation:

https://gcc.godbolt.org/z/766GYM8xf
```c++
// In real code this was
//   `load_le32_backwards(::std::reverse_iterator<const unsigned char*> ptr)
unsigned
load_le32_backwards(const unsigned char* ptr)
  {
    unsigned word =    ptr[-1];
    word = word << 8 | ptr[-2];
    word = word << 8 | ptr[-3];
    word = word << 8 | ptr[-4];
    return word;
  }
```

This is equivalent to `return ((unsigned*)ptr)[-1];` on x86_64, but GCC fails to optimize it:

GCC output:
```
load_le32_backwards(unsigned char const*):
        movzx   edx, BYTE PTR [rdi-1]
        movzx   eax, BYTE PTR [rdi-2]
        sal     edx, 8
        or      eax, edx
        movzx   edx, BYTE PTR [rdi-3]
        sal     eax, 8
        or      edx, eax
        movzx   eax, BYTE PTR [rdi-4]
        sal     edx, 8
        or      eax, edx
        ret
```

Clang output:
```
load_le32_backwards(unsigned char const*):             # @load_le32_backwards(unsigned char const*)
        mov     eax, dword ptr [rdi - 4]
        ret
```


---


### compiler : `gcc`
### title : `[12 Regression] Useless loop not fully optimized anymore`
### open_at : `2022-02-22T12:53:12Z`
### last_modified_date : `2022-04-11T09:03:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104639
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
the following code compiled with -02
-------------------
bool foo(int i) {
    while (i == 4)
        i += 2;
    return i;
}
------------------

Trunk generates the following assembly
---------
foo(int):
        cmp     edi, 4
        mov     eax, 6
        cmove   edi, eax
        test    edi, edi
        setne   al
        ret
---------

whereas 11.2 generate more optimized assembly

---------
foo(int):
        test    edi, edi
        setne   al
        ret
--------


---


### compiler : `gcc`
### title : `[12 Regression] i ? i % 2 : 0 not optimized anymore`
### open_at : `2022-02-22T19:13:11Z`
### last_modified_date : `2022-04-01T09:52:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104645
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Was told to file a new PR for that case :

------------------
int foo(unsigned i) {
    return i ? i % 2 : 0;
}
------------------

With trunk
------------------------
foo(unsigned int):
        mov     eax, edi
        xor     edx, edx
        and     eax, 1
        test    edi, edi
        cmove   eax, edx
        ret
-----------------------

With 11.2
-----------------------
foo(unsigned int):
        mov     eax, edi
        and     eax, 1
        ret
-----------------------

According to Jakub Jelinek in PR104639 it started with
r12-5358-g32221357007666124409ec3ee0d3a1cf263ebc9e


---


### compiler : `gcc`
### title : `Inefficient vectorization using mask CTORs`
### open_at : `2022-02-23T10:25:48Z`
### last_modified_date : `2022-05-04T13:15:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104658
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Originally observed as part of PR101636 - we are sometimes mixing mask & non-mask
vector defs resulting in external defs for SLP nodes of mask type which are not code-generated efficiently, resulting in

     <signed-boolean:1> _135 = _49 ? -1 : 0;
     _144 = { _135, .... }

and quite awful bit-insert code.  That's not expected cost wise and the appropriate fix is to the bool pattern recog which should have prepared the
IL to avoid the situation.


---


### compiler : `gcc`
### title : `[12 Regression] C-ray 1.1 performance is 50% worse at -O2 in GCC 12 than before on alderlake`
### open_at : `2022-02-23T14:19:20Z`
### last_modified_date : `2023-05-17T00:12:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104663
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Check the first graph at https://www.phoronix.com/scan.php?page=article&item=gcc12-feb-alderlake&num=3


---


### compiler : `gcc`
### title : `Failure to recognize memcpy`
### open_at : `2022-02-23T16:50:07Z`
### last_modified_date : `2022-02-24T09:57:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104665
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
The following serialization code:

void serialize_le(std::byte* __restrict dst, const std::uint32_t* __restrict src)
{
    for (int i = 0; i < 32; ++i, ++src)
    {
        *dst++ = static_cast<std::byte>((*src >>  0) & 0xff);
        *dst++ = static_cast<std::byte>((*src >>  8) & 0xff);
        *dst++ = static_cast<std::byte>((*src >> 16) & 0xff);
        *dst++ = static_cast<std::byte>((*src >> 24) & 0xff);
    }
}

is not recognized as just copying memory.
Both clang and gcc fail to optimize it properly, however gcc creates an "interesting" mess.
Flags used are `-std=c++17 -O3 -march=haswell`


<https://godbolt.org/z/GM8jqssYd>


---


### compiler : `gcc`
### title : `gcc and libatomic can use SSE for 128-bit atomic loads on Intel and AMD CPUs with AVX`
### open_at : `2022-02-25T14:22:23Z`
### last_modified_date : `2023-05-13T13:02:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104688
### status : `NEW`
### tags : `missed-optimization, patch`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
In Dec 2021, Intel updated the SDM and added the following content:

> Processors that enumerate support for Intel® AVX (by setting the feature flag CPUID.01H:ECX.AVX[bit 28]) guarantee that the 16-byte memory operations performed by the following instructions will always be carried out atomically:
> - MOVAPD, MOVAPS, and MOVDQA.
> - VMOVAPD, VMOVAPS, and VMOVDQA when encoded with VEX.128.
> - VMOVAPD, VMOVAPS, VMOVDQA32, and VMOVDQA64 when encoded with EVEX.128 and k0 (masking disabled).
> 
> (Note that these instructions require the linear addresses of their memory operands to be 16-byte aligned.)

(see Change 13, https://cdrdv2.intel.com/v1/dl/getContent/671294)

So we can use SSE for Intel CPUs with AVX, instead of a loop with LOCK CMPXCHG16B.

AMD has no such guarantee (at least for now), so we still need LOCK CMPXCHG16B on old Intel CPUs and (old or new) AMD CPUs.


---


### compiler : `gcc`
### title : `[OpenMP] component/array-ref/component (x.r[1].d) should use 'x' for GOMP_MAP_STRUCT (not yield 'x.r[1]' for nonptr 'x.r')`
### open_at : `2022-02-26T00:06:24Z`
### last_modified_date : `2023-05-08T12:23:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104696
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp, wrong-code`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 52519
implicit.f90 - compile with -fopenmp and run with a non-shared-memory device.

In my understanding, the following is valid and should work.

However, it fails – and I bet it is due to the implicit mapping of 'var3'. The code uses:

!$omp target map(to: var3%R(2)%d)
 ...

Printing on the host the  'loc (var3%R(2)%d)'  shows:
         21516C0
    7F51E5E001D8
    STOP 11

As var%R(2)%d  (or in the dump 'var3.r[1].d.data')  is now in device address space,
accessing it after the target region crashes the program.


implicit.f90.005t.original:
  #pragma omp target
     map(to:var3.r[1].d [len: 88])
     map(to:*(struct t2[0:] *) var3.r[1].d.data [len: D.4243 * 4])
     map(always_pointer:(struct t2[0:] *) var3.r[1].d.data [pointer assign, bias: 0])

implicit.f90.006t.gimple:
  #pragma omp target num_teams(1) thread_limit(0)
     map(tofrom:var3 [len: 440][implicit])
     map(to:var3.r[1].d [len: 88])
     map(to:MEM <struct t2[0:]> [(struct t2[0:] *)_9] [len: _8])
     map(always_pointer:var3.r[1].d.data [pointer assign, bias: 0])


---


### compiler : `gcc`
### title : `Inefficient code for DI to TI sign extend on power10`
### open_at : `2022-02-26T03:58:43Z`
### last_modified_date : `2023-10-13T22:30:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104698
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
On power10, signed conversion from DImode to TImode is inefficient for GCC 11 and the current GCC 12.  GCC 10 does not do this optimization.

On power10, GCC tries to generate the 'vextsd2q' instruction.  However, to generate this instruction, it would typically generate a 'mtvsrsdd' instruction to get the TImode value into an Altivec register in the bottom 64-bits, then it does the vextsd2g instruction, and finally it generates 'mfvsrd' and 'mfvsrld' instructions to get the value back into the GPR registers.

For power9, it generates a move instruction and then an arithmetic shift right 63 bits to fill the upper word with the copy of the sign bit.

GCC should generate the following code sequences:

1) For GPR register to GPR register: Move register, and 'sradi' to create the sign bits in the upper word.

2) For GPR register to VSX register to Altivec register: Splat the value to fill the bottom 64 bits, and then do 'vextsd2q'.

3) For memory to GPR register, load the value into the low register, and fill the high register with the sign bit.

4) For memory to Altivec register, load the value with load VSX vector rightmost doubleword, and then do 'vextsd2q'.


---


### compiler : `gcc`
### title : `[12/13/14 regression] Redundant usage of stack`
### open_at : `2022-03-01T08:35:42Z`
### last_modified_date : `2023-05-08T12:23:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104723
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
bool f256(char *a)
{
  char t[] = "012345678901234567890123456789012345678901234567";
  return __builtin_memcpy(a, &t[0], sizeof(t)) == 0;
}

https://godbolt.org/z/jcjbT4d8e

gcc12 generates

        vmovdqa64       ymm31, YMMWORD PTR .LC0[rip]
        xor     eax, eax
        vmovdqu64       YMMWORD PTR [rsp-72], ymm31
        vmovdqa64       ymm31, YMMWORD PTR .LC1[rip]
        vmovdqu64       YMMWORD PTR [rsp-55], ymm31
        vmovdqu64       ymm31, YMMWORD PTR [rsp-72]
        vmovdqu64       YMMWORD PTR [rdi], ymm31
        vmovdqu64       ymm31, YMMWORD PTR [rsp-55]
        vmovdqu64       YMMWORD PTR [rdi+17], ymm31

Why build “unaligned string" by stack instead of putting it directly into the constant pool.

gcc 11 seems fine.

f256(char*):
        vmovdqa xmm0, XMMWORD PTR .LC0[rip]
        mov     BYTE PTR [rdi+48], 0
        vmovdqu XMMWORD PTR [rdi], xmm0
        vmovdqa xmm0, XMMWORD PTR .LC1[rip]
        xor     eax, eax
        vmovdqu XMMWORD PTR [rdi+16], xmm0
        vmovdqa xmm0, XMMWORD PTR .LC2[rip]
        vmovdqu XMMWORD PT


---


### compiler : `gcc`
### title : `gcc.dg/pr102892-1.c FAILs`
### open_at : `2022-03-02T10:37:24Z`
### last_modified_date : `2023-05-08T12:24:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104754
### status : `NEW`
### tags : `missed-optimization, testsuite-fail`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The gcc.dg/pr102892-1.c test FAILs on Solaris/SPARC (32 and 64-bit) since its
introduction:

+FAIL: gcc.dg/pr102892-1.c (test for excess errors)

Excess errors:
Undefined                       first referenced
 symbol                             in file
foo                                 /var/tmp//ccwHT.Oa.o

According to gcc-testresults, there are quite a number of other targets affected
as well.


---


### compiler : `gcc`
### title : `[12 Regression] x86_64 538.imagick_r 8%-28% regressions and 10% 525.x264_r regressions after r12-7319-g90d693bdc9d718`
### open_at : `2022-03-02T16:20:27Z`
### last_modified_date : `2022-03-11T14:04:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104762
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Around 22nd of February 2022, SPEC 2017 538.imagick_r regressed on all
x86_64 systems used by our periodic benchmarker.  I have bisected the
zen3 -Ofast -march=native -flto case to revision
r12-7319-g90d693bdc9d718 but I think most if not all of the
regressions are caused by this:

commit 90d693bdc9d71841f51d68826ffa5bd685d7f0bc
Author: Richard Biener <rguenther@suse.de>
Date:   Fri Feb 18 14:32:14 2022 +0100

    target/99881 - x86 vector cost of CTOR from integer regs

    This uses the now passed SLP node to the vectorizer costing hook
    to adjust vector construction costs for the cost of moving an
    integer component from a GPR to a vector register when that's
    required for building a vector from components.  A cruical difference
    here is whether the component is loaded from memory or extracted
    from a vector register as in those cases no intermediate GPR is involved.

    The pr99881.c testcase can be Un-XFAILed with this patch, the
    pr91446.c testcase now produces scalar code which looks superior
    to me so I've adjusted it as well.


List of (selected) regressions with links to LNT graphs:

zen2 -O2 regressed by 26%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=297.507.0

zen2 -O2 -flto regressed by 26% too:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=296.507.0

zen2 -Ofast -march=native by 28%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=295.507.0

zen2 -Ofast -march=native -flto by 23:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=287.507.0


zen3 -O2 by 8%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=470.507.0

zen3 -O2 -march=native by 18%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=472.507.0

zen3 -Ofast -march=native by 17%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=471.507.0

zen3 -Ofast -march=native -flto by 10%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=475.507.0


kabylake -O2 by 9%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=226.507.0
(though this one looks suspiciously noisy)

kabylake -O2 -march=native by 16%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=28.507.0

kabylake -Ofast -march=native by 22%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=32.507.0

kabylake -Ofast -march=native -flto by 15%:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=11.507.0


---


### compiler : `gcc`
### title : `(a * even_cst) & CST does not remove the lower bit from the CST`
### open_at : `2022-03-03T00:11:56Z`
### last_modified_date : `2022-03-07T08:16:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104766
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
auto g(unsigned long x) {
    return (x * 6) % 4096;
}
auto g1(unsigned long x) {
    auto x1 = x * 6;
    return x1 % 4096;
}
----- CUT ----
These two would expect to produce the same results at -O2 but currently does not.
What we get is:
g(unsigned long):
        lea     rax, [rdi+rdi*2]
        add     rax, rax
        and     eax, 4094
        ret
g1(unsigned long):
        lea     rax, [rdi+rdi*2]
        add     rax, rax
        and     eax, 4095
        ret

Notice 4095 vs 4094.


---


### compiler : `gcc`
### title : `compare with 1 not merged with subtract 1`
### open_at : `2022-03-03T16:02:41Z`
### last_modified_date : `2023-10-25T22:10:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104773
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
std::bit_ceil(x) involves if(x == 0 || x == 1) return 1;
and 1u << (32-clz(x-1)).

The compare of course compiles to an unsigned <= 1, which can be done with a sub instead of cmp, producing the value we need as an input for the leading-zero count.  But GCC does *not* do this.  (Neither does clang for x86-64).  I trimmed down the libstdc++ <bit> code into something I could compile even when Godbolt is doesn't have working headers for some ISAs: https://godbolt.org/z/3EE7W5bna

// cut down from libstdc++ for normal integer cases; compiles the same
  template<typename _Tp>
    constexpr _Tp
    bit_ceil(_Tp __x) noexcept
    {
      constexpr auto _Nd = std::numeric_limits<_Tp>::digits;
      if (__x == 0 || __x == 1)
        return 1;
      auto __shift_exponent = _Nd - __builtin_clz((_Tp)(__x - 1u));
      // using __promoted_type = decltype(__x << 1); ... // removed check for x<<n widening the result
      return (_Tp)1u << __shift_exponent;
    }
}


for x86-64 with GCC trunk -O3 -march=ivybridge, we get this inefficient code:

roundup(unsigned int):
        mov     eax, 1
        cmp     edi, 1
        jbe     .L1
        sub     edi, 1        # could have just done a sub in the first place
        bsr     edi, edi      # correctly avoiding a false dependency by *not* using ECX as the destination
        lea     ecx, [rdi+1]  # could have shifted  2<<n instead of  1<<(n+1)
        sal     eax, cl       # 3 uops, vs. 1 for bts is a more efficient way to materialize 1<<n
.L1:
        ret

Also, Ivybridge has no problem with DEC instead of SUB 1, IDK why it's avoiding DEC here but not for Haswell for example.  (Haswell pessimizes by using 32-lzcnt instead of lzcnt^31 or something, or still just BSR because it performs identically on actual Haswell; lzcnt is only faster on AMD)

But this bug report is just about sub/cmp combining, not how to materialize 1<<(n+1) or other stuff:  Better would be

    sub  edi, 1
    jbe  .L1
    bsr  edi, edi
    xor  eax, eax
    inc  edi
    bts  eax, edi      # EAX |= 1<<EDI
    ret
.L1:
    mov  eax, 1
    ret


Intel SnB-family can macro-fuse sub/jbe.  AMD can't, so the change is break-even for front-end uops when the branch is not-taken, and worse when it is taken.  But it's still smaller code-size.

For ARM, clang finds a very clever way to combine it:

roundup(unsigned int):
        subs    r0, r0, #1
        clz     r0, r0
        rsb     r1, r0, #32         @ 32-clz
        mov     r0, #1
        lslhi   r0, r0, r1          @ using flags set by SUBS
        bx      lr                  @ 1<<(32-clz) or just 1

GCC on the other hand does much worse with -O3 -std=gnu++20 -mcpu=cortex-a53 -mthumb

roundup(unsigned int):
        cmp     r0, #1
        itttt   hi
        addhi   r3, r0, #-1
        movhi   r0, #1
        clzhi   r3, r3
        rsbhi   r3, r3, #32
        ite     hi
        lslhi   r0, r0, r3
        movls   r0, #1
        bx      lr

I suspect we could do better by combining the cmp and addhi, and doing `mov r0, #1` outside of predication.  (I think that's a separate bug, planning to report it separately.)  Then one `it` would be enough to cover things, I think.

That would basically reduce it to clang's strategy, although the predication of the clz and rsb are optional.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] -Wstringop-overflow false positive at -O3 for an unrolled loop`
### open_at : `2022-03-04T16:26:50Z`
### last_modified_date : `2023-05-08T12:24:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104789
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
It's originally reported here:
https://github.com/godotengine/godot/issues/58747

$ cat godot-stringop.cpp
#include <cstdint>
#include <cstdio>

uint32_t some_func(const uint32_t components) {
	uint8_t header[8];
	uint32_t header_bytes = 0;
	for (uint32_t i = 0; i < components; i++) {
		header_bytes += 2;
	}
	header_bytes += 2;
	// This works it around, but shouldn't be needed AFAICT.
	//while (header_bytes != 8 && header_bytes % 4 != 0) {
	while (header_bytes % 4 != 0) {
		header[header_bytes++] = 0;
	}
	for (uint32_t i = 0; i < header_bytes; i++) {
		printf("%d\n", header[i]);
	}
	return header_bytes;
}

int main() {
	some_func(1);
	some_func(3);
	return 0;
}

$ g++ godot-stringop.cpp -c -Werror=all -O3
godot-stringop.cpp: In function ‘uint32_t some_func(uint32_t)’:
godot-stringop.cpp:14:40: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
   14 |                 header[header_bytes++] = 0;
      |                 ~~~~~~~~~~~~~~~~~~~~~~~^~~
godot-stringop.cpp:5:17: note: at offset 8 into destination object ‘header’ of size 8
    5 |         uint8_t header[8];
      |                 ^~~~~~
cc1plus: some warnings being treated as errors


---


### compiler : `gcc`
### title : `[nvptx] Use .common directive (available starting ptx isa version 5.0)`
### open_at : `2022-03-11T09:34:51Z`
### last_modified_date : `2022-03-14T03:08:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104879
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
We currently have in the nvptx port in nvptx_option_override:
...
  /* Set flag_no_common, unless explicitly disabled.  We fake common                          
     using .weak, and that's not entirely accurate, so avoid it                               
     unless forced.  */
  if (!OPTION_SET_P (flag_no_common))
    flag_no_common = 1;
...
and in nvptx_output_aligned_decl:
...
  /* If this is public, it is common.  The nearest thing we have to                           
     common is weak.  */
  fprintf (file, "\t%s", TREE_PUBLIC (decl) ? ".weak " : "");
...

[ There's also some optimisation note related to .common:
...
/* Buffer needed to broadcast across workers and vectors.  This is                            
   used for both worker-neutering and worker broadcasting, and                                
   vector-neutering and boardcasting when vector_length > 32.  It is                          
   shared by all functions emitted.  The buffer is placed in shared                           
   memory.  It'd be nice if PTX supported common blocks, because then                         
   this could be shared across TUs (taking the largest size).  */
...
but I'm not sure whether this is safe, perhaps this would require a mutex to make it safe. ]

Starting with ptx isa 5.0, we have the .common directive available in ptx.

However, it "can be used only on variables with .global storage", so it's somewhat limited.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] 416.gamess regression after r12-7612-g69619acd8d9b58`
### open_at : `2022-03-14T11:06:33Z`
### last_modified_date : `2023-05-08T12:24:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104912
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
There's a 8% regression in runtime for 416.gamess when built with -Ofast -mtune=generic on Zen2 (less pronounced when using -march=native).  It seems
to be triggered by r12-7612


---


### compiler : `gcc`
### title : `Miss optimization for vec_setv8hi_0`
### open_at : `2022-03-14T12:23:32Z`
### last_modified_date : `2022-05-11T07:36:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104915
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
#include<immintrin.h>
__m128i
foo (short* p)
{
    return _mm_set_epi32 (0, 0, 0, (unsigned short) ((*(__m16_u *)p)[0]));
}

__m128i
foo1 (short* p)
{
    return _mm_set_epi16 (0, 0, 0, 0, 0, 0, 0, (*(__m16_u *)p)[0]);
}

under avx512fp16, foo could generate vmovw instead of movzx + vmovd, without avx512fp16 foo1 could generate movzx + movd instead of pxor + pinsrw.


---


### compiler : `gcc`
### title : `No runtime alias test required for dependent reductions`
### open_at : `2022-03-14T13:51:52Z`
### last_modified_date : `2022-03-15T09:21:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104917
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The testcase from PR87561 shows a case where we have two in-memory reductions that are possibly dependent but the runtime alias check isn't needed since
we only possibly re-order the summations in the reduction.  Small C testcase:

void foo (double *x, double *y, double * __restrict a, double * __restrict b)
{
  for (int i = 0; i < 1024; ++i)
    {
      x[i] += a[i];
      y[i] += b[i];
    }
}

here x[] and y[] are dependent but we can vectorize this just fine with
-fassociative-math, eliding the runtime alias check.


---


### compiler : `gcc`
### title : `bogus -Wformat-overflow=2 due to missing range for related variables`
### open_at : `2022-03-14T20:24:45Z`
### last_modified_date : `2022-03-14T20:47:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104922
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #104746 +++

As mentioned in bug 104746 comment 5, the following test case triggers -Wformat-overflow (level 2) due to the constraint on i and j not being fully exposed to the warning (each directive on its own can produce at most two bytes/digits, but when one does produce two digits the other must produce exactly one, so the output must fit in four bytes).  The same limitation affects string directives with strings of bounded lengths.

As Andrew explains in bug 104746 comment 13, this can be improved in Ranger, and should be made use of to avoid the warning.

char a[4];

void f (int i, int j)
{
  if (i < 0 || j < 0 || i + j > 19)
    return;

  __builtin_sprintf (a, "%u%u", i, j);
}

a.c: In function ‘f’:
a.c:8:26: warning: ‘%u’ directive writing between 1 and 10 bytes into a region of size 4 [-Wformat-overflow=]
    8 |   __builtin_sprintf (a, "%u%u", i, j);
      |                          ^~
a.c:8:25: note: using the range [0, 4294967295] for directive argument
    8 |   __builtin_sprintf (a, "%u%u", i, j);
      |                         ^~~~~~
a.c:8:25: note: using the range [0, 4294967295] for directive argument
a.c:8:3: note: ‘__builtin_sprintf’ output between 3 and 21 bytes into a destination of size 4
    8 |   __builtin_sprintf (a, "%u%u", i, j);
      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


---


### compiler : `gcc`
### title : `[nvptx] muniform-simt optimization: determine inside/outside SIMT region at compile time`
### open_at : `2022-03-15T11:45:28Z`
### last_modified_date : `2022-03-15T11:46:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104933
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
The switch -muniform-simt attempts to deal with a problem outside simt regions by rewriting the instruction stream.

In the general case we don't know for an insn whether it's inside or outside an simt region, so the reorg pass nvptx_reorg_uniform_simt rewrites all insns.

However, inside the simt regions, the rewrite should have no effect, and this is done by maintaining predicates in some registers which have different values inside and outside an simt region.

But for a function that contains SIMT entry/exit markers, we do known whether an insn is inside or outside and SIMT region, and we could evaluate these predicates at compile time, which might be more optimal, and might make the code more readable.


---


### compiler : `gcc`
### title : `[12 regression] Suboptimal gimple foding for blendvpd under sse4.1`
### open_at : `2022-03-16T05:54:48Z`
### last_modified_date : `2022-03-16T09:00:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104946
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
When working on PR104666, i found

cat test.c

typedef double __m128d __attribute__((__vector_size__(16), __may_alias__));
__m128d sse4_1_blendvpd (__m128d a, __m128d b, __m128d c) __attribute__((__target__("sse4.1")));

__m128d
generic_blendvpd (__m128d a, __m128d b, __m128d c)
{
  return __builtin_ia32_blendvpd (a, b, c);
}

gcc -O2 -msse4.1 -mno-sse4.2

generic_blendvpd:
        movq    rax, xmm2
        movapd  xmm3, xmm0
        test    rax, rax
        jns     .L3
        movapd  xmm0, xmm1
.L3:
        pextrq  rax, xmm2, 1
        unpckhpd        xmm3, xmm3
        test    rax, rax
        jns     .L5
        unpckhpd        xmm1, xmm1
        movapd  xmm3, xmm1
.L5:
        unpcklpd        xmm0, xmm3
        ret

It's because it pcmpgtq is under sse4.2 w/o which vec_cmpv2di will be lower to scalar operations and not combined back.

w/ sse4.2 gcc can generate optimal code.

generic_blendvpd:
        movapd  xmm3, xmm0
        movdqa  xmm0, xmm2
        blendvpd        xmm3, xmm1, xmm0
        movapd  xmm0, xmm3
        ret


---


### compiler : `gcc`
### title : `GCC does not emit branchless code for load next to each other`
### open_at : `2022-03-16T08:51:38Z`
### last_modified_date : `2022-03-21T08:19:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104950
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
In this example GCC fails to emit branchless code while CLANG does.
In the actual application, measurements shows slow down up to a factor 2.
I managed to force branchless (-DBL) but the code is pretty unfriendly
godbolt link (GCC, clang, GCC -DBL 

https://godbolt.org/z/KWY1rjhhY



and here inlined

include <vector>
const float defaultBaseResponse = 0.5;
class DForest {
public:
    //based on FastForest::evaluate() and BDTree::parseTree()
    DForest() {
    }
    float evaluate(const float* features) const;

    std::vector<int> rootIndices_;
    //"node" layout: cut, index, left, right
    struct Node{
        float v; int i,l,r;
        constexpr int eval(float const * f) const {
#ifdef BL 
          auto m = f[i] > v;
          return *((&l) + int(m));
#else
          return f[i] > v ? r : l;
#endif
        }
    };
    std::vector<Node> nodes_;
    std::vector<float> responses_;
    std::vector<float> baseResponses_;
};

float DForest::evaluate(const float* features) const{
    float sum{defaultBaseResponse + baseResponses_[0]};
    for(int index : rootIndices_){
        do {
            index = nodes_[index].eval(features);
        } while (index>0);
        sum += responses_[-index];
    }
    return sum;
}


---


### compiler : `gcc`
### title : `[12 Regression] FAIL: gcc.target/i386/bt-5.c by r12-7687`
### open_at : `2022-03-18T22:25:47Z`
### last_modified_date : `2022-03-22T08:46:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104982
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
On x86-64, r12-7687 caused:

FAIL: gcc.target/i386/bt-5.c scan-assembler-not sar[lq][ \t]
FAIL: gcc.target/i386/bt-5.c scan-assembler-times bt[lq][ \t] 7


---


### compiler : `gcc`
### title : `[12/13/14 Regression] bogus writing 1 byte into a region of size 0 with -fwrapv and -O2 -fpeel-loops since r12-4698-gf6d012338bf87f42`
### open_at : `2022-03-19T17:07:46Z`
### last_modified_date : `2023-09-02T20:02:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104986
### status : `NEW`
### tags : `diagnostic, missed-optimization, needs-bisection`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Hi,

recently started seeing bogus warnings using gcc 12 to build postgres. I reduced the problem using cvise with some manual cleanups / improvements afterwards - certainly doesn't quite make sense anymore, but afaics shows a problem.

Originally I hit this with -O3, but found that -O2 -fpeel-loops is sufficient to trigger the problem.

repro: https://godbolt.org/z/ejK9h6von

code:
struct inet_struct {
  char family;
  char ipaddr[16];
};

void
inetnot(struct inet_struct *dst1, struct inet_struct *dst2, struct inet_struct *src) {
  int nb = src->family ? 4 : 6;
  char *psrc = src->ipaddr;
  char *pdst = dst1 ? dst1->ipaddr : dst2->ipaddr;
  while (nb-- > 0)
    pdst[nb] = psrc[nb];
}


gcc-12 -fwrapv -O2 -fpeel-loops -c network2.i

network2.i: In function ‘inetnot’:
network2.i:12:14: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
   12 |     pdst[nb] = psrc[nb];
      |     ~~~~~~~~~^~~~~~~~~~
network2.i:3:8: note: at offset -1 into destination object ‘ipaddr’ of size 16
    3 |   char ipaddr[16];
      |        ^~~~~~
network2.i:3:8: note: at offset -1 into destination object ‘ipaddr’ of size 16


which afaics is bogus, because the loop terminates before reaching offset -1, the condition is > 0, not >= 0. So the post decrement can't lead to -1 being reached.

version: gcc version 12.0.1 20220314 (experimental) [master r12-7638-g823b3b79cd2] (Debian 12-20220313-1) 

Regards,

Andres


---


### compiler : `gcc`
### title : `[missed optimization] x / y * y == x not optimized to x % y == 0`
### open_at : `2022-03-20T15:19:25Z`
### last_modified_date : `2023-02-10T22:32:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104992
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.1`
### severity : `enhancement`
### contents :
This is especially helpful for constant y because checking division-by-a-constant remainders against zero allows further optimization.

Repro case:

unsigned foo(unsigned x, unsigned y)
{
    return x / y * y == x;
}

GCC -O3:
        mov     eax, edi
        xor     edx, edx
        div     esi
        mov     eax, edi
        sub     eax, edx
        cmp     eax, edi
        sete    al
        movzx   eax, al
        ret

Clang -O3:
        mov     eax, edi
        xor     edx, edx
        div     esi
        xor     eax, eax
        test    edx, edx
        sete    al
        ret


---


### compiler : `gcc`
### title : `[12 regression] gcc.target/powerpc/float128-minmax-3.c fails starting with r12-7687-g3a7ba8fd0cda38`
### open_at : `2022-03-21T15:28:37Z`
### last_modified_date : `2022-04-06T05:46:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105002
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
g:3a7ba8fd0cda387809e4902328af2473662b6a4a, r12-7687-g3a7ba8fd0cda38
make  -k check-gcc RUNTESTFLAGS="powerpc.exp=gcc.target/powerpc/float128-minmax-3.c"
FAIL: gcc.target/powerpc/float128-minmax-3.c scan-assembler \\mxsmaxcqp\\M
FAIL: gcc.target/powerpc/float128-minmax-3.c scan-assembler \\mxsmincqp\\M

The new generated code is worse I believe.

seurer@rain6p1:~/gcc/git/build/gcc-test$ diff float128-minmax-3.s.r12-7686 float128-minmax-3.s.r12-7687
13c13,14
< 	xsmincqp 2,2,3
---
> 	xscmpgtqp 0,3,2
> 	xxsel 34,35,34,32
28c29,30
< 	xsmaxcqp 2,3,2
---
> 	xscmpgtqp 0,3,2
> 	xxsel 34,34,35,32


---


### compiler : `gcc`
### title : `store motion if-change flag causes if-conversion optimization can't be taken.`
### open_at : `2022-03-23T01:29:15Z`
### last_modified_date : `2022-04-08T08:07:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105030
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
// source code
extern void bar (double *, int);

void foo (double a[], int n)
{
  double atemp = 0.5;
  for (int i = 0; i < n; i++)
    if (a[i] < atemp)
      atemp = a[i];
  bar (&atemp, n);
}

// -O3 -fdump-tree-lim2
  if (_4 < atemp.0_5)
    goto <bb 4>; [50.00%]
  else
    goto <bb 5>; [50.00%]

  <bb 4> [local count: 477815112]:
  atemp_lsm.4_24 = _4;
  atemp_lsm_flag.5_25 = 1;

It creates the lsm flag in lim2 pass. So the "then" block has two sets which blocks the if-conversion optimization.

//assemble -O3 -ffast-math -fno-unroll-loops on ppc64le
.L5:
        lfd 0,0(3)
        addi 3,3,8
        fcmpu 0,12,0
        ble 0,.L3
        fmr 12,0
        li 9,1
.L3:
        bdnz .L5
        andi. 9,9,0x1
        beq 0,.L2
        stfd 12,32(1)

Inefficient fcmpu is used. If the source code is tweaked as below, the efficient xvmindp is generated.

// tweaked source code
extern void bar (double *, int);

void foo (double a[], int n)
{
  double atemp = 0.5;
  for (int i = 0; i < n; i++)
    if (a[i] < atemp)
      atemp = a[i];
  double btemp = atemp;
  bar (&btemp, n);
}

//assembly
.L4:
        lxv 0,0(9)
        addi 9,9,16
        xvmindp 12,12,0
        bdnz .L4


---


### compiler : `gcc`
### title : `Suboptimal for vec_concat lower halves of two vectors.`
### open_at : `2022-03-23T05:51:09Z`
### last_modified_date : `2022-05-17T01:35:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105033
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
typedef _Float16 v8hf __attribute__((vector_size (16)));
 typedef _Float16 v4hf __attribute__((vector_size (8)));
 
 v8hf foov (v4hf a, v4hf b)
 {
   return __builtin_shufflevector (a, b, 0, 1, 2, 3, 4, 5, 6, 7);
}

 typedef short v8hi __attribute__((vector_size (16)));
 typedef short v4hi __attribute__((vector_size (8)));
 
 v8hi foov (v4hi a, v4hi b)
 {
   return __builtin_shufflevector (a, b, 0, 1, 2, 3, 4, 5, 6, 7);
}

with -march=skylake-avx512 -O2

_Z4foovDv4_DF16_S_:
        vmovq   xmm2, xmm0
        vmovdqa xmm0, XMMWORD PTR .LC0[rip]
        vmovq   xmm1, xmm1
        vpermi2w        xmm0, xmm2, xmm1
        ret
foov(short __vector(4), short __vector(4)):
        vmovq   xmm2, xmm0
        vmovdqa xmm0, XMMWORD PTR .LC0[rip]
        vmovq   xmm1, xmm1
        vpermi2w        xmm0, xmm2, xmm1
        ret
.LC0:
        .value  0
        .value  1
        .value  2
        .value  3
        .value  8
        .value  9
        .value  10
        .value  11

But with -march=skylake -O2

It can be optimized to

_Z4foovDv4_DF16_S_:
        vmovq   xmm1, xmm1
        vmovq   xmm0, xmm0
        vpunpcklqdq     xmm0, xmm0, xmm1
        ret
foov(short __vector(4), short __vector(4)):
        vmovq   xmm1, xmm1
        vmovq   xmm0, xmm0
        vpunpcklqdq     xmm0, xmm0, xmm1
        ret


---


### compiler : `gcc`
### title : `[11/12/13/14 regression]Suboptimal codegen for min/max with -Os`
### open_at : `2022-03-23T06:50:02Z`
### last_modified_date : `2023-07-07T10:42:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105034
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
#define max(a,b) (((a) > (b))? (a) : (b))
#define min(a,b) (((a) < (b))? (a) : (b))

int foo(int x)
{
  return max(x,0);
}

int bar(int x)
{
  return min(x,0);
}

unsigned int baz(unsigned int x)
{
  return min(x,1);
}

gcc10/11/12 -Os -msse4.1

foo(int):
        movd    xmm0, edi
        xorps   xmm1, xmm1
        pmaxsd  xmm0, xmm1
        movd    eax, xmm0
        ret
bar(int):
        movd    xmm0, edi
        xorps   xmm1, xmm1
        pminsd  xmm0, xmm1
        movd    eax, xmm0
        ret
baz(unsigned int):
        xor     eax, eax
        test    edi, edi
        setne   al
        ret

gcc9.4 -Os -msse4.1

foo(int):
        test    edi, edi
        mov     eax, 0
        cmovns  eax, edi
        ret
bar(int):
        test    edi, edi
        mov     eax, 0
        cmovle  eax, edi
        ret
baz(unsigned int):
        xor     eax, eax
        test    edi, edi
        setne   al
        ret

Os size 
   text    data     bss     dec     hex filename
    178       0       0     178      b2 Os.o


O2 size
   text    data     bss     dec     hex filename
    176       0       0     176      b0 O2.o

https://godbolt.org/z/1sYxdTcKz


---


### compiler : `gcc`
### title : `Suboptimal vectorization for reduction with several elements`
### open_at : `2022-03-26T14:14:24Z`
### last_modified_date : `2022-03-28T08:20:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105062
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The testcase is essentially the same as in PR105053, but here this is about performance, not correctness.

#include <vector>
#include <tuple>
#include <chrono>
#include <algorithm>
#include <random>
#include <iostream>

int main(){
  const long n = 100000000;
  std::vector<std::tuple<int,int,double>> vec;
  vec.reserve(n);
  std::random_device rd;
  std::default_random_engine re(rd());
  std::uniform_int_distribution<int> rand_int;
  std::uniform_real_distribution<double> rand_dbl;
  for(int i=0;i<n;++i)
    vec.emplace_back(rand_int(re), rand_int(re), rand_dbl(re));
  auto start = std::chrono::system_clock::now();
#ifdef SLOW
  {
    int sup = 0;
    for(int i=0;i<n;++i) sup=std::max(sup,std::max(std::get<0>(vec[i]),std::get<1>(vec[i])));
    volatile int noopt0 = sup;
  }
#else
  {
    int sup = 0;
    for(int i=0;i<n;++i) sup=std::max(std::max(sup,std::get<0>(vec[i])),std::get<1>(vec[i]));
    volatile int noopt1 = sup;
  }
#endif
  auto finish = std::chrono::system_clock::now();
  std::cout << std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() << '\n';
}


I compile with -O3 -march=skylake (originally noticed with -march=native on a i7-10875H CPU).

The second loop runs in about 60ms, while the first (compiling with -DSLOW) runs in 80ms. The generated asm also looks very different. For the fast code, the core loop is

.L64:
        vmovdqu (%rax), %ymm3
        addq    $64, %rax
        vpunpckldq      -32(%rax), %ymm3, %ymm0
        vpermd  %ymm0, %ymm2, %ymm0
        vpmaxsd %ymm0, %ymm1, %ymm1
        cmpq    %rdx, %rax
        jne     .L64

which looks nice and compact (well, I think we could do without the vpermd, but it is already great). Now for the slow code, we have

.L64:
        vmovdqu (%rax), %ymm0
        vmovdqu 32(%rax), %ymm10
        vmovdqu 64(%rax), %ymm2
        vmovdqu 96(%rax), %ymm9
        vpermd  %ymm10, %ymm6, %ymm8
        vpermd  %ymm0, %ymm7, %ymm1
        vpblendd        $240, %ymm8, %ymm1, %ymm1
        vpermd  %ymm9, %ymm6, %ymm11
        vpermd  %ymm2, %ymm7, %ymm8
        vpermd  %ymm0, %ymm4, %ymm0
        vpermd  %ymm10, %ymm3, %ymm10
        vpermd  %ymm2, %ymm4, %ymm2
        vpermd  %ymm9, %ymm3, %ymm9
        vpblendd        $240, %ymm11, %ymm8, %ymm8
        vpblendd        $240, %ymm10, %ymm0, %ymm0
        vpblendd        $240, %ymm9, %ymm2, %ymm2
        vpermd  %ymm1, %ymm4, %ymm1
        vpermd  %ymm8, %ymm3, %ymm8
        vpermd  %ymm0, %ymm4, %ymm0
        vpermd  %ymm2, %ymm3, %ymm2
        vpblendd        $240, %ymm8, %ymm1, %ymm1
        vpblendd        $240, %ymm2, %ymm0, %ymm0
        vpmaxsd %ymm0, %ymm1, %ymm1
        subq    $-128, %rax
        vpmaxsd %ymm1, %ymm5, %ymm5
        cmpq    %rdx, %rax
        jne     .L64

It is unrolled once more than the fast code and contains an excessive amount of shuffling. If I understand correctly, it vectorizes a reduction with MAX_EXPR on "sup" but does not consider the operation max(get<0>,get<1>) as being part of this reduction, so it generates code that would make sense if I used 2 different operations like

  sup=std::max(sup,std::get<0>(vec[i])+std::get<1>(vec[i]))

instead of both being the same MAX_EXPR. Maybe, when we discover a reduction, we could check if the elements are themselves computed with the same operation as the reduction and in that case try to make it a "bigger" reduction?


---


### compiler : `gcc`
### title : `GCC thinks pinsrw xmm, mem, 0 requires SSE4.1, not SSE2?  _mm_loadu_si16 bounces through integer reg`
### open_at : `2022-03-26T22:42:35Z`
### last_modified_date : `2022-03-28T09:28:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105066
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
PR99754 fixed the wrong-code for _mm_loadu_si16, but the resulting asm is not efficient without -msse4.1 (as part of -march= most things).  It seems GCC thinks that pinsrw / pextrw with a memory operand requires SSE4.1, like pinsr/extr for b/d/q operand-size.  But actually 16-bit insr/extr only needs SSE2

(We're also not efficiently folding it into a memory source operand for PMOVZXBQ, see below)

https://godbolt.org/z/dYchb6hec shows GCC trunk 12.0.1 20220321

__m128i load16(void *p){
    return _mm_loadu_si16( p );
}


load16(void*):     # no options, or -march=core2 or -mssse3
        movzwl  (%rdi), %eax
        pxor    %xmm1, %xmm1
        pinsrw  $0, %eax, %xmm1   # should be MOVD %eax, or PINSRW mem
        movdqa  %xmm1, %xmm0
        ret

vs. 

load16(void*):      # -msse4.1
        pxor    %xmm1, %xmm1
        pinsrw  $0, (%rdi), %xmm1
        movdqa  %xmm1, %xmm0
        ret


The second version is actually 100% fine with SSE2:
https://www.felixcloutier.com/x86/pinsrw shows that there's only a single opcode for PINSRW xmm, r32/m16, imm8 and it requires SSE2; reg vs. mem source is just a matter of the modr/m byte.

The same problem exists for _mm_storeu_si16 not using pextrw to memory (which is also SSE2), instead bouncing through EAX.  (Insanely still PEXTRW instead of MOVD).

----

There is a choice of strategy here, but pinsrw/extrw between eax and xmm0 is clearly sub-optimal everywhere.  Once we factor out the dumb register allocation that wastes a movdqa, the interesting options are:

    movzwl  (%rdi), %eax      # 1 uop on everything
    movd    %eax, %xmm0       # 1 uop on everything

vs.

    pxor    %xmm0, %xmm0        # 1 uop for the front-end, eliminated on Intel
    pinsrw  $0, (%rdi), %xmm0   # 2 uops  (load + shuffle/merge)


Similarly for extract,

    pextrw  $0, %xmm0, (%rdi)   # 2 uops on most

vs.

    movd    %xmm0, %eax         # 1 uop, only 1/clock even on Ice Lake
    movw    %ax, (%rdi)         # 1 uop

On Bulldozer-family, bouncing through an integer reg adds a lot of latency vs. loading straight into the SIMD unit.  (2 integer cores share a SIMD/FP unit, so movd between XMM and GP-integer is higher latency than most.)  So that would definitely favour pinsrw/pextrw with memory.

On Ice Lake, pextrw to mem is 2/clock throughput: the SIMD shuffle can run on p1/p5.  But MOVD r,v is still p0 only, and MOVD v,r is still p5 only.  So that also favours pinsrw/pextrw with memory, despite the extra front-end uop for pxor-zeroing the destination on load.

Of course, if _mm_storeu_si16 is used on a temporary that's later reloaded, being able to optimize to a movd (and optionally movzx) is very good.  Similar for _mm_loadu_si16 on a value we have in an integer reg, especially if we know it's already zero-extended to 32-bit for just a movd, we'd like to be able to do that.

---

It's also essential that these loads fold efficiently into memory source operands for PMOVZX; pmovzxbq is one of the major use-cases for a 16-bit load.

That may be a separate bug, IDK

https://godbolt.org/z/3a9T55n3q shows _mm_cvtepu8_epi32(_mm_loadu_si32(p)) does fold a 32-bit memory source operand nicely to pmovzxbd (%rdi), %xmm0 which can micro-fuse into a single uop on Intel CPUs (for the 128-bit destination version, not YMM), but disaster with 16-bit loads:

__m128i pmovzxbq(void *p){
    return _mm_cvtepu8_epi64(_mm_loadu_si16(p));
}

pmovzxbq(void*):  # -O3 -msse4.1 -mtune=haswell
        pxor    %xmm0, %xmm0                  # 1 uop
        pinsrw  $0, (%rdi), %xmm0             # 2 uops, one for shuffle port
        pmovzxbq        %xmm0, %xmm0          # 1 uop for the same shuffle port
        ret

(_mm_cvtepu8_epi64 requires SSE4.1 so there's no interaction with the -mno-sse4.1 implementation of the load.)


---


### compiler : `gcc`
### title : `Miss optimization for pmovzxbq.`
### open_at : `2022-03-28T03:50:43Z`
### last_modified_date : `2022-05-09T11:01:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105072
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
This is quoted from PR105066:

It's also essential that these loads fold efficiently into memory source operands for PMOVZX; pmovzxbq is one of the major use-cases for a 16-bit load.

That may be a separate bug, IDK

https://godbolt.org/z/3a9T55n3q shows _mm_cvtepu8_epi32(_mm_loadu_si32(p)) does fold a 32-bit memory source operand nicely to pmovzxbd (%rdi), %xmm0 which can micro-fuse into a single uop on Intel CPUs (for the 128-bit destination version, not YMM), but disaster with 16-bit loads:

__m128i pmovzxbq(void *p){
    return _mm_cvtepu8_epi64(_mm_loadu_si16(p));
}

pmovzxbq(void*):  # -O3 -msse4.1 -mtune=haswell
        pxor    %xmm0, %xmm0                  # 1 uop
        pinsrw  $0, (%rdi), %xmm0             # 2 uops, one for shuffle port
        pmovzxbq        %xmm0, %xmm0          # 1 uop for the same shuffle port
        ret

(_mm_cvtepu8_epi64 requires SSE4.1 so there's no interaction with the -mno-sse4.1 implementation of the load.)


---


### compiler : `gcc`
### title : `[nvptx] Generate sad insn (sum of absolute differences)`
### open_at : `2022-03-28T07:53:56Z`
### last_modified_date : `2022-03-28T13:15:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105075
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
ptx has sad ((sum of absolute differences)) insn, which is currently not modeled in the .md file.


---


### compiler : `gcc`
### title : `[OpenMP] Weak/template functions: split-off offload-region functions should also be weak`
### open_at : `2022-03-28T08:06:51Z`
### last_modified_date : `2022-03-28T08:06:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105076
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
When using something like

template<T>
void foo(T var)
{
  #pragma omp target ...
    ...
}

the function 'foo' is weak – but its associated offload function isn't.

Result: While at the end only one 'foo' function remains, there will be as many offload function as 'foo'-using translation units.

Expected: Also the split off target-regions should be a weak symbol.

NOTE: Requires a consistent naming of the offload functions, but I think that's already guaranteed by create_omp_child_function_name (invoked from create_omp_child_function_name).

Existing testcase:
  'test_map' in libgomp/testsuite/libgomp.c++/target-same-name-2.C
   of PR104285 / r12-7776-g1002a7ace11
   (checks only for link/run-time errors and not for how many offload functions remain)


---


### compiler : `gcc`
### title : `_mm_storeu_si16 inefficiently uses pextrw to an integer reg (without SSE4.1)`
### open_at : `2022-03-28T09:26:10Z`
### last_modified_date : `2022-05-03T16:03:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105079
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
With PR105066 fixed, we do _mm_loadu_si16 with pinsrw from memory, because that's available with just SSE2.  (And the cause wasn't tuning choices, it was a typo in what insns GCC thought were available.)  Related: PR105072 re: folding such 16-bit loads into memory source operands for PMOVZX/SXBQ.

But the famously non-orthogonal SSE2 only includes pextrw $imm, %xmm, reg.  Not reg/mem until SSE4.1 (with a longer opcode for no apparent reason, instead of just allowing mem addressing modes for the existing one.  But same mnemonic so the assembler takes care of it.  https://www.felixcloutier.com/x86/pextrw)

So we do need to care about tuning for _mm_storeu_si16(p, v) without the option of PEXTRW to memory.  Currently we do this, which is obviously bad:

    pextrw  $0, %xmm0, %eax      # 2 uops
    movw    %ax, (%rdi)

we should be doing this

    movd    %xmm0, %eax          # 1 uop
    mov     %ax, (%rdi)

https://godbolt.org/z/Ee3Ez174M

This is especially true if we don't need the integer value zero-extended into EAX.

If we *did* also want the value zero-extended in an integer register, the extra uop in PEXTRW (in addition to the port 0 uop like MOVD) is a port-5 shuffle to extract an arbitrary 16-bit element, vs. a separate integer movzwl %cx, %eax could run on any integer ALU port.  (Including port 6 on HSW/SKL, which doesn't compete with any vector ALUs).

Mov-elimination for movzwl doesn't work on any current CPUs, only movzbl on Intel, and movl / movq on both Intel and AMD.  So currently there's no benefit to picking a different register like %ecx, instead of just using movzwl %ax, %eax

When we both store and use the integer value:

int store16_and_use(void *p, __m128i v){
    _mm_storeu_si16( p, v );
    return 123 + *(unsigned short*)p;
}

https://godbolt.org/z/zq6TMo1oE current trunk GCC does this, which is not bad:

# -O3 with or without -msse4.1
        pextrw  $0, %xmm0, %eax
        movw    %ax, (%rdi)
        addl    $123, %eax
        ret

Clang13 uses MOVD + MOVZX like I was suggesting, even though it costs more code size.  That's not necessarily better

        movd    %xmm0, %eax
        movw    %ax, (%rdi)
        movzwl  %ax, %eax
        addl    $123, %eax
        retq

In this case it's not obviously wrong to use PEXTRW to an integer reg, but it's also fine to do it clang's way.  So however that corner case shakes out in the process of fixing the main bug (using movd / movw without SSE4.1 when we don't reload) is fine.

If SSE4.1 is available, the no-reload case should probably use PEXTRW to memory instead of movd + movw.  On some CPUs, the ALU op that's part of PEXTRW has more choice of ALU port than xmm->gp_int operations.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Dead Code Elimination Regression at -Os (trunk vs. 11.2.0) 25`
### open_at : `2022-03-28T17:06:03Z`
### last_modified_date : `2023-07-13T18:34:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105086
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 52702
The case presented.

cat case.c #25
void foo();
int main() {
  unsigned a = -23;
  for (; a >= 2; ++a)
    if (a == 55)
      foo();
}

`gcc-0127fb1b78a36a7b228d4b3fe32eedfc8d273363 (trunk) -Os` can not eliminate `foo` but `gcc-releases/gcc-11.2.0 -Os` can.

`gcc-0127fb1b78a36a7b228d4b3fe32eedfc8d273363 (trunk) -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movl	$-23, %eax
.L6:
	cmpl	$55, %eax
	je	.L10
	incl	%eax
	jne	.L6
	xorl	%eax, %eax
	ret
.L10:
	pushq	%rax
	.cfi_def_cfa_offset 16
.L7:
	xorl	%eax, %eax
	call	foo
	movl	$56, %eax
.L2:
	incl	%eax
	je	.L12
	cmpl	$55, %eax
	jne	.L2
	jmp	.L7
.L12:
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-11.2.0 -Os -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=502ffb1f389011b28ee51815242c7397790802d5

----- Build information -----
----- 0127fb1b78a36a7b228d4b3fe32eedfc8d273363 (trunk)
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ --prefix=/zdata/compiler_cache/gcc-0127fb1b78a36a7b228d4b3fe32eedfc8d273363
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.1 20220328 (experimental) (GCC)

----- releases/gcc-11.2.0
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ --prefix=/zdata/compiler_cache/gcc-01a9836c07053b3286b2259d6b628c7583413f55
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (GCC)


---


### compiler : `gcc`
### title : `BFI instructions are not generated on arm-none-eabi-g++`
### open_at : `2022-03-28T20:55:33Z`
### last_modified_date : `2022-08-03T17:29:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105090
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.1`
### severity : `normal`
### contents :
Created attachment 52704
.ii file for test case

The following code could generate a BFI instruction but does not. Clang will generate one.

#include <stdint.h>
uint32_t emplace(uint32_t into, uint32_t what) {
	constexpr uint32_t shift = 5;
	constexpr uint32_t width = 4;
	constexpr uint32_t mask = ((1 << width) - 1) << shift;
	return (into & ~mask) | ((what << shift) & mask);
}

you can write equivalent C code and get the same problem.

gcc 8.5 and clang generate:
        bfi     r0, r1, #5, #4
        bx      lr

Whereas 9.3+ generates:
        lsls    r1, r1, #5
        and     r1, r1, #480
        bic     r0, r0, #480
        orrs    r0, r0, r1
        bx      lr

These are compiled with arm-none-eabi-gcc -Wall -Wextra -mcpu=cortex-m4 -O3 -c
Compile output is silent. Attached is .ii
I also tried O2, O1 and Os.

See problem with godbolt: https://godbolt.org/z/57h5Yd9ov

A lot of embedded development involves setting certain chunks of memory mapped registers like this and would greatly benefit from this being fixed. 

GCC version info from my computer:
Using built-in specs.
COLLECT_GCC=arm-none-eabi-gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/arm-none-eabi/11.2.0/lto-wrapper
Target: arm-none-eabi
Configured with: /build/arm-none-eabi-gcc/src/gcc-11.2.0/configure --target=arm-none-eabi --prefix=/usr --with-sysroot=/usr/arm-none-eabi --with-native-system-header-dir=/include --libexecdir=/usr/lib --enable-languages=c,c++ --enable-plugins --disable-decimal-float --disable-libffi --disable-libgomp --disable-libmudflap --disable-libquadmath --disable-libssp --disable-libstdcxx-pch --disable-nls --disable-shared --disable-threads --disable-tls --with-gnu-as --with-gnu-ld --with-system-zlib --with-newlib --with-headers=/usr/arm-none-eabi/include --with-python-dir=share/gcc-arm-none-eabi --with-gmp --with-mpfr --with-mpc --with-isl --with-libelf --enable-gnu-indirect-function --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' --with-pkgversion='Arch Repository' --with-bugurl=https://bugs.archlinux.org/ --with-multilib-list=rmprofile
Thread model: single
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (Arch Repository)



Possibly related but I'm not sure:
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85628


---


### compiler : `gcc`
### title : `the division in x / (1 << y) is optimized away when x has unsigned type, but not when it's signed`
### open_at : `2022-03-31T18:03:44Z`
### last_modified_date : `2022-11-28T20:09:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105119
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
For the following on x86_64 with -O2

int f(int x, int y) {
  return x / (1 << y);
}

GCC generates an idiv instruction:

        movl    %esi, %ecx
        movl    $1, %edx
        movl    %edi, %eax
        sall    %cl, %edx
        movl    %edx, %ecx
        cltd
        idivl   %ecx
        ret

But I believe this is equivalent to the division-less

int g(int x, int y) {
  return (x + (x < 0) * ((1 << y) - 1)) >> y;
}

(which basically generalizes the existing x / (1 << y) -> x >> y transformation that we perform for unsigned x).  For this latter function, we generate

        movl    %esi, %ecx
        movl    $1, %eax
        xorl    %edx, %edx
        sall    %cl, %eax
        subl    $1, %eax
        testl   %edi, %edi
        cmovns  %edx, %eax
        addl    %edi, %eax
        sarl    %cl, %eax
        ret

which seems to be significantly faster according to some rough benchmarks.


---


### compiler : `gcc`
### title : `[12 Regression] Testsuite failures since r12-7931 on i686-linux`
### open_at : `2022-04-01T10:29:04Z`
### last_modified_date : `2022-04-06T06:15:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105122
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Starting with r12-7931-gb8207ad367174df5f2e2fdf3305c97ed227d8f78 I'm seeing on i686-linux (or x86_64-linux with RUNTESTFLAGS='--target_board=unix/-m32/-mno-sse'
testing:
+FAIL: gcc.dg/memcpy-6.c  (test for warnings, line 25)
+FAIL: gcc.dg/memcpy-6.c scan-tree-dump-not optimized "memcpy"
+FAIL: gcc.dg/memcpy-6.c scan-tree-dump-not optimized "memmove"
+FAIL: gcc.dg/strlenopt-73.c scan-tree-dump-times optimized "_not_eliminated_" 0
+FAIL: gcc.dg/strlenopt-73.c scan-tree-dump-times optimized "strlen" 0
+FAIL: gcc.dg/strlenopt-80.c scan-tree-dump-times optimized "failure_on_line \\\\(" 0
+FAIL: gcc.dg/ipa/remref-7.c scan-ipa-dump inline "Removed a reference"
Haven't checked if those tests existed before the r12-3482-g5f6a6c91d7c592cb49f7c519f289777eac09bb74 commit.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Optimization regression gcc inserts not needed movsx when using switch statement`
### open_at : `2022-04-01T13:42:13Z`
### last_modified_date : `2023-07-13T21:22:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105126
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Given the following source code:

```cpp
bool is_bin_0(const char c) { return (c == '0' || c == '1'); }

bool is_bin_1(const char c) {
    switch (c) {
        case '0':
        case '1':
            return true;

        default:
            return false;
    }
}

```

compiling with `-O3` gives the following output:

```asm
is_bin_0(char):
        sub     edi, 48
        cmp     dil, 1
        setbe   al
        ret
is_bin_1(char):
        movsx   edi, dil
        sub     edi, 48
        cmp     edi, 1
        setbe   al
        ret
```

The version using a switch generates an extra movsx instruction which, as far is I understand the x86 CPU registers is not needed since DIL is the lower 8 bits of EDI anyways, but correct me if I'm wrong there.

This also seems to be a regression introduced with GCC-8.1 since GCC-4.5 to GCC-7.5 generate the same assembly as for the first and second function and all version after that including trunk produce different outputs.

[link to godbolt](https://godbolt.org/z/r3vhYbK6o)


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Optimization regression for handrolled branchless assignment since r11-4717-g3e190757fa332d32`
### open_at : `2022-04-02T08:53:11Z`
### last_modified_date : `2023-05-29T10:06:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105135
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Given the following source code [godbolt](https://godbolt.org/z/rrP3bqGW7):

```cpp
char to_lower_1(const char c) { return c + ((c >= 'A' && c <= 'Z') * 32); }

char to_lower_2(const char c) { return c + (((c >= 'A') & (c <= 'Z')) * 32); }

char to_lower_3(const char c) {
    if (c >= 'A' && c <= 'Z') {
        return c + 32;
    }
    return c;
}
```

compiling with `-O3`

produces the following assembly

```asm
to_lower_1(char):
        lea     eax, [rdi-65]
        cmp     al, 25
        setbe   al
        sal     eax, 5
        add     eax, edi
        ret
to_lower_2(char):
        lea     eax, [rdi-65]
        cmp     al, 25
        setbe   al
        sal     eax, 5
        add     eax, edi
        ret
to_lower_3(char):
        lea     edx, [rdi-65]
        lea     eax, [rdi+32]
        cmp     dl, 26
        cmovnb  eax, edi
        ret
```

Note that gcc-10.3 did produce the same assembly for all 3 functions while gcc-11 and trunk do not.


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] Missed optimization regression with 32-bit adds and shifts`
### open_at : `2022-04-02T13:55:30Z`
### last_modified_date : `2023-07-08T01:41:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105136
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Given the following source code [godbolt](https://godbolt.org/z/daTxMYWKo)

#include <stdint.h>
int32_t foo(int64_t a, int32_t b, int cond) {
    if (cond) {
        a += ((int64_t)b) << 32;
    }
    return a >> 32;
}

int32_t bar(int64_t a, int32_t b, int cond) {
    int32_t r = a >> 32;
    if (cond) {
        r += b;
    }
    return r;
}

and compiling with "-O3" we get the following assembly:

foo:
        sal     rsi, 32
        mov     rax, rdi
        add     rax, rsi
        test    edx, edx
        cmove   rax, rdi
        shr     rax, 32
        ret
bar:
        sar     rdi, 32
        add     esi, edi
        test    edx, edx
        mov     eax, esi
        cmove   eax, edi
        ret

With gcc-10.3 we get for bar:
bar:
        sar     rdi, 32
        test    edx, edx
        lea     eax, [rsi+rdi]
        cmove   eax, edi
        ret

Also note that neither versions recognize that foo does the same as bar.

Credits: This was entirely found by Trevor Spiteri reported at the llvm-project here: https://github.com/llvm/llvm-project/issues/54718


---


### compiler : `gcc`
### title : `Missed optimization 64-bit adds and shifts`
### open_at : `2022-04-02T14:01:18Z`
### last_modified_date : `2023-01-01T17:01:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105137
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Given the following source code [godbolt](https://godbolt.org/z/8KMMhefqY)

#include <stdint.h>

typedef __int128_t int128_t;
int64_t foo(int128_t a, int64_t b, int cond) {
    if (cond) {
        a += ((int128_t)b) << 64;
    }
    return a >> 64;
}

int64_t bar(int128_t a, int64_t b, int cond) {
    int64_t r = a >> 64;
    if (cond) {
        r += b;
    }
    return r;
}

Compiling with "-O3" we get:

foo:
        mov     rax, rsi
        mov     rsi, rdi
        mov     rdi, rax
        test    ecx, ecx
        je      .L2
        xor     r8d, r8d
        add     rsi, r8
        adc     rdi, rdx
.L2:
        mov     rax, rdi
        ret
bar:
        add     rdx, rsi
        mov     rax, rsi
        test    ecx, ecx
        cmovne  rax, rdx
        ret

Although both functions do the same, gcc implements worse code for foo.

Credits: This was entirely found by Trevor Spiteri reported at the llvm-project here: https://github.com/llvm/llvm-project/issues/54718


---


### compiler : `gcc`
### title : `[12 Regression] Pointless warning about missed vector optimization`
### open_at : `2022-04-06T07:00:55Z`
### last_modified_date : `2022-04-08T06:35:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105175
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
For this code snippet extracted from Qemu source:

enum { QEMU_MIGRATION_COOKIE_PERSISTENT = 1 };
struct {
  unsigned flags;
  unsigned flagsMandatory
} qemuMigrationCookieGetPersistent_mig;
qemuMigrationCookieGetPersistent() {
  qemuMigrationCookieGetPersistent_mig.flags &=
      QEMU_MIGRATION_COOKIE_PERSISTENT;
  qemuMigrationCookieGetPersistent_mig.flagsMandatory &=
      QEMU_MIGRATION_COOKIE_PERSISTENT;
}

cc1 -O3 -mno-sse t.c -Wvector-operation-performance

gives me:

t.c: In function ‘qemuMigrationCookieGetPersistent’:
t.c:7:46: warning: vector operation will be expanded with a single scalar operation [-Wvector-operation-performance]
    7 |   qemuMigrationCookieGetPersistent_mig.flags &=

The generated code actually looks quite decent. Both integer AND operations are merged into a 64 bit AND since
https://gcc.gnu.org/git/gitweb.cgi?p=gcc.git;h=f31da42e047e8018ca6ad9809273bc7efb6ffcaf

This appears to be a nice optimization to me. However, in tree-vect-generic.cc we then complain about this being implemented with just a scalar instruction. Apart from this being pretty confusing for the programmer who never requested anything to be vectorized I also don't see why it is a bad thing to implement a vector operation with a scalar operation as long as it is able to cover the entire vector with that.

With GCC 12 we have auto-vectorization enabled already with -O2, so I expect this warning to surface much more frequently now. In particular on targets like s390 where older distros still have to build everything without hardware vector support this might be annoying. Also I'm not sure whether this warning ever points at an actual problem. To me it looks like we should just drop it altogether.


---


### compiler : `gcc`
### title : `Store and load with updating the pointer is not used as often as it should be on aarch64`
### open_at : `2022-04-06T15:25:32Z`
### last_modified_date : `2022-04-07T07:05:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105181
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
test case:
void loop(int N, double *a, double *b) {
  // #pragma clang loop vectorize_width(4, scalable)
  for (int i = 0; i < N; i++) {
    a[i] = b[i] + 1.0;
  }
}

gcc's kernel loop body:
.L4:
        ldr     q0, [x2, x3]
        fadd    v0.2d, v0.2d, v1.2d
        str     q0, [x1, x3]
        add     x3, x3, 16
        cmp     x3, x0
        bne     .L4

llvm's kernel loop body:
.LBB0_9:                                // =>This Inner Loop Header: Depth=1
        ldr     q1, [x12], #16
        subs    x10, x10, #2
        fadd    v1.2d, v1.2d, v0.2d
        str     q1, [x11], #16
        b.ne    .LBB0_9

see detail in https://godbolt.org/z/54nssME4f


---


### compiler : `gcc`
### title : `mis-optimization with -ffast-math and __builtin_powf`
### open_at : `2022-04-09T19:30:29Z`
### last_modified_date : `2022-04-12T17:35:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105206
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `minor`
### contents :



---


### compiler : `gcc`
### title : `[12/13/14 regression] 8% regression for m-queens compared to gcc11 O2 on CLX. since r12-3876-g4a960d548b7d7d94`
### open_at : `2022-04-11T07:57:34Z`
### last_modified_date : `2023-05-08T12:24:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105216
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 52778
g++ -O2 main.c;./a.out

Regression happens in pass_pre

      while (posib != UINT_FAST32_MAX) {
        // The standard trick for getting the rightmost bit in the mask
        uint_fast32_t bit = ~posib & (posib + 1);
        posib ^= bit; // Eliminate the tried possibility.
        uint_fast32_t new_diagl = (bit << 1) | diagl_shifted;
        uint_fast32_t new_diagr = (bit >> 1) | diagr_shifted;
        bit |= cols[d];
        uint_fast32_t new_posib = (bit | new_diagl | new_diagr);

        if (new_posib != UINT_FAST32_MAX) {
            uint_fast32_t lookahead1 = (bit | (new_diagl << (LOOKAHEAD - 2)) | (new_diagr >> (LOOKAHEAD - 2)));
            uint_fast32_t lookahead2 = (bit | (new_diagl << (LOOKAHEAD - 1)) | (new_diagr >> (LOOKAHEAD - 1)));
            uint_fast32_t allowed2 = l_rest > (int8_t)0;

            if(allowed2 && ((lookahead2 == UINT_FAST32_MAX) || (lookahead1 == UINT_FAST32_MAX))) {
                continue;
            }


          if(l_rest == (STORE_LEVEL + 1)) {
            start_level4[level4_cnt][0] = bit;   // cols
            start_level4[level4_cnt][1] = new_diagl; // diagl
            start_level4[level4_cnt][2] = new_diagr; // diagr
            level4_cnt++;
            continue;
          }

          l_rest--;

          // The next two lines save stack depth + backtrack operations
          // when we passed the last possibility in a row.
          // Go lower in the stack, avoid branching by writing above the current
          // position
          posibs[d] = posib;
          d += posib != UINT_FAST32_MAX; // avoid branching with this trick
          posib = new_posib;


          // make values current
          cols[d] = bit;
          diagl[d] = new_diagl;
          diagr[d] = new_diagr;
          rest[d] = l_rest;
          diagl_shifted = new_diagl << 1;
          diagr_shifted = new_diagr >> 1;
        }
      }
      d--;
      posib = posibs[d]; // backtrack ...
    }

For bit |= cols[d]; cols[d] is loop invariant if loop latch from *continue*, GCC11 catches it, GCC12 not.


---


### compiler : `gcc`
### title : `[13 Regression] __popcountdi2 calls generated in kernel code with gcc12`
### open_at : `2022-04-13T05:28:23Z`
### last_modified_date : `2022-11-07T18:01:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105253
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 52794
test case

gcc 12 when using -march=core2 generates __popcountdi2 calls which go unresolved in kernel and build fails. When I use -march=corei7 then the calls are optimized away.
Interestingly, gcc-11 did not emit these calls even with -march=core2,

a similar error is seen when compiling kernel for arm

arm-yoe-linux-gnueabi-ld.bfd: arch/arm/probes/kprobes/actions-common.o: in function `simulate_ldm1stm1':
actions-common.c:(.kprobes.text+0x74): undefined reference to `__popcountsi2'

Attached testcase demonstrates the behaviour on x86_64. Is it expected behavior in gcc 12 ?


---


### compiler : `gcc`
### title : `Union with user-defined empty destructor leads to worse code-gen`
### open_at : `2022-04-13T09:39:13Z`
### last_modified_date : `2022-11-07T17:55:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105260
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `normal`
### contents :
Following code leads to unnecessary stack spill of deserialized Foo variable:
g++ -std=c++17 -O2

#include <new>

inline unsigned deserializeUInt(const unsigned char* &in)
{
    unsigned out;
    __builtin_memcpy(&out, in, sizeof(out));
    in += sizeof(out);
    out = __builtin_bswap32(out);
    return out;
}

struct Foo
{
    unsigned a;
    unsigned b;

    static Foo deserialize(const unsigned char* &in)
    {
        return Foo{
            deserializeUInt(in),
            deserializeUInt(in)
        };
    }
};

struct Result
{
    unsigned idx;
    union
    {
        unsigned a;
        const void* ptr;
    };
};

Result dummyFunc(Foo);

void deserializeAndInvoke(const unsigned char* it)
{
#ifndef WORKAROUND
    union NoDestroy
    {
        ~NoDestroy() {}
        Foo value;
    };

    NoDestroy un{ 
        Foo::deserialize(it)
    };
    auto& arg = un.value;

#elif WORKAROUND == 1
    union NoDestroy
    {
        Foo value;
    };
    NoDestroy un{ 
        Foo::deserialize(it)
    };
    auto& arg = un.value;

#elif WORKAROUND == 2  
    alignas(Foo) char rawStorage[sizeof(Foo)];

    auto& arg = *new (&rawStorage[0]) Foo{
        Foo::deserialize(it)
    };
#endif

    dummyFunc(arg);
}

deserializeAndInvoke(unsigned char const*):
        mov     edx, DWORD PTR [rdi]
        mov     eax, DWORD PTR [rdi+4]
        bswap   edx
        bswap   eax
        mov     DWORD PTR [rsp-16], edx
        mov     DWORD PTR [rsp-12], eax
        mov     rdi, QWORD PTR [rsp-16]
        jmp     dummyFunc(Foo)

The spill can be avoided, when we remove user-defined destructor of NoDestroy union, or
when we construct Foo via placement new in raw buffer.

Generated code with either of the workarounds:
g++ -std=c++17 -O2 -DWORKAROUND=1
or
g++ -std=c++17 -O2 -DWORKAROUND=2

deserializeAndInvoke(unsigned char const*):
        mov     rax, rdi
        mov     edi, DWORD PTR [rdi]
        mov     eax, DWORD PTR [rax+4]
        bswap   edi
        mov     edi, edi
        bswap   eax
        sal     rax, 32
        or      rdi, rax
        jmp     dummyFunc(Foo)


---


### compiler : `gcc`
### title : `525.x264_r and 538.imagick_r regressed  on x86_64 at -O2 with PGO after r12-7319-g90d693bdc9d718`
### open_at : `2022-04-14T13:02:59Z`
### last_modified_date : `2023-01-18T15:56:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105275
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
I can see x86_64 regressions of 525.x264_r and 538.imagick_r when
built with plain -O2 (so generic march/mtune) and profile guided
optimization (PGO), compared to GCC 11.

The performance drop of 525.x264_r is about 11% on znver3 and 10% on
Intel cascadelake.  The performance drop of 538.imagick_r is about
6.4% on znver3.  FWIW, I bisected both to commit
r12-7319-g90d693bdc9d718:

   commit 90d693bdc9d71841f51d68826ffa5bd685d7f0bc
   Author: Richard Biener <rguenther@suse.de>
   Date:   Fri Feb 18 14:32:14 2022 +0100

   target/99881 - x86 vector cost of CTOR from integer regs
    
   This uses the now passed SLP node to the vectorizer costing hook
   to adjust vector construction costs for the cost of moving an
   integer component from a GPR to a vector register when that's
   required for building a vector from components.  A cruical difference
   here is whether the component is loaded from memory or extracted
   from a vector register as in those cases no intermediate GPR is involved.
   
   The pr99881.c testcase can be Un-XFAILed with this patch, the
   pr91446.c testcase now produces scalar code which looks superior
   to me so I've adjusted it as well.
   
   2022-02-18  Richard Biener  <rguenther@suse.de>
   
           PR tree-optimization/104582
           PR target/99881
           * config/i386/i386.cc (ix86_vector_costs::add_stmt_cost):
           Cost GPR to vector register moves for integer vector construction.

With PGo+LTO, the 538.imagick_r regression on znver3 is small (less
than 3%), the 525.x264_r ones are smaller but visible (9.4% and 7.1%
on the two machines).


---


### compiler : `gcc`
### title : `[12 Regression]  executed once loop not optimized anymore`
### open_at : `2022-04-14T13:47:31Z`
### last_modified_date : `2022-04-25T14:27:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105276
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
bool foo(unsigned i) {
    bool result = true;
    while (i) {
        i = i % 3;
        i = i - (i==2 ? 2 : i ? 1 : 0);
        result = !result;
    }
    return result;
}
--------------
compiled with g++ 11.2 and -O2 it produces:

-----------------
foo(unsigned int):
        test    edi, edi
        sete    al
        ret
----------------

With current trunk and -02 lots of instructions are generated, the loop is still present, about 30 instructions are produced.


Also, when compiled with -Os trunk produces loopless assembly:
------------------
foo(unsigned int):
        mov     dl, 1
        test    edi, edi
        je      .L1
        xor     edx, edx
.L1:
        mov     eax, edx
        ret
-------------------
Whereas using -Os and g++ 11.2 it uses one less register:
------------------
foo(unsigned int):
        mov     al, 1
        test    edi, edi
        je      .L4
        xor     eax, eax
.L4:
        ret


---


### compiler : `gcc`
### title : `missed optimization with -ftrapv for conditional constants`
### open_at : `2022-04-16T19:02:46Z`
### last_modified_date : `2022-04-19T20:16:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105295
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
~~~c
int sum_const(_Bool a, _Bool b)
{
        return (a ? 1 : 0 ) + (b ? 1 : 0);
}

int sum_unknown(int a, int b)
{
        return a + b;
}
~~~

For both functions, GCC calls __addvsi3, even though in sum_const, it is easy to see that no overflow can ever happen since the sum is always in the range [0 ... 2].

Clang optimizes this code reasonably.

ICC apparently does not care about -ftrapv at all.


---


### compiler : `gcc`
### title : `Specialize for_each`
### open_at : `2022-04-19T14:54:54Z`
### last_modified_date : `2022-04-23T22:06:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105308
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.0`
### severity : `enhancement`
### contents :
Hello,

with a balanced binary tree, as used for instance in std::set or std::map, it is relatively easy to perform an operation in parallel on all elements (like for_each): recurse on the 2 subtrees in parallel (and probably assign the top node to one of the subtrees arbitrarily). Of course there are technical details, we don't store the size of subtrees so we may want to decide in advance how deep to switch to sequential, etc. Doing this requires accessing details of the tree implementation and cannot be done by a user (plus, for_each doesn't seem to be a customization point...).

I am still confused that we have the traditional for_each, the new for_each with execution policy, the new range for_each, but no mixed range + execution policy. This specialization would be easier to implement for a whole tree than for an arbitrary subrange. It is still possible there, but likely less balanced, and we may need a first pass to find the common ancestor and possibly other relevant information (or check if the range is the whole container if that's possible and only optimize that case).

Possibly some other containers could specialize for_each, although it isn't as obvious.

Actually, even the sequential for_each could benefit from a specialization for various containers. Recursing on subtrees is a bit cheaper than having the iterator move up and down, forward_list could avoid pointing to the previous element, dequeue could try to split at block boundaries, etc.

Other algorithms that iterate through a range like reduce, all_of, etc could also benefit, hopefully most are simple wrappers around others so few would need a specialization.


---


### compiler : `gcc`
### title : `[12 Regression] ifcvt regression in noce_try_store_flag_mask`
### open_at : `2022-04-19T20:19:09Z`
### last_modified_date : `2022-04-26T08:13:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105314
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The function noce_try_store_flag_mask() in ifcvt.cc converts
"if (test) x = 0;" to "x &= -(test == 0);" (if costs permit that).

Commit 3a7ba8fd (which was introduced a month ago to fix PR104960) triggers a regression so that the if-conversion can't be performed anymore.

On RISC-V this manifests as follows:
"""
long func(long a, long b, long c)
{
  if (c)
    a = 0;
  return a;
}
"""

Old code:
0000000000000000 <func>:
   0:   00163613                seqz    a2,a2
   4:   40c00633                neg     a2,a2
   8:   8d71                    and     a0,a0,a2
   a:   8082                    ret

New (branching) code:
0000000000000000 <func>:
   0:   c211                    beqz    a2,4 <.L1>
   2:   4501                    li      a0,0
0000000000000004 <.L1>:
   4:   8082                    ret

Looking through the test suite, I could only find the file "gcc.target/arm/ifcvt-size-check.c" which should be affected by this regression as well. However, I haven't tested that.


---


### compiler : `gcc`
### title : `[x86] Failure to optimize out test instruction after add`
### open_at : `2022-04-21T07:32:34Z`
### last_modified_date : `2023-09-21T12:50:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105328
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
void f1();
void f2();
void f3();

void g(int b, int c)
{
	int a = b + c;
	if (a > 0)
		f1();
	else if (a < 0)
		f2();
	else
		f3();
}

With -O3, GCC outputs this:

g(int, int):
  add edi, esi
  test edi, edi
  jg .L5
  je .L3
  jmp f2()
.L3:
  jmp f3()
.L5:
  jmp f1()

LLVM instead outputs this:

g(int, int):
  add edi, esi
  jle .LBB0_1
  jmp f1()@PLT # TAILCALL
.LBB0_1:
  js .LBB0_4
  jmp f3()@PLT # TAILCALL
.LBB0_4:
  jmp f2()@PLT # TAILCALL

It appears like the `test` instruction can be removed (I assume without having to change anything about which functions get called on branchless paths and things like that). I'm not completely sure about this, considering how complex x86 performance can sometimes be, but I'd think removing that instruction should be beneficial everywhere.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Bogus restrict warning when assigning 1-char string literal to std::string since r12-3347-g8af8abfbbace49e6`
### open_at : `2022-04-21T07:41:57Z`
### last_modified_date : `2023-06-28T21:41:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105329
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
When compiling my codebase (build2) with GCC 12 (12.0.1 20220421) the output is littered with new bogus (AFAICS) -Wrestrict warnings that seems to be triggered by any attempt to assign or append a 1-char string literal to std::string:

$ cat <<EOF >bogus-restrict.cxx
#include <string>

void f (std::string& s)
{
  s = "5";
}
EOF

$ g++ -Wall -Wextra -O3 -c -std=c++23 bogus-restrict.cxx
In file included from /home/boris/work/build2/tests/modules/gcc2/gcc-install/include/c++/12.0.1/string:40,
                 from bogus-restrict.cxx:1:
In static member function ‘static constexpr std::char_traits<char>::char_type* std::char_traits<char>::copy(char_type*, const char_type*, std::size_t)’,
    inlined from ‘static constexpr void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_S_copy(_CharT*, const _CharT*, size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /home/boris/work/build2/tests/modules/gcc2/gcc-install/include/c++/12.0.1/bits/basic_string.h:423:21,
    inlined from ‘constexpr std::__cxx11::basic_string<_CharT, _Traits, _Allocator>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_replace(size_type, size_type, const _CharT*, size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /home/boris/work/build2/tests/modules/gcc2/gcc-install/include/c++/12.0.1/bits/basic_string.tcc:532:22,
    inlined from ‘constexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::assign(const _CharT*) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /home/boris/work/build2/tests/modules/gcc2/gcc-install/include/c++/12.0.1/bits/basic_string.h:1647:19,
    inlined from ‘constexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::operator=(const _CharT*) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /home/boris/work/build2/tests/modules/gcc2/gcc-install/include/c++/12.0.1/bits/basic_string.h:815:28,
    inlined from ‘void f(std::string&)’ at bogus-restrict.cxx:5:7:
/home/boris/work/build2/tests/modules/gcc2/gcc-install/include/c++/12.0.1/bits/char_traits.h:431:56: warning: ‘void* __builtin_memcpy(void*, const void*, long unsigned int)’ accessing 9223372036854775810 or more bytes at offsets -4611686018427387902 and [-4611686018427387903, 4611686018427387904] may overlap up to 9223372036854775813 bytes at offset -3 [-Wrestrict]
  431 |         return static_cast<char_type*>(__builtin_memcpy(__s1, __s2, __n));
      |


---


### compiler : `gcc`
### title : `[12 Regression] Regression: jump or cmove generated for pattern (x ? CST : 0)`
### open_at : `2022-04-21T17:47:22Z`
### last_modified_date : `2022-08-12T05:01:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105338
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
int f(int i) {
    return i ? 5 : 0;
}

int g(int i) {
    return i ? -2 : 0;
}

int h(int b) {
    return !!b * -2;
}

int i(int b) {
    return !!b * 5;
}

int j(int b) {
    if (!b) return 0;
    return -2;
}
-------------
With -02 gcc 11.2 the five functions above output branchless code like:

f(int):
        neg     edi
        sbb     eax, eax
        and     eax, 5
        ret
g(int):
        neg     edi
        sbb     eax, eax
        and     eax, -2
        ret
...
--------------

Whereas -02 gcc 12 now outputs a cmov or jump depending of sign of the constant:
f(int):
        mov     eax, edi
        test    edi, edi
        mov     edx, 5
        cmovne  eax, edx
        ret

g(int):
        mov     eax, edi
        test    edi, edi
        jne     .L11
        ret
.L11:
        mov     eax, -2
        ret

h(int):
        mov     eax, edi
        test    edi, edi
        jne     .L17
        ret
.L17:
        mov     eax, -2
        ret

i(int):
        mov     eax, edi
        test    edi, edi
        mov     edx, 5
        cmovne  eax, edx
        ret

j(int):
        mov     eax, edi
        test    edi, edi
        mov     edx, -2
        cmovne  eax, edx
        ret
---------------------------

Alternatively, with the following code

int k(int b) {
    bool b2 = b;
    return b2 * 5;
}
----------------

Both gcc 12 and 11.2 are outputing
k(int):
        xor     eax, eax
        test    edi, edi
        setne   al
        lea     eax, [rax+rax*4]
        ret


---


### compiler : `gcc`
### title : `[Extended Asm] Memory barrier greater than a function call`
### open_at : `2022-04-22T05:59:01Z`
### last_modified_date : `2022-04-24T19:15:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105342
### status : `NEW`
### tags : `inline-asm, missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
This is an enhancement request, not a bug.
According to doc, using the "memory" clobber effectively forms a read/write memory barrier for the compiler. Through my tests, the memory barrier's range in Extended-Asm is even greater than a function call, it will barrier the memory in the function's own stack. I think it is useless and it may even generate more complex code with inline funtion.


For example in the test case:
```test.c
extern unsigned long int x[512];

void test(long int,long int,long int,long int,long int);
void test1(long int);
void test2();

int kkk()
{
    unsigned long int k[512];
    for (size_t i=0; i<512; ++i )
    {
        k[i]=x[i];
    }
    test1(k[0]);
    k[1]=3;
    k[2]=3;
    k[3]=3;
    k[4]=3;
    k[5]=3;
    test2();
    test(k[1], k[2], k[3], k[4], k[5]);
    return 0;
}
```
and
```test2.c
void test2()
{
    __asm__ volatile
        (""
         :
         :
         :"memory"
         );
}
```
compile with 
```
gcc-12 -fno-stack-protector -fcf-protection=none -fno-asynchronous-unwind-tables -mgeneral-regs-only -O3 -S test.c test2.c
```
than generate
```test.s:
kkk:
	subq	$4096, %rsp
	orq	$0, (%rsp)
	subq	$8, %rsp
	leaq	x(%rip), %rsi
	movl	$512, %ecx
	movq	%rsp, %rdi
	rep movsq
	movq	(%rsp), %rdi
	call	test1@PLT
	xorl	%eax, %eax
	call	test2@PLT
	movl	$3, %r8d
	movl	$3, %ecx
	movl	$3, %edx
	movl	$3, %esi
	movl	$3, %edi
	call	test@PLT
	xorl	%eax, %eax
	addq	$4104, %rsp
	ret
```
```test2.s
test2:
	ret
```
The kkk's assembly code looks neat.

However, if I put the contents of test2.c in test.c, then it will generate:
```test.s
kkk:
	subq	$4096, %rsp
	orq	$0, (%rsp)
	subq	$8, %rsp
	leaq	x(%rip), %rsi
	movl	$512, %ecx
	movq	%rsp, %rdi
	rep movsq
	movq	(%rsp), %rdi
	call	test1@PLT
	movq	$3, 8(%rsp)
	movq	$3, 16(%rsp)
	movq	$3, 24(%rsp)
	movq	$3, 32(%rsp)
	movq	$3, 40(%rsp)
	movq	40(%rsp), %r8
	movq	32(%rsp), %rcx
	movq	24(%rsp), %rdx
	movq	16(%rsp), %rsi
	movq	8(%rsp), %rdi
	call	test@PLT
	xorl	%eax, %eax
	addq	$4104, %rsp
	ret
test2:
	ret
```
The compiler automatically inline the function test2() and think k[1], k[2], k[3], k[4], k[5] is barrier with the extended-asm, so the inlining test2() is even slower than not inlining it.

The gcc-12 is installed by apt on ubuntu 22.04. Full compile log:
```
$ gcc-12 -fno-stack-protector -fcf-protection=none -fno-asynchronous-unwind-tables -mgeneral-regs-only -O3 -S test.c test2.c -v
Using built-in specs.
COLLECT_GCC=gcc-12
OFFLOAD_TARGET_NAMES=nvptx-none:amdgcn-amdhsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 12-20220319-1ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-12/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-12 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --enable-cet --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-12-OcsLtf/gcc-12-12-20220319/debian/tmp-nvptx/usr,amdgcn-amdhsa=/build/gcc-12-OcsLtf/gcc-12-12-20220319/debian/tmp-gcn/usr --enable-offload-defaulted --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f] (Ubuntu 12-20220319-1ubuntu1) 
COLLECT_GCC_OPTIONS='-fno-stack-protector' '-fcf-protection=none' '-fno-asynchronous-unwind-tables' '-mgeneral-regs-only' '-O3' '-S' '-v' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-linux-gnu/12/cc1 -quiet -v -imultiarch x86_64-linux-gnu test.c -quiet -dumpbase test.c -dumpbase-ext .c -mgeneral-regs-only -mtune=generic -march=x86-64 -O3 -version -fno-stack-protector -fcf-protection=none -fno-asynchronous-unwind-tables -o test.s -Wformat -Wformat-security -fstack-clash-protection
GNU C17 (Ubuntu 12-20220319-1ubuntu1) version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f] (x86_64-linux-gnu)
	compiled by GNU C version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f], GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.24-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/12/include-fixed"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/12/../../../../x86_64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/lib/gcc/x86_64-linux-gnu/12/include
 /usr/local/include
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
GNU C17 (Ubuntu 12-20220319-1ubuntu1) version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f] (x86_64-linux-gnu)
	compiled by GNU C version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f], GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.24-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 200a3dd46f0674d1a8fcf2b133bc6014
COLLECT_GCC_OPTIONS='-fno-stack-protector' '-fcf-protection=none' '-fno-asynchronous-unwind-tables' '-mgeneral-regs-only' '-O3' '-S' '-v' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-linux-gnu/12/cc1 -quiet -v -imultiarch x86_64-linux-gnu test2.c -quiet -dumpbase test2.c -dumpbase-ext .c -mgeneral-regs-only -mtune=generic -march=x86-64 -O3 -version -fno-stack-protector -fcf-protection=none -fno-asynchronous-unwind-tables -o test2.s -Wformat -Wformat-security -fstack-clash-protection
GNU C17 (Ubuntu 12-20220319-1ubuntu1) version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f] (x86_64-linux-gnu)
	compiled by GNU C version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f], GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.24-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/12/include-fixed"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/12/../../../../x86_64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/lib/gcc/x86_64-linux-gnu/12/include
 /usr/local/include
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
GNU C17 (Ubuntu 12-20220319-1ubuntu1) version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f] (x86_64-linux-gnu)
	compiled by GNU C version 12.0.1 20220319 (experimental) [master r12-7719-g8ca61ad148f], GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.24-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 200a3dd46f0674d1a8fcf2b133bc6014
COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/12/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/12/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/12/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-fno-stack-protector' '-fcf-protection=none' '-fno-asynchronous-unwind-tables' '-mgeneral-regs-only' '-O3' '-S' '-v' '-mtune=generic' '-march=x86-64'
```


---


### compiler : `gcc`
### title : `Inefficient initialisation in some kinds of structs`
### open_at : `2022-04-22T07:16:27Z`
### last_modified_date : `2022-04-22T08:39:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105343
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
struct S { int a[1000]; };
struct X { struct S s; int b[2];};

extern int foobar(struct X * p);

int foo(struct S *s)
{
    struct X x = { *s };
    return foobar(&x);
}


When the size of the array "a" is small enough that the compiler does the initialisation inline, the code is fine.  With a bigger array it uses memset and memcpy, either as calls to external functions or inline loops depending on details of the version of gcc and the target.  (This too is appropriate.)

However, it does that by turning the code into the equivalent of :

    memset(&x, 0, sizeof(struct X));
    memcpy(&x, s, sizeof(struct S));

It /should/ be doing :

    memset(&x.b, 0, sizeof(struct X.b));
    memcpy(&x, s, sizeof(struct S));

In other words, it is first zeroing out the entire X structure, then copying from *s into the structure.  Only the extra part of X, the array "b", needs to be zero'ed.


---


### compiler : `gcc`
### title : `__builtin_shuffle for alignr generates suboptimal code unless SSSE3 is enabled`
### open_at : `2022-04-22T21:46:02Z`
### last_modified_date : `2023-05-11T13:21:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105354
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `enhancement`
### contents :
The below code generates suboptimal code if SSE2 is enabled but SSSE3 is not enabled:
#include <cstdint>

typedef std::uint8_t Simd128U8VectT __attribute__((__vector_size__(16)));

template<int RotateAmt>
static inline Simd128U8VectT RotateRightByByteAmt(Simd128U8VectT vect) noexcept {
    constexpr int NormalizedRotateAmt = RotateAmt & 15;

    if constexpr(NormalizedRotateAmt == 0)
        return vect;
    else
        return __builtin_shuffle(vect, vect, (Simd128U8VectT){
            NormalizedRotateAmt, NormalizedRotateAmt + 1,
            NormalizedRotateAmt + 2, NormalizedRotateAmt + 3,
            NormalizedRotateAmt + 4, NormalizedRotateAmt + 5,
            NormalizedRotateAmt + 6, NormalizedRotateAmt + 7,
            NormalizedRotateAmt + 8, NormalizedRotateAmt + 9,
            NormalizedRotateAmt + 10, NormalizedRotateAmt + 11,
            NormalizedRotateAmt + 12, NormalizedRotateAmt + 13,
            NormalizedRotateAmt + 14, NormalizedRotateAmt + 15 });
}

auto func1(Simd128U8VectT vect) noexcept {
    return RotateRightByByteAmt<5>(vect);
}

Here is the code that is generated on GCC 11 if the -O2 -mssse3 options are specified:
func1(unsigned char __vector(16)):
        palignr xmm0, xmm0, 5
        ret

Here is the code that is generated on GCC 11 if the -O2 option is specified but the -mssse3 option is not specified on 64-bit x86 platforms:
func1(unsigned char __vector(16)):
        sub     rsp, 144
        movd    ecx, xmm0
        movaps  XMMWORD PTR [rsp+8], xmm0
        movzx   edx, BYTE PTR [rsp+20]
        movzx   ecx, cl
        movaps  XMMWORD PTR [rsp+24], xmm0
        movzx   eax, BYTE PTR [rsp+35]
        sal     rdx, 8
        movaps  XMMWORD PTR [rsp+40], xmm0
        or      rdx, rax
        movzx   eax, BYTE PTR [rsp+50]
        movaps  XMMWORD PTR [rsp+56], xmm0
        sal     rdx, 8
        movaps  XMMWORD PTR [rsp+72], xmm0
        or      rdx, rax
        movzx   eax, BYTE PTR [rsp+65]
        movaps  XMMWORD PTR [rsp+88], xmm0
        sal     rdx, 8
        movaps  XMMWORD PTR [rsp+104], xmm0
        or      rdx, rax
        movzx   eax, BYTE PTR [rsp+80]
        movaps  XMMWORD PTR [rsp-104], xmm0
        sal     rdx, 8
        movaps  XMMWORD PTR [rsp-88], xmm0
        movzx   edi, BYTE PTR [rsp-85]
        or      rdx, rax
        movzx   eax, BYTE PTR [rsp+95]
        movaps  XMMWORD PTR [rsp-72], xmm0
        sal     rdx, 8
        movaps  XMMWORD PTR [rsp-56], xmm0
        or      rdx, rax
        movzx   eax, BYTE PTR [rsp+110]
        movaps  XMMWORD PTR [rsp-40], xmm0
        sal     rdx, 8
        movaps  XMMWORD PTR [rsp-24], xmm0
        or      rdx, rax
        movzx   eax, BYTE PTR [rsp-100]
        movaps  XMMWORD PTR [rsp+120], xmm0
        movzx   esi, BYTE PTR [rsp+125]
        movaps  XMMWORD PTR [rsp-8], xmm0
        sal     rdx, 8
        sal     rax, 8
        or      rdx, rsi
        or      rax, rdi
        movzx   edi, BYTE PTR [rsp-70]
        sal     rax, 8
        or      rax, rdi
        movzx   edi, BYTE PTR [rsp-55]
        sal     rax, 8
        or      rax, rdi
        sal     rax, 8
        or      rax, rcx
        movzx   ecx, BYTE PTR [rsp-25]
        sal     rax, 8
        or      rax, rcx
        movzx   ecx, BYTE PTR [rsp-10]
        sal     rax, 8
        or      rax, rcx
        movzx   ecx, BYTE PTR [rsp+5]
        mov     QWORD PTR [rsp-120], rdx
        sal     rax, 8
        or      rax, rcx
        mov     QWORD PTR [rsp-112], rax
        movdqa  xmm0, XMMWORD PTR [rsp-120]
        add     rsp, 144
        ret

Here is a more optimal implementation of the above code on 64-bit x86 platforms when SSE2 is enabled but SSSE3 is not enabled:
func1(unsigned char __vector(16)):
        movdqa  xmm1, xmm0
        psrldq  xmm1, 5
        pslldq  xmm0, 11
        por     xmm0, xmm1
        ret


---


### compiler : `gcc`
### title : `-ftree-slp-vectorize decreases performance significantly (x64)`
### open_at : `2022-04-24T14:25:04Z`
### last_modified_date : `2022-04-25T13:12:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105363
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 52857
Source file and outputs

Hello,

I found this example where using -O2 (which implies -ftree-slp-vectorize) decreases performance by about 4x wrt -O1. I've pinned it down to the -ftree-slp-vectorize, and -O3 -fno-tree-slp-vectorize works very well.

   $ gcc bug_opt.c -O3 -o bug_opt-O3
   $ time ./bug_opt-O3
   
   real	0m6.627s
   user	0m6.619s
   sys	0m0.005s

   $ gcc bug_opt.c -O3 -fno-tree-slp-vectorize -o bug_opt-O3-novec
   $ time ./bug_opt-O3-novec
   
   real	0m1.703s
   user	0m1.701s
   sys	0m0.000s

I've verified this with the current HEAD (1ceddd7497) and with 11.2 (though in that version -O2 does not imply -ftree-slp-vectorize, so the problem starts to appear at -O3).

I've minimized the example into a pretty basic insertion sort.

I have not checked the generated assembly.

I'm attaching the .c source, which has some more comments with timings. Also attaching my /proc/cpuinfo, and the temp files generated with -O3. I imagine the .o and binary is not too helpful, but can send them if needed.

Thanks,
Guido


---


### compiler : `gcc`
### title : `Unnecessary moves generated with _mm_crc32_u64`
### open_at : `2022-04-28T20:02:46Z`
### last_modified_date : `2022-09-29T14:01:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105429
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
The following C code:

>>>
#include <nmmintrin.h>
#include <stdint.h>
uint32_t crc(uint32_t current, const uint8_t *buffer, size_t size) {
    for(size_t i = 0; i < size; i++)
        current = _mm_crc32_u64(current, buffer[i]);
    return current;
}
<<<

Generates inefficient assembly on all optimisation presets due to the extra `mov eax, eax' - Os and O3 below:

>>>
crc:
        movl    %edi, %eax
        xorl    %ecx, %ecx
.L2:
        cmpq    %rdx, %rcx
        je      .L5
        movzbl  (%rsi,%rcx), %edi
        movl    %eax, %eax
        incq    %rcx
        crc32q  %rdi, %rax
        jmp     .L2
.L5:
        ret

crc:
        movl    %edi, %eax
        testq   %rdx, %rdx
        je      .L6
        leaq    (%rsi,%rdx), %rcx
.L3:
        movzbl  (%rsi), %edx
        movl    %eax, %eax
        addq    $1, %rsi
        crc32q  %rdx, %rax
        cmpq    %rsi, %rcx
        jne     .L3
.L6:
        ret
<<<

The problem seems to be present in all GCC versions I have access to. The redundant move greatly worsens the performance of the generated code. When `_mm_crc32_u64' is replaced by any other function, the problem seems to disappear.


---


### compiler : `gcc`
### title : `load introduced when passing a struct as argument`
### open_at : `2022-05-01T16:19:48Z`
### last_modified_date : `2022-05-02T07:05:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105448
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.3.0`
### severity : `normal`
### contents :
Created attachment 52915
C source file where a struct field has been loaded repeatedly

Hi, I find in this code: (the full file has been uploaded)

...

struct S0 {
   const uint64_t  f0;
   int32_t  f1;
   int64_t  f2;
   int32_t  f3;
   int64_t  f4;
   const int32_t  f5;
};

struct S0 g_19 = {0xF4275799D45EB447LL,0xD326D35DL,1L,1L,9L,0L};

...

const int16_t  func_2(uint32_t  p_3, const struct S1  p_4)
{
    if (func_7(g_19, 0, g_20))
    {
        return 0;
    }
    
    return g_6.f1;
}

int32_t  func_7(struct S0  p_10, uint32_t  p_11, int32_t  p_12)
{
    func_21(0, g_26, p_10);
    return p_10.f1;
}

int32_t  func_21(int32_t  p_22, int64_t  p_23, struct S0  p_24)
{
    if ((g_5[0] - (g_5[1] != p_23)) )
    {
        g_19.f3 = p_24.f4;
    }

    return g_5[4];
}

int main(){
    ...
    func_2(0, g_6);
    ...
}

when compiled on gcc-11.3.0 with O1 option, func_2 will be compiled as:

...
   0x000000000040118a <+5>:     movdqa 0x2ece(%rip),%xmm0      # 0x404060 <g_19>
   0x0000000000401192 <+13>:    movaps %xmm0,(%rsp)
   0x0000000000401196 <+17>:    mov    0x2ecc(%rip),%ebx     # 0x404068 <g_19+8>
   0x000000000040119c <+23>:    mov    %ebx,0x8(%rsp)
   0x00000000004011a0 <+27>:    pushq  0x2ee2(%rip)        # 0x404088 <g_19+40>
   0x00000000004011a6 <+33>:    pushq  0x2ed4(%rip)        # 0x404080 <g_19+32>
   0x00000000004011ac <+39>:    pushq  0x2ec6(%rip)        # 0x404078 <g_19+24>
   0x00000000004011b2 <+45>:    pushq  0x2eb8(%rip)        # 0x404070 <g_19+16>
...

it's obvious that g_19.f1 has been loaded twice in the first and third instruction abnormally.


---


### compiler : `gcc`
### title : `miss optimizations due to inconsistency in complex numbers associativity`
### open_at : `2022-05-02T10:07:23Z`
### last_modified_date : `2022-05-02T11:33:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105451
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
The two functions

#include <complex.h>

void f (complex double * restrict a, complex double *restrict b,
        complex double *restrict c, complex double *res, int n)
{
  for (int i = 0; i < n; i++)
   res[i] = a[i] * (b[i] * c[i]);
}

and

void g (complex double * restrict a, complex double *restrict b,
        complex double *restrict c, complex double *res, int n)
{
  for (int i = 0; i < n; i++)
   res[i] = (a[i] * b[i]) * c[i];
}

At -Ofast produce the same code, but internally they get there using different SLP trees.

The former creates a chain of VEC_PERM_EXPR nodes as is expected tinyurl.com/cmulslp1 however the latter avoids the need of the permutes by duplicating the elements of the complex number https://tinyurl.com/cmulslp2

The former we can detect as back to back complex multiplication but the latter we can't.

Not sure what the best way to get consistency here is.


---


### compiler : `gcc`
### title : `[11 Regression] load introduced by ce1 for conditional loads at -O1, might cause issues with the C/C++ memory model`
### open_at : `2022-05-02T14:48:18Z`
### last_modified_date : `2023-07-07T10:43:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105453
### status : `NEW`
### tags : `missed-optimization, needs-bisection, wrong-code`
### component : `rtl-optimization`
### version : `11.3.0`
### severity : `normal`
### contents :
Created attachment 52917
concurrent case

Here's the code:

int g_6[2] = {0x67D50F82L,0x67D50F82L};
int g_10 = 0L;

int func_1(void)
{ 
    int *l_9 = &g_10;
    (*l_9) = (g_6[0]);
    if (g_6[1])
    {
        return g_10;
    }
    return 1;
}

and on gcc-11.3.0 with -O1 option, the assembly code are as follows:

   0x000000000040114d <+0>:     mov    0x2eed(%rip),%eax        # 0x404040 <g_6>
   0x0000000000401153 <+6>:     mov    %eax,0x2ef3(%rip)        # 0x40404c <g_10>
   0x0000000000401159 <+12>:    cmpl   $0x0,0x2ee4(%rip)        # 0x404044 <g_6+4>
   0x0000000000401160 <+19>:    mov    $0x1,%eax
   0x0000000000401165 <+24>:    cmovne 0x2ed4(%rip),%eax        # 0x404040 <g_6>
   0x000000000040116c <+31>:    retq

yes, it assert that g_10 = g_6[0] and this may cause problem in concurrent environment. The case files uploaded can show that the generated code is the same as above in an environment that g_6 may be modified by other thread.


---


### compiler : `gcc`
### title : `Suboptimal code generation for access of function parameters and return values of type __float128 on x86-64 Windows target.`
### open_at : `2022-05-03T19:27:47Z`
### last_modified_date : `2022-05-04T06:18:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105468
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.3.1`
### severity : `normal`
### contents :
Created attachment 52921
example routine

From point of view of x86-64 Window ABI __float128 is yet another structure of size 16, 100% identical to the rest of them.
Like the rest of them, when it past by value into subroutine, it has to be stored by caller in temporary location (typically, caller's stack) and the pointer to the temporary is passed either in GPR or also on stack.
The same for return value - caller allocates temporary storage on its stack and passes pointer to it to callee in register RCX. Then a callee puts a return value where it was said.
There is absolutely no "floating-pointness" or "SIMDness" about it.

However, gcc compiler often (although not always) treats __float128 as if it was somehow related to floating-point side of the machine. I'd guess, it's somehow related to System V being a primary target and according to System V x86-64 ABI
__float128 values passed in and out in XMM registers.

In practice it leads to ugly code generation, less so in user code that uses __float128 for arithmetic, more so in library-type code that attempt to process __float128 objects with accordance to their binary layout.

Example 1. Access to individual words of __float128 function parameter
  vmovdqu (%rdx),%xmm2
  vmovdqu %xmm2,0x20(%rsp)
  mov     0x28(%rsp),%rdx
  mov     0x20(%rsp),%rcx
instead of simple:
  mov     %rcx,%r12
  mov     0x8(%rdx),%rcx
  mov    (%rdx),%rdx

Example 2. Function returns __float128 value composed of a pair of 64-bit words
  mov     %rax,0x20(%rsp)
  mov     %rdi,0x28(%rsp)
  vmovdqu 0x20(%rsp),%xmm3
  mov     %r12,%rax
  vmovdqu %xmm3,(%r12)
instead of simple:
  mov     %rax,(%r12)
  mov     %r12,%rax
  mov     %rdi,0x8(%r12)


If it was just ugly I wouldn't complain. Unfortunately, sometimes it's also quite slow, and some of the best modern CPUs are among the worst affected.
As expected, an exact impact varies. From measurable (2-4 clocks) to quite high (40 clocks).
The highest impact was measured on AMD Zen3 CPU, but in other situations the same CPU was among the least affected.
Intel CPUs (Ivy Bridge, Haswell, Skylake) showed impact in range from 2 to 21 clock, with both the lowest and the highest values seen on old Ivy Bridge cores while on newer Skylake  the impact was relatively consistent (6-10 clocks). I didn't measure yet on the newest Intel CPUs (Ice/Tiger Lake and Alder Lake).

Below, I attached the example code (which is not a mere dummy example, but actually quite good implementation of sqrtq()) compiled in two variants: normally and by tricking compiler into thinking that __float128 is "just another structure". Which, as said above, it is, at least under Win64, but appears compiler thinks otherwise. Also provided: 3 test benches that measure the difference in speed between the "normal" and the "tricky" variants in various surroundings.
Pay attention, the trick is valid only on Windows, don't try it on Linux.

I hope that this particular (i.e. Windows) variant of the problem can be fixed with relatively little effort.

On the other hand, System V x86-64 target is different matter. Here the impact is on average smaller, but in worst cases it is about the same as the worst case one on Windows, and since the problem there is fundamental (a stupid ABI) I don't believe that there could be an easy fix.


---


### compiler : `gcc`
### title : `RISC-V: Regression: Useless moves in conditional select return`
### open_at : `2022-05-04T10:46:12Z`
### last_modified_date : `2022-11-23T18:03:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105477
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Commit 3a7ba8fd triggers a regression so that on RISC-V two useless move instructions are generated.

Test code:
"""
long test(long a, long b, long c)
{
  return (!c ? a : b);
}
"""

GCC 10.2.0, GCC 11 or upstream/master before 3a7ba8fd generates (rv64gc + -O3):
test:
        beq     a2,zero,.L2
        mv      a0,a1
.L2:
        ret

Current upstream/master generates:
<test>:
   0:   87aa                    mv      a5,a0
   2:   852e                    mv      a0,a1
   4:   e211                    bnez    a2,8 <.L2>
   6:   853e                    mv      a0,a5
<.L2>:
   8:   8082                    ret

This might be an issue in the ifcvt code (in combination of the RISC-V backend) or something where the RISC-V backend needs to improve.

Some context to this issue:
* The mentioned change (3a7ba8fd) is not problematic at all and fixes an issue PR104960
* PR105314 reports a similar issue, that is also triggered by the same change


---


### compiler : `gcc`
### title : `unvectorized loop due to bool condition loaded from memory and different size data`
### open_at : `2022-05-05T08:39:09Z`
### last_modified_date : `2023-08-31T06:34:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105490
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Cloned from PR104595

#define N 256
typedef short T;
extern T a[N];
extern T b[N];
extern T c[N];
extern _Bool pb[N];

void predicate_by_bool()
{
  for (int i = 0; i < N; i++)
    c[i] = pb[i] ? a[i] : b[i];
}

where we expect vect_recog_mask_conversion_pattern to trigger here.  That
case can be fixed with

@@ -4658,9 +4660,9 @@ vect_recog_mask_conversion_pattern (vec_info *vinfo,
 
       if (TREE_CODE (rhs1) == SSA_NAME)
        {
-         rhs1_type = integer_type_for_mask (rhs1, vinfo);
-         if (!rhs1_type)
+         if (integer_type_for_mask (rhs1, vinfo))
            return NULL;
+         rhs1_type = TREE_TYPE (rhs1);
        }
       else if (COMPARISON_CLASS_P (rhs1))
        {

but we then run into the original issue again:

t.c:10:6: missed:   not vectorized: relevant stmt not supported: patt_28 = (<signed-boolean:16>) _1;

The cruical difference between working and not working is the _1 != 0 ?: vs.
_1 ?:  - I think we do have a duplicate bugreport here, possibly involving
combinations of different from memory bools.

Trying to make bool pattern recog inserting the relevant compensation code
is really iffy, the mask conversion pattern confuses things here - what
we are missing seems really be transforming the leafs.

Note that we do not try to patter-recog a pattern thus we cannot at the moment
have both, vect_recog_bool_pattern and vect_recog_mask_conversion_pattern
at the same time on the ?: stmt.

Note IIRC vect_recog_bool_pattern has to come last but it's now after
vect_recog_mask_conversion_pattern.  Unfortunately swapping things does
not make vect_recog_bool_pattern recognize and fixup

  patt_24 = (<signed-boolean:16>) _1;


---


### compiler : `gcc`
### title : `[12/13 Regression] x86_64 538.imagick_r 6% regressions and 2% 525.x264_r regressions on Alder Lake after r12-7319-g90d693bdc9d718`
### open_at : `2022-05-05T12:04:39Z`
### last_modified_date : `2022-07-27T01:18:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105493
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Similar issue with https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104762
they are all caused by the same commit 90d693bdc9d71841f51d68826ffa5bd685d7f0bc

options: -march=native -Ofast -lto

Alder Lake single copy:

            after Vs. before this commit
525.x264_r	     -9.09%
538.imagick_r	     -25.00%

Alder Lake multicopy:

            after Vs. before this commit
525.x264_r	     -2.00%
538.imagick_r	     -6.7%


---


### compiler : `gcc`
### title : ``__atomic_compare_exchange` prevents tail-call optimization`
### open_at : `2022-05-05T14:00:07Z`
### last_modified_date : `2022-10-31T07:55:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105495
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.3.1`
### severity : `enhancement`
### contents :
Godbolt: https://gcc.godbolt.org/z/7ob6zc17P

Offending testcase:

```c
typedef struct { int b; } cond;

int
__MCF_batch_release_common(cond* p, int c);

int
_MCF_cond_signal_some(cond* p, int x)
  {
    cond c = {x}, n = {2};
    __atomic_compare_exchange(p, &c, &n, 1, 0, 0);
    return __MCF_batch_release_common(p, x);
  }
```


GCC output:

```asm
_MCF_cond_signal_some:
        sub     rsp, 24
        mov     edx, 2
        mov     eax, esi
        mov     DWORD PTR [rsp+12], esi
        lock cmpxchg    DWORD PTR [rdi], edx
        je      .L2
        mov     DWORD PTR [rsp+12], eax      <------- note this extra store, which clang doesn't generate
.L2:
        call    __MCF_batch_release_common
        add     rsp, 24
        ret
```


Clang output:

```asm
_MCF_cond_signal_some:                  # @_MCF_cond_signal_some
        mov     ecx, 2
        mov     eax, esi
        lock            cmpxchg dword ptr [rdi], ecx
        jmp     __MCF_batch_release_common      # TAILCALL
```

1. If `cond` was defined as a scalar type such as `long`, there is no such issue.
2. `__atomic_exchange` doesn't suffer from this issue.


---


### compiler : `gcc`
### title : `Comparison optimizations result in unnecessary cmp instructions`
### open_at : `2022-05-05T14:18:53Z`
### last_modified_date : `2022-05-06T06:06:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105496
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
https://godbolt.org/z/1zdYsaqEj

Consider these equivalent functions:

int test1(int x) {
    if (x <= 10)
        return 123;
    if (x == 11)
        return 456;
    return 789;
}

int test2(int x) {
    if (x < 11)
        return 123;
    if (x == 11)
        return 456;
    return 789;
}

In test2 it is very clear that you can do a single cmp of x with 11 then use different flag bits to choose your case. In test1 it is less clear, but because x<=10 and x<11 are equivalent, you could always transform one to the other. Clang seems to do this correctly and transforms test1 into test2 and only emits a single cmp instruction in each. For some reason, not only does gcc miss this optimization, it seems to go the other direction and transform test2 into test1, emitting 2 cmp instructions for both!

test1(int):
        mov     eax, 123
        cmp     edi, 10
        jle     .L1
        cmp     edi, 11
        mov     eax, 456
        mov     edx, 789
        cmovne  eax, edx
.L1:
        ret
test2(int):
        mov     eax, 123
        cmp     edi, 10
        jle     .L6
        cmp     edi, 11
        mov     eax, 456
        mov     edx, 789
        cmovne  eax, edx
.L6:
        ret

Observed with at least -O2 and -O3. I initially observed this for code where each if generated an actual branch rather than a cmov, but when I reduced the example, the cmov was generated.

I'm not sure if this should be a middle-end or target specific optimization, since ideally it would be smart on all targets that use comparison flags, even if there are some targets that don't. Is there ever a down side to trying to make two adjacent comparisons compare the same number?


---


### compiler : `gcc`
### title : `Fails to break dependency for vcvtss2sd xmm, xmm, mem`
### open_at : `2022-05-06T09:12:40Z`
### last_modified_date : `2023-08-12T13:50:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105504
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
Created attachment 52933
testcase

Hit by core-math team at https://gcc.gnu.org/pipermail/gcc-help/2022-May/141480.html

Compile the attached testcase with -O2 -march=haswell (other AVX-capable Intel families except Alderlake are affected too) and observe that the big basic block begins with

.L6:
        vcvtss2sd       xmm1, xmm1, DWORD PTR [rsp-4]

This creates a false dependency on the previous assignment into xmm1, resulting in wildly varying (and suboptimal) throughput figures depending on how long the CPU stalls waiting for the previous assignment to complete.

GCC has code to emit such instructions in a manner that avoids false dependencies (see e.g. PR89071), but here it doesn't seem to work.


Also there's a potentially related issue that GCC copies the initial xmm0 value to eax via stack in the beginning of the function:

cr_exp10f:
        vmovss  DWORD PTR [rsp-4], xmm0
        mov     eax, DWORD PTR [rsp-4]

This seems wrong since xmm-reg moves on Haswell are 1 cycle afaict.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Unnecessary SSE spill since r9-5748-g1d4b4f4979171ef0`
### open_at : `2022-05-07T08:24:29Z`
### last_modified_date : `2023-07-07T10:43:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105513
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
Minimized from PR 105504.

Compile with -O2 -mtune=haswell -mavx (other -mtune variants are affected too):

static int as_int(float x)
{
    return (union{float x; int i;}){x}.i;
}

float f(double y, float x)
{
    int i = as_int(x);
    if (__builtin_expect(i > 99, 0)) return 0;
    if (i*2u < 77) if (i==2) return 0;
    return y*x;
}

GCC moves 'x' to 'i' via stack and then reloads from stack again when computing 'y*x':

f:
        vmovss  DWORD PTR [rsp-4], xmm1
        mov     eax, DWORD PTR [rsp-4]
        cmp     eax, 99
        jg      .L5
        lea     edx, [rax+rax]
        cmp     edx, 76
        ja      .L6
        cmp     eax, 2
        je      .L5
.L6:
        vcvtss2sd       xmm1, xmm1, DWORD PTR [rsp-4]
        vmulsd  xmm0, xmm1, xmm0
        vcvtsd2ss       xmm0, xmm0, xmm0
        ret
.L5:
        vxorps  xmm0, xmm0, xmm0
        ret

This is a regression relative to gcc-8. Interestingly, flipping '0' to '1' in __builtin_expect (i.e. making early exit likely) results in good code, so perhaps RA costing takes block probabilities backwards somewhere?


---


### compiler : `gcc`
### title : `Unnecessary memcpy() copy for empty asm volatile`
### open_at : `2022-05-07T23:52:19Z`
### last_modified_date : `2022-05-08T00:04:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105519
### status : `RESOLVED`
### tags : `inline-asm, missed-optimization`
### component : `rtl-optimization`
### version : `11.2.0`
### severity : `normal`
### contents :
Created attachment 52937
Source code preprocessed file

Use asm volatile("" : "+m,r"(value) : : "memory") with specific constraints ("+m,r") with large objects(array or structs with size > 8192) cause full copy for object and that copy stay unused.

Maybe it's an issue in the register allocator. The compiler uses "r" instead of "m" even for large objects.

The following code cause caused unnecessary copying with memcpy()

```
struct Large {
 int arr[2049];
};

extern Large obj;

namespace {

template <class Tp>
inline __attribute__((always_inline)) void DoNotOptimize(Tp& value) {
  asm volatile("" : "+m,r"(value) : : "memory");
}

}

void foo() {
  DoNotOptimize(obj);
}
```

Generate assembly code

g++ -Wall -Wextra -O3 -save-temps -fno-stack-protector -S do_not_optimize.cpp

Generated assembly code (x86_64)

```
foo():
  subq  $8216, %rsp      # 1. Extend stack size
  movl  $8196, %edx      # 2. %edx = 8196 = sizeof(Large) = sizeof(int) * 2049.
                         #    Prepare 3d arg (n - size) for memcpy()
  leaq  obj(%rip), %rsi  # 3. %rsi = &obj. Prepare 2d arg (src) for memcpy().
  movq  %rsp, %rdi       # 4. %rdi = %rsp. %rdi points to the top of the stack.
                         #    Prepace 1-st arg (dest) for memcpy().
  call  memcpy@PLT       # 5. Call memcpy(dest=%rdi, src=%rsi ,n=%edx=8196)
  addq  $8216, %rsp      # 6. Reduce stack size
  ret                    # 7. Return from function
```

What code do?

1. Extent stack size (line 1)
2. Copy data to new extended space in stack (line 2-7)
3. Reduce stack size back (line 6)

Looks like copy is not needed.

Notes

* -fno-stack-protector used just for small assembly code. Enable stack protection doesn't change behavior

* gcc generates memcpy() call only if size of object > 8192

* https://godbolt.org/z/hPYfcrqbW - Godbolt Compiler Explorer Playground

Versions: 11.2.0, but looks like versions 4.1.2 - 11.12(all available on godbolt) is also affected

# gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-pc-linux-gnu/11.2.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: /build/gcc/src/gcc/configure --enable-languages=c,c++,ada,fortran,go,lto,objc,obj-c++,d --enable-bootstrap --prefix=/usr --libdir=
/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --with-linker-hash-style
=gnu --with-system-zlib --enable-__cxa_atexit --enable-cet=auto --enable-checking=release --enable-clocale=gnu --enable-default-pie --enable-defaul
t-ssp --enable-gnu-indirect-function --enable-gnu-unique-object --enable-linker-build-id --enable-lto --enable-multilib --enable-plugin --enable-sh
ared --enable-threads=posix --disable-libssp --disable-libstdcxx-pch --disable-werror --with-build-config=bootstrap-lto --enable-link-serialization
=1 gdc_include_dir=/usr/include/dlang/gdc
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 11.2.0 (GCC)


---


### compiler : `gcc`
### title : `missed optimization in modulo arithmetic`
### open_at : `2022-05-08T04:08:22Z`
### last_modified_date : `2022-05-08T12:45:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105521
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.3.0`
### severity : `enhancement`
### contents :
I'm trying to compute (a*a)%n for uint64_t types on x86_64 using "gcc -O2 -W -Wall" like this:

  #include <stdint.h>
  #include <assert.h>

  uint64_t sqrmod(uint64_t a, uint64_t n) {
    assert(a < n);
    unsigned __int128 x = a;
    x *= a;
    return x % n;
  }

I expected to get the following code:

  sqrmod:
        cmpq    %rsi, %rdi
        jnb     .L13         // assert(a < n) failure
        movq    %rdi, %rax
        mul     %rdi
        div     %rsi
        movq    %rdx, %rax
        ret

The compiler does get the "mul" right but instead of the "div" it throws in a call to "__umodti3". The "__umodti3" function is horribly long code that will be worlds slower than a simple div.

Note: The "asset(a < n);" should tell the compiler that the "div" instruction can not overflow and will not cause a #DivisionError. Without the assert the compiler could (conditionally) add "a %= n;" for the same effect.

https://godbolt.org/z/cd57Wd4oo


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] ifconversion introduces many compares with loads`
### open_at : `2022-05-10T11:34:49Z`
### last_modified_date : `2023-05-29T10:07:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105546
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `target`
### version : `11.3.0`
### severity : `normal`
### contents :
Created attachment 52948
case file

Hi, here's the code:

...

struct S0 {
   unsigned short  f0;
   unsigned short  f1;
   short  f2;
   unsigned char  f3;
   long long  f4;
   long long  f5;
};

short g_344 = 0xC307L;

struct S0  func_1(void)
{ 
    struct S0 l_346 = {65526UL,1UL,-8L,0xA1L,0x2F513C84A35AAE1BLL,-6L};
    
    
    if (g_344)
    { 
        struct S0 l_345 = {0x0AB2L,0x6D27L,-1L,0xABL,3L,0x3E72F31FF82C1E35LL};
        return l_345;
    }
    else
    { 
        return l_346;
    }
}

...

with option -O1, gcc-11.3.0 generate the following code:

Dump of assembler code for function func_1:
   0x0000000000401126 <+0>:     mov    %rdi,%rax
   0x0000000000401129 <+3>:     cmpw   $0x1,0x2eff(%rip)        # 0x404030 <g_344>
   0x0000000000401131 <+11>:    sbb    %edi,%edi
   0x0000000000401133 <+13>:    and    $0xf544,%di
   0x0000000000401138 <+18>:    add    $0xab2,%di
   0x000000000040113d <+23>:    cmpw   $0x1,0x2eeb(%rip)        # 0x404030 <g_344>
   0x0000000000401145 <+31>:    sbb    %esi,%esi
   0x0000000000401147 <+33>:    and    $0x92da,%si
   0x000000000040114c <+38>:    add    $0x6d27,%si
   0x0000000000401151 <+43>:    cmpw   $0x1,0x2ed7(%rip)        # 0x404030 <g_344>
   0x0000000000401159 <+51>:    sbb    %ecx,%ecx
   0x000000000040115b <+53>:    and    $0xfffffff9,%ecx
   0x000000000040115e <+56>:    sub    $0x1,%ecx
   0x0000000000401161 <+59>:    cmpw   $0x1,0x2ec7(%rip)        # 0x404030 <g_344>
   0x0000000000401169 <+67>:    sbb    %edx,%edx
   0x000000000040116b <+69>:    and    $0xfffffff6,%edx
   0x000000000040116e <+72>:    sub    $0x55,%edx
   0x0000000000401171 <+75>:    cmpw   $0x0,0x2eb7(%rip)        # 0x404030 <g_344>
   0x0000000000401179 <+83>:    movabs $0x2f513c84a35aae1b,%r9
   0x0000000000401183 <+93>:    mov    $0x3,%r8d
   0x0000000000401189 <+99>:    cmovne %r8,%r9
   0x000000000040118d <+103>:   movabs $0x3e72f31ff82c1e35,%r10
   0x0000000000401197 <+113>:   mov    $0xfffffffffffffffa,%r8
   0x000000000040119e <+120>:   cmovne %r10,%r8
   0x00000000004011a2 <+124>:   mov    %di,(%rax)
   0x00000000004011a5 <+127>:   mov    %si,0x2(%rax)
   0x00000000004011a9 <+131>:   mov    %cx,0x4(%rax)
   0x00000000004011ad <+135>:   mov    %dl,0x6(%rax)
   0x00000000004011b0 <+138>:   mov    %r9,0x8(%rax)
   0x00000000004011b4 <+142>:   mov    %r8,0x10(%rax)
   0x00000000004011b8 <+146>:   retq

it seems the last 4 loads are unneed and can be optimized away, which bring vulnerabilities when facing concurrency and may decrease performance


---


### compiler : `gcc`
### title : `RA assigns an MMA vector input operand to vs0-vs31 causing an MMA accumulator to be spilled`
### open_at : `2022-05-10T20:37:38Z`
### last_modified_date : `2022-05-20T23:01:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105556
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
With current trunk and GCC 12, the MMA optimized dgemm kernel in OpenBLAS is seeing a performance regression compared to GCC 11 and GCC 10.  The problem is that the core loop in dgemm uses 8 accumulator variables, which want to use all 8 accumulator registers.  Using the 8 accumulators means we should not use the vs0 thru vs31 vector registers for the MMA instruction's normal vector input operands. However with trunk and GCC 12, the register allocator is assigning one vector input to one of the vs0-vs31 registers leading us to spill one of the accumulators and that causes a bad performance loss.

The trunk and GCC 12 asm for the core loop looks like:

.L5:
        lxvp 0,0(10)
        lxv 40,0(9)
        addi 10,10,64
        addi 9,9,64
        lxv 41,-48(9)
        lxv 42,-32(9)
        lxv 43,-16(9)
        lxvp 2,32(1)
        lxvp 32,-32(10)
        xvf64gerpp 4,0,40
        xvf64gerpp 6,0,41
        xvf64gerpp 3,0,42
        xvf64gerpp 2,0,43
        lxvp 0,64(1)
        xvf64gerpp 5,32,40
        xvf64gerpp 7,32,41
        xvf64gerpp 1,32,42
        xxmtacc 0
        xvf64gerpp 0,32,43
        xxmfacc 0
        stxvp 2,32(1)
        stxvp 0,64(1)
        bdnz .L5

Note the use of vs0 in the MMA instructions which forces the spilling of ACC0. The "better" GCC 11 and GCC 10 code looks like:
.L5:
        lxvp 44,0(10)
        lxvp 32,32(10)
        addi 9,9,64
        addi 10,10,64
        lxv 39,-64(9)
        lxv 40,-48(9)
        lxv 41,-32(9)
        lxv 42,-16(9)
        xvf64gerpp 4,44,39
        xvf64gerpp 5,32,39
        xvf64gerpp 6,44,40
        xvf64gerpp 7,32,40
        xvf64gerpp 3,44,41
        xvf64gerpp 1,32,41
        xvf64gerpp 2,44,42
        xvf64gerpp 0,32,42
        bdnz .L5


---


### compiler : `gcc`
### title : `[12 Regression] std::function<bool(char)>::_M_invoker may be used uninitialized in std::regex move with -fno-strict-aliasing`
### open_at : `2022-05-11T10:48:09Z`
### last_modified_date : `2023-09-12T12:04:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105562
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `libstdc++`
### version : `12.1.0`
### severity : `normal`
### contents :
Consider the following sample:

  #include <regex>
  
  int main()
  {
    std::regex a(".");
    std::regex b(std::move(a));
  }

Since GCC 12.1, compiling this code with "g++ -Wall -O1 -fno-strict-aliasing x.cpp" gives

In file included from /usr/include/c++/12.1.0/regex:48,
                 from x.cpp:1:
In constructor 'std::function<_Res(_ArgTypes ...)>::function(std::function<_Res(_ArgTypes ...)>&&) [with _Res = bool; _ArgTypes = {char}]',
    inlined from 'std::__detail::_State<_Char_type>::_State(std::__detail::_State<_Char_type>&&) [with _Char_type = char]' at /usr/include/c++/12.1.0/bits/regex_automaton.h:149:4,
    inlined from 'std::__detail::_StateIdT std::__detail::_NFA<_TraitsT>::_M_insert_subexpr_end() [with _TraitsT = std::__cxx11::regex_traits<char>]' at /usr/include/c++/12.1.0/bits/regex_automaton.h:290:24:
/usr/include/c++/12.1.0/bits/std_function.h:405:42: warning: '*(std::function<bool(char)>*)((char*)&__tmp + offsetof(std::__detail::_StateT, std::__detail::_State<char>::<unnamed>.std::__detail::_State_base::<unnamed>)).std::function<bool(char)>::_M_invoker' may be used uninitialized [-Wmaybe-uninitialized]
  405 |       : _Function_base(), _M_invoker(__x._M_invoker)
      |                                      ~~~~^~~~~~~~~~
In file included from /usr/include/c++/12.1.0/regex:63:
/usr/include/c++/12.1.0/bits/regex_automaton.h: In member function 'std::__detail::_StateIdT std::__detail::_NFA<_TraitsT>::_M_insert_subexpr_end() [with _TraitsT = std::__cxx11::regex_traits<char>]':
/usr/include/c++/12.1.0/bits/regex_automaton.h:287:17: note: '__tmp' declared here
  287 |         _StateT __tmp(_S_opcode_subexpr_end);
      |                 ^~~~~
In member function 'bool std::_Function_base::_M_empty() const',
    inlined from 'std::function<_Res(_ArgTypes ...)>::operator bool() const [with _Res = bool; _ArgTypes = {char}]' at /usr/include/c++/12.1.0/bits/std_function.h:574:25,
    inlined from 'std::function<_Res(_ArgTypes ...)>::function(std::function<_Res(_ArgTypes ...)>&&) [with _Res = bool; _ArgTypes = {char}]' at /usr/include/c++/12.1.0/bits/std_function.h:407:6,
    inlined from 'std::__detail::_State<_Char_type>::_State(std::__detail::_State<_Char_type>&&) [with _Char_type = char]' at /usr/include/c++/12.1.0/bits/regex_automaton.h:149:4,
    inlined from 'std::__detail::_StateIdT std::__detail::_NFA<_TraitsT>::_M_insert_subexpr_end() [with _TraitsT = std::__cxx11::regex_traits<char>]' at /usr/include/c++/12.1.0/bits/regex_automaton.h:290:24:
/usr/include/c++/12.1.0/bits/std_function.h:247:37: warning: '*(const std::_Function_base*)((char*)&__tmp + offsetof(std::__detail::_StateT, std::__detail::_State<char>::<unnamed>.std::__detail::_State_base::<unnamed>)).std::_Function_base::_M_manager' may be used uninitialized [-Wmaybe-uninitialized]
  247 |     bool _M_empty() const { return !_M_manager; }
      |                                     ^~~~~~~~~~
/usr/include/c++/12.1.0/bits/regex_automaton.h: In member function 'std::__detail::_StateIdT std::__detail::_NFA<_TraitsT>::_M_insert_subexpr_end() [with _TraitsT = std::__cxx11::regex_traits<char>]':
/usr/include/c++/12.1.0/bits/regex_automaton.h:287:17: note: '__tmp' declared here
  287 |         _StateT __tmp(_S_opcode_subexpr_end);
      |                 ^~~~~


There are no warnings with -fstrict-aliasing. Related resources:
* https://gcc.gnu.org/pipermail/gcc/2021-July/236851.html
* https://gcc.gnu.org/g:c22bcfd2f7dc9bb5ad394720f4a612327dc898ba


---


### compiler : `gcc`
### title : `Loop counter widened to 128-bit unnecessarily`
### open_at : `2022-05-13T16:43:03Z`
### last_modified_date : `2022-05-16T07:24:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105596
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For  total *= i  with a u128 total and a u32 loop counter, GCC pessimizes by widening i and doing a full 128x128 => 128-bit multiply, and having to do a 128-bit increment and compare.

uint64_t i to make it a full register width doesn't help.

unsigned __int128 fact(unsigned n){
    unsigned __int128 total = n;
    for (unsigned i=2 ; i < n ; i++)
        total *= i;
    return total;
}
// 0! = 0  isn't mathematically correct, but that's not the point

https://godbolt.org/z/W4MW9b6T3  (gcc trunk 13.0.0 20220508 (experimental) and clang 14, which makes efficient asm for all of these.)

# gcc -O3
fact:
        movl    %edi, %r9d
        xorl    %r11d, %r11d
        movq    %r9, %r10           # total = n  zext into  R11:R10
        cmpl    $2, %edi
        jbe     .L7                 # if n<=2 return r11:r10
        movl    $2, %esi            # i = 2  in  RDI:RSI
        xorl    %edi, %edi
.L9:                              # do{
        movq    %r11, %rcx
        movq    %rdi, %rdx
        movq    %r10, %rax
        movq    %r9, %r8              # copy original n to destroy later
        imulq   %r10, %rdx          # 128x128 multiply with 2x imul, 1x widening mul
        imulq   %rsi, %rcx
        addq    %rdx, %rcx
        mulq    %rsi
        movq    %rdx, %r11          # update total in r11:r10
        movq    %rax, %r10
        addq    %rcx, %r11          # last partial product

        addq    $1, %rsi            # i++ as a 128-bit integer
        adcq    $0, %rdi
        xorq    %rsi, %r8           #  r8 = n^i
        movq    %rdi, %rcx           # useless copy, we're already destroying r8
        orq     %r8, %rcx            # hi(i^n) | lo(i^n)
        jne     .L9               # }while(i != n);
.L7:
        movq    %r10, %rax
        movq    %r11, %rdx
        ret

So as well as creating extra work to do, it's not even doing it very efficiently, with multiple unnecessary mov instructions.

This doesn't seem to be x86-64 specific.  It also compiles similarly for AArch64 and MIPS64.  For some ISAs, I'm not sure if potentially-infinite loops are making a difference, e.g. PowerPC is hard for me to read.  RV64 has three multiply instructions in both versions.

I haven't tested a 32-bit equivalent with uint64_t total and uint32_t i.


This anti-optimization goes back to GCC4.6.  With GCC4.5 and earlier, the above C compiles to a tight loop with the expected mul reg + imul reg,reg and 1 register loop counter: https://godbolt.org/z/6KheaqTx4  (using __uint128_t, since unsigned __int128 wasn't supported on GCC4.4 or 4.1)

GCC 4.1 does an inefficient multiply, but one of the chunks is a freshly xor-zeroed register.  It's still just incrementing and comparing a 32-bit loop counter, but widening it for a 128x128-bit multiply recipe.  GCC4.4 optimizes away the parts that are useless for the high 64 bits of (u128)i being zero.


-----

A different version compiles efficiently with GCC6 and earlier, only becoming slow like the above with GCC7 and later.

unsigned __int128 fact_downcount(unsigned n){
    unsigned __int128 total = n;
    for (unsigned i=n-1 ; i > 1 ; i--)
        total *= i;
    return total;  // 0! = 0 isn't mathematically correct
}


-----

When the loop condition is possibly always-true, GCC can't prove the loop is non-infinite, and as usual can't widen the loop counter.  In this case, that's a good thing:

unsigned __int128 fact_gcc_handhold(unsigned n){
    unsigned __int128 total = 1;   // loop does do final n
    for (unsigned i=2 ; i <= n ; i++)  // potentially infinite loop defeats this pessimization
        total *= i;
    return total;  // fun fact:  0! = 1  is mathematically correct
}


fact_gcc_handhold:
        cmpl    $1, %edi
        jbe     .L4
        movl    $2, %ecx           # i = 2   in    ECX
        movl    $1, %eax           # total = 1  in RDX:RAX
        xorl    %edx, %edx
.L3:                             #do{
        movl    %ecx, %esi            # copy i instead of just incrementing it later :/

        movq    %rdx, %r8           # save high half of total
        addl    $1, %ecx              # i++
        imulq   %rsi, %r8           # lo x hi cross product
        mulq    %rsi                # lo x lo widening
        addq    %r8, %rdx           # 128x64-bit multiply

        cmpl    %ecx, %edi
        jnb     .L3               # }while(i < n)
        ret

Allocating total in RDX:RAX is nice, putting the lo part where we need it for mulq anyway.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Slp is maybe too aggressive in some/many cases`
### open_at : `2022-05-16T12:08:56Z`
### last_modified_date : `2023-08-26T18:35:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105617
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
It took many years until gcc caught up with MSVC and LLVM/clang in generation
of code for chains of  Intel's _addcarry_u64() intrinsic calls. But your finally managed to do it in gcc11.
Unfortunately, the luck didn't last for long.


void add4i(uint64_t dst[4], const uint64_t srcA[4], const uint64_t srcB[4])
{
  unsigned char c;
  unsigned long long r0; c = _addcarry_u64(0, srcA[0], srcB[0], &r0);
  unsigned long long r1; c = _addcarry_u64(c, srcA[1], srcB[1], &r1);
  unsigned long long r2; c = _addcarry_u64(c, srcA[2], srcB[2], &r2);
  unsigned long long r3; c = _addcarry_u64(c, srcA[3], srcB[3], &r3);
  dst[0] = r0;
  dst[1] = r1;
  dst[2] = r2;
  dst[3] = r3;
}

gcc 11.1 -O2

add4i:
        movq    (%rdx), %rax
        addq    (%rsi), %rax
        movq    8(%rsi), %rcx
        movq    %rax, %r8
        adcq    8(%rdx), %rcx
        movq    16(%rsi), %rax
        adcq    16(%rdx), %rax
        movq    24(%rdx), %rdx
        adcq    24(%rsi), %rdx
        movq    %r8, (%rdi)
        movq    %rcx, 8(%rdi)
        movq    %rax, 16(%rdi)
        movq    %rdx, 24(%rdi)
        ret


gcc 12.1  -O2

add4i:
        movq    (%rdx), %rax
        movq    8(%rsi), %rcx
        addq    (%rsi), %rax
        movq    16(%rsi), %r8
        adcq    8(%rdx), %rcx
        adcq    16(%rdx), %r8
        movq    %rax, %xmm1
        movq    24(%rdx), %rdx
        adcq    24(%rsi), %rdx
        movq    %r8, %xmm0
        movq    %rcx, %xmm3
        movq    %rdx, %xmm2
        punpcklqdq      %xmm3, %xmm1
        punpcklqdq      %xmm2, %xmm0
        movups  %xmm1, (%rdi)
        movups  %xmm0, 16(%rdi)
        ret

What ... ?!

BTW, gcc 12.1 -O1 is still o.k.


---


### compiler : `gcc`
### title : `[11 Regression] Missed loop body simplification by -O3 (trunk v.s. 10.3)`
### open_at : `2022-05-16T13:15:58Z`
### last_modified_date : `2022-07-22T11:22:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105618
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For the following code, gcc truck -O3 fails to optimize away the loop body, while gcc10.3 can. You can see from the assembly code that gcc10.3 figured out that the variable c would be a constant after the loop while gcc-trunk didn't.

$cat a.c
static int b=4;
int c;
int main() {
  int e[5] = {1,1,1,1,1};
  for (; b >= 0; b--) {
    c = e[b];
  }
  return 0;
}
$
$gcc-trunk -O3 -S -o trunk.s a.c
$gcc-10-3  -O3 -S -o 10-3.s a.c
$wc trunk.s 10-3.s
  66  147 1005 trunk.s
  52  106  698 10-3.s
 118  253 1703 total
$
$cat 10-3.s
main:
.LFB0:
	.cfi_startproc
	endbr64
	movl	b(%rip), %eax
	testl	%eax, %eax
	js	.L2
	movl	$1, c(%rip)
	movl	$-1, b(%rip)
.L2:
	xorl	%eax, %eax
	ret
$
$cat trunk.s
main:
.LFB0:
	.cfi_startproc
	movdqa	.LC0(%rip), %xmm0
	movl	b(%rip), %eax
	movl	$1, -24(%rsp)
	movaps	%xmm0, -40(%rsp)
	testl	%eax, %eax
	js	.L2
	movslq	%eax, %rdx
	leal	-1(%rax), %ecx
	movl	-40(%rsp,%rdx,4), %edx
	je	.L3
	leal	-2(%rax), %edx
	cmpl	$1, %eax
	je	.L10
	leal	-3(%rax), %ecx
	cmpl	$2, %eax
	je	.L6
	leal	-4(%rax), %edx
	cmpl	$3, %eax
	je	.L10
.L6:
	movslq	%edx, %rdx
	movl	-40(%rsp,%rdx,4), %edx
.L3:
	movl	%edx, c(%rip)
	movl	$-1, b(%rip)
.L2:
	xorl	%eax, %eax
	ret
.L10:
	movslq	%ecx, %rcx
	movl	-40(%rsp,%rcx,4), %edx
	jmp	.L3
$


---


### compiler : `gcc`
### title : `[13 Regression] g++.dg/opt/pr94589-2.C for cris, m68k, s390x`
### open_at : `2022-05-17T15:22:47Z`
### last_modified_date : `2022-10-27T02:21:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105629
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 52987
pr94589-2.C.252t.optimized

The test g++.dg/opt/pr94589-2.C is last known passing for cris-elf with r13-172-g1e2334302d6ad5.  For r13-211-g0c7bce0ac184c0 it failed, and has failed since then, including r13-555-g1815462a6e53:

testrun output:
FAIL: g++.dg/opt/pr94589-2.C  -std=gnu++20  scan-tree-dump-times optimized "[ij]_[0-9]+\\(D\\) (?:<|<=|==|!=|>|>=) [ij]_[0-9]+\\(D\\)" 12
FAIL: g++.dg/opt/pr94589-2.C  -std=gnu++20  scan-tree-dump-times optimized "i_[0-9]+\\(D\\) (?:<|<=|==|!=|>|>=) 5\\.0" 12

In g++.log:
PASS: g++.dg/opt/pr94589-2.C  -std=gnu++20 (test for excess errors)
g++.dg/opt/pr94589-2.C  -std=gnu++20 : pattern found 14 times
FAIL: g++.dg/opt/pr94589-2.C  -std=gnu++20  scan-tree-dump-times optimized "[ij]_[0-9]+\\(D\\) (?:<|<=|==|!=|>|>=) [ij]_[0-9]+\\(D\\)" 12
g++.dg/opt/pr94589-2.C  -std=gnu++20 : pattern found 14 times
FAIL: g++.dg/opt/pr94589-2.C  -std=gnu++20  scan-tree-dump-times optimized "i_[0-9]+\\(D\\) (?:<|<=|==|!=|>|>=) 5\\.0" 12

See attachment for contents of pr94589-2.C.252t.optimized at r13-555-g1815462a6e53.

The test appears to fail in the same way according to gcc-testresults@, for m68k-unknown-linux-gnu
(posts https://gcc.gnu.org/pipermail/gcc-testresults/2022-May/761149.html and https://gcc.gnu.org/pipermail/gcc-testresults/2022-May/761602.html) and s390x-ibm-linux-gnu (posts https://gcc.gnu.org/pipermail/gcc-testresults/2022-May/761118.html and https://gcc.gnu.org/pipermail/gcc-testresults/2022-May/761552.html).


---


### compiler : `gcc`
### title : `Redundant stores aren't removed by DSE`
### open_at : `2022-05-17T23:29:05Z`
### last_modified_date : `2022-06-14T13:53:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105638
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
For

$ cat foo.cpp
#include <stdint.h>
#include <vector>
#include <tr1/array>

class FastBoard {
public:
    typedef std::pair<int, int> movescore_t;
    typedef std::tr1::array<movescore_t, 24> scoredlist_t;
    
protected:
    std::vector<int> m_critical;

    int m_boardsize;    
};

class FastState {
public:        
    FastBoard board;
    
    int movenum;              
protected:
    FastBoard::scoredlist_t scoredmoves;
};

class KoState : public FastState {
private:         
    std::vector<uint64_t> ko_hash_history;   
    std::vector<uint64_t> hash_history;     
};

class GameState : public KoState {
public:                    
    void foo ();      
private:
    std::vector<KoState> game_history;                          
};

void GameState::foo() {
    game_history.resize(movenum);
}
$ g++ -O2 -march=skylake foo.cpp -S

generates:

...
        movl    $280, %edx
        xorl    %esi, %esi
        call    memset
        movq    %rax, %rcx
        vpxor   %xmm0, %xmm0, %xmm0
        addq    $280, %rcx
        vmovdqu %xmm0, 36(%rax)
        vmovdqu %xmm0, 52(%rax)
        vmovdqu %xmm0, 68(%rax)
        vmovdqu %xmm0, 84(%rax)
        vmovdqu %xmm0, 100(%rax)
        vmovdqu %xmm0, 116(%rax)
        vmovdqu %xmm0, 132(%rax)
        vmovdqu %xmm0, 148(%rax)
        vmovdqu %xmm0, 164(%rax)
        vmovdqu %xmm0, 180(%rax)
        vmovdqu %xmm0, 196(%rax)
        vmovdqu %xmm0, 212(%rax)
...

Here memset has cleared 280 bytes starting from RAX.  There is no need to
clear these bytes again.  The optimized tree dump shows:

  <bb 14> [local count: 444773291]:
  # __cur_154 = PHI <__cur_42(14), _6(13)>
  # __n_155 = PHI <__n_41(14), __n_20(D)(13)>
  *__cur_154 = {};
  MEM[(int * *)__cur_154] = 0B; 
  MEM[(int * *)__cur_154 + 8B] = 0B; 
  MEM[(int * *)__cur_154 + 16B] = 0B; 
  MEM[(struct array *)__cur_154 + 36B]._M_instance = {}; 
  MEM <vector(4) long unsigned int> [(long unsigned int * *)__cur_154 + 232B] = { 0, 0, 0, 0 }; 
  MEM[(long unsigned int * *)__cur_154 + 264B] = 0B; 
  MEM[(long unsigned int * *)__cur_154 + 272B] = 0B;

Some of them are removed by RTL DSE.  But vector stores aren't.  Should
SSA DSE remove them?


---


### compiler : `gcc`
### title : `[13 Regression] Code-Size regression for specrate 538.imagick_r`
### open_at : `2022-05-18T09:06:14Z`
### last_modified_date : `2022-10-27T01:00:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105643
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
We found on Intel platform, commit r13-128-g938a02a589dc22 (938a02a589dc22cef65bba2b131fc9e4874baddb) results 53.7% codesize increment when compiled with march_native_ofast_lto(-march=native -Ofast -funroll-loops -flto) for specrate 538.imagick_r on SkylakeW, Cascade Lake and IceLake.

On Zen3 Server/Client code size looks alright.


---


### compiler : `gcc`
### title : `Comparisons to atomic variables generates less efficient code`
### open_at : `2022-05-19T12:59:19Z`
### last_modified_date : `2022-10-26T23:35:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105661
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
With normal variables, gcc will generate a nice cmp/js pair when checking if the high bit is null. When using an atomic, gcc generates a movzx/test/js triple, even though it could use the same codegen as for a non-atomic.

https://godbolt.org/z/GorvWfrsh

#include <atomic>
#include <cstdint>

[[gnu::noinline]] void f();
uint8_t plain;
std::atomic<uint8_t> atomic;

void plain_test() {
    if (plain & 0x80) f();
}

void atomic_test() {
    if (atomic.load(std::memory_order_relaxed) & 0x80) f();
}

With both -O2 and -O3 this generates:

plain_test():
        cmp     BYTE PTR plain[rip], 0
        js      .L4
        ret
.L4:
        jmp     f()
atomic_test():
        movzx   eax, BYTE PTR atomic[rip]
        test    al, al
        js      .L7
        ret
.L7:
        jmp     f()

ARM64 seems to be hit even harder, but I don't know that platform well enough to know if the non-atomic codegen is valid there https://godbolt.org/z/c3h8Y1dan. It seems likely though, at least for a relaxed load.


---


### compiler : `gcc`
### title : `[x86] suboptimal code for branch over two bools`
### open_at : `2022-05-20T09:49:15Z`
### last_modified_date : `2023-09-17T08:33:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105670
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.1`
### severity : `enhancement`
### contents :
void test(bool a, bool b, int* t) {
  if (a & b)
    *t = 42;
}

With -O1, -O2, -O3, and -Os this produces:

test(bool, bool, int*):
        test    dil, dil
        je      .L1
        test    sil, sil
        je      .L1
        mov     DWORD PTR [rdx], 42
.L1:
        ret

The following would be 5B shorter and also faster, since it minimizes branch misses:

test(bool, bool, int*):
        test    sil, dil
        je      .L1
        mov     DWORD PTR [rdx], 42
.L1:
        ret

Note that this is (somewhat) ABI dependent, but works on x86-64 System V, since:
> bit 0 contains the truth value and bits 1 to 7 shall be zero


---


### compiler : `gcc`
### title : `Calling strlen on a string constant is optimized out, but calling wcslen on a string constant is not`
### open_at : `2022-05-20T18:13:39Z`
### last_modified_date : `2022-05-24T17:06:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105677
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.1`
### severity : `enhancement`
### contents :
strlen example, source code:

    #include <string.h>

    int main()
    {
        return strlen("Hello world!");
    }

strlen example, assembly code at -O3:

    movl    $12, %eax
    ret

wcslen example, source code:

    #include <wchar.h>

    int main()
    {
        return wcslen(L"Hello world!");
    }

wcslen example, assembly code at -O3:

    subq    $8, %rsp
    .cfi_def_cfa_offset 16
    leaq    .LC0(%rip), %rdi
    call    wcslen@PLT
    addq    $8, %rsp
    .cfi_def_cfa_offset 8
    ret

Interestingly, Clang produces identical assembly code for both examples, which is what I expected GCC to do.


---


### compiler : `gcc`
### title : `[13 Regression] missed RTL if-conversion with COND_EXPR change`
### open_at : `2022-05-24T09:26:11Z`
### last_modified_date : `2023-07-27T09:23:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105715
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
gcc.target/i386/pr45685.c with -march=cascadelake shows missing RTL if-conversion.  The cruical GIMPLE difference is

  _36 = _3 > 0;
  iftmp.0_13 = _36 ? 1 : -1;
  prephitmp_31 = ABS_EXPR <_3>;
  prephitmp_32 = _36 ? -1 : 1;
  prephitmp_33 = _36 ? 4294967295 : 1;
  prephitmp_35 = _36 ? 1 : 4294967295;
...
  _29 = prephitmp_31 != _42;
  val_12 = _29 ? prephitmp_32 : iftmp.0_13;
  prephitmp_37 = _29 ? prephitmp_33 : prephitmp_35;

vs.

  iftmp.0_13 = _3 > 0 ? 1 : -1;
  prephitmp_31 = ABS_EXPR <_3>;
  prephitmp_32 = _3 > 0 ? -1 : 1;
  prephitmp_33 = _3 > 0 ? 4294967295 : 1;
  prephitmp_35 = _3 > 0 ? 1 : 4294967295;
...
  val_12 = i.1_6 == prephitmp_31 ? iftmp.0_13 : prephitmp_32;
  prephitmp_37 = i.1_6 != prephitmp_31 ? prephitmp_33 : prephitmp_35;

where the split out condition is now CSEd and the multi-use makes us not
TER the comparison.  Previously we got two compare & jump sequences while
now we get the compare computing a QImode value and the then two
compare & jump sequences.

While without -march=cascadelake we do get the desired number of cmovs
the generated code is still worse.

The testcase is unfortunately a bit obfuscated due to the many
if-conversions taking place.  Smaller GIMPLE testcases do not exhibit
jumpy RTL expansion.

void __GIMPLE(ssa, startwith("optimized"))
foo (long *p, long a, long b, long c, long d, long e, long f)
{
  _Bool _2;
  long _3;
  long _8;

  __BB(2):
      _2 = a_1(D) < b_10(D);
      _3 = _2 ? c_4(D) : d_5(D);
      _8 = _2 ? f_6(D) : e_7(D);
      __MEM <long> (p_9(D)) = _3;
      __MEM <long> (p_9(D) + 4) = _8;
      return;
}

#if __GNUC__ < 13
void __GIMPLE(ssa, startwith("optimized"))
bar (long *p, long a, long b, long c, long d, long e, long f)
{
  long _3;
  long _8;

  __BB(2):
      _3 = a_1(D) < b_10(D) ? c_4(D) : d_5(D);
      _8 = a_1(D) >= b_10(D) ? e_7(D) : f_6(D);
      __MEM <long> (p_9(D)) = _3;
      __MEM <long> (p_9(D) + 4) = _8;
      return;
}
#endif


---


### compiler : `gcc`
### title : `superfluous second operation before conditional branch   -O2 -mcpu=cortex-m0plus`
### open_at : `2022-05-25T20:20:52Z`
### last_modified_date : `2022-09-04T18:27:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105731
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.1`
### severity : `normal`
### contents :
gcc 10.3.1 misses an easy oportunity for optimization:

Instruction:  
    if ((bits<<=1)>=0) goto p3;

generated code:
    strh    r7, [r0, #4]
    lsls    r3, r2, #1
    lsls    r2, r2, #1
    bpl     .L8

i believe the 2nd "lsls" is superfluous. 
It would save me 3MHz if it wasn't there. :-)

full example: 
    https://godbolt.org/z/xocbvjn5x


---


### compiler : `gcc`
### title : `riscv: Poor codegen for large stack frames`
### open_at : `2022-05-26T00:49:50Z`
### last_modified_date : `2022-11-16T21:34:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105733
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `enhancement`
### contents :
For the following test:

#define BUF_SIZE 2064

void
foo(unsigned long i)
{
    volatile char buf[BUF_SIZE];

    buf[i] = 0;
}

GCC currently generates:

foo:
        li      t0,-4096
        addi    t0,t0,2016
        li      a4,4096
        add     sp,sp,t0
        li      a5,-4096
        addi    a4,a4,-2032
        add     a4,a4,a5
        addi    a5,sp,16
        add     a5,a4,a5
        add     a0,a5,a0
        li      t0,4096
        sd      a5,8(sp)
        sb      zero,2032(a0)
        addi    t0,t0,-2016
        add     sp,sp,t0
        jr      ra

whereas Clang generates the much shorter:

foo:
        lui     a1, 1
        addiw   a1, a1, -2016
        sub     sp, sp, a1
        addi    a1, sp, 16
        add     a0, a0, a1
        sb      zero, 0(a0)
        lui     a0, 1
        addiw   a0, a0, -2016
        add     sp, sp, a0
        ret

The:

        li      a4,4096
        ...
        li      a5,-4096
        addi    a4,a4,-2032
        add     a4,a4,a5

sequence in particular is rather surprising to see rather than just li a4,-2032 and constant-folding that would halve the instruction count difference between GCC and Clang alone.

See: https://godbolt.org/z/8EGc85dsf


---


### compiler : `gcc`
### title : `GCC failed to reduce &= loop_inv in loop.`
### open_at : `2022-05-26T07:54:55Z`
### last_modified_date : `2022-09-22T02:04:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105735
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
This is another case from 47769, similar like 103462, but a little different.


unsigned long cfunc_one(unsigned long tmp, unsigned long bit2) {
    for (unsigned long bit = 0; bit < 64; bit++) {
        tmp &= bit2;
    }
    return tmp;
}


it should be equal to

unsigned long cfunc_one(unsigned long tmp, unsigned long bit2) {
   tmp &= bit2;
   return tmp;
}

but gcc generates a loop.

cfunc_one(unsigned long, unsigned long):
        mov     eax, 64
.L2:
        and     rdi, rsi
        dec     rax
        jne     .L2
        mov     rax, rdi
        ret

Similar for |= and ^=.


---


### compiler : `gcc`
### title : `missed optimization switch transformation for conditions with duplicate conditions`
### open_at : `2022-05-26T11:26:46Z`
### last_modified_date : `2023-06-26T06:43:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105740
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
Created attachment 53037
Attached code + asm from the compiler explorer link

Compiler Explorer Link: https://godbolt.org/z/q76s99Gj9

The code on the left does not generate a switch statement.
The code on the right does generate a switch statement.

The code works similar, the only difference is that the duplicate "f->len > 3" check is moved to the top for the "right" version.

The compiler explorer code is actually a minimized version of the code I work on (with way more conditions in a hot code path), where I can not easily move the length check to the top, because the "f-> len > 3 && ..." is done in a very complicated macro, but that's just details.

I expected both code versions to generate the same assembler.

Tested with GCC-12.1 and 11.3. 10.3 does not generate a switch version for both versions, as only 11 got this nice feature.
On x86_64 Linux.


Please let me know if you need any additional details or if this report was useful at all.


---


### compiler : `gcc`
### title : `gcc ignores the pure attribute in PRE when function is not nothrow`
### open_at : `2022-05-27T09:26:49Z`
### last_modified_date : `2022-10-24T21:36:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105748
### status : `NEW`
### tags : `EH, missed-optimization`
### component : `c++`
### version : `12.1.0`
### severity : `normal`
### contents :
Snippet:
const int g_value() __attribute__((pure));
int bar(int n) {
    int s = 0;
    for (int i=0; i<n; ++i) {
        s += g_value() * i;
    }
    return s;
}

Expected: call g_value only once
Actual: call g_value n times

The problem can be reproduced in godbolt compiler explorer:
1. command line argument: -O3 -march=skylake-avx512 -std=c++2b
2. gcc failed to do loop-invariant code motion: https://godbolt.org/z/PvWc1xYvr
3. clang successfully performed loop-invariant code motion: https://godbolt.org/z/aEhMT88jM


---


### compiler : `gcc`
### title : `std::array comparision does not inline memcmp`
### open_at : `2022-05-27T15:45:58Z`
### last_modified_date : `2022-12-12T17:14:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105751
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
On trunk and gcc 12.1, with -O2/-O3 the following array comparison is only optimized to a memcmp, but not to a direct comparision:

```
bool test(std::array<int8_t, 8>& a, std::array<int8_t, 8>& b)
{
    return a == b;
}
```
which is optimized to:
```
test(std::array<signed char, 8ul>&, std::array<signed char, 8ul>&):
        sub     rsp, 8
        mov     edx, 8
        call    memcmp
        test    eax, eax
        sete    al
        add     rsp, 8
        ret
```

However for a direct memcmp gcc is able to optimize the memcmp to a single 64 bit comparision:
```
bool test(int8_t* a, int8_t* b)
{
    return memcmp(a, b, sizeof(int8_t) * 8) == 0;
}
```
which is optimized to
```
test(signed char*, signed char*):
        mov     rax, QWORD PTR [rsi]
        cmp     QWORD PTR [rdi], rax
        sete    al
        ret
```

This is optimized to a single comparision both in clang and msvc: https://godbolt.org/z/99reqf3Yz


---
