### Total Bugs Detected: 4649
### Current Chunk: 21 of 30
### Bugs in this Chunk: 160 (From bug 3201 to 3360)
---


### compiler : `gcc`
### title : `failure to fold a conditional that's a subset of another expression`
### open_at : `2021-03-24T23:36:30Z`
### last_modified_date : `2022-01-12T00:39:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99755
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
GCC successfully folds to false the second conditional expression in f1() but it fails to do the same in f2() and f3().  In addition (and likely as a result), it triggers a bogus -Wmaybe-uninitialized in both functions. 

Clang and ICC fold all three expressions to false and emit optimal code for all three functions.

Initializing the local variable to any value lets GCC fold the conditional and avoid the warning.  But then, replacing  the test for x != i + 1 in f3() with x != 3 as shown at below the first test case, the conditional is again not folded  (making the same change in f2() allows the folding to take place).

The bogus -Wmaybe-uninitialized first appeared in 4.9.  As far as I can tell none of the failures to fold is a regression.

$ cat t.c && gcc -O2 -S -Wall t.c
void f1 (int i)
{ 
  int x;
  if (i > 1)
    x = i + 1;

  if (i == 2 && x != i + 1)   // folded to false
    __builtin_abort ();
}

void f2 (int i, int j)
{ 
  int x;
  if (i > 1 && j > 2)
    x = i + 1;

  if (i == 2 && j == 3 && x != i + 1)   // not folded
    __builtin_abort ();
}


void f3 (int i, int j, int k)
{
  int x;
  if (i > 1 && j > 2 && k > 3)
    x = i + 1;

  if (i == 2 && j == 3 && k == 4 && x != i + 1)   // not folded
    __builtin_abort ();
}


;; Function f1 (f1, funcdef_no=0, decl_uid=1943, cgraph_uid=1, symbol_order=0)

void f1 (int i)
{
  <bb 2> [local count: 1073741824]:
  return;

}
t.c: In function ‘f2’:
t.c:17:24: warning: ‘x’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   17 |   if (i == 2 && j == 3 && x != i + 1)
      |       ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
t.c: In function ‘f3’:
t.c:28:34: warning: ‘x’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   28 |   if (i == 2 && j == 3 && k == 4 && x != i + 1)
      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~


The following is also not folded:

void f3 (int i, int j, int k)
{ 
  int x = 0;
  if (i > 1 && j > 2 && k > 3)
    x = i + 1;

  if (i == 2 && j == 3 && k == 4 && x != 3)   // not folded
    __builtin_abort ();
}


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O1)`
### open_at : `2021-03-25T21:35:22Z`
### last_modified_date : `2021-05-04T12:29:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99776
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[687] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210325 (experimental) [master revision 08103e4d6ad:4e4f8ee0bf5:a29124d28253cdf603ba1977db2f09c9f233fea5] (GCC) 
[688] % 
[688] % gcctk -O1 -S -o small_O1.s small.c
[689] % gcctk -O3 -S -o small_O3.s small.c
[690] % 
[690] % wc small_O1.s small_O3.s 
  23   52  455 small_O1.s
  37   82  682 small_O3.s
  60  134 1137 total
[691] % 
[691] % grep foo small_O1.s 
[692] % grep foo small_O3.s 
	call	foo
[693] % 
[693] % cat small.c
extern void foo(void);

static int a[2], b, *c[2];

int main() {
  for (b = 0; b < 2; b++)
    c[b] = &a[1];
  if (!c[0])
    foo();
  return 0;
}


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O1)`
### open_at : `2021-03-26T10:44:50Z`
### last_modified_date : `2023-08-18T01:35:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99788
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[606] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210326 (experimental) [master revision 9d45e848d02:ca344bbd24f:6081d8994ed1a0aef6b7f5fb34f091faa3580416] (GCC) 
[607] % 
[607] % gcctk -O1 -S -o O1.s small.c
[608] % gcctk -O3 -S -o O3.s small.c
[609] % 
[609] % wc O1.s O3.s
  79  162  986 O1.s
 109  229 1433 O3.s
 188  391 2419 total
[610] % 
[610] % grep foo O1.s
[611] % grep foo O3.s
	call	foo
[612] % 
[612] % cat small.c
extern void foo(void);

char a;
int b, *c, g, h = 1, i = 1, j, *k = &i;

static void d();
static int *e() {
  for (a = 1; a; a = a+2)
    ;
  foo();
  h = (g % h) % i;
  *k = -j;
  return 0;
}
static void f() {
  if (b)
    d(e);
}
void d() {
  for (;;)
    c = e();
}
int main() {
  f();
  return 0;
}


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -Os, -O2 and -O3 (vs. -O1)`
### open_at : `2021-03-26T15:04:25Z`
### last_modified_date : `2021-09-28T10:50:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99793
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[610] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210326 (experimental) [master revision 9d45e848d02:ca344bbd24f:6081d8994ed1a0aef6b7f5fb34f091faa3580416] (GCC) 
[611] % 
[611] % gcctk -O1 -S -o O1.s small.c
[612] % gcctk -O3 -S -o O3.s small.c
[613] % 
[613] % wc O1.s O3.s
 17  38 365 O1.s
 37  78 633 O3.s
 54 116 998 total
[614] % 
[614] % grep foo O1.s
[615] % grep foo O3.s
        call    foo
[616] % 
[616] % cat small.c
extern void foo(void);
static int a, *b = &a, c, *d = &c;
int main() {
  int **e = &d;
  if (!((unsigned)((*e = d) == 0) - (*b = 1)))
    foo();
  return 0;
}


---


### compiler : `gcc`
### title : `-funroll-all-loops bugs when using contexpr variable`
### open_at : `2021-03-30T04:42:34Z`
### last_modified_date : `2021-03-31T02:56:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99823
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50483
constexpr.cpp && unroll.cpp

-funroll-all-loops indicates "Unroll all loops, even if their number of iterations is uncertain when the loop is entered. This usually makes programs run more slowly.". But in the following example constexpr.cpp can never turn to unroll.cpp, even if number of iterations is certain due to constexpr.

compiler flag: -O3 -std=c++2a -DNDEBUG -funroll-all-loops


---


### compiler : `gcc`
### title : `inlining failed in call to ‘always_inline’ ‘memcpy’: --param max-inline-insns-auto limit reached`
### open_at : `2021-03-30T13:14:23Z`
### last_modified_date : `2022-09-23T19:50:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99828
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `lto`
### version : `11.0`
### severity : `normal`
### contents :
Noticed by Andi Kleen in kernel, reduced to:

$ cat 1.i
__attribute__((__always_inline__)) void *memcpy();
void *foo = memcpy;

$ touch 2.s
$ cat 3.i
int kasan_check_range();
int __memcpy();

enum { false, true }
memcpy(void *dest, void *src, long len) {
  if (kasan_check_range(len, false, 0) || kasan_check_range(len, true, 0))
    return __memcpy(dest, src, len);
}

int *LZ4_decompress_generic_outputSize_op;
int mlen;

LZ4_decompress_generic_outputSize_match() {
  {
    __builtin_memcpy(LZ4_decompress_generic_outputSize_op,
                     LZ4_decompress_generic_outputSize_match, mlen);
  }
}

$ gcc 1.i 2.s 3.i -c -flto -Os -shared && gcc [123].o -shared
1.i:1:42: warning: ‘always_inline’ function might not be inlinable [-Wattributes]
    1 | __attribute__((__always_inline__)) void *memcpy();
      |                                          ^~~~~~
3.i:5:1: warning: conflicting types for built-in function ‘memcpy’; expected ‘void *(void *, const void *, long unsigned int)’ [-Wbuiltin-declaration-mismatch]
    5 | memcpy(void *dest, void *src, long len) {
      | ^~~~~~
3.i:1:1: note: ‘memcpy’ is declared in header ‘<string.h>’
  +++ |+#include <string.h>
    1 | int kasan_check_range();
3.i:13:1: warning: return type defaults to ‘int’ [-Wimplicit-int]
   13 | LZ4_decompress_generic_outputSize_match() {
      | ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1.i:1:42: warning: type of ‘memcpy’ does not match original declaration [-Wlto-type-mismatch]
    1 | __attribute__((__always_inline__)) void *memcpy();
      |                                          ^
3.i:5:1: note: return value type mismatch
    5 | memcpy(void *dest, void *src, long len) {
      | ^
<built-in>: warning: type of ‘__builtin_memcpy’ does not match original declaration [-Wlto-type-mismatch]
3.i:5:1: note: return value type mismatch
3.i:5:1: note: ‘memcpy’ was previously declared here
3.i:5:1: note: code may be misoptimized unless ‘-fno-strict-aliasing’ is used
3.i: In function ‘LZ4_decompress_generic_outputSize_match’:
1.i:1:42: error: inlining failed in call to ‘always_inline’ ‘memcpy’: --param max-inline-insns-auto limit reached
    1 | __attribute__((__always_inline__)) void *memcpy();
      |                                          ^
3.i:15:5: note: called from here
   15 |     __builtin_memcpy(LZ4_decompress_generic_outputSize_op,
      |     ^
lto-wrapper: fatal error: gcc returned 1 exit status
compilation terminated.
/usr/bin/ld: error: lto-wrapper failed
collect2: error: ld returned 1 exit status


---


### compiler : `gcc`
### title : `[9 Regression] Alpha Compositing auto vectorization regression`
### open_at : `2021-03-31T20:24:39Z`
### last_modified_date : `2021-04-01T09:43:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99856
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `normal`
### contents :
Such code vectorized with gcc 8.3, but failed to vectorized with 9.1.
Last version of clang works just fine.

```
#include <stdint.h>
#include <stddef.h>

#define SHIFTFORDIV255(a)\
    ((((a) >> 8) + a) >> 8)

#define DIV255(a)\
    SHIFTFORDIV255(a + 0x80)

void
opSourceOver_premul(uint8_t* restrict Rrgba,
                    const uint8_t* restrict Srgba,
                    const uint8_t* restrict Drgba, size_t len)
{
    size_t i = 0;
    for (; i < len*4; i += 4) {
        uint8_t Sa = Srgba[i + 3];
        Rrgba[i + 0] = DIV255(Srgba[i + 0] * 255 + Drgba[i + 0] * (255 - Sa));
        Rrgba[i + 1] = DIV255(Srgba[i + 1] * 255 + Drgba[i + 1] * (255 - Sa));
        Rrgba[i + 2] = DIV255(Srgba[i + 2] * 255 + Drgba[i + 2] * (255 - Sa));
        Rrgba[i + 3] = DIV255(Srgba[i + 3] * 255 + Drgba[i + 3] * (255 - Sa));
    }
}
```

see https://godbolt.org/z/PW65xzb4h
with assembler output for 8.3 and 9.1


---


### compiler : `gcc`
### title : `[11 Regression] GCC no longer makes as much use of ST3`
### open_at : `2021-04-01T18:19:39Z`
### last_modified_date : `2021-04-07T18:04:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99873
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For:

void
f (int *restrict x, int *restrict y, int *restrict z, int n)
{
  for (int i = 0; i < n; i += 3)
    {
      x[i] = y[i] + z[i];
      x[i + 1] = y[i + 1] - z[i + 1];
      x[i + 2] = y[i + 2] | z[i + 2];
    }
}

GCC 10 produced a nice loop using LD3 and ST3:

.L4:
        ld3     {v4.4s - v6.4s}, [x4], 48
        ld3     {v16.4s - v18.4s}, [x6], 48
        add     v1.4s, v16.4s, v4.4s
        sub     v2.4s, v5.4s, v17.4s
        orr     v3.16b, v18.16b, v6.16b
        st3     {v1.4s - v3.4s}, [x5], 48
        cmp     x8, x4
        bne     .L4

But GCC 11 instead uses lane stores:

.L4:
        ld3     {v4.4s - v6.4s}, [x9], 48
        mov     x8, x4
        ld3     {v16.4s - v18.4s}, [x11], 48
        add     x16, x4, 24
        add     x15, x4, 36
        add     x14, x4, 16
        add     x13, x4, 28
        add     x12, x4, 40
        add     v2.4s, v16.4s, v4.4s
        add     x7, x4, 20
        sub     v1.4s, v5.4s, v17.4s
        add     x6, x4, 32
        orr     v0.16b, v18.16b, v6.16b
        add     x5, x4, 44
        add     x4, x4, 48
        str     s2, [x8], 12
        st1     {v2.s}[1], [x8]
        st1     {v2.s}[2], [x16]
        st1     {v2.s}[3], [x15]
        str     s1, [x4, -44]
        st1     {v1.s}[1], [x14]
        st1     {v1.s}[2], [x13]
        st1     {v1.s}[3], [x12]
        str     s0, [x4, -40]
        st1     {v0.s}[1], [x7]
        st1     {v0.s}[2], [x6]
        st1     {v0.s}[3], [x5]
        cmp     x10, x9
        bne     .L4

I think this is due to r11-3966 optimistically splitting store groups
in a way that we can't recover from if SLP subsequently fails.

Maybe the easiest thing for GCC 11 would be to block the split if the
target supports IFN_STORE_LANES for the group size and element type.
That restores the above case.  Of the two tests affected by r11-3966,
vect-complex-5.c seems better with LD2 & ST4, while the motivating
case (pr97428.c) still uses SLP as intended.


---


### compiler : `gcc`
### title : `std::filesystem::absolute is inefficient`
### open_at : `2021-04-01T19:25:22Z`
### last_modified_date : `2021-10-12T16:32:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99876
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.1.0`
### severity : `normal`
### contents :
Created attachment 50498
Preprocesses source code

The documentation for std::filesystem::absolute states:

"For POSIX-based operating systems, std::filesystem::absolute(p) is equivalent to std::filesystem::current_path() / p except for when p is the empty path. "

g++ implements it is way -- that is correct, but wasteful.

If the given filename is already absolute, it should be simply returned.
There is no need to call current_path() which leads to a getcwd syscall.





# /usr/local/products/gcc/10.1.0/bin/g++ -std=gnu++17 -Wall -O2 ttt.C

# strace ./a.out 2>&1 | tail
getcwd("/work/nova7/23232/src", 4096)   = 22
getcwd("/work/nova7/23232/src", 4096)   = 22
getcwd("/work/nova7/23232/src", 4096)   = 22
getcwd("/work/nova7/23232/src", 4096)   = 22
getcwd("/work/nova7/23232/src", 4096)   = 22
getcwd("/work/nova7/23232/src", 4096)   = 22
getcwd("/work/nova7/23232/src", 4096)   = 22
getcwd("/work/nova7/23232/src", 4096)   = 22
exit_group(0)                           = ?
+++ exited with 0 +++

# cat ttt.C
#include <filesystem>

int
main()
{
  std::filesystem::path foo ("/home/welinder");

  for (int i = 0; i < 10000; i++)
    (void)std::filesystem::absolute(foo);
}

# uname -a
Linux monsterd03 5.3.18-lp152.66-default #1 SMP Tue Mar 2 13:18:19 UTC 2021 (73933a3) x86_64 x86_64 x86_64 GNU/Linux

# /usr/local/products/gcc/10.1.0/bin/g++ -v
Using built-in specs.
COLLECT_GCC=/usr/local/products/gcc/10.1.0/bin/g++
COLLECT_LTO_WRAPPER=/usr/local/products/gcc/10.1.0/lib/gcc/x86_64-suse-linux/10.1.0/lto-wrapper
Target: x86_64-suse-linux
Configured with: ../../gcc-10.1.0/configure --enable-languages=c,c++,fortran --enable-targets=x86_64-suse-linux,i686-suse-linux --prefix=/usr/local/products/gcc/10.1.0 --with-gnu-as --with-as=/usr/local/products/gcc/binutils-2.32/bin/as --with-gnu-ld --with-ld=/usr/local/products/gcc/binutils-2.32/bin/ld --enable-threads=posix --enable-shared --enable-__cxa_atexit --enable-libstdcxx-allocator=pool x86_64-suse-linux
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.1.0 (GCC)


---


### compiler : `gcc`
### title : `Failure to optimize log2 pattern to clz`
### open_at : `2021-04-03T00:04:07Z`
### last_modified_date : `2023-09-21T09:22:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99887
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
static inline unsigned int_log2_rec(unsigned x)
{
  return x == 0 ? 0 : int_log2_rec(x >> 1) + 1;
}

unsigned int_log2(unsigned x)
{
  return x == 0 ? 0 : int_log2_rec(x) - 1;
}

This can be optimized to `return x == 0 ? x : 31 - __builtin_clz(x);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `SIMD: negating logical + if_else has a suboptimal codegen.`
### open_at : `2021-04-04T12:30:38Z`
### last_modified_date : `2023-08-08T01:29:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99908
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Similar to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98461

Suboptimal codegen for simd negation of a logical + if_else
https://godbolt.org/z/WPE3cx75z


---


### compiler : `gcc`
### title : `Unnecessary / inefficient spilling of AVX2 ymm registers`
### open_at : `2021-04-05T01:53:16Z`
### last_modified_date : `2023-03-15T13:15:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99912
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50507
Compressed preprocessed source code

I am using "g++ (Spack GCC) 11.0.1 20210404 (experimental)" (fresh checkout) on MacOS 11.2.3 with a x86-64 Skylake CPU.

I am manually SIMD-vectorizing a loop kernel using AVX2 intrinsics. The generated code is correct, but has obvious inefficiencies. I find these issues:

1. There are spills (?) of AVX2 ymm registers that are overwritten by another spill a few instructions later, without being read in the mean time

2. The same register is spilled into multiple stack slots in consecutive instructions

3. After spilling an ymm register, the stack slot is copied to another stack slot, using xmm registers (i.e. using two loads/stores)

I tried to reproduce the issue in a small example, but failed. If this issue is really due to spilling, then it might not be possible to have a small test case.



Here is an example of issues 1 and 2; I show a few lines from the attached disassembled file to clarify:
{{{
    1520: c5 fd 29 8c 24 a0 24 00 00   	vmovapd	%ymm1, 9376(%rsp)
    1529: c5 fd 29 8c 24 20 29 00 00   	vmovapd	%ymm1, 10528(%rsp)
    1532: c5 fd 29 b4 24 80 28 00 00   	vmovapd	%ymm6, 10368(%rsp)
    153b: c5 fd 29 ac 24 a0 28 00 00   	vmovapd	%ymm5, 10400(%rsp)
    1544: c5 fd 29 a4 24 c0 28 00 00   	vmovapd	%ymm4, 10432(%rsp)
    154d: c5 fd 29 9c 24 e0 28 00 00   	vmovapd	%ymm3, 10464(%rsp)
    1556: c5 fd 29 94 24 00 29 00 00   	vmovapd	%ymm2, 10496(%rsp)
    155f: c4 a2 1d 2d 34 30            	vmaskmovpd	(%rax,%r14), %ymm12, %ymm6
    1565: 48 8b 84 24 00 05 00 00      	movq	1280(%rsp), %rax
    156d: c5 fd 29 b4 24 00 24 00 00   	vmovapd	%ymm6, 9216(%rsp)
    1576: c4 a2 1d 2d 2c 30            	vmaskmovpd	(%rax,%r14), %ymm12, %ymm5
    157c: 48 8b 84 24 38 07 00 00      	movq	1848(%rsp), %rax
    1584: c5 fd 29 ac 24 20 24 00 00   	vmovapd	%ymm5, 9248(%rsp)
    158d: c4 a2 1d 2d 24 30            	vmaskmovpd	(%rax,%r14), %ymm12, %ymm4
    1593: 48 8b 84 24 60 04 00 00      	movq	1120(%rsp), %rax
    159b: c5 fd 29 a4 24 40 24 00 00   	vmovapd	%ymm4, 9280(%rsp)
    15a4: c4 a2 1d 2d 1c 30            	vmaskmovpd	(%rax,%r14), %ymm12, %ymm3
    15aa: 48 8b 84 24 68 04 00 00      	movq	1128(%rsp), %rax
    15b2: c5 fd 29 9c 24 60 24 00 00   	vmovapd	%ymm3, 9312(%rsp)
    15bb: c4 a2 1d 2d 14 30            	vmaskmovpd	(%rax,%r14), %ymm12, %ymm2
    15c1: c5 fd 29 94 24 80 24 00 00   	vmovapd	%ymm2, 9344(%rsp)
    15ca: 48 8b 84 24 08 05 00 00      	movq	1288(%rsp), %rax
    15d2: c4 a2 1d 2d 0c 30            	vmaskmovpd	(%rax,%r14), %ymm12, %ymm1
    15d8: 48 8b 84 24 70 04 00 00      	movq	1136(%rsp), %rax
    15e0: c5 fd 29 8c 24 a0 24 00 00   	vmovapd	%ymm1, 9376(%rsp)
    15e9: c5 fd 29 b4 24 40 29 00 00   	vmovapd	%ymm6, 10560(%rsp)
    15f2: c5 fd 29 ac 24 60 29 00 00   	vmovapd	%ymm5, 10592(%rsp)
    15fb: c5 fd 29 a4 24 80 29 00 00   	vmovapd	%ymm4, 10624(%rsp)
    1604: c5 fd 29 9c 24 a0 29 00 00   	vmovapd	%ymm3, 10656(%rsp)
    160d: c5 fd 29 94 24 c0 29 00 00   	vmovapd	%ymm2, 10688(%rsp)
    1616: c5 fd 29 8c 24 e0 29 00 00   	vmovapd	%ymm1, 10720(%rsp)
}}}

The beginning and end of this sample are what I think might be spill instructions. The instruction at 1520 writes to 9376(%rsp), and the instruction at 15e0 overwrites this stack slot. Also, the register %ymm1 is written multiple times to different stack slots. (That by itself could be fine, but it looks strange.)

A few instructions later I find this code:
{{{
    16d7: c5 79 6f 84 24 80 28 00 00   	vmovdqa	10368(%rsp), %xmm8
    16e0: c5 79 6f ac 24 20 29 00 00   	vmovdqa	10528(%rsp), %xmm13
    16e9: c5 79 7f 84 24 e0 19 00 00   	vmovdqa	%xmm8, 6624(%rsp)
    16f2: c5 79 6f 84 24 90 28 00 00   	vmovdqa	10384(%rsp), %xmm8
    16fb: c5 79 7f ac 24 80 1a 00 00   	vmovdqa	%xmm13, 6784(%rsp)
    1704: c5 79 7f 84 24 f0 19 00 00   	vmovdqa	%xmm8, 6640(%rsp)
}}}
This copies the 32 bytes at 10368(%rsp) (written above), but uses %xmm8 to copy the stack slot in 16-byte chunks. This shouldn't happen; there is no reason to copy from one stack slot to another (presumably, since I know the code, but I could be mistaken here). There is also no reason to copy in 16-byte chunks. (All relevant local variables are ultimately of type __m256d, wrapped in C++ structs, and should thus be correctly aligned.)



To give some background information: The loop is quite large; it is part of a complex numerical kernel for the Einstein equations <http://einsteintoolkit.org>. I expect there to be a significant number of local variables / stack spill slots, but these should still fit into the L1 data cache. The instructions for the kernel occupy currently about 44 kB. I plan to reduce this later, and removing unnecessary stack spills would help.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] suboptimal code for bool bitfield tests`
### open_at : `2021-04-05T20:15:29Z`
### last_modified_date : `2023-07-19T04:13:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99918
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
GCC does a better job folding operations involving plain Booleans than it does with bool bit-fields.  The example below shows that in f() the return statement is folded to zero while in g() it's not.  This is behind a class of -Wmaybe-uninitialized warnings.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
struct A { _Bool i, j; };

_Bool f (struct A a)
{
  if (a.i)
    a.j = 0;
  else
    a.j = a.i;

  return a.j;    // folded to 0
}

struct B { _Bool i: 1, j: 1; };
  
_Bool g (struct B b)
{
  if (b.i)
    b.j = 0;
  else
    b.j = b.i;

  return b.j;    // not folded
}


;; Function f (f, funcdef_no=0, decl_uid=1946, cgraph_uid=1, symbol_order=0)

_Bool f (struct A a)
{
  <bb 2> [local count: 1073741824]:
  return 0;

}



;; Function g (g, funcdef_no=1, decl_uid=1953, cgraph_uid=2, symbol_order=1)

Removing basic block 5
_Bool g (struct B b)
{
  _Bool b$j;
  unsigned char _1;
  unsigned char _2;
  _Bool _3;

  <bb 2> [local count: 1073741824]:
  _1 = VIEW_CONVERT_EXPR<unsigned char>(b);
  _2 = _1 & 1;
  if (_2 != 0)
    goto <bb 4>; [50.00%]
  else
    goto <bb 3>; [50.00%]

  <bb 3> [local count: 536870913]:
  _3 = b.i;

  <bb 4> [local count: 1073741824]:
  # b$j_5 = PHI <0(2), _3(3)>
  return b$j_5;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] bogus -Wmaybe-uninitialized with a _Bool bit-field`
### open_at : `2021-04-05T20:29:06Z`
### last_modified_date : `2023-07-27T09:22:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99919
### status : `ASSIGNED`
### tags : `deferred, diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
This is the -Wmaybe-uninitialized subset of pr99918 showing how the failure to fail _Bool bit-field expressions can cause bogus warnings.  (Like pr99918, this was most likely caused by r225825.)

$ cat z.c && gcc -O2 -S -Wall z.c
struct B { _Bool i: 1; _Bool j: 1; };

_Bool z;

void g (struct B b)
{
  _Bool x;

  if (b.i)
    b.j = 0;
  else
    {
      b.j = b.i;
      x = b.i;
    }

  if (b.j)
    z = x;
}

z.c: In function ‘g’:
z.c:18:7: warning: ‘x’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   18 |     z = x;
      |     ~~^~~
z.c:7:9: note: ‘x’ was declared here
    7 |   _Bool x;
      |         ^


---


### compiler : `gcc`
### title : `Failure to optimize floating point -abs(x) in nontrivial code at -O2/3`
### open_at : `2021-04-06T10:38:17Z`
### last_modified_date : `2021-11-28T07:08:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99930
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Expected compiler output for -abs(x) is an orps setting the sign bit.


It works as expected with trivial code at -O1/2/3 optimization levels:

float q(float p)
{
    return -std::abs(p);
}

orps    xmm0, XMMWORD PTR .LC1[rip]
ret


With more complex code the compiler uses orps at -O1 but andps + xorps at -O2/3:

bool t(float n[2], float m)
{
    for (int i = 0; i < 2; i++)
        if (m > -std::abs(n[i]))
            return true;
    return false;
}

-O1
movss   xmm1, DWORD PTR [rdi]
orps    xmm1, XMMWORD PTR .LC1[rip]
comiss  xmm0, xmm1
ja      .L3
movss   xmm1, DWORD PTR [rdi+4]
orps    xmm1, XMMWORD PTR .LC1[rip]
comiss  xmm0, xmm1
seta    al
ret

-O2/3
movss   xmm1, DWORD PTR [rdi]
movss   xmm3, DWORD PTR .LC0[rip]
movss   xmm2, DWORD PTR .LC1[rip]
andps   xmm1, xmm3
xorps   xmm1, xmm2
comiss  xmm0, xmm1
ja      .L3
movss   xmm1, DWORD PTR [rdi+4]
andps   xmm1, xmm3
xorps   xmm1, xmm2
comiss  xmm0, xmm1
seta    al
ret

https://godbolt.org/z/5ch5ceEj7


---


### compiler : `gcc`
### title : `Optimization needed for ARM with single cycle multiplier`
### open_at : `2021-04-06T14:02:36Z`
### last_modified_date : `2021-04-07T16:23:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99937
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50512
Source file(s)

I am cross-compiling using "arm-none-eabi-gcc -mcpu=cortex-m0plus -Wall -Wextra -fno-strict-aliasing -fwrapv -O3 -S foobar.c" for a target architecture that performs a multiply in a single cycle, using gcc version 10.2.0 on a PC running Fedora Linux.

Is there an option to persuade the compiler to use the multiply instruction automatically instead of shifts and adds when multiplying by a constant?

In the example code attached, gcc uses the trick of multiplying by a big number instead of dividing by a small one (12 in this case). For my target, the code from "-O3" is both longer and slower then that for "-Os".


---


### compiler : `gcc`
### title : `fail to exchange if conditions in terms of likely/unlikely probability`
### open_at : `2021-04-07T02:01:51Z`
### last_modified_date : `2021-04-07T07:33:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99946
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For this simple case,

$ cat test_cond.c 
#define likely(x)       __builtin_expect((x),1)
#define unlikely(x)     __builtin_expect((x),0)

extern void g(void);

int a, b;
void f(void)
{
  if (likely(a>0))
    if (unlikely(b>0))
      g();
}

We expect gcc compiler can exchange the if conditions to be like below,

  if (unlikely(b>0))
    if (likely(a>0))
      g();

This way, performance can be improved due to saving the comparison for a>0.

At the moment, gcc generate code as below,

.LFB0:
	.cfi_startproc
	movl	a(%rip), %edx
	testl	%edx, %edx
	jle	.L1
	movl	b(%rip), %eax
	testl	%eax, %eax
	jg	.L4
.L1:
	ret


---


### compiler : `gcc`
### title : `In AVX, SIMD support environment, strlen performance without optimization is 3 times faster than optimized strlen function.`
### open_at : `2021-04-07T07:53:00Z`
### last_modified_date : `2021-04-07T09:05:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99953
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Created attachment 50519
the preprocessed file

I tested the performance of 65K bytes string and 65536 times for each O0, O1, O2, O3, and the related performance was not optimized as shown below. If it is not optimized, it has been confirmed that glibc@strlen_avx is called.

$ gcc -Wall -Wextra -fno-strict-aliasing -fwrapv -fno-aggressive-loop-optimizations  -fsanitize=undefined -save-temps strlen.c

$ ./a.out 
no optimize =>  0.000007655
o1 optimize =>  0.000062935
o2 optimize =>  0.000022461
o3 optimize =>  0.000023192

$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none:hsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.3.0-17ubuntu1~20.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-HskZEa/gcc-9-9.3.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)


---


### compiler : `gcc`
### title : `loop interchange fails when altering bwaves inner loop`
### open_at : `2021-04-07T12:21:05Z`
### last_modified_date : `2021-04-26T11:59:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99956
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
When scheduling an additional DSE pass the bwaves kernel (gfortran.dg/pr81303.f)
is no longer interchanged because prepare_perfect_loop_nest tries to use
a too outer loop where we fail to compute access functions for the data references.  If artificially restricting the nest to only cover the two innermost
loops the interchange still works.

A modified testcase simulating the effect of the DSE is provided below:

! { dg-options "-O3 -ffast-math -floop-interchange -fdump-tree-linterchange-details" }

        subroutine mat_times_vec(y,x,a,axp,ayp,azp,axm,aym,azm,
     $  nb,nx,ny,nz)
        implicit none
        integer nb,nx,ny,nz,i,j,k,m,l,kit,im1,ip1,jm1,jp1,km1,kp1

        real*8 y(nb,nx,ny,nz),x(nb,nx,ny,nz),tem

        real*8 a(nb,nb,nx,ny,nz),
     1  axp(nb,nb,nx,ny,nz),ayp(nb,nb,nx,ny,nz),azp(nb,nb,nx,ny,nz),
     2  axm(nb,nb,nx,ny,nz),aym(nb,nb,nx,ny,nz),azm(nb,nb,nx,ny,nz)


      do k=1,nz
         km1=mod(k+nz-2,nz)+1
         kp1=mod(k,nz)+1
         do j=1,ny
            jm1=mod(j+ny-2,ny)+1
            jp1=mod(j,ny)+1
            do i=1,nx
               im1=mod(i+nx-2,nx)+1
               ip1=mod(i,nx)+1
               do l=1,nb
                  tem=0.0
                  do m=1,nb
                     tem=tem+
     1               a(l,m,i,j,k)*x(m,i,j,k)+
     2               axp(l,m,i,j,k)*x(m,ip1,j,k)+
     3               ayp(l,m,i,j,k)*x(m,i,jp1,k)+
     4               azp(l,m,i,j,k)*x(m,i,j,kp1)+
     5               axm(l,m,i,j,k)*x(m,im1,j,k)+
     6               aym(l,m,i,j,k)*x(m,i,jm1,k)+
     7               azm(l,m,i,j,k)*x(m,i,j,km1)
                  enddo
                  y(l,i,j,k)=tem
               enddo
            enddo
         enddo
        enddo
        return
        end

! { dg-final { scan-tree-dump-times "is interchanged" 1 "linterchange" } }


---


### compiler : `gcc`
### title : `Bounds check not eliminated by assert`
### open_at : `2021-04-08T01:23:54Z`
### last_modified_date : `2022-11-28T22:09:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99966
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
#include <cassert>
#include <cstdint>
#include <cstddef>
#include <vector>

uint64_t f(std::vector<uint64_t>& data, size_t start, size_t end){
    assert(start < end && start < data.size() && end <= data.size());


    uint64_t total = 0;
    for (size_t i = start; i < end; i++) {
        total += data.at(i);
    }
    return total;
}

https://godbolt.org/z/Ksecrec11

Clang is able to eliminate the the check when using -mllvm -enable-constraint-elimination
https://godbolt.org/z/K67b8PTM9


---


### compiler : `gcc`
### title : `GCC generates partially vectorized and scalar code at once`
### open_at : `2021-04-08T14:41:49Z`
### last_modified_date : `2021-04-23T09:03:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99971
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Consider the following code sample:

struct A
{
    unsigned int a, b, c, d;

    A& operator+= (A const& that)
    {
        a += that.a;
        b += that.b;
        c += that.c;
        d += that.d;
        return *this;
    }

    A& operator-= (A const& that)
    {
        a -= that.a;
        b -= that.b;
        c -= that.c;
        d -= that.d;
        return *this;
    }
};

void test(A& x, A const& y1, A const& y2)
{
    x += y1;
    x -= y2;
}

The code, when compiled with options "-O3 -march=nehalem", generates:

test(A&, A const&, A const&):
        pushq   %rbp
        movdqu  (%rdi), %xmm1
        pushq   %rbx
        movl    4(%rsi), %r8d
        movdqu  (%rsi), %xmm0
        movl    (%rsi), %r9d
        paddd   %xmm1, %xmm0
        movl    8(%rsi), %ecx
        movl    12(%rsi), %eax
        movl    %r8d, %esi
        movl    (%rdi), %ebp
        movl    4(%rdi), %ebx
        movl    8(%rdi), %r11d
        movl    12(%rdi), %r10d
        movups  %xmm0, (%rdi)
        subl    (%rdx), %r9d
        subl    4(%rdx), %esi
        subl    8(%rdx), %ecx
        subl    12(%rdx), %eax
        addl    %ebp, %r9d
        addl    %ebx, %esi
        movl    %r9d, (%rdi)
        popq    %rbx
        addl    %r11d, %ecx
        popq    %rbp
        movl    %esi, 4(%rdi)
        addl    %r10d, %eax
        movl    %ecx, 8(%rdi)
        movl    %eax, 12(%rdi)
        ret

https://gcc.godbolt.org/z/Mzchj8bxG

Here you can see that the compiler has partially vectorized the test function - it converted "x += y1" to paddd, as expected, but failed to vectorize "x -= y2". But at the same time the compiler also generated scalar code, including for the already vectorized "x += y1" line, basically duplicating it.

Note that when either "x += y1" or "x -= y2" is commented, the compiler is able to vectorize the line that is left. It is also able to vectorize both lines when the += and -= operators are applied to different objects instead of x.

This is reproducible since gcc 8 up to and including 10.2. gcc 7 doesn't vectorize this code. With the current trunk on godbolt the generated code is different:

test(A&, A const&, A const&):
        movdqu  (%rsi), %xmm0
        movdqu  (%rdi), %xmm1
        paddd   %xmm1, %xmm0
        movups  %xmm0, (%rdi)
        movd    %xmm0, %eax
        subl    (%rdx), %eax
        movl    %eax, (%rdi)
        pextrd  $1, %xmm0, %eax
        subl    4(%rdx), %eax
        movl    %eax, 4(%rdi)
        pextrd  $2, %xmm0, %eax
        subl    8(%rdx), %eax
        movl    %eax, 8(%rdi)
        pextrd  $3, %xmm0, %eax
        subl    12(%rdx), %eax
        movl    %eax, 12(%rdi)
        ret

Here the compiler is able to vectorize "x += y1" but not "x -= y2". At least, it removed the duplicate scalar version of "x += y1".

Given that the compiler is able to vectorize each line in isolation, I would expect it to be able to vectorize them combined. Generating duplicate versions of code is certainly not expected.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] missed optimization for dead code elimination at -O3 (vs. -O2)`
### open_at : `2021-04-09T09:54:27Z`
### last_modified_date : `2023-08-18T06:36:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99987
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[748] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210409 (experimental) [master revision 96292c3e343:4e14cad25b9:019a922063f26784d5a070d9198a1f937b8a8343] (GCC) 
[749] % 
[749] % 
[749] % gcctk -O2 -S -o O2.s small.c
[750] % gcctk -O3 -S -o O3.s small.c
[751] % 
[751] % wc O2.s O3.s
  68  150  961 O2.s
  89  195 1278 O3.s
 157  345 2239 total
[752] % 
[752] % grep foo O2.s
[753] % grep foo O3.s
        call    foo
[754] % 
[754] % cat small.c
extern void foo(void);
static int a[8] = {0,0,0,0,0,0,0,0};
int b, c, j;
static int *e(int k) {
  if (k && b)
    j = 0;
  while (j)
    if (!k)
      foo();
  return 0;
}
static void d() {
  long g;
  unsigned h = 8;
  if (b)
    for (g = 2; 1; g--) {
      int i = 0;
      h--;
      c = (a[g] == (&i != e(h)));
    }
}
void f() { d(); }


---


### compiler : `gcc`
### title : `aarch64: GCC generates excessive consecutive bti j instructions`
### open_at : `2021-04-09T10:13:05Z`
### last_modified_date : `2021-05-12T15:08:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99988
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50535
minimal reproducer

For the attached testcase (reduced from the linux kernel), GCC generates multiple redundant sequences of back-to-back bti j instructions, the longest of which is 262 instructions long.

To reproduce:

$ aarch64-linux-gnu-gcc -c test.c -S -o - -O2 -mbranch-protection=standard | uniq -c | grep "bti j" | sort -nr
    262         hint    36 // bti j
      7         hint    36 // bti j
      6         hint    36 // bti j
      4         hint    36 // bti j
      4         hint    36 // bti j
      3         hint    36 // bti j
      2         hint    36 // bti j
      2         hint    36 // bti j
      2         hint    36 // bti j
      2         hint    36 // bti j
      2         hint    36 // bti j
      2         hint    36 // bti j
      2         hint    36 // bti j


---


### compiler : `gcc`
### title : `Missed optimisation with -Os`
### open_at : `2021-04-09T14:03:18Z`
### last_modified_date : `2023-09-21T13:55:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99997
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `normal`
### contents :
Created attachment 50538
Test source

Using the Fedora 33 x86_64 compiler:
gcc version 10.2.1 20201125 (Red Hat 10.2.1-9) (GCC) 

Building the following (see also attached file):

typedef _Bool bool;
#define __always_inline inline __attribute__((__always_inline__))
enum { PG_head = 16 };
struct page {
	unsigned long flags;
	unsigned long compound_head;	/* Bit zero is set */
};
static inline bool constant_test_bit(int nr, const void *addr)
{
	const unsigned int *p = (const unsigned int *)addr;
	return ((1UL << (nr & 31)) & (p[nr >> 5])) != 0;
}
static __always_inline bool PageTail(struct page *page)
{
	return page->compound_head & 1;
}
static __always_inline bool PageCompound(struct page *page)
{
	return constant_test_bit(PG_head, &page->flags) || PageTail(page);
}
bool PageTransCompound(struct page *page)
{
	return PageCompound(page);
}

with "gcc -Os" I get the following assembly:

PageTransCompound:
.LFB3:
        .cfi_startproc
        movl    (%rdi), %edx
        movl    $1, %eax
        btl     $16, %edx
        jc      .L2
        movq    8(%rdi), %rax
        andl    $1, %eax
.L2:
        andl    $1, %eax
        ret
        .cfi_endproc

There are two consecutive identical ANDL instructions, one of which is superfluous.  The compile could eliminate the one that's immediately prior to the .L2 instruction.


---


### compiler : `gcc`
### title : `non-leaf epologue/prologue used if MVE v4sf is used for load/return`
### open_at : `2021-04-09T14:43:21Z`
### last_modified_date : `2023-03-09T16:56:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100000
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef float __attribute((vector_size(16))) v4sf;

v4sf f(v4sf *p)
{
    return *p;
}

gives:

f:
        vldrw.32        q0, [r0]
        sub     sp, sp, #32
        add     sp, sp, #32
        bx      lr

with -march=armv8.1-m.main+mve -mfloat-abi=hard -O2 but the sub/add is redundant.


---


### compiler : `gcc`
### title : `Dead write not removed when indirection is introduced.`
### open_at : `2021-04-09T17:01:02Z`
### last_modified_date : `2021-04-12T08:02:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100004
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
struct Foo {
    int x;
};

struct Bar {
    int x;
};

void alias(Foo* foo, Bar* bar) {
    foo->x = 5;
    foo->x = bar->x;
}

struct Wrap1 {
    Foo foo;
};

struct Wrap2 {
    Foo foo;
};

void assign_direct(Wrap1* w1, Wrap2* w2)
{
    w1->foo.x = 5;
    w1->foo.x = w2->foo.x;
}

void assign_via_pointer(Wrap1* w1, Wrap2* w2)
{
    Foo* f1 = &w1->foo;
    Foo* f2 = &w2->foo;
    f1->x = 5;
    f1->x = f2->x;
}


$ gcc-arm64 -O2 -std=c++17 -fstrict-aliasing -S -o -

alias(Foo*, Bar*):
        ldr     w1, [x1]
        str     w1, [x0]
        ret
assign_direct(Wrap1*, Wrap2*):
        ldr     w1, [x1]
        str     w1, [x0]
        ret
assign_via_pointer(Wrap1*, Wrap2*):
        mov     w2, 5
        str     w2, [x0]
        ldr     w1, [x1]
        str     w1, [x0]
        ret


---


### compiler : `gcc`
### title : `[10 Regression] arm64 failure to generate bfxil`
### open_at : `2021-04-11T01:44:22Z`
### last_modified_date : `2023-07-07T09:33:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100028
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50555
should generate bfxil but doesn't

The attached code reproduced here:

#define W	3
#define L	11

int bfxil(int d, int s)
{
	int wmask = (1 << W) - 1;
	return (d & ~wmask) | ((s >> L) & wmask);
}

Should return:
	bfxil:
		bfxil	w0, w1, 11, 3
		ret

but instead returns:
	bfxil:
		ubfx	x1, x1, 11, 3
		and	w0, w0, -8
		orr	w0, w1, w0
		ret

The problem is still present in trunk, was also present in 9.3 but wasn't in GCC 8.2 (see https://gcc.godbolt.org/z/E6z31hr9r ).


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O2)`
### open_at : `2021-04-11T14:36:57Z`
### last_modified_date : `2023-06-09T17:16:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100033
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[583] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210411 (experimental) [master revision 1d54b138417:e83b9cf4549:936d500dfc17f58f2507ecd0f7f26e4f197052ee] (GCC) 
[584] % 
[584] % gcctk -O2 -S -o O2.s small.c
[585] % gcctk -O3 -S -o O3.s small.c
[586] % 
[586] % wc O2.s O3.s
  52  112  797 O2.s
  56  119  850 O3.s
 108  231 1647 total
[587] % 
[587] % grep foo O2.s
[588] % grep foo O3.s
	call	foo
[589] % 
[589] % cat small.c
extern void foo(void);
static volatile int a;
int b, i;
static void c() {
  unsigned d = i;
  a = d && a;
  if (d) {
    int e;
    while (b) {
      int *f, *g = &e, **h = &f;
      *h = g;
      *f = d;
    }
    if (!e)
      foo();
  }
}
int main() {
  c();
  return 0;
}


---


### compiler : `gcc`
### title : `Precomputing division`
### open_at : `2021-04-12T10:49:27Z`
### last_modified_date : `2022-01-21T08:18:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100045
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
Created attachment 50567
Test case

We use the method given in "Division by Invariant Integers using Multiplication"
by Granlund and Montgomery for optimizing division by divisors known to
be constant at compile time.

There can also be an advantage if many numbers are divided by the
same numbers; in this case, the invariant inverse can be moved out of
the loop.  This is target-dependent.

The attached test case performs 10000000 unsigned divisions of uint32_t
values read in randomly by a constant randomly chosen to be 12345678

- using the method from figure 4.1 from the publication cited above
  (timing in seconds given as pre_divide)

- using a simple loop with divisions (timing in seconcs given as divide).

On a AMD Ryzen 7 1700X, the timings are

pre_divide: t = 0.013330 s
divide    : t = 0.052511 s

OTOH, on POWER (gcc135), the difference is so small so that is very probably
not worth the bother:

pre_divide: t = 0.015183 s
divide    : t = 0.017454 s


---


### compiler : `gcc`
### title : `compare with itself`
### open_at : `2021-04-12T12:02:33Z`
### last_modified_date : `2023-06-18T18:55:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100046
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Created attachment 50569
compare with itself

The attached file reproduced here:

int b3_06(int x, int y, int z) {
  int a = (x | z) ^ (y | z);
  int b = (x ^ y) & ~z;
  return a == b;
}

The generated assembly for for arm64 is:
    b3_06:
        eor     w3, w1, w0
        bic     w3, w3, w2
        cmp     w3, w3
        cset    w0, eq
        ret

So, GCC is able to see that both expressions are equivalent. Nice.
But then there is this compare with itself :(

The problem seems to exist forever on all targets (see
https://gcc.godbolt.org/z/qrYWsznof ).


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -Os, -O2 and -O3 (vs. -O1)`
### open_at : `2021-04-12T20:15:16Z`
### last_modified_date : `2021-04-27T10:41:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100051
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[719] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210412 (experimental) [master revision 46c47420a5f:66634f1d5eb:8f17d44ad986e0ae7e81c81179463000ee9d7f43] (GCC) 
[720] % 
[720] % gcctk -O1 -S -o O1.s small.c
[721] % gcctk -O3 -S -o O3.s small.c
[722] % 
[722] % wc O1.s O3.s
  56  121  784 O1.s
  68  148  996 O3.s
 124  269 1780 total
[723] % 
[723] % grep foo O1.s
[724] % grep foo O3.s
	call	foo
[725] % 
[725] % cat small.c
extern void foo(void);
int a, c, *f, **d = &f;
char b;
static void e() {
  if ((2 ^ b) == 0)
    foo();
}
int main() {
  if (a) {
    b = 0;
    int *g = &c;
    *g = 0;
    f = *d;
    *d = f;
    e();
  }
  return 0;
}


---


### compiler : `gcc`
### title : `[10 Regression]  orr + lsl vs. [us]bfiz`
### open_at : `2021-04-13T00:09:45Z`
### last_modified_date : `2023-07-07T09:34:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100056
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Created attachment 50573
or-shift vs. [us]bfiz

On arm64, the following code:
    unsigned or_shift(unsigned char i)
    {
        return i | (i << 11);
    }

translate to the following assembly:
    or_shift:
        and     w1, w0, 255
        ubfiz   w0, w0, 11, 8
        orr     w0, w0, w1
        ret

where the ubfiz instruction is a bit weird since the code
matches directly what was generated in gcc 8.x and before:
    or_shift:
        and     w0, w0, 255
        orr     w0, w0, w0, lsl 11
        ret

Same with a signed argument (see https://gcc.godbolt.org/z/af4zffMYa ).


---


### compiler : `gcc`
### title : `[10/11 Regression] csel vs. csetm + and`
### open_at : `2021-04-13T23:45:05Z`
### last_modified_date : `2021-04-14T00:54:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100072
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.3.0`
### severity : `normal`
### contents :
Created attachment 50587
testcase

The following code:
    int sel_andn(int p, int a) { return (p ? ~0 : 0) & a; }
    int sel_andr(int p, int a) { return (p ? 0 : ~0) & a; }

translated to the following with GCC9 and before:
    sel_andn:
        cmp     w0, 0
        csel    w0, w1, wzr, ne
        ret
    sel_andr:
        cmp     w0, 0
        csel    w0, w1, wzr, eq
        ret

but since version 10 it translates into:
    sel_andn:
        cmp     w0, 0
        csetm   w0, ne
        and     w0, w0, w1
        ret
    sel_andr:
        cmp     w0, 0
        csetm   w0, eq
        and     w0, w0, w1
        ret

Same at https://gcc.godbolt.org/z/16fj1EYhx


---


### compiler : `gcc`
### title : `[10 Regression] unneeded sign extension`
### open_at : `2021-04-14T00:09:11Z`
### last_modified_date : `2023-07-07T09:35:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100075
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.3.0`
### severity : `normal`
### contents :
Created attachment 50588
test case

Until gcc8, the following code:
    struct s {
	short x, y;
    };
    struct s rot(struct s p)
    {
	return (struct s) { -p.y, p.x };
    }

was translated:
    rot90:
        neg     w1, w0, asr 16
        and     w1, w1, 65535
        orr     w0, w1, w0, lsl 16
        ret

but since gcc9 it translates less nicely, with an unneeded sign extension:
    rot90:
        mov     w1, w0
        sbfx    x0, x1, 16, 16
        neg     w0, w0
        bfi     w0, w1, 16, 16
        ret


See with another variant in attachment or https://gcc.godbolt.org/z/1oW1cEMGc


---


### compiler : `gcc`
### title : `x86: by-value floating point array in struct - xmm regs spilling to stack`
### open_at : `2021-04-14T10:17:32Z`
### last_modified_date : `2021-08-16T01:03:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100077
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.3.0`
### severity : `normal`
### contents :
Hi,

compiling a vec3 cross product using struct by-value on msvc,
clang and gcc. gcc is going through memory on the stack.
operands are by-value so I can't use restrict. same with -O2 and -Os.
i vaguely remember seeing this a couple of times but i searched
to see if i had reported it and couldn't find a duplicate report.

link with the 3 compilers here: https://godbolt.org/z/YWWfYxbM3

MSVC:  /O2 /fp:fast /arch:AVX2
Clang: -Os -mavx -x c
GCC: -Os -mavx -x c

--- BEGIN EXAMPLE ---

struct vec3a { float v[3]; };
typedef struct vec3a vec3a;

vec3a vec3f_cross_0(vec3a v1, vec3a v2)
{
    vec3a dest = {
        v1.v[1]*v2.v[2]-v1.v[2]*v2.v[1],
        v1.v[2]*v2.v[0]-v1.v[0]*v2.v[2],
        v1.v[0]*v2.v[1]-v1.v[1]*v2.v[0]
    };
    return dest;
}

struct vec3f { float x, y, z; };
typedef struct vec3f vec3f;

vec3f vec3f_cross_1(vec3f v1, vec3f v2)
{
    vec3f dest = {
        v1.y*v2.z-v1.z*v2.y,
        v1.z*v2.x-v1.x*v2.z,
        v1.x*v2.y-v1.y*v2.x
    };
    return dest;
}

void vec3f_cross_2(float dest[3], float v1[3], float v2[3])
{
    dest[0]=v1[1]*v2[2]-v1[2]*v2[1];
    dest[1]=v1[2]*v2[0]-v1[0]*v2[2];
    dest[2]=v1[0]*v2[1]-v1[1]*v2[0];
}

--- END EXAMPLE ---


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O2)`
### open_at : `2021-04-14T14:14:54Z`
### last_modified_date : `2023-08-18T06:52:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100080
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[604] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210414 (experimental) [master revision 006783f4b16:29da9c11552:0589be0c59767cf4cbb0ef0e7d918cf6aa3d606c] (GCC) 
[605] % 
[605] % gcctk -O2 -S -o O2.s small.c
[606] % gcctk -O3 -S -o O3.s small.c
[607] % 
[607] % wc O2.s O3.s
  68  147  999 O2.s
  95  202 1373 O3.s
 163  349 2372 total
[608] % 
[608] % grep foo O2.s
[609] % grep foo O3.s
	call	foo
[610] % 
[610] % cat small.c
extern void foo(void);
int a, b, d, f;
static unsigned c;
void e(int g, int *k) {
  int h, *i = &b, *j = &b;
  *i = g;
  c = &b == i;
  d = c >= (*j = b | *k) && b & (*i == 0);
  h = a ? 0 : d;
  if (h)
    foo();
}
int main() {
  while (f) {
    int l;
    e(1, &l);
  }
  return 0;
}


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O2)`
### open_at : `2021-04-14T14:36:12Z`
### last_modified_date : `2023-08-18T03:54:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100082
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[528] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210414 (experimental) [master revision 006783f4b16:29da9c11552:0589be0c59767cf4cbb0ef0e7d918cf6aa3d606c] (GCC) 
[529] % 
[529] % gcctk -O2 -S -o O2.s small.c
[530] % gcctk -O3 -S -o O3.s small.c
[531] % 
[531] % wc O2.s O3.s
 101  229 1393 O2.s
 145  329 2038 O3.s
 246  558 3431 total
[532] % 
[532] % grep foo O2.s
[533] % grep foo O3.s
	call	foo
[534] % 
[534] % cat small.c
extern void foo(void);
volatile int a, b, h;
int *c, d[4], f, i, j;
long g;
static unsigned e() {
  int k;
  while (b) {
    for (k = 0; k < 4; k++) {
      d[k] && a;
      h = j ? 0 : i;
    }
    *c = 0;
  }
  return 0;
}
int main() {
  for (f = 0; f < 5; f++)
    g = 1;
  if (!e() ^ g)
    foo();
  return 0;
}


---


### compiler : `gcc`
### title : `Bad code for union transfer from __float128 to vector types`
### open_at : `2021-04-14T18:19:50Z`
### last_modified_date : `2022-03-08T16:20:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100085
### status : `REOPENED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `normal`
### contents :
Created attachment 50595
Reduced example of union and __float128 to vector transfer.

GCC 10/9/8/7 will generate poor (-mcpu=power8) code when using a union to transfer a __float128 scalar to any vector type. __float128 is a scalar type and not typecast compatible with any vector type. Despite both being in Vector registers. 

But for runtime codes implementing __float128 operations for -mcpu=power8 it is useful (and faster) to perform some (data_class, conversions, etc) operations directly in vector registers. The only solution for this is to use union to transfer values between __float128/vector types. This should be a simple vector register transfer and optimized as such.

But when for GCC and PowerPCle and -mcpu=power8, we are consistently seeing store/reload sequences. For Power8 this can cause load-hit-store and pipe-line rejects (33 cycles).

We don't see this when targeting -mcpu=power9, but power9 supports hardware Float128 instruction. Also we don't see this when targeting BE.


---


### compiler : `gcc`
### title : `Redundant extend with compare against zero for baseline Armv8-M`
### open_at : `2021-04-14T21:22:07Z`
### last_modified_date : `2021-04-14T21:22:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100087
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For the following:

_Bool eq0(int x)
{
    return x == 0;
}

with -march=armv8-m.base -O2 we generate:

eq0:
        rsbs    r3, r0, #0
        adcs    r0, r0, r3
        uxtb    r0, r0
        bx      lr

but the uxtb is redundant.


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O2)`
### open_at : `2021-04-15T11:07:13Z`
### last_modified_date : `2023-08-09T21:57:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100095
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[654] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210415 (experimental) [master revision 4dd9e1c541e:7315804b0a0:b5f644a98b3f3543d3a8d2dfea7785c22879013f] (GCC) 
[655] % 
[655] % gcctk -O2 -S -c -o O2.s small.c
[656] % gcctk -O3 -S -c -o O3.s small.c
[657] % 
[657] % wc O2.s O3.s
  75  164  982 O2.s
 121  269 1617 O3.s
 196  433 2599 total
[658] % 
[658] % grep foo O2.s
[659] % grep foo O3.s
        jmp     foo
[660] % 
[660] % cat small.c
extern void foo(void);
int a, b, c, d, g, h;
static int *e = &d;
volatile int f;
void i() {
  for (d = 5; d >= 0; d--)
    for (c = 0; c < 4; c++)
      while (1) {
        h = b ? a % b : 0;
        if (g)
          f;
        if (*e)
          break;
      }
  foo();
}


---


### compiler : `gcc`
### title : `std::transform is 1.5 times faster than std::copy with -O3`
### open_at : `2021-04-15T18:06:45Z`
### last_modified_date : `2021-04-18T17:30:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100104
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
Consider:

std::vector<double> v1(100, 0);

// copy using copy
std::vector<int> v2;
std::copy(v1.begin(), v1.end(), std::back_inserter(v2));

// copy using transform
std::vector<int> v2;
std::transform(v1.begin(), v1.end(), std::back_inserter(v2), [](auto x) { return x; });

Those two will generate similar assembly code under -O2, but is very different under -O3/-Ofast, and transform will be 1.5 times faster than the copy. I don’t know if this is a bug since these two represent the same thing, and correct me if I am wrong.

quick-bench with -O2:
https://quick-bench.com/q/uKT8QEmPkS1wr153s3P-DRt90eY
quick-bench with -O3:
https://quick-bench.com/q/syuBCQYVtCoVwT2MRtLT25P-MQI
goldbot: 
https://godbolt.org/z/7ee77cs8W


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3, -Os (vs. -O1, -O2)`
### open_at : `2021-04-16T07:22:35Z`
### last_modified_date : `2022-05-13T20:19:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100112
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[545] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210416 (experimental) [master revision 89c863488bc:10ed13839be:76c7e7d6b003a17d183d0571bf9b34c691819d25] (GCC) 
[546] % 
[546] % gcctk -O1 -S -o O1.s small.c
[547] % gcctk -O3 -S -o O3.s small.c
[548] % 
[548] % wc O1.s O3.s
 17  38 365 O1.s
 37  78 633 O3.s
 54 116 998 total
[549] % 
[549] % grep foo O1.s
[550] % grep foo O3.s
	call	foo
[551] % 
[551] % cat small.c
extern void foo(void);
static int e, *a = &e, b, *c = &b;
static int d(int f, int i) {
  if (f ^ i)
    foo();
}
int main() {
  int **g = &c;
  (*a)++;
  *g = c;
  d(1, c != 0);
  return 0;
}


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O1)`
### open_at : `2021-04-16T07:28:12Z`
### last_modified_date : `2023-08-18T02:26:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100113
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
[584] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210416 (experimental) [master revision 89c863488bc:10ed13839be:76c7e7d6b003a17d183d0571bf9b34c691819d25] (GCC) 
[585] % 
[585] % gcctk -O1 -S -o O1.s small.c
[586] % gcctk -O3 -S -o O3.s small.c
[587] % 
[587] % wc O1.s O3.s
  84  185 1140 O1.s
 120  254 1768 O3.s
 204  439 2908 total
[588] % 
[588] % grep foo O1.s
[589] % grep foo O3.s
	call	foo
[590] % 
[590] % cat small.c
extern void foo(void);
int a, b, d, *c = &a, e, k;
static int f(int *h, int i, int **l) {
  for (b = 0; b < 1; b++) {
    int *f = &d;
    for (; d < 1; d++)
      f = c;
    if (f != &a)
      __builtin_abort();
    if (k)
      *c = 0;
  }
  *c = 0;
  return **l;
}
int main() {
  int *j = &b;
  if (!f(j, e, &j))
    foo();
  return 0;
}


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O2)`
### open_at : `2021-04-20T19:20:23Z`
### last_modified_date : `2023-08-18T04:37:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100162
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
[720] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/11.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.1 20210420 (experimental) [master revision 67378cd63d6:5e36407d599:250f234988b6231669a720c52101d3686d645072] (GCC) 
[721] % 
[721] % gcctk -O2 -S -o O2.s small.c
[722] % gcctk -O3 -S -o O3.s small.c
[723] % 
[723] % wc O2.s O3.s
  52  119  923 O2.s
  70  150 1143 O3.s
 122  269 2066 total
[724] % 
[724] % grep foo O2.s
[725] % grep foo O3.s
        call    foo
[726] % 
[726] % cat small.c
extern void foo(void);
int printf(const char *, ...);
int a, b, c[5][1];
int main() {
  for (a = 0; a < 5; a++)
    c[a][b] = 2;
  if ((b || 0) / c[0][0])
    foo();
  printf("checksum=0");
  return 0;
}


---


### compiler : `gcc`
### title : `fmov could be used to zero out the upper bits instead of movi/zip or movi/ins with __builtin_shuffle and zero vector`
### open_at : `2021-04-21T00:47:06Z`
### last_modified_date : `2021-08-25T08:13:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100165
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Take:
typedef double V __attribute__((vector_size(16)));
typedef long long VI __attribute__((vector_size(16)));

V
foo (V x)
{
  return __builtin_shuffle (x, (V) { 0, 0,  }, (VI) {0, 3});
}

----- CUT ----
Or
typedef float V __attribute__((vector_size(16)));
typedef int VI __attribute__((vector_size(16)));

V
foo (V x)
{
  return __builtin_shuffle (x, (V) { 0, 0, 0, 0 }, (VI) {0, 1, 4, 5});
}
---- CUT ----
Both should just produce:
fmov d0, d0
ret
---- CUT ----
The x86_64 specific version of this was PR 94680 which I just confirmed today.


---


### compiler : `gcc`
### title : `autovectorizer`
### open_at : `2021-04-21T03:36:14Z`
### last_modified_date : `2021-08-17T05:40:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100171
### status : `UNCONFIRMED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Hello gcc team,
I once wrote a small test case to show the problems with the autovectorizer https://godbolt.org/z/xs35P45MM . In particular, the += operator is not vectorized. The + operator works in the same context. I do not understand that. If you decrement the arraysize in foo from 2 to 1 it doesn't work at all anymore - scalar operations are always generated for ARR_2x.
In general, I made the experience that the autovectorizer starts much too late. It should always do this from 2 values, even if these are much smaller than a simd register. This also saves a lot of memory accesses - especially when the data is linear in the memory (as in the example). Usually, however, vectorization is only carried out when the data is at least as large as a simd register, but often only when it is twice or even four times as large.
I think you should urgently update/optimize the autovectorizer.

thx & regards
Gero


---


### compiler : `gcc`
### title : `telecom/viterb00data_1 has 16.92% regression compared O2 -ftree-vectorize -fvect-cost-model=very-cheap to O2  on CLX/ICX, 9% regression on znver3`
### open_at : `2021-04-21T05:48:46Z`
### last_modified_date : `2021-05-06T14:50:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100173
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50647
ACS.cpp

cat testcase

void
__attribute__ ((noipa))
ACS(e_s16 *pBranchMetric)
{
  n_int i;
  e_s16 esMetricIn, esMetric1, esMetric2;

  StatePathMetricData *pIn1 = BufPtr[BufSelector];
  StatePathMetricData *pIn2 = pIn1 + (1<<5)/2;
  StatePathMetricData *pOut = BufPtr[1 - BufSelector];

  BufSelector ^= 1;

  for (i = 0; i < (1<<5)/2; i++) {

    esMetricIn = *pBranchMetric++;

    esMetric1 = pIn1->m_esPathMetric - esMetricIn;
    esMetric2 = pIn2->m_esPathMetric + esMetricIn;

    if (esMetric1 >= esMetric2) {
      pOut->m_esPathMetric = esMetric1;
      pOut->m_esState = (pIn1->m_esState << 1);
    }
    else {
      pOut->m_esPathMetric = esMetric2;
      pOut->m_esState = (pIn2->m_esState << 1);
    }
    pOut++;

    esMetric1 = pIn1->m_esPathMetric + esMetricIn;
    esMetric2 = pIn2->m_esPathMetric - esMetricIn;

    if (esMetric1 >=esMetric2) {
      pOut->m_esPathMetric =esMetric1;
      pOut->m_esState = (pIn1->m_esState << 1) | 1;
    }
    else {
      pOut->m_esPathMetric =esMetric2;
      pOut->m_esState = (pIn2->m_esState << 1) | 1;
    }
    pOut++;

    pIn1++;
    pIn2++;
  }
}

It is if conditional store replacement plays here, it sinks 2 stores from IF_BB and ELSE_BB to JOIN_BB since they have same address. But failed to vectorize them with -fvect-cost-model=very-cheap, and it causes worse IPC for consecutive stores in JOIN_BB on both ICX and znver3. With -fvect-cost-model=cheap, the loop can be vectorized and 2.6x faster than O2.

So I think we should either vectorize this loop or not sink conditional stores when cost model is very-cheap.

and the codes related are here: 

  /* If either vectorization or if-conversion is disabled then do
     not sink any stores.  */
  if (param_max_stores_to_sink == 0
      || (!flag_tree_loop_vectorize && !flag_tree_slp_vectorize)
      || !flag_tree_loop_if_convert)
    return false;


---


### compiler : `gcc`
### title : `g++ emits spurious Wstring-compare warnings on strcmp()`
### open_at : `2021-04-22T03:35:51Z`
### last_modified_date : `2022-07-28T12:28:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100197
### status : `NEW`
### tags : `alias, diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50653
preprocessed file from teststring.cpp

Tested with g++ (Ubuntu 10.20-13ubuntu1) 10.2.0 (as installed via "sudp apt-get install g++") on an Ubuntu 20.10 64-bit VM running inside VMWare Fusion on a 2018 Mac Mini.

To reproduce, compile the attached .cpp file, like this:

$ g++ -O2 -W -c teststring.cpp
In member function ‘bool String::operator==(const char*) const’,
    inlined from ‘int main(int, char**)’ at teststring.cpp:52:21:
teststring.cpp:27:21: warning: ‘int strcmp(const char*, const char*)’ of a string of length 10 and an array of size 8 evaluates to nonzero [-Wstring-compare]
   27 |       return (strcmp(myStr, rhs) == 0);
      |               ~~~~~~^~~~~~~~~~~~
teststring.cpp: In function ‘int main(int, char**)’:
teststring.cpp:52:16: note: in this expression
   52 |    if ((rand())&&(s == "1234567890")) printf("Mwahey!\n");
      |        ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~

Note that the emitted warnings are (AFAICT) spurious, because in the example program the two strings that get passed to strcmp() are both "1234567890".  The 8-byte array (_smallBuffer[8]) that the warning mentions is not actually used.

It seems like maybe the compiler is assuming that the IsArrayDynamicallyAllocated() method in the example will always return false, which is not a valid assumption.

The fault only happens when I specifed -O2 or -O3, it doesn't happen if I leave optimization disabled or specify -O1.


---


### compiler : `gcc`
### title : `Arm/Cortex-M: Suboptimal code returning unaligned struct with non-empty stack frame`
### open_at : `2021-04-22T19:17:50Z`
### last_modified_date : `2021-08-16T01:13:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100219
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Consider the program below, which deals with functions returning a struct of two members, either using a literal value or by forwarding the return value from another function. When the struct has no alignment, this results in suboptimal code that breaks the struct (stored in a single registrer) apart into its members and reassembles them into the struct into a single register again, where it could just have done absolutely nothing. Giving the struct some alignment somehow prevents this problem from occuring.

Consider this program:

    $ cat Foo.c
    struct Result { char a, b; }
    #if defined(ALIGN)
    __attribute((aligned(ALIGN)))__
    #endif
    ;

    struct Result other(const int*);

    struct Result func1() {
      int x;
      return other(&x);
    }

    struct Result func2() {
      struct Result y = {0x12, 0x34};
      return y;
    }

    struct Result func3() {
      return other(0);
    }

Which produces the following code:

    $ arm-linux-gnueabi-gcc-10 --version
    arm-linux-gnueabi-gcc-10 (Ubuntu 10.2.0-5ubuntu1~20.04) 10.2.0
    $ arm-linux-gnueabi-gcc-10 -fno-stack-protector -mcpu=cortex-m4 -c -O3 ~/Foo.c && objdump -d Foo.o

    00000000 <func1>:
       0:   b500            push    {lr}
       2:   b083            sub     sp, #12
       4:   a801            add     r0, sp, #4
       6:   f7ff fffe       bl      0 <other>
       a:   4603            mov     r3, r0
       c:   b2da            uxtb    r2, r3
       e:   2000            movs    r0, #0
      10:   f362 0007       bfi     r0, r2, #0, #8
      14:   f3c3 2307       ubfx    r3, r3, #8, #8
      18:   f363 200f       bfi     r0, r3, #8, #8
      1c:   b003            add     sp, #12
      1e:   f85d fb04       ldr.w   pc, [sp], #4
      22:   bf00            nop

    00000024 <func2>:
      24:   f243 4312       movw    r3, #13330      ; 0x3412
      28:   f003 0212       and.w   r2, r3, #18
      2c:   2000            movs    r0, #0
      2e:   f362 0007       bfi     r0, r2, #0, #8
      32:   0a1b            lsrs    r3, r3, #8
      34:   b082            sub     sp, #8
      36:   f363 200f       bfi     r0, r3, #8, #8
      3a:   b002            add     sp, #8
      3c:   4770            bx      lr
      3e:   bf00            nop

    00000040 <func3>:
      40:   b082            sub     sp, #8
      42:   2000            movs    r0, #0
      44:   b002            add     sp, #8
      46:   f7ff bffe       b.w     0 <other>
      4a:   bf00            nop


Especially note func2, which correctly builds the struct using a single word literal, and then continues to break it apart and rebuild it.

Note that I added -fno-stack-protector to make the generated code more consise, but the problem occurs even without this option.

Somehow, the alignment influences this, since adding some alignment makes the problem disappear:

    $ arm-linux-gnueabi-gcc-10 -fno-stack-protector -mcpu=cortex-m4 -c -O3 ~/Foo.c -DALIGN=2 && objdump -d Foo.o

    Foo.o:     file format elf32-littlearm


    Disassembly of section .text:

    00000000 <func1>:
       0:   b500            push    {lr}
       2:   b083            sub     sp, #12
       4:   a801            add     r0, sp, #4
       6:   f7ff fffe       bl      0 <other>
       a:   b003            add     sp, #12
       c:   f85d fb04       ldr.w   pc, [sp], #4

    00000010 <func2>:
      10:   f243 4012       movw    r0, #13330      ; 0x3412
      14:   4770            bx      lr
      16:   bf00            nop

    00000018 <func3>:
      18:   2000            movs    r0, #0
      1a:   f7ff bffe       b.w     0 <other>
      1e:   bf00            nop


Other things I've observed:
 - When using ALIGN=2 or ALIGN=4, the problem disappears as shown above. ALIGN=1 is equivalent to no alignment. Using ALIGN=8 also makes the problem disappear, but it seams this cause the return value to be passed in memory, rather than in r0 directly.
 - Using -mcpu=arm8, or arm7tdmi, or some other arm cpus I tried, the problem disappears. With all cortex variants I tried the problem stays, though sometimes it seems slightly less severe.
 - I could not reproduce this on x86_64.
 - Using a struct with just 1 char, the problem disappears.
 - Using a struct with 4 chars, the problem stays (and becomes more pronounced because there's more work to rebuild the struct).
 - Using a struct with 2 shorts, the problem disappears for func2, but stays for func1.
 - Writing something equivalent in C++, the problem also appears (I originally saw this problem in C++ and then tried reproducing in C).
 - When running with -Os, the problem disappears for func2 but stays for func1.

Also note that in almost all cases (except with ALIGN=4 and no stack variables), the stack frame size seems to be 8 bytes bigger then I'd expect, and in some cases there are some pointless add/sub instructions messing with the stack for no (apparent to me) reason.


---


### compiler : `gcc`
### title : `Takes two passes at DSE to remove some dead stores`
### open_at : `2021-04-22T20:07:53Z`
### last_modified_date : `2022-05-24T06:21:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100221
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
[551] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/12.0.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 12.0.0 20210422 (experimental) [master revision 3cf04d1afa8:0e51007a40c:d42088e453042f4f8ba9190a7e29efd937ea2181] (GCC) 
[552] % 
[552] % gcctk -O1 -S -o O1.s small.c
[553] % gcctk -O3 -S -o O3.s small.c
[554] % 
[554] % wc O1.s O3.s
  49  100  693 O1.s
  73  151 1072 O3.s
 122  251 1765 total
[555] % 
[555] % grep foo O1.s 
[556] % grep foo O3.s
	call	foo
[557] % 
[557] % cat small.c
extern void foo(void);
int a, b;
static int c;
static void f() {
  while (a)
    for (; b; b--)
      ;
}
void i() {
  if (c)
    foo();
  int *g = &c;
  {
    int **h[1] = {&g};
    f();
  }
}
int main() {
  i();
  return 0;
}


---


### compiler : `gcc`
### title : `Missing early return in std::partial_sort`
### open_at : `2021-04-23T03:16:56Z`
### last_modified_date : `2021-04-25T04:44:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100223
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.0`
### severity : `normal`
### contents :
Hi, can we add an early return to std::partial_sort to avoid unnecessary O(n) operations when __first is equal to __middle, just like std::rotate does, and make the result more consistent with ranges::partial_sort?

https://godbolt.org/z/Kb5x8zfeh


---


### compiler : `gcc`
### title : `x86-64 bad register allocation for unsigned type`
### open_at : `2021-04-24T16:38:51Z`
### last_modified_date : `2021-04-25T02:41:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100247
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
GCC 12.0.0 allocates unnecessary register for unsigned type. Below is the reproduction code:

Compile with: `gcc -Wall -Wextra -O3 -fno-tree-vectorize -fno-unroll-loops`

--------------------------------------
    #include <stddef.h>

    long add_arrays(long *arr1, long *arr2, size_t num) {
        size_t i = 0;
        long sum = 0;

        for (i = 0; i < num; ++i) {
            sum += arr1[i];
            sum += arr2[i];
        }

        return sum;
    }

    unsigned long unsigned_add_arrays(unsigned long *arr1, unsigned long *arr2, size_t num) {
        size_t i = 0;
        unsigned long sum = 0;

        for (i = 0; i < num; ++i) {
            sum += arr1[i];
            sum += arr2[i];
        }

        return sum;
    }

    #define PSTR(P) #P
    #define XSTR(P) PSTR(P)

    const char gcc_ver[] = 
    "It is GCC "
    XSTR(__GNUC__) "."
    XSTR(__GNUC_MINOR__) "."
    XSTR(__GNUC_PATCHLEVEL__);

--------------------------------------
GCC 12.0.0 Result

    add_arrays:
        xorl    %eax, %eax
        testq   %rdx, %rdx
        je      .L4
        xorl    %ecx, %ecx
    .L3:
        addq    (%rdi,%rcx,8), %rax
        addq    (%rsi,%rcx,8), %rax
        addq    $1, %rcx
        cmpq    %rcx, %rdx
        jne     .L3
        ret                         # Why do even we need this ret?
    .L4:
        ret

    unsigned_add_arrays:
        xorl    %r8d, %r8d          # Using %r8 is unnecessary
        testq   %rdx, %rdx
        je      .L7
        xorl    %eax, %eax
    .L9:
        movq    (%rsi,%rax,8), %rcx
        addq    (%rdi,%rax,8), %rcx
        addq    $1, %rax
        addq    %rcx, %r8           # %r8 is used as `sum` variable
        cmpq    %rax, %rdx
        jne     .L9
    .L7:
        movq    %r8, %rax           # set return value to %r8
        ret

    gcc_ver:
        .string "It is GCC 12.0.0"

--------------------------------------
GCC 4.6.4 produces better result here:

    add_arrays:
        xorl    %eax, %eax
        testq   %rdx, %rdx
        je      .L2
        xorl    %ecx, %ecx
    .L3:
        addq    (%rdi,%rcx,8), %rax
        addq    (%rsi,%rcx,8), %rax
        addq    $1, %rcx
        cmpq    %rdx, %rcx
        jne     .L3
    .L2:
        rep
        ret

    unsigned_add_arrays:
        xorl    %eax, %eax
        testq   %rdx, %rdx
        je      .L8
        xorl    %ecx, %ecx
    .L9:
        addq    (%rdi,%rcx,8), %rax
        addq    (%rsi,%rcx,8), %rax
        addq    $1, %rcx
        cmpq    %rdx, %rcx
        jne     .L9
    .L8:
        rep
        ret

    gcc_ver:
        .string "It is GCC 4.6.4"
--------------------------------------
Golbolt link: https://godbolt.org/z/9Pj5Ph1Gn


---


### compiler : `gcc`
### title : `poor codegen with vcvtph2ps / stride of 6`
### open_at : `2021-04-25T22:08:17Z`
### last_modified_date : `2021-04-26T12:40:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100257
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
gcc (Compiler-Explorer-Build) 12.0.0 20210424 (experimental)


https://godbolt.org/z/n6ooMdnz8


This C code:

```
#include <stdint.h>
#include <string.h>
#include <immintrin.h>

struct float3 {
    float f1;
    float f2;
    float f3;
};

struct util_format_r16g16b16_float {
   uint16_t r;
   uint16_t g;
   uint16_t b;
};

static inline struct float3 _mesa_half3_to_float3(uint16_t val_0, uint16_t val_1, uint16_t val_2) {
#if defined(__F16C__)
      //const __m128i in = {val_0, val_1, val_2};
      //__m128 out;
      //__asm volatile("vcvtph2ps %1, %0" : "=v"(out) : "v"(in));

      const __m128i in = _mm_setr_epi16(val_0, val_1, val_2, 0, 0, 0, 0, 0);
      const __m128 out = _mm_cvtph_ps(in);

      const struct float3 r = {out[0], out[1], out[2]};
      return r;
#endif
}


void
util_format_r16g16b16_float_unpack_rgba_float(void *restrict dst_row, const uint8_t *restrict src, unsigned width)
{
   float *dst = dst_row;
   for (unsigned x = 0; x < width; x += 1) {
        const struct util_format_r16g16b16_float pixel;
        memcpy(&pixel, src, sizeof pixel);

        struct float3 r = _mesa_half3_to_float3(pixel.r, pixel.g, pixel.b);
        dst[0] = r.f1; /* r */
        dst[1] = r.f2; /* g */
        dst[2] = r.f3; /* b */
        dst[3] = 1; /* a */

        src += 6;
        dst += 4;
   }
}

```

Is compiled "poorly" by gcc, even worse when compiled on i386 (with -mf16c enabled) when using -FPIE.

Example:


gcc -O3 -m32 -march=znver2 -mfpmath=sse -fPIE

util_format_r16g16b16_float_unpack_rgba_float:
        push    ebp
        push    edi
        push    esi
        push    ebx
        sub     esp, 28
        mov     ecx, DWORD PTR 56[esp]
        mov     edx, DWORD PTR 48[esp]
        call    __x86.get_pc_thunk.ax
        add     eax, OFFSET FLAT:_GLOBAL_OFFSET_TABLE_
        mov     ebx, DWORD PTR 52[esp]
        test    ecx, ecx
        je      .L8
        vmovss  xmm3, DWORD PTR .LC0@GOTOFF[eax]
        xor     esi, esi
        xor     ebp, ebp
        vpxor   xmm2, xmm2, xmm2
.L3:
        mov     eax, DWORD PTR [ebx]
        vmovss  DWORD PTR 12[edx], xmm3
        add     ebx, 6
        add     edx, 16
        inc     esi
        mov     ecx, eax
        vmovd   xmm0, eax
        shr     ecx, 16
        mov     edi, ecx
        movzx   ecx, WORD PTR -2[ebx]
        vpinsrw xmm0, xmm0, edi, 1
        vmovd   xmm1, ecx
        vpinsrw xmm1, xmm1, ebp, 1
        vpunpckldq      xmm0, xmm0, xmm1
        vpunpcklqdq     xmm0, xmm0, xmm2
        vcvtph2ps       xmm0, xmm0
        vmovss  DWORD PTR -16[edx], xmm0
        vextractps      DWORD PTR -12[edx], xmm0, 1
        vextractps      DWORD PTR -8[edx], xmm0, 2
        cmp     DWORD PTR 56[esp], esi
        jne     .L3
.L8:
        add     esp, 28
        pop     ebx
        pop     esi
        pop     edi
        pop     ebp
        ret
.LC0:
        .long   1065353216
__x86.get_pc_thunk.ax:
        mov     eax, DWORD PTR [esp]
        ret



clang:

util_format_r16g16b16_float_unpack_rgba_float: # @util_format_r16g16b16_float_unpack_rgba_float
        mov     eax, dword ptr [esp + 12]
        test    eax, eax
        je      .LBB0_3
        mov     ecx, dword ptr [esp + 8]
        mov     edx, dword ptr [esp + 4]
.LBB0_2:                                # =>This Inner Loop Header: Depth=1
        vmovd   xmm0, dword ptr [ecx]           # xmm0 = mem[0],zero,zero,zero
        vpinsrw xmm0, xmm0, word ptr [ecx + 4], 2
        add     ecx, 6
        vcvtph2ps       xmm0, xmm0
        vmovss  dword ptr [edx], xmm0
        vextractps      dword ptr [edx + 4], xmm0, 1
        vextractps      dword ptr [edx + 8], xmm0, 2
        mov     dword ptr [edx + 12], 1065353216
        add     edx, 16
        dec     eax
        jne     .LBB0_2
.LBB0_3:
        ret


clang code is essentially optimal.


The issue persist if I use `vcvtph2ps` directly via asm, or via intrinsics.

The issue might be the src stride, of 6, instead 8, that is confusing gcc.

Additionally, constant 1065353216  (which is weird, I would expect it to be 0), is stored in data section, instead inline as immediate, this makes code actually larger, and in PIE mode, requires extra pointer trickery, and on -m32, even calling extra function.

Even without -fPIE the main loop has poor codegen even on x86-64 / amd64 compared to clang or what I would considered good code.

gcc -m64 -O3 -march=native

util_format_r16g16b16_float_unpack_rgba_float:
        test    edx, edx
        je      .L8
        mov     edx, edx
        sal     rdx, 4
        vmovss  xmm3, DWORD PTR .LC0[rip]
        lea     rcx, [rdi+rdx]
        xor     r9d, r9d
        vpxor   xmm2, xmm2, xmm2
.L3:
        mov     eax, DWORD PTR [rsi]
        vmovss  DWORD PTR 12[rdi], xmm3
        mov     edx, eax
        shr     edx, 16
        mov     r8d, edx
        movzx   edx, WORD PTR 4[rsi]
        vmovd   xmm0, eax
        vmovd   xmm1, edx
        vpinsrw xmm0, xmm0, r8d, 1
        vpinsrw xmm1, xmm1, r9d, 1
        vpunpckldq      xmm0, xmm0, xmm1
        vpunpcklqdq     xmm0, xmm0, xmm2
        vcvtph2ps       xmm0, xmm0
        add     rdi, 16
        vmovlps QWORD PTR -16[rdi], xmm0
        vextractps      DWORD PTR -8[rdi], xmm0, 2
        add     rsi, 6
        cmp     rdi, rcx
        jne     .L3
.L8:
        ret
.LC0:
        .long   1065353216


If you know what is going on, please rename more accurately and reassign to proper component.


---


### compiler : `gcc`
### title : `constant store pulled out of the loop causes an extra memory load`
### open_at : `2021-04-25T23:41:26Z`
### last_modified_date : `2021-04-26T09:17:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100258
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Take:
void f(float *x, int t)
{
  for(int i = 0; i < t; i++)
    x[i*3] = 1.0;
}

Right now this produces for it at -O2:
        testl   %esi, %esi
        jle     .L5
        leal    -1(%rsi), %eax
        leaq    (%rax,%rax,2), %rax
        vmovss  .LC0(%rip), %xmm0
        leaq    12(%rdi,%rax,4), %rax
        .p2align 4,,10
        .p2align 3
.L3:
        vmovss  %xmm0, (%rdi)
        addq    $12, %rdi
        cmpq    %rax, %rdi
        jne     .L3
.L5:
        ret

----- CUT ----
If we don't have a loop, e.g. just a store to *x, we get:
        movl    $0x3f800000, (%rdi)
Which is 1000000x more effiecent and we just need a loop around that without doing the load of .LC0.


---


### compiler : `gcc`
### title : `DSE: join stores`
### open_at : `2021-04-26T03:38:53Z`
### last_modified_date : `2021-04-26T15:57:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100260
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
#include <string.h>

struct pam {
  void *p1;
  void *p2;
  #ifdef LONG
  unsigned long size;
  #else
  unsigned int pad;
  unsigned int size;
  #endif
};

extern int use(struct pam *param);

unsigned int foo(void) {
  struct pam s_pam;
  memset(&s_pam, 0, sizeof(struct pam));
  s_pam.size = 1;
  return use(&s_pam);
}

INT

foo():
  sub rsp, 40
  pxor xmm0, xmm0
  mov rdi, rsp
  mov DWORD PTR [rsp+16], 0
  mov DWORD PTR [rsp+20], 1
  movaps XMMWORD PTR [rsp], xmm0
  call use(pam*)
  add rsp, 40
  ret

LONG

foo():
  sub rsp, 40
  pxor xmm0, xmm0
  mov rdi, rsp
  movaps XMMWORD PTR [rsp], xmm0
  mov QWORD PTR [rsp+16], 1
  call use(pam*)
  add rsp, 40
  ret

Stores
  mov DWORD PTR [rsp+16], 0
  mov DWORD PTR [rsp+20], 1
can be replaced with one mov QWORD..


---


### compiler : `gcc`
### title : `REE does not work on PARALLEL expressions with a single register SET child`
### open_at : `2021-04-26T11:29:42Z`
### last_modified_date : `2021-09-17T06:44:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100264
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
REE iterates over all SET expressions and tries to eliminate redundant zero-extensions. In case of a PARALLEL expression, there is a check in place to ensure that "only one child of a PARALLEL expression is a SET".

Let's imagine a PARALLEL expression with two register SETs.
An example would be store-conditional, which sets a memory location (store target) as well as a register (return success value).
The current implementation is not able to optimize a zero-extension of the return value of store-conditional.

This can be solved, by moving the check for register targets (i.e. REG_P ()) into the function get_sub_rtx () and change the restriction of REE to "only one child of a PARALLEL expression is a SET register".


---


### compiler : `gcc`
### title : `gcc -O2 for avx512 instrincts generates extra warnings and less optimizations`
### open_at : `2021-04-26T12:48:02Z`
### last_modified_date : `2021-06-22T01:47:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100267
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `10.2.0`
### severity : `normal`
### contents :
The code snippet below compiles ok with '-O2' for gcc-9.
But with gcc-10 (and gcc-11) it generates -Wuninitialized warnings.
Another thing (which is probably worse) 'gcc-10 -O2' generates code with unnecessary loads for ymm registers from the initiliazed portion of the stack.
As I understand, thats where from these -Wuninitialized warnings come from:
by some reason gcc-10 wants to put local '__m256i pdatap[2]' variables
on the stack.
Note that only '-O2' affected, '-O3' looks good for all versions I tried (gcc-9, gcc-10, gcc-11)..

=====================
$ cat tavx512u5.c

#include <stddef.h>
#include <stdint.h>
#include <x86intrin.h>


struct flow_avx512 {
        uint32_t num_packets;
        uint32_t total_packets;
        const uint8_t **idata;
};

static inline void
start_flow_avx512x8(const struct flow_avx512 *flow, uint32_t num,
                    uint32_t msk, __m256i pdata[2])
{
        uint32_t n, m[2], nm[2];
        __m256i nd[2];

        m[0] = msk & 0xF;
        m[1] = msk >> 4;

        n = __builtin_popcount(m[0]);
        nm[0] = (1 << n) - 1;
        nm[1] = (1 << (num - n)) - 1;

        nd[0] = _mm256_maskz_loadu_epi64(nm[0],
                                flow->idata + flow->num_packets);
        nd[1] = _mm256_maskz_loadu_epi64(nm[1],
                        flow->idata + flow->num_packets + n);

        pdata[0] = _mm256_mask_expand_epi64(pdata[0], m[0], nd[0]);
        pdata[1] = _mm256_mask_expand_epi64(pdata[1], m[1], nd[1]);
}

__m256i
dummyf1_avx512x8(const struct flow_avx512 *flow)
{
        __m256i pdata[2];

        start_flow_avx512x8(flow, 8, 0xFF, pdata);
        return _mm256_add_epi64(pdata[0], pdata[1]);
}

====================
Good version (gcc-9) first:
gcc-9 -m64 -mavx512f -mavx512vl -mavx512cd -mavx512bw -Wall -O2 -o tavx512u5.gcc9-O2.o -c tavx512u5.c

$ objdump -d tavx512u5.gcc9-O2.o

tavx512u5.gcc9-O2.o:     file format elf64-x86-64

Disassembly of section .text:

0000000000000000 <dummyf1_avx512x8>:
   0:   f3 0f 1e fa             endbr64
   4:   8b 17                   mov    (%rdi),%edx
   6:   48 8b 47 08             mov    0x8(%rdi),%rax
   a:   b9 0f 00 00 00          mov    $0xf,%ecx
   f:   c5 f8 92 c9             kmovw  %ecx,%k1
  13:   62 f2 fd a9 89 0c d0    vpexpandq (%rax,%rdx,8),%ymm1{%k1}{z}
  1a:   62 f2 fd a9 89 44 d0    vpexpandq 0x20(%rax,%rdx,8),%ymm0{%k1}{z}
  21:   04
  22:   c5 f5 d4 c0             vpaddq %ymm0,%ymm1,%ymm0
  26:   c3                      retq

=======================
Now gcc-10:
$ gcc-10 -m64 -mavx512f -mavx512vl -mavx512cd -mavx512bw -Wall -O2 -o tavx512u5.gcc9-O2.o  -c tavx512u5.c
tavx512u5.c: In function ‘dummyf1_avx512x8’:
tavx512u5.c:32:13: warning: ‘pdata’ is used uninitialized in this function [-Wuninitialized]
   32 |  pdata[0] = _mm256_mask_expand_epi64(pdata[0], m[0], nd[0]);
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tavx512u5.c:33:13: warning: ‘*((void *)&pdata+32)’ is used uninitialized in this function [-Wuninitialized]
   33 |  pdata[1] = _mm256_mask_expand_epi64(pdata[1], m[1], nd[1]);
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

$ objdump -d tavx512u5.gcc10-O2.o 

tavx512u5.gcc10-O2.o:     file format elf64-x86-64

0000000000000000 <dummyf1_avx512x8>:
   0:   f3 0f 1e fa             endbr64
   4:   55                      push   %rbp
   5:   b9 0f 00 00 00          mov    $0xf,%ecx
   a:   c5 f8 92 c9             kmovw  %ecx,%k1
   e:   48 89 e5                mov    %rsp,%rbp
  11:   48 83 e4 e0             and    $0xffffffffffffffe0,%rsp
  15:   48 83 ec 60             sub    $0x60,%rsp
  19:   8b 17                   mov    (%rdi),%edx
  1b:   64 48 8b 04 25 28 00    mov    %fs:0x28,%rax
  22:   00 00
  24:   48 89 44 24 58          mov    %rax,0x58(%rsp)
  29:   31 c0                   xor    %eax,%eax
  2b:   48 8b 47 08             mov    0x8(%rdi),%rax
  2f:   c5 fd 6f 04 24          vmovdqa (%rsp),%ymm0  <=== load uninit data
  34:   c5 fd 6f 4c 24 20       vmovdqa 0x20(%rsp),%ymm1 <=== from stack
  3a:   62 f2 fd 29 89 04 d0    vpexpandq (%rax,%rdx,8),%ymm0{%k1}
  41:   62 f2 fd 29 89 4c d0    vpexpandq 0x20(%rax,%rdx,8),%ymm1{%k1}
  48:   04
  49:   c5 fd d4 c1             vpaddq %ymm1,%ymm0,%ymm0
  4d:   48 8b 44 24 58          mov    0x58(%rsp),%rax
  52:   64 48 2b 04 25 28 00    sub    %fs:0x28,%rax
  59:   00 00
  5b:   75 02                   jne    5f <dummyf1_avx512x8+0x5f>
  5d:   c9                      leaveq
  5e:   c3                      retq
  5f:   c5 f8 77                vzeroupper
  62:   e8 00 00 00 00          callq  67 <dummyf1_avx512x8+0x67>


================
Running gcc-10 with -fdump-tree-optimized shows similar picture 
(as I can uderstand it wants to put pdata[2] on the stack):
$ cat tavx512u5.gcc10-O2.optimized

;; Function dummyf1_avx512x8 (dummyf1_avx512x8, funcdef_no=5593, decl_uid=32966, cgraph_uid=5594, symbol_order=5593)

dummyf1_avx512x8 (const struct flow_avx512 * flow)
{
  __m256i pdata[2];
  vector(4) long long unsigned int _6;
  vector(4) long long unsigned int _8;
  vector(4) long long unsigned int _9;
  vector(4) long long int _10;
  const uint8_t * * _22;
  unsigned int _23;
  long unsigned int _24;
  long unsigned int _25;
  const uint8_t * * _26;
  vector(4) long long int _29;
  const uint8_t * * _30;
  unsigned int _31;
  sizetype _32;
  sizetype _34;
  sizetype _35;
  const uint8_t * * _36;
  vector(4) long long int _39;
  vector(4) long long int _41;
  vector(4) long long int _42;
  vector(4) long long int _45;
  vector(4) long long int _46;

  <bb 2> [local count: 1073741824]:
  _22 = flow_4(D)->idata;
  _23 = flow_4(D)->num_packets;
  _24 = (long unsigned int) _23;
  _25 = _24 * 8;
  _26 = _22 + _25;
  _29 = __builtin_ia32_loaddqudi256_mask (_26, { 0, 0, 0, 0 }, 15);
  _30 = flow_4(D)->idata;
  _31 = flow_4(D)->num_packets;
  _32 = (sizetype) _31;
  _34 = _32 + 4;
  _35 = _34 * 8;
  _36 = _30 + _35;
  _39 = __builtin_ia32_loaddqudi256_mask (_36, { 0, 0, 0, 0 }, 15);
  _41 = MEM[(__m256i * {ref-all})&pdata];
  _42 = __builtin_ia32_expanddi256_mask (_29, _41, 15);
  _45 = MEM[(__m256i * {ref-all})&pdata + 32B];
  _46 = __builtin_ia32_expanddi256_mask (_39, _45, 15);
  _6 = VIEW_CONVERT_EXPR<vector(4) long long unsigned int>(_42);
  _8 = VIEW_CONVERT_EXPR<vector(4) long long unsigned int>(_46);
  _9 = _6 + _8;
  _10 = VIEW_CONVERT_EXPR<__m256i>(_9);
  pdata ={v} {CLOBBER};
  return _10;

}

=========================
While gcc-9:

$ cat tavx512u5.gcc9-O2.optimized                                               
;; Function dummyf1_avx512x8 (dummyf1_avx512x8, funcdef_no=5525, decl_uid=32562, cgraph_uid=5526, symbol_order=5525)

dummyf1_avx512x8 (const struct flow_avx512 * flow)
{
  vector(4) long long int pdata$32;
  vector(4) long long int pdata;
  vector(4) long long unsigned int _3;
  vector(4) long long unsigned int _5;
  vector(4) long long unsigned int _6;
  vector(4) long long int _7;
  const uint8_t * * _9;
  unsigned int _10;
  long unsigned int _11;
  long unsigned int _12;
  const uint8_t * * _13;
  vector(4) long long int _14;
  const uint8_t * * _15;
  unsigned int _16;
  sizetype _17;
  sizetype _18;
  sizetype _19;
  const uint8_t * * _20;
  vector(4) long long int _21;
  vector(4) long long int _22;
  vector(4) long long int _23;

  <bb 2> [local count: 1073741824]:
  _9 = MEM[(const uint8_t * * const *)flow_2(D) + 8B];
  _10 = MEM[(const uint32_t *)flow_2(D)];
  _11 = (long unsigned int) _10;
  _12 = _11 * 8;
  _13 = _9 + _12;
  _14 = __builtin_ia32_loaddqudi256_mask (_13, { 0, 0, 0, 0 }, 15);
  _15 = MEM[(const uint8_t * * const *)flow_2(D) + 8B];
  _16 = MEM[(const uint32_t *)flow_2(D)];
  _17 = (sizetype) _16;
  _18 = _17 + 4;
  _19 = _18 * 8;
  _20 = _15 + _19;
  _21 = __builtin_ia32_loaddqudi256_mask (_20, { 0, 0, 0, 0 }, 15);
  _22 = __builtin_ia32_expanddi256_mask (_14, pdata_4(D), 15);
  _23 = __builtin_ia32_expanddi256_mask (_21, pdata$32_8(D), 15);
  _3 = VIEW_CONVERT_EXPR<vector(4) long long unsigned int>(_22);
  _5 = VIEW_CONVERT_EXPR<vector(4) long long unsigned int>(_23);
  _6 = _3 + _5;
  _7 = VIEW_CONVERT_EXPR<__m256i>(_6);
  return _7;

}


---


### compiler : `gcc`
### title : `sum of __int128 - regression since 8.2`
### open_at : `2021-04-28T01:45:17Z`
### last_modified_date : `2021-04-28T04:58:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100301
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
For such simple function:

__int128 add1(__int128 a, __int128 b) {
    return a + b;
}

gcc 8.2 generates for a + b:

        mov     r9, rdi
        mov     r10, rsi
        add     r9, rdx
        adc     r10, rcx
        mov     rax, r9
        mov     rdx, r10
        ret
and for b + a:

add1(__int128, __int128):
        mov     rax, rdx
        mov     rdx, rcx
        add     rax, rdi
        adc     rdx, rsi
        ret

but gcc 11.1 generates for both cases:
add1(__int128, __int128):
        mov     r9, rdi
        mov     rax, rdx
        mov     r8, rsi
        mov     rdx, rcx
        add     rax, r9
        adc     rdx, r8
        ret

4 moves instead of 2.

Recent versions of clang and icc generates the same number and type of instruction of both cases,
and only 2 moves and 2 additions:

icc:
add1(__int128, __int128):
        add       rdi, rdx
        mov       rax, rdi
        adc       rsi, rcx
        mov       rdx, rsi
        ret        

add1(__int128, __int128):
        add       rdx, rdi
        mov       rax, rdx
        adc       rcx, rsi
        mov       rdx, rcx
        ret                                       

clang:

add1(__int128, __int128):
        mov     rax, rdi
        add     rax, rdx
        adc     rsi, rcx
        mov     rdx, rsi
        ret

add1(__int128, __int128): 
        mov     rax, rdi
        add     rax, rdx
        adc     rsi, rcx
        mov     rdx, rsi
        ret

Not sure is this target specific issue,
I get this example from this article: https://habr.com/ru/post/554760/ ,
where risc-v gcc 8.2.0 has similar problem that "a + b" and "b + a" uses different number of instructions.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] 32-bit x86 memcpy is suboptimal`
### open_at : `2021-04-28T15:09:25Z`
### last_modified_date : `2023-07-07T10:39:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100320
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Bug 21329 has returned.

32-bit x86 memory block moves are using "movl $LEN,%ecx; rep movsl" insns.

However, for fixed short blocks it is more efficient to just repeat a few "movsl" insns - this allows to drop "mov $LEN,%ecx" insn.

It's shorter, and more importantly, "rep movsl" are slow-start microcoded insns (they are faster than moves using general-purpose registers only on blocks larger than 100-200 bytes) - OTOH, bare "movsl" are not microcoded and take ~4 cycles to execute.

21329 was closed with it fixed:

CVSROOT:	/cvs/gcc
Module name:	gcc
Branch: 	gcc-4_0-rhl-branch
Changes by:	jakub@gcc.gnu.org	2005-05-18 19:08:44
Modified files:
	gcc            : ChangeLog 
	gcc/config/i386: i386.c 
Log message:
	2005-05-06  Denis Vlasenko  <vda@port.imtp.ilyichevsk.odessa.ua>
	Jakub Jelinek  <jakub@redhat.com>	
	PR target/21329
	* config/i386/i386.c (ix86_expand_movmem): Don't use rep; movsb
	for -Os if (movsl;)*(movsw;)?(movsb;)? sequence is shorter.
	Don't use rep; movs{l,q} if the repetition count is really small,
	instead use a sequence of movs{l,q} instructions.

(the above is commit 95935e2db5c45bef5631f51538d1e10d8b5b7524 in gcc.gnu.org/git/gcc.git,
seems that code was largely replaced by:
commit 8c996513856f2769aee1730cb211050fef055fb5
Author: Jan Hubicka <jh@suse.cz>
Date:   Mon Nov 27 17:00:26 2006 +010
    expr.c (emit_block_move_via_libcall): Export.
)


With gcc version 11.0.0 20210210 (Red Hat 11.0.0-0) (GCC) I see "rep movsl"s again:

void *f(void *d, const void *s)
{ return memcpy(d, s, 16); }

$ gcc -Os -m32 -fomit-frame-pointer -c -o z.o z.c && objdump -drw z.o
z.o:     file format elf32-i386
Disassembly of section .text:
00000000 <f>:
   0:	57                   	push   %edi
   1:	b9 04 00 00 00       	mov    $0x4,%ecx
   6:	56                   	push   %esi
   7:	8b 44 24 0c          	mov    0xc(%esp),%eax
   b:	8b 74 24 10          	mov    0x10(%esp),%esi
   f:	89 c7                	mov    %eax,%edi
  11:	f3 a5                	rep movsl %ds:(%esi),%es:(%edi)
  13:	5e                   	pop    %esi
  14:	5f                   	pop    %edi
  15:	c3                   	ret 

The expected code would not have "mov $0x4,%ecx" and would have "rep movsl" replaced by "movsl;movsl;movsl;movsl".

The testcase from 21329 with implicit block moves via struct copies, from here
        https://gcc.gnu.org/bugzilla/attachment.cgi?id=8790
also demonstrates it:

$ gcc -Os -m32 -fomit-frame-pointer -c -o z1.o z1.c && objdump -drw z1.o
z1.o:     file format elf32-i386
Disassembly of section .text:
00000000 <f10>:
   0:	a1 00 00 00 00       	mov    0x0,%eax	1: R_386_32	w10
   5:	a3 00 00 00 00       	mov    %eax,0x0	6: R_386_32	t10
   a:	c3                   	ret    
0000000b <f20>:
   b:	a1 00 00 00 00       	mov    0x0,%eax	c: R_386_32	w20
  10:	8b 15 04 00 00 00    	mov    0x4,%edx	12: R_386_32	w20
  16:	a3 00 00 00 00       	mov    %eax,0x0	17: R_386_32	t20
  1b:	89 15 04 00 00 00    	mov    %edx,0x4	1d: R_386_32	t20
  21:	c3                   	ret    
00000022 <f21>:
  22:	57                   	push   %edi
  23:	b9 09 00 00 00       	mov    $0x9,%ecx
  28:	bf 00 00 00 00       	mov    $0x0,%edi	29: R_386_32	t21
  2d:	56                   	push   %esi
  2e:	be 00 00 00 00       	mov    $0x0,%esi	2f: R_386_32	w21
  33:	f3 a4                	rep movsb %ds:(%esi),%es:(%edi)
  35:	5e                   	pop    %esi
  36:	5f                   	pop    %edi
  37:	c3                   	ret    
00000038 <f22>:
  38:	57                   	push   %edi
  39:	b9 0a 00 00 00       	mov    $0xa,%ecx
  3e:	bf 00 00 00 00       	mov    $0x0,%edi	3f: R_386_32	t22
  43:	56                   	push   %esi
  44:	be 00 00 00 00       	mov    $0x0,%esi	45: R_386_32	w22
  49:	f3 a4                	rep movsb %ds:(%esi),%es:(%edi)
  4b:	5e                   	pop    %esi
  4c:	5f                   	pop    %edi
  4d:	c3                   	ret    
0000004e <f23>:
  4e:	57                   	push   %edi
  4f:	b9 0b 00 00 00       	mov    $0xb,%ecx
  54:	bf 00 00 00 00       	mov    $0x0,%edi	55: R_386_32	t23
  59:	56                   	push   %esi
  5a:	be 00 00 00 00       	mov    $0x0,%esi	5b: R_386_32	w23
  5f:	f3 a4                	rep movsb %ds:(%esi),%es:(%edi)
  61:	5e                   	pop    %esi
  62:	5f                   	pop    %edi
  63:	c3                   	ret    
00000064 <f30>:
  64:	57                   	push   %edi
  65:	b9 03 00 00 00       	mov    $0x3,%ecx
  6a:	bf 00 00 00 00       	mov    $0x0,%edi	6b: R_386_32	t30
  6f:	56                   	push   %esi
  70:	be 00 00 00 00       	mov    $0x0,%esi	71: R_386_32	w30
  75:	f3 a5                	rep movsl %ds:(%esi),%es:(%edi)
  77:	5e                   	pop    %esi
  78:	5f                   	pop    %edi
  79:	c3                   	ret    
0000007a <f40>:
  7a:	57                   	push   %edi
  7b:	b9 04 00 00 00       	mov    $0x4,%ecx
  80:	bf 00 00 00 00       	mov    $0x0,%edi	81: R_386_32	t40
  85:	56                   	push   %esi
  86:	be 00 00 00 00       	mov    $0x0,%esi	87: R_386_32	w40
  8b:	f3 a5                	rep movsl %ds:(%esi),%es:(%edi)
  8d:	5e                   	pop    %esi
  8e:	5f                   	pop    %edi
  8f:	c3                   	ret    
00000090 <f50>:
  90:	57                   	push   %edi
  91:	b9 05 00 00 00       	mov    $0x5,%ecx
  96:	bf 00 00 00 00       	mov    $0x0,%edi	97: R_386_32	t50
  9b:	56                   	push   %esi
  9c:	be 00 00 00 00       	mov    $0x0,%esi	9d: R_386_32	w50
  a1:	f3 a5                	rep movsl %ds:(%esi),%es:(%edi)
  a3:	5e                   	pop    %esi
  a4:	5f                   	pop    %edi
  a5:	c3                   	ret    
000000a6 <f60>:
  a6:	57                   	push   %edi
  a7:	b9 06 00 00 00       	mov    $0x6,%ecx
  ac:	bf 00 00 00 00       	mov    $0x0,%edi	ad: R_386_32	t60
  b1:	56                   	push   %esi
  b2:	be 00 00 00 00       	mov    $0x0,%esi	b3: R_386_32	w60
  b7:	f3 a5                	rep movsl %ds:(%esi),%es:(%edi)
  b9:	5e                   	pop    %esi
  ba:	5f                   	pop    %edi
  bb:	c3                   	ret    
000000bc <f>:
...


---


### compiler : `gcc`
### title : `Switching from std=c++17 to std=c++20 causes performance regression in relationals`
### open_at : `2021-04-28T17:15:54Z`
### last_modified_date : `2021-04-29T08:55:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100322
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `normal`
### contents :
Experiment here: https://godbolt.org/z/PT73cn5e5

#include <chrono>

using clk = std::chrono::steady_clock;
bool compare_count(clk::duration a, clk::duration b) {
    return a.count() > b.count();
}
bool compare(clk::duration a, clk::duration b) {
    return a > b;
}

Compiling with -std=c++17 I get:

_Z13compare_countNSt6chrono8durationIxSt5ratioILx1ELx1000000000EEEES3_:
        cmp     r2, r0
        sbcs    r3, r3, r1
        ite     lt
        movlt   r0, #1
        movge   r0, #0
        bx      lr
_Z7compareNSt6chrono8durationIxSt5ratioILx1ELx1000000000EEEES3_:
        cmp     r2, r0
        sbcs    r3, r3, r1
        ite     lt
        movlt   r0, #1
        movge   r0, #0
        bx      lr

Compiling with -std=c++20 I get:

_Z13compare_countNSt6chrono8durationIxSt5ratioILx1ELx1000000000EEEES3_:
        cmp     r2, r0
        sbcs    r3, r3, r1
        ite     lt
        movlt   r0, #1
        movge   r0, #0
        bx      lr
_Z7compareNSt6chrono8durationIxSt5ratioILx1ELx1000000000EEEES3_:
        cmp     r1, r3
        it      eq
        cmpeq   r0, r2
        beq     .L4
        cmp     r0, r2
        sbcs    r3, r1, r3
        bge     .L5
        mov     r0, #-1
.L3:
        cmp     r0, #0
        ite     le
        movle   r0, #0
        movgt   r0, #1
        bx      lr
.L4:
        movs    r0, #0
        b       .L3
.L5:
        movs    r0, #1
        b       .L3

(Note that clang doesn't have this problem)


---


### compiler : `gcc`
### title : `#pragma and attribute optimize don't enable inlining`
### open_at : `2021-04-28T17:54:17Z`
### last_modified_date : `2021-12-15T11:08:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100323
### status : `NEW`
### tags : `missed-optimization, opt-attribute`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
I expected the effect of the attribute and the #pragma on the calls to f() in both g() and h() to be to inline them.  They aren't.  With #pragma clang optimize on, Clang inlines the call in h().

If this should be the expected behavior in GCC it should be documented.


$ cat a.c && gcc -O0 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
static int f (void) { return 0; }

__attribute__ ((optimize (2)))
int g (void) { return f (); }

#pragma GCC optimize ("2")
int h (void) { return f (); }

;; Function f (f, funcdef_no=0, decl_uid=1943, cgraph_uid=1, symbol_order=0)

int f ()
{
  int D.1952;
  int _1;

  <bb 2> :
  _1 = 0;

  <bb 3> :
<L0>:
  return _1;

}



;; Function g (g, funcdef_no=1, decl_uid=1946, cgraph_uid=2, symbol_order=1)

__attribute__((optimize (2)))
int g ()
{
  int _3;

  <bb 2> [local count: 1073741824]:
  _3 = f (); [tail call]
  return _3;

}



;; Function h (h, funcdef_no=2, decl_uid=1949, cgraph_uid=3, symbol_order=2)

__attribute__((optimize ("2")))
int h ()
{
  int _3;

  <bb 2> [local count: 1073741824]:
  _3 = f (); [tail call]
  return _3;

}


---


### compiler : `gcc`
### title : `128 bit arithmetic --- suboptimal after shifting when referencing other variables`
### open_at : `2021-04-29T10:27:51Z`
### last_modified_date : `2021-04-29T17:49:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100331
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Created attachment 50706
Reproduction test case

Compile the given C program with -O2.  Enabling the #if 1 branch results in:

compute(unsigned long, unsigned long):
        mov     ecx, edi
        xor     edx, edx
        mov     rax, rsi
        xor     esi, esi
        and     ecx, 63
        shrd    rax, rdx, cl
        shr     rdx, cl
        test    cl, 64
        mov     r8d, ecx
        cmovne  rax, rdx
        cmovne  rdx, rsi
        and     r8d, 63
        mov     rsi, rax
        mov     rax, r8
        mov     rdi, rdx
        xor     edx, edx
        add     rax, rsi
        adc     rdx, rdi
        ret

Note test cl, 64 and the subsequent cmovs are unnecessary because the result of the test is already known after and ecx, 63.  Note also mov r8d, ecx, followed by and r8d, 63, redoing work.

Enabling the #if 0 branch results in this code instead.

compute(unsigned long, unsigned long):
        mov     rcx, rdi
        xor     edx, edx
        mov     rax, rsi
        shrd    rax, rdx, cl
        shr     rdx, cl
        ret

That is, now gcc realizes the range of possible values for cl and does not emit the test, the cmovs, and the redoing of the and with r8d.  One way or another, the double precision shift is also unnecessary because only the lower 64 bits of result may be non-zero.

Verified on Ubuntu 20.04 LTS, as well as Godbolt with gcc 9.3.0, gcc 11.1, and gcc trunk.  This issue is similar to other 128 bit arithmetic reported bugs, but unlike those others this one seems to be controlled exclusively by the addition in the #if 1 branch.

For the sake of comparison, clang trunk emits the following code for the #if 1 and #if 0 branches, as per Godbolt.

compute(unsigned long, unsigned long):                           # @compute(unsigned long, unsigned long)
        mov     rcx, rdi
        mov     eax, ecx
        shr     rsi, cl
        and     eax, 63
        xor     edx, edx
        add     rax, rsi
        setb    dl
        ret


compute(unsigned long, unsigned long):                           # @compute(unsigned long, unsigned long)
        mov     rax, rsi
        mov     rcx, rdi
        shr     rax, cl
        xor     edx, edx
        ret


---


### compiler : `gcc`
### title : `RISC-V extra pointer adjustments for memcpy() from glibc`
### open_at : `2021-04-29T23:40:25Z`
### last_modified_date : `2022-11-23T18:05:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100348
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50714
Source C code for trimmed example

The wordcopy_fwd_aligned() portion of glibc contains generated code like the following assembly dump:

.L16:
  ld  a6,16(a1)
  addi  a3,a5,24
  sd  a4,0(a0)
.L15:
  ld  a4,24(a1)
  addi  a0,a5,32
  sd  a6,0(a3)
.L14:
  ld  a6,32(a1)
  addi  a3,a5,40
  sd  a4,0(a0)
.L13:
  ld  a4,40(a1)
  addi  a0,a5,48
  sd  a6,0(a3)
.L12:
  ld  a6,48(a1)
  addi  a3,a5,56
  sd  a4,0(a0)

For some reason, gcc is able to do the proper immediate-offset thinking so that the loads use different offsets and a fixed base register (until we get to the bottom of the loop, and bump the a1 pointer), but for the stores, it bumps the pointer each time. This adds 50% more instructions to the innermost loop of memcpy() arguably a bit of code we should try to have optimal code for.

I've simplified the code here, to make it about as small as it can be and still demonstrate the issue.

Here is the generated assembly, with my commentary to the right in comments:

        .file   "simple.c"
        .option nopic
        .attribute arch, "rv64i2p0_m2p0_a2p0_f2p0_d2p0_c2p0"
        .attribute unaligned_access, 0
        .attribute stack_align, 16
        .text
        .align  1
        .globl  not_really_wordcopy_fwd_aligned
        .type   not_really_wordcopy_fwd_aligned, @function
not_really_wordcopy_fwd_aligned:
        andi    a5,a2,3
        li      a4,2
        beq     a5,a4,.L2
        li      a4,3
        beq     a5,a4,.L3
        li      a4,1
        beq     a5,a4,.L20
        bne     a2,zero,.L21
        ret
.L20:
        addi    a2,a2,-1
        ld      a3,0(a1)
        bne     a2,zero,.L22
.L10:
        sd      a3,0(a0)
.L23:
        ret
.L3:
        ld      a3,0(a1)
        addi    a4,a1,-40
        addi    a5,a0,-48
        addi    a2,a2,5
        addi    a1,a1,-8
        addi    a7,a0,-16
        j       .L7
.L11:
        sd      a3,32(a5) // Uses an immediate offset off of a5. Good.
        ld      a3,32(a4)
        addi    a7,a5,32
        addi    a1,a4,32
.L9:
        sd      a3,40(a5) // Uses an immediate offset off of a5. Good again.
        ld      a3,40(a4)
        addi    a0,a5,48  // Uh-oh, bumping a pointer to be a5+48.
.L7:
        sd      a3,0(a0)  // Uh-oh, using the pointer. Why not 48(a5)?
        ld      a6,48(a4)
        addi    a0,a5,56
        mv      a3,a4
        mv      a5,a7
        mv      a4,a1
.L6:
        sd      a6,0(a0)
        addi    a2,a2,-4
        ld      a3,56(a3)
        mv      a0,a5
        bne     a2,zero,.L11
        sd      a3,0(a0)
        j       .L23
.L2:
        ld      a6,0(a1)
        addi    a3,a1,-48
        addi    a2,a2,6
        addi    a4,a1,-16
        addi    a5,a0,-24
        j       .L6
.L22:
        addi    a4,a1,8
        mv      a5,a0
        j       .L11
.L21:
        ld      a3,0(a1)
        mv      a4,a1
        addi    a5,a0,-8
        addi    a1,a1,32
        addi    a7,a0,24
        j       .L9
        .size   not_really_wordcopy_fwd_aligned, .-not_really_wordcopy_fwd_aligned
        .ident  "GCC: (SiFive GCC-Metal 10.2.0-2020.12.9) 10.2.0"

I realize that the different paths to L7 make this a bit confusing, but it seems that since the only path to L7 is through the code at L3 or through the fallthrough path where it is currently adjusting the pointer, the proper adjustments could be made before, outside the loop, instead of each time.

In the full memcpy() source code, this behavior occurs multiple times, roughly for every store, as shown at the top of this bug report.


---


### compiler : `gcc`
### title : `missed optimization for dead code elimination at -O3 (vs. -O2)`
### open_at : `2021-04-30T16:03:58Z`
### last_modified_date : `2022-01-10T10:09:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100359
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
This seems to be a very recent regression as it does not reproduce for GCC 11.1 (and earlier).  

[524] % gcctk -v
Using built-in specs.
COLLECT_GCC=gcctk
COLLECT_LTO_WRAPPER=/local/suz-local/software/local/gcc-trunk/libexec/gcc/x86_64-pc-linux-gnu/12.0.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk/configure --disable-bootstrap --prefix=/local/suz-local/software/local/gcc-trunk --enable-languages=c,c++ --disable-werror --enable-multilib --with-system-zlib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 12.0.0 20210430 (experimental) [master revision 4cf3b10f27b:939d67a66c9:c111f6066043d3b7bc4141ca0411eae9294aa6c5] (GCC) 
[525] % 
[525] % gcctk -O2 -S -o O2.s small.c
[526] % gcctk -O3 -S -o O3.s small.c
[527] % 
[527] % wc O2.s O3.s
  37   83  635 O2.s
  49  109  808 O3.s
  86  192 1443 total
[528] % 
[528] % grep foo O2.s
[529] % grep foo O3.s
	call	foo
[530] % 
[530] % cat small.c
extern void foo(void);
static int b, f, *a = &b;
int **c = &a;
static void d() {
  int g, h;
  for (f = 0; f < 1; f++) {
    int *i = &b;
    {
      int *j[3], **k = &a;
      for (g = 0; g < 3; g++)
        for (h = 0; h < 1; h++)
          j[g] = &b;
      *k = j[0];
    }
    *c = i;
  }
}
int main() {
  d();
  *a = 0;
  if (**c)
    foo();
  return 0;
}


---


### compiler : `gcc`
### title : `[11/12 Regression] spurious warning - std::vector::clear followed by std::vector::insert(vec.end(), ...) with -O2`
### open_at : `2021-05-01T02:36:53Z`
### last_modified_date : `2023-05-29T10:04:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100366
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
I'm seeing this with both gcc 11.1 on: https://godbolt.org/z/1Wv9jj3r1

and gcc (GCC) 11.0.1 20210324 (Red Hat 11.0.1-0), below:

Compiling the following with -O2 -Wall -Werror:

#include <vector>
#include <memory>

static char UTC[4];

void func(std::vector<char> &vec)
{
	vec.clear();
	vec.insert(vec.end(), UTC, UTC+4);
}

This generates a massive complaint that boils down to:

/usr/include/c++/11/bits/stl_algobase.h:431:30: error: ‘void* __builtin_memcpy(v
oid*, const void*, long unsigned int)’ writing 1 or more bytes into a region of 
size 0 overflows the destination [-Werror=stringop-overflow=]
  431 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /usr/include/c++/11/x86_64-redhat-linux/bits/c++allocator.

...skipping 1 line
                 from /usr/include/c++/11/bits/allocator.h:46,
                 from /usr/include/c++/11/vector:64,
                 from t.C:1:

Removing vec.clear() makes this go away.

Adding vec.reserve(4) after vec.clear() makes this go away.


---


### compiler : `gcc`
### title : `needless stack adjustment when passing struct in register`
### open_at : `2021-05-01T19:20:51Z`
### last_modified_date : `2021-08-16T00:57:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100377
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Created attachment 50726
testcases

When compiling with optimization for example -O2), the following code:

    struct sb {
	signed char a;
	char b;
	short y[3];
    };
    struct ub {
	unsigned char a;
	char b;
	short y[3];
    };
    int fsb(struct sb s) { return s.a; }
    int fub(struct ub s) { return s.a; }

produces the following assembly code on arm64:
    fsb:
        sub     sp, sp, #16
        sxtb    w0, w0
        add     sp, sp, 16
        ret
    fub:
        sub     sp, sp, #16
        and     w0, w0, 255
        add     sp, sp, 16
        ret

the following on mips64:
    fsb:
        daddiu  $sp,$sp,-16
        dsll    $2,$4,56
        dsra    $2,$2,56
        j       $31
        daddiu  $sp,$sp,16

    fub:
        daddiu  $sp,$sp,-16
        andi    $2,$4,0xff
        j       $31
        daddiu  $sp,$sp,16

the following on riscv64:
    fsb:
        addi    sp,sp,-16
        slli    a0,a0,24
        srai    a0,a0,24
        addi    sp,sp,16
        jr      ra
    fub:
        addi    sp,sp,-16
        andi    a0,a0,0xff
        addi    sp,sp,16
        jr      ra

OTOH, things seems OK on ppc64:
    fsb:
        extsb 3,3
        blr
    fub:
        rlwinm 3,3,0,0xff
        blr

and x86_64:
    fsb:
        movsx   eax, dil
        ret
    fub:
        movzx   eax, dil
        ret


Similar problems happen on 32-bit platforms too.
For example on arm32, the following code:
    struct ub32 {
	unsigned char a;
	char b;
	short y[1];
    };
    int fub32(struct ub32 s) { return s.a; }

produces:
    fub32:
        sub     sp, sp, #8
        uxtb    r0, r0
        add     sp, sp, #8
        bx      lr


All these seem to happen on all versions.
See https://gcc.godbolt.org/z/x9zc1EnYn

Note: similar PRs exist but reported for x86_64 only


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] arm64: lsl + asr used instead of sxth`
### open_at : `2021-05-01T19:30:34Z`
### last_modified_date : `2023-07-13T21:15:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100378
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 50727
testcase

On arm64, when compiling with optimization, for example with -O2,
the following code:

    struct sh {
	short a;
	short b;
	short y[2];
    };
    int fooh(struct sh s) { return s.a; }

produces the following assembly code since GCC9.x:
    fooh:
        lsl     x0, x0, 16
        asr     w0, w0, 16
        ret

but with GCC8.x and before it produces the shorter:
    fooh(sh):
        sxth    w0, w0
        ret


See https://gcc.godbolt.org/z/YrW7E3cro


---


### compiler : `gcc`
### title : `128 bit arithmetic --- many unnecessary instructions when extracting smaller parts`
### open_at : `2021-05-03T08:43:55Z`
### last_modified_date : `2021-05-05T18:59:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100391
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `enhancement`
### contents :
Created attachment 50738
Sample code

Consider the attached code, compiled with -O2.  The return value of both functions is just the low 32 bits of num.  Whether the top 4 bits of kt were zero, or became zero because of the shifts in the if statement, is irrelevant.  So, this both functions should have resulted in something like

twostep(unsigned __int128):                            # @twostep(unsigned __int128)
        mov     rax, rdi
        ret
onestep(unsigned __int128):                            # @onestep(unsigned __int128)
        mov     rax, rdi
        ret


Instead, gcc added many unnecessary instructions to twostep() as shown below.

twostep(unsigned __int128):
        mov     rcx, rdi
        mov     rax, rdi
        shr     rcx, 60
        je      .L2
        movabs  rdx, 1152921504606846975
        and     rax, rdx
.L2:
        ret
onestep(unsigned __int128):
        mov     rax, rdi
        ret


This particular behavior was isolated while examining the output of gcc 9.3.0 on Ubuntu 20.04 LTS, then verified for the stated versions (and a few others) using Godbolt.

Incidentally, it might be worth checking whether movabs + and is indeed faster than shl + shr, assuming doing so was necessary.  If too many movabs instructions are generated for bit masking like this, it will run against the Intel optimization manual's recommendation not to include too many full size literals in code.


---


### compiler : `gcc`
### title : `[nvptx][OpenMP] Enable SIMT for user-defined reduction`
### open_at : `2021-05-04T06:21:10Z`
### last_modified_date : `2022-03-02T12:54:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100408
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Follow-up to PR100321 - which is about the same issue but solved it by disabling SIMT

From the patch:

    The test-case included in this patch contains this target region:
    ...
      for (int i0 = 0 ; i0 < N0 ; i0++ )
        counter_N0.i += 1;
    ...
    
    When running with nvptx accelerator, the counter variable is expected to
    be N0 after the region, but instead is N0 / 32.  The problem is that rather
    than getting the result for all warp lanes, we get it for just one lane.
    
    This is caused by the implementation of SIMT being incomplete.  It handles
    regular reductions, but appearantly not user-defined reductions.
    
    For now, handle this by disabling SIMT in this case, specifically by setting
    sctx->max_vf to 1.

For details (code location, longer testcases, etc.) see PR100321


---


### compiler : `gcc`
### title : `gcc arm compilation binary output is bigger with -Os than -O2`
### open_at : `2021-05-05T12:18:32Z`
### last_modified_date : `2022-01-05T10:37:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100432
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.1.1`
### severity : `normal`
### contents :
example: https://godbolt.org/z/rYxTPdTcd

while compiling for ARM, gcc versions after 4 (>= 5) generate a redundant stack frame as can be seen in the attached example, while gcc version 4 generates correct code that is also minimal in size as requested (-Os).

The code generation in newer gcc versions is strange even without thinking about the -Os flag, since the generated stack frame is not even used, and the whole 3 instructions (sub sp, str, add sp) are totally not needed (the variable on the stack is not used).
also reported in https://bugs.launchpad.net/gcc-arm-embedded/+bug/1927055 (not sure where this should be reported)


---


### compiler : `gcc`
### title : `[meta-bug] Bugs relating to the enabling of vectorization at -O2 in GCC 12+`
### open_at : `2021-05-06T14:49:34Z`
### last_modified_date : `2023-08-08T22:02:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100457
### status : `NEW`
### tags : `meta-bug, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
This metabug is used to track all the O2 vectorization issues.

We're trying to enable vectorization in O2 for GCC12. To do that, we need to make  there's no regression comparing -O2 -ftree-vectorize -fvect-cost-model=very-cheap to plain -O2, or those regressions are acceptable.


---


### compiler : `gcc`
### title : `Fail to remove dead code in loop`
### open_at : `2021-05-11T03:12:25Z`
### last_modified_date : `2021-05-11T07:53:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100511
### status : `UNCONFIRMED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
For this simple case, gcc doesn't know the if condition (i > c2) is always false.

#include <stdio.h>

typedef struct {
    int count;
} XX;

int g;

__attribute__((noinline)) void f(XX *x)
{
    int c1 = 0;
    if (x)
        c1 = x->count;
    for (int i=0; i<c1; i++) {
        int c2 = x->count;
        if (i > c2) {
            printf("Unreachable!");
            break;
        }
        else
            g = i;
    }
}

void main(void)
{
    XX x;
    x.count = 100;
    f(&x);
}

If we change variable the type of variable g to float, gcc does optimize away this if condition inside the loop, so why alias analysis can't recognize g is different from x->count?


---


### compiler : `gcc`
### title : `[11 Regression] Unexpected -Wstringop-overread in deque<char> initialization from empty initializer_list`
### open_at : `2021-05-11T09:26:29Z`
### last_modified_date : `2022-04-12T21:14:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100516
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `libstdc++`
### version : `11.1.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `[coroutines] Poor codegen using an optional-like type with co_await`
### open_at : `2021-05-13T03:24:45Z`
### last_modified_date : `2022-03-11T00:30:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100579
### status : `NEW`
### tags : `C++-coroutines, missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `enhancement`
### contents :
#ifdef __clang__
#include <experimental/coroutine>
namespace stdx = std::experimental;
#else
#include <coroutine>
namespace stdx = std;
#endif
struct O {
 ~O() {}
 struct promise_type {
     O get_return_object() { return O{this};}
     stdx::suspend_never initial_suspend() { return {}; }
     stdx::suspend_never final_suspend() noexcept { return {}; }
     void return_void(){ value->b = true; }
     void unhandled_exception(){ throw; }

     auto await_transform(O o) {
         struct A {
             bool await_ready() {
                 return o_.b;
             }
             void await_resume() {}
             void await_suspend(stdx::coroutine_handle<> h){
                 h.destroy();
             }
             O o_;
         };
         return A{o};
     }
     O* value;
 };

 explicit O(promise_type* p){ p->value = this; }
 explicit O(bool b) : b(b) {}

 bool b = false;
};

O g();

O f() { co_await g();  co_return; }

This is basically a reduced version of https://github.com/facebook/folly/blob/99f856ae2009a80b157b5121e44b1f70f61bd7c9/folly/Optional.h#L613-L678

GCC does not appear to be able to optimize away the coroutine frame allocation, while Clang does: https://gcc.godbolt.org/z/7981o81ne


---


### compiler : `gcc`
### title : `vectorize failed to generate VEC_COND_EXPR for v32qi`
### open_at : `2021-05-13T08:48:49Z`
### last_modified_date : `2021-05-17T13:28:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100582
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
https://godbolt.org/z/nfdsGPTxj

cat test.c 

v32qi
f2 (v32qi x, v32qi a, v32qi b)
{
    v32qi e;
  for (int i = 0; i != 32; i++)
     e[i] = x[i] ? a[i] : b[i];

  return e;
}

clang can generate vplendvb, gcc doesn't vectorize it.

also clang can generate clean vplendvb for f1.

v32qi
f1 (v32qi x, v32qi a, v32qi b)
{
    v32qi e;
  for (int i = 0; i != 32; i++)
     e[i] = x[i] & -128 ? a[i] : b[i];

  return e;
}


---


### compiler : `gcc`
### title : `bool - 1 is not simplified to -a`
### open_at : `2021-05-15T04:03:43Z`
### last_modified_date : `2021-05-15T04:19:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100609
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(_Bool a)
{
  int t = a;
  return t - 1;
}

int g(_Bool a)
{
  int t = a;
  return -t;
}
---- CUT ----
Both of these are the same and should produce the same code gen.


---


### compiler : `gcc`
### title : `Conversion to smaller unsigned type in loop`
### open_at : `2021-05-16T08:37:39Z`
### last_modified_date : `2021-09-17T06:49:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100622
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Consider

unsigned int foo(unsigned int *a, int n)
{
  int i;
  unsigned int res = 0;
  for (i=0; i<n; i++)
    res += a[i];

  return res;
}

unsigned int foo2 (unsigned int *a, int n)
{
  int i;
  unsigned long res = 0;
  for (i=0; i<n; i++)
    res += a[i];

  return res;
}

Given modular 2^n arithmetic, these two functions are identical in effect.

On POWER with a reasonably recent trunk with -O1, this gets compiled to
(using -O1 in order to avoid loop unrolling for better visibility)

$ gcc -O1 -c add.c && objdump --disassemble add.o

add.o:     file format elf64-powerpcle


Disassembly of section .text:

0000000000000000 <foo>:
   0:   00 00 04 2c     cmpwi   r4,0
   4:   30 00 81 40     ble     34 <foo+0x34>
   8:   20 00 89 78     clrldi  r9,r4,32
   c:   fc ff 43 39     addi    r10,r3,-4
  10:   00 00 60 38     li      r3,0
  14:   a6 03 29 7d     mtctr   r9
  18:   04 00 0a 85     lwzu    r8,4(r10)
  1c:   14 1a 68 7c     add     r3,r8,r3
  20:   20 00 63 78     clrldi  r3,r3,32
  24:   ff ff 29 39     addi    r9,r9,-1
  28:   20 00 29 79     clrldi  r9,r9,32
  2c:   ec ff 00 42     bdnz    18 <foo+0x18>
  30:   20 00 80 4e     blr
  34:   00 00 60 38     li      r3,0
  38:   20 00 80 4e     blr
        ...

0000000000000048 <foo2>:
  48:   00 00 04 2c     cmpwi   r4,0
  4c:   30 00 81 40     ble     7c <foo2+0x34>
  50:   20 00 89 78     clrldi  r9,r4,32
  54:   fc ff 43 39     addi    r10,r3,-4
  58:   00 00 60 38     li      r3,0
  5c:   a6 03 29 7d     mtctr   r9
  60:   04 00 0a 85     lwzu    r8,4(r10)
  64:   14 42 63 7c     add     r3,r3,r8
  68:   ff ff 29 39     addi    r9,r9,-1
  6c:   20 00 29 79     clrldi  r9,r9,32
  70:   f0 ff 00 42     bdnz    60 <foo2+0x18>
  74:   20 00 63 78     clrldi  r3,r3,32
  78:   20 00 80 4e     blr
  7c:   00 00 60 38     li      r3,0
  80:   f4 ff ff 4b     b       74 <foo2+0x2c>

so there is an extra instruction to mask the result of the
addition in foo.  This should not be needed.


---


### compiler : `gcc`
### title : `missing optimization`
### open_at : `2021-05-16T19:23:46Z`
### last_modified_date : `2021-05-21T19:15:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100627
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Hello gcc team,
i think i wrote something like that a long time ago, but i'm not sure. I think the standard conversion uint64_t -> float/double is inefficient when AVX512 is not available. At least on x86, but with SVE or other CPUs this may not be the case. Problems:
- a lot of conditional jumps are generated, not BPU-friendly
- and therefore not branchfree
- larger codesize
I briefly implemented a few conversions for SSE/SSE2 (https://godbolt.org/z/n63WedKT9). Advantages:
- branchfree
- mostly smaller codesize
- more quickly
Wouldn't it make sense to implement the standard conversion in this way (including for AVX/AVX2)?

thx
Gero


---


### compiler : `gcc`
### title : `FP16 vector compare missed optimization on AArch64`
### open_at : `2021-05-17T13:50:05Z`
### last_modified_date : `2021-05-22T23:53:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100638
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
The following testcase

```
#include <arm_neon.h>

void foo(float16_t *x, uint16x8_t *out) {
    float16x8x2_t xk = vld2q_f16(x);
    float16x8_t xk_re = xk.val[0];
    float16x8_t xk_im = xk.val[1];
    uint16x8_t theta_rx = xk_re < 0;
    uint16x8_t theta_ix = xk_im < 0;
    out[0] = theta_rx;
    out[1] = theta_ix;
}
```
on AArch64 with `-Ofast -march=armv8.2-a+fp16` generates

```
foo:
        ld2     {v0.8h - v1.8h}, [x0]
        mov     w2, -1
        fcvt    s2, h0
        fcvt    s18, h1
        dup     h3, v0.h[1]
        dup     h24, v0.h[2]
        dup     h23, v0.h[3]
        fcmpe   s2, #0.0
        fcvt    s3, h3
        fcvt    s24, h24
        dup     h22, v0.h[4]
        csel    w0, w2, wzr, mi
        fcvt    s23, h23
        fcmpe   s3, #0.0
        dup     v2.4h, w0
        dup     h17, v1.h[1]
        dup     h16, v1.h[2]
        csel    w0, w2, wzr, mi
        fcmpe   s24, #0.0
        dup     h7, v1.h[3]
        dup     h6, v1.h[4]
        dup     h5, v1.h[5]
        dup     h4, v1.h[6]
        dup     h3, v1.h[7]
        mov     v1.16b, v2.16b
        dup     h21, v0.h[5]
        fcvt    s22, h22
        fcvt    s17, h17
        dup     h20, v0.h[6]
        ins     v1.h[1], w0
        csel    w0, w2, wzr, mi
        fcmpe   s23, #0.0
        fcvt    s21, h21
        dup     h19, v0.h[7]
        fcvt    s20, h20
        fcvt    s16, h16
        ins     v1.h[2], w0
        fcvt    s7, h7
        csel    w0, w2, wzr, mi
        fcmpe   s22, #0.0
        fcvt    s19, h19
        fcvt    s6, h6
        fcvt    s5, h5
        fcvt    s4, h4
        ins     v1.h[3], w0
        fcvt    s3, h3
        csel    w0, w2, wzr, mi
        fcmpe   s21, #0.0
        ins     v1.h[4], w0
        csel    w0, w2, wzr, mi
        fcmpe   s20, #0.0
        ins     v1.h[5], w0
        csel    w0, w2, wzr, mi
        fcmpe   s19, #0.0
        ins     v1.h[6], w0
        csel    w0, w2, wzr, mi
        fcmpe   s18, #0.0
        ins     v1.h[7], w0
        csel    w0, w2, wzr, mi
        fcmpe   s17, #0.0
        dup     v0.4h, w0
        csel    w0, w2, wzr, mi
        fcmpe   s16, #0.0
        ins     v0.h[1], w0
        csel    w0, w2, wzr, mi
        fcmpe   s7, #0.0
        ins     v0.h[2], w0
        csel    w0, w2, wzr, mi
        fcmpe   s6, #0.0
        ins     v0.h[3], w0
        csel    w0, w2, wzr, mi
        fcmpe   s5, #0.0
        ins     v0.h[4], w0
        csel    w0, w2, wzr, mi
        fcmpe   s4, #0.0
        ins     v0.h[5], w0
        csel    w0, w2, wzr, mi
        fcmpe   s3, #0.0
        ins     v0.h[6], w0
        csel    w2, w2, wzr, mi
        ins     v0.h[7], w2
        stp     q1, q0, [x1]
        ret
```

instead of simply

```
foo:
        ld2     { v0.8h, v1.8h }, [x0]
        fcmlt   v2.8h, v0.8h, #0.0
        fcmlt   v0.8h, v1.8h, #0.0
        stp     q2, q0, [x1]
        ret
```

This is happening because veclower doesn't find a pattern for the FP16 vector compare and then lowers to operations on scalar.

However even the lowered operations are inefficient:

```
        fcvt    s23, h23
        fcmpe   s23, #0.0
```

indicates that the backend doesn't know how to do this operation on fp16.


---


### compiler : `gcc`
### title : `Optimize away x<0 as mask argument of a blend for AVX512 mask`
### open_at : `2021-05-18T08:59:33Z`
### last_modified_date : `2021-06-28T01:35:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100648
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Similar to PR target/54700, but for AVX512 mask


---


### compiler : `gcc`
### title : `attribute optimize ("O2") doesn't inline lambda`
### open_at : `2021-05-19T17:20:08Z`
### last_modified_date : `2021-12-15T11:09:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100686
### status : `UNCONFIRMED`
### tags : `missed-optimization, opt-attribute`
### component : `c++`
### version : `11.1.0`
### severity : `normal`
### contents :
Declaring with attribute optimize ("O2") a function that makes use of a trivial lambda function with doesn't inline the lambda unless the whole translation unit is compiled with optimization enabled.  I'm guessing this happens for the same reason as pr100323, although here the problem seems more severe since the lambda is defined fully in the scope of the called function.

$ cat t.C && gcc -S -Wall -fdump-tree-optimized=/dev/stdout t.C
__attribute__ ((optimize ("O2"))) int f ()
{
  return [](int x){ return x; }( 1 );
}


;; Function f()::<lambda(int)>::operator() (_ZZ1fvENKUliE_clEi, funcdef_no=1, decl_uid=2352, cgraph_uid=1, symbol_order=0)

int f()::<lambda(int)>::operator() (const struct ._anon_0 * const __closure, int x)
{
  <bb 2> [local count: 1073741824]:
  return x_1(D);

}



;; Function f (_Z1fv, funcdef_no=0, decl_uid=2346, cgraph_uid=4, symbol_order=3)

__attribute__((optimize ("O2")))
int f ()
{
  struct ._anon_0 D.2371;
  int _2;

  <bb 2> [local count: 1073741824]:
  _2 = f()::<lambda(int)>::operator() (&D.2371, 1); [tail call]
  D.2371 ={v} {CLOBBER};
  return _2;

}


---


### compiler : `gcc`
### title : `PPC: initialization of __int128 is very inefficient`
### open_at : `2021-05-20T07:40:14Z`
### last_modified_date : `2022-07-28T09:11:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100694
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `normal`
### contents :
Initializing a __int128 from 2 64-bit integers is implemented very inefficient.

The most natural code which works good on all other platforms generate additional 2 li 0 + 2 or instructions.

void test2(unsigned __int128* res, unsigned long long hi, unsigned long long lo)
{
   unsigned __int128 i = hi;
   i <<= 64;
   i |= lo;
   *res = i;
}

_Z5test2Poyy:
.LFB15:
        .cfi_startproc
        li 8,0
        li 11,0
        or 10,5,8
        or 11,11,4
        std 10,0(3)
        std 11,8(3)
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0
        .cfi_endproc


While for the above sample, "+" instead "|" solves the issues, it generates addc+addz in other more complicated scenarsion.

The most ugly workaround I can think of I now use as workaround.

void test4(unsigned __int128* res, unsigned long long hi, unsigned long long lo)
{
   union
   { unsigned __int128 i;
        struct
   {
     unsigned long long lo;
     unsigned long long hi;
   } s;
   } u;
   u.s.lo = lo;
   u.s.hi = hi;
   *res = u.i;
}

This generates the expected code sequence in all cases I have looked at.

_Z5test4Poyy:
.LFB17:
        .cfi_startproc
        std 5,0(3)
        std 4,8(3)
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0
        .cfi_endproc

Please merge li 0 + or to nop.


---


### compiler : `gcc`
### title : `mult_higpart is not vectorized`
### open_at : `2021-05-20T09:26:18Z`
### last_modified_date : `2021-08-25T03:01:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100696
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Following testcases:

--cut here--
#define N 4

short r[N], a[N], b[N];
unsigned short ur[N], ua[N], ub[N];

void mul (void)
{
  int i;

  for (i = 0; i < N; i++)
    r[i] = a[i] * b[i];
}

/* { dg-final { scan-assembler "pmullw" } } */

void mulhi (void)
{
  int i;

  for (i = 0; i < N; i++)
    r[i] = ((int) a[i] * b[i]) >> 16;
}

/* { dg-final { scan-assembler "pmulhw" } } */

void umulhi (void)
{
  int i;

  for (i = 0; i < N; i++)
    ur[i] = ((unsigned int) ua[i] * ub[i]) >> 16;
}

/* { dg-final { scan-assembler "pmulhuw" } } */

void smulhrs (void)
{
  int i;

  for (i = 0; i < N; i++)
    r[i] = ((((int) a[i] * b[i]) >> 14) + 1) >> 1;
}

/* { dg-final { scan-assembler "pmulhrsw" } } */
--cut here--

should all vectorize for x86_64 with "-O3 -mssse3" to their vector instructions.

Currently the compiler vectorizes only pmullw and much more complex pmulhrsw, but not pmulhw and pmulhuw.

For N = 2 (SLP vectorization?), the compiler manages to vectorize mul and none of the other testcases.


---


### compiler : `gcc`
### title : `Scheduler before RA causes conflict with return register and incoming argument register`
### open_at : `2021-05-20T09:49:27Z`
### last_modified_date : `2022-02-06T09:39:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100697
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `10.3.0`
### severity : `enhancement`
### contents :
cat test.c

extern double top[100];

int foo (long long j, double a) {
  top[j] += a;
  return 0;
}

gcc10.3 -g0 -O3 -march=armv8.2-a test.c -save-temps -S, can also be test base on https://gcc.godbolt.org.

	.type	foo, %function
foo:
.LFB0:
	.cfi_startproc
	mov	x2, x0   -- redundant could be fwprop
	adrp	x1, top
	add	x1, x1, :lo12:top
	mov	w0, 0
	ldr	d1, [x1, x2, lsl 3]
	fadd	d0, d1, d0
	str	d0, [x1, x2, lsl 3]
	ret

If x0 fwprop into both ldr and str, then the insn mov	x2, x0 can be deleted.


---


### compiler : `gcc`
### title : `Miss optimization for pandn`
### open_at : `2021-05-21T02:22:00Z`
### last_modified_date : `2023-07-05T07:49:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100711
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c

typedef int v4si __attribute__((vector_size (16)));
v4si
foo (int a, v4si b)
{
    return (__extension__ (v4si) {~a, ~a, ~a, ~a}) & b;
}

generate

        notl    %edi
        vmovdqa %xmm0, %xmm1
        vpbroadcastd    %edi, %xmm0
        vpand   %xmm1, %xmm0, %xmm0
        ret

it should be better as

        vmovdqa %xmm0, %xmm1
        vpbroadcastd    %edi, %xmm0
        vpandn   %xmm1, %xmm0, %xmm0


---


### compiler : `gcc`
### title : `Gimple failed to simplify ((v4si) ~a) < 0 ? c : d to ((v4si)a) >= 0 ? c : d`
### open_at : `2021-05-24T08:21:42Z`
### last_modified_date : `2021-12-17T05:56:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100738
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
cat test.c



typedef int v4si __attribute__((vector_size (16)));
typedef char v16qi __attribute__((vector_size (16)));
v4si
foo (v16qi a, v4si b, v4si c, v4si d)
{
    return ((v4si)~a) < 0 ? c : d;
}

gcc -Ofast -mavx2

foo(char __vector(16), int __vector(4), int __vector(4), int __vector(4)):
        vpcmpeqd        %xmm1, %xmm1, %xmm1
        vpxor   %xmm1, %xmm0, %xmm0
        vblendvps       %xmm0, %xmm2, %xmm3, %xmm0
        ret

it can be better with 

        vblendvps       xmm0, xmm3, xmm2, xmm0

gimple failed to simplify  ((v4si)~a) < 0 ? c : d to ((v4si)a) >= 0 ? c : d

With https://gcc.gnu.org/pipermail/gcc-patches/2021-May/571056.html, i observe rtl also won't simplify things like (vec_merge op1 op2 (lt (subreg （not op3) 0) const0_rtx)) to (vec_merge op2 op1 (lt (subreg op3 0) const0_rtx))


---


### compiler : `gcc`
### title : `GCC generates suboptimal assembly from vector extensions on AArch64`
### open_at : `2021-05-24T14:11:28Z`
### last_modified_date : `2021-05-25T07:40:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100745
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50861
The profile.c file minimal benchmark/test case

As part of an attempt to make mpv's scaletempo2 audio filter faster, two vectorised implementations were written:

The first one, mine, uses aarch64 intrinsics. It shows a 3.14x speedup on my test system, and is referred to as "new" or "nicolas" in the code.

The second one, by haasn, also referred to as "niklas" in the code, uses GCC's vector extensions to automatically generate vectorised code for a wide variety of architectures. It shows a slower speedup on my system and another aarch64 test system (1.45x) but shows a much better speedup on x86_64 (>2x for generic, >10x for -march=native on this zen+ laptop thanks to avx).

Clang, on the other hand compiles the vector extension code down to something more efficient than gcc, beating my intrinsics SIMD (even in absolute terms compared to gcc). I believe this is due to a bug in gcc making it produce subpar vector assembly on aarch64 in this case.

Since we'd rather not keep platform specific vector code around in mpv, and clang's codegen is overall worse in non-vector code, we'd much appreciate it if someone could look into what gcc is tripping over here.

Attached is the minimal microbenchmark profile.c, which needs no special options or includes aside from stdio so no .i file if that's alright. My test system is a cortex-a53 in-order core, though -mtune -march for that does not fix it, and the problem also exhibits itself on a cortex-a55 in-order core.

The test was compiled with gcc -O3 -o profile profile.c, though it is worth noting that the pure C implementation performs much better under -O2 (possibly a separate bug) while both SIMD versions are largely unaffected by this.

GCC Version: 10.2.0
Distribution: Arch Linux ARM
Platform: ROCK64 with a RK3328 (4x Cortex A-53, 2GB RAM)

The options used for building gcc can be found here, in build(): https://archlinuxarm.org/packages/aarch64/gcc/files/PKGBUILD

I've looked at the disassembly of gcc trunk on godbolt, but it did not look significantly different enough to me to think this has already been fixed in trunk. If required, I can try building gcc trunk from source.


---


### compiler : `gcc`
### title : `NRVO should not introduce aliasing`
### open_at : `2021-05-24T16:16:58Z`
### last_modified_date : `2021-05-25T07:41:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100746
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
Consider the example:

struct NrvoPassed {
    NrvoPassed() = default;
    NrvoPassed(const NrvoPassed&);
    NrvoPassed(NrvoPassed&&);

    int i = 0;
};

auto test(int* data) {
    NrvoPassed x;
    *data = 3;
    if (x.i != 0) __builtin_abort();
    return x;
}


Resulting assembly contains call to `abort`:

test(int*):
  mov DWORD PTR [rdi], 0
  mov DWORD PTR [rsi], 3
  mov edx, DWORD PTR [rdi]
  test edx, edx
  jne .L3
  mov rax, rdi
  ret
test(int*) [clone .cold]:
.L3:
  push rax
  call abort

Optimizer thinks that the value of `x.i` is aliased by `data`, however `data` is a local variable and it's address could not leak before the object is constructed.

Some other compilers already have the proposed optimization: https://godbolt.org/z/aqdveadnE

Adding `__restrict` to `data` fixes the codegen:

test2(int*):
  mov DWORD PTR [rdi], 0
  mov rax, rdi
  mov DWORD PTR [rsi], 3
  ret

Probably `__restrict` should be always added to the storage address passed for NRVO.


---


### compiler : `gcc`
### title : `[12 Regression] vect: Superfluous epilog created on s390x`
### open_at : `2021-05-25T16:22:26Z`
### last_modified_date : `2023-05-08T12:22:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100756
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Since g:d846f225c25c5885250c303c8d118caa08c447ab we create an epilog loop on s390 for the following test case:

/* { dg-do compile } */
/* { dg-options "-O3 -mzarch -march=z13" } */
/* { dg-require-effective-target s390_vx } */

int
foo (int * restrict a, int n)
{
  int i, result = 0;

  for (i = 0; i < n * 4; i++)
    result += a[i];
  return result;
}

vec.c:10:17: note:  epilog loop required

The following check in tree-vect-loop.c:vect_need_peeling_or_partial_vectors_p() is now true:

               || ((tree_ctz (LOOP_VINFO_NITERS (loop_vinfo))                                                                                                                       
                     < (unsigned) exact_log2 (const_vf)) 

We now have LOOP_VINFO_NITERS (loop_vinfo) = _15 > 0 ? (unsigned int) _15 : 1 as compared to (unsigned int) _15 before. tree_ctz() returns 0 for the conditional and 2 before which did not trigger the epilog requirement.

may_be_zero is _15 > 0 so it looks to me like we rather want to check the not-may_be_zero part of niter for alignment. Not sure if this is the right/safe thing to do, though.


---


### compiler : `gcc`
### title : `__builtin___sprintf_chk not optimized when it could be`
### open_at : `2021-05-26T21:00:02Z`
### last_modified_date : `2021-05-26T21:00:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100780
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
Calls to __builtin___sprintf_chk that can be proven not to overflow the destination can be folded to calls to plain sprintf (or even memcpy).  Only the simplest are, but the more interesting ones aren't, even though the sprintf pass has all the smarts to do it.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c 
extern char a[32];

void f (void)
{
  __builtin___sprintf_chk (a, 0, sizeof a, "%s", "1234");  // optimized
}

void g (void)
{
  __builtin___sprintf_chk (a, 0, sizeof a, "%i", 1234);    // not optimized
}

;; Function f (f, funcdef_no=0, decl_uid=1944, cgraph_uid=1, symbol_order=0)

void f ()
{
  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "1234", 5); [tail call]
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1947, cgraph_uid=2, symbol_order=1)

void g ()
{
  <bb 2> [local count: 1073741824]:
  __builtin___sprintf_chk (&a, 0, 32, "%i", 1234); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `suboptimal code due to missing pre2 when vectorization fails`
### open_at : `2021-05-27T07:48:11Z`
### last_modified_date : `2021-06-09T02:20:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100794
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
I was investigating one degradation from SPEC2017 554.roms_r on Power9, the baseline is -O2 -mcpu=power9 -ffast-math while the test line is -O2 -mcpu=power9 -ffast-math -ftree-vectorize -fvect-cost-model=very-cheap.

One reduced C test case is as below:

#include <math.h>

#define MIN fmin
#define MAX fmax

#define N1 400
#define N2 600
#define N3 800

extern int j_0, j_n, i_0, i_n;
extern double diff2[N1][N2];
extern double dZdx[N1][N2][N3];
extern double dTdz[N1][N2][N3];
extern double dTdx[N1][N2][N3];
extern double FS[N1][N2][N3];

void
test (int k1, int k2)
{
  for (int j = j_0; j < j_n; j++)
    for (int i = i_0; i < i_n; i++)
      {
        double cff = 0.5 * diff2[j][i];
        double cff1 = MIN (dZdx[k1][j][i], 0.0);
        double cff2 = MIN (dZdx[k2][j][i + 1], 0.0);
        double cff3 = MAX (dZdx[k2][j][i], 0.0);
        double cff4 = MAX (dZdx[k1][j][i + 1], 0.0);

        FS[k2][j][i]
          = cff
            * (cff1 * (cff1 * dTdz[k2][j][i] - dTdx[k1][j][i])
               + cff2 * (cff2 * dTdz[k2][j][i] - dTdx[k2][j][i + 1])
               + cff3 * (cff3 * dTdz[k2][j][i] - dTdx[k2][j][i])
               + cff4 * (cff4 * dTdz[k2][j][i] - dTdx[k1][j][i + 1]));
      }
}

O2 fast:

  <bb 8> [local count: 955630225]:
  # prephitmp_107 = PHI <_6(8), pretmp_106(7)>
  # prephitmp_109 = PHI <_4(8), pretmp_108(7)>
  # prephitmp_111 = PHI <_23(8), pretmp_110(7)>
  # prephitmp_113 = PHI <_13(8), pretmp_112(7)>
  # doloop.9_55 = PHI <doloop.9_57(8), doloop.9_105(7)>
  # ivtmp.33_102 = PHI <ivtmp.33_101(8), ivtmp.44_70(7)>
  _87 = (double[400][600] *) ivtmp.45_60;
  _1 = MEM[(double *)_87 + ivtmp.33_102 * 1];
  cff_38 = _1 * 5.0e-1;
  cff1_40 = MIN_EXPR <prephitmp_107, 0.0>;
  _4 = MEM[(double *)&dZdx + 8B + ivtmp.33_102 * 1];
  cff2_42 = MIN_EXPR <_4, 0.0>;
  cff3_43 = MAX_EXPR <prephitmp_109, 0.0>;
  _6 = MEM[(double *)_79 + ivtmp.33_102 * 1];
  cff4_44 = MAX_EXPR <_6, 0.0>;


O2 fast vect (very-cheap)
  <bb 6> [local count: 955630225]:
  # doloop.9_55 = PHI <doloop.9_57(6), doloop.9_105(5)>
  # ivtmp.37_102 = PHI <ivtmp.37_101(6), ivtmp.46_72(5)>
  # ivtmp.38_92 = PHI <ivtmp.38_91(6), ivtmp.38_90(5)>
  _77 = (double[400][600] *) ivtmp.48_62;
  _1 = MEM[(double *)_77 + ivtmp.37_102 * 1];
  cff_38 = _1 * 5.0e-1;
  _2 = MEM[(double *)&dZdx + ivtmp.38_92 * 1];   // redundant load
  cff1_40 = MIN_EXPR <_2, 0.0>;
  _4 = MEM[(double *)&dZdx + 8B + ivtmp.37_102 * 1];
  cff2_42 = MIN_EXPR <_4, 0.0>;
  _5 = MEM[(double *)&dZdx + ivtmp.37_102 * 1];  // redundant load 
  cff3_43 = MAX_EXPR <_5, 0.0>;
  _6 = MEM[(double *)&dZdx + 8B + ivtmp.38_92 * 1];
  cff4_44 = MAX_EXPR <_6, 0.0>;


I found the root cause is that: in the baseline version, PRE makes it to reuse some load result from previous iterations, it saves some loads. while in the test line version, with the check below:

      /* Inhibit the use of an inserted PHI on a loop header when
	 the address of the memory reference is a simple induction
	 variable.  In other cases the vectorizer won't do anything
	 anyway (either it's loop invariant or a complicated
	 expression).  */
      if (sprime
	  && TREE_CODE (sprime) == SSA_NAME
	  && do_pre
	  && (flag_tree_loop_vectorize || flag_tree_parallelize_loops > 1)

PRE doesn't optimize it to avoid introducing loop carried dependence. It makes sense. But unfortunately the expected downstream loop vectorization isn't performed on the given loop since with "very-cheap" cost model, it doesn't allow vectorizer to peel for niters. Later there seems no downstream pass which is trying to optimize it, it eventually results in sub-optimal code.

To rerun pre once after loop vectorization did fix the degradation, but not sure it's practical, since iterating pre seems much time-consuming. Or tagging this kind of loop and later just run pre on the tagged one? It seems also not practical to predict one loop whether can be loop-vectorized later. Also not sure whether there are some passes which can be taught for this.


---


### compiler : `gcc`
### title : `a?~t:t and (-(!!a))^t don't produce the same assembly code`
### open_at : `2021-05-27T10:39:53Z`
### last_modified_date : `2023-08-09T19:27:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100798
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
While working on non GCC code, I notice these two functions don't produce the assembly code:
int f(int a, int t)
{
  return (a=='s' ? ~t : t);
}

int f1(int a, int t)
{
  int t1 = -(a=='s');
  return (t1 ^ t);
}

For aarch64, the first case produce the best, while on x86_64, I don't know which is better, sete or cmov.

Note LLVM produces the same code for both, for aarch64, the csinv and sete/neg/xor for x86_64.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Aggressive loop optimizations cause incorrect warning`
### open_at : `2021-05-27T14:03:54Z`
### last_modified_date : `2023-07-07T10:40:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100801
### status : `ASSIGNED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
The following warning is triggered

> $ gcc-11 -c constproploopopt.c -O2 -Wall -mavx -g
> constproploopopt.c: In function ‘test’:
> constproploopopt.c:22:18: warning: iteration 4611686018427387903 invokes undefined behavior [-Waggressive-loop-optimizations]
>    22 |     dest[i] = src[i];
>       |                  ^
> constproploopopt.c:21:12: note: within this loop
>    21 |   for (; i < count; ++i) {  // handle residual elements
>       |          ~~^~~~~~~

by this (minimal) code:

> #include <stdint.h>
> #include <stdio.h>
> #if defined(_MSC_VER)
> #include <intrin.h>
> #else
> #include <x86intrin.h>
> #endif
> 
> void copy_32_unaligned(uint32_t* dest, const uint32_t* src, size_t count) {
>   // invariant/nop
>   __m128i shufmask =  _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
> 
>   size_t i;
>   for (i = 0; i + 4 <= count; i += 4) {
>     __m128i input = _mm_loadu_si128((const __m128i*)(&src[i]));
>     __m128i output = input;
>     // no warning without the shuffle:
>     output = _mm_shuffle_epi8(input, shufmask);
>     _mm_storeu_si128((__m128i*)(&dest[i]), output);
>   }
>   for (; i < count; ++i) {  // handle residual elements
>     dest[i] = src[i];
>   }
> }
> 
> void test(uint32_t* buf1, uint32_t* buf2) {
>     copy_32_unaligned(buf2, buf1,
>                       // multiples of 4 and greater or equal then 12 trigger it:
>                       12);
> }

From objdump output I believe the generated code is correct though. The warning seems to be incorrect in this context, especially since the "residual" loop should be skipped for count=n*4 anyways.


---


### compiler : `gcc`
### title : `VRP fails to fold comparison using known value orders`
### open_at : `2021-05-27T14:56:49Z`
### last_modified_date : `2021-11-16T01:49:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100802
### status : `VERIFIED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
In the case below, it is obvious that we have value orders as i < a <= b inside loop. So "if (i >= b)" could be optimized away, but VRP fails to do that.


int f1();
int f2();

int foo (unsigned a, unsigned b)
{
   if (a <= b)
    {
       for (unsigned i = 0; i < a; i++)
         {
            f1 ();

            if (i >= b)
              f2 ();
         }

    }

  return 0;
}


---


### compiler : `gcc`
### title : `PPC: __int128 divide/modulo does not use P10 instructions vdivsq/vdivuq`
### open_at : `2021-05-28T07:58:35Z`
### last_modified_date : `2021-08-15T12:11:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100809
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `normal`
### contents :
unsigned __int128 div(unsigned __int128 a, unsigned __int128 b)
{
   return a/b;
}

__int128 div(__int128 a, __int128 b)
{
   return a/b;
}

gcc -mcpu=power10 -save-temps -O2 int128.C

Output:
_Z3divoo:
.LFB0:
        .cfi_startproc
        .localentry     _Z3divoo,1
        mflr 0
        std 0,16(1)
        stdu 1,-32(1)
        .cfi_def_cfa_offset 32
        .cfi_offset 65, 16
        bl __udivti3@notoc
        addi 1,1,32
        .cfi_def_cfa_offset 0
        ld 0,16(1)
        mtlr 0
        .cfi_restore 65
        blr
        .long 0
        .byte 0,9,0,1,128,0,0,0
        .cfi_endproc
.LFE0:
        .size   _Z3divoo,.-_Z3divoo
        .globl __divti3
        .align 2
        .p2align 4,,15
        .globl _Z3divnn
        .type   _Z3divnn, @function
_Z3divnn:
.LFB1:
        .cfi_startproc
        .localentry     _Z3divnn,1
        mflr 0
        std 0,16(1)
        stdu 1,-32(1)
        .cfi_def_cfa_offset 32
        .cfi_offset 65, 16
        bl __divti3@notoc
        addi 1,1,32
        .cfi_def_cfa_offset 0
        ld 0,16(1)
        mtlr 0
        .cfi_restore 65
        blr
        .long 0
        .byte 0,9,0,1,128,0,0,0
        .cfi_endproc

Expected is the use of vdivsq/vdivuq.

GCC version:

/opt/rh/devtoolset-10/root/usr/bin/gcc -v
Using built-in specs.
COLLECT_GCC=/opt/rh/devtoolset-10/root/usr/bin/gcc
COLLECT_LTO_WRAPPER=/opt/rh/devtoolset-10/root/usr/libexec/gcc/ppc64le-redhat-linux/10/lto-wrapper
Target: ppc64le-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-10/root/usr --mandir=/opt/rh/devtoolset-10/root/usr/share/man --infodir=/opt/rh/devtoolset-10/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-targets=powerpcle-linux --disable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --with-default-libstdcxx-abi=gcc4-compatible --enable-plugin --enable-initfini-array --with-isl=/builddir/build/BUILD/gcc-10.2.1-20200804/obj-ppc64le-redhat-linux/isl-install --disable-libmpx --enable-gnu-indirect-function --enable-secureplt --with-long-double-128 --with-cpu-32=power8 --with-tune-32=power8 --with-cpu-64=power8 --with-tune-64=power8 --build=ppc64le-redhat-linux
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.2.1 20200804 (Red Hat 10.2.1-2) (GCC)


---


### compiler : `gcc`
### title : `Different vector handling for strided IVs and modulo conditions`
### open_at : `2021-06-01T07:22:02Z`
### last_modified_date : `2021-07-03T05:16:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100846
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following loops are equivalent, but we only vectorise the second:

void f1 (int *x)
{
  for (int i = 0; i < 100; ++i)
    if ((i & 1) == 0)
      x[i] += 1;
}

void f2 (int *x)
{
  for (int i = 0; i < 100; i += 2)
    x[i] += 1;
}

I guess this isn't vectorisation-specific.  Perhaps this is something that
ivcanon could handle?


---


### compiler : `gcc`
### title : `Cases that require -fallow-store-data-races aren't vectorised with IFN_MASK_LOAD/STORE`
### open_at : `2021-06-01T07:57:33Z`
### last_modified_date : `2021-07-03T05:17:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100848
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
If a loop could be vectorised with -fallow-store-data-races, that approach
trumps IFN_MASK_LOAD/STORE even if -fallow-store-data-races is off.

This means that, in the following example, we can vectorise f2
with -O3 -march=armv8.2-a+sve, but we can't vectorise f1 without
-fallow-store-data-races:

int x[100], y[100];

void f1 (void)
{
  for (int i = 0; i < 100; ++i)
    if (x[i])
      y[i] += 1;
}

void f2 (int *restrict x, int *restrict y)
{
  for (int i = 0; i < 100; ++i)
    if (x[i])
      y[i] += 1;
}

Even if -fallow-store-data-races is passed, it would be better
to avoid load, select and store in the data-races version of:

int x[100], y[100];

void f1 (void)
{
  for (int i = 0; i < 100; ++i)
    if (x[i])
      y[i] = 1;
}

when a single IFN_MASK_STORE is enough.


---


### compiler : `gcc`
### title : `Poor placement of vector IVs`
### open_at : `2021-06-01T08:11:54Z`
### last_modified_date : `2021-06-02T20:38:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100849
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Vector IV increments are usually placed at the beginning of a loop body.
This means that both the old and new IV values are live at the same time,
forcing a move.

E.g.:

int x[100], y[100];

void f1 (void)
{
  for (int i = 0; i < 100; ++i)
    x[i] = (i & 11) == 2 ? y[i] : 1;
}

produces:

  <bb 3> [local count: 268435400]:
  # vect_vec_iv_.7_47 = PHI <_48(3), { 4, 5, 6, 7 }(2)>
  # ivtmp.21_21 = PHI <ivtmp.21_16(3), 0(2)>
  _48 = vect_vec_iv_.7_47 + { 4, 4, 4, 4 };
  vect__1.8_50 = vect_vec_iv_.7_47 & { 11, 11, 11, 11 };
  vect_iftmp.11_54 = MEM <vector(4) int> [(int *)&y + 16B + ivtmp.21_21 * 1];
  vect_iftmp.12_58 = .VCOND (vect__1.8_50, { 2, 2, 2, 2 }, vect_iftmp.11_54, { 1, 1, 1, 1 }, 113);
  MEM <vector(4) int> [(int *)&x + 16B + ivtmp.21_21 * 1] = vect_iftmp.12_58;
  ivtmp.21_16 = ivtmp.21_21 + 16;
  if (ivtmp.21_16 != 384)
    goto <bb 3>; [96.00%]
  else
    goto <bb 4>; [4.00%]

It might be better to place the vector IV at the same place as
the original scalar increment (or at the end of the loop body?)

The AArch64 Advanced SIMD code is:

.L2:
        mov     v0.16b, v1.16b
        add     x2, x4, x0
        add     v1.4s, v1.4s, v6.4s
        add     x1, x3, x0
        add     x0, x0, 16
        ldr     q3, [x2, 16]
        and     v0.16b, v0.16b, v5.16b
        cmeq    v0.4s, v0.4s, v4.4s
        bsl     v0.16b, v3.16b, v2.16b
        str     q0, [x1, 16]
        cmp     x0, 384
        bne     .L2


---


### compiler : `gcc`
### title : `Simple common code sinking is not performed`
### open_at : `2021-06-01T15:14:12Z`
### last_modified_date : `2021-06-03T05:42:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100857
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
void bar(int);

void foo(bool f)
{
    if (f) {
        bar(1);
    }
    else {
        bar(2);
    }
}

; GCC
foo(bool):
        test    dil, dil
        je      .L2
        mov     edi, 1
        jmp     bar(int)
.L2:
        mov     edi, 2
        jmp     bar(int)

; Clang
foo(bool):
        mov     eax, 2
        sub     eax, edi
        mov     edi, eax
        jmp     bar(int)

https://godbolt.org/z/4oEPY4ncb


---


### compiler : `gcc`
### title : `Simple common code hoisting is not performed`
### open_at : `2021-06-01T15:16:06Z`
### last_modified_date : `2021-07-28T20:17:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100858
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
void bar();

int foo(bool f)
{
    if (f) {
        bar();
        return 1;
    }
    else {
        bar();
        return 2;
    }
}

; GCC
foo(bool):
        sub     rsp, 8
        test    dil, dil
        je      .L2
        call    bar()
        mov     eax, 1
        add     rsp, 8
        ret
.L2:
        call    bar()
        mov     eax, 2
        add     rsp, 8
        ret

; Clang
foo(bool):
        push    rbx
        mov     ebx, edi
        call    bar()
        mov     eax, 2
        sub     eax, ebx
        pop     rbx
        ret

https://godbolt.org/z/j9eKG1dds


---


### compiler : `gcc`
### title : `(a&!b) | b is not opimized to a | b for comparisons`
### open_at : `2021-06-01T23:48:33Z`
### last_modified_date : `2023-07-31T17:14:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100864
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
_Bool f(int a, int b, _Bool e)
{
  _Bool c = (a > b);
  _Bool d = !c;
  return (e & d) | c;
}
--- CUT ----
We get currently:
  c_5 = a_3(D) > b_4(D);
  _2 = a_3(D) <= b_4(D);
  _1 = _2 & e_7(D);
  _6 = _1 | c_5;

But this should be optimized to just:

c_5 = a_3(D) <= b_4(D);
_6 = e_7(D) | c_5;

I noticed this while fixing PR 96923.


---


### compiler : `gcc`
### title : `PPC: Inefficient code for vec_revb(vector unsigned short) < P9`
### open_at : `2021-06-02T07:14:05Z`
### last_modified_date : `2022-12-14T05:52:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100866
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `enhancement`
### contents :
Input:

vector unsigned short revb(vector unsigned short a)
{
   return vec_revb(a);
}

creates:

_Z4revbDv8_t:
.LFB1:
        .cfi_startproc
.LCF1:
0:      addis 2,12,.TOC.-.LCF1@ha
        addi 2,2,.TOC.-.LCF1@l
        .localentry     _Z4revbDv8_t,.-_Z4revbDv8_t
        addis 9,2,.LC1@toc@ha
        addi 9,9,.LC1@toc@l
        lvx 0,0,9
        xxlnor 32,32,32
        vperm 2,2,2,0
        blr


Optimal code sequence:
vector unsigned short revb_pwr7(vector unsigned short a)
{
   return vec_rl(a, vec_splats((unsigned short)8));
}

_Z9revb_pwr7Dv8_t:
.LFB2:
        .cfi_startproc
        .localentry     _Z9revb_pwr7Dv8_t,1
        vspltish 0,8
        vrlh 2,2,0
        blr


---


### compiler : `gcc`
### title : `z13: Inefficient code for vec_revb(vector unsigned short)`
### open_at : `2021-06-02T07:20:08Z`
### last_modified_date : `2021-06-02T08:09:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100867
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `normal`
### contents :
Input:

vector unsigned short revb(vector unsigned short a)
{
   return vec_revb(a);
}

Creates:

_Z4revbDv4_j:
.LFB1:
        .cfi_startproc
        larl    %r5,.L4
        vl      %v0,.L5-.L4(%r5),3
        vperm   %v24,%v24,%v24,%v0
        br      %r14

Optimal code sequence:

vector unsigned short revb_z13(vector unsigned short a)
{
   return vec_rli(a, 8);
}

Creates:
_Z8revb_z13Dv8_t:
.LFB5:
        .cfi_startproc
        verllh  %v24,%v24,8
        br      %r14
        .cfi_endproc


---


### compiler : `gcc`
### title : `PPC: Inefficient code for vec_reve(vector double)`
### open_at : `2021-06-02T07:28:20Z`
### last_modified_date : `2021-12-07T03:42:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100868
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `enhancement`
### contents :
Input:

vector double reve(vector double a)
{
   return vec_reve(a);
}

creates:

_Z4reveDv2_d:
.LFB3:
        .cfi_startproc
.LCF3:
0:      addis 2,12,.TOC.-.LCF3@ha
        addi 2,2,.TOC.-.LCF3@l
        .localentry     _Z4reveDv2_d,.-_Z4reveDv2_d
        addis 9,2,.LC2@toc@ha
        addi 9,9,.LC2@toc@l
        lvx 0,0,9
        xxlnor 32,32,32
        vperm 2,2,2,0
        blr

Optimal sequence would be:

vector double reve_pwr7(vector double a)
{
   return vec_xxpermdi(a,a,2);
}

which creates:

_Z9reve_pwr7Dv2_d:
.LFB4:
        .cfi_startproc
        xxpermdi 34,34,34,2
        blr


---


### compiler : `gcc`
### title : `z13: Inefficient code for vec_reve(vector double)`
### open_at : `2021-06-02T07:31:36Z`
### last_modified_date : `2022-08-29T09:18:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100869
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `normal`
### contents :
Input:

vector double reve(vector double a)
{
   return vec_reve(a);
}

creates:
_Z4reveDv2_d:
.LFB3:
        .cfi_startproc
        larl    %r5,.L12
        vl      %v0,.L13-.L12(%r5),3
        vperm   %v24,%v24,%v24,%v0
        br      %r14


Optimal code sequence:

vector double reve_z13(vector double a)
{
   return vec_permi(a,a,2);
}

creates:

_Z6reve_2Dv2_d:
.LFB6:
        .cfi_startproc
        vpdi    %v24,%v24,%v24,4
        br      %r14
        .cfi_endproc


---


### compiler : `gcc`
### title : `[12 Regression] gcc.target/aarch64/subs_compare_2.c fails since r12-1152-g9f55df63`
### open_at : `2021-06-02T09:31:10Z`
### last_modified_date : `2021-06-02T09:35:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100873
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
For the test gcc.target/aarch64/subs_compare_2.c, we used to generate:

foo:
        subs    w0, w0, 4
        csel    w0, w0, wzr, lt
        ret

but since r12-1152-g9f55df63154a39d67ef5b24def7044bf87300831 we generate:

foo:
        mov     w1, 4
        cmp     w0, w1
        csel    w0, w0, w1, le
        sub     w0, w0, #4
        ret


---


### compiler : `gcc`
### title : `[12 Regression] slight missed optimization with min<a,CST>-CST on aarch64 (subs_compare_2.c)`
### open_at : `2021-06-02T09:32:35Z`
### last_modified_date : `2022-02-15T18:15:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100874
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Take:
int f(int a) { return a < 4 ? a - 4 : 0; }
int f1(int a) { int x = a - 4; if (a < 4) return x; return 0; }
int f2(int a) { int t = a < 4 ? a : 4; return t - 4; }

In GCC 11, we produce:
f(int):
        mov     w1, 4
        cmp     w0, w1
        csel    w0, w0, w1, le
        sub     w0, w0, #4
        ret
f1(int):
        subs    w0, w0, 4
        csel    w0, w0, wzr, lt
        ret
f2(int):
        mov     w1, 4
        cmp     w0, w1
        csel    w0, w0, w1, le
        sub     w0, w0, #4
        ret

On the trunk all three give the same code gen (due to PHI-OPT being improved) but of what f and f2 used to give.
All three should produce what f1 had produed instead.
This is gcc.target/aarch64/subs_compare_2.c


---


### compiler : `gcc`
### title : `ifcombine should use (gimple) match and simplify instead of fold`
### open_at : `2021-06-02T19:22:52Z`
### last_modified_date : `2021-06-02T23:59:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100883
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Like PHI-OPT, ifcombine was written before match and simplify was around so it calls fold_build* to do the folding of the if statements.  It would be better if it calls gimple_simplify instead of doing the gimplification manually.


---


### compiler : `gcc`
### title : `CSE leads to fully redundant (back to back) zero-extending loads of the same thing in a loop, or a register copy`
### open_at : `2021-06-05T07:58:59Z`
### last_modified_date : `2021-06-09T00:02:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100922
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 50948
redundant_zero_extend.c

It's rarely a good idea to load the same thing twice; generally better to copy a register.  Or to read the same register twice when a copy isn't needed.  So the following asm should never happen, but it does with current trunk, and similar with GCC as old as 4.5

        movzbl  (%rax), %edx
        movzbl  (%rax), %ecx    # no branch target between these instructions

or 

        ldrb    w4, [x2]
        ldrb    w3, [x2], 1     # post-indexed *x2++

(Happens at -O3.  With -O2 we have a redundant register copy, so either way still a wasted instruction.  And there are other differences earlier in the function with -O2 vs. -O3.)

https://godbolt.org/z/jT7WaWeK8 - minimal test case. x86-64 and AArch64 trunk show basically identical code structure.  x86-64 gcc (Compiler-Explorer-Build) 12.0.0 20210603 and aarch64-unknown-linux-gnu-gcc (GCC) 12.0.0 20210524

void remove_chars_inplace(char *str, const unsigned char keep_lut[256])
{
  while(keep_lut[(unsigned char)*str]){     // can be an if() and still repro
    str++;            // keep_lut[0] is false
  }

  char *out = str;
  unsigned char c;   /* must be unsigned char for correctness. */
  do {
      c = *str++;
      unsigned char inc = keep_lut[c];  // unsigned long doesn't help
      *out = c;
      out += inc;   // inc=0 or 1 to let next char overwrite or not
    } while(c);
}

x86-64 asm:

remove_chars_inplace:
        jmp     .L8
.L3:                            # top of search loop for first char to remove
        addq    $1, %rdi
.L8:                            # loop entry point
        movzbl  (%rdi), %eax
        cmpb    $0, (%rsi,%rax)  # un-laminates and doesn't macro-fuse ...
        jne     .L3

        cmpb    $0, (%rdi)      # 2nd loop body can be skipped if *str == 0
                                # should be test %al,%al  - this char was already loaded.
        leaq    1(%rdi), %rax    # even -march=znver2 fails to move this earlier or later to allow cmp/je fusion.  (Intel won't macro-fuse cmp imm,mem / jcc)
        je      .L1

.L5:                             # TOP OF 2ND LOOP
        movzbl  (%rax), %edx
        movzbl  (%rax), %ecx     # redundant load of *str
        addq    $1, %rax
        movzbl  (%rsi,%rdx), %edx  # inc = lut[c]
        movb    %cl, (%rdi)
        addq    %rdx, %rdi       # out += inc
        testb   %cl, %cl
        jne     .L5            # }while(c != 0)
.L1:
        ret

IDK if it's interesting or not that the   cmpb $0, (%rdi)  is also a redundant load.  The first loop left *str, i.e. (%rdi), in EAX.  Putting the LEA between cmp and je (even with -march=znver2) is a separate missed optimization.  (unless that's working around Intel's JCC erratum)

With only -O2 instead of -O3, we get better asm in that part: it takes advantage of having the char in AL, and jumps into the middle of the next loop after xor-zeroing the `inc` variable.


Replacing    c = *str++;  with
      c = *str;
      str++;
results in a wasted register copy with trunk, instead of a 2nd load (on x86-64 and arm64).  Still a missed opt, but less bad.  GCC7 and earlier still do an extra load with either way of writing that.

Removing the first loop, or making its loop condition something like  *str && keep_lut[*str],  removes the problem entirely.  The CSE possibility is gone.  (Same even if we use lut[*(unsigned char*)str] - type-pun the pointer to unsigned char instead of casting the signed char value to unsigned char, on x86 where char is signed, but not on arm64 where char is unsigned.)

---

I didn't find any clear duplicates; the following are barely worth mentioning:
*  pr94442 looks like extra spilling, not just redundant loading.
*  pr97366 is due to vectors of different types, probably.
*  pr64319 needs runtime aliasing detection to avoid, unlike this.

The AArch64 version of this does seem to demo pr71942 (a useless  and x4, x2, 255 on an LDRB result) when you get it to copy a register instead of doing a 2nd load.


---


### compiler : `gcc`
### title : `PPCLE: Inefficient code for vec_xl_be(unsigned short *) < P9`
### open_at : `2021-06-05T18:19:16Z`
### last_modified_date : `2022-03-08T16:20:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100926
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `normal`
### contents :
Input:

vector unsigned short load_be(unsigned short *c)
{
   return vec_xl_be(0L, c);
}

creates:
_Z7load_bePt:
.LFB6:
        .cfi_startproc
.LCF6:
0:      addis 2,12,.TOC.-.LCF6@ha
        addi 2,2,.TOC.-.LCF6@l
        .localentry     _Z7load_bePt,.-_Z7load_bePt
        addis 9,2,.LC4@toc@ha
        lxvw4x 34,0,3
        addi 9,9,.LC4@toc@l
        lvx 0,0,9
        vperm 2,2,2,0
        blr


Optimal sequence:

vector unsigned short load_be_opt2(unsigned short *c)
{
   vector signed int vneg16;
   __asm__("vspltisw %0,-16":"=v"(vneg16));
   vector unsigned int tmp = vec_xl_be(0L, (unsigned int *)c);
   tmp = vec_rl(tmp, (vector unsigned int)vneg16);
   return (vector unsigned short)tmp;
}

creates:
_Z12load_be_opt2Pt:
.LFB8:
        .cfi_startproc
        lxvw4x 34,0,3
#APP
 # 77 "vec.C" 1
        vspltisw 0,-16
 # 0 "" 2
#NO_APP
        vrlw 2,2,0
        blr

rotate left (-16) = rotate right (+16) as only the 5 bits get evaluated.

Please note that the inline assembly is required, because vec_splats(-16) gets converted into a very inefficient constant generation.

vector unsigned short load_be_opt(unsigned short *c)
{
   vector signed int vneg16 = vec_splats(-16);
   vector unsigned int tmp = vec_xl_be(0L, (unsigned int *)c);
   tmp = vec_rl(tmp, (vector unsigned int)vneg16);
   return (vector unsigned short)tmp;
}

creates:
_Z11load_be_optPt:
.LFB7:
        .cfi_startproc
        li 9,48
        lxvw4x 34,0,3
        vspltisw 0,0
        mtvsrd 33,9
        xxspltw 33,33,1
        vsubuwm 0,0,1
        vrlw 2,2,0
        blr


---


### compiler : `gcc`
### title : `gcc fails to optimize less to min for SIMD code`
### open_at : `2021-06-06T09:39:04Z`
### last_modified_date : `2022-04-03T12:56:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100929
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.1.0`
### severity : `enhancement`
### contents :
Stand alone float - x86 example:
https://godbolt.org/z/vr3cjvY5G

Using a library x86 float, int, aarch64: https://godbolt.org/z/zPP48vzrq

less + blend or greater + blend should become min/max.


---


### compiler : `gcc`
### title : `[x86-64] Failure to optimize 2 32-bit stores converted to a 64-bit store into using movabs instead of loading from a constant`
### open_at : `2021-06-06T14:07:12Z`
### last_modified_date : `2023-09-21T09:21:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100931
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
void g(int *p) {
  *p = 2;
  p[1] = 2;
}

void h(long long *p)
{
    *p = 0x200000002;
}

g compiles to this on GCC on plenty of architectures:

g(int*):
  mov rax, QWORD PTR .LC0[rip]
  mov QWORD PTR [rdi], rax
  ret

.LC0:
  .long 2
  .long 2

h is equivalent to g (non-withstanding aliasing) and instead compiles to this:

h(long long*):
  movabs rax, 8589934594
  mov QWORD PTR [rdi], rax
  ret

g has been compiled differently from h since GCC 10. 

I'm somewhat doubtful about filing this bug actually, I personally think that h will be faster and that g is simply a regression from GCC 9, but I can't really be sure there isn't some architecture-specific reasoning to use a separate constant, especially since this transformation seems to only occur on specific architectures (generic, core2, nehalem, westmere, sandybridge, ivybridge, haswell, broadwell, znver1, znver2 and znver3)


---


### compiler : `gcc`
### title : `ccmp is not used if the value is used later`
### open_at : `2021-06-07T07:51:59Z`
### last_modified_date : `2022-02-03T08:59:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100942
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
void foo(void);
int f1(int a, int b)
{
  int c = a == 0 || b == 0;
  if (c) foo();
  return c;
}
------ CUT -----
Right now we produce at -O2:
        stp     x29, x30, [sp, -32]!
        cmp     w0, 0
        mov     x29, sp
        str     x19, [sp, 16]
        cset    w19, eq
        cmp     w1, 0
        cset    w0, eq
        orr     w19, w19, w0
        cbnz    w19, .L8
        mov     w0, w19
        ldr     x19, [sp, 16]
        ldp     x29, x30, [sp], 32
        ret

Notice how we have cmp/cset/cmp/cset/orr.
This is a (much) reduced testcase from Xalan in SPEC 2k6.


---


### compiler : `gcc`
### title : `vec_duplicate<mode> leads to worse code`
### open_at : `2021-06-07T20:08:24Z`
### last_modified_date : `2021-09-17T06:48:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100951
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
After I added

;; Modes handled by broadcast patterns.
(define_mode_iterator INT_BROADCAST_MODE
  [(V64QI "TARGET_AVX512F") (V32QI "TARGET_AVX") V16QI
   (V32HI "TARGET_AVX512F") (V16HI "TARGET_AVX") V8HI
   (V16SI "TARGET_AVX512F") (V8SI "TARGET_AVX") V4SI
   (V8DI "TARGET_AVX512F") (V4DI "TARGET_64BIT") V2DI])

;; Broadcast from an integer.  NB: Enable broadcast only if we can move
;; from GPR to SSE register directly.  */
(define_expand "vec_duplicate<mode>"
  [(set (match_operand:INT_BROADCAST_MODE 0 "register_operand")
        (vec_duplicate:INT_BROADCAST_MODE
          (match_operand:<ssescalarmode> 1 "nonimmediate_operand")))]
  "TARGET_SSE2 && TARGET_INTER_UNIT_MOVES_TO_VEC"
{
  ix86_expand_integer_vec_duplicate (operands);
  DONE; 
})

to x86 backend, I got

[hjl@gnu-cfl-2 gcc]$ cat /tmp/x.c 
typedef short __attribute__((__vector_size__ (8 * sizeof (short)))) V;
V v, w;

void
foo (void)
{
  w = __builtin_shuffle (v != v, 0 < (V) {}, (V) {192} >> 5);
}
[hjl@gnu-cfl-2 gcc]$ ./xgcc -B./ -S /tmp/x.c
[hjl@gnu-cfl-2 gcc]$ cat x.s
	.file	"x.c"
	.text
	.globl	v
	.bss
	.align 16
	.type	v, @object
	.size	v, 16
v:
	.zero	16
	.globl	w
	.align 16
	.type	w, @object
	.size	w, 16
w:
	.zero	16
	.text
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	movl	$0, %eax
	movd	%eax, %xmm0
	punpcklwd	%xmm0, %xmm0
	pshufd	$0, %xmm0, %xmm0
	movaps	%xmm0, w(%rip)
	nop
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 12.0.0 20210607 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-2 gcc]$ 

since middld-end leaves us with an unfolded constant CTOR.


---


### compiler : `gcc`
### title : `varargs causes extra stores to/from stack`
### open_at : `2021-06-08T00:50:29Z`
### last_modified_date : `2023-03-24T12:18:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100955
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
#include <stdarg.h>
int vfprintf1 (const char *, va_list);
int
__fprintf1 (const char *format, ...)
{
  va_list arg;
  int done;

  va_start (arg, format);
  done = vfprintf1 (format, arg);
  va_end (arg);

  return done;
}
---- CUT ---
Currently at -O2 we produce:
        stp     x29, x30, [sp, -272]!
        mov     w9, -56
        mov     w8, -128
        mov     x29, sp
        add     x10, sp, 208
        add     x11, sp, 272
        stp     x11, x11, [sp, 48]
        str     x10, [sp, 64]
        stp     w9, w8, [sp, 72]
        str     q0, [sp, 80]
        ldp     q0, q16, [sp, 48]
        str     q1, [sp, 96]
        str     q2, [sp, 112]
        stp     q0, q16, [sp, 16]
        str     q3, [sp, 128]
        str     q4, [sp, 144]
        str     q5, [sp, 160]
        str     q6, [sp, 176]
        str     q7, [sp, 192]
        stp     x1, x2, [sp, 216]
        add     x1, sp, 16
        stp     x3, x4, [sp, 232]
        stp     x5, x6, [sp, 248]
        str     x7, [sp, 264]
        bl      vfprintf1

Notice how we store to arg (va_list) and then do a copy of arg (va_list) to pass to the vfprintf1.
This is due to
__builtin_va_start (&arg, 0);

If we had arg = __builtin_va_start_internal(); and expanded that instead, there would be no needed.


---


### compiler : `gcc`
### title : `two_value_replacement should move to match.pd`
### open_at : `2021-06-08T06:42:23Z`
### last_modified_date : `2023-04-28T14:50:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100958
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
two_value_replacement can be fully implemented in match.pd now.
So it should move.  I did not do it originally but it.

Comment from two_value_replacement :
/* Optimize
   # x_5 in range [cst1, cst2] where cst2 = cst1 + 1
   if (x_5 op cstN) # where op is == or != and N is 1 or 2
     goto bb3;
   else
     goto bb4;
   bb3:
   bb4:
   # r_6 = PHI<cst3(2), cst4(3)> # where cst3 == cst4 + 1 or cst4 == cst3 + 1

   to r_6 = x_5 + (min (cst3, cst4) - cst1) or
   r_6 = (min (cst3, cst4) + cst1) - x_5 depending on op, N and which
   of cst3 and cst4 is smaller.  */


---


### compiler : `gcc`
### title : `Poor optimization of AVR code when using structs in __flash`
### open_at : `2021-06-08T09:42:23Z`
### last_modified_date : `2023-01-25T18:39:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100962
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `5.4.0`
### severity : `normal`
### contents :
Example code here: https://godbolt.org/z/1hnPoGdTd

In this code a const __flash struct holds some data used to initialize peripherals. Line 59 is the definition of the struct.

With the __flash attribute the generated AVR assembly uses the X register as a pointer to the peripheral. The X pointer lacks displacement with LDI so rather inefficient code is generated, e.g.

141     channels[ch].dma.ch->TRFCNT = BUFFER_SIZE;
142     channels[ch].dma.ch->REPCNT = 0;

        ldi r18,lo8(26)
        ldi r19,0
        adiw r26,4
        st X+,r18
        st X,r19
        sbiw r26,4+1
        adiw r26,6
        st X,__zero_reg__
        sbiw r26,6

Removing the __flash attribute produces much better code, with the Z register used with displacement.

The issue appears to be because the other pointer register that supports displacement, Y, is used for the stack so unavailable. Introducing the need to use LPM instructions to read data from flash seems to cause Z not to be used for the peripheral, with X used instead. Z is used only for LPM.

The best possible optimisation here seems to be to read all values needed from flash first, and then switch to using Z as a pointer to the peripheral.


---


### compiler : `gcc`
### title : `[AArch64] __builtin_roundeven[f] is not inlined`
### open_at : `2021-06-08T12:28:49Z`
### last_modified_date : `2021-10-20T12:41:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100966
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.3.1`
### severity : `normal`
### contents :
The new roundeven builtins are not inlined on AArch64:

double f1 (double x)
{
  return __builtin_roundeven (x);
}

float f2 (float x)
{
  return __builtin_roundevenf (x);
}

f1:
        b       roundeven
f2:
        b       roundevenf

These should use the frintn instructions instead of calling the GLIBC functions.


---


### compiler : `gcc`
### title : `gcc does not optimise based on knowing that `_mm256_movemask_ps` returns less than 255`
### open_at : `2021-06-08T17:33:01Z`
### last_modified_date : `2021-06-09T06:43:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=100973
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Options: -O3 -std=c++20 -DNDEBUG -mavx

Code:

```
#include <immintrin.h>

int masking_should_evaporate(__m256 values) {
  int top_bits = _mm256_movemask_ps(values);
  top_bits &= 255;
  return top_bits;
}
```

Godbolt: https://gcc.godbolt.org/z/a81qPWcon


For this code top_bits &= 255 does not actually do anything. Clang can optimise based on that:

```
       vmovmskps       eax, ymm0
       vzeroupper
       ret
```

It comes from real code.


---


### compiler : `gcc`
### title : `GCC should consider using PLI/SLDI/PADDI to load up 64-bit constants on power10`
### open_at : `2021-06-10T15:48:45Z`
### last_modified_date : `2022-03-08T16:20:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101019
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC should consider using a sequence of PLI, SLDI, and PADDI to load up 64-bit constants on power10.

For example:
long foo_2 (void) { return (1L << 53) | (1L << 35) | (1L << 30) | (1L << 2); }

Generates:
        lis 3,0x20
        ori 3,3,0x8
        sldi 3,3,32
        oris 3,3,0x4000
        ori 3,3,0x4

when it could generate:
        pli 3,2097160
        sldi 3,3,32
        paddi 3,3,1073741828


---


### compiler : `gcc`
### title : `PSHUFB is emitted instead of PSHUFD, PSHUFLW and PSHUFHW with -msse4`
### open_at : `2021-06-10T18:07:14Z`
### last_modified_date : `2021-06-13T19:51:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101021
### status : `RESOLVED`
### tags : `missed-optimization, ssemmx`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
typedef char S;
typedef S VV __attribute__((vector_size(16 * sizeof(S))));

VV ref_perm_pshufd (VV x, VV y)
{
  return __builtin_shuffle (x, y, (VV) { 8,9,10,11, 8,9,10,11, 8,9,10,11, 12,13,14,15 });
}

VV ref_perm_pshuflw (VV x)
{
  return __builtin_shuffle (x, (VV) { 0,1, 2,3, 2,3, 6,7, 8,9,10,11,12,13,14,15 });
}

VV ref_perm_pshufhw (VV x)
{
  return __builtin_shuffle (x, (VV) { 0,1,2,3,4,5,6,7, 8,9, 10,11, 10,11, 14,15 });
--cut here--

compiles with -O2 -msse2 to:

<ref_perm_pshufd>:

     pshufd $0xea,%xmm0,%xmm0
     retq   

<ref_perm_pshuflw>:

     pshuflw $0xd4,%xmm0,%xmm0
     retq   

<ref_perm_pshufhw>:

     pshufhw $0xd4,%xmm0,%xmm0
     retq   

Using -msse4 (or higher ISA), the compiler is too eager to emit less optimal PSHUFB:

<ref_perm_pshufd>:

     pshufb 0x0(%rip),%xmm0
     retq   

<ref_perm_pshuflw>:

     pshufb 0x0(%rip),%xmm0
     retq   

<ref_perm_pshufhw>:

     pshufb 0x0(%rip),%xmm0
     retq


---


### compiler : `gcc`
### title : `Missed min expression at phiopt1`
### open_at : `2021-06-11T04:00:23Z`
### last_modified_date : `2023-06-01T02:07:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101024
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
unsigned f(int nunits)
{
  int units_per = 32 / (8);


  unsigned int chunk_nunits = ((nunits) < (units_per) ? (nunits) : (units_per));
  return chunk_nunits;
}
-------- CUT ------
Compile with the C++ front-end and -O2 (note C++ is needed as fold won't convert ?: into min/max expression due to lvalue requirements).
You will notice this is not caught by phiopt1 even though it should be while it is caught now (on the trunk) by phiopt2 (match-and-simplify).  That is the minmax_replacement does not handle the above case but match does.

Note this shows up in read code (GCC) in assemble_real (varasm.c). There are two cases of this in assemble_real but only one of the two is caught in phiopt2 though because there looks like some jump threading that had happened before that causes other issues.

I am filing this as a bug only so I can keep track of moving minmax_replacement to match-and-simplify.


---


### compiler : `gcc`
### title : `Some simple fold_cond_expr_with_comparison with CMP 0 is not simplified if expanded`
### open_at : `2021-06-12T10:27:30Z`
### last_modified_date : `2021-07-05T20:43:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101039
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take at -O2:
int f0(int A)
{
//     A == 0? A : -A    same as -A
  if (A == 0)  return A;
  return -A;
}

int f1(int A)
{
//     A != 0? A : -A    same as A
  if (A != 0)  return A;
  return -A;
}
int f2(int A)
{
//     A >= 0? A : -A    same as abs (A)
  if (A >= 0)  return A;
  return -A;
}
int f3(int A)
{
//     A > 0?  A : -A    same as abs (A)
  if (A > 0)  return A;
  return -A;
}
int f4(int A)
{
//     A <= 0? A : -A    same as -abs (A)
  if (A <= 0)  return A;
  return -A;
}
int f5(int A)
{
//     A < 0?  A : -A    same as -abs (A)
  if (A < 0)  return A;
  return -A;
}

------ CUT -------
Only f1-f3 are optimized at the gimple level.  f0, f4, f5 are not.
With -Dint=float -Ofast, only f0 is not optimized but note f1 takes until *.phiopt3 be optimized; and unlike int, f4 and f5 are optimized in phiopt2.

Note phiopt1 is still confused by predictor but I think there might be another bug for that.


---


### compiler : `gcc`
### title : `z13: Inefficient handling of vector register passed to function`
### open_at : `2021-06-12T13:16:04Z`
### last_modified_date : `2021-06-12T13:16:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101041
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `normal`
### contents :
#include <vecintrin.h>
vector unsigned long long mul64(vector unsigned long long a, vector unsigned long long b)
{
   return a * b;
}

creates:
_Z5mul64Dv2_yS_:
.LFB9:
        .cfi_startproc
        ldgr    %f4,%r15
        .cfi_register 15, 18
        lay     %r15,-192(%r15)
        .cfi_def_cfa_offset 352
        vst     %v24,160(%r15),3
        vst     %v26,176(%r15),3
        lg      %r2,160(%r15)
        lg      %r1,176(%r15)
        lgr     %r4,%r2
        lg      %r0,168(%r15)
        lgr     %r2,%r1
        lg      %r1,184(%r15)
        lgr     %r5,%r0
        lgr     %r3,%r1
        vlvgp   %v2,%r4,%r5
        vlvgp   %v0,%r2,%r3
        vlgvg   %r4,%v2,0
        vlgvg   %r1,%v2,1
        vlgvg   %r2,%v0,0
        vlgvg   %r3,%v0,1
        msgr    %r2,%r4
        msgr    %r1,%r3
        lgdr    %r15,%f4
        .cfi_restore 15
        .cfi_def_cfa_offset 160
        vlvgp   %v24,%r2,%r1
        br      %r14

Store to stack of v24,v26, then lg+lgr for all 4 parts, then constructing new vector register v0 and v2 and then extract the 4 elements again using vlgvg.

Expected 4 * vlgvg + 2 * msgr + vlvgp


---


### compiler : `gcc`
### title : `Bogus -Wstringop-overread with 11.1.0 and -O1 because of a call to printf _after_ assertions`
### open_at : `2021-06-12T15:34:48Z`
### last_modified_date : `2022-10-23T00:21:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101042
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
Created attachment 50989
reproducer.c

Hi,

It looks like printf-ing a pointer after running assertions on it throws gcc 11 off:

#include <assert.h>
#include <stdio.h>
#include <string.h>
#include <stdlib.h>

enum field_type {
    INTEGER,
    STRING,
};

struct field {
    enum field_type type;

    union {
        int integer;
        const char *string;
    };
};

int
main()
{
    const struct field FIELD = {
        .type = INTEGER,
        .integer = 1,
    };
    struct field *field = calloc(1, sizeof(struct field));
    assert(field);

    assert(field->type == FIELD.type);
    printf("field = %p\n", field);
    switch (field->type) {
    case STRING:
        assert(strcmp(field->string, FIELD.string) == 0);
        break;
    default:
        break;
    }

    free(field);
    return 0;
}

$ gcc -Werror -O0 reproducer.c # <== Runs fine
$ gcc -Werror -O1 reproducer.c
In file included from reproducer.c:1:
reproducer.c: In function ‘main’:
reproducer.c:34:16: error: ‘strcmp’ reading 1 or more bytes from a region of size 0 [-Werror=stringop-overread]
   34 |         assert(strcmp(field->string, FIELD.string) == 0);
      |                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cc1: all warnings being treated as errors

Any optimization level above 0 yields the warning.

If you remove the printf() statement, or place it before one of the two assert(), the warning goes away.
Downgrading to 10.2.0 also fixes the issue.


---


### compiler : `gcc`
### title : `-ABS(A) produces two neg instructions`
### open_at : `2021-06-13T02:37:07Z`
### last_modified_date : `2021-07-01T09:24:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101044
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Take:
int f4(int A)
{
  return A <= 0? A : -A;
}
int f5(int A)
{
return A < 0?  A : -A;
}
int f2(int A)
{
  A = A < 0 ? -A : A;
  return -A;
}
---- CUT ---
On x86_64 we get two neg for each of these functions now, we used to get none sometime ago.
Right now we get for each of them:
        movl    %edi, %eax
        negl    %eax
        cmovs   %edi, %eax
        negl    %eax
----- CUT ---
Note aarch64 seems to be fine:
        cmp     w0, 0
        csneg   w0, w0, w0, lt


---


### compiler : `gcc`
### title : `std::variant:  missed optimization in std::visit() on more than one variant`
### open_at : `2021-06-13T17:19:44Z`
### last_modified_date : `2021-09-13T00:25:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101049
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.1.0`
### severity : `normal`
### contents :
// https://godbolt.org/z/T8f89fq1z
// --------------------------------------
#include <variant>

struct Base {};
struct A : Base { int a; };
struct B : Base {};
struct C : Base {};
struct D : Base {};

using V = std::variant<A, B, C, D>;

template<class... Ts> struct overloaded : Ts... { using Ts::operator()...; };

bool operator==(const V& v1, const V& v2)
{
    return std::visit(overloaded{
        [](const A& x, const A& y) { return x.a == y.a; },
        [&](const Base&, const Base&) { return v1.index() == v2.index(); }
    }, v1, v2);
}
// --------------------------------------

The call to std::visit() is implemented using a table containing 4x4 = 16 function pointers. However 15 out of these 16 sub-functions are identical. It would be nice if these would be shared.

Also (before sharing) in these 16 sub-functions the value of a sub-expression like 'v1.index()' could be known during compile-time. It would also be nice if the compiler could exploit this knowledge.

So ideally I'd like the compiler to produce only 3 different sub-functions:
        [](const A& x, const A& y) { return x.a == y.a; },
        []() { return true; }
        []() { return false; }
And then fill-in the table with 16 function-pointers to the appropriate of these 3 functions.

Thanks.


---


### compiler : `gcc`
### title : `Range comparisons with more significant bits constant are not optimised into masked equality tests`
### open_at : `2021-06-13T17:25:38Z`
### last_modified_date : `2021-06-14T00:59:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101050
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 50995
Sample functions f and g

Attached sample functions f and g have identical behaviour, but GCC optimises them differently at -O3. The former is optimised into an arithmetic offset and unsigned comparison, while the latter compiles into bit masking and an equality test, as written. I presume it would be advantageous to reduce the former to the latter whenever possible (that is, to simplify (C <= x && x < C + (1 << N)) where (C & ((1 << N) - 1)) == 0 into (x & ~(1 << N)) == C), since the operations involved are slightly weaker. The Other Compiler does this.

And yes, the values in the example are not accidental. This comes up in Unicode processing, so I think it’s worth the trouble.


---


### compiler : `gcc`
### title : `v4sf reduction not optimal`
### open_at : `2021-06-14T09:30:02Z`
### last_modified_date : `2021-09-27T07:51:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101059
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
When expanding

float f (v4df v)
{
  _11 = .REDUC_PLUS (v_7(D)); [tail call]
  return _11;
}

thus exercising

(define_expand "reduc_plus_scal_<mode>"
 [(plus:REDUC_SSE_PLUS_MODE
   (match_operand:<ssescalarmode> 0 "register_operand")
   (match_operand:REDUC_SSE_PLUS_MODE 1 "register_operand"))]
 ""
{
  rtx tmp = gen_reg_rtx (<MODE>mode);
  ix86_expand_reduc (gen_add<mode>3, tmp, operands[1]);
  emit_insn (gen_vec_extract<mode><ssescalarmodelower> (operands[0], tmp,
                                                        const0_rtx));
  DONE;
})

we see with -Ofast

f:
.LFB0:
        .cfi_startproc
        movaps  %xmm0, %xmm1
        movhlps %xmm0, %xmm1
        addps   %xmm0, %xmm1
        movaps  %xmm1, %xmm0
        shufps  $85, %xmm1, %xmm0
        addps   %xmm1, %xmm0
        ret

https://stackoverflow.com/questions/6996764/fastest-way-to-do-horizontal-sse-vector-sum-or-other-reduction

suggests this is a regression from GCC 5 which could reuse a shuffle,
avoiding one movaps

    # gcc 5.3 -O3:  looks optimal
    movaps  xmm1, xmm0     # I think one movaps is unavoidable, unless we have a 2nd register with known-safe floats in the upper 2 elements
    shufps  xmm1, xmm0, 177
    addps   xmm0, xmm1
    movhlps xmm1, xmm0     # note the reuse of shuf, avoiding a movaps
    addss   xmm0, xmm1

but it also suggests that with SSE3 we can use movshdup which is both
fast and has a write-only destination:

    movshdup    xmm1, xmm0
    addps       xmm0, xmm1
    movhlps     xmm1, xmm0
    addss       xmm0, xmm1

most of ix86_expand_reduc is dead now since expanders narrow down to SSE
before dispatching to it.


---


### compiler : `gcc`
### title : `calloc result not treated as zeroed out`
### open_at : `2021-06-15T00:44:59Z`
### last_modified_date : `2021-06-15T08:23:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101074
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
GCC folds malloc + bzero (or malloc + memset zero) to calloc and "knows" that the returned memory is zeroed out.  But it doesn't seem to understand that the memory returned from a call specifically made to calloc is also zeroed out.  In the test case below both functions should be optimized equivalently but only the first produces optimal code, the second is suboptimal.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
struct A { int i; };

void* f (void)
{
  struct A *p = __builtin_malloc (sizeof *p);
  __builtin_memset (p, 0, sizeof *p);
  if (p->i)   // folded to false
    __builtin_abort ();
  return p;
}

void* g (void)
{
  struct A *p = __builtin_calloc (1, sizeof *p);
  if (p->i)   // not folded
    __builtin_abort ();
  return p;
}

;; Function f (f, funcdef_no=0, decl_uid=1945, cgraph_uid=1, symbol_order=0)

void * f ()
{
  struct A * p;

  <bb 2> [local count: 1073741824]:
  p_3 = __builtin_calloc (4, 1); [tail call]
  return p_3;

}



;; Function g (g, funcdef_no=1, decl_uid=1949, cgraph_uid=2, symbol_order=1)

void * g ()
{
  struct A * p;
  int _1;

  <bb 2> [local count: 1073741824]:
  p_4 = __builtin_calloc (1, 4);
  _1 = p_4->i;
  if (_1 != 0)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return p_4;

}


---


### compiler : `gcc`
### title : `RTL Combine pass won't generate sign_extnd RTX in some senario`
### open_at : `2021-06-15T03:37:27Z`
### last_modified_date : `2023-06-02T05:25:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101076
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
I found a little bug in the combine pass.
When I use 64bit gcc to compile the below test.c, I found some issue
cat test.c
int test(int a)

{   return (a << 16) >> 16; }

In the combine pass, the log shows
Trying 6 -> 7:
    6: r76:SI=r78:DI#0<<0x10
      REG_DEAD r78:DI
    7: r75:SI=r76:SI>>0x10
      REG_DEAD r76:SI
Failed to match this instruction:
(set (reg:SI 75)
    (ashiftrt:SI (subreg:SI (ashift:DI (reg:DI 78)
                (const_int 16 [0x10])) 0)
        (const_int 16 [0x10])))

Then I debugged and checkd the gcc code and fonud some issue in the function make_compound_operation_int which is in the combine.c, 

    case ASHIFTRT:
      lhs = XEXP (x, 0);
      rhs = XEXP (x, 1);

      /* If we have (ashiftrt (ashift foo C1) C2) with C2 >= C1,
	 this is a SIGN_EXTRACT.  */
      if (CONST_INT_P (rhs)
	  && GET_CODE (lhs) == ASHIFT
	  && CONST_INT_P (XEXP (lhs, 1))
	  && INTVAL (rhs) >= INTVAL (XEXP (lhs, 1))
	  && INTVAL (XEXP (lhs, 1)) >= 0
	  && INTVAL (rhs) < mode_width)
	{
	  new_rtx = make_compound_operation (XEXP (lhs, 0), next_code);
	  new_rtx = make_extraction (mode, new_rtx,
				     INTVAL (rhs) - INTVAL (XEXP (lhs, 1)),
				     NULL_RTX, mode_width - INTVAL (rhs),
				     code == LSHIFTRT, 0, in_code == COMPARE);
	  break;
	}

      /* See if we have operations between an ASHIFTRT and an ASHIFT.
	 If so, try to merge the shifts into a SIGN_EXTEND.  We could
	 also do this for some cases of SIGN_EXTRACT, but it doesn't
	 seem worth the effort; the case checked for occurs on Alpha.  */

      if (!OBJECT_P (lhs)
	  && ! (GET_CODE (lhs) == SUBREG
		&& (OBJECT_P (SUBREG_REG (lhs))))
	  && CONST_INT_P (rhs)
	  && INTVAL (rhs) >= 0
	  && INTVAL (rhs) < HOST_BITS_PER_WIDE_INT
	  && INTVAL (rhs) < mode_width
	  && (new_rtx = extract_left_shift (mode, lhs, INTVAL (rhs))) != 0)
	new_rtx = make_extraction (mode, make_compound_operation (new_rtx,
								  next_code),
				   0, NULL_RTX, mode_width - INTVAL (rhs),
				   code == LSHIFTRT, 0, in_code == COMPARE);

      break;

The issue code is "(new_rtx = extract_left_shift (mode, lhs, INTVAL (rhs))) != 0)", this part wants to extract_left_shift information, but the second input parameter lhs is not right, it's still SUBREG, but it should  transmit  ashift part into the function. So the second parameter "lhs" should change to "XEXP (lhs, 0)".
After change this issue, we can get the right combine result(sign_extend):
Trying 6 -> 7:
    6: r138:SI=r140:DI#0<<0x10
      REG_DEAD r140:DI
    7: r137:SI=r138:SI>>0x10
      REG_DEAD r138:SI
Successfully matched this instruction:
(set (reg:SI 137)
    (sign_extend:SI (subreg:HI (reg:DI 140) 0)))
allowing combination of insns 6 and 7
original costs 4 + 4 = 8
replacement cost 8
deferring deletion of insn with uid = 6.
modifying insn i3     7: r137:SI=sign_extend(r140:DI#0)
      REG_DEAD r140:DI
deferring rescan insn with uid = 7.


---


### compiler : `gcc`
### title : `AVX512 VPMOV instruction should be used to downconvert vectors`
### open_at : `2021-06-16T14:45:28Z`
### last_modified_date : `2023-04-26T06:52:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101096
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Following testcases should use VPMOV downconvert instruction with AVX512VL:

void
foo (unsigned short* p1, unsigned short* p2, char* __restrict p3)
{
    for (int i = 0 ; i != 16; i++)
     p3[i] = p1[i] + p2[i];
     return;
}

void
foo1 (unsigned int* p1, unsigned int* p2, short* __restrict p3)
{
    for (int i = 0 ; i != 8; i++)
     p3[i] = p1[i] + p2[i];
     return;
}

gcc -O3 -mavx512vl:

foo:
        vpbroadcastw    .LC1(%rip), %xmm0
        vpand   16(%rsi), %xmm0, %xmm2
        vpand   (%rsi), %xmm0, %xmm1
        vpackuswb       %xmm2, %xmm1, %xmm1
        vpand   (%rdi), %xmm0, %xmm2
        vpand   16(%rdi), %xmm0, %xmm0
        vpackuswb       %xmm0, %xmm2, %xmm0
        vpaddb  %xmm0, %xmm1, %xmm0
        vmovdqu %xmm0, (%rdx)
        ret

foo1:
        vpbroadcastd    .LC3(%rip), %xmm0
        vpand   16(%rsi), %xmm0, %xmm2
        vpand   (%rsi), %xmm0, %xmm1
        vpackusdw       %xmm2, %xmm1, %xmm1
        vpand   (%rdi), %xmm0, %xmm2
        vpand   16(%rdi), %xmm0, %xmm0
        vpackusdw       %xmm0, %xmm2, %xmm0
        vpaddw  %xmm0, %xmm1, %xmm0
        vmovdqu %xmm0, (%rdx)
        ret


---


### compiler : `gcc`
### title : `Vectorizer is too eager to use vec_unpack`
### open_at : `2021-06-16T15:13:47Z`
### last_modified_date : `2021-07-01T02:53:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101097
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Following two testcases:

void
foo (unsigned short* p1, unsigned short* p2, int* __restrict p3)
{
    for (int i = 0 ; i != 8; i++)
     p3[i] = p1[i] + p2[i];
     return;
}

void
bar (unsigned short* p1, unsigned short* p2, int* __restrict p3)
{
    for (int i = 0 ; i != 4; i++)
     p3[i] = p1[i] + p2[i];
     return;
}

compile with -O3 -mavx2 to:

foo:
        vmovdqu (%rdi), %xmm1
        vmovdqu (%rsi), %xmm0
        vpmovzxwd       %xmm1, %xmm3
        vpsrldq $8, %xmm1, %xmm1
        vpmovzxwd       %xmm0, %xmm2
        vpsrldq $8, %xmm0, %xmm0
        vpmovzxwd       %xmm1, %xmm1
        vpaddd  %xmm3, %xmm2, %xmm2
        vpmovzxwd       %xmm0, %xmm0
        vmovdqu %xmm2, (%rdx)
        vpaddd  %xmm1, %xmm0, %xmm0
        vmovdqu %xmm0, 16(%rdx)
        ret

bar:
        vpmovzxwd       (%rsi), %xmm1
        vpmovzxwd       (%rdi), %xmm0
        vpaddd  %xmm1, %xmm0, %xmm0
        vmovdqu %xmm0, (%rdx)
        ret

However, with "foo" the vec_unpack* named patterns somehow interfere with the compilation, preventing the compiler to generate code, similar to "bar", but with %ymm registers.

Disabling vec_unpacku_hi_<mode> and vec_unpacku_lo_<mode> patterns in sse.md results in the optimal code for foo:

foo:
        vpmovzxwd       (%rsi), %ymm0
        vpmovzxwd       (%rdi), %ymm1
        vpaddd  %ymm1, %ymm0, %ymm0
        vmovdqu %ymm0, (%rdx)
        vzeroupper
        ret


---


### compiler : `gcc`
### title : `Unable to remove double byteswap in fast path`
### open_at : `2021-06-20T09:37:10Z`
### last_modified_date : `2021-06-27T11:23:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101139
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `normal`
### contents :
The following code is reduced from a real interpreter:

extern void (*a[])();
int d, e, h, l;
typedef struct {
  char ab;
} f;
f g;
short i();
short m68ki_read_imm_16() {
  short j, k;
  int b = d;
  f f = g;
  if (b < h)
    return __builtin_bswap16((&f.ab)[0]);
  k = i();
  short c = k;
  j = __builtin_bswap16(c);
  return j;
}
int b() {
  short m;
  do {
    m = m68ki_read_imm_16();
    short c = m;
    l = __builtin_bswap16(c);
    a[l]();
  } while (e);
  return e;
}

Compiling with arm-linux-gnueabihf-gcc-10 -O2 yields this interesting sequence in the function:

	b	.L11
.L15:
	ldrb	r3, [r5, #8]	@ zero_extendqisi2
	rev16	r3, r3
	uxth	r3, r3
.L10:
	rev16	r3, r3
	uxth	r3, r3

The original code intention was to have a reusable function that returned in big-endian, but that a specific use of it would be able to ignore endianness into a table lookup, removing the double-swap entirely. GCC can normally do that, but it seems that the branch in m68ki_read_imm_16() somehow gets in the way. Just to be clear, I expect zero rev16 instructions altogether in b() when m68ki_read_imm_16() is inlined.

The problem is not ARM-specific; x86 shows a similar problematic sequence:

	leaq	a(%rip), %rbx
	jmp	.L11
	.p2align 4,,10
	.p2align 3
.L15:
	movsbw	g(%rip), %ax
	rolw	$8, %ax
.L10:
	rolw	$8, %ax
	movzwl	%ax, %edx

Also verified with

gcc version 12.0.0 20210527 (experimental) [master revision 262e75d22c3:7bb6b9b2f47:9d3a953ec4d2695e9a6bfa5f22655e2aea47a973] (Debian 20210527-1)


---


### compiler : `gcc`
### title : `[11 regression] Regression due to  supporting bitwise operators on AVX512 masks.`
### open_at : `2021-06-21T03:25:25Z`
### last_modified_date : `2021-07-19T12:02:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101142
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Created attachment 51040
test.cpp

options to reproduce regression: g++ byteswap.cpp test.cpp -march=native -O3 -DDTYPE32

The regression is due to r11-2796-g388cb292a94f98a276548cd6ce01285cf36d17df which supports bitwise operator for avx512 masks. In byteswap.cpp there're a lot of bitwise operations, the register pressure is very high, so LRA allocate some bitwise operations to mask registers to avoid spills, the problem is mask bitwise instructions has 1/4 throught of those gpr versions which causes the regression.


---


### compiler : `gcc`
### title : `niter analysis fails for until-wrap condition`
### open_at : `2021-06-21T08:59:46Z`
### last_modified_date : `2022-11-19T22:44:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101145
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
unsigned foo(unsigned val, unsigned start)
{
  unsigned cnt = 0;
  for (unsigned i = start; i > val; ++i)
    cnt++;
  return cnt;
}

fails niter analysis.  At least for the following variant
the number of iterations should be about UINT_MAX - start.

unsigned foo(unsigned val, unsigned start)
{
  unsigned cnt = 0;
  if (start > val)
    {
      unsigned i = start;
      do
        {
          cnt++;
        }
      while (++i > val);
    }
  return cnt;
}


---


### compiler : `gcc`
### title : `[11/12 Regression] null pointer dereference false positive disappears when compiling an additional function`
### open_at : `2021-06-21T11:48:22Z`
### last_modified_date : `2023-05-29T10:05:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101150
### status : `NEW`
### tags : `diagnostic, missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
Created attachment 51042
source code, plus the three preprocessed versions

I'm observing the following behavior with gcc-snapshot on Debian
unstable as well as when using "x86-64 gcc (trunk)" and "x86-64 gcc
(11.1)" on Godbolt.  This false positive is not reported by gcc 10.

% g++ --version | sed 1q
g++ (Debian 20210527-1) 12.0.0 20210527 (experimental) [master revision 262e75d22c3:7bb6b9b2f47:9d3a953ec4d2695e9a6bfa5f22655e2aea47a973]

% cat foo.cc
#include <vector>

#ifdef FOO
void foo(const std::vector<int>& other)
{
  std::vector<int> v;
  std::size_t sz = other.size();
  v.resize(sz);
  int i = 0;
  for (int o: other)
    v[i++] = o;
}
#endif

#ifdef BAR
void bar(const std::vector<int>& other)
{
  std::vector<int> v;
  unsigned sz = other.size();
  v.resize(sz);
  int i = 0;
  for (int o: other)
    v[i++] = o;
}
#endif

% g++ -O3 -Wnull-dereference -c foo.cc -DBAR
% g++ -O3 -Wnull-dereference -c foo.cc -DFOO -DBAR
% g++ -O3 -Wnull-dereference -c foo.cc -DFOO
In function 'void foo(const std::vector<int>&)':
cc1plus: warning: potential null pointer dereference [-Wnull-dereference]

The two functions differ only by the type of sz, and the warning
occurs only if foo() is compiled but bar() is not.


I *believe* the warning comes from the fact that if sz is 0, the data
pointer of v will still be nullptr after resize(), and that would
render v[i++]=o invalid.  However if sz is 0, the loop will not do
any iteration, so that's a false positive.

However I can't explain
- why changing size_t into unsigned makes the warning go away,
- why compiling the two functions makes the warning go away.

I was expecting the diagnostics about foo() to be independent of the presence of bar(), and I was expecting to get the same diagnostics for both functions (preferably none, but I understand it's only a "potential" issue)


---


### compiler : `gcc`
### title : `[10 regression] test case gcc.target/powerpc/fold-vec-extract-char.p7.c fails after r10-9880`
### open_at : `2021-06-22T21:03:37Z`
### last_modified_date : `2023-10-19T05:50:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101169
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.3.1`
### severity : `normal`
### contents :
g:eb13f3f81d56910626529af52e03e74770ffca98, r10-9880
make  -k check-gcc RUNTESTFLAGS="powerpc.exp=gcc.target/powerpc/fold-vec-extract-char.p7.c"
FAIL: gcc.target/powerpc/fold-vec-extract-char.p7.c scan-assembler-times \\maddi\\M 6
# of expected passes		7
# of unexpected failures	1


Previously there were 6 addi instructions and now there are 9.  This may have a performance impact.


commit eb13f3f81d56910626529af52e03e74770ffca98
Author: Vladimir N. Makarov <vmakarov@redhat.com>
Date:   Tue Jan 12 11:26:15 2021 -0500

    [PR97969] LRA: Transform pattern `plus (plus (hard reg, const),     pseudo)` after elimination


---


### compiler : `gcc`
### title : `SLP permute propagation doesn't handle VEC_PERM`
### open_at : `2021-06-23T10:12:33Z`
### last_modified_date : `2021-07-01T07:48:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101178
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The current permute propagation code simply treats VEC_PERM nodes as materialization points (they can consume incoming permutes) but it does
neither handle them as sources for permutes nor does it consider propagating
a common source permute through itself.  The latter can be seen for

double x[2], y[2], z[2], w[2];

void foo ()
{
  double tem0 = x[1] + y[1];
  double tem1 = x[0] - y[0];
  double tem2 = z[1] * tem0;
  double tem3 = z[0] * tem1;
  z[0] = tem2 - w[0];
  z[1] = tem3 + w[1];
}

where we do not end up materializing the x[], y[] and w[] permute at
the last +- node but instead materialize at the first +- node and thus
end up with incoming permute differences at the second +- one:

  <bb 2> [local count: 1073741824]:
  _21 = &x[1] + 18446744073709551608;
  vect__3.9_22 = MEM <vector(2) double> [(double *)_21];
  _1 = x[1];
  _23 = &y[1] + 18446744073709551608;
  vect__4.12_24 = MEM <vector(2) double> [(double *)_23];
  vect_tem1_13.14_26 = vect__3.9_22 - vect__4.12_24;
  vect_tem0_12.13_25 = vect__3.9_22 + vect__4.12_24;
  _27 = VEC_PERM_EXPR <vect_tem0_12.13_25, vect_tem1_13.14_26, { 1, 2 }>;
  _2 = y[1];
  tem0_12 = _1 + _2;
  _3 = x[0];
  _4 = y[0];
  tem1_13 = _3 - _4;
  _18 = &z[1] + 18446744073709551608;
  vect__5.5_19 = MEM <vector(2) double> [(double *)_18];
  vect__6.6_20 = VEC_PERM_EXPR <vect__5.5_19, vect__5.5_19, { 1, 0 }>;
  vect_tem2_14.15_28 = vect__6.6_20 * _27;
  _5 = z[1];
  tem2_14 = _5 * tem0_12;
  _6 = z[0];
  tem3_15 = _6 * tem1_13;
  vect__7.18_29 = MEM <vector(2) double> [(double *)&w];
  vect__10.20_31 = vect_tem2_14.15_28 + vect__7.18_29;
  vect__8.19_30 = vect_tem2_14.15_28 - vect__7.18_29;
  _32 = VEC_PERM_EXPR <vect__8.19_30, vect__10.20_31, { 0, 3 }>;
  _7 = w[0];
  _8 = tem2_14 - _7;
  _9 = w[1];
  _10 = _9 + tem3_15;
  MEM <vector(2) double> [(double *)&z] = _32;

The permute vect__6.6_20 = VEC_PERM_EXPR <vect__5.5_19, vect__5.5_19, { 1, 0 }>
could have been elided.


---


### compiler : `gcc`
### title : `y % (x ? 16 : 4) and y % (4 << (2 * (bool)x)) produce different code`
### open_at : `2021-06-23T11:05:32Z`
### last_modified_date : `2023-05-06T21:52:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101179
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
These produces different assembly:

int f1(int y)
{
    const bool x = y % 100 == 0;
    return y % (x ? 16 : 4) == 0;
}

int f2(int y)
{
    const bool x = y % 100 == 0;
    return y % (4 << (x * 2)) == 0;
}

Since they do the same calculation, I would expect them to produce the same code.

Currently f1 produces slightly smaller code for aarch64 and x86_64.

With Clang they produce the same code (but using cmov which might not be optimal).


---


### compiler : `gcc`
### title : `predictable comparison of integer variables not folded`
### open_at : `2021-06-24T04:25:51Z`
### last_modified_date : `2021-07-19T06:35:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101186
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC fail to remove dead codes for following cases:

#include <stdio.h>

void f (unsigned int a, unsigned int b, unsigned int c)
// if a,b,c are signed, VRP can remove dead code
{
  if (a == b)
    {
      printf ("a");
      if (c < a)
	{
	  printf ("b");
	  if (c >= b)
	    printf ("Unreachable!");
	}
    }
}

void g (int a, int b, int x, int y)
{
  int c = y;
  if (a != 0)
    c = x;
  while (b < 1000)
  // without this loop, jump thread & VRP can remove dead code
    {
      if (a != 0)
	{
	  if (c > x)
	    printf ("Unreachable!");
	}
      else
	printf ("a\n");
      b++;
    }
}


---


### compiler : `gcc`
### title : `vectorizer failed to generate vashlv8hi, but extend to int and use vashlv4si instead`
### open_at : `2021-06-24T08:05:13Z`
### last_modified_date : `2021-07-28T20:12:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101190
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c

void
foo (unsigned short* __restrict pdst, unsigned short* psrc)
{
  for (int i = 0; i != 8; i++)
    pdst[i] <<= psrc[i];
}

typedef short v8hi __attribute__((vector_size(16)));
v8hi
foo1 (v8hi a, v8hi b)
{
    return a << b;
}

After support vashl_optab, gcc still failed to generate vpsllvw for foo, only success for foo1.

I wonder shouldn't foo be equal to foo1.


---


### compiler : `gcc`
### title : `__builtin_memmove does not perform constant optimizations`
### open_at : `2021-06-24T14:36:06Z`
### last_modified_date : `2021-08-29T08:32:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101197
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
https://godbolt.org/z/qTMEo93j1

Two code does exactly the same thing but gcc refuses to optimize this.

While clang generates exactly the same output of assembly
https://godbolt.org/z/7a4r1hxj7


---


### compiler : `gcc`
### title : `Unneeded AND after shift`
### open_at : `2021-06-24T20:19:04Z`
### last_modified_date : `2021-06-29T17:40:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101200
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.1.0`
### severity : `enhancement`
### contents :
The code after reduction is:

struct {
  int b[6];
} c;
unsigned char d;
void e() {
  unsigned char a = d >> 4, f = d & 15;
  c.b[a] = c.b[f];
}

with g++-11 -O2, this produces

	movzbl	d(%rip), %eax
	movq	%rax, %rdx
	shrq	$4, %rax
	andl	$15, %edx
	andl	$15, %eax
	movl	c(,%rdx,4), %edx
	movl	%edx, c(,%rax,4)
	ret

The second AND with 15 is unneeded and should have been optimized away by VRP as I understand it. I can't reproduce it with ARM, though, so maybe there's something x86-specific?

Compiler is

  gcc version 11.1.0 (Debian 11.1.0-3) 

The same code is generated back to at least 4.9. Also present in

  gcc version 12.0.0 20210527 (experimental) [master revision 262e75d22c3:7bb6b9b2f47:9d3a953ec4d2695e9a6bfa5f22655e2aea47a973] (Debian 20210527-1)


---


### compiler : `gcc`
### title : `Remove unnecessary empty check in std::function`
### open_at : `2021-06-25T04:49:42Z`
### last_modified_date : `2021-08-31T17:03:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101203
### status : `UNCONFIRMED`
### tags : `ABI, missed-optimization`
### component : `libstdc++`
### version : `12.0`
### severity : `normal`
### contents :
Hello,

this empty() check in std_function.h:

https://github.com/gcc-mirror/gcc/blob/master/libstdc++-v3/include/bits/std_function.h#L558

      _Res
      operator()(_ArgTypes... __args) const
      {
	if (_M_empty())
	  __throw_bad_function_call();
	return _M_invoker(_M_functor, std::forward<_ArgTypes>(__args)...);
      }

can, as I assume, be removed to gain better performance.

An empty, default constructed std::function could be initialized with __throw_bad_function_call() instead with nullptr/zeros.

If std::function::operator()(...) gets called, still __throw_bad_function_call() would get called as before.

The implementation of `explicit operator bool()` or better `_M_empty()` would then check if the function is initialized with the address of __throw_bad_function_call.

This would be a similar technic as used in std::any:
https://github.com/gcc-mirror/gcc/blob/master/libstdc++-v3/include/experimental/any#L436


---


### compiler : `gcc`
### title : `csinv does not have an zero_extend version`
### open_at : `2021-06-25T07:56:35Z`
### last_modified_date : `2021-07-19T15:56:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101205
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
While improving phi-opt I ran into this issue.
gcc.target/aarch64/csinv-neg.c started to fail because we move the cast out of the conditional and such.
Here is a testcase which shows the problem even without improvment of phiopt:
unsigned long long
inv1(unsigned a, unsigned b, unsigned c)
{
  unsigned t = a ? b : ~c;
  return t;
}


---


### compiler : `gcc`
### title : `Example where y % 16 == 0 seems more expensive than y % 400 == 0.`
### open_at : `2021-06-26T15:52:07Z`
### last_modified_date : `2021-06-28T13:23:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101225
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
Consider this implementation of is_leap_year:

bool is_leap_year_1(short year) {
  return year % 100 == 0 ? year % 400 == 0 : year % 4 == 0;
}

If a number is multiple of 100, then it's divisible by 400 if and only if it's divisible by 16. Since checking divisibility by 16 is cheap, one would expect the following version to be more efficient (at least, not worse):

bool is_leap_year_2(short year) {
  return year % 100 == 0 ? year % 16 == 0 : year % 4 == 0;
}

According to [1] the latter is 1.4x slower than the former.

The emitted instructions with -O3 [2] don't seem bad and, except for a leal x addw, the difference is a localized strength-reduction from "y % 400 == 0" to "y % 16 == 0":

is_leap_year_1(short):
  imulw $23593, %di, %ax
  leal 1308(%rax), %edx
  rorw $2, %dx
  cmpw $654, %dx
  ja .L2
  addw $1296, %ax # Begin: year % 400 == 0
  rorw $4, %ax    #
  cmpw $162, %ax  #
  setbe %al       # End  : year % 400 == 0
  ret
.L2:
  andl $3, %edi
  sete %al
  ret

is_leap_year_2(short):
  imulw $23593, %di, %ax
  addw $1308, %ax
  rorw $2, %ax
  cmpw $654, %ax
  ja .L6
  andl $15, %edi # Begin: y % 16 == 0
  sete %al       # End  : y % 16 == 0
  ret
.L6:
  andl $3, %edi
  sete %al
  ret

FWIW: My educated **guess** is that the issue is the choice of registers: for version 1 just after leal, the register rax/ax/al is free and regardless of the branch taken, the CPU can continue the calculation of "y % 100 == 0" in parallel with the other divisibility check, up to "sete %al". For version 2, rax/ax/al is busy during the whole execution of "y % 100" and "sete %al" can't be preemptively executed. As a test for my theory I reimplemented half of is_leap_year_2 in inline asm (see in [1] and [2]) using similar choices of registers as in is_leap_year_1 and I got the performance boost that I was expecting.

[1] https://quick-bench.com/q/3U8t4qzXxtSpsehbWNOh3SWxBGQ
[2] https://godbolt.org/z/jfK3j5777

Note: [1] runs GCC 10.2 but the same happens on GCC 11.0.0.


---


### compiler : `gcc`
### title : `Suboptimal codegen for >>/>>>`
### open_at : `2021-06-26T16:14:57Z`
### last_modified_date : `2023-05-02T04:06:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101226
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.1.0`
### severity : `enhancement`
### contents :
example.d:

void f1()
{
    __gshared ubyte value;
    value >>>= 5;
}

void f2()
{
    __gshared ubyte value;
    value = value >>> 5;
}

----
avr-gcc -fno-druntime -O3 example.d:

example.f1():
        lds r24,example.f1().value
        swap r24
        lsr r24
        andi r24,lo8(7)
        sts example.f1().value,r24
        ret
example.f2():
        lds r24,example.f2().value
        ldi r25,0
        ldi r27,0
        ldi r26,0
        ldi r18,5
        1:
        lsr r27
        ror r26
        ror r25
        ror r24
        dec r18
        brne 1b
        sts example.f2().value,r24
        ret

The compiler apparently fails to optimize out the integer promotion for >>> in f2 and produces inefficient machine code.


---


### compiler : `gcc`
### title : `[missed optimization] Transitivity of less-than and less-or-equal`
### open_at : `2021-06-28T12:03:52Z`
### last_modified_date : `2023-07-13T18:27:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101240
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Consider the following C++ code

#define ALWAYS_TRUE(x) do { if (x) __builtin_unreachable(); } while (false)

int divide(int a, int b) {
  ALWAYS_TRUE(a == b);
  return a / b;
}

void test_array(unsigned (&arr)[3]) {
  ALWAYS_TRUE(a[0] < a[1] && a[1] < a[2]);
  return a[0] < a[2];
}

The first function is optimiozed away:

divide(int, int):
        mov     eax, 1
        ret

While the second still does the comparison:

test_array(unsigned int (&) [3]):
        mov     eax, DWORD PTR [rdi+8]
        cmp     DWORD PTR [rdi], eax
        setb    al
        ret

It would be nice if GCC could deduce that 

a < b && b < c --> a < c

And optimize that second function (provided no other UB is involved)


---


### compiler : `gcc`
### title : `adjust_iv_update_pos update the iv statement unexpectedly cause memory address offset mismatch`
### open_at : `2021-06-29T08:39:48Z`
### last_modified_date : `2021-07-07T01:06:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101250
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Test case:

unsigned int foo (unsigned char *ip, unsigned char *ref, unsigned int maxlen)
{
  unsigned int len = 2;
  do {
      len++;
  }while(len < maxlen && ip[len] == ref[len]);
  return len;
}


ivopts:

  <bb 4> [local count: 1014686026]:
  _3 = MEM[(unsigned char *)ip_10(D) + ivtmp.16_15 * 1];
  ivtmp.16_16 = ivtmp.16_15 + 1;
  _19 = ref_12(D) + 18446744073709551615;
  _6 = MEM[(unsigned char *)_19 + ivtmp.16_16 * 1];
  if (_3 == _6)
    goto <bb 6>; [94.50%]
  else
    goto <bb 10>; [5.50%]

Disable adjust_iv_update_pos will produce:

  <bb 4> [local count: 1014686026]:
  _3 = MEM[(unsigned char *)ip_10(D) + ivtmp.16_15 * 1];
  _6 = MEM[(unsigned char *)ref_12(D) + ivtmp.16_15 * 1];
  ivtmp.16_16 = ivtmp.16_15 + 1;
  if (_3 == _6)
    goto <bb 6>; [94.50%]
  else
    goto <bb 10>; [5.50%]


discussions:
https://gcc.gnu.org/pipermail/gcc-patches/2021-June/573709.html


---


### compiler : `gcc`
### title : `Optimize i % (b ? C0 : C1) into i & (b ? C0-1 : C1-1) for power of 2 C0 and C1`
### open_at : `2021-06-29T08:41:47Z`
### last_modified_date : `2021-07-01T20:43:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101251
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Consider the following code

bool test_optim01(bool b, short i) {
    return i % (b ? 4 : 16)==0;
}

It could be optimized into

bool test_optim0(bool b, short i) {
    return (i & (b ? 15 : 3))==0;
}


Godbolt playground: https://godbolt.org/z/j15br4Kd4

P.S.: Inspired by the manual optimizations in libstdc++ https://github.com/gcc-mirror/gcc/commit/b92d12d3fe3f1aa56d190d960e40c62869a6cfbb


---


### compiler : `gcc`
### title : `Optimize (b ? i % C0 : i % C1) into i & (b ? C0-1 : C1-1) for power of 2 C0 and C1`
### open_at : `2021-06-29T08:44:50Z`
### last_modified_date : `2021-07-01T20:43:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101252
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Consider the following code

bool test_naive0(bool b, short i) {
    return (b ? i % 4 : i % 16)==0;
}

It could be optimized into

bool test_optim0(bool b, short i) {
    return (i & (b ? 15 : 3))==0;
}


Godbolt playground: https://godbolt.org/z/8vj999M3c

P.S.: Inspired by the manual optimizations in libstdc++ https://github.com/gcc-mirror/gcc/commit/b92d12d3fe3f1aa56d190d960e40c62869a6cfbb


---


### compiler : `gcc`
### title : `Optimize i % C1 == C0 || i % C1*C2 == C0 to i % C1 == C0`
### open_at : `2021-06-29T08:53:39Z`
### last_modified_date : `2022-03-10T08:12:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101253
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Consider the following code

bool test_naive(short i) {
    return i % 100 == 0 || i % 400 == 0;
}

It could be optimized into

bool test_optim(short i) {
    return i % 100 == 0;
}


Godbolt playground: https://godbolt.org/z/zW49qcs7G

P.S.: Inspired by the manual optimizations in libstdc++ https://github.com/gcc-mirror/gcc/commit/b92d12d3fe3f1aa56d190d960e40c62869a6cfbb


---


### compiler : `gcc`
### title : `Assert in constructor generates code for statically checkable expression`
### open_at : `2021-07-01T06:26:04Z`
### last_modified_date : `2021-07-09T03:51:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101277
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.1`
### severity : `normal`
### contents :
Created attachment 51092
-v output for Compiler Explorer compilation

Given the following code segment compiled with C++14 and above:

#include <cassert>

struct Positive { constexpr Positive(int x) {assert(x >= 0);} };

int foo(Positive) {return 1;}

int main()
{
    return foo(Positive{1});
}

GCC 10.1 and 10.2 will generate code to check if the assertion is true even though it is statically checkable. This occurs for optimization levels -O0, -Os, and -O2, but the assertion is elided for optimization levels -Og, -O1, and -O3.

The assertion is always elided in GCC 9.4 and below as well as GCC 10.3 and above.
Please see https://godbolt.org/z/3scPr1P7W for a comparison of x86-64 assembly with different GCC versions.


---


### compiler : `gcc`
### title : `[12 Regression] TSVC s231 slower with -Ofast -march=znver1 since r12-1836-g0ad9d88a3d7170b3`
### open_at : `2021-07-01T09:50:02Z`
### last_modified_date : `2021-10-13T10:09:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101280
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Seen e.g. here:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=14.847.0


---


### compiler : `gcc`
### title : `LIM ref canonicalization incomplete`
### open_at : `2021-07-02T09:56:54Z`
### last_modified_date : `2021-07-02T11:55:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101293
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
We fail to apply store-motion to

struct X { int i; int j; };

void foo(struct X *x, int n)
{
  for (int i = 0; i < n; ++i)
    {
      int *p = &x->j;
      int tem = *p;
      x->j += tem * i;
    }
}

because we end up with two distinct memory references:

Memory reference 1: MEM[(int *)x_7(D) + 4B]
Memory reference 2: x_7(D)->j


---


### compiler : `gcc`
### title : `Addition of  x86 addsub SLP patterned slowed down 433.milc by 12% on znver2 with -Ofast -flto`
### open_at : `2021-07-02T12:17:46Z`
### last_modified_date : `2023-01-31T11:26:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101296
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Commit g:7a6c31f0f84 (Add x86 addsub SLP pattern) has slowed down
433.milc from SPECFP 2006 by 12% on a znver2 based machine when
compiled with -Ofast -flto -march=native.

Note however that the master branch has afterwards recovered some of
these losses and today's bc8f0ed7042 is only 6% slower than what it
was before the addsub addition.  Nevertheless, this effect of this
particular change may be worth looking at.

LNT tracking graph is available for example here:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=289.70.0


---


### compiler : `gcc`
### title : `Improving sparse switch statement`
### open_at : `2021-07-02T15:32:02Z`
### last_modified_date : `2023-04-03T09:04:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101301
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following two functions do the same thing:

int foo(int x)
{
     switch (x) {
      case 11111: return 1;
      case 22222: return 2;
      case 33333: return 3;
      case 44444: return 4;
      case 55555: return 5;
      case 66666: return 6;
      case 77777: return 7;
      case 88888: return 8;
      case 99999: return 9;
      default: return 0;
     }
}

int foo2(int n)
{
  if (n >= 55555)
    {
      if (n >= 77777)
	{
	  if (n == 77777)
	    return 7;
	  if (n == 88888)
	    return 8;
	  if (n == 99999)
	    return 9;

	  return 0;
	}
      else
	{
	  if (n == 55555)
	    return 5;
	  if (n == 66666)
	    return 6;

	  return 0;
	}
    }
  else
    {
      if (n >= 33333)
	{
	  if (n == 33333)
	    return 3;
	  if (n == 44444)
	    return 4;

	  return 0;
	}
      else
	{
	  if (n == 11111)
	    return 1;
	  if (n == 22222)
	    return 2;

	  return 0;
	}
    }
}

but foo2 is translated into code with fewer conditional branches
on average.  Considering how expensive a mispredicted branch
can be, translating foo like foo2 could be an improvement.


---


### compiler : `gcc`
### title : `GCC refuses to use SSE registers to carry out an explicit XOR on a float.`
### open_at : `2021-07-03T21:00:17Z`
### last_modified_date : `2021-11-28T06:49:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101311
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.1.0`
### severity : `enhancement`
### contents :
// ---------------
int func(float a, float b) {
    float tmp = a * b;
    *reinterpret_cast<int*>(&tmp) ^= 0x80000000;

    return tmp;
}

int main() {
    return func(2, 4);
}
// ---------------

Compiling this with `g++ test.cpp -O3 -Wall -Wextra -fno-strict-aliasing -fwrapv -fno-aggressive-loop-optimizations -fsanitize=undefined` (removing the various strict flags achieves the same thing) gives no compile warnings and successfully returns `248` (-8) when run.

Looking at the assembly for `func`, GCC generates:
# ---------------
mulss      xmm0, xmm1
mov        eax, -2147483648
movd       DWORD PTR [rsp-20], xmm0
add        eax, DWORD PTR [rsp-20]
moved      xmm0, eax
cvttss2si  eax, xmm0
ret
# ---------------

I find a couple of things odd with this:
  - Memory is used as a temporary buffer. There shouldn't be any latency between the write and read due to store forwarding, but that cache line is going to have to be written to memory at some point.
 - Necessitating the previous point, GCC uses eax to carry out the XOR, requiring a move to and from the register.
  - GCC seems to favor an `add` instead of `xor`. I've seen it mentioned that an add should be slightly faster due to consecutive instructions not being blocked in the pipeline, but I'm don't see why a `xor` would be (don't quote me on this though).

Replacing the explicit XOR with a negation (`tmp = -tmp`) generates much more sensible assembly (.LC0 contains the xor constant):
# ---------------
mulss      xmm0, xmm1
xorps      xmm0, XMMWORD PTR .LC0[rip]
cvttss2si  eax, xmm0
ret
# ---------------

To be fair, in my example, negation is easily just the better method, but it seems silly that GCC goes to such lengths in the first snippet as to not use `xorps` (which as far as I can tell is just as fast as `add`). It looks like maybe GCC is confused by the cast to int (and as such doesn't want to use the xmm regs)?

Exact version is 11.1.0 under x86_64-linux-gnu, but I was able to reproduce this as far back as 4.9.


---


### compiler : `gcc`
### title : `std::optional returns forced through stack`
### open_at : `2021-07-05T16:37:40Z`
### last_modified_date : `2023-07-12T21:35:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101326
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
The simple example

#include <optional>

std::optional<long> foo() {
    return 0;
}

shows suboptimal codegen at -O3 -std=c++17.

The values seem to be forced through the stack.  On AArch64 we get

foo():
        sub     sp, sp, #16
        mov     w0, 1
        strb    w0, [sp, 8]
        mov     x0, 0
        ldr     x1, [sp, 8]
        add     sp, sp, 16
        ret

instead of (what clang gives)

foo():                                // @foo()
        mov     w1, #1
        mov     x0, xzr
        ret

On x86 it's the same:

foo():
        mov     QWORD PTR [rsp-24], 0
        mov     rax, QWORD PTR [rsp-24]
        mov     BYTE PTR [rsp-16], 1
        mov     rdx, QWORD PTR [rsp-16]
        ret

vs

foo():                                // @foo()
        mov     w1, #1
        mov     x0, xzr
        ret


---
