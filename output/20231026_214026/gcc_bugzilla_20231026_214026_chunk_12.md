### Total Bugs Detected: 4649
### Current Chunk: 12 of 30
### Bugs in this Chunk: 160 (From bug 1761 to 1920)
---


### compiler : `gcc`
### title : `missing constant propagation on SSE/AVX conversion intrinsics`
### open_at : `2018-04-10T14:48:30Z`
### last_modified_date : `2021-09-07T15:29:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85324
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `enhancement`
### contents :
The following test case shows that constant propagation through conversion intrinsics does not work:

#include <x86intrin.h>

template <class T> using V [[gnu::vector_size(16)]] = T;

// missed optimization:
auto a1() { return 1 + (V<  int>)_mm_cvttps_epi32(_mm_set1_ps(1.f)); }
auto b1() { return 1 + (V< long>)_mm_cvttps_epi64(_mm_set1_ps(1.f)); }
auto c1() { return 1 + (V<  int>)_mm_cvttpd_epi32(_mm_set1_pd(1.)); }
auto d1() { return 1 + (V< long>)_mm_cvttpd_epi64(_mm_set1_pd(1.)); }
auto e1() { return 1 + (V<short>)_mm_cvtepi32_epi16(_mm_set1_epi32(1)); }

The resulting asm is (`-O3 -march=skylake-avx512 -std=c++17`):
a1():
  vcvttps2dq .LC0(%rip), %xmm0
  vpaddd %xmm0, %xmm0, %xmm0
  ret
b1():
  vcvttps2qq .LC0(%rip), %xmm0
  vpaddq %xmm0, %xmm0, %xmm0
  ret
c1():
  vmovdqa64 .LC1(%rip), %xmm0
  vcvttpd2dqx .LC5(%rip), %xmm1
  vpaddd %xmm0, %xmm1, %xmm0
  ret
d1():
  vcvttpd2qq .LC5(%rip), %xmm0
  vpaddq %xmm0, %xmm0, %xmm0
  ret
e1():
  vmovdqa64 .LC7(%rip), %xmm1
  vmovdqa64 .LC1(%rip), %xmm0
  vpmovdw %xmm0, %xmm0
  vpaddw %xmm1, %xmm0, %xmm0
  ret

It should be a single load of a constant in each function. (A wrapper using __builtin_constant_p can work around it; cf. https://godbolt.org/g/8dta7B)


---


### compiler : `gcc`
### title : `-Os generates bigger code than -O2 due to disabled strlen pass`
### open_at : `2018-04-10T22:38:09Z`
### last_modified_date : `2020-01-15T10:34:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85330
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
When the test case below is compiled with -O2, GCC emits object code that's  optimally efficient both in space and runtime.  But when the same test case is compiled with -Os it results in much bigger object code that will run far slower than the -O2 equivalent.  That's because the strlen optimization doesn't run with -Os.

At the same time, it's possible to create test cases involving string functions where the opposite is true (often involving long strings).  I wonder if enabling the strlen optimization to a limited extent at -Os (to at least track string lengths without other transformations) would give closer to optimal results.

$ cat z.c && gcc -O2 -fdump-tree-optimized=/dev/stdout -c z.c && objdump -d z.o && gcc -Os -fdump-tree-optimized=/dev/stdout -c z.c && objdump -d z.o
#define S "012345678"

int f (void)
{
  char a[sizeof S];
  __builtin_strcpy (a, S);
  return __builtin_strlen (a);
}

;; Function f (f, funcdef_no=0, decl_uid=1958, cgraph_uid=0, symbol_order=0)

f ()
{
  <bb 2> [local count: 1073741825]:
  return 9;

}



z.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 <f>:
   0:	b8 09 00 00 00       	mov    $0x9,%eax
   5:	c3                   	retq   

;; Function f (f, funcdef_no=0, decl_uid=1958, cgraph_uid=0, symbol_order=0)

f ()
{
  char a[10];
  long unsigned int _1;
  int _4;

  <bb 2> [local count: 1073741825]:
  __builtin_strcpy (&a, "012345678");
  _1 = __builtin_strlen (&a);
  _4 = (int) _1;
  a ={v} {CLOBBER};
  return _4;

}



z.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 <f>:
   0:	48 83 ec 18          	sub    $0x18,%rsp
   4:	be 00 00 00 00       	mov    $0x0,%esi
   9:	48 8d 54 24 06       	lea    0x6(%rsp),%rdx
   e:	48 89 d7             	mov    %rdx,%rdi
  11:	e8 00 00 00 00       	callq  16 <f+0x16>
  16:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
  1a:	48 89 c2             	mov    %rax,%rdx
  1d:	31 c0                	xor    %eax,%eax
  1f:	48 89 d7             	mov    %rdx,%rdi
  22:	f2 ae                	repnz scas %es:(%rdi),%al
  24:	48 83 c4 18          	add    $0x18,%rsp
  28:	48 89 c8             	mov    %rcx,%rax
  2b:	48 f7 d0             	not    %rax
  2e:	48 ff c8             	dec    %rax
  31:	c3                   	retq


---


### compiler : `gcc`
### title : `Failure to use both div and mod results of one IDIV in a prime-factor loop while(n%i==0) { n/=i; }`
### open_at : `2018-04-12T06:56:05Z`
### last_modified_date : `2021-08-15T21:24:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85366
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
From https://codereview.stackexchange.com/questions/191792/find-prime-factors-in-c/191801#191801, simplified to use a pointer instead of returning std::vector<int>.  Interestingly, the version with std::vector can be more easily coaxed to use both results of one idiv, see the Godbolt link.

void find_prime_factors_ptr(int n, int *p)
{
    // inefficient to test even numbers > 2, but that's a separate missed optimization.
    for (int i = 2; i <= n; i++) {
        while (n % i == 0) {
            *p++ = i;
            n /= i;   // reordering the loop body doesn't help
        }
    }
}

https://godbolt.org/g/ogyZW8

g++ 8.0.1 20180411 -O3 -march=haswell gives us this inner loop:

     ...
     # outer loop
     movl    %edi, %eax
        # idiv to test if inner loop should even run once, leaving n/i in eax
.L4:
        movl    %edi, %eax        # but instead we discard it
        addq    $4, %rsi
        movl    %ecx, -4(%rsi)
        cltd
        idivl   %ecx
        cltd                      # then modulo that division result to see if the next iteration should run
        movl    %eax, %edi
        idivl   %ecx              # leaves n/i in eax, ready for next iteration...
        testl   %edx, %edx
        je      .L4
     ...

So both ways to get to .L4 (fall in or loop) have n/i in EAX from an idiv already!  The loop doesn't need to be re-structured to take advantage, gcc just needs to keep track of what it's doing.

## Hand optimized version of the whole function:
        cmpl    $1, %edi
        jle     .L9
        movl    $2, %ecx
.L5:
        movl    %edi, %eax
        cltd
        idivl   %ecx          # eax = tmp = n/i
        testl   %edx, %edx
        jne     .L3
.L4:
        movl    %ecx, (%rsi)
        addq    $4, %rsi      # we're tuning for Haswell, no register-read stalls so increment after reading and save a byte in the addressing mode
        movl    %eax, %edi    # n = tmp
        cltd
        idivl   %ecx          # eax = tmp = n/i
        testl   %edx, %edx
        je      .L4
.L3:
        incl    %ecx
        cmpl    %edi, %ecx
        jle     .L5
.L9:
        ret


I didn't make *any* changes to the code outside the inner loop.  I ended up just removing movl %edi, %eax / cltd / idiv %ecx.

Changing the inner loop to

        int tmp;
        while (tmp = n/i, n % i == 0) {
            *p++ = i;
            n = tmp;
        }

gives us the asm almost that good (an extra mov inside the loop), but we get a jmp into the loop instead of peeling the while condition from before the first iteration:


# gcc8.0.1 -O3 -march=haswell output, commented but unmodified
find_prime_factors_ptr_opt(int, int*):
        cmpl    $1, %edi
        jle     .L18
        movl    $2, %ecx
        jmp     .L19
.L16:                                 # top of inner loop
        addq    $4, %rsi
        movl    %ecx, -4(%rsi)
        movl    %eax, %edi            # extra mov puts this and the next mov on the critical path
.L19:                        # inner loop entry point
        movl    %edi, %eax
        cltd
        idivl   %ecx
        testl   %edx, %edx
        je      .L16                  # bottom of inner
        incl    %ecx
        cmpl    %edi, %ecx
        jle     .L19               # bottom of outer
.L18:
        ret

Saving code-size here with the dependent chain of movl %eax, %edi / movl %edi, %eax is pretty minor even on CPUs like original Sandybridge, or Bulldozer, without mov-elimination, because idiv's latency dominates.  But it could easily be taken out of the inner loop by duplicating it outside the outer loop, then moving it to the outer-only part of the loop body, like this:

        cmpl    $1, %edi
        jle     .L18
        movl    $2, %ecx
        movl    %edi, %eax   # eax = n added here
        jmp     .L19
.L16:                                 # top of inner loop
        addq    $4, %rsi
        movl    %ecx, -4(%rsi)
        movl    %eax, %edi     # n = tmp  still here
.L19:                        # inner loop entry point
         #movl    %edi, %eax  # eax = n removed from here in inner/outer loop
        cltd
        idivl   %ecx
        testl   %edx, %edx
        je      .L16                  # bottom of inner

        movl    %edi, %eax    # eax = n also added here, in the outer-only part
        incl    %ecx
        cmpl    %edi, %ecx
        jle     .L19               # bottom of outer
.L18:
        ret

In the inner loop, the loop-carried dep chain is just idiv -> cdq -> idiv via eax, without any mov instructions.

In the outer loop (.L19: to jle .L19), the loop-carried dep is only inc %ecx.  (Bottleneck on idiv throughput, not latency, like before.)


---


### compiler : `gcc`
### title : `possible missed optimisation / regression from 6.3 with while (__builtin_ffs(x) && x)`
### open_at : `2018-04-12T11:16:14Z`
### last_modified_date : `2021-09-05T00:03:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85375
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
Input:

extern int a;

int f(int x)
{
    while (__builtin_ffs(x) && x)
        x -= a;

    return x;
}

gcc 6.3.0 with -O3 compiled this as:

f(int):
  movl %edi, %eax
  movl a(%rip), %esi
  movl $-1, %ecx
  jmp .L3
.L11:
  testl %eax, %eax
  je .L2
  subl %esi, %eax
.L3:
  bsfl %eax, %edx
  cmove %ecx, %edx
  cmpl $-1, %edx
  jne .L11
.L2:
  rep ret

whereas current trunk (also with -O3) compiles it as:

f(int):
  movl $-1, %ecx
  bsfl %edi, %eax
  cmove %ecx, %eax
  cmpl %ecx, %eax
  je .L5
  testl %edi, %edi
  je .L6
  movl a(%rip), %esi
  movl %edi, %eax
  jmp .L3
.L4:
  testl %eax, %eax
  je .L1
.L3:
  subl %esi, %eax
  bsfl %eax, %edx
  cmove %ecx, %edx
  cmpl $-1, %edx
  jne .L4
  ret
.L6:
  xorl %eax, %eax
.L1:
  ret
.L5:
  movl %edi, %eax
  ret

There are fewer instructions overall for the case where x is 0 on entry, but trunk still has longer code overall even if we change the while-condition to __builtin_expect(!!__builtin_ffs(x) && x, 1) which ideally should pessimise this case.

For the same input, clang trunk with -O3 gives:

f(int): # @f(int)
  test edi, edi
  je .LBB0_3
  mov eax, dword ptr [rip + a]
  neg edi
.LBB0_2: # =>This Inner Loop Header: Depth=1
  add edi, eax
  jne .LBB0_2
.LBB0_3:
  xor eax, eax
  ret

This seems to rely simply on the fact that (__builtin_ffs(x) == 0) and (x == 0) are equivalent.

If you simplify the while-condition to simply __builtin_ffs(x), then the difference is smaller but still there:

6.3.0:

f(int):
  movl %edi, %eax
  movl a(%rip), %esi
  movl $-1, %ecx
  jmp .L3
.L5:
  subl %esi, %eax
.L3:
  bsfl %eax, %edx
  cmove %ecx, %edx
  cmpl $-1, %edx
  jne .L5
  rep ret

trunk:

f(int):
  movl $-1, %ecx
  bsfl %edi, %eax
  cmove %ecx, %eax
  cmpl %ecx, %eax
  je .L4
  movl a(%rip), %esi
  movl %edi, %eax
.L3:
  subl %esi, %eax
  bsfl %eax, %edx
  cmove %ecx, %edx
  cmpl $-1, %edx
  jne .L3
  ret
.L4:
  movl %edi, %eax
  ret


---


### compiler : `gcc`
### title : `[10/11/12/13/14 Regression] possible missed optimisation / regression from 6.3 with conditional expression`
### open_at : `2018-04-13T08:15:06Z`
### last_modified_date : `2023-05-30T07:41:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85390
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
Input:

extern int a, b, c;

int f(int x)
{
    __builtin_prefetch((void *) (x ? a : b));
    return c;
}

Current trunk with -O3 produces this:

f(int):
  testl %edi, %edi
  je .L2
  movslq a(%rip), %rax
  prefetcht0 (%rax)
  movl c(%rip), %eax
  ret
.L2:
  movslq b(%rip), %rax
  prefetcht0 (%rax)
  movl c(%rip), %eax
  ret

While 6.3.0 did not have a branch:

f(int):
  movslq a(%rip), %rdx
  movslq b(%rip), %rax
  testl %edi, %edi
  cmovne %rdx, %rax
  prefetcht0 (%rax)
  movl c(%rip), %eax
  ret

For reference, clang also outputs a branchless (but slightly longer) version:

f(int): # @f(int)
  testl %edi, %edi
  movl $a, %eax
  movl $b, %ecx
  cmovneq %rax, %rcx
  movslq (%rcx), %rax
  prefetcht0 (%rax)
  movl c(%rip), %eax
  retq

In my tests, the 6.3.0 code is equally fast in the x == 0 and x != 0 cases, whereas trunk/8.0.1 is only half as fast as 6.3.0 in the x == 0 (branch taken) case. In the branch not taken case, the 8.0.1 code has the same speed as the 6.3.0 code.


---


### compiler : `gcc`
### title : `g++ reports "array subscript is above array bounds" when it cannot be sure`
### open_at : `2018-04-13T19:36:42Z`
### last_modified_date : `2021-08-01T21:17:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85398
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `6.4.0`
### severity : `normal`
### contents :
In following test program:


------------------------------
#define NB_DEV 1
extern unsigned int max;

unsigned long left[NB_DEV];
unsigned long right[NB_DEV];

void foo()
{
    unsigned int i;

    for (i=1; i < max; i++)
      left[i] = right[i-1];
}
------------------------------

compiled with:

$(CXX) -Werror -Wall -O2 -c reprod.cc

g++ gives following warning/error:

reprod.cc: In function 'void foo()':
reprod.cc:13:13: error: array subscript is above array bounds [-Werror=array-bounds]
       left[i] = right[i-1];
       ~~~~~~^
cc1plus: all warnings being treated as errors
make: *** [Makefile:4: all] Error 1


While there _could_ be an array overflow, g++ cannot know for sure because the loop boundary 'max' is an external variable. The code is perfectly fine in case max == 1. In that case, the loop does nothing.

This is a reduced version of real code where the arrays left and right are dimensioned to some maximum value NB_DEV, and 'max' will be at most that NB_DEV but possibly smaller. We are thus sure there will not be an array overflow.


Going back to the reproduction code above, if you change NB_DEV to 2 (for example), no warning is thrown, even though there could still be an overflow in case max == 5, for example.

According to me, no warning should be thrown because g++ cannot surely say there is a problem.

Same problem is seen if you compile this as C code rather than C++.
Problem is not seen with -O1, only with -O2 or -O3.

This problem was tested with gcc 6.4.0 (x86_64), gcc 6.3.0 (armeb), gcc 5.4.0 (armeb) and gcc 4.9.4 (armeb).


---


### compiler : `gcc`
### title : `Unnecessary blend when vectorizing short-cutted calculations`
### open_at : `2018-04-15T15:05:42Z`
### last_modified_date : `2021-09-05T02:58:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85406
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
If you have something like this:

inline unsigned qPremultiply(unsigned x)
{
    const unsigned a = x >> 24;
    if (a == 255)
      return x;

    unsigned t = (x & 0xff00ff) * a;
    t = (t + ((t >> 8) & 0xff00ff) + 0x800080) >> 8;
    t &= 0xff00ff;

    x = ((x >> 8) & 0xff) * a;
    x = (x + ((x >> 8) & 0xff) + 0x80);
    x &= 0xff00;
    return x | t | (a << 24);

}

Gcc will vectorize it so that the longer calculation is always performed and with an added blend in the end to merge the two different return values. This is however unnecessary as the calculation will give the same result, and thus the blend can be saved.

Also in any case it is actually a bit unsafe to vectorize as the performance difference between the two branches is substantial, and it happens that in this case the short-cut is likely to be valid most of the time, so a nonvectorized loop might be faster than a vectorized one by doing a lot less.

The latter can be fixed, if the short-cut was also vectorized, for instance making the test for 4 values at a time and skip the long route if none of them need it.


---


### compiler : `gcc`
### title : `[8 Regression] Larger code generated from GMP template meta-programming`
### open_at : `2018-04-18T23:08:32Z`
### last_modified_date : `2019-04-17T15:53:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85459
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 43982
preprocessed testcase

The attached testcase is a preprocessed version of https://gmplib.org/list-archives/gmp-bugs/2014-January/003331.html, which I was about to commit to GMP.

Compiling for x86_64 with g++-7 -O3, I get a nice .s file of size 4174. With g++-8 -O3, I get a much larger size of 7077. Looking more closely at the difference, I see a ton of moves. Besides, g++-8 finds fewer __builtin_constant_p opportunities than g++-7 (__gmpq_mul instead of __gmpq_mul_2exp for instance).

The dump I glanced at makes me suspect I?SRA, but I am likely completely off.


---


### compiler : `gcc`
### title : `unnecessary vmovaps/vmovapd/vmovdqa emitted`
### open_at : `2018-04-20T09:27:16Z`
### last_modified_date : `2021-08-21T23:07:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85482
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `normal`
### contents :
Test case (cf. https://godbolt.org/g/QkJYSK):
#include <x86intrin.h>

__m256 zero_extend1(__m128 a) {
    return _mm256_insertf128_ps(__m256(), a, 0);
}
__m256d zero_extend1(__m128d a) {
    return _mm256_insertf128_pd(__m256d(), a, 0);
}
__m256i zero_extend1(__m128i a) {
    return _mm256_insertf128_si256(__m256i(), a, 0);
}

__m512 zero_extend2(__m128 a) {
    return _mm512_insertf32x4(__m512(), a, 0);
}
__m512d zero_extend2(__m128d a) {
    return _mm512_insertf64x2(__m512d(), a, 0);
}
__m512i zero_extend2(__m128i a) {
    return _mm512_inserti32x4(__m512i(), a, 0);
}

__m512 zero_extend3(__m256 a) {
    return _mm512_insertf32x8(__m512(), a, 0);
}
__m512d zero_extend3(__m256d a) {
    return _mm512_insertf64x4(__m512d(), a, 0);
}
__m512i zero_extend3(__m256i a) {
    return _mm512_inserti64x4(__m512i(), a, 0);
}

template <class T> T blackhole;

void test(void *mem) {
    blackhole<__m256 > = zero_extend1(_mm_load_ps((float *)mem));
    blackhole<__m256d> = zero_extend1(_mm_load_pd((double *)mem));
    blackhole<__m256i> = zero_extend1(_mm_load_si128((__m128i *)mem));
    blackhole<__m512 > = zero_extend2(_mm_load_ps((float *)mem));
    blackhole<__m512d> = zero_extend2(_mm_load_pd((double *)mem));
    blackhole<__m512i> = zero_extend2(_mm_load_si128((__m128i *)mem));
    blackhole<__m512 > = zero_extend3(_mm256_load_ps((float *)mem));
    blackhole<__m512d> = zero_extend3(_mm256_load_pd((double *)mem));
    blackhole<__m512i> = zero_extend3(_mm256_load_si256((__m256i *)mem));
}

Between every load and store instruction in the `test` function, the vmov(aps|apd|dqa) is superfluous. The preceding load instruction already zeroes the high bits. Instead of the load instruction, there could also be a different instructions, or instruction sequences, that already guarantee the high bits to be zero.


---


### compiler : `gcc`
### title : `missed if-conversion / phiopt trick`
### open_at : `2018-04-23T10:36:11Z`
### last_modified_date : `2023-05-06T01:07:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85501
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `enhancement`
### contents :
PR85466 shows

For the "if" case, llvm turns:

if (myVector[n] > 0.5){
    result[n] = 0.8f;
}
else {
    result[n] = 0.1f;
}

into

const float tab[2] = { .8f, .1f };
result[n] = tab[item > .5f];

which is a neat trick.  Profitability is not without questions though
(if you'd do this to scalar code).


---


### compiler : `gcc`
### title : `[missed-optimization] gcc does not convert multiple compares against constants to a shift+bitmask test`
### open_at : `2018-04-24T15:03:33Z`
### last_modified_date : `2021-11-29T15:29:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85516
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Consider

=== example starts ===
enum class E { a, b, c, d, e, f, g, h, i, j };

bool check_mask(E v) {
    return v == E::b || v == E::e || v == E::f || v == E::j;
}
=== example ends ===

This can be compiled to just three instructions (x86):

  mov $0x232, %eax
  bt %edi, %eax
  setc %al

but instead gcc compiles it to:

  cmpl $1, %edi
  sete %al
  cmpl $4, %edi
  sete %dl
  orb %dl, %al
  jne .L1
  subl $5, %edi
  andl $-5, %edi
  sete %al
 .L1:

which is three times as large and contains a possibly unpredictable branch. More bits in the mask will presumably generate larger code.


---


### compiler : `gcc`
### title : `x86_64: loads are not always narrowed`
### open_at : `2018-04-26T15:47:27Z`
### last_modified_date : `2021-09-04T21:17:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85539
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
When casting a 64-bit memory value to 32 bits, it's possible to only load the lower 32 bits (this also avoids the REX prefix). GCC doesn't always do this: https://godbolt.org/g/EzqKNo


#include <cstdint>

uint32_t foo(uint64_t *p)
{
    return *p;
}


Actual:
        mov     rax, QWORD PTR [rdi]
        ret

Expected:
        mov     eax, DWORD PTR [rdi]
        ret


---


### compiler : `gcc`
### title : `No strength reduction of modulo and integer vision`
### open_at : `2018-04-27T11:46:10Z`
### last_modified_date : `2021-08-14T05:28:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85551
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Created attachment 44030
strmod.cpp

Many simple loops using modulo naively can be optimized too not perform the expensive module/division every iterations, but GCC does not perform this strength reduction.

I have attached a motivating example including two iterations of optimizations. An easy safe one (though it might interfere with vectorization if the arch has vectorized integer divisions), and a more agressive one that is much faster in some cases but not always.


---


### compiler : `gcc`
### title : `Missed optimization in niter analysis for bit-by-bit variable zeroing`
### open_at : `2018-04-28T14:16:11Z`
### last_modified_date : `2023-06-10T01:13:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85560
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
The following C++ code loops until a variable is cleared bit-by-bit:

#include <cstdint>

uint32_t b;
uint32_t test() {
    uint32_t a = b;
    uint32_t bits = 1;

    while (a != 0) {
        a &= ~bits;
        bits <<= 1;
    }

    return 0;
}

After looping, it returns 0. While the loop is useless, it still exists in the assembly even with -O3.

GCC version: 9.0.0 20180427 
Demo: https://godbolt.org/g/7gRMqu

Notably, the loop is optimized away on ICC 18


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] -Wmaybe-uninitialized false alarm regression with __builtin_unreachable and GCC 8`
### open_at : `2018-04-28T20:02:38Z`
### last_modified_date : `2023-08-04T08:33:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85563
### status : `NEW`
### tags : `deferred, diagnostic, missed-optimization`
### component : `middle-end`
### version : `8.0.1`
### severity : `normal`
### contents :
When compiling Emacs with gcc (GCC) 8.0.1 20180324 (Red Hat 8.0.1-0.20) x86-64 I noticed a regression compared to previous GCC versions, a regression that causes a false alarm. Briefly, Emacs uses a statement like this:

  ((CONSP (Vframe_list)) ? (void) 0 : __builtin_unreachable ());

to tell Emacs that Vframe_list is a cons (CONSP is inline and side effect free), and then follows this with a loop:

  for ((tail) = Vframe_list;
       (CONSP (tail) && (frame1 = XCAR (tail), 1));
       tail = XCDR (tail))
     { loop body }

which obviously must initialize frame1. In a later use of frame1, though, GCC 8.0.1 incorrectly complains:

f.i: In function ‘delete_frame’:
f.i:69688:7: warning: ‘frame1’ may be used uninitialized in this function [-Wmaybe-uninitialized]
       do_switch_frame (frame1, 0, 1, builtin_lisp_symbol (0));
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

where GCC 7 and earlier do not complain. To reproduce the problem, use the command:

gcc -O2 -S -Wmaybe-uninitialized f.i

on the attached file f.i. Although strictly speaking this isn't a bug since the "may be used uninitialized" is deliberately wishy-washy, still, it's an annoyance that I'll have to fiddle with Emacs source to pacify the misguided compiler here.


---


### compiler : `gcc`
### title : `[og7, nvptx] make generic and per-worker broadcast buffers overlap`
### open_at : `2018-05-01T11:00:22Z`
### last_modified_date : `2021-02-17T13:51:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85584
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
Consider this fortran testcase compiled at -O2 with -foffload=-mlong-vector-in-workers:
...
module param
  integer, parameter :: N = 32
end module param

program main
  use param
  integer :: i, j
  integer :: a(N)

  do i = 1, N
    a(i) = i
  end do

  !$acc parallel copy (a) vector_length (128)
  !$acc loop worker
    do i = 1, N
       !$acc loop vector 
       do j = j, N
          a(j) = a(j) - a(j)
       end do
    end do
  !$acc end parallel

  do i = 1, N
    if (a(i) .ne. 0) call abort
  end do

end program main
...

In the ptx, we generate a broadcast buffer:
...
.shared .align 8 .u8 __oacc_bcast[504];
...
which consists of 9 partitions of 56. 1 generic partition, and 8 per-worker partitions.

The generic partition is addressed using __oacc_bcast, the per-worker partitions are addressed using %r109 calculated here:
...
{
    .reg .u32 %tidy;
    .reg .u64 %t_bcast;
    .reg .u64 %y64;
    mov.u32 %tidy,%tid.y;
    cvt.u64.u32 %y64,%tidy;
    add.u64 %y64,%y64,1;
    cvta.shared.u64 %t_bcast,__oacc_bcast;
    mad.lo.u64 %r109,%y64,56,%t_bcast;
}
...

The generic partition broadcasting is guarded with bar.sync 0, the per-worker partition broadcasting is guarded with bar.sync %r110,128, where %r110 is calculated here:
...
  {
    .reg .u32 %tidy;
    mov.u32 %tidy,%tid.y;
    add.u32 %r110,%tidy,1;
  }
...

In principle, it should be possible to make the generic partition overlap with the per-worker partitions, which would mean less shared memory used.


---


### compiler : `gcc`
### title : `switch to select a string based on an enum can profitably optimize away the table of pointers/offsets into fixed-length char[] blocks.  Or use byte offsets into a string table`
### open_at : `2018-05-01T12:56:49Z`
### last_modified_date : `2019-09-19T06:26:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85585
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Bug 84011 shows some really silly code-gen for PIC code and discussion suggested using a table of offsets instead of a table of actual pointers, so you just need one base address.

A further optimization is possible when the strings are all similar length, and/or the longest one isn't much longer than a pointer:

Pad all strings to the same length with trailing 0 bytes, and calculate a pointer instead of loading it from an array.  This removes the possibility of multiple entries sharing the same suffix (which is a missed optimization gcc wasn't already doing), but avoids needing any space for storing pointers in memory at all.

In the case discussed in bug 84011 (Linux's phy.h const char *phy_modes(phy_interface_t interface)), the longest strings are 11 bytes (including the \0), and there are 23 of them.  So it takes 253 bytes of char data to store everything (not counting the "unknown" for the default: special case) with all strings padded to 11 bytes.

----

The current strings + pointer-table implementation doesn't merge string literals where one string is a suffix of another; this is another a missed-optimization that would save many bytes here.  (e.g. instead of .string "mii" and .string "gmii", just have .LC4 .byte 's'; .LC3: .byte 'g'; .LC2: .string "mii".)

That optimization plus byte or 16-bit offsets into the table would be nice and compact, and most CPUs have efficient zero-extending narrow loads.  So for cases where the other optimization I'm suggesting isn't good, that would probably be best.

----

The current packed string-data takes 158 bytes , so with 4-byte offsets it takes 158+23*4 = 250 bytes.  Or with 8-byte pointers/offsets, it takes 158 + 23*8 = 342 bytes.  Or with 1-byte offsets, 158 + 23*1 = 181 bytes: load with movzbl.  (If you can't use the offset directly as an 8-byte memory source operand for ADD to a pointer, there's no point making it 32 bits instead of 8.)

The code for *using* such a table is quite simple.  This C source compiles to what I'm suggesting:

https://godbolt.org/g/E8J3iS

struct foo {
    char str[11];
} const table[23] = {};

const char *lookup(unsigned long idx) {
    if(idx > 23) {
        return "unknown";
        //idx=23;
    }
    return table[idx].str;
}

Multiply by 11 only takes 2 LEA instructions on x86, so for PIC code with a RIP-relative LEA we end up with 4 ALU instructions total to get a string address, after checking the if condition:

       # gcc7.3 -march=haswell -O3 -fPIE output:  https://godbolt.org/g/qMzaY8
        leaq    .LC0(%rip), %rax    # "unknown"
        cmpq    $23, %rdi
        ja      .L4                 # branchless is also an option
        leaq    (%rdi,%rdi,4), %rax
        leaq    table(%rip), %rdx   # RIP-relative table base address
        leaq    (%rdi,%rax,2), %rax
        addq    %rdx, %rax          # table + 11*idx
.L4:
        ret

This is even better in no-PIE mode where a static address is usable as a signed 32-bit immediate:

lookup(unsigned long):
        movl    $.LC0, %eax
        cmpq    $23, %rdi
        ja      .L4
        leaq    (%rdi,%rdi,4), %rax
        leaq    table(%rdi,%rax,2), %rax    # 3 cycle latency for 3-component LEA on SnB-family
.L4:
        ret

So this has extremely low code-size cost on x86-64, for the benefit of removing a table load in the dependency chain from enum to string data.  It does cost significant data size vs. a byte-offset table with suffix-merging, but it's  better than what gcc is doing now in non-PIE (table of qword pointers), and *much* better in PIE (insane jump table).

-----

The byte-index version is equivalent to transforming the C source like this:

const char packedstrings[158] = {};
const unsigned char offsets[23] = {};
const char *lookup_byteidx(unsigned long idx) {
    if(idx>23)
        return "unknown";
    return &packedstrings[offsets[idx]];
}

        leaq    .LC0(%rip), %rax      # "unknown"
        cmpq    $23, %rdi
        ja      .L9
        leaq    offsets(%rip), %rax
        leaq    packedstrings(%rip), %rdx
        movzbl  (%rax,%rdi), %eax
        addq    %rdx, %rax
.L9:
        ret

We can save an instruction here by making the relative position of packedstrings and offsets a compile-time constant, i.e. by effectively putting them in a struct.  So

        ...
        ja      .L9
        leaq    packedstrings(%rip), %rdx
        movzbl  offsets-packedstrings(%rdx,%rdi), %eax
        add     %rdx, %rax

base+idx + disp8 for a load isn't slower than base+idx on modern x86, unlike for LEA.  This lets us still use add instead of a 3-component LEA.  Arrange them so offsets-packedstrings fits in a signed 8-bit integer if possible, so you can use a short displacement.  (i.e. put the shorter one first, almost always offsets unless a lot of the strings are duplicated.)

We can represent this idea in C source like this:

struct {
    unsigned char offsets[23];
    char packedstrings[158];
}const stringtab = {};

const char *lookup_stringtab(unsigned long idx) {
    if(idx>23)
        return "unknown";
    unsigned off = stringtab.offsets[idx];
    // TODO: make off relative to stringtab so we avoid a separate +23
    return &stringtab.packedstrings[off];
}

but unfortunately we get this asm with -O3 -march=haswell

        leaq    stringtab(%rip), %rax
        movzbl  (%rax,%rdi), %edx
        leaq    23(%rax,%rdx), %rax     # 3-component LEA

We could optimize away the +23 by baking that into the byte offsets, if we don't need the full 0..255 range.

        leaq    stringtab(%rip), %rax
        movzbl  (%rax,%rdi), %edx       # offset relative to the start of stringtab
        add     %rdx, %rax


It's very nice with no-PIE, whether we tell gcc to keep the arrays together or not:

        movzbl  stringtab(%rdi), %eax
        addq    $packedstrings, %rax 
          # missed opt: addl is safe; address is inside a static object and thus fits in 32 bits.

These compile fairly nicely for ARM32 and AArch64, even with -fPIE.  ARM32 could maybe save an instruction or two by doing a +440 once and using a 2-register addressing mode, though.  Or choosing a smarter PIC anchor closer to the data.


I guess this could be broken up into multiple missed-optimization bug reports, some of which are independent enough to file separately:

1. look for one string literal being a suffix of another (in general, regardless of tables)

2. pad strings to fixed length and calculate, if it's worth it, for integer->string mapping functions.  (Need a heuristic to trade off code size vs. data size vs. removing a level of indirection for latency)

3. use byte or 16-bit offsets to the base of consecutive strings (unless literal-merging between separate tables means we want 32-bit offsets).  Merging with literals that aren't part of a different table is fine; they can just reference a string as part of a table.

4. address one static array relative to the other when they're defined in the same compilation unit, so save on RIP-relative LEA instructions.  ARM already does this, but MIPS64 and x86 PIC/PIE miss it.  (At least the old gcc5.4 on Godbolt does, didn't check newer MIPS gcc).  Especially try to put them next to each other so it can be a disp8 instead of disp32 on x86.

5. For struct members, put the disp8 on a load so we can use add instead of 3-component LEA, with -mtune=intel.  (If generating this internally for a string lookup, bake in the offset so you don't need either, if that still fits in byte offsets.)

https://godbolt.org/g/E8J3iS demonstrates all of these (same link as above).


---


### compiler : `gcc`
### title : `Potentially missing optimization under x64 and ARM: seemingly unnecessary branch in codegen`
### open_at : `2018-05-02T09:13:54Z`
### last_modified_date : `2023-06-04T19:48:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85605
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.3.0`
### severity : `enhancement`
### contents :
Code:

==========

#include <stdint.h>
#include <type_traits>

template<class T,class T2>
inline bool cmp(T a, T2 b) {
  return a<0 ? true : T2(a) < b;
}

template<class T,class T2>
inline bool cmp2(T a, T2 b) {
  return (a<0) | (T2(a) < b);
}

bool f(int a, int b) {
    return cmp(int64_t(a), unsigned(b));
}

bool f2(int a, int b) {
    return cmp2(int64_t(a), unsigned(b));
}

====

Functions cmp and cmp2 seem to be equivalent (at least under "as if" rule, as side effects of reading and casting are non-observable). However, under GCC/x64, cmp() generates code with branch, while seemingly-equivalent cmp2() - manages to do without branching:

===============

f(int, int):
  testl %edi, %edi
  movl $1, %eax
  js .L1
  cmpl %edi, %esi
  seta %al
.L1:
  rep ret

f2(int, int):
  movl %edi, %edx
  shrl $31, %edx
  cmpl %edi, %esi
  seta %al
  orl %edx, %eax
  ret

===============

And f2() is expected to be significantly faster than f1() in most usage scenarios (*NB: if you feel it is necessary to create a case to illustrate detriment of branching - please LMK, but hopefully it is quite obvious*). 

Per Godbolt, similar behavior is observed under both GCC/x64, and GCC/ARM; however, Clang manages to do without branching both for f1() and f2(). 

*Godbolt link*: https://godbolt.org/g/ktovvP


---


### compiler : `gcc`
### title : `Suboptimal code generation for (potentially) redundant atomic loads`
### open_at : `2018-05-02T14:49:55Z`
### last_modified_date : `2022-09-29T21:20:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85611
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.3.1`
### severity : `normal`
### contents :
$ cat test.cpp
#include <atomic>

std::atomic<int> atomic_var{100};
int somevar;
bool cond;

void run1() {
    auto a = atomic_var.load(std::memory_order_relaxed);
    auto b = atomic_var.load(std::memory_order_relaxed);
   // Some code using a and b;
}

void run2() {
    if (atomic_var.load(std::memory_order_relaxed) == 2 && cond) {
         if (atomic_var.load(std::memory_order_relaxed) * somevar > 3) {
               /*...*/
         }
    }
}


$ g++-7 -O3 -std=c++17 -S -o - test.cpp -fno-exceptions

	.text
	.align 4,0x90
	.globl __Z4run1v
__Z4run1v:
LFB339:
	movl	_atomic_var(%rip), %eax
	movl	_atomic_var(%rip), %eax
	ret
LFE339:
	.align 4,0x90
	.globl __Z4run2v
__Z4run2v:
LFB340:
	movl	_atomic_var(%rip), %eax
	cmpl	$2, %eax
	je	L5
L3:
	ret
	.align 4,0x90
L5:
	cmpb	$0, _cond(%rip)
	je	L3
	movl	_atomic_var(%rip), %eax
	ret


---


### compiler : `gcc`
### title : `Make better use of BFI (BFXIL)`
### open_at : `2018-05-03T14:47:43Z`
### last_modified_date : `2021-12-12T13:07:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85628
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
The testcase:
unsigned long long combine(unsigned long long a, unsigned long long b) {
    return (a & 0xffffffff00000000ll) | (b & 0x00000000ffffffffll);
}
 
void read2(unsigned long long a, unsigned long long b, unsigned long long *c, unsigned long long *d) {
    *c = combine(a, b); *d = combine(b, a);
}

on aarch64 with -O2 currently generates:
combine:
        bfi     x0, x1, 0, 32
        ret

read2:
        and     x5, x1, 4294967295
        and     x4, x0, -4294967296
        orr     x4, x4, x5
        and     x1, x1, -4294967296
        and     x0, x0, 4294967295
        str     x4, [x2]
        orr     x0, x0, x1
        str     x0, [x3]
        ret

With LLVM it does a better job:
combine:                                // @combine
        bfxil   x0, x1, #0, #32
        ret

read2:                                  // @read2
        mov     x8, x0
        bfxil   x8, x1, #0, #32
        bfxil   x1, x0, #0, #32
        str     x8, [x2]
        str     x1, [x3]
        ret

This should just be a matter of adding the necessary patterns in aarch64.md.
Combine already tries to match:

(set (reg:DI 105)
    (ior:DI (and:DI (reg/v:DI 97 [ b ])
            (const_int -4294967296 [0xffffffff00000000]))
        (and:DI (reg/v:DI 96 [ a ])
            (const_int 4294967295 [0xffffffff]))))


---


### compiler : `gcc`
### title : `Tree if-conversion inserts redundant loads`
### open_at : `2018-05-03T19:44:49Z`
### last_modified_date : `2021-07-20T07:30:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85636
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Compiling this at -O3 on aarch64-linux-gnu:

void f (int *a, int *b, int *c, int x, int y)
{
  for (int i = 0; i < 100; ++i)
    {
      int v = c[i];
      a[i] = (v == 20 ? x : y);
      b[i] = (v != 20 ? x : y);
    }
}

gives the following basic block after if-conversion:

  _1 = (long unsigned int) i_31;
  _2 = _1 * 4;
  _3 = c_11(D) + _2;
  v_12 = *_3;
  _21 = a_15(D) + _2;
  _ifc__41 = *_21;
  _ifc__42 = x_13(D);
  _ifc__43 = v_12 == 20 ? _ifc__42 : _ifc__41;
  *_21 = _ifc__43;
  _ifc__44 = *_21;
  _ifc__45 = y_14(D);
  _ifc__46 = v_12 == 20 ? _ifc__44 : _ifc__45;
  *_21 = _ifc__46;
  iftmp.1_8 = v_12 != 20 ? x_13(D) : y_14(D);
  _5 = b_17(D) + _2;
  *_5 = iftmp.1_8;

Note the two extra loads from a[i] (_21), which didn't occur in the original source.  The handling of b[i] is fine.


---


### compiler : `gcc`
### title : `Unneeded store of member variables in inner loop`
### open_at : `2018-05-03T20:01:56Z`
### last_modified_date : `2021-08-10T22:43:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85637
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.3.1`
### severity : `normal`
### contents :
Created attachment 44058
source

Attached a simple Adler32 checksum class. When updating with an array of bytes, the inner loop just accumulates the two sums, then the modulo is done in the outer loop. This way the cost of the two modulos is amortized.

At the start the two member variables are loaded into registers, however, they are stored back to memory in each inner loop iteration. Then, also at the end after the modulo, but before the end of the outer loop. There is only one exit from the function.

Why not store the registers back just once right before the ret?

Dump of assembler code for function Adler32::Update(void const*, unsigned int):
   0x0000000000400500 <+0>:     test   %edx,%edx
   0x0000000000400502 <+2>:     je     0x400578 <Adler32::Update(void const*, unsigned int)+120>
   0x0000000000400504 <+4>:     mov    (%rdi),%ecx	; ecx is m_s1
   0x0000000000400506 <+6>:     mov    0x4(%rdi),%r8d	; r8d is m_s2
   0x000000000040050a <+10>:    mov    $0x80078071,%r10d
   0x0000000000400510 <+16>:    xor    %r9d,%r9d
   0x0000000000400513 <+19>:    cmp    $0x15af,%edx
   0x0000000000400519 <+25>:    jbe    0x400527 <Adler32::Update(void const*, unsigned int)+39>
   0x000000000040051b <+27>:    lea    -0x15b0(%rdx),%r9d
   0x0000000000400522 <+34>:    mov    $0x15b0,%edx
   0x0000000000400527 <+39>:    lea    -0x1(%rdx),%eax
   0x000000000040052a <+42>:    lea    0x1(%rsi,%rax,1),%rdx
   0x000000000040052f <+47>:    nop
   0x0000000000400530 <+48>:    add    $0x1,%rsi
   0x0000000000400534 <+52>:    movzbl -0x1(%rsi),%eax
   0x0000000000400538 <+56>:    add    %eax,%ecx	; m_s1 += *buf
   0x000000000040053a <+58>:    add    %ecx,%r8d	; m_s2 += m_s1
   0x000000000040053d <+61>:    cmp    %rdx,%rsi
   0x0000000000400540 <+64>:    mov    %ecx,(%rdi)	; !!! unneeded store
   0x0000000000400542 <+66>:    mov    %r8d,0x4(%rdi)	; !!! ditto
   0x0000000000400546 <+70>:    jne    0x400530 <Adler32::Update(void const*, unsigned int)+48>
   0x0000000000400548 <+72>:    mov    %ecx,%eax
   0x000000000040054a <+74>:    mul    %r10d
   0x000000000040054d <+77>:    mov    %r8d,%eax
   0x0000000000400550 <+80>:    shr    $0xf,%edx
   0x0000000000400553 <+83>:    imul   $0xfff1,%edx,%edx
   0x0000000000400559 <+89>:    sub    %edx,%ecx
   0x000000000040055b <+91>:    mul    %r10d
   0x000000000040055e <+94>:    mov    %ecx,(%rdi)	; !!! this could be done after the jne at +118
   0x0000000000400560 <+96>:    shr    $0xf,%edx
   0x0000000000400563 <+99>:    imul   $0xfff1,%edx,%edx
   0x0000000000400569 <+105>:   sub    %edx,%r8d
   0x000000000040056c <+108>:   test   %r9d,%r9d
   0x000000000040056f <+111>:   mov    %r9d,%edx
   0x0000000000400572 <+114>:   mov    %r8d,0x4(%rdi)	; !!! ditto
   0x0000000000400576 <+118>:   jne    0x400510 <Adler32::Update(void const*, unsigned int)+16>
   0x0000000000400578 <+120>:   repz retq 

The above code is generated w/ 7.3.1, 6.3.1 generates the exact same code. 

8.0.1  and 8.1.1 generates somewhat different code, longer by 32 bytes, but the placing of the stores are the same. The size difference is odd, but I'll open another bug for that.

Platform: AMD64 (FX-8150), Debian 9.4

$ g++-6.3.1 -v
Using built-in specs.
COLLECT_GCC=g++-6.3.1
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/6.3.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --enable-languages=c,c++ --disable-multilib --program-suffix=-6.3.1 --disable-bootstrap --enable-checking=release CFLAGS='-O2 -march=native' CXXFLAGS='-O2 -march=native'
Thread model: posix
gcc version 6.3.1 20170120 (GCC)

$ g++-7.3.1 -v
Using built-in specs.
COLLECT_GCC=g++-7.3.1
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/7.3.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --enable-languages=c,c++ --disable-multilib --program-suffix=-7.3.1 --disable-bootstrap CFLAGS='-O2 -march=native -mtune=native' CXXFLAGS='-O2 -march=native -mtune=native'
Thread model: posix
gcc version 7.3.1 20180429 (GCC)

$ g++-8.0.1 -v
Using built-in specs.
COLLECT_GCC=g++-8.0.1
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/8.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --enable-languages=c,c++ --disable-multilib --program-suffix=-8.0.1 --disable-bootstrap CFLAGS='-O2 -march=native' CXXFLAGS='-O2 -march=native'
Thread model: posix
gcc version 8.0.1 20180214 (experimental) (GCC)

$ g++-8.1.1 -v
Using built-in specs.
COLLECT_GCC=g++-8.1.1
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/8.1.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --enable-languages=c,c++ --disable-multilib --program-suffix=-8.1.1 --disable-bootstrap CFLAGS='-O2 -march=native -mtune=native' CXXFLAGS='-O2 -march=native -mtune=native'
Thread model: posix
gcc version 8.1.1 20180502 (GCC)


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] code size regression vs 7.x`
### open_at : `2018-05-03T21:21:28Z`
### last_modified_date : `2023-07-07T10:33:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85640
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.1`
### severity : `normal`
### contents :
Created attachment 44062
source

Attached the source of a simple Adler32 checksum class. The Update() fn is 32 bytes longer compared to the code generated with 7.3.1.

Dump of assembler code for function Adler32::Update(void const*, unsigned int):
7.3.1 0x0000000000400500 <+0>:     test   %edx,%edx
7.3.1 0x0000000000400502 <+2>:     je     0x400578 <Adler32::Update(void const*, unsigned int)+120>
7.3.1 0x0000000000400504 <+4>:     mov    (%rdi),%ecx
7.3.1 0x0000000000400506 <+6>:     mov    0x4(%rdi),%r8d
7.3.1 0x000000000040050a <+10>:    mov    $0x80078071,%r10d
7.3.1 0x0000000000400510 <+16>:    xor    %r9d,%r9d
7.3.1 0x0000000000400513 <+19>:    cmp    $0x15af,%edx
7.3.1 0x0000000000400519 <+25>:    jbe    0x400527 <Adler32::Update(void const*, unsigned int)+39>
7.3.1 0x000000000040051b <+27>:    lea    -0x15b0(%rdx),%r9d
7.3.1 0x0000000000400522 <+34>:    mov    $0x15b0,%edx
7.3.1 0x0000000000400527 <+39>:    lea    -0x1(%rdx),%eax
7.3.1 0x000000000040052a <+42>:    lea    0x1(%rsi,%rax,1),%rdx
7.3.1 0x000000000040052f <+47>:    nop
7.3.1 0x0000000000400530 <+48>:    add    $0x1,%rsi
7.3.1 0x0000000000400534 <+52>:    movzbl -0x1(%rsi),%eax
7.3.1 0x0000000000400538 <+56>:    add    %eax,%ecx
7.3.1 0x000000000040053a <+58>:    add    %ecx,%r8d
7.3.1 0x000000000040053d <+61>:    cmp    %rdx,%rsi
7.3.1 0x0000000000400540 <+64>:    mov    %ecx,(%rdi)
7.3.1 0x0000000000400542 <+66>:    mov    %r8d,0x4(%rdi)
7.3.1 0x0000000000400546 <+70>:    jne    0x400530 <Adler32::Update(void const*, unsigned int)+48>
7.3.1 0x0000000000400548 <+72>:    mov    %ecx,%eax
7.3.1 0x000000000040054a <+74>:    mul    %r10d
7.3.1 0x000000000040054d <+77>:    mov    %r8d,%eax
7.3.1 0x0000000000400550 <+80>:    shr    $0xf,%edx
7.3.1 0x0000000000400553 <+83>:    imul   $0xfff1,%edx,%edx
7.3.1 0x0000000000400559 <+89>:    sub    %edx,%ecx
7.3.1 0x000000000040055b <+91>:    mul    %r10d
7.3.1 0x000000000040055e <+94>:    mov    %ecx,(%rdi)
7.3.1 0x0000000000400560 <+96>:    shr    $0xf,%edx
7.3.1 0x0000000000400563 <+99>:    imul   $0xfff1,%edx,%edx
7.3.1 0x0000000000400569 <+105>:   sub    %edx,%r8d
7.3.1 0x000000000040056c <+108>:   test   %r9d,%r9d
7.3.1 0x000000000040056f <+111>:   mov    %r9d,%edx
7.3.1 0x0000000000400572 <+114>:   mov    %r8d,0x4(%rdi)
7.3.1 0x0000000000400576 <+118>:   jne    0x400510 <Adler32::Update(void const*, unsigned int)+16>
7.3.1 0x0000000000400578 <+120>:   repz retq 

Dump of assembler code for function Adler32::Update(void const*, unsigned int):
8.1.1 0x0000000000400500 <+0>:     test   %edx,%edx
8.1.1 0x0000000000400502 <+2>:     je     0x400598 <Adler32::Update(void const*, unsigned int)+152>
8.1.1 0x0000000000400508 <+8>:     mov    (%rdi),%ecx
8.1.1 0x000000000040050a <+10>:    mov    0x4(%rdi),%r8d
8.1.1 0x000000000040050e <+14>:    push   %rbx
8.1.1 0x000000000040050f <+15>:    mov    $0x80078071,%ebx
8.1.1 0x0000000000400514 <+20>:    nopl   0x0(%rax)
8.1.1 0x0000000000400518 <+24>:    xor    %r11d,%r11d
8.1.1 0x000000000040051b <+27>:    cmp    $0x15af,%edx
8.1.1 0x0000000000400521 <+33>:    jbe    0x40052f <Adler32::Update(void const*, unsigned int)+47>
8.1.1 0x0000000000400523 <+35>:    lea    -0x15b0(%rdx),%r11d
8.1.1 0x000000000040052a <+42>:    mov    $0x15b0,%edx
8.1.1 0x000000000040052f <+47>:    mov    %edx,%r10d
8.1.1 0x0000000000400532 <+50>:    mov    %rsi,%rax
8.1.1 0x0000000000400535 <+53>:    add    %rsi,%r10
8.1.1 0x0000000000400538 <+56>:    nopl   0x0(%rax,%rax,1)
8.1.1 0x0000000000400540 <+64>:    add    $0x1,%rax
8.1.1 0x0000000000400544 <+68>:    movzbl -0x1(%rax),%r9d
8.1.1 0x0000000000400549 <+73>:    add    %r9d,%ecx
8.1.1 0x000000000040054c <+76>:    add    %ecx,%r8d
8.1.1 0x000000000040054f <+79>:    mov    %ecx,(%rdi)
8.1.1 0x0000000000400551 <+81>:    mov    %r8d,0x4(%rdi)
8.1.1 0x0000000000400555 <+85>:    cmp    %r10,%rax
8.1.1 0x0000000000400558 <+88>:    jne    0x400540 <Adler32::Update(void const*, unsigned int)+64>
8.1.1 0x000000000040055a <+90>:    lea    -0x1(%rdx),%eax
8.1.1 0x000000000040055d <+93>:    lea    0x1(%rsi,%rax,1),%rsi
8.1.1 0x0000000000400562 <+98>:    mov    %ecx,%eax
8.1.1 0x0000000000400564 <+100>:   mul    %ebx
8.1.1 0x0000000000400566 <+102>:   mov    %r8d,%eax
8.1.1 0x0000000000400569 <+105>:   shr    $0xf,%edx
8.1.1 0x000000000040056c <+108>:   imul   $0xfff1,%edx,%edx
8.1.1 0x0000000000400572 <+114>:   sub    %edx,%ecx
8.1.1 0x0000000000400574 <+116>:   mul    %ebx
8.1.1 0x0000000000400576 <+118>:   mov    %ecx,(%rdi)
8.1.1 0x0000000000400578 <+120>:   shr    $0xf,%edx
8.1.1 0x000000000040057b <+123>:   imul   $0xfff1,%edx,%edx
8.1.1 0x0000000000400581 <+129>:   sub    %edx,%r8d
8.1.1 0x0000000000400584 <+132>:   mov    %r11d,%edx
8.1.1 0x0000000000400587 <+135>:   mov    %r8d,0x4(%rdi)
8.1.1 0x000000000040058b <+139>:   test   %r11d,%r11d
8.1.1 0x000000000040058e <+142>:   jne    0x400518 <Adler32::Update(void const*, unsigned int)+24>
8.1.1 0x0000000000400590 <+144>:   pop    %rbx
8.1.1 0x0000000000400591 <+145>:   retq   
8.1.1 0x0000000000400592 <+146>:   nopw   0x0(%rax,%rax,1)
8.1.1 0x0000000000400598 <+152>:   retq   

Here is an interwoven version, hopefully easier to follow:
7.3.1 0x0000000000400500 <+0>:     test   %edx,%edx
7.3.1 0x0000000000400502 <+2>:     je     0x400578 <Adler32::Update(void const*, unsigned int)+120>
7.3.1 0x0000000000400504 <+4>:     mov    (%rdi),%ecx
7.3.1 0x0000000000400506 <+6>:     mov    0x4(%rdi),%r8d
7.3.1 0x000000000040050a <+10>:    mov    $0x80078071,%r10d

8.1.1 0x0000000000400500 <+0>:     test   %edx,%edx
8.1.1 0x0000000000400502 <+2>:     je     0x400598 <Adler32::Update(void const*, unsigned int)+152>
8.1.1 0x0000000000400508 <+8>:     mov    (%rdi),%ecx
8.1.1 0x000000000040050a <+10>:    mov    0x4(%rdi),%r8d
8.1.1 0x000000000040050e <+14>:    push   %rbx
8.1.1 0x000000000040050f <+15>:    mov    $0x80078071,%ebx
8.1.1 0x0000000000400514 <+20>:    nopl   0x0(%rax)

Two things so far:
- the je is 6 bytes in 8.1.1 vs 2 bytes in 7.3.1 because the jump offset can't fit in a byte
- in 8.1.1 ebx is used for the modulo magic, which is callee saved, so have to push before use



7.3.1 0x0000000000400510 <+16>:    xor    %r9d,%r9d
7.3.1 0x0000000000400513 <+19>:    cmp    $0x15af,%edx
7.3.1 0x0000000000400519 <+25>:    jbe    0x400527 <Adler32::Update(void const*, unsigned int)+39>
7.3.1 0x000000000040051b <+27>:    lea    -0x15b0(%rdx),%r9d
7.3.1 0x0000000000400522 <+34>:    mov    $0x15b0,%edx
7.3.1 0x0000000000400527 <+39>:    lea    -0x1(%rdx),%eax
7.3.1 0x000000000040052a <+42>:    lea    0x1(%rsi,%rax,1),%rdx
7.3.1 0x000000000040052f <+47>:    nop

8.1.1 0x0000000000400518 <+24>:    xor    %r11d,%r11d
8.1.1 0x000000000040051b <+27>:    cmp    $0x15af,%edx
8.1.1 0x0000000000400521 <+33>:    jbe    0x40052f <Adler32::Update(void const*, unsigned int)+47>
8.1.1 0x0000000000400523 <+35>:    lea    -0x15b0(%rdx),%r11d
8.1.1 0x000000000040052a <+42>:    mov    $0x15b0,%edx
8.1.1 0x000000000040052f <+47>:    mov    %edx,%r10d
8.1.1 0x0000000000400532 <+50>:    mov    %rsi,%rax
8.1.1 0x0000000000400535 <+53>:    add    %rsi,%r10
8.1.1 0x0000000000400538 <+56>:    nopl   0x0(%rax,%rax,1)

This is the inner loop init, pretty similar.



7.3.1 0x0000000000400530 <+48>:    add    $0x1,%rsi
7.3.1 0x0000000000400534 <+52>:    movzbl -0x1(%rsi),%eax
7.3.1 0x0000000000400538 <+56>:    add    %eax,%ecx
7.3.1 0x000000000040053a <+58>:    add    %ecx,%r8d
7.3.1 0x000000000040053d <+61>:    cmp    %rdx,%rsi
7.3.1 0x0000000000400540 <+64>:    mov    %ecx,(%rdi)
7.3.1 0x0000000000400542 <+66>:    mov    %r8d,0x4(%rdi)
7.3.1 0x0000000000400546 <+70>:    jne    0x400530 <Adler32::Update(void const*, unsigned int)+48>

8.1.1 0x0000000000400540 <+64>:    add    $0x1,%rax
8.1.1 0x0000000000400544 <+68>:    movzbl -0x1(%rax),%r9d
8.1.1 0x0000000000400549 <+73>:    add    %r9d,%ecx
8.1.1 0x000000000040054c <+76>:    add    %ecx,%r8d
8.1.1 0x000000000040054f <+79>:    mov    %ecx,(%rdi)
8.1.1 0x0000000000400551 <+81>:    mov    %r8d,0x4(%rdi)
8.1.1 0x0000000000400555 <+85>:    cmp    %r10,%rax
8.1.1 0x0000000000400558 <+88>:    jne    0x400540 <Adler32::Update(void const*, unsigned int)+64>

These are the same, except the cmp is before/after the two stores, and the movzbl and the first add is one byte cheaper in 7.3.1.


7.3.1 0x0000000000400548 <+72>:    mov    %ecx,%eax
7.3.1 0x000000000040054a <+74>:    mul    %r10d
7.3.1 0x000000000040054d <+77>:    mov    %r8d,%eax
7.3.1 0x0000000000400550 <+80>:    shr    $0xf,%edx
7.3.1 0x0000000000400553 <+83>:    imul   $0xfff1,%edx,%edx
7.3.1 0x0000000000400559 <+89>:    sub    %edx,%ecx
7.3.1 0x000000000040055b <+91>:    mul    %r10d
7.3.1 0x000000000040055e <+94>:    mov    %ecx,(%rdi)
7.3.1 0x0000000000400560 <+96>:    shr    $0xf,%edx
7.3.1 0x0000000000400563 <+99>:    imul   $0xfff1,%edx,%edx
7.3.1 0x0000000000400569 <+105>:   sub    %edx,%r8d
7.3.1 0x000000000040056c <+108>:   test   %r9d,%r9d
7.3.1 0x000000000040056f <+111>:   mov    %r9d,%edx
7.3.1 0x0000000000400572 <+114>:   mov    %r8d,0x4(%rdi)
7.3.1 0x0000000000400576 <+118>:   jne    0x400510 <Adler32::Update(void const*, unsigned int)+16>
7.3.1 0x0000000000400578 <+120>:   repz retq 

8.1.1 0x000000000040055a <+90>:    lea    -0x1(%rdx),%eax
8.1.1 0x000000000040055d <+93>:    lea    0x1(%rsi,%rax,1),%rsi
8.1.1 0x0000000000400562 <+98>:    mov    %ecx,%eax
8.1.1 0x0000000000400564 <+100>:   mul    %ebx
8.1.1 0x0000000000400566 <+102>:   mov    %r8d,%eax
8.1.1 0x0000000000400569 <+105>:   shr    $0xf,%edx
8.1.1 0x000000000040056c <+108>:   imul   $0xfff1,%edx,%edx
8.1.1 0x0000000000400572 <+114>:   sub    %edx,%ecx
8.1.1 0x0000000000400574 <+116>:   mul    %ebx
8.1.1 0x0000000000400576 <+118>:   mov    %ecx,(%rdi)
8.1.1 0x0000000000400578 <+120>:   shr    $0xf,%edx
8.1.1 0x000000000040057b <+123>:   imul   $0xfff1,%edx,%edx
8.1.1 0x0000000000400581 <+129>:   sub    %edx,%r8d
8.1.1 0x0000000000400584 <+132>:   mov    %r11d,%edx
8.1.1 0x0000000000400587 <+135>:   mov    %r8d,0x4(%rdi)
8.1.1 0x000000000040058b <+139>:   test   %r11d,%r11d
8.1.1 0x000000000040058e <+142>:   jne    0x400518 <Adler32::Update(void const*, unsigned int)+24>
8.1.1 0x0000000000400590 <+144>:   pop    %rbx
8.1.1 0x0000000000400591 <+145>:   retq   
8.1.1 0x0000000000400592 <+146>:   nopw   0x0(%rax,%rax,1)
8.1.1 0x0000000000400598 <+152>:   retq   

The loop variables in 8.1.1 handled a bit differently, that's why the minor size increase. The two lea's here in the 8.1.1 version are very similar to the ones before the inner loop in 7.3.1, those calculate the ptr range of the inner loop. These lea's here calculate the start ptr for the next inner loop run, however, this ptr is already present in eax. I don't know if this is some special optimization, or eax should have been used but the compiler missed it.

The double retq at the end is interesting, but maybe the debug info is to blame.

cmdline used to compile:
$ g++-7.3.1 -g -O3 -Wall a32.cpp
$ g++-8.1.1 -g -O3 -Wall a32.cpp

Platform is AMD64 (FX-8150), Debian 9.4

$ g++-7.3.1 -v
Using built-in specs.
COLLECT_GCC=g++-7.3.1
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/7.3.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --enable-languages=c,c++ --disable-multilib --program-suffix=-7.3.1 --disable-bootstrap CFLAGS='-O2 -march=native -mtune=native' CXXFLAGS='-O2 -march=native -mtune=native'
Thread model: posix
gcc version 7.3.1 20180429 (GCC)

$ g++-8.1.1 -v
Using built-in specs.
COLLECT_GCC=g++-8.1.1
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/8.1.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --enable-languages=c,c++ --disable-multilib --program-suffix=-8.1.1 --disable-bootstrap CFLAGS='-O2 -march=native -mtune=native' CXXFLAGS='-O2 -march=native -mtune=native'
Thread model: posix
gcc version 8.1.1 20180502 (GCC)


---


### compiler : `gcc`
### title : `Missed optimization for value-init of variable-sized allocation`
### open_at : `2018-05-07T14:52:10Z`
### last_modified_date : `2021-07-23T22:41:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85680
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `enhancement`
### contents :
https://godbolt.org/g/6yZEKf (compiled with -O3)


char* variableSize(long n) {
    auto p = new char[n]();
    for (int i = 0; i < n; i++) {
        p[i] = 0xff;
    }
    return p;
}

char* small() {
    return variableSize(8);
}

char* large() {
    return variableSize(10'000);
}


The variableSize() case generates two calls two memset with the both being conditional on n > 0 if I'm reading this right), but the two checks are done in different ways. That may be a second optimizer bug that it missed a jump-threading opportunity.

variableSize(long):
        push    rbx
        mov     rbx, rdi
        call    operator new[](unsigned long)
        mov     rcx, rax
        mov     rax, rbx
        sub     rax, 1
        js      .L5
        lea     rax, [rbx-2]
        mov     edx, 1
        mov     rdi, rcx
        cmp     rax, -1
        cmovge  rdx, rbx
        xor     esi, esi
        call    memset
        mov     rcx, rax
.L5:
        test    rbx, rbx
        jle     .L1
        mov     rdi, rcx
        mov     rdx, rbx
        mov     esi, 255
        call    memset
        mov     rcx, rax
.L1:
        mov     rax, rcx
        pop     rbx
        ret

Note that when g++ can see the allocation size it *does* elide the initial memset:


small(): # @small()
  push rax
  mov edi, 8
  call operator new[](unsigned long)
  mov qword ptr [rax], -1
  pop rcx
  ret
large(): # @large()
  push rbx
  mov edi, 10000
  call operator new[](unsigned long)
  mov rbx, rax
  mov esi, 255
  mov edx, 10000
  mov rdi, rax
  call memset
  mov rax, rbx
  pop rbx
  ret



Related clang bug: https://bugs.llvm.org/show_bug.cgi?id=37351


---


### compiler : `gcc`
### title : `At -Os nontrivial ctor does not use SSE to zero`
### open_at : `2018-05-08T14:54:19Z`
### last_modified_date : `2021-08-10T17:52:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85697
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `enhancement`
### contents :
struct alignas(16) A {
    A (void) :a(0),b(0),c(0),d(0) {}
    int a,b,c,d;
};
__attribute__((noinline)) void UseA (A& a) { a.a=1; }

int main (void)
{
    A a {};
    UseA (a);
    return a.a;
}

-Os -march=native on Haswell, generates:

main:
        subq    $16, %rsp
        movq    %rsp, %rdi
        movq    $0, (%rsp)
        movq    $0, 8(%rsp)
        call    _Z4UseAR1A
        movl    (%rsp), %eax
        addq    $16, %rsp
        ret

Using 16 bytes to zero A with 2 movq. With -O3:

main:
        subq    $24, %rsp
        vpxor   %xmm0, %xmm0, %xmm0
        movq    %rsp, %rdi
        vmovaps %xmm0, (%rsp)
        call    _Z4UseAR1A
        movl    (%rsp), %eax
        addq    $24, %rsp
        ret

using only 9 bytes for pxor/movaps. With -mno-avx it is 7 bytes for xorps/movaps. With multiple objects of type A, the savings would be even greater, since only one pxor would be needed for all and only 4 bytes per object for zeroing.

Removing A constructor also results in SSE instruction use.


---


### compiler : `gcc`
### title : `bad codegen for looped assignment of primitives at -O2`
### open_at : `2018-05-09T18:37:35Z`
### last_modified_date : `2021-08-10T18:07:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85720
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `enhancement`
### contents :
https://godbolt.org/g/qp19Cv

using SIZE_T = decltype(sizeof(0));
void fill(char* p, SIZE_T n) {
    for (SIZE_T i = 0; i < n; i++){
        p[i] = -1;
    }
}

fill(char*, unsigned long):
        test    rsi, rsi
        je      .L1
        add     rsi, rdi
.L3:
        mov     BYTE PTR [rdi], -1
        add     rdi, 1
        cmp     rdi, rsi
        jne     .L3
.L1:
        ret

At -O3 it basically just tail-calls memset.

Also applies to other types than char, but char is most egregious.

This ticket is spun out of from https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85707


---


### compiler : `gcc`
### title : `bad codegen for looped copy of primitives at -O2 and -O3 (differently bad)`
### open_at : `2018-05-09T18:56:09Z`
### last_modified_date : `2021-08-29T01:13:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85721
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `enhancement`
### contents :
https://godbolt.org/g/Gg9fFt

Related to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85720, but filed separately because this also affects -O3. Similarly, while this affects types other than char, char is most egregious.

using SIZE_T = decltype(sizeof(0));
void copy(char* out, const char* in, SIZE_T n) {
    for (SIZE_T i = 0; i < n; i++){
        out[i] = in[i];
    }
}

This should probably just be compiled to check size then jmp memmove. At O2 it copies byte-by-byte:

copy(char*, char const*, unsigned long):
        test    rdx, rdx
        je      .L1
        xor     eax, eax
.L3:
        movzx   ecx, BYTE PTR [rsi+rax]
        mov     BYTE PTR [rdi+rax], cl
        add     rax, 1
        cmp     rdx, rax
        jne     .L3
.L1:
        ret


At O3 it generates a TON of code:

copy(char*, char const*, unsigned long):
  test rdx, rdx
  je .L1
  lea rax, [rsi+16]
  cmp rdi, rax
  lea rax, [rdi+16]
  setnb cl
  cmp rsi, rax
  setnb al
  or cl, al
  je .L7
  lea rax, [rdx-1]
  cmp rax, 14
  jbe .L7
  mov rcx, rdx
  xor eax, eax
  and rcx, -16
.L4:
  movdqu xmm0, XMMWORD PTR [rsi+rax]
  movups XMMWORD PTR [rdi+rax], xmm0
  add rax, 16
  cmp rax, rcx
  jne .L4
  mov rax, rdx
  and rax, -16
  cmp rdx, rax
  je .L1
  movzx ecx, BYTE PTR [rsi+rax]
  mov BYTE PTR [rdi+rax], cl
  lea rcx, [rax+1]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+1+rax]
  mov BYTE PTR [rdi+1+rax], cl
  lea rcx, [rax+2]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+2+rax]
  mov BYTE PTR [rdi+2+rax], cl
  lea rcx, [rax+3]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+3+rax]
  mov BYTE PTR [rdi+3+rax], cl
  lea rcx, [rax+4]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+4+rax]
  mov BYTE PTR [rdi+4+rax], cl
  lea rcx, [rax+5]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+5+rax]
  mov BYTE PTR [rdi+5+rax], cl
  lea rcx, [rax+6]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+6+rax]
  mov BYTE PTR [rdi+6+rax], cl
  lea rcx, [rax+7]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+7+rax]
  mov BYTE PTR [rdi+7+rax], cl
  lea rcx, [rax+8]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+8+rax]
  mov BYTE PTR [rdi+8+rax], cl
  lea rcx, [rax+9]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+9+rax]
  mov BYTE PTR [rdi+9+rax], cl
  lea rcx, [rax+10]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+10+rax]
  mov BYTE PTR [rdi+10+rax], cl
  lea rcx, [rax+11]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+11+rax]
  mov BYTE PTR [rdi+11+rax], cl
  lea rcx, [rax+12]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+12+rax]
  mov BYTE PTR [rdi+12+rax], cl
  lea rcx, [rax+13]
  cmp rdx, rcx
  jbe .L1
  movzx ecx, BYTE PTR [rsi+13+rax]
  mov BYTE PTR [rdi+13+rax], cl
  lea rcx, [rax+14]
  cmp rdx, rcx
  jbe .L1
  movzx edx, BYTE PTR [rsi+14+rax]
  mov BYTE PTR [rdi+14+rax], dl
  ret
.L7:
  xor eax, eax
.L3:
  movzx ecx, BYTE PTR [rsi+rax]
  mov BYTE PTR [rdi+rax], cl
  add rax, 1
  cmp rdx, rax
  jne .L3
.L1:
  ret

A) This should probably just call memmove which has a tuned implementation for many architectures and uses ifunc dispatch to choose the right one based on the runtime CPU rather than the compile-time settings. Also, all functions like this for all types would all just jump to a single function, there should be I$ advantages.

B) If you really want to emit code for this rather than calling into libc, it is probably best to use their technique of overlapped reads and writes for the last vector rather than going into an unrolled byte-by-byte loop: https://github.molgen.mpg.de/git-mirror/glibc/blob/20003c49884422da7ffbc459cdeee768a6fee07b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S#L331-L335


---


### compiler : `gcc`
### title : `[7/8/9 Regression] div C1 to div C2 match.pd suboptimization`
### open_at : `2018-05-09T23:32:46Z`
### last_modified_date : `2018-12-06T15:28:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85726
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
See the attached test-case, ready to drop into testsuite/gcc.dg.

The suboptimization results in separate div and mod sequences
across all architectures where division or modulus by a constant
is implemented as a widened multiplication and right-shift
(actually most; at least x86_64, mips, ppc, aarch64).  The
test-case doesn't fail for e.g. sparc, that doesn't do it that
way.

This is a somewhat long-standing suboptimality, but a regression.
The title hints at the cause and solution; patch to be posted.


---


### compiler : `gcc`
### title : `complex code for modifying lowest byte in a 4-byte vector`
### open_at : `2018-05-10T11:38:57Z`
### last_modified_date : `2021-10-12T16:21:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85730
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 44109
reduced testcase

The attached testcase has 3 implementations of the same function, yet the compiled code differs: (@ -O3)

foo:
        movsx   edx, dil
        mov     eax, edi
        add     edx, edx
        mov     al, dl
        ret

bar:
        mov     eax, edi
        add     al, al
        ret

baz:
        movsx   edx, dil
        mov     eax, edi
        add     edx, edx
        mov     al, dl
        ret

bar() has the shortest code and is also using fewer registers. I tried benchmarking all 3 functions on a Skylake CPU; I could not find out which function is the fastest (the jitter was too high).

The difference between foo() and bar() is that bar() is compiled with -fno-tree-ccp -fno-tree-fre. baz() has one extra constant in the code, which needs to be propagated in foor() and bar().


---


### compiler : `gcc`
### title : `Non-optimal determining maximum in a loop`
### open_at : `2018-05-10T20:33:58Z`
### last_modified_date : `2021-08-21T20:19:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85740
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
There is a problem with loops determining the maximum (or minimum)
of a value. I noticed this when looking at inline code generated
by gfortran for maxloc/minloc.

Consider the two programs which select the position of a
maximum from a rather large integer array. They are identical,
except that b.c uses __builtin_expect in the inner loop:

$ cat b.c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <x86intrin.h>

#define N (1<<28)

int foo(int *a, int n)
{
  int m, nm;
  int i;

  m = -20000;
  nm = -1;
  for (i=0; i<n; i++)
    {
      if (__builtin_expect (a[i] > m, 0))
        {
          m = a[i];
          nm = i;
        }
    }
  return nm;
}

int 
main(int argc, char **argv) {
  unsigned long long t1, t2;

  int *p = malloc(N * sizeof(int));
  for (int i=0; i<N; i++)
    p[i] = random();
  t1 = __rdtsc();
  int res = foo(p, N);
  t2 = __rdtsc();
  printf("%d\n", res);
  printf("time used: %llu cycles\n", t2-t1);
  return 0;
}
$ cat c.c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <x86intrin.h>

#define N (1<<28)

int foo(int *a, int n)
{
  int m, nm;
  int i;

  m = -20000;
  nm = -1;
  for (i=0; i<n; i++)
    {
      if (a[i] > m)
        {
          m = a[i];
          nm = i;
        }
    }
  return nm;
}

int 
main(int argc, char **argv) {
  unsigned long long t1, t2;

  int *p = malloc(N * sizeof(int));
  for (int i=0; i<N; i++)
    p[i] = random();
  t1 = __rdtsc();
  int res = foo(p, N);
  t2 = __rdtsc();
  printf("%d\n", res);
  printf("time used: %llu cycles\n", t2-t1);
  return 0;
}

$ gcc -Ofast b.c && ./a.out
13068230
time used: 281813216 cycles
$ gcc -Ofast c.c && ./a.out
13068230
time used: 492848802 cycles

So, the program using the __builtin_expect is faster by
a factor of around 1.75, quite significant.

The problem is obvious from the assembly. The inner loop has, for
the version without the hint,

.L6:
	movq	%rcx, %rdx
.L4:
	movl	(%rdi,%rdx,4), %ecx
	cmpl	%ecx, %esi
	jge	.L3
	movl	%edx, %eax
	movl	%ecx, %esi
.L3:
	leaq	1(%rdx), %rcx
	cmpq	%rdx, %r8
	jne	.L6
.L1:
	ret

so what is likely to be the most common case (not finding a higher
maximum) leads to a jump over two instrucitons.  On the other hand,
the assembly for the version with the hint is

.L6:
	movq	%rcx, %rdx
.L4:
	movl	(%rdi,%rdx,4), %ecx
	cmpl	%ecx, %esi
	jl	.L8
.L3:
	leaq	1(%rdx), %rcx
	cmpq	%rdx, %r8
	jne	.L6
.L1:
	ret
	.p2align 4,,10
	.p2align 3
.L8:
	movl	%edx, %eax
	movl	%ecx, %esi
	jmp	.L3

so the control flow is not interrupted in the most common case.

The version with the hint becomes even faster when unrolling
the loop:

$ gcc -Ofast -funroll-loops b.c && ./a.out
13068230
time used: 254444202 cycles

another speed gain of around 15%.

This is for an AMD Ryzen 7 1700X.


---


### compiler : `gcc`
### title : `Premature evaluation of __builtin_constant_p?`
### open_at : `2018-05-11T07:53:34Z`
### last_modified_date : `2019-10-22T14:46:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85746
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
int f(int a,int b){
  int c = __builtin_constant_p(a < b);
  return c;
}

In C or C++98, __builtin_constant_p is passed to the middle-end for further optimization. In C++11, the front-end produces "c = 0;". That's because C++11 says we should check if the initializer can be constexpr-evaluated, and inside constexpr evaluation we force the early (pessimistic) evaluation of __builtin_constant_p.
Maybe we should only evaluate __builtin_constant_p to false when we are committed to a constexpr evaluation, not for tentative ones as in initializers? Or maybe we need different versions of __builtin_constant_p for people who want to use it as if constexpr() and for those who want late optimization?


---


### compiler : `gcc`
### title : `suboptimal code without constexpr`
### open_at : `2018-05-11T09:01:46Z`
### last_modified_date : `2021-12-28T05:33:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85747
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Consider the following code snippet:

// Bubble-like sort. Anything complex enough will work
template <class It>
constexpr void sort(It first, It last) {
    for (;first != last; ++first) {
        auto it = first;
        ++it;
        for (; it != last; ++it) {
            if (*it < *first) {
                auto tmp = *it;
                *it = *first;
                *first = tmp;
            }
        }
    }
}

static int generate() {
    int a[7] = {3, 7, 4, 2, 8, 0, 1};
    sort(a + 0, a + 7);
    return a[0] + a[6];
}

int no_constexpr() {
    return generate();
}



Above code generates ~30 assembly instructions instead of just generating:

no_constexpr():
  mov eax, 8
  ret



But if we change `static` to `constexpr` then the compiler will optimize the code correctly.

Could the compiler detect that `a[7]` holds values known at compile time and force the constexpr on `sort(a + 0, a + 7);`? Could the compiler detect that the function `generate()` is an `__attribute__((const))` function without arguments and fully evaluate it's body?


---


### compiler : `gcc`
### title : `[8/9 Regression] range-v3 abstraction overhead not optimized away`
### open_at : `2018-05-12T19:24:54Z`
### last_modified_date : `2019-04-17T15:53:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85762
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 44124
preprocessed source code for run_range()

GCC 8 is less aggressive than earlier versions when eliminating abstraction overhead in the range-v3 library, which can be seen with the function

  #include <vector>
  #include <range/v3/all.hpp>

  long run_range(std::vector<int> const &lengths, long to_find)
  {
    auto const found_index = ranges::distance(lengths
            | ranges::view::transform(ranges::convert_to<long>{})
            | ranges::view::partial_sum()
            | ranges::view::take_while([=](auto const i) {
                  return !(to_find < i);
              }));
    return found_index;
  }


GCC 7 compiled the loop to

  <bb 5> [10.87%]:
  # it$_M_current_41 = PHI <_6(4), _27(8)>
  # it$16_26 = PHI <it$16_24(4), _31(8)>
  _53 = to_find_2(D) < it$16_26;

  <bb 6> [100.00%]:
  # it$_M_current_23 = PHI <it$_M_current_41(5), _27(7)>
  _20 = _7 == it$_M_current_23;
  _5 = _20 | _53;
  if (_5 != 0)
    goto <bb 9>; [7.36%]
  else
    goto <bb 7>; [92.64%]

  <bb 7> [92.60%]:
  _27 = it$_M_current_23 + 4;
  if (_7 != _27)
    goto <bb 8>; [3.75%]
  else
    goto <bb 6>; [96.25%]

  <bb 8> [3.47%]:
  _29 = MEM[(const int &)it$_M_current_23 + 4];
  _30 = (long int) _29;
  _31 = it$16_26 + _30;
  goto <bb 5>; [100.00%]

  <bb 9> [7.36%]:
  _33 = (long int) it$_M_current_23;
  _34 = (long int) _6;
  _35 = _33 - _34;
  _36 = _35 /[ex] 4;
  return _36;

while the loop compiled by GCC 8 updates some structures in each iteration

  <bb 5> [local count: 1478210893]:
  # it_47 = PHI <SR.352_183(4), _64(8)>
  # it$16$sum__115 = PHI <SR.353_184(4), _67(8)>
  _42 = to_find_2(D) < it$16$sum__115;

  <bb 6> [local count: 1651554780]:
  # it_30 = PHI <it_47(5), _64(7)>
  _46 = it_30 == SR.355_137;
  _40 = _42 | _46;
  if (_40 != 0)
    goto <bb 9>; [65.00%]
  else
    goto <bb 7>; [35.00%]

  <bb 7> [local count: 577812955]:
  SR.80_62 = MEM[(const struct __normal_iterator &)SR.354_185 + 24];
  MEM[(struct adaptor_cursor *)&pos] = SR.80_62;
  MEM[(struct box *)&D.417725].value = pos;
  SR.396_209 = MEM[(struct adaptor_cursor *)&D.417725];
  _64 = it_30 + 4;
  if (_64 != SR.396_209)
    goto <bb 8>; [70.00%]
  else
    goto <bb 6>; [30.00%]

  <bb 8> [local count: 404469068]:
  _65 = MEM[(const int &)it_30 + 4];
  _66 = (long int) _65;
  _67 = _66 + it$16$sum__115;
  goto <bb 5>; [100.00%]

  <bb 9> [local count: 1073279389]:
  _32 = it_30 - SR.352_183;
  _33 = _32 /[ex] 4;
  D.357125 ={v} {CLOBBER};
  D.311383 ={v} {CLOBBER};
  return _33;

which makes this loop about 10x slower on my computer.

GCC 8 also generates lots of code setting up the function that GCC 7 manages to eliminate.


This regression was introduced by r255510:

  2017-12-08  Martin Jambor  <mjambor@suse.cz>

        PR tree-optimization/83141
        * tree-sra.c (contains_vce_or_bfcref_p): Move up in the file, also
        test for MEM_REFs implicitely changing types with padding.  Remove
        inline keyword.
        (build_accesses_from_assign): Added contains_vce_or_bfcref_p checks.


To reproduce the problem, compile the attached file as

  g++ -O2 -S ranges.ii

and notice the difference in the generated code.


---


### compiler : `gcc`
### title : `multiply overflow (128 bit)`
### open_at : `2018-05-15T13:09:28Z`
### last_modified_date : `2021-12-14T22:03:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85791
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
Just noticed that code is generated differently when using __builtin_mul_overflow and an explicit check:

1. unsigned long long func(unsigned long long a, unsigned long long b)
{
        unsigned long long c;
        if (__builtin_mul_overflow(a, b, &c))
                return 0;
        return c;
}

yields:
func:
.LFB0:
        .cfi_startproc
        movq    %rdi, %rax
        mulq    %rsi
        jo      .L7
        rep ret
.L7:
        xorl    %eax, %eax
        ret

2. unsigned long long func(unsigned long long a, unsigned long long b)
{
        __uint128_t c = (__uint128_t) a * b;
        if (c > (unsigned long long) -1LL) {
                return 0;
        }
        return (unsigned long long) c;
}

yields slightly less efficient code:

func:
.LFB0:
        .cfi_startproc
        movq    %rdi, %rax
        mulq    %rsi
        cmpq    $0, %rdx
        jbe     .L2
        xorl    %eax, %eax
.L2:
        rep ret

3. clang/llvm can generate better code (identical) in both cases:

func:                                   # @func
        .cfi_startproc
# %bb.0:
        xorl    %ecx, %ecx
        movq    %rdi, %rax
        mulq    %rsi
        cmovoq  %rcx, %rax
        retq


---


### compiler : `gcc`
### title : `conversion from __v[48]su to __v[48]sf should use FMA`
### open_at : `2018-05-17T14:00:59Z`
### last_modified_date : `2021-09-07T13:42:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85819
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Testcase (cf. https://godbolt.org/g/UoU3zj):

using T = float;
using To [[gnu::vector_size(32)]] = T;
using From [[gnu::vector_size(32)]] = unsigned;

#define A2(I) (T)a[I], (T)a[1+I]
#define A4(I) A2(I), A2(2+I)
#define A8(I) A4(I), A4(4+I)

To f(From a) {
    return To{A8(0)};
}

This compiles to:
  vpand .LC0(%rip), %ymm0, %ymm1
  vpsrld $16, %ymm0, %ymm0
  vcvtdq2ps %ymm0, %ymm0
  vcvtdq2ps %ymm1, %ymm1
  vmulps .LC1(%rip), %ymm0, %ymm0
  vaddps %ymm0, %ymm1, %ymm0
  ret

The last vmulps and vaddps can be contracted to vfmadd132ps .LC1(%rip), %ymm1, %ymm0.

The same is true for vector_size(16).


---


### compiler : `gcc`
### title : `Performance regression from gcc 4.9.2`
### open_at : `2018-05-21T13:06:24Z`
### last_modified_date : `2021-05-30T04:41:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85854
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.0`
### severity : `normal`
### contents :
Created attachment 44154
Preprocessing file.

I got a serious (10%) performance degradation using cross-compiler for ARM.

Compiler version: 
arm-linux-gnueabihf-gcc-8 -v
Using built-in specs.
COLLECT_GCC=arm-linux-gnueabihf-gcc-8
COLLECT_LTO_WRAPPER=/usr/lib/gcc-cross/arm-linux-gnueabihf/8/lto-wrapper
Target: arm-linux-gnueabihf
Configured with: ../src/configure -v --with-pkgversion='Debian 8.1.0-3' --with-bugurl=file:///usr/share/doc/gcc-8/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libitm --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-multiarch --disable-sjlj-exceptions --with-arch=armv7-a --with-fpu=vfpv3-d16 --with-float=hard --with-mode=thumb --disable-werror --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=arm-linux-gnueabihf --program-prefix=arm-linux-gnueabihf- --includedir=/usr/arm-linux-gnueabihf/include
Thread model: posix
gcc version 8.1.0 (Debian 8.1.0-3) 

System: Linux x-debian 4.16.0-1-amd64 #1 SMP Debian 4.16.5-1 (2018-04-29) x86_64 GNU/Linux

Compiler options:
arm-linux-gnueabihf-gcc-8 -c  -DSTRICT_OFFSET -mfpu=neon -O1 -D_REENTRANT  -DTHREAD_TYPE=THREAD_TYPE_PTHREADS -fPIC -DNDEBUG -DCC_NO_RDS_SOCKET -DCC_NO_RDS_FILECNT -DCC_NO_RDS_NP -D_GNU_SOURCE -pedantic -Wall -Wextra -Wno-long-long -g -save-temps gosthash2012.c

The preprocessed file is attached. The most time-taking part of the code is the function named g.

The new version does not use neon vector ops (such as veor) at all, but the 4.9.2 on native ARM system does.

The system withoud degradation is 
Linux odroid 3.10.107 #1 SMP PREEMPT Wed Sep 13 18:48:10 CEST 2017 armv7l GNU/Linux

gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/arm-linux-gnueabihf/4.9/lto-wrapper
Target: arm-linux-gnueabihf
Configured with: ../src/configure -v --with-pkgversion='Debian 4.9.2-10' --with-bugurl=file:///usr/share/doc/gcc-4.9/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.9 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.9 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libitm --disable-libquadmath --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.9-armhf/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.9-armhf --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.9-armhf --with-arch-directory=arm --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-sjlj-exceptions --with-arch=armv7-a --with-fpu=vfpv3-d16 --with-float=hard --with-mode=thumb --enable-checking=release --build=arm-linux-gnueabihf --host=arm-linux-gnueabihf --target=arm-linux-gnueabihf
Thread model: posix
gcc version 4.9.2 (Debian 4.9.2-10)


---


### compiler : `gcc`
### title : `[8/9 regression] GCC omits array constant in .rodata causing a segmentation fault.`
### open_at : `2018-05-22T15:23:46Z`
### last_modified_date : `2023-08-11T17:08:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85873
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.1.0`
### severity : `normal`
### contents :
Created attachment 44162
Preprocessed reproducer

GCC 8.1.0 (Debian 8.1.0-3) omits array constant in .rodata when optimizations are enabled causing a segmentation fault. Previous versions (7.3.0 used for reference) included the array definition as expected.

Test.cpp:

    #include <cstdio>
    #include <initializer_list>
    
    constexpr std::initializer_list<char const*> retArray() noexcept
    {
        return std::initializer_list<char const*>{"Test 1", "Test 2", "Test 3"};
    }
    
    int main(int, char const* const*)
    {
        for (auto&& i : retArray())
        {
            std::puts(i);
        }
        return 0;
    }

Compiled with:
    gcc -std=c++17 -O1 -Wall -Wextra Test.cpp

Expected output:

    Test 1
    Test 2
    Test 3

Actual output:
    <SOME_GARBAGE>
    Segmenation fault.

 
Notes:
    * The preprocessed output has no functional differences between 7.3.0 and 8.1.0, which leads me to believe that this issue is not caused by changes to the C++ standard library.
    * Using objdump -s shows the .rodata section to be basically empty on 8.1.0, while containing the three strings on 7.3.0.
    * No error messages or warnings are emitted during compilation.
    * Building with -fsanitize=undefined does not result in any runtime messages prior to the segmentation fault.
    * Turning optimizations off yields the expected result.
    * Changing the C++ standard to C++11 or C++14 does not affect the result.

Environment details:
    g++ -v -std=c++17 -O1 -Wall -Wextra -save-temps Test.cpp

    Using built-in specs.
    COLLECT_GCC=g++
    COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/8/lto-wrapper
    OFFLOAD_TARGET_NAMES=nvptx-none
    OFFLOAD_TARGET_DEFAULT=1
    Target: x86_64-linux-gnu
    Configured with: ../src/configure -v --with-pkgversion='Debian 8.1.0-3' --with-bugurl=file:///usr/share/doc/gcc-8/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-8 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
    Thread model: posix
    gcc version 8.1.0 (Debian 8.1.0-3) 
    COLLECT_GCC_OPTIONS='-v' '-std=c++17' '-O1' '-Wall' '-Wextra' '-save-temps' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
     /usr/lib/gcc/x86_64-linux-gnu/8/cc1plus -E -quiet -v -imultiarch x86_64-linux-gnu -D_GNU_SOURCE Test.cpp -mtune=generic -march=x86-64 -std=c++17 -Wall -Wextra -O1 -fpch-preprocess -o Test.ii
    ignoring duplicate directory "/usr/include/x86_64-linux-gnu/c++/8"
    ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
    ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/8/../../../../x86_64-linux-gnu/include"
    #include "..." search starts here:
    #include <...> search starts here:
     /usr/include/c++/8
     /usr/include/x86_64-linux-gnu/c++/8
     /usr/include/c++/8/backward
     /usr/lib/gcc/x86_64-linux-gnu/8/include
     /usr/local/include
     /usr/lib/gcc/x86_64-linux-gnu/8/include-fixed
     /usr/include/x86_64-linux-gnu
     /usr/include
    End of search list.
    COLLECT_GCC_OPTIONS='-v' '-std=c++17' '-O1' '-Wall' '-Wextra' '-save-temps' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
     /usr/lib/gcc/x86_64-linux-gnu/8/cc1plus -fpreprocessed Test.ii -quiet -dumpbase Test.cpp -mtune=generic -march=x86-64 -auxbase Test -O1 -Wall -Wextra -std=c++17 -version -o Test.s
    GNU C++17 (Debian 8.1.0-3) version 8.1.0 (x86_64-linux-gnu)
    	compiled by GNU C version 8.1.0, GMP version 6.1.2, MPFR version 4.0.1, MPC version 1.1.0, isl version isl-0.19-GMP
    
    GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
    GNU C++17 (Debian 8.1.0-3) version 8.1.0 (x86_64-linux-gnu)
    	compiled by GNU C version 8.1.0, GMP version 6.1.2, MPFR version 4.0.1, MPC version 1.1.0, isl version isl-0.19-GMP
    
    GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
    Compiler executable checksum: 5cdcbbb44235e5167a7c38e34cfe93e5
    COLLECT_GCC_OPTIONS='-v' '-std=c++17' '-O1' '-Wall' '-Wextra' '-save-temps' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
     as -v --64 -o Test.o Test.s
    GNU assembler version 2.30 (x86_64-linux-gnu) using BFD version (GNU Binutils for Debian) 2.30
    COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/8/:/usr/lib/gcc/x86_64-linux-gnu/8/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/8/:/usr/lib/gcc/x86_64-linux-gnu/
    LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/8/:/usr/lib/gcc/x86_64-linux-gnu/8/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/8/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/8/../../../:/lib/:/usr/lib/
    COLLECT_GCC_OPTIONS='-v' '-std=c++17' '-O1' '-Wall' '-Wextra' '-save-temps' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
     /usr/lib/gcc/x86_64-linux-gnu/8/collect2 -plugin /usr/lib/gcc/x86_64-linux-gnu/8/liblto_plugin.so -plugin-opt=/usr/lib/gcc/x86_64-linux-gnu/8/lto-wrapper -plugin-opt=-fresolution=Test.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc --sysroot=/ --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu -dynamic-linker /lib64/ld-linux-x86-64.so.2 -pie /usr/lib/gcc/x86_64-linux-gnu/8/../../../x86_64-linux-gnu/Scrt1.o /usr/lib/gcc/x86_64-linux-gnu/8/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/8/crtbeginS.o -L/usr/lib/gcc/x86_64-linux-gnu/8 -L/usr/lib/gcc/x86_64-linux-gnu/8/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/8/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/8/../../.. Test.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/lib/gcc/x86_64-linux-gnu/8/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/8/../../../x86_64-linux-gnu/crtn.o
    COLLECT_GCC_OPTIONS='-v' '-std=c++17' '-O1' '-Wall' '-Wextra' '-save-temps' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
    )


---


### compiler : `gcc`
### title : `Really Simple "If" with one function call inside is not optimized efficiently`
### open_at : `2018-05-29T12:06:18Z`
### last_modified_date : `2021-08-19T18:37:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85971
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
GCC: 8 or 9/trunk

Optimization: O3 or Ofast

Code:

```
int PolyMod(int s);
void CreateChecksum(int isTestNet, int *mod) {
    if (isTestNet == 0) {
        *mod = PolyMod(5);
    } else {
        *mod = PolyMod(9);
    }
}
```

It is optimized very inefficiently. The
assembly has one branch.

However, if the compiler was as smart as the
people who develop it, he'd transform the code into
this:

```
int PolyMod(int s);
void CreateChecksum(int isTestNet, int *mod) {
    int a;
    if (isTestNet == 0) {
        a = 5;
    } else {
        a = 9;
    }
    *mod = PolyMod(a);
}
```

which compiles to assembly with zero branches.
Another way to reach the same efficient assembly would be

```
int PolyMod(int s);
void CreateChecksum(int isTestNet, int *mod) {
	*mod = PolyMod(isTestNet == 0 ? 5 : 9);
}
```

So, the compiler has problems seeing little powerful
argument optimizations.


---


### compiler : `gcc`
### title : `cstore does not work with a store in one of the branches`
### open_at : `2018-05-29T21:51:01Z`
### last_modified_date : `2023-05-06T18:58:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85987
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Take:
int PolyMod(int s);
void CreateChecksum(int isTestNet, int *mod, int *t) {
    if (isTestNet == 0) {
        *t += 1;
        *mod = (5);
    } else {
        *mod = (9);
    }
}

Cstore does not work here and commonize the store to mod.


---


### compiler : `gcc`
### title : `416.gamess slowed down by BB vectorization`
### open_at : `2018-05-30T16:43:30Z`
### last_modified_date : `2019-10-30T06:26:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85999
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Adding -ftree-slp-vectorize to -O2 -march=haswell [-mprefer-avx128] on 416.gamess
slows down the benchmark significantly.  Adding -fno-tree-slp-vectorize to
-Ofast -march=haswell speeds it up.

Thus somehow BB vectorization slows down 416.gamess.


---


### compiler : `gcc`
### title : `[8 Regression] redundant memset with smaller size not eliminated`
### open_at : `2018-05-31T02:41:11Z`
### last_modified_date : `2021-05-14T10:46:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86010
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Both GCC 6 and Clang 3.5 and later emit equally efficient code for both functions in the test case below.  Starting with GCC 7, the code emitted for h()  is suboptimal.

$ cat b.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout b.c
void f (void*);

void g (void)
{
  char a[8];
  __builtin_memset (a, 0, 8);
  __builtin_memset (a, 0, 8);

  f (a);
}

void h (void)
{
  char a[8];
  __builtin_memset (a, 0, 8);
  __builtin_memset (a, 0, 7);

  f (a);
}


;; Function g (g, funcdef_no=0, decl_uid=1958, cgraph_uid=0, symbol_order=0)

g ()
{
  char a[8];

  <bb 2> [local count: 1073741825]:
  __builtin_memset (&a, 0, 8);
  f (&a);
  a ={v} {CLOBBER};
  return;

}



;; Function h (h, funcdef_no=1, decl_uid=1962, cgraph_uid=1, symbol_order=1)

h ()
{
  char a[8];

  <bb 2> [local count: 1073741825]:
  __builtin_memset (&MEM[(void *)&a + 6B], 0, 2);
  __builtin_memset (&a, 0, 7);
  f (&a);
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `Inefficient code generated for ldivmod with constant value`
### open_at : `2018-05-31T04:43:49Z`
### last_modified_date : `2021-12-21T11:39:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86011
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
Tested with 7.2.0 and 8.1.0.

The following example unnecessarily calls ldivmod twice:

struct foo { long a, b; };
struct foo test(long long x)
{
	return (struct foo){x / 77, x % 77};
}

armv7m-linux-musleabi-gcc -c -O2 test.c
armv7m-linux-musleabi-objdump -d test.o

00000000 <test>:
   0:	b5d0      	push	{r4, r6, r7, lr}
   2:	4616      	mov	r6, r2
   4:	461f      	mov	r7, r3
   6:	4604      	mov	r4, r0
   8:	224d      	movs	r2, #77	; 0x4d
   a:	2300      	movs	r3, #0
   c:	4630      	mov	r0, r6
   e:	4639      	mov	r1, r7
  10:	f7ff fffe 	bl	0 <__aeabi_ldivmod>
  14:	4639      	mov	r1, r7
  16:	6020      	str	r0, [r4, #0]
  18:	224d      	movs	r2, #77	; 0x4d
  1a:	4630      	mov	r0, r6
  1c:	2300      	movs	r3, #0
  1e:	f7ff fffe 	bl	0 <__aeabi_ldivmod>
  22:	4620      	mov	r0, r4
  24:	6062      	str	r2, [r4, #4]
  26:	bdd0      	pop	{r4, r6, r7, pc}

If the test is rearranged so that the denominator is a function argument the generated code is as expected:

struct foo { long a, b; };
struct foo test(long long x, long den)
{
	return (struct foo){x / den, x % den};
}

armv7m-linux-musleabi-gcc -c -O2 test.c
armv7m-linux-musleabi-objdump -d test.o

00000000 <test>:
   0:	b5d0      	push	{r4, r6, r7, lr}
   2:	4616      	mov	r6, r2
   4:	461f      	mov	r7, r3
   6:	9a04      	ldr	r2, [sp, #16]
   8:	4604      	mov	r4, r0
   a:	4639      	mov	r1, r7
   c:	4630      	mov	r0, r6
   e:	17d3      	asrs	r3, r2, #31
  10:	f7ff fffe 	bl	0 <__aeabi_ldivmod>
  14:	e9c4 0200 	strd	r0, r2, [r4]
  18:	4620      	mov	r0, r4
  1a:	bdd0      	pop	{r4, r6, r7, pc}


---


### compiler : `gcc`
### title : `std::vector::shrink_to_fit() could sometimes use realloc()`
### open_at : `2018-05-31T09:48:05Z`
### last_modified_date : `2020-01-17T14:05:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86013
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `8.1.1`
### severity : `normal`
### contents :
std::vector::shrink_to_fit() when reducing the size it still calls new()+copy.
It could use realloc() when the objects are trivially copyable resulting in no copy during size reduction.

Maybe it could even always call realloc() for size reduction of any type of objects and just assert the returned pointer did not change.


---


### compiler : `gcc`
### title : `multiple consecutive calls to bzero/memset not merged`
### open_at : `2018-05-31T17:16:35Z`
### last_modified_date : `2021-08-21T23:57:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86017
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Even though the two functions defined in the test case below are equivalent, GCC emits considerably less efficient code the one with multiple calls to memset than for the one with just a single call.  Clang emits the same optimally efficient code for both.

$ cat b.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout -o/dev/stdout b.c
void f (void*);

void g (void)
{
  char a[8];
  __builtin_memset (a, 0, 8);

  f (a);
}

void h (void)
{
  char a[8];
  __builtin_memset (a, 0, 1);
  __builtin_memset (a + 1, 0, 1);
  __builtin_memset (a + 2, 0, 1);
  __builtin_memset (a + 3, 0, 1);
  __builtin_memset (a + 4, 0, 1);
  __builtin_memset (a + 5, 0, 1);
  __builtin_memset (a + 6, 0, 1);
  __builtin_memset (a + 7, 0, 1);

  f (a);
}

	.file	"b.c"
	.text

;; Function g (g, funcdef_no=0, decl_uid=1958, cgraph_uid=0, symbol_order=0)

g ()
{
  char a[8];

  <bb 2> [local count: 1073741825]:
  __builtin_memset (&a, 0, 8);
  f (&a);
  a ={v} {CLOBBER};
  return;

}


	.p2align 4,,15
	.globl	g
	.type	g, @function
g:
.LFB0:
	.cfi_startproc
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movq	$0, 8(%rsp)
	leaq	8(%rsp), %rdi
	call	f
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:
	.size	g, .-g

;; Function h (h, funcdef_no=1, decl_uid=1962, cgraph_uid=1, symbol_order=1)

h ()
{
  char a[8];

  <bb 2> [local count: 1073741825]:
  MEM[(void *)&a] = 0;
  __builtin_memset (&MEM[(void *)&a + 1B], 0, 1);
  __builtin_memset (&MEM[(void *)&a + 2B], 0, 1);
  __builtin_memset (&MEM[(void *)&a + 3B], 0, 1);
  __builtin_memset (&MEM[(void *)&a + 4B], 0, 1);
  __builtin_memset (&MEM[(void *)&a + 5B], 0, 1);
  __builtin_memset (&MEM[(void *)&a + 6B], 0, 1);
  __builtin_memset (&MEM[(void *)&a + 7B], 0, 1);
  f (&a);
  a ={v} {CLOBBER};
  return;

}


	.p2align 4,,15
	.globl	h
	.type	h, @function
h:
.LFB1:
	.cfi_startproc
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	leaq	8(%rsp), %rdi
	movb	$0, 8(%rsp)
	movb	$0, 9(%rsp)
	movb	$0, 10(%rsp)
	movb	$0, 11(%rsp)
	movb	$0, 12(%rsp)
	movb	$0, 13(%rsp)
	movb	$0, 14(%rsp)
	movb	$0, 15(%rsp)
	call	f
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1:
	.size	h, .-h
	.ident	"GCC: (GNU) 8.1.1 20180522"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Unref implementation using atomic_thread_fence generates worse code on x86-64 in gcc 8.1 than 7.3`
### open_at : `2018-05-31T18:25:31Z`
### last_modified_date : `2023-07-07T10:33:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86019
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.0`
### severity : `normal`
### contents :
See https://godbolt.org/g/Tu80RI (specifically the godegen for do_unref3())

Simplified version: https://godbolt.org/g/Xbn6n6

This is the unref half of a refcount implementation:

#include <atomic>

std::atomic<int> refcount;

bool do_unref() {
    int old_count = refcount.fetch_sub(1, std::memory_order_release);
    if (old_count == 1) {
        std::atomic_thread_fence(std::memory_order_acquire);
    }
    return old_count == 1;
}

In particular, unref needs release semantics on every decrement, but only needs acquire semantics on the last decrement.

std::atomic_thread_fence(std::memory_order_acquire) should be (approximately) a no-op on x86.

gcc 7.3 generated the right code here:

do_unref():
        lock sub        DWORD PTR refcount[rip], 1
        sete    al
        ret

gcc 8.1 generates a branch choosing between duplicate codepaths based on whether old_count == 1:

do_unref():
        mov     eax, -1
        lock xadd       DWORD PTR refcount[rip], eax
        cmp     eax, 1
        je      .L4
        cmp     eax, 1
        sete    al
        ret
.L4:
        cmp     eax, 1
        sete    al
        ret

It also appears to fail to optimize based on decrementing the constant value 1.


---


### compiler : `gcc`
### title : `[8 Regression] Performance regression in Eigen geometry.cpp test starting with r248334`
### open_at : `2018-05-31T19:58:36Z`
### last_modified_date : `2019-02-10T13:58:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86020
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `normal`
### contents :
GCC 8.1 has regressed by about 30% compared with GCC 7.3 on one of the Eigen test cases when measured on Power9 hardware (powerpc64le-linux-gnu).  Similar performance loss is seen on Power8 hardware as well.  Pat Haugen did some bisecting and found that this began with r248334.

2017-05-22  Jan Hubicka  <hubicka@ucw.cz>
	
	        * ipa-inline.c (edge_badness): Use inlined_time instead of
	        inline_summaries->get.

The performance difference seems to be due to a change in inlining as a result of the above patch.  The symptom is a marked increase in stack load/store activity.

An example occurs in one of the hottest routines:  _ZN5Eigen8internal20generic_product_implINS_5BlockIKNS_6MatrixIfLi4ELi4ELi0ELi4ELi4EEELi3ELi3ELb0EEENS3_IfLi3ELi8ELi0ELi3ELi8EEENS_10DenseShapeES8_Li3EE5addToINS2_IS7_Li3ELi8ELb1EEEEEvRT_RKS6_RKS7_

Below is the code with GCC 8.1.0 : 
0.15 :        1000b274:   lfs     f7,8(r9)
0.00 :        1000b278:   li      r5,2
0.00 :        1000b27c:   addi    r4,r1,464
0.06 :        1000b280:   lfs     f8,4(r9)
0.09 :        1000b284:   lfs     f9,0(r9)
0.00 :        1000b288:   lfs     f0,4(r31)
0.00 :        1000b28c:   addi    r3,r1,648
2.61 :        1000b290:   lfs     f10,32(r10)
0.71 :        1000b294:   lfs     f11,16(r10)
0.46 :        1000b298:   lfs     f12,0(r10)
1.99 :        1000b29c:   fmuls   f10,f10,f7
4.36 :        1000b2a0:   fmadds  f11,f11,f8,f10
4.20 :        1000b2a4:   fmadds  f12,f12,f9,f11
4.08 :        1000b2a8:   fadds   f0,f0,f12
7.67 :        1000b2ac:   stfs    f0,4(r31)
0.00 :        1000b2b0:   bl      10004838
0.00 :        1000b2b4:   nop
0.06 :        1000b2b8:   ld      r5,664(r1)
0.00 :        1000b2bc:   ld      r10,680(r1)
0.00 :        1000b2c0:   ld      r9,512(r1)
0.00 :        1000b2c4:   addi    r4,r1,32
0.00 :        1000b2c8:   ld      r6,688(r1)
0.06 :        1000b2cc:   ld      r7,696(r1)
0.00 :        1000b2d0:   ld      r8,712(r1)
0.03 :        1000b2d4:   ld      r0,648(r1)
0.00 :        1000b2d8:   addi    r3,r1,1104
0.00 :        1000b2dc:   ld      r11,704(r1)
0.09 :        1000b2e0:   std     r28,144(r1)
0.00 :        1000b2e4:   std     r30,152(r1)
0.00 :        1000b2e8:   std     r26,160(r1)
0.18 :        1000b2ec:   std     r5,48(r1)
0.09 :        1000b2f0:   std     r10,64(r1)
0.03 :        1000b2f4:   ld      r5,728(r1)
0.00 :        1000b2f8:   ld      r10,720(r1)
0.06 :        1000b2fc:   std     r9,136(r1)
0.00 :        1000b300:   add     r9,r9,r29
0.09 :        1000b304:   std     r0,32(r1)
0.46 :        1000b308:   std     r6,600(r1)
0.06 :        1000b30c:   std     r6,72(r1)
0.18 :        1000b310:   std     r7,608(r1)
   
0.09 :        1000b314:   std     r7,80(r1)
0.12 :        1000b318:   std     r11,88(r1)
0.40 :        1000b31c:   std     r9,120(r1)
0.09 :        1000b320:   std     r8,624(r1)
0.06 :        1000b324:   std     r8,96(r1)
0.03 :        1000b328:   std     r10,632(r1)
0.06 :        1000b32c:   std     r10,104(r1)
0.06 :        1000b330:   std     r5,112(r1)
0.15 :        1000b334:   bl      1000a248  <calls the map base evaluator function>
The additional instructions can be seen between addresses 1000b2b8 and 1000b330. 

Below is the code for GCC 7.3.0 : 
0.16 :        1000ac88:   lfs     f7,8(r9)
0.04 :        1000ac8c:   ld      r10,488(r1)
0.08 :        1000ac90:   addi    r4,r1,176
0.35 :        1000ac94:   lfs     f8,4(r9)
0.28 :        1000ac98:   lfs     f9,0(r9)
0.12 :        1000ac9c:   lfs     f0,0(r31)
0.00 :        1000aca0:   ld      r9,496(r1)
0.00 :        1000aca4:   addi    r3,r1,1160
3.39 :        1000aca8:   lfs     f10,32(r8)
0.87 :        1000acac:   lfs     f11,16(r8)
0.83 :        1000acb0:   lfs     f12,0(r8)
0.32 :        1000acb4:   std     r10,864(r1)
0.12 :        1000acb8:   std     r10,776(r1)
0.04 :        1000acbc:   std     r9,872(r1)
0.24 :        1000acc0:   std     r9,784(r1)
1.93 :        1000acc4:   fmuls   f10,f10,f7
4.38 :        1000acc8:   fmadds  f11,f11,f8,f10
5.48 :        1000accc:   fmadds  f12,f12,f9,f11
5.99 :        1000acd0:   fadds   f0,f0,f12
10.25 :        1000acd4:   stfs    f0,0(r31)
0.00 :        1000acd8:   std     r7,304(r1)
0.20 :        1000acdc:   std     r9,224(r1)
0.00 :        1000ace0:   std     r21,176(r1)
0.24 :        1000ace4:   std     r27,192(r1)
0.00 :        1000ace8:   std     r23,208(r1)
0.00 :        1000acec:   std     r24,232(r1)
0.04 :        1000acf0:   std     r26,240(r1)
0.00 :        1000acf4:   std     r30,248(r1)
0.12 :        1000acf8:   std     r26,256(r1)
0.79 :        1000acfc:   std     r28,264(r1)
0.16 :        1000ad00:   std     r25,280(r1)
0.08 :        1000ad04:   std     r30,288(r1)
0.28 :        1000ad08:   std     r29,296(r1)
0.00 :        1000ad0c:   std     r10,216(r1)
0.00 :        1000ad10:   bl      1000aae8  <calls the map base evaluator function>

The GCC 8.1 code has an additional call, no longer inlined, to the constructor Eigen::Matrix().  We suspect this and similar changes in other variants of this function are responsible.

It is hard to understand the patch that made this change out of context.  I went back to the gcc-patches mailing list to look for discussion, but I wasn't able to find a justification for this change.  Jan, could you explain its purpose?


---


### compiler : `gcc`
### title : `Fake triviality test for internal purposes`
### open_at : `2018-06-01T11:10:40Z`
### last_modified_date : `2019-03-14T14:12:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86023
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.0`
### severity : `enhancement`
### contents :
While we cannot make std::pair or std::tuple trivial for now for ABI reasons, it should still be safe to use memcpy-type optimizations for them when it is safe for each member. We probably don't want to lie to the user in is_trivial or is_trivially_copyable (?), but we could at least introduce an internal version of those traits, specialized for a few types like pair/tuple, and use them so _GLIBCXX_MOVE_BACKWARD3 would use memmove on std::pair<int,int> for instance.

According to some benchmark, this might (they weren't exactly testing this) change the average performance of insert/erase in boost::flat_map<,,std::vector<>> by a factor of 2. Of course, it won't affect the default flat_map, which uses boost's vector and traits, so it isn't a real solution, just a small band-aid.

The exact traits to specialize depend on PR 68350.

#include <vector>
#include <utility>
#ifdef FAST
struct A {
  int first,second;
  A(int a,int b):first(a),second(b){}
  A()=default;
};
#else
typedef std::pair<int,int> A;
#endif
typedef std::vector<A> V;

int main(int argc,char**){
  V v;
  for(int i=0;i<100000;++i){
    v.insert(v.begin(),{i,i});
  }
  return v[argc].second;
}

At -O3, I get 3.41s for std::pair, 1.00s for the struct, and an intermediate 1.99s for the struct minus the default constructor.


---


### compiler : `gcc`
### title : `Missed memcpy loop distribution with elementwise copy`
### open_at : `2018-06-01T11:27:01Z`
### last_modified_date : `2021-08-29T01:14:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86024
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
typedef struct A { int a, b; } A;
void*f(A*restrict p){
  A*q=__builtin_malloc(1024*sizeof(A));
  for(int i=0;i<1024;++i){
#ifdef HELP
    q[i]=p[i];
#else
    q[i].a=p[i].a;
    q[i].b=p[i].b;
#endif
  }
  return q;
}

At -O3, with HELP, we get the expected memcpy. Without it, the loop is only vectorized.


---


### compiler : `gcc`
### title : `gcc -O3 make very slow product`
### open_at : `2018-06-01T16:30:21Z`
### last_modified_date : `2021-05-30T04:42:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86029
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
I tested a signal processing sample code, and the gcc -O3 give me very slow binary. It's a compiler error or it is good?

My CPU is i5-3337u, and I use Ubuntu-18.04 64bit OS.
gcc 7.3 and clang 6.0 and I compared with speed of Rust code (rustc 1.24.1)


My sample code and test script:

$ git clone https://github.com/hg2ecz/smallrx
$ cd smallrx/
$ ./gcc-clang-rustc-bench.sh


gcc   -Ofast -march=native rx.c -lm -o rx-gcc-Ofast
real 3.81
user 3.80
sys 0.01

clang -Ofast -march=native rx.c -lm -o rx-clang-Ofast
real 6.79
user 6.77
sys 0.02

gcc   -O3 -march=native rx.c -lm -o rx-gcc-O3
real 51.76   ----> very slow, clang make faster binary with [-O0, -O1, -O2, -O3]
user 51.69
sys 0.07

clang -O3 -march=native rx.c -lm -o rx-clang-O3
real 11.05
user 11.03
sys 0.01

./Rust/target/release/smallrx
real 8.26
user 8.22
sys 0.03


---


### compiler : `gcc`
### title : `Inline break tail-call optimization`
### open_at : `2018-06-04T20:06:12Z`
### last_modified_date : `2021-08-10T18:49:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86050
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `6.3.0`
### severity : `normal`
### contents :
I have deep tail-recursion and try check how many bytes it consumed. Algorithm for this very simple - pointer to variable in main function minus pointer to variable in current recursive call. If function for this check have noinline attribute, tail-call optimization doing well and my recursion consume very little amount of memory. But if I remove noinline attribute, optimization don't work and I get stack overflow.

OS - Debian Stretch.
g++ --version - g++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
Command line - g++ -O2 main.cpp -o main
Code:

#include <iostream>

using namespace std;

struct Parent
{
    virtual void tailCall(const char*,long )const{}
};

struct Child:Parent
{
    //remove noinline and you get stack overflow
    static __attribute__((noinline)) void stackConsume(const char*stack){
        char value;
        std::cout<<"Consumed "<<stack-&value<<" bytes"<<std::endl;
    }

    void tailCall(const char *stack, long deep) const
    {
        if(deep==1000000000000L)
            return;
        stackConsume(stack);
        next->tailCall(stack,++deep);
    }
    Parent*next=this;
};

int main()
{
    Parent*parent=new Child;
    char stack;
    parent->tailCall(&stack,0);
    return 0;
}


---


### compiler : `gcc`
### title : `dead memset followed by strncpy and assignment not eliminated`
### open_at : `2018-06-05T21:23:20Z`
### last_modified_date : `2019-07-23T20:08:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86061
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
This is similar to bug 86010 except the optimization is absent from all versions of GCC all the way back to 4.1.  In the test case below the memset call zeroes out the entire destination array, only to have its contents overwritten by the subsequent call to strncpy followed by the write to its last element.  Rather than eliminating the memset call and keeping the single-character store GCC does the opposite, which is obviously suboptimal.

This code is reduced from code that's idiomatic in the Linux kernel when copying between kernel data (that's not necessarily nul-terminated) and user-space strings (which are expected to be nul-terminated).

$ cat c.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout c.c

struct S { int i; char n[256]; int j; };

void f (char*);

void g (struct S *p)
{
  char a[sizeof p->n + 1];

  __builtin_memset (a, 0, sizeof a);   // dead store, can be eliminated

  __builtin_strncpy (a, p->n, sizeof a - 1);
  a[sizeof a - 1] = '\0';

  f (a);
}


;; Function g (g, funcdef_no=0, decl_uid=1964, cgraph_uid=0, symbol_order=0)

g (struct S * p)
{
  char a[257];
  char[256] * _1;

  <bb 2> [local count: 1073741825]:
  __builtin_memset (&a, 0, 257);
  _1 = &p_4(D)->n;
  __builtin_strncpy (&a, _1, 256);
  f (&a);
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `I/O built-ins considered argument clobbers`
### open_at : `2018-06-07T19:57:18Z`
### last_modified_date : `2021-09-05T03:58:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86085
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `enhancement`
### contents :
The buggy test case in pr67610 should be diagnosed by -Wrestrict but isn't.  Debugging shows that the strlen pass discards the length information for the array in g() in response to the alias oracle determining that the printf() call might clobber the array argument.  Except for printf with the %n directive, standard output functions don't modify their arguments.

$ cat z.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout z.c
void f (void)
{
  char s[] = "123";
  char d[sizeof s];

  __builtin_sprintf (d, "%s", s);   // eliminated (transformed to strcpy by sprintf pass)

  if (__builtin_strlen (s) != 3)    // folded
    __builtin_abort ();

  if (__builtin_strlen (d) != 3)    // also folded
    __builtin_abort ();
}

void g (void)
{
  char s[] = "123";

  __builtin_puts (s);

  if (__builtin_strlen (s) != 3)   // not folded
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1956, cgraph_uid=0, symbol_order=0)

f ()
{
  <bb 2> [local count: 1073741825]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1961, cgraph_uid=1, symbol_order=1)

g ()
{
  char s[4];
  long unsigned int _1;

  <bb 2> [local count: 1073741825]:
  s = "123";
  __builtin_puts (&s);
  _1 = __builtin_strlen (&s);
  if (_1 != 3)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  s ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `powerpc: Suboptimal logical operation`
### open_at : `2018-06-11T09:23:37Z`
### last_modified_date : `2022-03-08T16:20:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86106
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.0`
### severity : `normal`
### contents :
unsigned int g(unsigned int val)
{
	unsigned int mask = 0x7f7f7f7f;

	return ~(((val & mask) + mask) | val | mask);
}

generates the following:

00000020 <g>:
  20:	3d 20 7f 7f 	lis     r9,32639
  24:	61 29 7f 7f 	ori     r9,r9,32639
  28:	7c 69 48 38 	and     r9,r3,r9
  2c:	3d 29 7f 7f 	addis   r9,r9,32639
  30:	39 29 7f 7f 	addi    r9,r9,32639
  34:	7d 23 1b 78 	or      r3,r9,r3
  38:	64 63 7f 7f 	oris    r3,r3,32639
  3c:	60 63 7f 7f 	ori     r3,r3,32639
  40:	7c 63 18 f8 	not     r3,r3
  44:	4e 80 00 20 	blr

Whereas I'd expect something like:

	lis     r4,32639
	ori     r4,r4,32639
	and	r9,r3,r4
	or	r3,r3,r4
	add	r9,r9,r4
	nor	r3,r9,r3
	blr


---


### compiler : `gcc`
### title : `powerpc: gcc uses costly multiply instead of shift left`
### open_at : `2018-06-13T10:54:54Z`
### last_modified_date : `2022-03-08T16:20:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86131
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.0`
### severity : `normal`
### contents :
unsigned long f1(unsigned long a, unsigned long b)
{
	return a >> ((4 - a) * 8);
}

unsigned long f2(unsigned long a, unsigned long b)
{
	return a >> ((4 - a) << 3);
}

unsigned long g1(unsigned long a, unsigned long b)
{
	return a >> (32 - a * 8);
}

unsigned long g2(unsigned long a, unsigned long b)
{
	return a >> (32 - (a << 3));
}

when compiling with GCC 8.1 with -O2 -mcpu=860, the following result is obtained:

00000000 <f1>:
   0:	1d 23 ff f8 	mulli   r9,r3,-8
   4:	39 29 00 20 	addi    r9,r9,32
   8:	7c 63 4c 30 	srw     r3,r3,r9
   c:	4e 80 00 20 	blr

00000010 <f2>:
  10:	21 23 00 04 	subfic  r9,r3,4
  14:	55 29 18 38 	rlwinm  r9,r9,3,0,28
  18:	7c 63 4c 30 	srw     r3,r3,r9
  1c:	4e 80 00 20 	blr

00000020 <g1>:
  20:	1d 23 ff f8 	mulli   r9,r3,-8
  24:	39 29 00 20 	addi    r9,r9,32
  28:	7c 63 4c 30 	srw     r3,r3,r9
  2c:	4e 80 00 20 	blr

00000030 <g2>:
  30:	54 69 18 38 	rlwinm  r9,r3,3,0,28
  34:	21 29 00 20 	subfic  r9,r9,32
  38:	7c 63 4c 30 	srw     r3,r3,r9
  3c:	4e 80 00 20 	blr

mulli requires 2 cycles, therefore it shouldn't be used, should it ?

The same code compiled with -mcpu=e300c2 gives:

00000000 <f1>:
   0:	54 69 18 38 	rlwinm  r9,r3,3,0,28
   4:	21 29 00 20 	subfic  r9,r9,32
   8:	7c 63 4c 30 	srw     r3,r3,r9
   c:	4e 80 00 20 	blr

00000010 <f2>:
  10:	21 23 00 04 	subfic  r9,r3,4
  14:	55 29 18 38 	rlwinm  r9,r9,3,0,28
  18:	7c 63 4c 30 	srw     r3,r3,r9
  1c:	4e 80 00 20 	blr

00000020 <g1>:
  20:	54 69 18 38 	rlwinm  r9,r3,3,0,28
  24:	21 29 00 20 	subfic  r9,r9,32
  28:	7c 63 4c 30 	srw     r3,r3,r9
  2c:	4e 80 00 20 	blr

00000030 <g2>:
  30:	54 69 18 38 	rlwinm  r9,r3,3,0,28
  34:	21 29 00 20 	subfic  r9,r9,32
  38:	7c 63 4c 30 	srw     r3,r3,r9
  3c:	4e 80 00 20 	blr


---


### compiler : `gcc`
### title : `Modular multiplication optimization`
### open_at : `2018-06-13T14:58:05Z`
### last_modified_date : `2021-11-17T03:34:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86136
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Optimization: Ofast

GCC can't see that

(a * n) % k = (a * (n % k)) % k

(even) when n is known at compile time.

As a result,

int k (int a) {
    return (a * t) % 5;
}

always gives a different assembly for t = 1, 2, ...

you can also replace 5 with any number

Play with it here: https://godbolt.org/g/HkyHqg

Clang has this bug too.


---


### compiler : `gcc`
### title : `Inefficient homogeneous struct return`
### open_at : `2018-06-14T09:44:51Z`
### last_modified_date : `2021-10-02T19:09:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86145
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.1.0`
### severity : `enhancement`
### contents :
GCC currently does a suboptimal job of returning structs.
Consider the testcases:

typedef struct { double x, y; } D2;
typedef struct { double x, y, a; } D3;
typedef struct { double x, y, a, b; } D4;
typedef struct { float x, y; } F2;
typedef struct { float x, y, a; } F3;
typedef struct { float x, y, a, b; } F4;
 
D2 f1(double x, double y)
{
  D2 s = { x, y }; //this is actually optimal!
  return s;
}
 
D2 f1a(D2 *p)
{
  return p[2];
}
 
D3 f1b(D3 *p)
{
  return p[2];
}
 
D4 f1c(D4 *p)
{
  return p[1];
}
 
F2 f2(float x, float y)
{
  F2 s = { x, y };
  return s;
}
 
F2 f2a(F2 *p)
{
  return p[3];
}
 
F3 f2b(F3 *p)
{
  return p[4];
}
 
F4 f2c(F4 *p)
{
  return p[1];
}

For aarch64 we generate:

f1:
	ret

f1a:
	ldp	x1, x0, [x0, 32]
	fmov	d1, x0
	fmov	d0, x1
	ret

f1b:
	sub	sp, sp, #64
	ldr	x0, [x0, 64]
	str	x0, [sp, 56]
	ldp	d0, d1, [sp, 40]
	ldr	d2, [sp, 56]
	add	sp, sp, 64
	ret

f1c:
	sub	sp, sp, #64
	ldp	d0, d1, [sp, 32]
	ldp	d2, d3, [sp, 48]
	add	sp, sp, 64
	ret

f2:
	fmov	x1, d0
	mov	x0, 0
	bfi	x0, x1, 0, 32
	fmov	x1, d1
	bfi	x0, x1, 32, 32
	lsr	x1, x0, 32
	lsr	w0, w0, 0
	fmov	s1, w1
	fmov	s0, w0
	ret

f2a:
	ldr	x0, [x0, 24]
	lsr	x1, x0, 32
	lsr	w0, w0, 0
	fmov	s1, w1
	fmov	s0, w0
	ret

f2b:
	sub	sp, sp, #32
	ldr	x1, [x0, 48]
	ldr	w0, [x0, 56]
	str	x1, [sp, 16]
	str	w0, [sp, 24]
	ldp	s0, s1, [sp, 16]
	ldr	s2, [sp, 24]
	add	sp, sp, 32
	ret

f2c:
	ldp	x1, x0, [x0, 16]
	lsr	x3, x1, 32
	lsr	x2, x0, 32
	fmov	s1, w3
	fmov	s3, w2
	lsr	w1, w1, 0
	lsr	w0, w0, 0
	fmov	s0, w1
	fmov	s2, w0
	ret

This also appears on x86:

f1:
	ret

f1a:
	movq	32(%rdi), %rdx
	movq	40(%rdi), %rax
	movq	%rdx, -8(%rsp)
	movsd	-8(%rsp), %xmm0
	movq	%rax, -8(%rsp)
	movsd	-8(%rsp), %xmm1
	ret

f1b:
	movdqu	48(%rsi), %xmm0
	movq	64(%rsi), %rdx
	movq	%rdi, %rax
	movups	%xmm0, (%rdi)
	movq	%rdx, 16(%rdi)
	ret

f1c:
	movdqu	32(%rsi), %xmm0
	movdqu	48(%rsi), %xmm1
	movq	%rdi, %rax
	movups	%xmm0, (%rdi)
	movups	%xmm1, 16(%rdi)
	ret

f2:
	movd	%xmm1, %eax
	salq	$32, %rax
	movq	%rax, %rdx
	movd	%xmm0, %eax
	orq	%rdx, %rax
	movq	%rax, -8(%rsp)
	movq	-8(%rsp), %xmm0
	ret

f2a:
	movq	24(%rdi), %rax
	movq	%rax, -8(%rsp)
	movq	-8(%rsp), %xmm0
	ret

f2b:
	movq	48(%rdi), %rax
	movl	56(%rdi), %edx
	movq	%rax, -48(%rsp)
	movq	-48(%rsp), %xmm0
	movl	%edx, -12(%rsp)
	movss	-12(%rsp), %xmm1
	ret

f2c:
	movq	16(%rdi), %rdx
	movq	24(%rdi), %rax
	movq	%rdx, -8(%rsp)
	movq	-8(%rsp), %xmm0
	movq	%rax, -8(%rsp)
	movq	-8(%rsp), %xmm1
	ret


The compiler does the structure load as an opaque TImode (or wider) move and then tries to unpack with subregs later on. Can we get the expander to expand struct components more intelligently?


---


### compiler : `gcc`
### title : `Implement isinf on PowerPC`
### open_at : `2018-06-14T23:59:22Z`
### last_modified_date : `2022-11-08T08:25:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86160
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
PowerPC with the VSX instruction set has the xststdcdp instruction that can be used to test if a value is infinity (and the xststdcqp instruction for IEEE 128-bit).  We should use this for the IEEE math builtins like __builtin_isinf.


---


### compiler : `gcc`
### title : `Add insn support for __builtin_isnormal and __builtin_fpclassify`
### open_at : `2018-06-15T00:08:53Z`
### last_modified_date : `2022-03-08T16:20:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86161
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
I was tracking down a bug in PowerPC with isnormal, and I noticed we don't have optab support for doing either the isnormal or fpclassify builtins in-line.  Instead we generate a series of test (isnan, isinfinite, etc.) to calculate the answer.

The PowerPC has an instruction (xststdcdp) that can simplify this operation.  There may be other backends that could improve their code if we add the optab support for these operations.


---


### compiler : `gcc`
### title : `Default construction of a union (in std::optional)`
### open_at : `2018-06-16T09:58:57Z`
### last_modified_date : `2021-10-29T06:43:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86173
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
Default construction of std::optional<A> always starts with a memset of the whole optional to 0, while it doesn't with clang using the same libstdc++.

#include <optional>
struct AA {
  double a[1024];
#ifndef TRIVIAL
  AA(); AA(AA const&); AA& operator=(AA const&); ~AA();
#endif
};
typedef std::optional<AA> O;
// O fff(){ return {}; }
O fff(){ O o; return o; }

The .original dump has

*<retval> = {.D.34926={._M_payload={.D.34026={._M_empty={}}, ._M_engaged=0}}}

which looks good, it says it is initializing the small _M_empty part of the union, but the gimple dump has

*<retval> = {};

which eagerly zeroes everything.


---


### compiler : `gcc`
### title : `Peephole does not happen because the type of zero/sign extended operands is not the same.`
### open_at : `2018-06-19T09:28:39Z`
### last_modified_date : `2021-08-29T21:05:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86209
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
While implementing peephole2 for combining shorter types loads/stores into larger type load/store, following testcase was found for aarch64 for which peephole does not happen because the type of zero/sign extended operands is not the same.

Test program:
unsigned short
subus (unsigned short *array)
{
  return array[0] + array[1];
}

Expander generated RTL:
(insn 6 3 7 2 (set (reg:HI 96)
        (mem:HI (reg/v/f:DI 94 [ array ]) [1 *array_4(D)+0 S2 A16]))
     (nil))
(insn 7 6 8 2 (set (reg:HI 97)
        (mem:HI (plus:DI (reg/v/f:DI 94 [ array ])
                (const_int 2 [0x2])) [1 MEM[(short unsigned int *)array_4(D) + 2B]+0 S2 A16]))
     (nil))
(insn 8 7 9 2 (set (reg:SI 99)
        (subreg:SI (reg:HI 97) 0))
     (nil))
(insn 9 8 10 2 (set (reg:SI 98)
        (plus:SI (subreg:SI (reg:HI 96) 0)
            (reg:SI 99)))
     (expr_list:REG_EQUAL (plus:SI (subreg:SI (reg:HI 96) 0)
            (subreg:SI (reg:HI 97) 0))
        (nil)))

The combiner combines insn 7 and 8 to generate zero extension to SI mode.
 
(insn 8 7 9 2 (set (reg:SI 99 [ MEM[(short unsigned int *)array_4(D) + 2B] ])
        (zero_extend:SI (mem:HI (plus:DI (reg/v/f:DI 94 [ array ])
                    (const_int 2 [0x2])) [1 MEM[(short unsigned int *)array_4(D) + 2B]+0 S2 A16]))) {*zero_extendhisi2_aarch64}
     (expr_list:REG_DEAD (reg/v/f:DI 94 [ array ])
        (nil)))

 The reload pass removes SUBREGs, which holds information about desired type, because of which HImode regs are zero extended to DImode.

(insn 8 7 6 2 (set (reg:SI 1 x1 [orig:99 MEM[(short unsigned int *)array_4(D) + 2B] ] [99])
        (zero_extend:SI (mem:HI (plus:DI (reg/v/f:DI 0 x0 [orig:94 array ] [94])
                    (const_int 2 [0x2])) [1 MEM[(short unsigned int *)array_4(D) + 2B]+0 S2 A16]))) {*zero_extendhisi2_aarch64}
     (nil))
(insn 6 8 9 2 (set (reg:DI 0 x0)
        (zero_extend:DI (mem:HI (reg/v/f:DI 0 x0 [orig:94 array ] [94]) [1 *array_4(D)+0 S2 A16]))) {*zero_extendhidi2_aarch64}
     (nil))
(insn 9 6 14 2 (set (reg:SI 0 x0 [98])
        (plus:SI (reg:SI 0 x0 [orig:96 *array_4(D) ] [96])
            (reg:SI 1 x1 [orig:99 MEM[(short unsigned int *)array_4(D) + 2B] ] [99]))){*addsi3_aarch64}
     (nil))
(insn 14 9 15 2 (set (reg/i:HI 0 x0)
        (reg:HI 0 x0 [98])) {*movhi_aarch64}
     (nil))
(insn 15 14 17 2 (use (reg/i:HI 0 x0)) 
     (nil))
(note 17 15 18 NOTE_INSN_DELETED)
(note 18 17 0 NOTE_INSN_DELETED)

Now as both memory accesses have different extended types, they cannot be combined by peephole.

Because of this, even when sched_fusion has brought the loads/stores closer, they cannot be merged.


---


### compiler : `gcc`
### title : `[8 Regression] Strongly increased stack usage`
### open_at : `2018-06-19T16:48:38Z`
### last_modified_date : `2021-05-14T11:00:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86214
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `normal`
### contents :
Created attachment 44296
Test case

Hi,

We noticed that MySQL does not pass its test suite when compiled with GCC 8; it runs out of stack. (GCC 7 is fine.) A reduced test case is included (mostly by C-Reduce, but it needed some help by hand); most of it appears to be fluff that keeps the compiler from just optimizing away the entire thing, but the gist of it seems to be that it inlines the bg::bl() function several times without caring that it balloons the stack size, and then doesn't manage to shrink the stack again by overlapping variables. Putting the noinline attribute on bg::bl() seems to be a workaround for now.

For comparison:

> g++-7 -O2 -Wstack-usage=1 -Wno-return-type -Wno-unused-result -c stack.i
stack.i: In function ‘void c()’:
stack.i:34:6: warning: stack usage is 8240 bytes [-Wstack-usage=]
 void c() {
      ^

> g++-8 -O2 -Wstack-usage=1 -Wno-return-type -Wno-unused-result -c stack.i                 
stack.i: In function ‘void c()’:
stack.i:34:6: warning: stack usage is 32816 bytes [-Wstack-usage=]
 void c() {
      ^

The actual, unreduced file can be found at https://github.com/mysql/mysql-server/blob/8.0/storage/innobase/row/row0ins.cc#L926 (the line is positioned on a function whose adding noinline helps, although I don't think it corresponds directly to bg::bl; I think bg::bl might be ib::error, and the 8192-sized buffer comes from ib::logger::msg).


---


### compiler : `gcc`
### title : `missing -Warray-bounds on an access to an implicitly zeroed out array`
### open_at : `2018-06-19T22:11:06Z`
### last_modified_date : `2022-10-31T23:47:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86223
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
When both bounds of an array index are out of range, GCC diagnoses the use of the index to refer beyond the bounds of a local array whose elements have all been explicitly initialized, but it fails to diagnose the same out-of-bounds access when the array has not been fully initialized.

$ cat d.c && gcc -O2 -S -Wall d.c
void f (int);

void g (unsigned i)
{
  if (i < 5 || 123 < i)
    i = 5;

  int a[3] = { 0 };
  f (a[i]);           // missing -Warray-bounds
}

void h (unsigned i)
{
  if (i < 5 || 123 < i)
    i = 5;

  int a[3] = { 0, 0, 0 };
  f (a[i]);           // -Warray-bounds (good)
}

d.c: In function ‘h’:
d.c:18:3: warning: array subscript 5 is above array bounds of ‘int[3]’ [-Warray-bounds]
   f (a[i]);           // -Warray-bounds (good)
   ^~~~~~~~


---


### compiler : `gcc`
### title : `duplicate strlen-like snprintf calls not folded`
### open_at : `2018-06-20T14:29:01Z`
### last_modified_date : `2021-09-15T22:40:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86241
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The calls to snprintf in the function below compute the length of the string s without storing any output.  As mentioned in bug 86203 comment #4, the result of the function with the same arguments (and a null destination) is constant so the duplicate snprintf call could be replaced by the result of the first, analogously to how the strlen optimization pass folds the corresponding call to strlen().

$ cat c.c && gcc -S -O2 -Wall -Wextra -fdump-tree-optimized=/dev/stdout c.c
  int f (char *s)
  {
    int n = __builtin_strlen (s);
    int m = __builtin_strlen (s);   // folded into m = n;
    return m + n;
  }


  int g (char *s)
  {
    int n = __builtin_snprintf (0, 0, "%s", s);
    int m = __builtin_snprintf (0, 0, "%s", s);   // could be folded into m = n
    return m + n;
  }

;; Function f (f, funcdef_no=0, decl_uid=1956, cgraph_uid=0, symbol_order=0)

f (char * s)
{
  int n;
  long unsigned int _1;
  int _5;

  <bb 2> [local count: 1073741825]:
  _1 = __builtin_strlen (s_3(D));
  n_4 = (int) _1;
  _5 = n_4 * 2;
  return _5;

}



;; Function g (g, funcdef_no=1, decl_uid=1961, cgraph_uid=1, symbol_order=1)

g (char * s)
{
  int m;
  int n;
  int _7;

  <bb 2> [local count: 1073741825]:
  n_4 = __builtin_snprintf (0B, 0, "%s", s_2(D));
  m_6 = __builtin_snprintf (0B, 0, "%s", s_2(D));
  _7 = n_4 + m_6;
  return _7;

}


---


### compiler : `gcc`
### title : `detect conversions between bitmasks and vector masks`
### open_at : `2018-06-21T13:09:47Z`
### last_modified_date : `2021-12-20T02:27:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86267
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Testcase (cf. https://godbolt.org/g/gi6f7V):

#include <x86intrin.h>

auto f(__m256i a, __m256i b) {
    __m256i k = a < b;
    long long bitmask = _mm256_movemask_pd((__m256d)k) & 0xf;
    return _mm256_cmpgt_epi64(
        __m256i{bitmask, bitmask, bitmask, bitmask} & __m256i{1, 2, 4, 8},
        __m256i()
    );
}

This should be optimized to "return a < b;".

A more complex case also allows conversion of the vector mask (cf. https://godbolt.org/g/FLAEgC):

#include <x86intrin.h>

auto f(__m256i a, __m256i b) {
    using V [[gnu::vector_size(16)]] = int;
    __m256i k = a < b;
    int bitmask = _mm256_movemask_pd((__m256d)k) & 0xf;
    return (V{bitmask, bitmask, bitmask, bitmask} & V{1, 2, 4, 8}) != 0;
}

I believe the most portable and readable strategy would be to introduce new builtins that convert between bitmasks and vector masks. (This can be especially helpful with AVX512, where the builtin comparison operators return vector masks, but Intel intrinsics require bitmasks.)

E.g.:
using W [[gnu::vector_size(32)]] = long long;
using V [[gnu::vector_size(16)]] = int;
V f(W a, W b) {
    unsigned bitmask = __builtin_vector_to_bitmask(a < b);
    return __builtin_bitmask_to_vector(bitmask, V);
}

I'd define __builtin_vector_to_bitmask to only consider the MSB of each element. And, to make optimization simpler, consider all remaining input bits to be whatever the canonical mask representation on the target system is.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Simple loop needs an extra register and an extra instruction`
### open_at : `2018-06-21T16:18:30Z`
### last_modified_date : `2023-08-04T08:35:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86270
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Compiling the following simple example with GCC 8 on an x86_64 with
just -O2 -S:

----------------------------------------
int *a;
long len;

int
test ()
{
  for (int i = 0; i < len + 1; i++)
    a[i]=i;
}
----------------------------------------

Results in a loop comparing the value before increment with the upper
bound after actually doing the incrementation, which means the loop
needs an extra register, an extra instruction, and has a rather
convoluted structure (alignment directives and some labels omitted):

----------------------------------------
test:
	.cfi_startproc
	movq	len(%rip), %rcx
	testq	%rcx, %rcx
	js	.L2
	movq	a(%rip), %rsi
	xorl	%eax, %eax
	jmp	.L3
.L4:
	movq	%rdx, %rax
.L3:
	movl	%eax, (%rsi,%rax,4)
	leaq	1(%rax), %rdx
	cmpq	%rax, %rcx
	jne	.L4
.L2:
	ret
----------------------------------------

as opposed to GCC 7 or when compiling with -fno-tree-fwprop:

----------------------------------------
test:
	.cfi_startproc
	movq	len(%rip), %rdx
	testq	%rdx, %rdx
	js	.L2
	movq	a(%rip), %rcx
	addq	$1, %rdx
	xorl	%eax, %eax
.L3:
	movl	%eax, (%rcx,%rax,4)
	addq	$1, %rax
	cmpq	%rdx, %rax
	jne	.L3
.L2:
	ret
----------------------------------------

This problem (specifically the need for an extra register) causes
that, on an AMD Ryzen machine, 465.tonto is almost 5% faster when
compiled with -fno-tree-fwprop.


---


### compiler : `gcc`
### title : `Poor codegen when returning a std::vector`
### open_at : `2018-06-22T01:43:44Z`
### last_modified_date : `2021-07-22T21:22:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86276
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
https://godbolt.org/g/aCiuAy

While I'm a bit sad that good() isn't just "ret", I think that the current rules for allocation elision require something like that codegen.

I'd expect bad() to codegen to roughly the same as good(), but then rather than calling operator delete, it should store [rax, rax + 1, rax + 22] to the three qwords starting at the out pointer. Instead, it does things like checking if the vector pointer it just zeroed is still zero to see if it needs to be deleted. It also seems to register an exception landing pad (I'm guessing to cover the operator new call) to handle freeing the vector's memory, even though it should know it isn't holding any. 

If I had to guess, I'd say the problem is that it thinks the hidden return out pointer has escaped when it hasn't really until a successful return. I'm pretty sure nothing in the language allows any way to access the return value before a function returns like that, but I'm now really curious if I'm wrong. If there is, is there any way to tell gcc that I promise I'm not doing anything quite that stupid?

PS- is it helpful to include the code and asm here in addition to the godbolt links?

#include <vector>
#include <cstdint>

auto good() {
std::vector<uint8_t> something;
something.reserve(22);
something = {0x02};
//return something; Only difference from bad
}


auto bad() {
std::vector<uint8_t> something;
something.reserve(22);
something = {0x02};
return something;
}

good():
        sub     rsp, 8
        mov     edi, 22
        call    operator new(unsigned long)
        mov     BYTE PTR [rax], 2
        mov     rdi, rax
        add     rsp, 8
        jmp     operator delete(void*)
bad():
        push    rbp
        pxor    xmm0, xmm0
        push    rbx
        mov     rbx, rdi
        sub     rsp, 24
        mov     QWORD PTR [rdi+16], 0
        movups  XMMWORD PTR [rdi], xmm0
        mov     edi, 22
        call    operator new(unsigned long)
        mov     rdi, QWORD PTR [rbx]
        test    rdi, rdi
        je      .L5
        mov     QWORD PTR [rsp+8], rax
        call    operator delete(void*)
        mov     rax, QWORD PTR [rsp+8]
.L5:
        mov     BYTE PTR [rax], 2
        lea     rdx, [rax+22]
        mov     QWORD PTR [rbx], rax
        add     rax, 1
        mov     QWORD PTR [rbx+8], rax
        mov     rax, rbx
        mov     QWORD PTR [rbx+16], rdx
        add     rsp, 24
        pop     rbx
        pop     rbp
        ret
        mov     rbp, rax
        jmp     .L6
bad() [clone .cold.25]:
.L6:
        mov     rdi, QWORD PTR [rbx]
        test    rdi, rdi
        je      .L7
        call    operator delete(void*)
.L7:
        mov     rdi, rbp
        call    _Unwind_Resume


---


### compiler : `gcc`
### title : `const local aggregates can be assumed not to be modified even when escaped`
### open_at : `2018-06-25T23:58:10Z`
### last_modified_date : `2023-01-09T10:06:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86318
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Because the local object in all three function is defined const it can be assumed not to change as a result of the call to f()  (otherwise the behavior of a program that changed it would be undefined).  GCC takes advantage of this constraint when the object is of a basic/fundamental type like in g0() but not when the object is an aggregate like in g1() or g2(), unless the object has a static storage duration.

$ cat c.c && gcc -S -O2 -Wall -Wextra -Wpedantic -Wstrict-prototypes -fdump-tree-optimized=/dev/stdout c.c
struct S { int i; };

void f (const void*);

void g0 (void)
{
  const int i = 1;

  f (&i);

  if (i != 1)             // folded into false
    __builtin_abort ();
}

void g1 (void)
{
  const int a[1] = { 1 };

  f (a);

  if (*a != 1)             // not folded but could be
    __builtin_abort ();
}

void g2 (void)
{
  const struct { int i; } s = { 1 };

  f (&s.i);

  if (s.i != 1)             // not folded but could be
    __builtin_abort ();
}

;; Function g0 (g0, funcdef_no=0, decl_uid=1902, cgraph_uid=1, symbol_order=0)

g0 ()
{
  const int i;

  <bb 2> [local count: 1073741825]:
  i = 1;
  f (&i);
  i ={v} {CLOBBER};
  return;

}



;; Function g1 (g1, funcdef_no=1, decl_uid=1906, cgraph_uid=2, symbol_order=1)

g1 ()
{
  const int a[1];
  int _1;

  <bb 2> [local count: 1073741825]:
  a[0] = 1;
  f (&a);
  _1 = a[0];
  if (_1 != 1)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  a ={v} {CLOBBER};
  return;

}



;; Function g2 (g2, funcdef_no=2, decl_uid=1910, cgraph_uid=3, symbol_order=2)

g2 ()
{
  const struct 
  {
    int i;
  } s;
  int _1;

  <bb 2> [local count: 1073741825]:
  s.i = 1;
  f (&s.i);
  _1 = s.i;
  if (_1 != 1)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  s ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `DOM does not handle RHS COND_EXPRs well`
### open_at : `2018-06-28T02:47:20Z`
### last_modified_date : `2022-06-13T12:27:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86339
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
As outlined in this discussion of one of Kugan's patches, DOM does not handle statements like

x = COND ? val1 : val2;

When COND has a known result.

The discussion is part of this thread:

https://gcc.gnu.org/ml/gcc-patches/2018-06/msg01383.html


---


### compiler : `gcc`
### title : `setc/movzx introduced into loop to provide a constant 0 value for a later rep stos`
### open_at : `2018-06-28T20:53:30Z`
### last_modified_date : `2021-08-29T13:08:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86352
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
The wrong-code bug 86314 also revealed some very weird code-gen decisions, which the fix didn't improve.

(I think the lock bts peephole is seen pretty late, and that's one necessary factor for this problem.  But even without it, an unnecessary data dependency between the lock bts loop and clearing memory is silly.)

This ended up being about 5 separate bugs, but IDK which belong together or are already reported:

* useless mov %rsi, %rcx and useless mov %rdx, %rdi
* using setc/movzx instead of xor %eax,%eax to get a constant 0; slower and creating a data dependency
* Doing that inside the loop instead of after
* Not adjusting register allocation to allow xor / set-flags / setc
* rep stos vs. vector stores as a zeroing strategy vs. any other repeated value.



The reproducer test-case for bug 86314  loops until it finds and claims a zero bit in a uint64_t, then returns a Bucket() object (with a constructor that zero-initializes it) with no data dependency on anything.

But gcc decides to introduce a flag -> integer 0/1 inside the acquire() loop instead of just using  xor eax,eax  before rep stosq.  The loop can only exit when CF = 0, so RAX = 0, so it's not a correctness problem.

The loop is branching on CF as set by BTS, so there's no need to have the 0/1 in a register at all inside the loop, and setc/movzx from a known-zero CF is more expensive that xor-zeroing.  (Plus it gives the STOSQ a data dependency on the LOCK BTS flag result which it wouldn't have otherwise.  The stores can't commit until after the lock memory barrier, but they can execute.)

This is the actual code-gen from (GCC-Explorer-Build) 9.0.0 20180627 https://godbolt.org/g/XGF5tR


BucketMap::acquireBucket():
        movq    %rdi, %rdx
        movq    %rsi, %rcx      # useless, lock bts can use (%rsi)
.L2:
        movq    (%rsi), %rax
        andl    $1, %eax        # source is simplified to only check positions 0 or 1
        lock btsq       %rax, (%rcx)  # Why not (%rsi)?
        setc    %al
        movzbl  %al, %eax       # xor / bts / setc would have been possible with a different reg
        jc      .L2
        # rax = 0 because the loop can only exit when CF=0

        # should use  xor %eax,%eax  here instead

        movq    %rdx, %rdi      # Useless, RDI still == RDX
        movl    $16, %ecx
        rep stosq
        movq    %rdx, %rax      # can't be done before rep stosq: RAX needs to be 0
        ret     



With -m32, where 64-bit lock bts isn't available, we have lock cmpxchg8b ending with an OR.  So there is a zero in an integer register from that, but it's not in EAX, so the code gen includes an extra `mov %esi, %eax`, which is not cheaper than xor %eax,%eax especially with -march=haswell.  Sandybridge-family has xor-zeroing as cheap as a NOP, but mov-elimination isn't always perfect and SnB itself doesn't have it.

And of course mov still has a data dependency on the source of the zero, so it defeats the effect of branch prediction + speculative breaking (control) dependencies.  This last applies on any out-of-order x86.

I guess the lock bts peephole is seen too late to notice that it can't recycle the 0 from the loop condition anymore, and ends up generating code to materialize it.  But why inside the loop?

------


Even if we *did* need an integer 0/1 in a register inside the loop, we could still use the xor / set-flags / setcc optimization: Simply use a register other than RAX for the load / AND $1 / bts source.  And you can hoist the xor-zeroing out of the loop.


        xor     %eax, %eax
.L2:
        movq    (%rsi), %rcx
        andl    $1, %ecx
        lock btsq       %rax, (%rsi)
        setc    %al
        # use %rax
        jc      .L2


---

Separately:

If the initializer is non-zero, it uses SSE or AVX stores.  That makes no sense either: if rep stosq is optimal, use  mov eax, 1 for the all-ones case.  (See the ifdef in the Godbolt link to try it)

If it's not optimal, use xorps xmm0,xmm0 to create an all-zero vector.

I guess gcc is checking for all-zeros as a common special case, but doesn't check for repeats of any other value, except for repeated bytes recognized as memset.

So it makes sense that gcc uses a different strategy, but I think for only 16x 8 bytes (128 bytes) that vector stores beat rep stos on current CPUs.  (That may change when IceLake introduces fast short-rep stos/movs.)

GCC does notice that it can reuse the same vector repeatedly, though, but not that it could have loaded it with vpbroadcastq from an 8-byte constant instead of a 32-byte constant (with -march=haswell for AVX2).  Broadcast-loads are no more expensive than regular vector loads, but use less space for constants.


---


### compiler : `gcc`
### title : `Address comparison not a constant expression`
### open_at : `2018-06-28T23:26:39Z`
### last_modified_date : `2022-01-06T16:17:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86354
### status : `RESOLVED`
### tags : `missed-optimization, rejects-valid`
### component : `c++`
### version : `8.1.0`
### severity : `normal`
### contents :
The following code:

```
struct X {};

struct Y: X {};
struct Z: X {};

extern Y y;
extern Z z;

constexpr X const& x1() { return y; }
constexpr X const& x2() { return z; }

static_assert( &x1() != &x2() );
```

yields (with g++ 7.3 and 8.1)

```
testbed2017.cpp:12:1: error: non-constant condition for static assertion
 static_assert( &x1() != &x2() );
 ^~~~~~~~~~~~~
testbed2017.cpp:12:22: error: '(((const X*)(& y)) != ((const X*)(& z)))' is not a constant expression
 static_assert( &x1() != &x2() );
                ~~~~~~^~~~~~~~
```

but other compilers accept it.


---


### compiler : `gcc`
### title : `Omnetpp is slower on PowerPC using -ffast-math than not using -ffast-math`
### open_at : `2018-07-06T16:53:37Z`
### last_modified_date : `2018-11-15T19:08:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86423
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
I did some Spec 2006 runs, and one of the runs I was comparing normal speed with -ffast-math.  As expected a lot of SpecFP benchmarks were faster using the -ffast-math option.  However, 3 of the benchmarks were slower, including omnetpp.

In particular, cMessageHeap::shiftup(int) [clone .part.16], which is the hotest function took 2,442,614 cycles when compiled with -ffast-math, and 2,378,917 cycles when compiled without -ffast-math.

Looking at the profile within the function, we see:

The C++ name is cMessageHeap::shiftup(int) [clone .part.16].

Percent   Percent   |  Samples   Samples   |  Line #  Filename   
slowmath  fastmath  |  slowmath  fastmath  |  Line #  Filename   
--------  --------  |  --------  --------  |  ------  -----------
39.3521%  40.3151%  |   936,158   984,740  |     198  cmsgheap.cc
17.4123%  10.5189%  |   414,223   256,938  |     200  cmsgheap.cc
 4.6070%  16.3806%  |   109,595   400,114  |      43  cmsgheap.cc
14.2110%  14.2290%  |   338,071   347,561  |     199  cmsgheap.cc
 9.0102%   7.1333%  |   214,345   174,237  |      45  cmsgheap.cc
 7.4063%   1.8450%  |   176,191    45,066  |      44  cmsgheap.cc
 3.4912%   4.9701%  |    83,054   121,401  |     196  cmsgheap.cc
 2.3383%   2.3092%  |    55,627    56,405  |     204  cmsgheap.cc
 2.1165%   2.2862%  |    50,347    55,841  |      46  cmsgheap.cc


---


### compiler : `gcc`
### title : `Milc is slower on PowerPC using -ffast-math than without using -ffast-math`
### open_at : `2018-07-06T17:00:54Z`
### last_modified_date : `2018-11-15T19:09:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86424
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
I did some Spec 2006 runs, and one of the runs I was comparing normal speed with -ffast-math.  As expected a lot of SpecFP benchmarks were faster using the -ffast-math option.  However, 3 of the benchmarks were slower, including milc.

The two hotest functions seem to be slower using -ffast-math:

slowmath  fastmath  |   slowmath   fastmath  |  Function                  
========  ========  |   ========   ========  |  ========                  
17.5310%  17.4259%  |  5,489,872  5,580,840  |  mult_su3_na               
13.4533%  15.6660%  |  4,212,953  5,017,232  |  mult_adj_su3_mat_vec      

Function mult_su3_na, 17.43 - 17.53% of total in 2 files
--------------------------------------------------------

Percent   Percent   |   Samples    Samples   |  Line #  Filename  
slowmath  fastmath  |   slowmath   fastmath  |  Line #  Filename  
--------  --------  |  ---------  ---------  |  ------  ----------
 9.8993%  75.9216%  |    543,453  4,237,070  |      38  m_mat_na.c
65.7913%   2.8468%  |  3,611,855    158,877  |      33  m_mat_na.c
           6.9169%  |               386,020  |      40  m_mat_na.c
 1.1277%   6.8534%  |     61,915    382,477  |      48  m_mat_na.c
 5.6730%            |    311,441             |      49  m_mat_na.c
 3.8513%   2.1642%  |    211,433    120,784  |      46  m_mat_na.c
 3.5253%            |    193,542             |      45  m_mat_na.c
 2.8633%            |    157,188             |      41  m_mat_na.c
 2.1619%            |    118,689             |      39  m_mat_na.c
 1.4727%            |     80,847             |      43  m_mat_na.c
 1.1929%            |     65,486             |      36  m_mat_na.c
           1.1072%  |                61,790  |      44  m_mat_na.c

Function mult_adj_su3_mat_vec, 13.45 - 15.67% of total in 2 files
-----------------------------------------------------------------

Percent   Percent   |   Samples    Samples   |  Line #  Filename   
slowmath  fastmath  |   slowmath   fastmath  |  Line #  Filename   
--------  --------  |  ---------  ---------  |  ------  -----------
71.3008%  77.9373%  |  3,003,867  3,910,288  |     116  m_amatvec.c
12.1258%  13.8781%  |    510,853    696,296  |     126  m_amatvec.c
11.0201%            |    464,272             |     121  m_amatvec.c
           3.4868%  |               174,944  |     123  m_amatvec.c
 2.1345%   1.4971%  |     89,925     75,109  |     129  m_amatvec.c
 1.3833%            |     58,278             |     132  m_amatvec.c


---


### compiler : `gcc`
### title : `Spec 2006 soplex seems to be slower on PowerPC using -ffast-math than without -ffast-math`
### open_at : `2018-07-06T17:06:08Z`
### last_modified_date : `2022-03-08T16:20:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86425
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
I did some Spec 2006 runs, and one of the runs I was comparing normal speed with -ffast-math.  As expected a lot of SpecFP benchmarks were faster using the -ffast-math option.  However, 3 of the benchmarks were slower, including soplex.

The 4 hottest functions in soplex are all slower using -ffast-math:

Percent   Percent   |   Samples    Samples   |                                                                                      
slowmath  fastmath  |   slowmath   fastmath  |  Function                                                                            
========  ========  |   ========   ========  |  ========                                                                            
32.3283%  33.0005%  |  4,005,273  4,344,449  |  _ZN6soplex10SPxSteepPR8entered4ENS_5SPxIdEi                                         
10.8967%  11.2537%  |  1,350,038  1,481,531  |  _ZN6soplex8SSVector20assign2product4setupERKNS_5SVSetERKS0_                         
10.4306%  10.9159%  |  1,292,285  1,437,055  |  _ZN6soplex8SSVector5setupEv                                                         
 7.0912%   7.5644%  |    878,557    995,832  |  _ZN6soplex9CLUFactor16vSolveUrightNoNZEPdS1_Piid

Function _ZN6soplex10SPxSteepPR8entered4ENS_5SPxIdEi, 32.33 - 33.00% of total in 2 files
----------------------------------------------------------------------------------------

The C++ name is soplex::SPxSteepPR::entered4(soplex::SPxId, int).

Percent   Percent   |   Samples    Samples   |  Line #  Filename     
slowmath  fastmath  |   slowmath   fastmath  |  Line #  Filename     
--------  --------  |  ---------  ---------  |  ------  -------------
50.4925%  51.6016%  |  2,022,362  2,241,806  |     175  svector.h    
15.1681%  15.6850%  |    607,528    681,424  |     161  vector.h     
13.8269%  14.9673%  |    553,806    650,241  |     295  svector.h    
10.0395%   8.8185%  |    402,106    383,111  |     293  svector.h    
 5.0159%   1.5512%  |    200,903     67,390  |     403  spxsteeppr.cc
           2.3271%  |               101,098  |     409  spxsteeppr.cc
 2.2996%   1.9382%  |     92,107     84,204  |     346  dataset.h    
 1.2001%   1.2045%  |     48,071     52,327  |     402  spxsteeppr.cc

Function _ZN6soplex8SSVector20assign2product4setupERKNS_5SVSetERKS0_, 10.90 - 11.25% of total in 2 files
--------------------------------------------------------------------------------------------------------

The C++ name is soplex::SSVector::assign2product4setup(soplex::SVSet const&, soplex::SSVector const&).

Percent   Percent   |  Samples   Samples   |  Line #  Filename   
slowmath  fastmath  |  slowmath  fastmath  |  Line #  Filename   
--------  --------  |  --------  --------  |  ------  -----------
60.5187%  59.4371%  |   817,025   880,583  |     984  ssvector.cc
29.4260%  29.5739%  |   397,262   438,147  |     981  ssvector.cc
 4.9556%   5.2965%  |    66,903    78,469  |     983  ssvector.cc
 3.9925%   4.4770%  |    53,900    66,328  |     980  ssvector.cc


Function _ZN6soplex8SSVector5setupEv, 10.43 - 10.92% of total in 2 files
------------------------------------------------------------------------

The C++ name is soplex::SSVector::setup().

Percent   Percent   |  Samples   Samples   |  Line #  Filename    
slowmath  fastmath  |  slowmath  fastmath  |  Line #  Filename    
--------  --------  |  --------  --------  |  ------  ------------
32.1165%  35.4180%  |   415,037   508,976  |     220  ssvector.cc 
22.2823%  25.1495%  |   287,951   361,412  |     212  ssvector.cc 
21.6266%  17.6770%  |   279,478   254,029  |     216  ssvector.cc 
20.2979%  19.6491%  |   262,308   282,368  |     214  ssvector.cc 
 3.6149%   2.1003%  |    46,715    30,182  |     193  spxdefines.h

Function _ZN6soplex9CLUFactor16vSolveUrightNoNZEPdS1_Piid, 7.09 - 7.56% of total in 2 files
-------------------------------------------------------------------------------------------

The C++ name is soplex::CLUFactor::vSolveUrightNoNZ(double*, double*, int*, int, double).

Percent   Percent   |  Samples   Samples   |  Line #  Filename 
slowmath  fastmath  |  slowmath  fastmath  |  Line #  Filename 
--------  --------  |  --------  --------  |  ------  ---------
14.3154%  11.5548%  |   125,771   115,066  |     479  vsolve.cc
 8.4523%  10.0335%  |    74,259    99,917  |      72  vsolve.cc
 9.5626%   7.6039%  |    84,013    75,722  |     470  vsolve.cc
 8.0533%   9.5256%  |    70,753    94,859  |      73  vsolve.cc
 8.0590%   6.2612%  |    70,802    62,351  |     469  vsolve.cc
 5.9002%   7.1628%  |    51,836    71,330  |     490  vsolve.cc
 6.3017%   4.9831%  |    55,364    49,624  |     474  vsolve.cc
 4.5466%   2.8041%  |    39,944    27,924  |     502  vsolve.cc
 3.7633%   4.3653%  |    33,063    43,471  |      76  vsolve.cc
 3.9925%   3.2432%  |    35,076    32,297  |     477  vsolve.cc
 3.0480%   3.6673%  |    26,779    36,520  |      89  vsolve.cc
 2.9868%   3.4680%  |    26,241    34,535  |     491  vsolve.cc
           2.8291%  |              28,173  |     499  vsolve.cc
 2.3567%   2.8041%  |    20,705    27,924  |      91  vsolve.cc
 2.6888%   2.1113%  |    23,623    21,025  |     466  vsolve.cc
 2.2936%   2.6018%  |    20,151    25,910  |      78  vsolve.cc
 1.7462%   1.4687%  |    15,342    14,626  |     478  vsolve.cc
 1.4010%   1.6444%  |    12,309    16,375  |     102  vsolve.cc
 1.3051%   1.6403%  |    11,466    16,335  |     464  vsolve.cc
 1.1126%   1.2585%  |     9,775    12,533  |     498  vsolve.cc
           1.0127%  |              10,085  |     108  vsolve.cc


---


### compiler : `gcc`
### title : `strlen not folded after strcpy into a zeroed-out local array`
### open_at : `2018-07-06T20:20:16Z`
### last_modified_date : `2019-10-15T21:53:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86427
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC eliminates a call to strlen() on a local copy into an uninitialized array and from a character string of known length, but doesn't do the same when local array is first zeroed-out.

$ cat c.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout c.c
const char a[] = "123";

int f (void)
{
  char d[8];

  __builtin_strcpy (d, a);       // eliminated
  return __builtin_strlen (d);   // folded
}

int g (void)
{
  char d[8] = "";

  __builtin_strcpy (d, a);       // not eliminated
  return __builtin_strlen (d);   // not folded
}


;; Function f (f, funcdef_no=0, decl_uid=1899, cgraph_uid=1, symbol_order=1)

f ()
{
  <bb 2> [local count: 1073741825]:
  return 3;

}



;; Function g (g, funcdef_no=1, decl_uid=1903, cgraph_uid=2, symbol_order=2)

g ()
{
  char d[8];
  long unsigned int _1;
  int _5;

  <bb 2> [local count: 1073741825]:
  d = "";
  MEM[(char * {ref-all})&d] = MEM[(char * {ref-all})&a];
  _1 = __builtin_strlen (&d);
  _5 = (int) _1;
  d ={v} {CLOBBER};
  return _5;

}


---


### compiler : `gcc`
### title : `early string folding defeats strlen optimization and -Warray-bounds`
### open_at : `2018-07-07T20:53:58Z`
### last_modified_date : `2022-10-31T23:48:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86434
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
When the call to strlen() below is folded to (3 - i) it is done very early, and without setting the range of the result of the subtraction.  This early folding prevents later optimizations from determining that the result of the strlen() call (or that of the subtraction) cannot be greater than 3.  It's an example that illustrates why early folding should be restricted to constants and leave more advanced optimizations to later passes that have access to the results of data flow analyses.

$ cat d.c && gcc -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout d.cconst char a[] = "123";
const char a[] = "123";

void f (int i)
{
  if (__builtin_strlen (&a[i]) > 3)
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1957, cgraph_uid=0, symbol_order=1)

f (int i)
{
  ssizetype _1;
  ssizetype _2;
  long unsigned int _3;

  <bb 2> :
  _1 = (ssizetype) i_4(D);
  _2 = 3 - _1;
  _3 = (long unsigned int) _2;
  if (_3 > 3)
    goto <bb 3>; [INV]
  else
    goto <bb 4>; [INV]

  <bb 3> :
  __builtin_abort ();

  <bb 4> :
  return;

}


---


### compiler : `gcc`
### title : `GCC/libstdc++ outputs inferior code for std::fill and std::fill_n vs std::memset on c-style arrays`
### open_at : `2018-07-10T22:28:43Z`
### last_modified_date : `2021-12-23T03:31:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86471
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.3.0`
### severity : `normal`
### contents :
Test setup: 
============
Xubuntu 18, Core2Duo E8500 CPU, GCC 7.3



Results in release mode (-O2 -march=native):
=============================================

2018-07-10 21:28:11
Running ./google_benchmark_test
Run on (2 X 3800.15 MHz CPU s)
CPU Caches:
  L1 Data 32K (x2)
  L1 Instruction 32K (x2)
  L2 Unified 6144K (x1)
-----------------------------------------------------
Benchmark              Time           CPU Iterations
-----------------------------------------------------
memory_filln       16488 ns      16477 ns      42460
memory_fill        16493 ns      16493 ns      42440
memory_memset       8414 ns       8408 ns      83022



Results in debug mode (-O0):
=============================

2018-07-10 21:48:09
Running ./google_benchmark_test
Run on (2 X 3800.15 MHz CPU s)
CPU Caches:
  L1 Data 32K (x2)
  L1 Instruction 32K (x2)
  L2 Unified 6144K (x1)
-----------------------------------------------------
Benchmark              Time           CPU Iterations
-----------------------------------------------------
memory_filln       87209 ns      87139 ns       8029
memory_fill        94593 ns      94533 ns       7411
memory_memset       8441 ns       8434 ns      82833




Code:
======

// Uses Google Benchmark. Rearrange the code any way you want, results stay similar:
#include <cstring>
#include <algorithm>
#include <benchmark/benchmark.h>


static void memory_memset(benchmark::State& state)
{
	int ints[50000];
	
	for (auto _ : state)
	{
		std::memset(ints, 0, sizeof(int) * 50000);
	}
}


static void memory_filln(benchmark::State& state)
{
	int ints[50000];
	
	for (auto _ : state)
	{
		std::fill_n(ints, 50000, 0);
	}
}


static void memory_fill(benchmark::State& state)
{
	int ints[50000];
	
	for (auto _ : state)
	{
		std::fill(std::begin(ints), std::end(ints), 0);
	}
}


// Register the function as a benchmark
BENCHMARK(memory_filln);
BENCHMARK(memory_fill);
BENCHMARK(memory_memset);



int main (int argc, char ** argv)
{
    benchmark::Initialize (&argc, argv);
    benchmark::RunSpecifiedBenchmarks ();
    return 0;
}


---


### compiler : `gcc`
### title : `malloc attribute when pointer is returned as part of a struct`
### open_at : `2018-07-11T11:52:28Z`
### last_modified_date : `2023-08-15T14:39:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86488
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `enhancement`
### contents :
I am trying to implementing P0901r0 [0] in jemalloc. The signature of the new memory allocation function looks like this: 

typedef struct {
	void *ptr;
	size_t usize;
} smallocx_return_t;
smallocx_return_t je_smallocx(size_t size, int flags); 

I'd like to set the "malloc" [1] attribute for this function but AFAICT this is not currently possible for this new function because the pointer smallocx_return_t::ptr is part of a struct. 

[0]: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0901r0.html

[1]: From https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html

> malloc
> This tells the compiler that a function is malloc-like, i.e., that the pointer P 
> returned by the function cannot alias any other pointer valid when the function 
> returns, and moreover no pointers to valid objects occur in any storage 
> addressed by P.
>
> Using this attribute can improve optimization. Functions like malloc and calloc 
> have this property because they return a pointer to uninitialized or zeroed-out 
> storage. However, functions like realloc do not have this property, as they can 
> return a pointer to storage containing pointers.


---


### compiler : `gcc`
### title : `[8/9 regression] wasted instructions for x86 float x!=x`
### open_at : `2018-07-11T21:39:51Z`
### last_modified_date : `2018-12-06T04:25:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86497
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.0`
### severity : `normal`
### contents :
When compiling

bool is_nan1(double x){
    return x!=x;
}

with g++-8.1  -O3 the resulting assembler (https://godbolt.org/g/BBFM3Q) is 

_Z7is_nan1d:
  ucomisd %xmm0, %xmm0
  movl $1, %edx
  setne %al
  cmovp %edx, %eax
  ret

However, for version 7.3 the result was (https://godbolt.org/g/tR69jf) better:

_Z7is_nan1d:
  ucomisd %xmm0, %xmm0
  setp %al
  ret

Also for 8.1 -Os is the assembler somewhat strange:

_Z7is_nan1d:
  ucomisd %xmm0, %xmm0
  movb $1, %al
  jp .L2
  setne %al


---


### compiler : `gcc`
### title : `vectorization failure for a nest loop`
### open_at : `2018-07-12T07:35:15Z`
### last_modified_date : `2019-11-21T16:02:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86504
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 44386
bad vectorizatoin result for boundary size 16

For the case below, the code generated by “gcc -O3” is very ugly, and the inner loop can be correctly vectorized. Please refer to attached file test_loop_inner_16.s.

char g_d[1024], g_s1[1024], g_s2[1024];
void test_loop(void)
{
    char *d = g_d, *s1 = g_s1, *s2 = g_s2;

    for ( int y = 0; y < 128; y++ )
    {
        for ( int x = 0; x < 16; x++ )
            d[x] = s1[x] + s2[x];
        d += 16;
    }
}

If we change inner loop “for ( int x = 0; x < 16; x++ )” to be like “for ( int x = 0; x < 32; x++ )”, i.e. the loop boundary size changes from 16 to 32, very beautiful vectorization code would be generated. For example, the code below is the aarch64 result for loop boundary size 32, and it the same case for x86.

test_loop:
.LFB0:
        .cfi_startproc
        adrp    x2, g_s1
        adrp    x3, g_s2
        add     x2, x2, :lo12:g_s1
        add     x3, x3, :lo12:g_s2
        adrp    x0, g_d
        adrp    x1, g_d+2048
        add     x0, x0, :lo12:g_d
        add     x1, x1, :lo12:g_d+2048
        ldp     q1, q2, [x2]
        ldp     q3, q0, [x3]
        add     v1.16b, v1.16b, v3.16b
        add     v0.16b, v0.16b, v2.16b
        .p2align 3,,7
.L2:
        str     q1, [x0]
        str     q0, [x0, 16]!
        cmp     x0, x1
        bne     .L2
        ret

The code generated for loop boundary size 8 is also very bad. 

Any idea?


---


### compiler : `gcc`
### title : `[missed-optimization] extraneous instruction emitted in switch converted to look-uptable load`
### open_at : `2018-07-15T15:16:12Z`
### last_modified_date : `2023-09-21T14:01:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86525
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The code 


enum  xx  { x1 = 1, x2 = 2, x3 = 3, x4, x5, x6 };

unsigned char f(xx x) {
    switch (x) {
    case xx::x1:
        return 2;
    case xx::x2:
        return 2;
    case xx::x3:
        return 7;
    case xx::x4:
        return 7;
    case xx::x5:
        return 7;
    case xx::x6:
        return 9;
    }
}

compiles to (thanks godbolt)

f(xx):
  leal -1(%rdi), %eax
  movzbl CSWTCH.1(%rax), %eax
  ret
CSWTCH.1:
  .byte 2
  .byte 2
  .byte 7
  .byte 7
  .byte 7
  .byte 9

which is lovely, but the lea instruction can be folded into the movzbl instruction:


  movzbl CSWTCH.1 - 1(%rax), %eax


This assumes that CSWTCH.1 is placed at offset != 0.


---


### compiler : `gcc`
### title : `Vectorization failure for a simple loop`
### open_at : `2018-07-16T07:44:02Z`
### last_modified_date : `2022-05-09T08:20:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86530
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC -O3 can't vectorize the following simple case. 

$ cat test_loop_2.c
int test_loop_2(char *p1, char *p2)
{
    int s = 0;
    for(int i=0; i<4; i++, p1+=4, p2+=4)
    {
        s += (p1[0]-p2[0]) + (p1[1]-p2[1]) + (p1[2]-p2[2]) + (p1[3]-p2[3]);
    }

    return s;
}

The vector size is 4*1=4 bytes, and it doesn't directly fit into 8-byte or 16-byte vector, but we still can extend the element to be 32-bit, and use the vector operations on 4*4=16 bytes vector.


---


### compiler : `gcc`
### title : `Use SSE to emulate __attribute__((vector_size(8)))`
### open_at : `2018-07-16T18:27:10Z`
### last_modified_date : `2021-08-19T20:09:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86541
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
In order to be more compatible across platforms, it would be
helpful if vector_size(8) was better supported for i386/x86_64.

The vast majority of the operations can be supported easily
with existing vector_size(16) instructions, and using either
(V)MOVQ to zero-extend the input or VPBROADCASTD/MOVDDUP to
replicate the input across the xmm register.

For integer operations it probably doesn't matter, but fp
operations would have different exception characteristics
with a zero-extension.  Replicating the inputs across the
lanes would avoid extra fp exceptions.


---


### compiler : `gcc`
### title : `missed vectorization with std::vector compared to icc 18`
### open_at : `2018-07-17T23:46:37Z`
### last_modified_date : `2021-08-03T00:34:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86557
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.0`
### severity : `enhancement`
### contents :
ICC 18 is able to vectorize this loop, while GCC 8 is not.

#include <vector>

std::size_t f(std::vector<std::vector<float>> const & v) {
    std::size_t ret = 0;
    for (std::size_t i = 0; i < v.size(); ++i)
      ret += v[i].size();
    return ret;
}


---


### compiler : `gcc`
### title : `Conditional statement doesn't trigger sincos transform (with -ffast-math)`
### open_at : `2018-07-18T17:20:58Z`
### last_modified_date : `2021-12-27T03:58:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86570
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
For the following test-case:

double f2(double x, double a, double b)
{
  if (a == b)
    return __builtin_sin (a * x) + __builtin_cos (b * x);
  return 0;
}

Optimized dump with -O2 -ffast-math -funsafe-math-optimizations yields:

f2 (double x, double a, double b)
{
  double _1;
  double _2;
  double _3;
  double _4;
  double _5;
  double _9;

  <bb 2> [local count: 1073741825]:
  if (a_6(D) == b_7(D))
    goto <bb 3>; [34.00%]
  else
    goto <bb 4>; [66.00%]

  <bb 3> [local count: 365072220]:
  _1 = a_6(D) * x_8(D);
  _2 = __builtin_sin (_1);
  _3 = b_7(D) * x_8(D);
  _4 = __builtin_cos (_3);
  _9 = _2 + _4;

  <bb 4> [local count: 1073741825]:
  # _5 = PHI <_9(3), 0.0(2)>
  return _5;

}

I assume the sincos transform would have been valid in the above case ?
Similarly missed for the divmod transform.

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `[8 regression] gcc.target/powerpc/altivec-7-le.c and gcc.target/powerpc/vsx-7-be.c fail starting with r262440`
### open_at : `2018-07-19T15:59:31Z`
### last_modified_date : `2021-05-14T10:54:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86589
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.1`
### severity : `normal`
### contents :
gcc.target/powerpc/vsx-7-be.c fails on both BE and LE while gcc.target/powerpc/altivec-7-le.c fails only on LE.

FAIL: gcc.target/powerpc/altivec-7-le.c scan-assembler-times vupkhpx 1
FAIL: gcc.target/powerpc/vsx-7-be.c scan-assembler-times vupkhpx 1


Executing on host: /home/seurer/gcc/build/gcc-8/gcc/xgcc -B/home/seurer/gcc/build/gcc-8/gcc/ /home/seurer/gcc/gcc-8/gcc/testsuite/gcc.target/powerpc/altivec-7-le.c     -fno-diagnostics-show-caret -fdiagnostics-color=never  -maltivec -ffat-lto-objects -S -o altivec-7-le.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/build/gcc-8/gcc/xgcc -B/home/seurer/gcc/build/gcc-8/gcc/ /home/seurer/gcc/gcc-8/gcc/testsuite/gcc.target/powerpc/ltivec-7-le.c -fno-diagnostics-show-caret -fdiagnostics-color=never -maltivec -ffat-lto-objects -S -o altivec-7-le.s
PASS: gcc.target/powerpc/altivec-7-le.c (test for excess errors)
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times vpkpx 2
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times vmulesb 1
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times vmulosb 1
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times \\mlxvd2x\\M|\\mlxv\\M 36
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times lvewx 2
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times lvxl 1
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times vupklsh 1
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times vupkhsh 1
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times xxlnor 4
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times xxland 4
PASS: gcc.target/powerpc/altivec-7-le.c scan-assembler-times xxlxor 5
gcc.target/powerpc/altivec-7-le.c: vupkhpx found 0 times
FAIL: gcc.target/powerpc/altivec-7-le.c scan-assembler-times vupkhpx 1

Executing on host: /home/seurer/gcc/build/gcc-8/gcc/xgcc -B/home/seurer/gcc/build/gcc-8/gcc/ /home/seurer/gcc/gcc-8/gcc/testsuite/gcc.target/powerpc/vsx-7-be.c     -fno-diagnostics-show-caret -fdiagnostics-color=never  -mvsx -ffat-lto-objects -S -o vsx-7-be.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/build/gcc-8/gcc/xgcc -B/home/seurer/gcc/build/gcc-8/gcc/ /home/seurer/gcc/gcc-8/gcc/testsuite/gcc.target/powerpc/vsx-7-be.c -fno-diagnostics-show-caret -fdiagnostics-color=never -mvsx -ffat-lto-objects -S -o vsx-7-be.s
PASS: gcc.target/powerpc/vsx-7-be.c (test for excess errors)
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times vpkpx 2
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times vmulesb 1
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times vmulosb 1
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times \\mlxvd2x\\M 36
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times lvewx 2
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times lvxl 1
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times vupklsh 1
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times vupkhsh 1
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times xxlnor 4
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times xxland 4
PASS: gcc.target/powerpc/vsx-7-be.c scan-assembler-times xxlxor 5
gcc.target/powerpc/vsx-7-be.c: vupkhpx found 0 times
FAIL: gcc.target/powerpc/vsx-7-be.c scan-assembler-times vupkhpx 1


---


### compiler : `gcc`
### title : `phiopt missed optimization of conditional add`
### open_at : `2018-07-20T07:56:35Z`
### last_modified_date : `2021-12-20T14:45:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86604
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Summary: The compiler thinks variables as of the widest domain.
(32 bits for int, 64 bits for int64_t) It's possible to optimize
further by giving the compiler a hint about the domain. It can
be used to eliminate most of the branches.

For clarity, I'm going to define __builtin_guarantee first.

#define __builtin_guarantee(a) \
if (!(a)) {                    \
    __builtin_unreachable();   \
}

The optimization: We can tell the compiler about the range
of a parameter, and it'll be able to use this knowledge
to eliminate most of the branches.

void CreateChecksum(int isTestNet, int *t) {
    if (isTestNet == 0)
        *t += 1;
}

Output with `-Ofast -march=native`:

CreateChecksum:
  test edi, edi
  jne .L3
  inc DWORD PTR [rsi]
.L3:
  ret

But if we could do that:

void CreateChecksum(int isTestNet, int *t) {
    __builtin_guarantee(isTestNet == 0 || isTestNet == 1);
    if (isTestNet == 0)
        *t += 1;
}

It'd see that

*t += isTestNet ^ 1

But for some reason, Compiler doesn't limit the number of
possibilities of variables according to builtin unreachable.


---


### compiler : `gcc`
### title : `Suboptimal code for pointer arithmetic with 'this'`
### open_at : `2018-07-20T10:33:51Z`
### last_modified_date : `2021-07-27T06:13:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86605
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
In GCC 5.5 an optimization of following code was broken:

struct Test {
    Test(Test* p) : i{this-p ? 10 : 20} {}
    int i;
};

int f() {
    Test test{(Test*)4};
    return test.i;
}



GCC 5.4 generates optimal code:
f():
  mov eax, 10
  ret

GCC 5.5 generates:
f():
  lea rax, [rsp-5]
  cmp rax, 7
  sbb eax, eax
  and eax, 10
  add eax, 10
  ret

GCC (trunk) generates also suboptimal code:
f():
  cmp rsp, 8
  mov edx, 20
  mov eax, 10
  cmove eax, edx
  ret



Note, that in GCC 5.5 the optimizer was fixed to generate optimal code for the following:
struct Test {
    Test(Test* p) : i{this-p ? 10 : 20} {}
    int i;
};

int f() {
    Test test{(Test*)3}; // 3 instead of 4
    return test.i;
}

So that now GCC (trunk) generates optimal code for that case:
f():
  mov eax, 10
  ret
but fail if we change 3 back to 4


---


### compiler : `gcc`
### title : `Missed optimization opportunity with array aliasing`
### open_at : `2018-07-21T15:48:25Z`
### last_modified_date : `2021-12-03T07:40:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86619
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
// gcc version 9.0.0 20180720 (experimental) 
// Compiled with -O3

int f(std::array<int, 3> & a, std::array<int, 2> & b)
{
  a[0] = 1;
  b[0] = 2;
  return a[0];
}

Produces:
f(std::array<int, 3ul>&, std::array<int, 2ul>&):
  mov DWORD PTR [rdi], 1
  mov DWORD PTR [rsi], 2
  mov eax, DWORD PTR [rdi]
  ret

Instead of
  mov DWORD PTR [rdi], 1
  mov eax, 1
  mov DWORD PTR [rsi], 2
  ret

But this does not seem to be something that libstdc++ can do anything about.
Consider a simplified array implementation:

template <class T, size_t size>
struct ar
{
  T ar[size];
  T &operator[](size_t offset) { return ar[offset]; }
};

int f1(ar<int, 3> & a, ar<int, 2> & b)
{
  a.ar[0] = 1;
  b.ar[0] = 2;
  return a.ar[0];
// This is perfect:
/*
  mov DWORD PTR [rdi], 1
  mov eax, 1
  mov DWORD PTR [rsi], 2
  ret
*/
}

// BUT:
int f2(ar<int, 3> & a, ar<int, 2> & b)
{
  a[0] = 1;
  b[0] = 2;
  return a[0];
// Too conservative alias analysis 
/*
  mov DWORD PTR [rdi], 1
  mov DWORD PTR [rsi], 2
  mov eax, DWORD PTR [rdi]
*/
}

It seems that by returning a reference, operator[] makes the compiler lose the fact that a and b can't alias.

I'm not a language lawyer, but the following also seems to be another lost optimization opportunity for arrays. After all, a and b have different types:
int g(int (&a)[2], int (&b)[3])
{
   a[0] = 1;
   b[0] = 2;
   return a[0];
/*
  mov DWORD PTR [rdi], 1
  mov DWORD PTR [rsi], 2
  mov eax, DWORD PTR [rdi]
  ret
*/
}


---


### compiler : `gcc`
### title : `Missed simplification of division`
### open_at : `2018-07-22T17:34:50Z`
### last_modified_date : `2021-12-13T01:04:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86628
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `enhancement`
### contents :
Hello,

for code:
int f(int x, int y, int z) {
   return (x * y  * z) / (y * z);
}

GCC 8.1 (x86-64) with -O3 emits:
f(int, int, int):
  mov eax, edi
  imul eax, esi
  imul esi, edx
  imul eax, edx
  cdq
  idiv esi
  ret


but one multiplication can be removed, as Clang does it:
f(int, int, int):
  imul esi, edx
  imul edi, esi
  mov eax, edi
  cdq
  idiv esi
  ret


Also, for:
unsigned f2(unsigned x, unsigned y, unsigned z) {
   return (x*z) / (y*z);
}

f2(unsigned int, unsigned int, unsigned int):
  mov eax, edi
  imul esi, edx
  imul eax, edx
  xor edx, edx
  div esi
  ret

This could be simplified to "x/y". For a signed case it could be possible too, just z = -1 needs to be checked.


---


### compiler : `gcc`
### title : `possible gcc optimization`
### open_at : `2018-07-26T10:46:51Z`
### last_modified_date : `2023-06-09T18:13:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86680
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `enhancement`
### contents :
Created attachment 44444
testcase

I can see this on x86_64 and aarch64. The first function is compiled with much
bigger code. Seems the alignment to 8 bytes and thus this multiple of 8
is forgotten in some optimization step.

best regards,

Florian La Roche




$ aarch64-linux-gnu-gcc-8 -O2 -c test.c
$ aarch64-linux-gnu-objdump -d test.o 

test.o:     Dateiformat elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <clear_bss1>:
   0:	90000001 	adrp	x1, 0 <__bss_start1>
   4:	90000000 	adrp	x0, 0 <__bss_end1>
   8:	f9400022 	ldr	x2, [x1]
   c:	f9400000 	ldr	x0, [x0]
  10:	eb00005f 	cmp	x2, x0
  14:	54000142 	b.cs	3c <clear_bss1+0x3c>  // b.hs, b.nlast
  18:	d1000401 	sub	x1, x0, #0x1
  1c:	aa0203e0 	mov	x0, x2
  20:	cb020021 	sub	x1, x1, x2
  24:	927df021 	and	x1, x1, #0xfffffffffffffff8
  28:	91002021 	add	x1, x1, #0x8
  2c:	8b020021 	add	x1, x1, x2
  30:	f800841f 	str	xzr, [x0], #8
  34:	eb01001f 	cmp	x0, x1
  38:	54ffffc1 	b.ne	30 <clear_bss1+0x30>  // b.any
  3c:	d65f03c0 	ret

0000000000000040 <clear_bss2>:
  40:	90000000 	adrp	x0, 0 <__bss_start2>
  44:	90000001 	adrp	x1, 0 <__bss_end2>
  48:	f9400000 	ldr	x0, [x0]
  4c:	f9400021 	ldr	x1, [x1]
  50:	f9400000 	ldr	x0, [x0]
  54:	f9400021 	ldr	x1, [x1]
  58:	eb01001f 	cmp	x0, x1
  5c:	54000082 	b.cs	6c <clear_bss2+0x2c>  // b.hs, b.nlast
  60:	f800841f 	str	xzr, [x0], #8
  64:	eb01001f 	cmp	x0, x1
  68:	54ffffc3 	b.cc	60 <clear_bss2+0x20>  // b.lo, b.ul, b.last
  6c:	d65f03c0 	ret



Please note how the second function is compiled much smaller. The first
function from "18" to "2c" should basically be optimized away.


Compiling with -Os is also much better:
$ aarch64-linux-gnu-gcc-8 -Os -c test.c
$ aarch64-linux-gnu-objdump -d test.o 

test.o:     Dateiformat elf64-littleaarch64


Disassembly of section .text:

0000000000000000 <clear_bss1>:
   0:	90000000 	adrp	x0, 0 <__bss_start1>
   4:	90000001 	adrp	x1, 0 <__bss_end1>
   8:	f9400000 	ldr	x0, [x0]
   c:	f9400021 	ldr	x1, [x1]
  10:	eb01001f 	cmp	x0, x1
  14:	54000043 	b.cc	1c <clear_bss1+0x1c>  // b.lo, b.ul, b.last
  18:	d65f03c0 	ret
  1c:	f800841f 	str	xzr, [x0], #8
  20:	17fffffc 	b	10 <clear_bss1+0x10>

0000000000000024 <clear_bss2>:
  24:	90000000 	adrp	x0, 0 <__bss_start2>
  28:	90000001 	adrp	x1, 0 <__bss_end2>
  2c:	f9400000 	ldr	x0, [x0]
  30:	f9400021 	ldr	x1, [x1]
  34:	f9400000 	ldr	x0, [x0]
  38:	f9400021 	ldr	x1, [x1]
  3c:	eb00003f 	cmp	x1, x0
  40:	54000048 	b.hi	48 <clear_bss2+0x24>  // b.pmore
  44:	d65f03c0 	ret
  48:	f800841f 	str	xzr, [x0], #8
  4c:	17fffffc 	b	3c <clear_bss2+0x18>







The problem also shows up on x86_64 from "13" to "22":
$ gcc -O2 -c test.c
$ objdump -d test.o

test.o:     Dateiformat elf64-x86-64


Disassembly of section .text:

0000000000000000 <clear_bss1>:
   0:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 7 <clear_bss1+0x7>
   7:	48 8d 15 00 00 00 00 	lea    0x0(%rip),%rdx        # e <clear_bss1+0xe>
   e:	48 39 d0             	cmp    %rdx,%rax
  11:	73 25                	jae    38 <clear_bss1+0x38>
  13:	48 8d 48 08          	lea    0x8(%rax),%rcx
  17:	48 83 c2 07          	add    $0x7,%rdx
  1b:	48 29 ca             	sub    %rcx,%rdx
  1e:	48 83 e2 f8          	and    $0xfffffffffffffff8,%rdx
  22:	48 01 ca             	add    %rcx,%rdx
  25:	0f 1f 00             	nopl   (%rax)
  28:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)
  2f:	48 83 c0 08          	add    $0x8,%rax
  33:	48 39 d0             	cmp    %rdx,%rax
  36:	75 f0                	jne    28 <clear_bss1+0x28>
  38:	f3 c3                	repz retq 
  3a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

0000000000000040 <clear_bss2>:
  40:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 47 <clear_bss2+0x7>
  47:	48 8b 15 00 00 00 00 	mov    0x0(%rip),%rdx        # 4e <clear_bss2+0xe>
  4e:	48 39 d0             	cmp    %rdx,%rax
  51:	73 16                	jae    69 <clear_bss2+0x29>
  53:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
  58:	48 83 c0 08          	add    $0x8,%rax
  5c:	48 c7 40 f8 00 00 00 	movq   $0x0,-0x8(%rax)
  63:	00 
  64:	48 39 d0             	cmp    %rdx,%rax
  67:	72 ef                	jb     58 <clear_bss2+0x18>
  69:	f3 c3                	repz retq


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] 436.cactusADM regression on aarch64`
### open_at : `2018-07-26T15:42:00Z`
### last_modified_date : `2023-07-07T10:34:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86685
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.1.1`
### severity : `normal`
### contents :
For GCC 8 since r254378 we use SCHED_PRESSURE_MODEL in aarch64 during scheduling. This gives an improvement in SPEC2017 but we have noticed it regresses 436.cactusADM from SPEC2006 significantly. On a Cortex-A72 we see a 10% regression from that change.

The vector loop in the hot function bench_staggeredleapfrog2_ contains significantly more stack spills. I appreciate the goal of that scheduling model isn't necessarily to minimise stack spills, but it should do a better job at keeping them under reasonable control

Given that SCHED_PRESSURE_MODEL works quite well for us in other cases it would be good to fix it going haywire here to get the performance back up.


---


### compiler : `gcc`
### title : `inefficient atomic_fetch_xor`
### open_at : `2018-07-27T02:47:40Z`
### last_modified_date : `2021-12-28T06:23:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86693
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.1`
### severity : `enhancement`
### contents :
(Compiled with O2 on x86-64)

Consider the following example:

void func1();

void func(unsigned long *counter)
{
        if (__atomic_fetch_xor(counter, 1, __ATOMIC_ACQ_REL) == 1) {
                func1();
        }
}

It is clear that the code can be optimized to simply do 'lock xorq' rather than cmpxchg loop since the xor operation can easily be inverted 1^1 = 0, i.e. can be tested from flags directly (just like for similar cases with fetch_sub and fetch_add which gcc optimizes well).

However, gcc currently generates cmpxchg loop:
func:
.LFB0:
        .cfi_startproc
        movq    (%rdi), %rax
.L2:
        movq    %rax, %rcx
        movq    %rax, %rdx
        xorq    $1, %rcx
        lock cmpxchgq   %rcx, (%rdi)
        jne     .L2
        cmpq    $1, %rdx
        je      .L7
        rep ret

Compare this with fetch_sub instead of fetch_xor:
func:
.LFB0:
        .cfi_startproc
        lock subq       $1, (%rdi)
        je      .L4
        rep ret


---


### compiler : `gcc`
### title : `Calls to builtins do not use visibility information`
### open_at : `2018-07-27T08:44:22Z`
### last_modified_date : `2021-09-17T00:27:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86695
### status : `NEW`
### tags : `missed-optimization, visibility`
### component : `c`
### version : `9.0`
### severity : `normal`
### contents :
On the following testcase all 4 generated calls to memcpy/malloc should use direct calls thanks to provided visibility attribute, but only the one in 'g2' does. The other calls are assumed to be external and go via the PLT with the corresponding size/speed penalty with -fpic -m32.

// gcc -O2 -fpic -m32
#pragma GCC visibility push(hidden)
void *memcpy(void *, const void *, __SIZE_TYPE__);
void *malloc(__SIZE_TYPE__);
#pragma GCC visibility pop

struct s {
  char c[1024*1024];
};

void f1(struct s a[2])
{
  a[1] = a[0];
}

void f2(struct s a[2])
{
  memcpy(a+1, a, sizeof *a);
}

void *g1()
{
  return __builtin_malloc(42);
}

void *g2()
{
  return malloc(42);
}


---


### compiler : `gcc`
### title : `Missed optimization: optimizing set of if statements`
### open_at : `2018-07-27T20:42:07Z`
### last_modified_date : `2023-05-05T07:12:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86707
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
gcc(trunk) with -O3 -std=c++17 for this code:

unsigned int foo(unsigned int x)
{
    if(x % 2 == 0)
    {
        return x * 2;
    }
    if(x % 4 == 0)
    {
        return x * 4;
    }
    if(x % 8 == 0)
    {
        return x * 8;
    }
    if(x % 16 == 0)
    {
        return x * 16;
    }
    if(x % 32 == 0)
    {
        return x * 32;
    }
    return 100;
}

generates this:

foo(unsigned int):
  test dil, 1
  je .L9
  test dil, 3
  je .L10
  test dil, 7
  je .L11
  mov eax, edi
  test dil, 15
  je .L12
  sal eax, 5
  and edi, 31
  mov edi, 100
  cmovne eax, edi
  ret
.L10:
  lea eax, [0+rdi*4]
  ret
.L9:
  lea eax, [rdi+rdi]
  ret
.L12:
  sal eax, 4
  ret
.L11:
  lea eax, [0+rdi*8]
  ret


As you see, generated code is suboptimal: here we can leave only first 'if' statement and otherwise return 100.


---


### compiler : `gcc`
### title : `strlen of an empty aggregate element or member string not folded`
### open_at : `2018-07-27T21:36:02Z`
### last_modified_date : `2021-09-05T06:41:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86708
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC 9 can fold explicitly initialized members of aggregates but it doesn't know to do the same for members or array elements that are initialized implicitly to the empty string, as in the following test case:

$ cat c.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout c.c
const char a[][4] = { "123" };

int f (int i)
{
  return __builtin_strlen (a[1]);   // not folded
}

const struct {
  char a[4], b[4];
} s = { "123" };

int g (int i)
{
  return __builtin_strlen (s.b);   // not folded
}


;; Function f (f, funcdef_no=0, decl_uid=1899, cgraph_uid=1, symbol_order=1)

f (int i)
{
  long unsigned int _1;
  int _3;

  <bb 2> [local count: 1073741825]:
  _1 = __builtin_strlen (&a[1]);
  _3 = (int) _1;
  return _3;

}



;; Function g (g, funcdef_no=1, decl_uid=1906, cgraph_uid=2, symbol_order=3)

g (int i)
{
  long unsigned int _1;
  int _3;

  <bb 2> [local count: 1073741825]:
  _1 = __builtin_strlen (&s.b);
  _3 = (int) _1;
  return _3;

}


---


### compiler : `gcc`
### title : `3 missing logarithm optimizations`
### open_at : `2018-07-28T07:08:16Z`
### last_modified_date : `2021-12-20T02:31:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86710
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
I've looked at match.pd, and spotted missing
properties of logarithm. I could poorly write
the code for two, still needs to check that
the log base is the same:

 (for logs (LOG LOG2 LOG10)
  /* logN(a) + logN(b) -> logN(a * b). */
  (simplify
   (plus (logs (@0)) (logs (@1)))
   (logs (mult:c (@0) (@1))))

 (for logs (LOG LOG2 LOG10)
  /* logN(a) - logN(b) -> logN(a / b). */
  (simplify
   (sub (logs (@0)) (logs (@1)))
   (logs (rdiv (@0) (@1))))

I couldn't code the last one

/* logN(b)/logN(a) * logN(c)/logN(b) -> logN(c)/logN(a). */

Here are the test cases:

1: https://godbolt.org/g/boqxQb

2: https://godbolt.org/g/m7zHxK

3: https://godbolt.org/g/KXrbV4


---


### compiler : `gcc`
### title : `ifcvt produces x&0 that is never cleaned up`
### open_at : `2018-07-29T11:49:47Z`
### last_modified_date : `2022-06-23T20:22:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86722
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
(could be rtl-optimization or target)

void f(double*d,double*e){
  for(;d<e;++d)
    *d=(*d<.5)?.7:0;
}

Compiling at -O2, noce_try_cmove is triggered and produces

	pxor	%xmm4, %xmm4
[...]
	cmpltsd	%xmm2, %xmm0
	andpd	%xmm0, %xmm3
	andnpd	%xmm4, %xmm0
	orpd	%xmm3, %xmm0

The last 2 instructions are useless since xmm4 is 0. I don't know if we should have something to cleanup x&0 later than pass ce2 or if the cmove expansion should have a special case for the value 0.


---


### compiler : `gcc`
### title : `not optimizing with bswap, that casts to int aftwards`
### open_at : `2018-07-29T13:18:31Z`
### last_modified_date : `2021-11-28T23:28:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86723
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `enhancement`
### contents :
Created attachment 44461
souce.cpp

Compiling attachment with `-std=c++17 -O3`, I expect g++ to optimize the swap_unsigned function to a bswap instruction.

It does so, like for equivalent functions (not included here) for 32/16 bits swap.
Unfortunately, if I add constexpr to the 64 bits swap_unsigned function, it gets inlined and no longer use bswap.
Interestingely, using __attribute__((noinline)) will force g++ to use bswap.


---


### compiler : `gcc`
### title : `[9/10 Regression] gcc.target/aarch64/sve/vcond_[45].c fail after recent combine patch`
### open_at : `2018-07-31T12:54:12Z`
### last_modified_date : `2019-10-18T08:38:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86753
### status : `RESOLVED`
### tags : `missed-optimization, xfail`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `Missing sin(atan(x)) and cos(atan(x)) optimizations`
### open_at : `2018-08-02T11:38:13Z`
### last_modified_date : `2019-02-21T15:56:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86829
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 44486
add sin(atan(x)) and cos(atan(x)) substitutions rules.

The file named 'match.pd' does not contain the following simplifications rules: sin(atan(x)) -> x / sqrt(x*x + 1)  and  cos(atan(x)) -> 1 / sqrt(x*x + 1). According to the simple brenchmark I made, these substitutions can provide  a 10x speedup in the code. I wrote a patch to add these optimizations.

link to the perf test: https://pastebin.com/5ujSRmhq

assembly dump of the perftest: https://pastebin.com/gLJeWHY8

The code I wrote add an instruction 'CVTSS2SD'. I don't know why it happens.


---


### compiler : `gcc`
### title : `missing -Wformat-overflow on %s with a constant string plus variable offset`
### open_at : `2018-08-03T19:30:22Z`
### last_modified_date : `2020-05-01T18:58:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86851
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
Even though it's possible to determine the range of lengths of the constant string in the sprintf call below the sprintf pass fails to do so and so misses the buffer overflow and also emits suboptimal code.  The problem is in the string_constant function failing to determine the string the &a[i] expression refers to.  The expression the function sees is SSA_NAME (L"123...") + offset but it only handles ADDR_EXPR, PLUS_EXPR/POINTER_PLUS_EXPR, and DECL.

$ cat d.c && gcc -O2 -S -Wall -fdump-tree-printf-return-value=/dev/stdout d.c
static const char a[] = "123456789";

char d[4];

void f (int i)
{
  if (i > 2)
    i = 2;

  __builtin_sprintf (d, "%s", &a[i]);
}


;; Function f (f, funcdef_no=0, decl_uid=1907, cgraph_uid=1, symbol_order=2)

d.c:10: __builtin_sprintf: objsize = 4, fmtstr = "%s"
  Directive 1 at offset 0: "%s"
    Result: 0, 0, -1, 9223372036854775807 (0, 0, -1, -1)
  Directive 2 at offset 2: "", length = 1

f (int i)
{
  const char * _1;
  sizetype _2;

  <bb 2> [local count: 1073741825]:
  i_6 = MIN_EXPR <i_3(D), 2>;
  _2 = (sizetype) i_6;
  _1 = &a + _2;
  __builtin_strcpy (&d, _1);
  return;

}


---


### compiler : `gcc`
### title : `s390x gcc build fails when configured with --disable-checking`
### open_at : `2018-08-08T13:43:01Z`
### last_modified_date : `2020-06-10T17:45:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86889
### status : `ASSIGNED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Seen on master (18d371d3):

build$ ../configure --disable-checking
build$ make -j$(getconf _NPROCESSORS_ONLN)

../../gcc/bitmap.c: In function ‘unsigned int bitmap_last_set_bit(const_bitmap)’:
../../gcc/bitmap.c:841:26: error: array subscript -1 is below array bounds of ‘const BITMAP_WORD [2]’ {aka ‘const long unsigned int [2]’} [-Werror=array-bounds]
       word = elt->bits[ix];

The code in question is:

 839   for (ix = BITMAP_ELEMENT_WORDS - 1; ix >= 0; ix--)
 840     {
 841       word = elt->bits[ix];
 842       if (word)
 843         goto found_bit;
 844     }

BITMAP_ELEMENT_WORDS on s390x is 2.
I narrowed this down to cunrolli pass, which unrolls this loop 3 times instead of 2, so ix=[1, 0, -1] instead of just [1, 0].
And indeed, building this individual file with -fdisable-tree-cunrolli helps.


---


### compiler : `gcc`
### title : `RTL CSE  commoning trivial constants across call and/or too early`
### open_at : `2018-08-08T16:54:04Z`
### last_modified_date : `2022-10-16T18:17:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86892
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
For the following testcase cse.c will common the constant 0 across the call which then requires the use of a non-volatile register (and prologue/epilogue save/restore).

void bar();
int a, b;
void foo()
{
  a = 0;
  bar();
  b = 0;
}


I have also observed a situation where early cse of a constant prevented some combine transformations from occurring because the register's lifetime had been extended.

The feeling is that cse of trivial constants should not be done so early in the pass schedule and should not be done across calls at all.


---


### compiler : `gcc`
### title : `[AArch64] Suboptimal register allocation for int/float reinterpret`
### open_at : `2018-08-09T16:03:37Z`
### last_modified_date : `2021-08-22T08:37:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86901
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
This example (narrowed down from GLIBC) shows inefficient register allocation:

typedef unsigned int uint32_t;

float g (float);

static inline uint32_t
top12 (float x)
{
  union
  {
    float f;
    uint32_t i;
  } u = {x};
  return (u.i >> 20) & 0x7ff;
}

void
f1 (float y, float *p)
{
  if (__builtin_expect (top12 (y) < top12 (1.0), 1))
    *p = y * y;
  else
    g (y + y);
}

void
f2 (float y, float *p)
{
  if (__builtin_expect (top12 (y) < top12 (1.0), 1))
    *p = y * y;
  else
    g (y);
}

On AArch64 this generates with -O2:

f1:
        fmov    x1, d0
        ubfx    x1, x1, 20, 11
        cmp     w1, 1015
        bhi     .L2
        fmul    s0, s0, s0
        str     s0, [x0]
        ret
.L2:
        fadd    s0, s0, s0
        b       g

f2:
        .cfi_startproc
        fmov    s1, s0   // eh?
        fmov    x1, d1   // why not fmov w1, s0???
        ubfx    x1, x1, 20, 11
        cmp     w1, 1015
        bhi     .L7
        fmul    s1, s0, s0
        str     s1, [x0]
        ret
.L7:
        b       g

Also the move is done as 64 bits rather than 32.


---


### compiler : `gcc`
### title : `Missing common subexpression elimination for types other than int`
### open_at : `2018-08-10T10:50:10Z`
### last_modified_date : `2021-07-24T10:07:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86909
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Consider the following minimized example of the std::variant visitation:

using size_t = unsigned long long;

struct A {};
static const size_t variant_npos = -1;

struct variant {
    A a;

    using __index_type = unsigned char; // !!!!
    __index_type _M_index;

    size_t index() const noexcept {
	    return (_M_index == __index_type(variant_npos) ? variant_npos : _M_index);
    }
};

template<size_t _Np>
static A* get_if(variant* __ptr) noexcept {
    return (__ptr->index() == _Np ? &__ptr->a : nullptr);
}

A* foo(variant& in) {
    int i = in.index();
    if (i==0) return get_if<0>(&in);
    if (i==1) return get_if<1>(&in);
    if (i==2) return get_if<2>(&in);
    if (i==3) return get_if<3>(&in);
    if (i==4) return get_if<4>(&in);
    if (i==5) return get_if<5>(&in);
    if (i==6) return get_if<6>(&in);

    return get_if<7>(&in);
}


GCC generates assembly with multiple comparisons:
foo(variant&):
<...>
        cmp     ecx, 1
        je      .L4
        cmp     ecx, 2
        je      .L4
        cmp     ecx, 3
        je      .L4
        cmp     ecx, 4
        je      .L4
        cmp     ecx, 5
        je      .L4
        cmp     ecx, 6
        je      .L4
        cmp     dl, 7
        je      .L4

Clang eliminates the subexpressions:
foo(variant&):                        # @foo(variant&)
        movzx   ecx, byte ptr [rdi + 1]
        cmp     cl, -1
        mov     edx, -1
        cmovne  edx, ecx
        cmp     edx, 7
        jb      .LBB0_2
        mov     dl, 7
.LBB0_2:
        xor     eax, eax
        cmp     cl, dl
        cmove   rax, rdi
        ret


Note that if we change `__index_type` to `int` then GCC generates great assembly:
foo(variant&):
        mov     edx, DWORD PTR [rdi+4]
        xor     eax, eax
        cmp     edx, -1
        je      .L1
        cmp     edx, 7
        cmovbe  rax, rdi
.L1:
        ret


---


### compiler : `gcc`
### title : `Function pointer imposes an optimization barrier`
### open_at : `2018-08-10T12:42:44Z`
### last_modified_date : `2021-06-14T11:14:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86912
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
Consider the following minimized example of std::variant visitation:

struct A {};
struct B : A {};
struct C : A {};
struct D : A {};
struct E : A {};
struct X : A {};

template <class T>
struct helper {
    static A& call(void* value) {
        return *static_cast<T*>(value);
    }
};

A& get_base(int index, void* value) {
    using f_ptr = A&(*)(void*);
    constexpr f_ptr visitors[] = {
        helper<A>::call, helper<B>::call, helper<C>::call,
        helper<D>::call, helper<E>::call, helper<X>::call,
    };
    return visitors[index](value);
}


For the above code GCC generates very suboptimal assembly that fills an array with pointers and does the jmp:
<...>
get_base(int, void*):
  mov QWORD PTR [rsp-56], OFFSET FLAT:(anonymous namespace)::helper<A>::call(void*)
  movsx rax, edi
  mov rdi, rsi
  mov QWORD PTR [rsp-48], OFFSET FLAT:(anonymous namespace)::helper<B>::call(void*)
  mov QWORD PTR [rsp-40], OFFSET FLAT:(anonymous namespace)::helper<C>::call(void*)
  mov QWORD PTR [rsp-32], OFFSET FLAT:(anonymous namespace)::helper<D>::call(void*)
  mov QWORD PTR [rsp-24], OFFSET FLAT:(anonymous namespace)::helper<E>::call(void*)
  mov QWORD PTR [rsp-16], OFFSET FLAT:(anonymous namespace)::helper<X>::call(void*)
  mov rax, QWORD PTR [rsp-56+rax*8]
  jmp rax


Optimal assembly should be the following:
get_base(int, void*):
  mov rax, rsi
  ret


If we rewrite `get_base` to avoid calls by function pointer, the assembly becomes optimal:
A& get_base(int index, void* value) {
    if (index == 0) return helper<A>::call(value);
    if (index == 1) return helper<B>::call(value);
    if (index == 2) return helper<C>::call(value);
    if (index == 3) return helper<D>::call(value);
    if (index == 4) return helper<E>::call(value);
    if (index == 5) return helper<X>::call(value);
}


---


### compiler : `gcc`
### title : `strnlen() of a constant not folded due to laddress transformation`
### open_at : `2018-08-13T14:58:37Z`
### last_modified_date : `2020-06-20T23:20:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86936
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Despite the argument being a constant string, as a result of the laddress transformation the call to strnlen() in the function below is not folded into a constant because it loses the knowledge about which element of the array the strnlen argument refers to.

$ cat f.c && gcc -O2 -S -Wall -Wextra -Wnull-dereference -fdump-tree-bswap=/dev/stdout -fdump-tree-laddress=/dev/stdout f.c
const char a[][4] = { "123", "1234" };

int f (int i)
{
  return __builtin_strnlen (&a[1][i], 4);
}

;; Function f (f, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=1)

f (int i)
{
  const char * _1;
  long unsigned int _2;
  int _5;

  <bb 2> [local count: 1073741825]:
  _1 = &a[1][i_3(D)];
  _2 = __builtin_strnlen (_1, 4);
  _5 = (int) _2;
  return _5;

}



;; Function f (f, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=1)

f (int i)
{
  const char * _1;
  long unsigned int _2;
  int _5;
  sizetype _6;
  sizetype _7;

  <bb 2> [local count: 1073741825]:
  _6 = (sizetype) i_3(D);
  _7 = _6 + 4;
  _1 = &a + _7;
  _2 = __builtin_strnlen (_1, 4);
  _5 = (int) _2;
  return _5;

}


---


### compiler : `gcc`
### title : `strnlen() of a conditional expression with constant operands not folded`
### open_at : `2018-08-13T15:12:48Z`
### last_modified_date : `2020-06-20T23:20:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86937
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC folds the strlen() call below with a conditional expression involving constant operands but it fails to do the same for the equivalent strnlen() call.

Besides missing this optimization opportunity it also prevents it from diagnosing strnlen() calls with unterminated character arrays where the bound is greater than the size of the array.

$ cat f.c && gcc -O2 -S -Wall -Wextra -Wnull-dereference -fdump-tree-optimized=/dev/stdout f.c
const char a[4] = "123";

int f (int i)
{
  return __builtin_strlen (i ? a : "");
}

int g (int i)
{
  return __builtin_strnlen (i ? a : "", 4);
}

;; Function f (f, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=1)

Removing basic block 3
f (int i)
{
  int prephitmp_7;

  <bb 2> [local count: 1073741825]:
  if (i_3(D) != 0)
    goto <bb 4>; [50.00%]
  else
    goto <bb 3>; [50.00%]

  <bb 3> [local count: 536870913]:

  <bb 4> [local count: 1073741825]:
  # prephitmp_7 = PHI <3(2), i_3(D)(3)>
  return prephitmp_7;

}



;; Function g (g, funcdef_no=1, decl_uid=1909, cgraph_uid=2, symbol_order=2)

Removing basic block 3
g (int i)
{
  long unsigned int _1;
  const char * iftmp.1_2;
  int _5;

  <bb 2> [local count: 1073741825]:
  if (i_3(D) != 0)
    goto <bb 4>; [50.00%]
  else
    goto <bb 3>; [50.00%]

  <bb 3> [local count: 536870913]:

  <bb 4> [local count: 1073741825]:
  # iftmp.1_2 = PHI <&a(2), ""(3)>
  _1 = __builtin_strnlen (iftmp.1_2, 4);
  _5 = (int) _1;
  return _5;

}


---


### compiler : `gcc`
### title : `Avoid jump table for switch statement with -mindirect-branch=thunk`
### open_at : `2018-08-14T15:24:18Z`
### last_modified_date : `2019-06-15T00:29:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86952
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Why is GCC generating a jump table for a five-entry switch statement if
retpolines are on?  This has got to be a *huge* performance loss.  The
retpoline sequence is very, very slow, and branches aren't that slow.
A five-entry switch is only three branches deep.


---


### compiler : `gcc`
### title : `strlen of a known string in member array plus offset not folded`
### open_at : `2018-08-14T17:31:19Z`
### last_modified_date : `2021-11-27T10:00:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86955
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
GCC is able to fold three out of the four strlen calls in the test case below, even though folding all four should be possible.

$ cat f.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout f.c
char a[8];

struct A { char i, a[8]; };

void f0 (void)
{
  __builtin_strcpy (a, "123");

  if (__builtin_strlen (a) != 3)   // folded
    __builtin_abort ();
}

void f1 (void)
{
  __builtin_strcpy (a, "123");

  if (__builtin_strlen (a + 1) != 2)   // folded
    __builtin_abort ();
}

void f2 (struct A* p)
{
  __builtin_strcpy (p->a, "123");

  if (__builtin_strlen (p->a) != 3)   // folded
    __builtin_abort ();
}

void f3 (struct A* p)
{
  __builtin_strcpy (p->a, "123");

  if (__builtin_strlen (p->a + 1) != 2)   // not folded
    __builtin_abort ();
}


;; Function f0 (f0, funcdef_no=0, decl_uid=1910, cgraph_uid=1, symbol_order=1)

f0 ()
{
  <bb 2> [local count: 1073741825]:
  __builtin_memcpy (&a, "123", 4); [tail call]
  return;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=1913, cgraph_uid=2, symbol_order=2)

f1 ()
{
  <bb 2> [local count: 1073741825]:
  __builtin_memcpy (&a, "123", 4); [tail call]
  return;

}



;; Function f2 (f2, funcdef_no=2, decl_uid=1916, cgraph_uid=3, symbol_order=3)

f2 (struct A * p)
{
  char[8] * _1;

  <bb 2> [local count: 1073741825]:
  _1 = &p_3(D)->a;
  __builtin_memcpy (_1, "123", 4); [tail call]
  return;

}



;; Function f3 (f3, funcdef_no=3, decl_uid=1919, cgraph_uid=4, symbol_order=4)

f3 (struct A * p)
{
  char[8] * _1;
  const char * _2;
  long unsigned int _3;

  <bb 2> [local count: 1073741825]:
  _1 = &p_4(D)->a;
  __builtin_memcpy (_1, "123", 4);
  _2 = &MEM[(void *)p_4(D) + 2B];
  _3 = __builtin_strlen (_2);
  if (_3 != 2)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  return;

}


---


### compiler : `gcc`
### title : `nios2 optimizer forgets about known upper bits of register`
### open_at : `2018-08-15T17:02:11Z`
### last_modified_date : `2018-11-11T23:00:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86965
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
Created attachment 44545
source code that demonstrates my case

Target: nios2-elf
Configured with: ../gcc-8.2.0/configure \
 --target=nios2-elf \
 --prefix=/home/m/opt/cross \
 --disable-nls \
 --enable-languages=c,c++ \
 --without-headers \
 --enable-multiarch
Thread model: single
gcc version 8.2.0 (GCC) 


I am not sure if two cases in my report are related to each other. To me it looks like they are since both cases appear related to optimizer losing track of known state of upper 24 bits of register.

I know that  bug writing guidelines discourage inclusion of the content of the object file, but I made an effort to make it as short as possible. It really looks like the only sane way to report.

00000000 <good1>:
   0:	20c00007 	ldb	r3,0(r4)
   4:	18bff404 	addi	r2,r3,-48
   8:	28800015 	stw	r2,0(r5)
   c:	18801960 	cmpeqi	r2,r3,101
  10:	18c01120 	cmpeqi	r3,r3,68
  14:	10c4b03a 	or	r2,r2,r3
  18:	f800283a 	ret

0000001c <bad1>:
  1c:	20800003 	ldbu	r2,0(r4)
  20:	10c03fcc 	andi	r3,r2,255
  24:	18c0201c 	xori	r3,r3,128
  28:	18ffe004 	addi	r3,r3,-128
  2c:	18fff404 	addi	r3,r3,-48
  30:	108037cc 	andi	r2,r2,223
  34:	28c00015 	stw	r3,0(r5)
  38:	10801160 	cmpeqi	r2,r2,69
  3c:	f800283a 	ret

#-- comments:

bad1 should generate approximately the same code as good1. 
7 instructions instead of 9
Or, if compiler really wants to be smart, it can generate 6 instructions:
 ldb	r2,0(r4)
 addi	r3,r2,-48
 stw	r3,0(r5)
 andi	r2,r2,223 ; sign extension in bits[31..24] does not matter!
 cmpeqi	r2,r2,69
 ret

#-- end of comments:



00000040 <good2>:
  40: 	21800007 	ldb	r6,0(r4)
  44:	30c00b60 	cmpeqi	r3,r6,45
  48:	31800ae0 	cmpeqi	r6,r6,43
  4c:	28c00015 	stw	r3,0(r5)
  50:	1986b03a 	or	r3,r3,r6
  54:	20c5883a 	add	r2,r4,r3
  58:	f800283a 	ret

0000005c <bad2>:
  5c:	21800007 	ldb	r6,0(r4)
  60:	30c00b60 	cmpeqi	r3,r6,45
  64:	31800ae0 	cmpeqi	r6,r6,43
  68:	18803fcc 	andi	r2,r3,255 # this instruction serves no purpose
  6c:	1986b03a 	or	r3,r3,r6
  70:	28800115 	stw	r2,4(r5)
  74:	28800015 	stw	r2,0(r5)
  78:	20c5883a 	add	r2,r4,r3
  7c:	f800283a 	ret

#-- comments:

bad2 should be identical to good2 except of addition of the second store. 

#-- end of comments:


---


### compiler : `gcc`
### title : `Unaligned big-endian (scalar_storage_order) access on armv7-a yields 4 ldrb instructions rather than ldr+rev`
### open_at : `2018-08-16T01:54:51Z`
### last_modified_date : `2021-10-01T20:03:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86968
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
Created attachment 44547
C source code illustrating the problem

armv7-a support unaligned memory access. In case of unaligned little-endian access, gcc generates a single ldr instrction. Also, for aligned big-endian access, gcc generates an ldr followed by a rev instruction (reverses byte order).

However, when big-endian access is not aligned, gcc does not use ldr+rev. Instead, it generates 4 ldrb instructions plus the code to move the 4 bytes into a single register.

Find the source and generated assembler code attached.

My compiler command was
arm-none-eabi-gcc -O3 -mthumb -S -o - -march=armv7-a endian.c

The version of gcc is 8.2.0


---


### compiler : `gcc`
### title : `wrong peephole optimization applied on nios2 and mips targets`
### open_at : `2018-08-16T08:42:43Z`
### last_modified_date : `2020-01-27T23:31:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86975
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
On MIPS and Nios2 architectures logical instruction immediate (andi, ori) zero-extend immediate field. It means that on this targets small negative constants in logical expressions are more expensive than small (or not so small) positive constants. Peephole optimization phase should take it into account when selecting substitution patterns, but right now it's not smart enough.

Code:

int isE(int x) { return (x=='e') || (x=='E') ; }

RISV32 (-Os -rv32im) :
00000000 <isE>:
   0:   fdf57513                andi    a0,a0,-33
   4:   fbb50513                addi    a0,a0,-69
   8:   00153513                seqz    a0,a0
   c:   8082                    ret

That's good, because RISC-V sign-extends field in logical instructions which means that it has no problems with small integer constants.

MIPS ( -Os -mips32r6 -mcompact-branches=always) :
00000000 <isE>:
   0:   2402ffdf        li      v0,-33
   4:   00821024        and     v0,a0,v0
   8:   38420045        xori    v0,v0,0x45
   c:   2c420001        sltiu   v0,v0,1
  10:   d81f0000        jrc     ra

Nios2 (-Os) :
00000000 <isE>:
   0:   00bff7c4        movi    r2,-33
   4:   2084703a        and     r2,r4,r2
   8:   10801160        cmpeqi  r2,r2,69
   c:   f800283a        ret

That not so good. Both of them are sub-optimal. 
These targets should use substitution pattern based on small positive constant. Like that:

MIPS:
00000000 <smarter_isE>:
   0:   34820020        ori     v0,a0,0x20
   4:   38420065        xori    v0,v0,0x65
   8:   2c420001        sltiu   v0,v0,1
   c:   d81f0000        jrc     ra

Nios2:
00000000 <smarter_isE>:
   0:   20800814        ori     r2,r4,32
   4:   10801960        cmpeqi  r2,r2,101
   8:   f800283a        ret


---


### compiler : `gcc`
### title : `[9 regression] 64-bit gcc.target/i386/20040112-1.c FAILs`
### open_at : `2018-08-17T12:15:35Z`
### last_modified_date : `2021-12-09T16:52:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86994
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Between 20180816 (r263590) and 20180817 (r263613), gcc.target/i386/20040112-1.c
started to FAIL for 64-bit:

+FAIL: gcc.target/i386/20040112-1.c scan-assembler testb

Seen on i386-pc-solaris2.11 and x86_64-pc-linux-gnu.

Compared to the gcc-8 branch, the code changed from

-       testb   %al, %al
-       jns     .L2
+       cmpb    $-1, %al
+       jg      .L2


---


### compiler : `gcc`
### title : `missed FMA optimization with -fassociative-math`
### open_at : `2018-08-17T17:03:03Z`
### last_modified_date : `2021-09-06T18:37:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86999
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
Consider the following trivial example (https://godbolt.org/g/5ms6Bf):

  #include <limits.h>

  typedef float v4f __attribute__((vector_size(16)));
  typedef int v4i __attribute__((vector_size(16)));

  v4f foo(v4f n, v4f p)
  {
     return n * p + p;
  }

  template <int N> v4f __neg1(v4f a)
  {
    v4i v = {((N & 1) ? INT_MIN : 0), ((N & 2) ? INT_MIN : 0), ((N & 4) ? INT_MIN : 0), ((N & 8) ? INT_MIN : 0)};
    return __builtin_ia32_xorps(a, (v4f)v);
  }

  template <int N> v4f __neg2(v4f a)
  {
    v4i v = {((N & 1) ? INT_MIN : 0), ((N & 2) ? INT_MIN : 0), ((N & 4) ? INT_MIN : 0), ((N & 8) ? INT_MIN : 0)};
    return (v4f)((v4i)a ^ v);
  }

  v4f neg1C(v4f a)
  {
    return __neg1<0x0C>(a);
  }

  v4f neg2C(v4f a)
  {
    return __neg2<0x0C>(a);
  }

On GCC 7.x/8.x with -fno-signed-zeros (or implied by other flags eg.: -Ofast) foo() is not optimal on FMA capable hardware:

  foo(float __vector(4), float __vector(4)):
        vmulps  xmm0, xmm0, xmm1
        vaddps  xmm0, xmm0, xmm1
        ret

With -fsigned-zeros:

  foo(float __vector(4), float __vector(4)):
        vfmadd132ps     xmm0, xmm1, xmm1
        ret

Incorrect code is generated only on GCC 8.x with -fno-signed-zeros:

  neg1C(float __vector(4)):
        ret

With -fsigned-zeros or with GCC 7.x:
  
  neg1C(float __vector(4)):
        vxorps  xmm0, xmm0, XMMWORD PTR .LC1[rip]
        ret
  .LC1:
        .long   0
        .long   0
        .long   2147483648
        .long   2147483648

Note however when using bitwise xor instead of __builtin_ia32_xorps() the generated code is correct in all cases:

  neg2C(float __vector(4)):
        vxorps  xmm0, xmm0, XMMWORD PTR .LC1[rip]
        ret
  .LC1:
        .long   0
        .long   0
        .long   2147483648
        .long   2147483648


---


### compiler : `gcc`
### title : `[8 Regression] 10% slowdown with -march=skylake-avx512`
### open_at : `2018-08-18T15:23:42Z`
### last_modified_date : `2021-05-14T10:59:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87007
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
On Intel Skylake server, r262649 caused 10% slowdown for 538.imagick_r
in SPEC CPU 2017 when compiled with:

gcc -Ofast -march=skylake-avx512 -mfpmath=sse -fno-associative-math -funroll-loops -flto

For

[hjl@gnu-cfl-1 skx-2]$ cat foo.i
extern float f;
extern double d;
extern int i;

void
foo (void)
{
  d = f;
  f = i;
}

r262649 turned on sse_partial_reg_dependency, which generates

	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	f(%rip), %xmm0, %xmm0
	vmovsd	%xmm0, d(%rip)
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsi2ss	i(%rip), %xmm0, %xmm0
	vmovss	%xmm0, f(%rip)
	ret

instead of

	vcvtss2sd	f(%rip), %xmm0, %xmm0
	vmovsd	%xmm0, d(%rip)
	vcvtsi2ss	i(%rip), %xmm0, %xmm0
	vmovss	%xmm0, f(%rip)
	ret

One "vxorpd %xmm0, %xmm0, %xmm0" is necessary.  But both

vxorps	%xmm0, %xmm0, %xmm0

and

vxorps	%xmm0, %xmm0, %xmm0

are bad for performance.


---


### compiler : `gcc`
### title : `[8 Regression] gimple mem-to-mem assignment badly optimized`
### open_at : `2018-08-18T15:37:50Z`
### last_modified_date : `2019-04-17T15:54:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87008
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.1`
### severity : `normal`
### contents :
Created attachment 44554
preprocessed testcase

Original testcase posted at
https://listengine.tuxfamily.org/lists.tuxfamily.org/eigen/2018/08/msg00012.html

#include <Eigen/Dense>

using Vec = Eigen::Matrix<double, 2, 1>;

Vec f ()
{
  Vec sum = Vec::Zero();
  for (int i = 0; i < 1024; ++i)
  {
    const Vec dirA = sum;
    const Vec dirB = dirA;

    sum += dirA.dot(dirB) * dirA;
  }
  return sum;
}

compiled on x86_64 with eigen 3.3.4, -DEIGEN_DONT_VECTORIZE -O3 .

The .optimized dump contains

  MEM[(struct DenseStorage *)&dirA].m_data = MEM[(const struct DenseStorage &)sum_5(D)].m_data;
  dirA_18 = MEM[(struct plain_array *)&dirA];
  dirA$8_3 = MEM[(struct plain_array *)&dirA + 8B];
  MEM[(struct DenseStorage *)&dirB].m_data = MEM[(const struct DenseStorage &)&dirA].m_data;
  dirB_35 = MEM[(struct plain_array *)&dirB];
  dirB$8_48 = MEM[(struct plain_array *)&dirB + 8B];

which translates to

	movdqu	(%rax), %xmm1
	movaps	%xmm1, -40(%rsp)
	movsd	-40(%rsp), %xmm2
	movsd	-32(%rsp), %xmm0
	movaps	%xmm1, -24(%rsp)
	movsd	-16(%rsp), %xmm1
	movsd	-24(%rsp), %xmm5

This is clearly quite bad, we should for instance CSE dirA_18 and dirB_35. This is yet another case where gimple optimizers have a hard time handling mem-to-mem assignment. I think we have relevant code in vn_reference_lookup_3 (case 5 in particular). ESRA used to help.

I would also have expected better from RTL optimization, but that may be too optimistic.


---


### compiler : `gcc`
### title : `Can't find XOR pattern applying De Morgan sequentially`
### open_at : `2018-08-18T17:30:22Z`
### last_modified_date : `2023-09-03T17:59:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87009
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
Optimization: -Ofast

Suggested debugging layout: https://godbolt.org/g/ERWctt

int xor_int(int a, int b) {
...
}

Don't work:

int x = ~(a|b);
return ~(x|a)|~(x|b);

return ~(~(a|b)|a)|~(~(a|b)|b);

int x = ~a&~b;
return ~((x|a)&(x|b));

return ~(((~a&~b)|a)&((~a&~b)|b));

int x = ~a&~b;
return ~(x|a)|~(x|b);

return ~((~a&~b)|a)|~((~a&~b)|b);

int x = ~a&~b;
return (~x&~a)|(~x&~b);

int x = ~a&~b;
return ~x&(~a|~b);

Those two work:

return (~(~a&~b)&~a)|(~(~a&~b)&~b);

return ~(~a&~b)&(~a|~b);

A different calculation:
Don't work:

return ~(~(~a&b)&~(a&~b));

return ~(~(~a|~b)|~(a|b));

return ~((a&b)|~(a|b));

This works:

return ~(a&b)&(a|b);

Different calculation:

return ~((a|~b)&(~a|b));

return ~(a|~b)|~(~a|b);

return (~a&b)|~(~a|b);

This works:

return (~a&b)|(a&~b);

SUGGESTED SOLUTTION

At match.pd:774 There's the De Morgan law
for bitwise AND but there isn't one for OR.

https://github.com/gcc-mirror/gcc/blob/fe4311f/gcc/match.pd#L774

No one noticed this, because it can (somehow) transform ~(~a|b) to a&~b sometimes, (for example, if BMI instructions is enabled, it can use the 'andn' instruction which calculates a&~b) If I understand it correctly, it only works if it's not simplified a different way until RTL optimization comes.

/* ~(~a | b)  -->  a & ~b  */
(simplify
 (bit_not (bit_ior:cs (bit_not @0) @1))
 (bit_and @0 (bit_not @1)))


---


### compiler : `gcc`
### title : `[9 Regression] partially dead memset before strcpy not eliminated`
### open_at : `2018-08-18T21:54:40Z`
### last_modified_date : `2018-11-24T07:26:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87011
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
In released versions of GCC the memset call would be optimized by DSE to memset (_6, 0, 12).  On trunk, however, this is not done:

$ cat f.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout f.c
struct S { char a[4]; void (*pf)(void); };

void f (struct S *p)
{
  __builtin_memset (p, 0, sizeof *p);
  __builtin_strcpy (p->a, "123");
}
;; dump 228: optimized (enabled by -ftree-optimized)
;; Function f (f, funcdef_no=0, decl_uid=1910, cgraph_uid=1, symbol_order=0)

f (struct S * p)
{
  char[4] * _1;

  <bb 2> [local count: 1073741825]:
  __builtin_memset (p_3(D), 0, 16);
  _1 = &p_3(D)->a;
  __builtin_memcpy (_1, "123", 4); [tail call]
  return;
}


---


### compiler : `gcc`
### title : `[7/8/9 Regression] performance regression because of if-conversion`
### open_at : `2018-08-21T18:31:54Z`
### last_modified_date : `2019-11-06T13:54:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87047
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.0`
### severity : `normal`
### contents :
Created attachment 44570
demonstrate performance regression because of if-conversion

Very significant performance regression from gcc 6.x to 7.x and 8.x cause by if-conversion of predictable branch.

Compilation flags: -O2 -Wall
Target: x86-64 (my test machine is IvyBridge)
It is possible that the problem is specific to x86-64 target. I tested (by observing compiler output) aarch64 target and it looks o.k.

The problem occurs here:
    if ((i & 15)==0) {
      const uint64_t PROD_ONE = (uint64_t)(1) << 19;
      uint64_t prod = umulh(invRange, range);
      invRange = umulh(invRange, (PROD_ONE*2-1-prod)<<44)<<1;
    }

The condition has low probability and is easily predicted by branch predictor, while code within if has relatively high latency.
gcc, starting from  gcc.7.x and up to the latest, is convinced that always executing the inner part of the if is a bright idea. Measurements, on my real-world code, do not agree and show 30% slowdown. I'm sure that on artificial sequences I can demonstrate a slowdown of 100% and more.

What is special about this case is that compiler is VERY confident in its stupid decision. It does not change its mind even when I replace 
    if ((i & 15)==0) {
by
    if (__builtin_expect((i & 15)==0, 0)) {

I found only two ways of forcing sane code generation:
1. -fno-if-conversion
2.
    if ((i & 15)==0) {
      asm volatile("");
      ...
    }


---


### compiler : `gcc`
### title : `Unoptimal address calculation`
### open_at : `2018-08-22T07:27:56Z`
### last_modified_date : `2021-08-15T01:21:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87055
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
Following testcase:

--cut here--
int a[256];

int foo (int i)
{
        return a[i+1];
}
--cut here--

compiles on x86_64 (-O2) to:

        addl    $1, %edi
        movslq  %edi, %rdi
        movl    a(,%rdi,4), %eax
        ret

clang merges the addition with the address:

        movslq  %edi, %rax
        movl    a+4(,%rax,4), %eax
        retq

Related: PR54589


---


### compiler : `gcc`
### title : `Combine popcount on pieces to a single popcountll`
### open_at : `2018-08-23T11:00:54Z`
### last_modified_date : `2021-08-19T18:47:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87070
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
#include <bitset>
int f(unsigned long long x){ return std::bitset<64>(x).count(); }

should be a portable way to call a 64-bit popcount if it exists. Bug 63218 (bitset always uses long, never anything larger or smaller) sadly makes it harder. The optimizer could compensate, but it doesn't, not in the "too small" case (bug 83171), and not in the "too large" case, which is the topic of this report. What I get with gcc on win64:

  _13 = (long unsigned int) x_2(D);
  _14 = x_2(D) >> 32;
  _15 = (long unsigned int) _14;
  _22 = __builtin_popcountl (_13);
  _29 = __builtin_popcountl (_15);
  _12 = (unsigned int) _22;
  _3 = (unsigned int) _29;
  _9 = _3 + _12;
  _4 = (int) _9;
  return _4;

This dump is from gcc-7, but testing

int f(unsigned long long x){
  return __builtin_popcount(x)+__builtin_popcount(x>>32);
}

with trunk on linux shows it can't have changed much.

To be clear, this report is specifically about transforming the sum of popcount on the 2 halves of a number into a single bigger popcount. A match.pd transformation may be sufficient.


---


### compiler : `gcc`
### title : `missed optimization for horizontal add for x86 SSE`
### open_at : `2018-08-23T19:21:19Z`
### last_modified_date : `2021-08-03T07:09:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87077
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
During some experiments with toy programs I find out that GCC do not do any horizontal adding for xmm registers.

Some benchmark code:
http://quick-bench.com/HhZPnOtb9SYYK8z4IMKb_XAWYCI

If I'm not mistaken both function do same work and one hand written is faster.
And IIRC `_mm_hadd_ps` is consider a slow way to do this but is still faster than standard function.

Is my finding correct or I simply miss some important details why GCC do not do this?


---


### compiler : `gcc`
### title : `nios2 optimization for size - case of regression relatively to 5.3.0`
### open_at : `2018-08-23T20:41:46Z`
### last_modified_date : `2018-11-03T18:25:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87079
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
Created attachment 44586
5.3->8.3 regression - shifts instead of mulx

The issue was already reported together with another case that proved unrelated to nios2. The original thread is now dedicated to this more general issue, so I am repeating the reort of the first issue for convenience of tracking.

A case of significant regression in optimization for size between 5.3.0 (a version supplied in Altera Nios2 SDK) and 8.2.0.
Compiled with -Wall -Os -ffreestanding -fbuiltin -mhw-div -mhw-mul -mhw-mulx

It looks like -Os flag caused to compiler to forget that hw-mulx is present.

The problem appears unique to nios2 back end. Similar back ends (MIPS32r6 and RISCV32IM) are not affected.


---


### compiler : `gcc`
### title : `missed &, == optimization makes Emacs ~0.4% slower on x86-64`
### open_at : `2018-08-25T21:10:21Z`
### last_modified_date : `2023-06-25T18:57:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87104
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.1.1`
### severity : `enhancement`
### contents :
Created attachment 44595
In this file, f and g are equivalent, but f is not optimized as well as g

GNU Emacs uses the low-order three bits of its words to represent object type tags, and therefore contains a lot of code that (at the low level) looks like this:

   if ((x & 7) == 6)

when checking whether an object has type tag 6, say. On x86-64 gcc -O2 generates insns like this:

   movq %rax, %rbx
   andl $7, %ebx
   cmpq $6, %rbx

whereas the following would be smaller and faster:

   leal -6 (%rax), %ebx
   tstl $7, %ebx

Doing this to a test version of Emacs made it 0.4% faster overall, on my standard benchmark of Emacs byte-compiling all its Lisp files.

I'm attaching a source file fg.c that illustrates the problem. It defines a function f that uses the Emacs idiom and is poorly optimized compared to the equivalent function g. I also plan to attach the assembly language output fg.s, which shows that f has one more instruction than g.

I plan to tune Emacs so that it cajoles GCC into the better insns; however, GCC should optimize this code better, for the sake of programs other than Emacs.


---


### compiler : `gcc`
### title : `Autovectorization [X86, SSE2, AVX2, DoublePrecision]`
### open_at : `2018-08-25T22:58:42Z`
### last_modified_date : `2019-10-01T11:02:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87105
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `normal`
### contents :
GCC is unable to autovectorize the following code. It seems that it doesn't like min/max, but I'm not entirely sure. I stripped the code off my project so it's a bit longer, hope that's fine. I attached also a code compiled by clang, which is perfectly vectorized and what I would like to get from GCC.


The demonstration code
----------------------

#include <algorithm>
#include <cmath>
#include <stdint.h>

// Point structure [x, y]
struct Point {
  double x, y;

  inline Point() noexcept = default;
  constexpr Point(const Point&) noexcept = default;

  constexpr Point(double x, double y) noexcept
    : x(x), y(y) {}
};

// Box structure [x0, y0, x1, y1]
struct Box {
  double x0, y0, x1, y1;

  inline void reset(double x0, double y0, double x1, double y1) noexcept {
    this->x0 = x0;
    this->y0 = y0;
    this->x1 = x1;
    this->y1 = y1;
  }
};

// Overloads to make vector processing simpler.
static constexpr Point operator-(const Point& a) noexcept { return Point(-a.x, -a.y); }

static constexpr Point operator+(const Point& a, double b) noexcept
{ return Point(a.x + b, a.y + b); }
static constexpr Point operator-(const Point& a, double b) noexcept
{ return Point(a.x - b, a.y - b); }
static constexpr Point operator*(const Point& a, double b) noexcept
{ return Point(a.x * b, a.y * b); }
static constexpr Point operator/(const Point& a, double b) noexcept
{ return Point(a.x / b, a.y / b); }

static constexpr Point operator+(const Point& a, const Point& b) noexcept
{ return Point(a.x + b.x, a.y + b.y); }
static constexpr Point operator-(const Point& a, const Point& b) noexcept
{ return Point(a.x - b.x, a.y - b.y); }
static constexpr Point operator*(const Point& a, const Point& b) noexcept
{ return Point(a.x * b.x, a.y * b.y); }
static constexpr Point operator/(const Point& a, const Point& b) noexcept
{ return Point(a.x / b.x, a.y / b.y); }

static constexpr Point operator+(double a, const Point& b) noexcept
{ return Point(a + b.x, a + b.y); }
static constexpr Point operator-(double a, const Point& b) noexcept
{ return Point(a - b.x, a - b.y); }
static constexpr Point operator*(double a, const Point& b) noexcept
{ return Point(a * b.x, a * b.y); }
static constexpr Point operator/(double a, const Point& b) noexcept
{ return Point(a / b.x, a / b.y); }

// Min/Max - different semantics compared to std.
template<typename T> constexpr T myMin(const T& a, const T& b) noexcept
{ return b < a ? b : a; }
template<typename T> constexpr T myMax(const T& a, const T& b) noexcept
{ return a < b ? b : a; }

// Linear interpolation, works with points as well.
template<typename V, typename T = double>
inline V lerp(const V& a, const V& b, const T& t) noexcept {
  return (a * (1.0 - t)) + (b * t);
}

// Merge a point into a box by possibly increasing its bounds.
inline void boxMergePoint(Box& box, const Point& p) noexcept {
  box.x0 = myMin(box.x0, p.x);
  box.y0 = myMin(box.y0, p.y);
  box.x1 = myMax(box.x1, p.x);
  box.y1 = myMax(box.y1, p.y);
}

void quadBoundingBoxA(const Point bez[3], Box& bBox) noexcept {
  // Bounding box of start and end points.
  bBox.reset(myMin(bez[0].x, bez[2].x), myMin(bez[0].y, bez[2].y),
             myMax(bez[0].x, bez[2].x), myMax(bez[0].y, bez[2].y));

  Point t = (bez[0] - bez[1]) / (bez[0] - bez[1] * 2.0 + bez[2]);

  t.x = myMax(t.x, 0.0);
  t.y = myMax(t.y, 0.0);
  t.x = myMin(t.x, 1.0);
  t.y = myMin(t.y, 1.0);

  boxMergePoint(bBox, lerp(lerp(bez[0], bez[1], t),
                               lerp(bez[1], bez[2], t), t));
}




GCC Output [-std=c++17 -O3 -mavx2 -fno-math-errno]
--------------------------------------------------

quadBoundingBoxA(Point const*, Box&):
        push    rbp
        mov     rbp, rsp
        and     rsp, -32
        vmovsd  xmm1, QWORD PTR [rdi+8]
        vmovsd  xmm0, QWORD PTR [rdi]
        vmovsd  xmm5, QWORD PTR [rdi+40]
        vmovsd  xmm6, QWORD PTR [rdi+32]
        vmaxsd  xmm13, xmm5, xmm1
        vmaxsd  xmm12, xmm6, xmm0
        vminsd  xmm5, xmm5, xmm1
        vminsd  xmm6, xmm6, xmm0
        vunpcklpd       xmm0, xmm12, xmm13
        vunpcklpd       xmm1, xmm6, xmm5
        vmovups XMMWORD PTR [rsi+16], xmm0
        vmovups XMMWORD PTR [rsi], xmm1
        vmovsd  xmm2, QWORD PTR [rdi+24]
        vmovsd  xmm10, QWORD PTR [rdi+8]
        vmovsd  xmm1, QWORD PTR [rdi+40]
        vmovsd  xmm7, QWORD PTR [rdi+16]
        vaddsd  xmm4, xmm2, xmm2
        vsubsd  xmm9, xmm10, xmm2
        vmovsd  xmm3, QWORD PTR [rdi]
        vmovsd  xmm0, QWORD PTR [rdi+32]
        vsubsd  xmm8, xmm3, xmm7
        vsubsd  xmm4, xmm10, xmm4
        vaddsd  xmm4, xmm4, xmm1
        vdivsd  xmm9, xmm9, xmm4
        vaddsd  xmm4, xmm7, xmm7
        vsubsd  xmm4, xmm3, xmm4
        vaddsd  xmm4, xmm4, xmm0
        vdivsd  xmm8, xmm8, xmm4
        vxorpd  xmm4, xmm4, xmm4
        vcomisd xmm4, xmm8
        ja      .L6
        vcomisd xmm4, xmm9
        jbe     .L36
        vmovsd  xmm11, QWORD PTR .LC1[rip]
        vmulsd  xmm14, xmm1, xmm4
        vmulsd  xmm9, xmm2, xmm4
        vcomisd xmm8, xmm11
        jbe     .L37
        vmovsd  QWORD PTR [rsp-16], xmm2
        vmovapd xmm1, xmm14
        vmovapd xmm2, xmm9
        vxorpd  xmm14, xmm14, xmm14
        vmovsd  QWORD PTR [rsp-8], xmm7
        vmulsd  xmm3, xmm3, xmm4
        vmovapd xmm15, xmm11
        vmovapd xmm8, xmm11
        vmulsd  xmm7, xmm7, xmm4
        vxorpd  xmm9, xmm9, xmm9
        jmp     .L13
.L6:
        vmulsd  xmm11, xmm7, xmm4
        vcomisd xmm4, xmm9
        vxorpd  xmm8, xmm8, xmm8
        vmulsd  xmm0, xmm0, xmm4
        vmovsd  QWORD PTR [rsp-8], xmm11
        vmovsd  xmm11, QWORD PTR .LC1[rip]
        vmovapd xmm14, xmm11
        jbe     .L10
.L19:
        vmovsd  QWORD PTR [rsp-16], xmm2
        vmulsd  xmm1, xmm1, xmm4
        vmovapd xmm15, xmm11
        vxorpd  xmm9, xmm9, xmm9
        vmulsd  xmm2, xmm2, xmm4
        jmp     .L13
.L36:
        vmovsd  xmm11, QWORD PTR .LC1[rip]
        vcomisd xmm8, xmm11
        jbe     .L29
        vmovsd  QWORD PTR [rsp-8], xmm7
        vmulsd  xmm3, xmm3, xmm4
        vxorpd  xmm14, xmm14, xmm14
        vmovapd xmm8, xmm11
        vmulsd  xmm7, xmm7, xmm4
.L10:
        vcomisd xmm9, xmm11
        jbe     .L30
        vmulsd  xmm15, xmm2, xmm4
        vmovapd xmm9, xmm11
        vmulsd  xmm10, xmm10, xmm4
        vmovsd  QWORD PTR [rsp-16], xmm15
        vxorpd  xmm15, xmm15, xmm15
.L13:
        vaddsd  xmm1, xmm1, QWORD PTR [rsp-16]
        vaddsd  xmm3, xmm3, QWORD PTR [rsp-8]
        vaddsd  xmm2, xmm2, xmm10
        vaddsd  xmm0, xmm0, xmm7
        vmulsd  xmm9, xmm1, xmm9
        vmulsd  xmm15, xmm2, xmm15
        vmulsd  xmm8, xmm0, xmm8
        vmulsd  xmm14, xmm3, xmm14
        vaddsd  xmm9, xmm9, xmm15
        vaddsd  xmm14, xmm8, xmm14
        vminsd  xmm5, xmm9, xmm5
        vmaxsd  xmm9, xmm9, xmm13
        vminsd  xmm6, xmm14, xmm6
        vmaxsd  xmm14, xmm14, xmm12
        vmovsd  QWORD PTR [rsi+8], xmm5
        vmovsd  QWORD PTR [rsi+24], xmm9
        vmovsd  QWORD PTR [rsi], xmm6
        vmovsd  QWORD PTR [rsi+16], xmm14
        leave
        ret
.L29:
        vmulsd  xmm15, xmm7, xmm8
        vsubsd  xmm14, xmm11, xmm8
        vmulsd  xmm0, xmm0, xmm8
        vmulsd  xmm3, xmm3, xmm14
        vmulsd  xmm7, xmm7, xmm14
        vmovsd  QWORD PTR [rsp-8], xmm15
        jmp     .L10
.L37:
        vmulsd  xmm15, xmm7, xmm8
        vsubsd  xmm14, xmm11, xmm8
        vmulsd  xmm0, xmm0, xmm8
        vmulsd  xmm3, xmm3, xmm14
        vmulsd  xmm7, xmm7, xmm14
        vmovsd  QWORD PTR [rsp-8], xmm15
        jmp     .L19
.L30:
        vsubsd  xmm15, xmm11, xmm9
        vmulsd  xmm1, xmm1, xmm9
        vmulsd  xmm4, xmm2, xmm15
        vmulsd  xmm10, xmm10, xmm15
        vmulsd  xmm2, xmm2, xmm9
        vmovsd  QWORD PTR [rsp-16], xmm4
        jmp     .L13




Clang Output [-std=c++17 -O3 -mavx2 -fno-math-errno]
----------------------------------------------------

.LCPI0_0:
        .quad   4607182418800017408     # double 1
        .quad   4607182418800017408     # double 1
quadBoundingBoxA(Point const*, Box&):      # @quadBoundingBoxA(Point const*, Box&)
        vmovupd xmm0, xmmword ptr [rdi]
        vmovupd xmm1, xmmword ptr [rdi + 16]
        vmovupd xmm2, xmmword ptr [rdi + 32]
        vminpd  xmm3, xmm2, xmm0
        vmaxpd  xmm4, xmm2, xmm0
        vsubpd  xmm5, xmm0, xmm1
        vaddpd  xmm6, xmm1, xmm1
        vsubpd  xmm6, xmm0, xmm6
        vaddpd  xmm6, xmm2, xmm6
        vdivpd  xmm5, xmm5, xmm6
        vxorpd  xmm6, xmm6, xmm6
        vmaxpd  xmm5, xmm6, xmm5
        vmovapd xmm6, xmmword ptr [rip + .LCPI0_0] # xmm6 = [1.000000e+00,1.000000e+00]
        vminpd  xmm5, xmm6, xmm5
        vsubpd  xmm6, xmm6, xmm5
        vmulpd  xmm0, xmm0, xmm6
        vmulpd  xmm7, xmm1, xmm5
        vaddpd  xmm0, xmm7, xmm0
        vmulpd  xmm1, xmm1, xmm6
        vmulpd  xmm2, xmm2, xmm5
        vaddpd  xmm1, xmm2, xmm1
        vmulpd  xmm0, xmm6, xmm0
        vmulpd  xmm1, xmm5, xmm1
        vaddpd  xmm0, xmm0, xmm1
        vminpd  xmm1, xmm0, xmm3
        vmovupd xmmword ptr [rsi], xmm1
        vmaxpd  xmm0, xmm0, xmm4
        vmovupd xmmword ptr [rsi + 16], xmm0
        ret


---


### compiler : `gcc`
### title : `Group move and destruction of the source, where possible, for speed`
### open_at : `2018-08-26T10:23:08Z`
### last_modified_date : `2020-01-16T19:37:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87106
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.0`
### severity : `enhancement`
### contents :
Just a random testcase so I can give numbers, I don't claim this is a good testcase at all

#include <string>
#include <vector>

__attribute__((flatten))
void f(){
  int n = 1024*1024;
  std::vector<std::string> v(n);
  v.resize(n+1);
}
int main(){
  for(int i=0;i<256;++i) f();
}

runs in about 2.4s now. In _M_default_append, we have a first loop that copies (moves) strings from old to new, and a second loop that destroys old. If I comment out the destroying loop (not something we should do in general, this is just for the numbers), the running time goes down to 2.0s. If I replace the 2 loops with a single loop that does both move and destroy, the running time is now 1.6s. Move+destroy (aka destructive move, relocation, etc) are 2 operations that go well together and are not unlikely to simplify. Ideally the compiler would merge the 2 loops (loop fusion) for us, but it doesn't. Doing the operations in this order is only valid here because std::string can be moved+destroyed nothrow.

I think it would be nice to introduce a special case for nothrow-relocatable types in several functions for std::vector (_M_default_append is just one among several, and probably not the most important one). If that makes the code simpler, we could use if constexpr and limit the optimization to recent standards. If one of the relocation papers ever makes it through the committee, it will likely require this optimization (or at least make it an important QoI point).

There are probably places outside of vector that could also benefit, but vector looks like a good starting point.


---


### compiler : `gcc`
### title : `[9 regression]  gcc.dg/vect/costmodel/ppc/costmodel-vect-33.c fails starting with r263981`
### open_at : `2018-08-30T18:01:17Z`
### last_modified_date : `2018-11-27T19:25:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87157
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
On power 8/9 it fails like this:
> FAIL: gcc.dg/vect/costmodel/ppc/costmodel-vect-33.c scan-tree-dump-times vect "vectorized 1 loops" 1

On power 7 it fails like this:
> FAIL: gcc.dg/vect/costmodel/ppc/costmodel-vect-33.c scan-tree-dump-times vect "loop versioned for vectorization to enhance alignment" 1
> FAIL: gcc.dg/vect/costmodel/ppc/costmodel-vect-33.c scan-tree-dump-times vect "vectorized 1 loops" 1


Do the test cases just need updating?


make -k check-gcc RUNTESTFLAGS=ppc-costmodel-vect.exp=gcc.dg/vect/costmodel/ppc/costmodel-vect-33.c

# of expected passes		1
# of unexpected failures	1
FAIL: gcc.dg/vect/costmodel/ppc/costmodel-vect-33.c scan-tree-dump-times vect "vectorized 1 loops" 1


r263981 | hubicka | 2018-08-30 07:58:42 -0500 (Thu, 30 Aug 2018) | 8 lines


	* sreal.h (SREAL_PART_BITS): Change to 31; remove seemingly unnecessary
	comment that it has to be even number.
	(class sreal): Change m_sig type to int32_t.
	* sreal.c (sreal::dump, sreal::to_int, opreator+, operator-): Use
	int64_t for temporary calculations.
	(sreal_verify_basics): Drop one bit from minimum and maximum.


---


### compiler : `gcc`
### title : `Suboptimal code generation for __atomic_compare_exchange_n followed by a comparison`
### open_at : `2018-09-03T16:16:17Z`
### last_modified_date : `2021-12-21T11:28:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87206
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `enhancement`
### contents :
I tried to build the example #5 from https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80080 on x86_64 and observed a similar issue:

$ cat 1.c
extern void bar (int *);

void foo5(int *mem)
{
  int oldval = 0;
  __atomic_compare_exchange_n (mem, (void *) &oldval, 1,
                               1, __ATOMIC_ACQUIRE, __ATOMIC_RELAXED);
  if (oldval != 0)
    bar (mem);
}

$ gcc-8 -c 1.c -O3 -g

$ objdump -d 1.o
# skip
0000000000000000 <_foo5>:
   0:	31 c0                	xor    %eax,%eax
   2:	ba 01 00 00 00       	mov    $0x1,%edx
   7:	f0 0f b1 17          	lock cmpxchg %edx,(%rdi)
   b:	85 c0                	test   %eax,%eax
   d:	75 01                	jne    10 <_foo5+0x10>
   f:	c3                   	retq
  10:	e9 00 00 00 00       	jmpq   15 <_foo5+0x15>

We don't have to do "test %eax,%eax", because this information is already available through ZF, which is set by CMPXCHG.

I wonder if it would be possible to come up with a common solution for all architectures, including x86_64 and s390?


---


### compiler : `gcc`
### title : `Wuninitialized or Wmaybe-uninitialized doesn't warn when malloc's return value is used without being initialized`
### open_at : `2018-09-03T19:24:53Z`
### last_modified_date : `2020-08-11T16:47:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87209
### status : `RESOLVED`
### tags : `alias, diagnostic, missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
There's no warnings emitted for the following test-case:

int f(void)
{
  int *p = __builtin_malloc (sizeof (*p));
  return *p;
}

I assume this should have been diagnosed with Wuninitialized or Wmaybe-uninitialized ?

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `-Os produces sub-optimal x86 machine code for initialization with zero`
### open_at : `2018-09-05T07:52:10Z`
### last_modified_date : `2021-12-21T10:57:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87223
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `enhancement`
### contents :
My understanding of -Os is that it is aimed at reducing code size. It produces code with suboptimal size for the constructor below (and similar cases where a constructor needs to initialize lots of members to zero):

struct S {
  char a = 0;
  void * b = 0;
  short c = 0;
  long d = 0;

  S();
};

S::S() = default;

Generated code on x86-64 using "g++ -Os -S x.cc":

        movb    $0, (%rdi)
        movq    $0, 8(%rdi)
        movw    $0, 16(%rdi)
        movq    $0, 24(%rdi)
        ret

It would be more efficient space-wise to first zero a register with "xor %eax, %eax" (should implicitly zero all of %rax) and then use %rax or a sub-register thereof as the source for the moves. This avoids putting 0 as constants into the machine instructions over and over again, enlarging their size (and, due to their size, possibly clogging the CPU's instruction decoder).


---


### compiler : `gcc`
### title : `Redundant Restore of $x0 when memcpy always returns the first argument.`
### open_at : `2018-09-05T21:07:45Z`
### last_modified_date : `2021-08-22T08:47:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87238
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `7.3.0`
### severity : `enhancement`
### contents :
$ cat test.cpp

struct BigStruct {
  int x[64];
};

void structByValue(BigStruct s);

void callStructByValue(int unused, int unused2, BigStruct s) {
  structByValue(s);
}

$ g++ -O3 -arch arm64 test.cpp -S -o -

callStructByValue(int, int, BigStruct):
  stp x29, x30, [sp, -272]!
  mov x1, x2
  mov x2, 256
  add x29, sp, 0
  add x0, x29, 16 <<-------- 
  bl memcpy
  add x0, x29, 16 <<-------- redundant
  bl structByValue(BigStruct)
  ldp x29, x30, [sp], 272
  ret


We could just do remove the second 'add x0, x29, 16' as memcpy is guaranteed to return the pointer to desination. http://man7.org/linux/man-pages/man3/memcpy.3.html


Possibly duplicate of PR82991 but not sure.


---


### compiler : `gcc`
### title : `[missed optimization] switching on indices of struct members`
### open_at : `2018-09-07T01:32:27Z`
### last_modified_date : `2021-10-25T12:45:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87245
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Consider a simple struct whose members can be accessed by name or by index:

  struct vec {double x,y,z;};

  double get(const struct vec *v,unsigned long i) {
    switch(i) {
      case 0: return v->x;
      case 1: return v->y;
      case 2: return v->z;
    }
  }

Godbolt's trunk x86_64 build produces at -O3

  get:
    cmpq $1, %rsi
    ja .L2
    je .L11
    movsd (%rdi), %xmm0
    ret
  .L2:
    cmpq $2, %rsi
    jne .L12
    movsd 16(%rdi), %xmm0
    ret
  .L11:
    movsd 8(%rdi), %xmm0
    ret
  .L12:
    ret

when of course

  get:
    movsd (%rdi,%rsi,8), %xmm0
    ret

would suffice.  (Apologies if I picked the wrong -optimization component.)

Bonus points for collapsing

  double get(const struct vec *arr,unsigned long i) {
    arr+=i/3;
    switch(i%3) {
      case 0: return arr->x;
      case 1: return arr->y;
      case 2: return arr->z;
    }
  }

to the same two instructions.


---


### compiler : `gcc`
### title : `Move signed (x % pow2) == 0 optimization to gimple`
### open_at : `2018-09-12T14:31:45Z`
### last_modified_date : `2021-08-18T05:37:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87287
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
As can be seen on:

void f0 (void);

int
f1 (int x)
{
  return x % 16 == 0;
}

int
f2 (int x)
{
  int y = x % 16;
  return y != 0;
}

void
f3 (int x)
{
  if (x % 16 != 0)
    f0 ();
}

void
f4 (int x)
{
  int y = x % 16;
  if (y == 0)
    f0 ();
}

we perform this optimization in fold-const.c only, it should be moved to match.pd.


---


### compiler : `gcc`
### title : `attribute malloc not used for alias analysis when it could be`
### open_at : `2018-09-14T21:04:51Z`
### last_modified_date : `2020-08-11T17:28:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87313
### status : `REOPENED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Attribute malloc is documented as:

    This tells the compiler that a function is malloc-like, i.e., that the pointer P returned by the function cannot alias any other pointer valid when the function returns, and moreover no pointers to valid objects occur in any storage addressed by P.

The test case below shows that although GCC takes advantage of this property to eliminate impossible tests when calling __builtin_malloc it doesn't do the same when calling a user-defined function declared with the attribute.

$ cat x.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout x.c
void f (int **p)
{
  int *x = *p;

  int **q = __builtin_malloc (sizeof (int*));
  *q = 0;   // *q cannot be equal to *p prior to the assignment

  if (x != *p)   // folded to false
    __builtin_abort ();
}

__attribute__ ((malloc)) void* g (int);

void h (int **p)
{
  int *x = *p;

  int **q = g (sizeof (int*));
  *q = 0;   // *q cannot be equal to *p prior to the assignment

  if (x != *p)   // not folded
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=0)

f (int * * p)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function h (h, funcdef_no=1, decl_uid=1913, cgraph_uid=2, symbol_order=1)

h (int * * p)
{
  int * x;
  int * _1;

  <bb 2> [local count: 1073741824]:
  x_4 = *p_3(D);
  g (8);
  _1 = *p_3(D);
  if (_1 != x_4)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312328]:
  return;

}


---


### compiler : `gcc`
### title : `pointless comparison of malloc result to a string not eliminated`
### open_at : `2018-09-14T21:19:38Z`
### last_modified_date : `2019-05-03T11:32:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87314
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC fails to eliminate the impossible test in the test case below (despite the warning).  Clang eliminates it as expected.

$ cat x.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout x.c
void f (void)
{
  char *p = __builtin_malloc (8);

  if (p == "")
    __builtin_abort ();
}
x.c: In function ‘f’:
x.c:5:9: warning: comparison with string literal results in unspecified behavior [-Waddress]
5 |   if (p == "")
  |         ^~

;; Function f (f, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=0)

f ()
{
  char * p;

  <bb 2> [local count: 1073741824]:
  p_3 = __builtin_malloc (8);
  if (p_3 == "")
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312328]:
  return;

}


---


### compiler : `gcc`
### title : `Missed optimisation: merging VMOVQ with operations that only use the low 8 bytes`
### open_at : `2018-09-15T05:26:43Z`
### last_modified_date : `2018-11-21T13:21:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87317
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
Test:

#include <immintrin.h>

int f(void *ptr)
{
    __m128i data = _mm_loadl_epi64((__m128i *)ptr);
    data = _mm_cvtepu8_epi16(data);
    return _mm_cvtsi128_si32(data);
}

GCC generates (-march=haswell or -march=skylake):

        vmovq   (%rdi), %xmm0
        vpmovzxbw       %xmm0, %xmm0
        vmovd   %xmm0, %eax
        ret

Note that the VPMOVZXBW instruction only reads the low 8 bytes from the source, including if it is a memory reference. Both Clang and ICC generate:

        vpmovzxbw       (%rdi), %xmm0
        vmovd   %xmm0, %eax
        retq

Similarly for:

void f(void *dst, void *ptr)
{
    __m128i data = _mm_cvtsi32_si128(*(int*)ptr);
    data = _mm_cvtepu8_epi32(data);
    _mm_storeu_si128((__m128i*)dst, data);
}

GCC:

        vmovd   (%rsi), %xmm0
        vpmovzxbd       %xmm0, %xmm0
        vmovups %xmm0, (%rdi)
        ret

Clang and ICC:

        vpmovzxbd       (%rsi), %xmm0
        vmovdqu %xmm0, (%rdi)
        retq

There are other instructions that might benefit from this.

AVX-512 memory instructions where the OpMask is a constant might be candidates too.


---


### compiler : `gcc`
### title : `When vector is wrapped, expression is not optimized.`
### open_at : `2018-09-15T13:43:23Z`
### last_modified_date : `2021-07-21T01:45:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87319
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
I was playing with vector extensions and intrinsics, checking if gcc would be able to optimize vector expression a*c+b*c to (a+b)*c. It turned out that this works for intrinsics (both wrapped in class and non-wrapped), and vector extensions (non-wrapped only). When built-in operators for vector extensions were used, code was not optimized (test3). Code was compiled with "-O3 -mavx2 -std=c++11".

[code]
#include <stdint.h>
#include <immintrin.h>

typedef int32_t VInt __attribute((vector_size(32)));

class V1
{
    VInt v;
public:
    constexpr V1(const V1& v) : v(v.v) {}
    constexpr V1(const VInt& v) : v(v) {}

    constexpr V1 operator+(const V1& v2) const
    { return V1(v + v2.v); }

    constexpr V1 operator*(const V1& v2) const
    { return V1(v * v2.v); }

    constexpr operator VInt() const
    { return v; }
};

class V2
{
    __m256i v;
public:
    constexpr V2(const V2& v) : v(v.v) {}
    constexpr V2(const __m256i& v) : v(v) {}

    V2 operator+(const V2& v2) const
    { return V2(_mm256_add_epi32(v, v2.v)); }

    V2 operator*(const V2& v2) const
    { return V2(_mm256_mullo_epi32(v, v2.v)); }

    constexpr operator __m256i() const
    { return v; }
};

void test1(const int* a, const int* b, const int* c, int* d)
{
    const VInt va = *(VInt*)a;
    const VInt vb = *(VInt*)b;
    const VInt vc = *(VInt*)c;
    *(VInt*)d = va * vc + vb * vc;
}

void test2(const int* a, const int* b, const int* c, int* d)
{
    const __m256i va = *(__m256i*)a;
    const __m256i vb = *(__m256i*)b;
    const __m256i vc = *(__m256i*)c;
    const __m256i vd =_mm256_add_epi32(
        _mm256_mullo_epi32(va, vc),
        _mm256_mullo_epi32(vb, vc)
    );
    *(__m256i*)d = vd;
}

void test3(const int* a, const int* b, const int* c, int* d)
{
    const V1 va = V1(*(VInt*)a);
    const V1 vb = V1(*(VInt*)b);
    const V1 vc = V1(*(VInt*)c);
    *(VInt*)d = va * vc + vb * vc;
}

void test4(const int* a, const int* b, const int* c, int* d)
{
    const V2 va(*(__m256i*)a);
    const V2 vb(*(__m256i*)b);
    const V2 vc(*(__m256i*)c);
    *(__m256i*)d = va * vc + vb * vc;
}
[/code]

[out]
test1(int const*, int const*, int const*, int*):
  vmovdqa ymm0, YMMWORD PTR [rdi]
  vpaddd ymm0, ymm0, YMMWORD PTR [rsi]
  vpmulld ymm0, ymm0, YMMWORD PTR [rdx]
  vmovdqa YMMWORD PTR [rcx], ymm0
  vzeroupper
  ret
test2(int const*, int const*, int const*, int*):
  vmovdqa ymm0, YMMWORD PTR [rdi]
  vpaddd ymm0, ymm0, YMMWORD PTR [rsi]
  vpmulld ymm0, ymm0, YMMWORD PTR [rdx]
  vmovdqa YMMWORD PTR [rcx], ymm0
  vzeroupper
  ret
test3(int const*, int const*, int const*, int*):
  vmovdqa ymm0, YMMWORD PTR [rdx]
  vpmulld ymm1, ymm0, YMMWORD PTR [rdi]
  vpmulld ymm0, ymm0, YMMWORD PTR [rsi]
  vpaddd ymm0, ymm1, ymm0
  vmovdqa YMMWORD PTR [rcx], ymm0
  vzeroupper
  ret
test4(int const*, int const*, int const*, int*):
  vmovdqa ymm0, YMMWORD PTR [rdi]
  vpaddd ymm0, ymm0, YMMWORD PTR [rsi]
  vpmulld ymm0, ymm0, YMMWORD PTR [rdx]
  vmovdqa YMMWORD PTR [rcx], ymm0
  vzeroupper
  ret
[/out]


---


### compiler : `gcc`
### title : `missed comparison optimizations (grep DFA, x86-64)`
### open_at : `2018-09-18T18:40:57Z`
### last_modified_date : `2023-08-19T21:35:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87355
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.1`
### severity : `enhancement`
### contents :
Created attachment 44720
source code illustrating missed optimizations

I found this when attempting to tune grep's DFA code on x86-64, and simplified the issue to the attached source code t.c which defines two functions f and g that are logically equivalent, and which can both be implemented via a single machine-language comparison to THRESHOLD. However, GCC generates two comparisons for f and three comparisons for g, as shown in the attached assembly-language file t.s generated by 'gcc -O2 -S t.c'. I am running Fedora 28 x86-64 with 8.1.1 20180712 (Red Hat 8.1.1-5).

I'm not sure whether this problem is limited to x86-64 or is more general, and for now am labeling its component as 'target'.


---


### compiler : `gcc`
### title : `[7/8/9 Regression] Inefficient return code of struct values`
### open_at : `2018-09-20T17:16:04Z`
### last_modified_date : `2018-12-14T12:41:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87370
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Test case: https://gcc.godbolt.org/z/58JsxE

```
struct A
{
    int b[4];
};
struct B
{
    char a[12];
    int b;
};
struct C
{
    char a[16];
};

A f1(int i)
{
    return { };
}

B f2(int i)
{
    return { };
}

C f3(int i)
{
    return { };
}
```

On x86_64 it create assembly:
```
f1(int):
  xor eax, eax
  xor edx, edx
  ret
f2(int):
  pxor xmm0, xmm0
  xor eax, eax
  movaps XMMWORD PTR [rsp-24], xmm0
  mov rdx, QWORD PTR [rsp-16]
  ret
f3(int):
  xor eax, eax
  xor edx, edx
  ret
```

Clang and GCC 6.3 generate same code for every function functions.


---


### compiler : `gcc`
### title : `[7 Regression] G++ produces >300MB .rodata section to initialize struct with big array`
### open_at : `2018-09-25T18:09:28Z`
### last_modified_date : `2019-11-14T11:32:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87436
### status : `RESOLVED`
### tags : `compile-time-hog, memory-hog, missed-optimization`
### component : `c++`
### version : `8.2.1`
### severity : `normal`
### contents :
Created attachment 44749
test code

I'm on Linux x86_64, more specifically XUbuntu 16.04, and use GCC version "Ubuntu 5.4.0-6ubuntu1~16.04.10", so I selected Version 5.4.0
I think this bug still exists in the latest GCC release, but I was only able to test that on gcc.godbolt.org (there it produces either a timeout or out of memory error). It also seems like the bug was introduced between 4.9.x and 5.1

Anyway: I have a simple struct with a few fields that are all have default values, and another struct containing a huge (16mio elements) array of the aforementioned type (see attachment for test code).
I built it with "g++ -std=c++11 -c -o testsize.o testsize.cpp" - it takes over a minute to build, cc1plus uses over 4GB of main memory in the process and it produces a 385MB .o file.
Inspecting that .o shows that the .rodata section is really big, and its contents are used for the constructor of the struct containing the array - it just memcpy()s that massive blob of data into the new object..

Now in general this certainly is a clever optimization, but I don't think it's a good idea if the object is this big.
Furthermore, it even happens with -0s builds and I guess even for saner object sizes this kind of optimization is not what you'd want in size-optimized builds?

This might be related to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82294 - however I opened a new bug because that report was about C++17 with constexpr, while my case is C++11.

Godbolt link: https://gcc.godbolt.org/z/Cseqhi


---


### compiler : `gcc`
### title : `sse_packed_single_insn_optimal is suboptimal on Zen`
### open_at : `2018-09-27T15:57:34Z`
### last_modified_date : `2020-01-04T08:30:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87455
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
GCC by default enables -mtune-ctrl=sse_packed_single_insn_optimal on -mtune=znver1, even though that microarchitecture doesn't like it for the same reason Intel's microarchitectures don't: there's additional latency for domain crossing operations, using e.g. xorps for integer data costs one cycle more than using pxor.

Example code:

#include <immintrin.h>

int main() {
    auto x = _mm_setr_epi32(1, 2, 3, 4);
    auto y = _mm_setr_epi32(5, 6, 7, 8);
    auto z = _mm_setr_epi32(9, 10, 11, 12);

    for(int i = 0; i < 1000000000; ++i) {
        x = _mm_add_epi32(x, y);
        y = _mm_xor_si128(y, z);
        z = _mm_add_epi32(z, x);
        x = _mm_xor_si128(x, y);
        y = _mm_add_epi32(y, z);
        z = _mm_xor_si128(z, x);
    }

    asm volatile("" :: "m"(x), "m"(y), "m"(z));
}

Compiled with GCC 8.2, with -O3 -mtune=znver1 running it yields the following perf counters:

$ perf stat -e task-clock,cycles,instructions ./a.out

 Performance counter stats for './a.out':

          1 193,69 msec task-clock:u              #    0,989 CPUs utilized          
     4 040 330 384      cycles:u                  # 3386697,723 GHz                 
    10 002 005 027      instructions:u            #    2,48  insn per cycle                                            

       1,206801245 seconds time elapsed

       1,190625000 seconds user
       0,003995000 seconds sys

However, the code compiled with -O3 -mtune=znver1 -mtune-ctrl=^sse_packed_single_insn_optimal is significantly faster:

$ perf stat -e task-clock,cycles,instructions ./a.out

 Performance counter stats for './a.out':

            894,08 msec task-clock:u              #    0,998 CPUs utilized          
     3 012 492 242      cycles:u                  # 3369678,123 GHz                 
    10 002 004 492      instructions:u            #    3,32  insn per cycle                                            

       0,895728255 seconds time elapsed

       0,894688000 seconds user
       0,000000000 seconds sys

This is on a Ryzen 5 2500U.


---


### compiler : `gcc`
### title : `Poor code generation for std::string("c-style string")`
### open_at : `2018-10-03T14:34:23Z`
### last_modified_date : `2022-01-07T02:20:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87502
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `8.2.0`
### severity : `normal`
### contents :
Created attachment 44776
Preprocessed source code

It appears that gcc is creating quite poor code when "c-style strings"
are used to construct std::string objects.  Ideally, the result ought
to be just a few move instructions for small strings.


Host: Linux x86_64 4.4.140-62-default (OpenSuSE)

Test code:
---------------------------------------------------------------
#include <string>

extern void foo (const std::string &);

void
bar ()
{
  foo ("abc");
  foo (std::string("abc"));
}
---------------------------------------------------------------



# /usr/local/products/gcc/8.2.0/bin/g++ -std=gnu++1z  -S -m32 -O3 ttt.C
# grep 'call.*construct' ttt.s 
	call	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE12_M_constructIPKcEEvT_S8_St20forward_iterator_tag.constprop.18
	call	_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE12_M_constructIPKcEEvT_S8_St20forward_iterator_tag.constprop.18

Here gcc generates complete calls to the generic string construction
even though the strings are constructed from small, known strings.

"-std=gnu++1z" is important; "-m32" and "-O3" (as opposed to "-m64" and
"-O2") are not.

# /usr/local/products/gcc/8.2.0/bin/g++ -S -m32 -O3 ttt.C
# grep 'call.*construct' ttt.s
# (nada)

No calls -- good.  In this case gcc generates this fragment:

_Z3barv:
.LFB1084:
	.cfi_startproc
	.cfi_personality 0,__gxx_personality_v0
	.cfi_lsda 0,.LLSDA1084
	pushl	%ebp
	.cfi_def_cfa_offset 8
	.cfi_offset 5, -8
	movl	$25185, %edx
	movl	%esp, %ebp
	.cfi_def_cfa_register 5
	pushl	%edi
	pushl	%esi
	.cfi_offset 7, -12
	.cfi_offset 6, -16
	leal	-48(%ebp), %esi
	pushl	%ebx
	.cfi_offset 3, -20
	leal	-40(%ebp), %ebx
	subl	$56, %esp
	movl	%ebx, -48(%ebp)
	pushl	%esi
	movw	%dx, -40(%ebp)
	movb	$99, -38(%ebp)
	movl	$3, -44(%ebp)
	movb	$0, -37(%ebp)
.LEHB6:
	.cfi_escape 0x2e,0x10
	call	_Z3fooRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
[...]

This is better than a call, but not great:
1. The string is moved into position in three chunks (25185, 99, 0).
   This probably comes from inlined memcpy of 3 bytes, but the source
   is zero-terminated so rounding the memcpy size up to 4 would have
   been better.
2. It's unclear why 25185 is passed through a register.


---


### compiler : `gcc`
### title : `IRA unnecessarily uses non-volatile registers during register assignment`
### open_at : `2018-10-03T18:25:29Z`
### last_modified_date : `2018-11-14T02:22:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87507
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Current trunk and GCC 7 (haven't tested anything else) unnecessarily use non-volatile regs forcing unneeded save/restore code, along with unnecessary reg moves.  Using a different 128-bit type (ie, long double, etc.) for the struct field types, I see the code I would expect to see.

I'm guessing this is a target issue (insn constraints?), so setting the Component to that for now.


bergner@pike:~$ cat bug.c
typedef struct
{
  __int128_t i0;
  __int128_t i1;
} i2_t;

void
foo (long cond, i2_t *dst, __int128_t  src)
{
  if (cond)
  {
    dst->i0 = src;
    dst->i1 = src;
  }
}
bergner@pike:~$ .../xgcc -B... -O2 -S bug.c
bergner@pike:~$ cat bug2.s 
	.file	"bug.c"
	.abiversion 2
	.section	".text"
	.align 2
	.p2align 4,,15
	.globl foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	cmpdi 7,3,0
	beqlr 7
	std 30,-16(1)
	std 31,-8(1)
	.cfi_offset 30, -16
	.cfi_offset 31, -8
	mr 30,6
	mr 31,5
	addi 9,4,16
	mr 10,30
	mr 11,31
	std 31,0(4)
	std 30,8(4)
	std 11,0(9)
	std 10,8(9)
	ld 30,-16(1)
	ld 31,-8(1)
	.cfi_restore 31
	.cfi_restore 30
	blr
	.long 0
	.byte 0,0,0,0,0,2,0,0
	.cfi_endproc
.LFE0:
	.size	foo,.-foo
	.ident	"GCC: (GNU) 9.0.0 20181002 (experimental) [trunk revision 264800]"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `std::make_shared could avoid virtual call to _Sp_counted_ptr_inplace::_M_get_deleter()`
### open_at : `2018-10-04T11:00:26Z`
### last_modified_date : `2019-01-21T13:29:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87514
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.0`
### severity : `enhancement`
### contents :
In __shared_ptr(_Sp_make_shared_tag, const _Alloc&, _Args&&...) we do:

	  // _M_ptr needs to point to the newly constructed object.
	  // This relies on _Sp_counted_ptr_inplace::_M_get_deleter.
#if __cpp_rtti
	  void* __p = _M_refcount._M_get_deleter(typeid(__tag));
#else
	  void* __p = _M_refcount._M_get_deleter(_Sp_make_shared_tag::_S_ti());
#endif

_M_refcount._M_get_deleter does:

      void*
      _M_get_deleter(const std::type_info& __ti) const noexcept
      { return _M_pi ? _M_pi->_M_get_deleter(__ti) : nullptr; }

which does an indirect virtual call _Sp_counted_ptr_inplace::_M_get_deleter.

In the __shared_ptr(_Sp_make_shared_tag ...) constructor we know the dynamic type of *_M_refcount._M_pi is definitely _Sp_counted_ptr_inplace and so could theoretically avoid the dynamic dispatch.

This would require breaking encapsulation to allow __shared_ptr to access the private __shared_count::_M_pi member. Is it worth it?

It's possible this can already be devirtualized by the compiler, but only when optimising. If it isn't, then this might help:

@@ -653,9 +652,8 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
          typedef _Sp_counted_ptr_inplace<_Tp, _Alloc, _Lp> _Sp_cp_type;
          typename _Sp_cp_type::__allocator_type __a2(__a);
          auto __guard = std::__allocate_guarded(__a2);
-         _Sp_cp_type* __mem = __guard.get();
-         ::new (__mem) _Sp_cp_type(__a, std::forward<_Args>(__args)...);
-         _M_pi = __mem;
+         _M_pi = ::new (__guard.get())
+           _Sp_cp_type(__a, std::forward<_Args>(__args)...);
          __guard = nullptr;
        }


---


### compiler : `gcc`
### title : `Popcount changes caused 531.deepsjeng_r run-time regression on Skylake`
### open_at : `2018-10-05T13:42:31Z`
### last_modified_date : `2021-12-22T10:08:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87528
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
According to my repeated measurements, r262486 and r262864 caused ~14%
regression (roughly 7% and 7% each) in run-time of SPEC 2017
531.deepsjeng_r, with generic tuning (only), both at -O2 and -Ofast,
on an Intel Skylake machine (Intel Xeon Platinum 8164 CPU).

Martin Liška could not reproduce this on his Kabylake machine, so I'd
be very grateful if someone else could attempt to reproduce this.
Having said that, I really can reproduce the regression very reliably.


---


### compiler : `gcc`
### title : `Redundant vmovaps`
### open_at : `2018-10-05T21:06:15Z`
### last_modified_date : `2020-06-19T12:42:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87537
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-skx-1 gcc]$ cat /export/gnu/import/git/sources/gcc/gcc/testsuite/gcc.target/i386/avx2-vbroadcastss_ps256-1.c
/* { dg-do compile } */
/* { dg-options "-mavx2 -O2" } */
/* { dg-final { scan-assembler "vbroadcastss\[ \\t\]+\[^\n\]*%xmm\[0-9\]" } } */

#include <immintrin.h>

__m128 x;
__m256 y;

void extern
avx2_test (void)
{
  y = _mm256_broadcastss_ps (x);
}
[hjl@gnu-skx-1 gcc]$ ./xgcc -B./ -S /export/gnu/import/git/sources/gcc/gcc/testsuite/gcc.target/i386/avx2-vbroadcastss_ps256-1.c -mavx2 -O2
cat[hjl@gnu-skx-1 gcc]$ cat avx2-vbroadcastss_ps256-1.s 
	.file	"avx2-vbroadcastss_ps256-1.c"
	.text
	.p2align 4
	.globl	avx2_test
	.type	avx2_test, @function
avx2_test:
.LFB5178:
	.cfi_startproc
	vmovaps	x(%rip), %xmm1
	vbroadcastss	%xmm1, %ymm0
	vmovaps	%ymm0, y(%rip)
	vzeroupper
	ret
	.cfi_endproc
.LFE5178:
	.size	avx2_test, .-avx2_test
	.comm	y,32,32
	.comm	x,16,16
	.ident	"GCC: (GNU) 9.0.0 20180901 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-skx-1 gcc]$ 

We should generate

[hjl@gnu-cfl-1 gcc]$ cat avx2-vbroadcastss_ps256-1.s
	.file	"avx2-vbroadcastss_ps256-1.c"
	.text
	.p2align 4
	.globl	avx2_test
	.type	avx2_test, @function
avx2_test:
.LFB5178:
	.cfi_startproc
	vbroadcastss	x(%rip), %ymm0
	vmovaps	%ymm0, y(%rip)
	vzeroupper
	ret
	.cfi_endproc
.LFE5178:
	.size	avx2_test, .-avx2_test
	.comm	y,32,32
	.comm	x,16,16
	.ident	"GCC: (GNU) 9.0.0 20181005 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 gcc]$


---


### compiler : `gcc`
### title : `Missed inner loop hoist if the loop does not depend on outer loop`
### open_at : `2018-10-06T21:31:54Z`
### last_modified_date : `2021-12-27T04:14:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87540
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
Not sure how often this happens in the real world apps but anyway idea is..

int foo(void)
{
    double  *array = calloc(ARRAY_SIZE, sizeof(double));
    double  sum = 0;
    int     i;
    for (i = 0; i < N_TIMES; i++) {
        // lot of code
        // well, this loop does not even depend on "i", hoist it?
        for (int j = 0; j < ARRAY_SIZE; j += 8) {
            sum += array[j] + array[j+1] + array[j+2] + array[j+3] + array[j+4] + array[j+5] +  array[j+6] + array[j+7];
        }
    }

    return sum;
}

Let's say we have the big outer loop with many inner loops. GCC should detect if these loops really depend on the outer loop and if not, hoist them out of this loop.


---


### compiler : `gcc`
### title : `Inconsistency in noticing a constant result rather than emitting code for a loop`
### open_at : `2018-10-06T22:02:38Z`
### last_modified_date : `2021-12-27T04:11:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87543
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `enhancement`
### contents :
Brief illustration on GodBolt: https://godbolt.org/z/sQyNGA
A related question on StackOverflow: https://stackoverflow.com/q/52677512/1593077


Consider the following two functions:

    #include <numeric> 
   
    int f1()
    {
        int arr[] = {1, 2, 3, 4, 5};
        auto n = sizeof(arr)/sizeof(arr[0]);
        return std::accumulate(arr,  arr + n, 0);
    }
    
    int f2()
    {
        int arr[] = {1, 2, 3, 4, 5};
        auto n = sizeof(arr)/sizeof(arr[0]);
        int sum = 0;
        for(int i = 0; i < n; i++) {
            sum += arr[i];
        }
        return sum;
    }

Both functions return 15, always; and while they're not marked constexpr, this can clearly be realized by the compiler. In fact, it is, if we compiler with -O3 (with GCC 8.2). However, with -O2, we get the following result:

    f1():
            movabs  rax, 8589934593
            lea     rdx, [rsp-40]
            mov     ecx, 1
            mov     DWORD PTR [rsp-24], 5
            mov     QWORD PTR [rsp-40], rax
            lea     rsi, [rdx+20]
            movabs  rax, 17179869187
            mov     QWORD PTR [rsp-32], rax
            xor     eax, eax
            jmp     .L3
    .L5:
            mov     ecx, DWORD PTR [rdx]
    .L3:
            add     rdx, 4
            add     eax, ecx
            cmp     rdx, rsi
            jne     .L5
            ret
    f2():
            mov     eax, 15
            ret


I don't think `std::accumulate` should have any code which should make -O2 fail to notice the optimization opportunity in `f1()`. But if that assertion might be debatable, surely adding -march=skylake to the -O3 can only result in stronger optimization, right? However, it results in _both_ functions, rather than just `f1()`, failing to fully optimize.


I asked about part of this issue at StackOverflow and a reply (by Florian Weimer) suggested this might be a regression relative to GCC 6.3 . And, indeed, if we switch the GCC version to 6.3 - both functions are not-fully-optimized in -O2, and fully-optimized with -O3:
https://godbolt.org/z/JOqCoC

if I try GCC 7.3, things get weird in yet a different way: -O2 optimizes both functions fully, and -O3 optimizes just the _first_ one.


---


### compiler : `gcc`
### title : `Optimize fetch atomics with unused results`
### open_at : `2018-10-08T08:31:43Z`
### last_modified_date : `2021-12-21T11:04:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87548
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
Atomic fetch and and/or/xor/etc., are not natively supported on amd64/x86_64, so they're simulated with CAS loops. There's a special case when this is unnecessary: when the fetched result is never used. In this case, it might be better to use a non-fetching operation instead.


---


### compiler : `gcc`
### title : `There is no need for UNSPEC_FMADDSUB`
### open_at : `2018-10-08T14:22:11Z`
### last_modified_date : `2021-09-12T23:44:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87555
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
sse.md has

;; It would be possible to represent these without the UNSPEC as
;;
;; (vec_merge
;;   (fma op1 op2 op3)
;;   (fma op1 op2 (neg op3)) 
;;   (merge-const))
;;
;; But this doesn't seem useful in practice.

(define_expand "fmaddsub_<mode>"
  [(set (match_operand:VF 0 "register_operand")
        (unspec:VF
          [(match_operand:VF 1 "nonimmediate_operand")
           (match_operand:VF 2 "nonimmediate_operand")
           (match_operand:VF 3 "nonimmediate_operand")]
          UNSPEC_FMADDSUB))]
  "TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F")

There is no need for UNSPEC_FMADDSUB.


---


### compiler : `gcc`
### title : `[9 Regression] 416.gamess is slower by ~10% starting from r264866 with -Ofast`
### open_at : `2018-10-09T08:34:27Z`
### last_modified_date : `2022-03-14T13:52:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87561
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
It's visible here:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=25.50.0

or here:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=24.50.0

It's slower both for native and generic tuning. Visible on Zen, Kaby Lake and Haswell CPUs.


---


### compiler : `gcc`
### title : `suboptimal memory-indirect tailcalls on arm`
### open_at : `2018-10-09T14:28:46Z`
### last_modified_date : `2021-12-21T11:05:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87565
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
When tailcalling via a pointer that needs to be loaded from memory, gcc could use 'ldr pc, [...]' instead of an ldr-bx sequence.

void foo(int a, int b, void (*p[])(int, int))
{
    p[1](a, b);
}

I see at -Os

foo:
        ldr     r3, [r2, #4]
        bx      r3  @ indirect register sibling call

But afaict this could be simply

foo:
        ldr     pc, [r2, #4]

(x86 has memory-indirect jumps too and there GCC gets this right via dedicated sibcall patterns)


---


### compiler : `gcc`
### title : `Static analysis generating errors on branch never taken`
### open_at : `2018-10-10T12:22:30Z`
### last_modified_date : `2021-12-13T01:34:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87576
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
Created attachment 44824
source file exhibiting the bug


---


### compiler : `gcc`
### title : `__tls_get_addr should be __attribute__((__noplt__))`
### open_at : `2018-10-11T23:08:01Z`
### last_modified_date : `2021-09-07T15:27:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87595
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `enhancement`
### contents :
The relevant code seems to be in targets, but this enhancement request applies to all targets.

__tls_get_addr is sufficiently a bottleneck that many projects (even gcc target libs) try to bypass it by using initial-exec model. In general, bypassing the PLT and calling directly through the GOT will save at least an icache line and 1 instruction. On some targets it takes several instruction to get through the PLT, and also imposes constraints on register allocation (e.g. ebx on i386).

My initial testing shows -fno-plt makes GD TLS access about 8% faster on i386 and no worse on x86_64. I will try to post some reproducible benchmarks as a follow-up later.


---


### compiler : `gcc`
### title : `Missed opportunity for flag reuse and macro-op fusion on x86`
### open_at : `2018-10-12T18:39:00Z`
### last_modified_date : `2021-12-21T11:09:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87601
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `enhancement`
### contents :
When I compile the following code with gcc 8.2 and options -O2 (or Os) and -mtune=intel (or broadwell):

int sum(int *vals, int l) {
    int a = 0;
    if (l <= 0) {
        return 0;
    }
    for (int i = l; i != 0; i--) {
        a += vals[i-1];
    }
    return a;
}


The following code is generated:

sum(int*, int):
  xor eax, eax
  test esi, esi
  jle .L1
  movsx rsi, esi
.L3:
  add eax, DWORD PTR [rdi-4+rsi*4]
  sub rsi, 1
  test esi, esi
  jne .L3
.L1:
  ret


When passing -march=broadwell or -Os, sub is replaced by dec but otherwise it's the same.

Inside the loop, the sequence:
  sub rsi, 1
  test esi, esi
  jne .L3

can be replaced with:
  sub rsi, 1
  jne .L3

since sub rsi, 1 since that would set the same zero flag that test would. This would improve macro-op fusion on relatively recent architectures as well. Anecdotally, I've seen similar decisions being made along the lines of 

sub index, 1

// some more asm here not using index

test index, index
jne loop_start

But don't have a nice clean test case for it. This suggests to me that the optimization around flag reuse and macro-op fusion could be improved in general, and I'll work on getting some clean test cases for other cases.


---


### compiler : `gcc`
### title : `outer loop auto-vectorization fails for exponentiation code`
### open_at : `2018-10-16T10:27:48Z`
### last_modified_date : `2018-11-09T10:54:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87621
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
https://godbolt.org/z/bgieBT

template <typename T>
T pow(T x, unsigned int n)
{
	if (!n)
		return 1;

	T y = 1;
	while (n > 1)
	{
		if (n%2)
			y *= x;
		x = x*x; // unsupported use in stmt
		n /= 2;
	}
	return x*y;
}

void testVec(int* x)
{
	// loop nest containing two or more consecutive inner loops cannot be vectorized
	for (int i = 0; i < 8; ++i)
		x[i] = pow(x[i], 10);
}


---


### compiler : `gcc`
### title : `GCC generates rube-goldberg machine for trivial tail call on 32-bit x86`
### open_at : `2018-10-17T01:05:07Z`
### last_modified_date : `2021-08-19T19:15:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87627
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Simple test case:

extern void bar();
extern void bah(int, int, int);
void foo(int a, int b, int c)
{
    bar();
    bah(a, b, c);
}

Expected output:

  subl $12, %esp
  call bar
  addl $12, %esp
  jmp bah

Actual:

  pushl %edi
  pushl %esi
  pushl %ebx
  movl 16(%esp), %ebx
  movl 20(%esp), %esi
  movl 24(%esp), %edi
  call bar
  movl %edi, 24(%esp)
  movl %esi, 20(%esp)
  movl %ebx, 16(%esp)
  popl %ebx
  popl %esi
  popl %edi
  jmp bah

I'm not clear on whether GCC is unaware that the argument space belongs to the callee and is preserved across calls, or whether it somehow thinks using call-saved registers is more optimal in typical cases and is missing the trivial reason why it's not here.


---


### compiler : `gcc`
### title : `Redundant check of pointer when operator delete is called`
### open_at : `2018-10-17T05:36:47Z`
### last_modified_date : `2023-05-23T19:45:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87628
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `unknown`
### severity : `normal`
### contents :
https://godbolt.org/z/DY9ruv

void if_delete(char *p) {
    if (p) {
        delete(p);
    }
}

$ gcc-8.2 -Os -fno-exceptions

if_delete(char*):
  test rdi, rdi
  je .L1
  mov esi, 1
  jmp operator delete(void*, unsigned long)
.L1:
  ret


While clang removes the check at -Oz:

$ clang -Oz -fno-exceptions
if_delete(char*):
        jmp     operator delete(void*)


---


### compiler : `gcc`
### title : `CSE for dynamic_cast`
### open_at : `2018-10-17T16:32:56Z`
### last_modified_date : `2021-07-22T16:46:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87634
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
With the code

    struct A { virtual void foo() = 0; };

    struct B : A { virtual void foo() {} void bar() const; };

    void test(A *a)
    {
        if(auto b = dynamic_cast<B *>(a))
            b->bar();
        if(auto b = dynamic_cast<B *>(a))
            b->bar();
    }

I'd expect the type of the object to be unchanged between the two `dynamic_cast` invocations, so the second type check would be unnecessary. The generated code does two checks, however.

It is in theory possible to replace the object in-place with one of different type if it is also accessible through a global pointer from within `B::bar()`, but is this a good enough reason to repeat the type check, or would it be possible to optimize out the second dynamic_cast<> here?


---
