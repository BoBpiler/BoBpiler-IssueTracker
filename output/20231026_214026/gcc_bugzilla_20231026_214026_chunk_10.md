### Total Bugs Detected: 4649
### Current Chunk: 10 of 30
### Bugs in this Chunk: 160 (From bug 1441 to 1600)
---


### compiler : `gcc`
### title : `Missed path isolation opportunity`
### open_at : `2017-02-20T03:56:26Z`
### last_modified_date : `2020-02-19T23:29:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79621
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
At least gcc-7.0.0-alpha20170212 and gcc-7.0.0-alpha20170219 snapshots ICE when compiling the following snippet w/ -O2:

int b5;

void
h6 (int zb, int e7)
{
  while (b5 > 0)
    {
      int gv;

      for (gv = 1; gv < 4; ++gv)
        {
          ((zb != 0) ? b5 : gv) && (b5 /= e7);
          zb = 0;
        }
      e7 = 0;
    }
}

% gcc-7.0.0-alpha20170219 -O2 -c glm1ku95.c 
glm1ku95.c: In function 'h6':
glm1ku95.c:4:1: internal compiler error: in operator[], at vec.h:732
 h6 (int zb, int e7)
 ^~


---


### compiler : `gcc`
### title : `Pointless reg1 <- reg2; reg2 <- reg1 moves inside loop`
### open_at : `2017-02-22T12:17:12Z`
### last_modified_date : `2021-08-07T06:17:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79675
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `middle-end`
### version : `7.0`
### severity : `normal`
### contents :
I'm looking at the code:
char *
foo (char *s1, char *s2, unsigned int n)
{
  char c1, c2;

  c2 = *(char *)s2++;

  do
    {
      do
        {
          c1 = *s1++;
          if (c1 == 0)
            return 0;
        }
      while (c1 != c2);
    }
  while (__builtin_strncmp (s1, (char *)s2, n) != 0);

  return --s1;
}

On aarch64 at -O2 this generates some bad code for the inner loop:
...
.L9:
        cmp     w21, w1
        beq     .L8
.L4:
        mov     x20, x19 // (1)
        mov     x19, x20 // (2)
        ldrb    w1, [x19], 1 (3)
        cbnz    w1, .L9
...
Note the moves between x20 and x19, (1) and (2)
Looking at the RTL dumps the two moves were in separate basic blocks but were moved together at some point.

Instructions (2) and (3) were transformed by autoincdec from:
add x19, x20, #1
ldrb w1, [x19, #-1]

but nothing eliminated (2) because x19 is later used as an argument to strncmp and in another basic block.

Doesn't the peephole2 pass try to eliminate such back-to-back copies?


---


### compiler : `gcc`
### title : `[meta-bug] SciMark 2.0 performance issues`
### open_at : `2017-02-24T09:20:30Z`
### last_modified_date : `2021-08-24T09:07:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79703
### status : `NEW`
### tags : `meta-bug, missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `normal`
### contents :
This meta-bug tracks SciMark 2.0 [1] performance issues.

[1] http://math.nist.gov/scimark2/download_c.html


---


### compiler : `gcc`
### title : `[meta-bug] Phoronix Test Suite compiler performance issues`
### open_at : `2017-02-24T09:35:37Z`
### last_modified_date : `2023-05-31T13:21:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79704
### status : `NEW`
### tags : `meta-bug, missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `normal`
### contents :
This meta-bug tracks Phoronix Test Suite [1] compiler performance issues.

[1] https://www.phoronix-test-suite.com/


---


### compiler : `gcc`
### title : `Subobtimal code with -mavx and explicit vector`
### open_at : `2017-02-24T20:30:21Z`
### last_modified_date : `2021-08-19T20:13:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79709
### status : `ASSIGNED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `7.0.1`
### severity : `enhancement`
### contents :
For the following code

typedef double v4do __attribute__((vector_size (32)));
typedef long int v4i __attribute__((vector_size (32)));

#define VSET(vect,val) do { vect[0]=val; vect[1]=val; vect[2]=val; vect[3]=val; } while (0)
void foo(v4do cx, v4do cy, v4i *r)
{
  v4do x, y, xn, yn;
  v4i add, res;
  v4do two, four;
  long int done;

  VSET(res, 0L);
  VSET(two, 2.0);
  VSET(four, 4.0);
  x = cx;
  y = cy;
  done = 0;
  while (1)
    {
      xn = x*x - y*y + cx;
      yn = two*x*y + cy;
      add = xn+xn + yn*yn < four;
      res += add;
      if (add[0] == 0 || add[1] == 0 || add[2] || add[3])
        break;
      x = xn;
      y = yn;
    }
  *r = res;
}

gcc compares strange code.  The loop is translated with 7.0.1 20170212
with "gcc -O3 -S -mavx v.c" into

.L14:
	vpextrq	$1, %xmm2, %rax
	testq	%rax, %rax
	je	.L2
	vmovdqa	-48(%rbp), %ymm5
	vextractf128	$0x1, %ymm5, %xmm2
	vmovq	%xmm2, %rax
	testq	%rax, %rax
	jne	.L2
	vpextrq	$1, %xmm2, %rax
	vmovapd	%ymm3, %ymm5
	testq	%rax, %rax
	jne	.L2
.L3:
	vmulpd	%ymm5, %ymm5, %ymm3
	vmulpd	%ymm8, %ymm5, %ymm5
	vsubpd	%ymm6, %ymm3, %ymm3
	vmulpd	%ymm4, %ymm5, %ymm4
	vaddpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm1, %ymm4, %ymm4
	vaddpd	%ymm3, %ymm3, %ymm2
	vmulpd	%ymm4, %ymm4, %ymm6
	vaddpd	%ymm6, %ymm2, %ymm2
	vcmpltpd	%ymm7, %ymm2, %ymm5
	vmovapd	%ymm5, -48(%rbp)
	vmovdqa	-48(%rbp), %xmm5
	vpaddq	-112(%rbp), %xmm5, %xmm5
	vmovaps	%xmm5, -80(%rbp)
	vmovdqa	-32(%rbp), %xmm5
	vpaddq	-96(%rbp), %xmm5, %xmm2
	vmovaps	%xmm2, -64(%rbp)
	vmovdqa	-80(%rbp), %ymm2
	vmovdqa	%ymm2, -112(%rbp)
	vmovdqa	-48(%rbp), %xmm2
	vmovq	%xmm2, %rax
	testq	%rax, %rax
	jne	.L14

which contains quite a few unnecessary instructions for moving stuff around.

By comparision, clang translates the inner loop to

.LBB0_1:                                # =>This Inner Loop Header: Depth=1
        vmulpd  %ymm5, %ymm5, %ymm6
        vmulpd  %ymm4, %ymm4, %ymm7
        vsubpd  %ymm7, %ymm6, %ymm6
        vaddpd  %ymm5, %ymm5, %ymm7
        vaddpd  %ymm0, %ymm6, %ymm5
        vmulpd  %ymm7, %ymm4, %ymm4
        vaddpd  %ymm1, %ymm4, %ymm4
        vaddpd  %ymm5, %ymm5, %ymm6
        vmulpd  %ymm4, %ymm4, %ymm7
        vaddpd  %ymm7, %ymm6, %ymm6
        vcmpltpd        %ymm8, %ymm6, %ymm6
        vextractf128    $1, %ymm6, %xmm7
        vextractf128    $1, %ymm2, %xmm3
        vpaddq  %xmm3, %xmm7, %xmm3
        vpaddq  %xmm2, %xmm6, %xmm2
        vinsertf128     $1, %xmm3, %ymm2, %ymm2
        vmovq   %xmm7, %rax
        vpextrq $1, %xmm7, %rcx
        orq     %rax, %rcx
        jne     .LBB0_4
# BB#2:                                 #   in Loop: Header=BB0_1 Depth=1
        vpextrq $1, %xmm6, %rax
        testq   %rax, %rax
        je      .LBB0_4
# BB#3:                                 #   in Loop: Header=BB0_1 Depth=1
        vmovq   %xmm6, %rax
        testq   %rax, %rax
        jne     .LBB0_1

which looks much more straighforward, and should be faster.


---


### compiler : `gcc`
### title : `memset followed by overwrite not eliminated`
### open_at : `2017-02-26T00:30:33Z`
### last_modified_date : `2022-01-28T23:37:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79716
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
The following two functions represent a common programming pattern of initializing a dynamically allocated object by allocating storage, clearing it, and then proceeding to write data into all or most of it.  The clearing is often done with large data structures "just in case" the subsequent assignments do not set all the data members.

GCC transforms both functions into a call calloc to allocate and clear the storage in one step, followed by either a call to memcpy or assignments to the individual members.  In both functions, however, clearing all the allocated memory is unnecessary and inefficient because it immediately gets overwritten.  When all or most of the allocated storage is written into it would be more efficient to just malloc the memory and then call memcpy or assign to it, and call memset to clear (or assign zero to) only the members that aren't explicitly written into.

Clang 5.0 emits optimal code for both functions.


$ cat t.c && gcc -O2 -S -Wall -Wextra -Wpedantic -Wunused-result -fdump-tree-optimized=/dev/stdout t.c
#include <stdlib.h>
#include <string.h>

void* f (const char *s, size_t n)
{
  char *p = malloc (n);
  memset (p, 0, n);
  memcpy (p, s, n);
  return p;
}

struct S { int a, b, c, d; };

void* g (void)
{
  struct S *p = malloc (sizeof (struct S));
  memset (p, 0, sizeof (struct S));

  p->a = 1;
  p->b = 2;
  p->c = 3;
  p->d = 4;

  return p;
}

;; Function f (f, funcdef_no=22, decl_uid=2738, cgraph_uid=22, symbol_order=22)

f (const char * s, size_t n)
{
  char * p;

  <bb 2> [100.00%]:
  p_4 = __builtin_calloc (n_2(D), 1);
  memcpy (p_4, s_6(D), n_2(D));
  return p_4;

}



;; Function g (g, funcdef_no=23, decl_uid=2747, cgraph_uid=23, symbol_order=23)

g ()
{
  struct S * p;

  <bb 2> [100.00%]:
  p_3 = __builtin_calloc (16, 1);
  MEM[(int *)p_3] = 8589934593;
  MEM[(int *)p_3 + 8B] = 17179869187;
  return p_3;

}


---


### compiler : `gcc`
### title : `Missing optimisation: Type conversion not vectorised in simple additive reduction`
### open_at : `2017-02-27T14:22:50Z`
### last_modified_date : `2021-02-23T11:00:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79726
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0.1`
### severity : `normal`
### contents :
Consider:

double f(double x[]) {
  float p = 1.0;
  for (int i = 0; i < 16; i++)
    p += x[i];
  return p;
}

gcc with -O3 -march=core-avx2 -ffast-math gives:

f:
        vmovsd  xmm0, QWORD PTR .LC0[rip]
        vaddsd  xmm0, xmm0, QWORD PTR [rdi]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+8]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+16]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+24]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+32]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+40]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+48]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+56]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+64]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+72]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+80]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+88]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+96]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+104]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+112]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        vaddsd  xmm0, xmm0, QWORD PTR [rdi+120]
        vcvtsd2ss       xmm0, xmm0, xmm0
        vcvtss2sd       xmm0, xmm0, xmm0
        ret
.LC0:
        .long   0
        .long   1072693248


However more efficient would be:

f:
        vcvtpd2ps xmm0, YMMWORD PTR [rdi]                       #4.5
        vcvtpd2ps xmm1, YMMWORD PTR [32+rdi]                    #4.5
        vcvtpd2ps xmm2, YMMWORD PTR [64+rdi]                    #4.5
        vcvtpd2ps xmm3, YMMWORD PTR [96+rdi]                    #4.5
        vaddps    xmm4, xmm0, xmm1                              #2.11
        vaddps    xmm5, xmm2, xmm3                              #2.11
        vaddps    xmm6, xmm4, xmm5                              #2.11
        vmovhlps  xmm7, xmm6, xmm6                              #2.11
        vaddps    xmm8, xmm6, xmm7                              #2.11
        vshufps   xmm9, xmm8, xmm8, 245                         #2.11
        vaddss    xmm10, xmm8, xmm9                             #2.11
        vaddss    xmm0, xmm10, DWORD PTR .L_2il0floatpacket.0[rip] #2.11
        vcvtss2sd xmm0, xmm0, xmm0                              #5.10
        vzeroupper                                              #5.10
        ret                                                     #5.10
.L_2il0floatpacket.0:
        .long   0x3f800000


---


### compiler : `gcc`
### title : `vec_init<> expander misses V2TImode with AVX and V2OImode and V2TImode with AVX512`
### open_at : `2017-02-28T11:13:39Z`
### last_modified_date : `2018-10-30T10:17:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79745
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `7.0.1`
### severity : `normal`
### contents :
The AVX case pessimizes CPUv6 x264 when vectorized with 256bit vectors.  The vectorizer tries to load the two 128bit halves and build a 256bit vector
(the halves are separated by a gap) via

              /* Avoid emitting a constructor of vector elements by performing
                 the loads using an integer type of the same size,
                 constructing a vector of those and then re-interpreting it
                 as the original vector type.  This works around the fact
                 that the vec_init optab was only designed for scalar
                 element modes and thus expansion goes through memory.
                 This avoids a huge runtime penalty due to the general
                 inability to perform store forwarding from smaller stores
                 to a larger load.  */
              unsigned lsize
                = group_size * TYPE_PRECISION (TREE_TYPE (vectype));
              enum machine_mode elmode = mode_for_size (lsize, MODE_INT, 0);
              enum machine_mode vmode = mode_for_vector (elmode,
                                                         nunits / group_size);
              /* If we can't construct such a vector fall back to
                 element loads of the original vector type.  */
              if (VECTOR_MODE_P (vmode)
                  && optab_handler (vec_init_optab, vmode) != CODE_FOR_nothing)
                {
                  nloads = nunits / group_size;
                  lnel = group_size;
                  ltype = build_nonstandard_integer_type (lsize, 1);
                  lvectype = build_vector_type (ltype, nloads);
                }


See also PR65832 which is broader.


---


### compiler : `gcc`
### title : `Vectorization of descending-index loops can produce unnecessary permutes`
### open_at : `2017-03-07T02:34:36Z`
### last_modified_date : `2019-07-15T08:43:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79934
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
Consider

long *a, *c;
int b;
void d() {
  for (; b; b--)
    a[b] |= c[b];
}

On POWER9, the core vectorized loop for this looks like this:

.L8:
    	lxvx 0,6,9           // load a[b] and a[b-1]
	lxvx 12,8,9          // load c[b] and c[b-1]
	xxpermdi 0,0,0,2     // swap elements of a
	xxpermdi 12,12,12,2  // swap elements of c
	xxlor 0,0,12         // produce a | c
	xxpermdi 0,0,0,2     // swap elements of result
	stxvx 0,6,9          // store a[b] and a[b-1]
	addi 9,9,-16
	bdnz .L8

The xxpermdi instructions are reversing the order of vector elements in a register.  However, the only operation being performed on these values is "lane-insensitive."  Since the swaps occur on all inputs and outputs to the computation, and none of the values escape the iteration, performing the operation in the "wrong" lanes does not affect the result.  Better code would be:

.L8:
    	lxvx 0,6,9           // load a[b] and a[b-1]
	lxvx 12,8,9          // load c[b] and c[b-1]
	xxlor 0,0,12         // produce a | c in "wrong" lanes
	stxvx 0,6,9          // store a[b] and a[b-1]
	addi 9,9,-16
	bdnz .L8

This code is introduced by the vectorizer:

  <bb 10> [55.08%]:
  # b.7_18 = PHI <_10(11), b.7_78(9)>
  # vectp.21_110 = PHI <vectp.21_111(11), vectp.22_104(9)>
  # vectp.25_119 = PHI <vectp.25_120(11), vectp.26_114(9)>
  # vectp.30_129 = PHI <vectp.30_130(11), vectp.31_124(9)>
  # ivtmp_133 = PHI <ivtmp_134(11), 0(9)>
  _2 = (long unsigned int) b.7_18;
  _3 = _2 * 8;
  _4 = a.0_1 + _3;
  vect__5.23_112 = MEM[(long int *)vectp.21_110];
  vect__5.24_113 = VEC_PERM_EXPR <vect__5.23_112, vect__5.23_112, { 1, 0 }>;
  _5 = *_4;
  _7 = c.2_6 + _3;
  vect__8.27_121 = MEM[(long int *)vectp.25_119];
  vect__8.28_122 = VEC_PERM_EXPR <vect__8.27_121, vect__8.27_121, { 1, 0 }>;
  _8 = *_7;
  vect__9.29_123 = vect__5.24_113 | vect__8.28_122;
  _9 = _5 | _8;
  vect__9.32_131 = VEC_PERM_EXPR <vect__9.29_123, vect__9.29_123, { 1, 0 }>;
  MEM[(long int *)vectp.30_129] = vect__9.32_131;
  _10 = b.7_18 + -1;
  vectp.21_111 = vectp.21_110 + 18446744073709551600;
  vectp.25_120 = vectp.25_119 + 18446744073709551600;
  vectp.30_130 = vectp.30_129 + 18446744073709551600;
  ivtmp_134 = ivtmp_133 + 1;
  if (ivtmp_134 < bnd.18_99)
    goto <bb 11>; [83.34%]
  else
    goto <bb 12>; [16.66%]

The unnecessary VEC_PERM_EXPRs (element reversals) could be removed after vectorization by scanning for closed computations in each iteration having these properties:

 - All loads feed into an element reversal and nothing else;
 - All stores are fed by an element reversal and nothing else;
 - All intermediate computations are lane-insensitive;
 - The computation has no other inputs than the element-reversed loads; and
 - The computation has no other outputs than the element-reversed stores.

This is quite similar to the requirements of the analyze_swaps() pass in the POWER back end (rs6000.c), which is somewhat more general in that closed computations aren't limited to a single loop iteration but can contain general control flow.


---


### compiler : `gcc`
### title : `gcc unnecessarily spills xmm register to stack when inserting vector items`
### open_at : `2017-03-07T10:32:07Z`
### last_modified_date : `2021-08-05T19:37:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79938
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `6.2.0`
### severity : `enhancement`
### contents :
Created attachment 40906
assembler output

When adding together values from one vector and storing the results to another, gcc uses two xmm registers instead of one and spills the second xmm register to stack when it runs out of general purpose registers.

Instead of spilling the second xmm register to stack, it should use only one xmm register as destination, because the addition is already being done using four general purpose registers.

Using gcc -msse4.1 -O3 -S hadd.c -Wall -Wextra -fno-strict-aliasing -fwrapv -o hadd.s

mika@LENOVO:~$ gcc --version
gcc (Ubuntu 6.2.0-3ubuntu11~14.04) 6.2.0 20160901
---
#include <x86intrin.h>
#include <inttypes.h>
#include <stdio.h>

typedef uint8_t   v1si __attribute__ ((vector_size (16)));
typedef uint16_t  v2si __attribute__ ((vector_size (16)));
typedef uint32_t  v4si __attribute__ ((vector_size (16)));
typedef uint64_t  v8si __attribute__ ((vector_size (16)));

static __m128i haddd_epu8(__m128i a)
{
  v1si b = (v1si)a;
  v4si ret;
  ret[0]  = (b[ 0] + b[ 1]) + (b[ 2] + b[ 3]);
  ret[1]  = (b[ 4] + b[ 5]) + (b[ 6] + b[ 7]);
  ret[2]  = (b[ 8] + b[ 9]) + (b[10] + b[11]);
  ret[3]  = (b[12] + b[13]) + (b[14] + b[15]);
  return (__m128i)ret;
}

int main(int argc, char *argv[])
{
  __m128i a = _mm_set1_epi8(atoi(argv[1]));
  __m128i b = haddd_epu8(a);
  v4si c = (v4si)b;
  printf("b[0] = %i, b[1] = %i, b[2] = %i, b[3] = %i\n", c[0], c[1], c[2], c[3]);
}


---


### compiler : `gcc`
### title : `Suboptimal code with AVX2 copying all arguments to stack`
### open_at : `2017-03-07T19:05:38Z`
### last_modified_date : `2021-07-19T01:45:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79946
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0.1`
### severity : `enhancement`
### contents :
Created attachment 40916
Assembly output for gfortran

I looked at a reduced version of the code in PR 79930:

module foo
  use, intrinsic :: iso_fortran_env
  implicit none
  integer, parameter :: dp = real64 ! KIND for double precision
  type Vect3D
    real(dp) :: x,y,z
  end type
contains
  type(Vect3D) recursive pure function TP_LEFT(NU, D, NV) result(tensorproduct)
    real(dp),     intent(in) :: NU(4), NV(4)
    type(Vect3D), intent(in) :: D(4,4)
    real(dp)                 :: Dx(4,4), Dy(4,4), Dz(4,4)
    real(dp)                 :: tmp(4)
    Dx = D%x
    Dy = D%y
    Dz = D%z
    tmp = matmul(NU,Dx);
    tensorproduct%x = dot_product(tmp,NV)
    tmp = matmul(NU,Dy);
    tensorproduct%y = dot_product(tmp,NV)
    tmp = matmul(NU,Dz);
    tensorproduct%z = dot_product(tmp,NV)
  end function
end module foo

Translating with 

$ gfortran -mavx2 -mfma -S -Ofast -o tp_o_gfortran.s tp_o.f90

led to code at the beginning of the function where all of the
arguments appear to be copied to the stack:

_foo_MOD_tp_left:
.LFB0:
        .cfi_startproc
        leaq    8(%rsp), %r10
        .cfi_def_cfa 10, 0
        andq    $-32, %rsp
        movq    %rdi, %rax
        pushq   -8(%r10)
        pushq   %rbp
        .cfi_escape 0x10,0x6,0x2,0x76,0
        movq    %rsp, %rbp
        pushq   %r10
        .cfi_escape 0xf,0x3,0x76,0x78,0x6
        subq    $304, %rsp
        vmovsd  (%rdx), %xmm0
        vmovsd  %xmm0, -400(%rbp)
        vmovsd  24(%rdx), %xmm0
        vmovsd  %xmm0, -392(%rbp)
        vmovsd  48(%rdx), %xmm0
        vmovsd  %xmm0, -384(%rbp)
        vmovsd  72(%rdx), %xmm0
        vmovsd  %xmm0, -376(%rbp)
        vmovsd  96(%rdx), %xmm0
        vmovsd  %xmm0, -368(%rbp)
        vmovsd  120(%rdx), %xmm0
        vmovsd  %xmm0, -360(%rbp)
        vmovsd  144(%rdx), %xmm0
 
... and so on. The code appears to unload all of the arguments onto
the stack, then operate from there.

This results in 96 vmovsd instructions and a total of 211 instructions
overall.

By comparision, ifort starts out its code with

        pushq     %rbp                                          #12.40
        movq      %rsp, %rbp                                    #12.40
        andq      $-32, %rsp                                    #12.40
        movq      %rdi, %rax                                    #27.3
        vmovups   (%rdx), %xmm4                                 #18.5
        vmovups   16(%rdx), %xmm10                              #18.5
        vmovups   32(%rdx), %xmm11                              #18.5
        vinsertf128 $1, 48(%rdx), %ymm4, %ymm15                 #18.5
        vinsertf128 $1, 64(%rdx), %ymm10, %ymm1                 #18.5
        vblendpd  $10, %ymm1, %ymm15, %ymm2                     #18.5
   
resulting in hardly any memory use and only 136 instructions.


---


### compiler : `gcc`
### title : `Missed tree DSE`
### open_at : `2017-03-08T14:13:25Z`
### last_modified_date : `2023-08-28T20:51:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=79958
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `enhancement`
### contents :
At -O3 in bar we have various dead temporaries, they don't have address taken and so should be DSEd, but they aren't.  In foo which doesn't have the EH edges this works well.

#include <cstddef>
#include <functional>
#include <stdint.h>

namespace
{
  struct bitarray
  {
    bitarray(int i) : mSlice(i) {}
    template<typename EXPR>
    bitarray(const EXPR &expr) { mSlice = expr.slice(0); }
    uint32_t
    slice(ptrdiff_t index) const { if (index != 0) return 0; return mSlice; }
  private:
    uint32_t mSlice;
  };
  template<typename LEFT, typename RIGHT, template<typename T> class OP>
  struct bit_binary_expr
  {
    bit_binary_expr(const LEFT &left, const RIGHT &right) : mLeft(left), mRight(right) {}
    uint32_t slice(ptrdiff_t index) const { OP<uint32_t> op; return op(mLeft.slice(index), mRight.slice(index)); }
  private:
    const LEFT &mLeft;
    const RIGHT &mRight;
  };
  template<typename LEFT, typename RIGHT>
  bit_binary_expr<LEFT, RIGHT, ::std::bit_and>
  operator &(const LEFT &left, const RIGHT &right) { return bit_binary_expr<LEFT, RIGHT, ::std::bit_and>(left, right); }
  template<typename LEFT, typename RIGHT>
  bit_binary_expr<LEFT, RIGHT, ::std::bit_or> operator |(const LEFT &left, const RIGHT &right)
  { return bit_binary_expr<LEFT, RIGHT, ::std::bit_or>(left, right); }
  template<typename OPERAND>
  struct bit_shift_left
  {
    bit_shift_left(const OPERAND &operand, size_t nBits) : mOperand(operand), mBits(nBits) {}
    uint32_t slice(ptrdiff_t index) const
    {
      const ptrdiff_t offset = mBits / 32;
      const size_t bits = mBits % 32;
      if (bits == 0)
        return mOperand.slice(index - offset);
      else
        return (mOperand.slice(index - offset) << bits) | (mOperand.slice(index - offset - 1) >> (32 - bits));
    }
  private:
    const OPERAND &mOperand;
    const size_t mBits;
  };
  template<typename OPERAND>
  bit_shift_left<OPERAND> operator <<(const OPERAND &operand, size_t nBits) { return bit_shift_left<OPERAND>(operand, nBits); }
}
struct Foo
{
  void write(const bitarray &);
};
void
foo(int i)
{
  Foo f;
  f.write(bitarray(( ((((bitarray(i)  & bitarray(0x00000001))) << 31) | bitarray(0x40000000)))
            | ((((bitarray(42) & bitarray(0x0000ffff))) <<  8) | bitarray(0x00000080))));
}
struct Bar
{
  ~Bar(void);
  void write(const bitarray &) /* throw() */;
};
void
bar(int i)
{
  Bar b;
  b.write(bitarray(( ((((bitarray(i)  & bitarray(0x00000001))) << 31) | bitarray(0x40000000)))
            | ((((bitarray(42) & bitarray(0x0000ffff))) <<  8) | bitarray(0x00000080))));
}


---


### compiler : `gcc`
### title : `loss of range information due to spurious widening conversion`
### open_at : `2017-03-11T21:39:32Z`
### last_modified_date : `2021-10-20T07:36:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80006
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.0`
### severity : `normal`
### contents :
This bug is the cause of bug 79356 (discussed here: https://gcc.gnu.org/ml/gcc-patches/2017-02/msg00684.html).

When the following test case is compiled for some targets (such as powerpc64) it emits a diagnostic for the call to ff in f as expected.  This is thanks the range of the x variable is correctly represented as the anti-range ~[-3, 7], which means that valid (i.e., non-negative) argument to the allocation function is greater greater than 7 (negative argument are considered invalid).

However, the same warning is not issued for the call to gg, or even for ff on on other targets (such as x86_64).  The reason is that VRP appears to "lose" the range of the signed char variable x when it's converted to int.  On x86_64, this conversion from signed char to int is for some reason performed even in function f, so the test program triggers no warnings.  On powerpc64, the conversion is not done in f but it is done in g where it is explicit.

In summary, it seems as though there are two problems:
1) the spurious signed char to int conversion on x86_64 inserted (very early on -- as can be seen in the tree-original dump)
2) the loss of range info when converting a signed char variable to signed int.

$ cat t.c && /build/powerpc64le-linux-gnu/gcc-trunk/gcc/xgcc -B /build/powerpc64le-linux-gnu/gcc-trunk/gcc -O2 -S -Wall -Wextra -Wpedantic -Walloc-size-larger-than=7 -fdump-tree-vrp=/dev/stdout t.c
void* f (signed char x)
{
  extern void* ff (signed char) __attribute__ ((alloc_size (1)));

  if (-3 <= x && x <= 7)
    x = -4;

  return ff (x);
}

void* g (signed char x)
{
  extern void* gg (int) __attribute__ ((alloc_size (1)));

  if (-3 <= x && x <= 7)
    x = -4;

  return gg (x);
}



;; Function f (f, funcdef_no=0, decl_uid=2492, cgraph_uid=0, symbol_order=0)

;; 1 loops found
;;
;; Loop 0
;;  header 0, latch 1
;;  depth 0, outer -1
;;  nodes: 0 1 2 3 4
;; 2 succs { 3 4 }
;; 3 succs { 4 }
;; 4 succs { 1 }
Adding assert for x_4(D) from (unsigned char) x_4(D) + 3
Adding assert for x_4(D) from (unsigned char) x_4(D) + 3

SSA replacement table
N_i -> { O_1 ... O_j } means that N_i replaces O_1, ..., O_j

x_8 -> { x_4(D) }
x_9 -> { x_4(D) }
Incremental SSA update started at block: 2
Number of blocks in CFG: 6
Number of blocks to update: 4 ( 67%)



Value ranges after VRP:

x.0_1: [0, +INF]
_2: [0, +INF]
x_3: ~[-3, 7]  EQUIVALENCES: { } (0 elements)
x_4(D): VARYING
_7: VARYING
x_8: [-3, 7]  EQUIVALENCES: { x_4(D) } (1 elements)
x_9: ~[-3, 7]  EQUIVALENCES: { x_4(D) } (1 elements)


Removing basic block 3
f (signed char x)
{
  unsigned char x.0_1;
  unsigned char _2;
  void * _7;

  <bb 2> [100.00%]:
  x.0_1 = (unsigned char) x_4(D);
  _2 = x.0_1 + 3;
  if (_2 <= 10)
    goto <bb 4>; [54.00%]
  else
    goto <bb 3>; [46.00%]

  <bb 3> [46.00%]:

  <bb 4> [100.00%]:
  # x_3 = PHI <x_4(D)(3), -4(2)>
  _7 = ff (x_3);
  return _7;

}



;; Function f (f, funcdef_no=0, decl_uid=2492, cgraph_uid=0, symbol_order=0)

;; 1 loops found
;;
;; Loop 0
;;  header 0, latch 1
;;  depth 0, outer -1
;;  nodes: 0 1 2 3 4
;; 2 succs { 3 4 }
;; 3 succs { 4 }
;; 4 succs { 1 }
Adding assert for x_4(D) from (unsigned char) x_4(D) + 3
Adding assert for x_4(D) from (unsigned char) x_4(D) + 3

SSA replacement table
N_i -> { O_1 ... O_j } means that N_i replaces O_1, ..., O_j

x_8 -> { x_4(D) }
x_9 -> { x_4(D) }
Incremental SSA update started at block: 2
Number of blocks in CFG: 6
Number of blocks to update: 4 ( 67%)



Value ranges after VRP:

x.0_1: [0, +INF]
_2: [0, +INF]
x_3: ~[-3, 7]  EQUIVALENCES: { } (0 elements)
x_4(D): VARYING
_7: VARYING
x_8: [-3, 7]  EQUIVALENCES: { x_4(D) } (1 elements)
x_9: ~[-3, 7]  EQUIVALENCES: { x_4(D) } (1 elements)


Removing basic block 3
f (signed char x)
{
  unsigned char x.0_1;
  unsigned char _2;
  void * _7;

  <bb 2> [100.00%]:
  x.0_1 = (unsigned char) x_4(D);
  _2 = x.0_1 + 3;
  if (_2 <= 10)
    goto <bb 4>; [54.00%]
  else
    goto <bb 3>; [46.00%]

  <bb 3> [46.00%]:

  <bb 4> [100.00%]:
  # x_3 = PHI <x_4(D)(3), -4(2)>
  _7 = ff (x_3);
  return _7;

}


t.c: In function ‘f’:
t.c:8:10: warning: argument 1 range [8, 127] exceeds maximum object size 7 [-Walloc-size-larger-than=]
   return ff (x);
          ^~~~~~
t.c:3:16: note: in a call to allocation function ‘ff’ declared here
   extern void* ff (signed char) __attribute__ ((alloc_size (1)));
                ^~

;; Function g (g, funcdef_no=1, decl_uid=2498, cgraph_uid=1, symbol_order=1)

;; 1 loops found
;;
;; Loop 0
;;  header 0, latch 1
;;  depth 0, outer -1
;;  nodes: 0 1 2 3 4
;; 2 succs { 3 4 }
;; 3 succs { 4 }
;; 4 succs { 1 }
Adding assert for x_5(D) from (unsigned char) x_5(D) + 3
Adding assert for x_5(D) from (unsigned char) x_5(D) + 3

SSA replacement table
N_i -> { O_1 ... O_j } means that N_i replaces O_1, ..., O_j

x_9 -> { x_5(D) }
x_10 -> { x_5(D) }
Incremental SSA update started at block: 2
Number of blocks in CFG: 6
Number of blocks to update: 4 ( 67%)



Value ranges after VRP:

x.1_1: [0, +INF]
_2: [0, +INF]
_3: [-128, 127]
x_4: ~[-3, 7]  EQUIVALENCES: { } (0 elements)
x_5(D): VARYING
_8: VARYING
x_9: [-3, 7]  EQUIVALENCES: { x_5(D) } (1 elements)
x_10: ~[-3, 7]  EQUIVALENCES: { x_5(D) } (1 elements)


Removing basic block 3
g (signed char x)
{
  unsigned char x.1_1;
  unsigned char _2;
  int _3;
  void * _8;

  <bb 2> [100.00%]:
  x.1_1 = (unsigned char) x_5(D);
  _2 = x.1_1 + 3;
  if (_2 <= 10)
    goto <bb 4>; [54.00%]
  else
    goto <bb 3>; [46.00%]

  <bb 3> [46.00%]:

  <bb 4> [100.00%]:
  # x_4 = PHI <x_5(D)(3), -4(2)>
  _3 = (int) x_4;
  _8 = gg (_3);
  return _8;

}



;; Function g (g, funcdef_no=1, decl_uid=2498, cgraph_uid=1, symbol_order=1)

;; 1 loops found
;;
;; Loop 0
;;  header 0, latch 1
;;  depth 0, outer -1
;;  nodes: 0 1 2 3 4
;; 2 succs { 4 3 }
;; 3 succs { 4 }
;; 4 succs { 1 }
Adding assert for x_5(D) from (unsigned char) x_5(D) + 3

SSA replacement table
N_i -> { O_1 ... O_j } means that N_i replaces O_1, ..., O_j

x_4 -> { x_5(D) }
Incremental SSA update started at block: 2
Number of blocks in CFG: 5
Number of blocks to update: 2 ( 40%)



Value ranges after VRP:

x.1_1: [0, +INF]
_2: [0, +INF]
x_4: ~[-3, 7]  EQUIVALENCES: { x_5(D) } (1 elements)
x_5(D): VARYING
_8: VARYING
_9: [-128, 127]
prephitmp_10: [-128, 127]


g (signed char x)
{
  unsigned char x.1_1;
  unsigned char _2;
  void * _8;
  int _9;
  int prephitmp_10;

  <bb 2> [100.00%]:
  x.1_1 = (unsigned char) x_5(D);
  _2 = x.1_1 + 3;
  if (_2 <= 10)
    goto <bb 4>; [54.00%]
  else
    goto <bb 3>; [46.00%]

  <bb 3> [46.00%]:
  _9 = (int) x_5(D);

  <bb 4> [100.00%]:
  # prephitmp_10 = PHI <_9(3), -4(2)>
  _8 = gg (prephitmp_10);
  return _8;

}


---


### compiler : `gcc`
### title : `auto vectorization leaves scalar epilogue even if it is unreachable due to dominating n % 4 check`
### open_at : `2017-03-12T16:55:07Z`
### last_modified_date : `2023-10-08T01:21:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80015
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0.1`
### severity : `normal`
### contents :
Consider these two versions of dot_product:

#include <cstdlib>

float dot_product(float const* a,
                  float const* b,
                  size_t n)
{
    a = (float const*)__builtin_assume_aligned(a, 16);
    b = (float const*)__builtin_assume_aligned(b, 16);
  
    if ((n % 4) != 0)
       return 0.;                    // (1)
//       __builtin_unreachable();    // (2)

    float result = 0.f;
  
  	for (size_t i = 0; i != n; ++i)
      result += a[i] * b[i];
  
    return result;
}

The code should be compiled with flags -O3 -ffast-math.

In case of (1) the return 0. is performed when n is not a multiple of 4, in (2) __builtin_unreachable() is invoked. The code (2) with __builtin_unreachable() is optimized to the point where only packed operations are used. In the code (1) with return the scalar operations are still left.

The expected behavior is that gcc should not emit scalar operations in both versions.


---


### compiler : `gcc`
### title : `[6 Regression] C++ excessive stack usage (no stack reuse)`
### open_at : `2017-03-13T15:22:05Z`
### last_modified_date : `2021-08-05T09:39:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80032
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `6.3.1`
### severity : `normal`
### contents :
Created attachment 40959
testcase

I have a unlucky code pattern that resulted in 10592 bytes stack usage when built with GCC 6.3, whereas it only used 304 bytes when compiled with GCC 5.4.

gcc-5 (Ubuntu 5.4.1-8ubuntu1) 5.4.1 20170304
gcc (Ubuntu 6.3.0-8ubuntu1) 6.3.0 20170221

Attached is a C-Reduced testcase. If the if() block is repeated multiple times the stack increases more. Looks as if stack isn't reused.

if() block occurs 
 1x : GCC5:  96 bytes , GCC6:  96 bytes
 2x : GCC5: 112 bytes , GCC6: 176 bytes
10x : GCC5: 112 bytes , GCC6: 688 bytes

Testcase compiled with : -fstack-usage -std=c++11 -O2 -Wall

Using -fstack-reuse=none gives GCC 5 the same stack usage as GCC 6.

I think this is a regression.


---


### compiler : `gcc`
### title : `SSE4.1 ptest not always merged`
### open_at : `2017-03-14T16:01:41Z`
### last_modified_date : `2023-06-05T01:58:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80040
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `6.3.1`
### severity : `enhancement`
### contents :
Created attachment 40971
Example

The intrinsics _mm_testz_si128 and _mm_testc_si128 both map to the exact same instruction and parameters. They are sometimes merged to just one instruction call, but not always. 

I have attached and example where in the first function the two intrinsics are merge but in the second are not.


---


### compiler : `gcc`
### title : `S390: Isses with emitted cs-instructions for __atomic builtins.`
### open_at : `2017-03-17T09:07:23Z`
### last_modified_date : `2018-12-03T09:49:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80080
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `7.0`
### severity : `normal`
### contents :
For s390, I am now using the c11 atomic builtins in glibc.
There are now some issues with the emitted cs-instructions.
If __atomic_compare_exchange_n is used within a condition for if/while,
it is sometimes not using the condition code directly to jump away.
Instead it extracts the condition code to a general register via ipm followed by
further instructions in order to compare it. Afterwards it jumps according to
this comparison.

int foo1 (int *mem)
{
  int val, newval;
  val = __atomic_load_n (mem, __ATOMIC_RELAXED);
  do
    {
      newval = val | 123;
    }
  while (!__atomic_compare_exchange_n (mem, &val, newval, 1, __ATOMIC_ACQUIRE,
				       __ATOMIC_RELAXED));

  /*
    0000000000000000 <foo>:
   0:   58 10 20 00             l       %r1,0(%r2)
   4:   18 31                   lr      %r3,%r1
   6:   a5 3b 00 7b             oill    %r3,123
   a:   ba 13 20 00             cs      %r1,%r3,0(%r2)
   e:   b2 22 00 30             ipm     %r3
  12:   8a 30 00 1c             sra     %r3,28
  16:   ec 36 ff f7 00 7e       cijne   %r3,0,4 <foo+0x4>
  1c:   a7 29 00 00             lghi    %r2,0
  20:   07 fe                   br      %r14
  22:   07 07                   nopr    %r7
  24:   07 07                   nopr    %r7
  26:   07 07                   nopr    %r7
   */
  return 0;
}



For __atomic_exchange_n, s390 has no special instruction and thus a cs-loop is
used. An extra register is needed to save the old-value instead of using the
"old"-register of cs-instruction which is updated if the new value is not stored
to memory. This extra register is reloaded in every loop.

extern int bar (int *mem);
int foo2 (int *mem)
{
  int old = __atomic_exchange_n (mem, 0, __ATOMIC_ACQUIRE);
  if (old >= 2)
    return bar (mem);

  /*
0000000000000028 <foo2>:
  28:   a7 48 00 00             lhi     %r4,0
  2c:   58 10 20 00             l       %r1,0(%r2)
  30:   18 31                   lr      %r3,%r1
  32:   ba 14 20 00             cs      %r1,%r4,0(%r2)
  36:   a7 74 ff fd             jne     30 <foo2+0x8>
  3a:   ec 3c 00 06 01 7e       cijnh   %r3,1,46 <foo2+0x1e>
  40:   c0 f4 00 00 00 00       jg      40 <foo2+0x18>
                        42: R_390_PC32DBL       bar+0x2
  46:   a7 29 00 00             lghi    %r2,0
  4a:   07 fe                   br      %r14
  4c:   07 07                   nopr    %r7
  4e:   07 07                   nopr    %r7
   */
  return 0;
}

In case of exchanging to a zero value, like above, the load-and-and instruction
can be used. Then only one register is needed for the new and old value and
there is no loop.


If the exchanged memory is a global variable, the address of it is loaded
within the loop instead of before the loop.

extern int foo3_mem;
int foo3 (void)
{
  return __atomic_exchange_n (&foo3_mem, 5, __ATOMIC_ACQUIRE);

  /*
0000000000000050 <foo3>:
  50:   c4 1d 00 00 00 00       lrl     %r1,50 <foo3>
                        52: R_390_PC32DBL       foo3_mem+0x2
  56:   a7 38 00 05             lhi     %r3,5
  5a:   18 21                   lr      %r2,%r1
  5c:   c0 40 00 00 00 00       larl    %r4,5c <foo3+0xc>
                        5e: R_390_PC32DBL       foo3_mem+0x2
  62:   ba 13 40 00             cs      %r1,%r3,0(%r4)
  66:   a7 74 ff fa             jne     5a <foo3+0xa>
  6a:   b9 14 00 22             lgfr    %r2,%r2
  6e:   07 fe                   br      %r14
   */
}


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] Performance regression with code hoisting enabled`
### open_at : `2017-03-22T15:22:26Z`
### last_modified_date : `2023-07-07T10:32:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80155
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
Created attachment 41022
Reproducers for Cortex-M0+ and Cortex-M7

Hi,

Performance of a popular embedded benchmark regress in GCC 7 at -O2 optimization level on arm-none-eabi targets (at least on Cortex-M0+ or Cortex-M7). They key to trigger it is to have -fcode-hoisting enabled. If disabled, performance is improved.

Attached to this bug report are reduced testcase for Cortex-M0+ and Cortex-M7. Compile respectively with:

arm-none-eabi-gcc -S -O2 -mcpu=cortex-m0plus
arm-none-eabi-gcc -S -O2 -mcpu=cortex-m7

and compare to the same command line with the extra -fno-code-hoisting option added to see the amount of register push decrease.


---


### compiler : `gcc`
### title : `pgo dramatically pessimizes scimark2 MonteCarlo benchmark`
### open_at : `2017-03-26T13:41:58Z`
### last_modified_date : `2023-07-22T03:06:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80197
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.0.1`
### severity : `normal`
### contents :
Created attachment 41053
self contained benchmark of scimark2 MC

while chasing the regression I then found identified and solved in #79389
I discovered that pgo manages to do much worse than the regression above.
The symptom is the same: a huge increase in branch-miss.
This is not a regression: it is the same at least since gcc5.3
Attached a self contained single file, copy of scimark2 MC, and a couple of scripts to compile and run it

just
tar -xzf fullMC.tgz
cd fullMC
# standard compilation -O2 -O3
./runit 
# same with pgo passes
./dopgo

or just do
[innocent@vinavx3 fullMC]$ rm -rf pgo/* ; c++ -O3 fullMC.c -g -fprofile-generate=pgo ; time ./a.out
1.848u 0.000s 0:01.85 99.4%	0+0k 0+8io 0pf+0w
[innocent@vinavx3 fullMC]$ c++ -O3 fullMC.c -g -fprofile-use=./pgo ; time ./a.out
0.967u 0.001s 0:00.96 100.0%	0+0k 0+0io 0pf+0w
[innocent@vinavx3 fullMC]$ c++ -O3 fullMC.c -g; time ./a.out
0.328u 0.000s 0:00.32 100.0%	0+0k 0+0io 0pf+0w


for reference:
cat dopgo
cat /proc/cpuinfo | grep name | head -n 1
gcc -v
rm -rf pgo/*;gcc -O2 fullMC.c -g -fprofile-generate=pgo; ./a.out
gcc -O2 fullMC.c -g -fprofile-use=pgo; ./a.out
perf stat -e task-clock -e cycles -e instructions -e branches -e branch-misses ./a.out
rm -rf pgo/*;gcc -O3 fullMC.c -g -fprofile-generate=pgo; ./a.out
gcc -O3 fullMC.c -g -fprofile-use=pgo; ./a.out
perf stat -e task-clock -e cycles -e instructions -e branches -e branch-misses ./a.out


on my machine the result is
# standard compilation
[innocent@vinavx3 fullMC]$ ./runit 
model name	: Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/afs/cern.ch/work/i/innocent/public/w5/bin/../libexec/gcc/x86_64-pc-linux-gnu/7.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk//configure --prefix=/afs/cern.ch/user/i/innocent/w5 -enable-languages=c,c++,lto,fortran --enable-lto -enable-libitm -disable-multilib
Thread model: posix
gcc version 7.0.1 20170326 (experimental) [trunk revision 246482] (GCC) 
gcc -O2 fullMC.c -g

real	0m0.489s
user	0m0.485s
sys	0m0.002s

 Performance counter stats for './a.out':

        486.303424      task-clock (msec)         #    0.999 CPUs utilized          
        1901271534      cycles                    #    3.910 GHz                    
        6403589598      instructions              #    3.37  insn per cycle         
         700683389      branches                  # 1440.836 M/sec                  
             13582      branch-misses             #    0.00% of all branches        

       0.486571089 seconds time elapsed

gcc -O3 fullMC.c -g

real	0m0.330s
user	0m0.330s
sys	0m0.000s

 Performance counter stats for './a.out':

        327.385696      task-clock (msec)         #    0.999 CPUs utilized          
        1279958668      cycles                    #    3.910 GHz                    
        5009002909      instructions              #    3.91  insn per cycle         
         306481761      branches                  #  936.149 M/sec                  
             10805      branch-misses             #    0.00% of all branches        

       0.327637485 seconds time elapsed


// pro generation and use (perf after use...)
[innocent@vinavx3 fullMC]$ ./dopgo 
model name	: Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/afs/cern.ch/work/i/innocent/public/w5/bin/../libexec/gcc/x86_64-pc-linux-gnu/7.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk//configure --prefix=/afs/cern.ch/user/i/innocent/w5 -enable-languages=c,c++,lto,fortran --enable-lto -enable-libitm -disable-multilib
Thread model: posix
gcc version 7.0.1 20170326 (experimental) [trunk revision 246482] (GCC) 

 Performance counter stats for './a.out':

        964.399833      task-clock (msec)         #    1.000 CPUs utilized          
        3770455888      cycles                    #    3.910 GHz                    
        5007987488      instructions              #    1.33  insn per cycle         
         816525627      branches                  #  846.667 M/sec                  
          88982233      branch-misses             #   10.90% of all branches        

       0.964699603 seconds time elapsed


 Performance counter stats for './a.out':

        964.540691      task-clock (msec)         #    1.000 CPUs utilized          
        3771010753      cycles                    #    3.910 GHz                    
        5007957589      instructions              #    1.33  insn per cycle         
         816522043      branches                  #  846.540 M/sec                  
          88992086      branch-misses             #   10.90% of all branches        

       0.964758684 seconds time elapsed


---


### compiler : `gcc`
### title : `[10/11/12/13 Regression] does not vectorize generic inplace integer operation`
### open_at : `2017-03-26T14:44:36Z`
### last_modified_date : `2023-03-24T07:35:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80198
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `6.3.0`
### severity : `normal`
### contents :
In the following code the generic function does not vectorize when provided with arguments that do an inplace operation. GCC does emit vector code but the code is only executed in out of place operations where |a - b| > 16.
This is despite the explicit hint to the compiler that the two pointers are the same.

As the explicit inplace variant does execute the same vectorized code there should be no issue with the method chosen and GCC should allow the inplace operations to choose the vector path.

void __attribute__((noinline)) fun(int * a, int * b, int c)
{
    int i;
    if (a == b) {
	for (i=0; i < 256; i++) {
	    a[i] = b[i] | c;
	}
    }
    else {
	for (i=0; i < 256; i++) {
	    a[i] = b[i] | c;
	}
    }
}


void __attribute__((noinline)) inplace(int * a, int c)
{
    int i;
    for (i=0; i < 256; i++) {
        a[i] = a[i] | c;
    }
}

int main()
{
    int a[256];
    generic(a, a, 0);
    inplace(a, 0);
}


$ gcc -v
Using built-in specs.
COLLECT_GCC=/usr/bin/gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/6/lto-wrapper
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 6.2.0-5ubuntu12' --with-bugurl=file:///usr/share/doc/gcc-6/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-6 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-6-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-6-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-6-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 6.2.0 20161005 (Ubuntu 6.2.0-5ubuntu12) 

$ gcc test.c  -ftree-vectorize -g -O2


---


### compiler : `gcc`
### title : `VN has trouble with conditional equivalences (DOM/FRE)`
### open_at : `2017-03-27T11:41:26Z`
### last_modified_date : `2023-08-09T22:31:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80215
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0.1`
### severity : `enhancement`
### contents :
Currently DOM doesn't handle either redundancy in foo or bar while FRE only
handles the first.

int d, e;
int foo (int a, int c)
{
  e = a * c;
  if (a == c)
    return a * c;
  return 0;
}
int bar (int a, int c)
{
  e = a * c;
  if (a == c)
    return c * c;
  return 0;
}

another example is

int e;
int baz (int a, int c)
{
  e = a + c;
  if (a == c)
    return c + c;
  return 0;
}

with the additional complication of c + c -> 2 * c canonicalization.

The issue is unifying of two values (plus eventual followup simplifications
that might be possible).  VN algorithms generally do not handle this well.
For DOM-VRP I played with re-propagating starting from uses of the
newly recorded equivalency.


---


### compiler : `gcc`
### title : `sparse access to Array of structures does not vectorize using gathers`
### open_at : `2017-03-29T07:26:39Z`
### last_modified_date : `2021-10-01T02:59:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80248
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0.1`
### severity : `normal`
### contents :
in the following example "aos" does not vectorize  while the equivalent aos2 does vectorize using vgatherdps instruction

On a slight different matter:
"soa" vectorizes and produces code that is apparently 20% faster than "aos2":
I may open a different PR with a benchmark attached...


cat simpleGather.cc
struct float3 {
  float x;
  float y;
  float z;
};
 
#define N 1024
float fx[N], g[N];
float fy[N];
float fz[N]; 
int k[N];

float3 f3[N];


void
aos (void)
{
  int i;
  for (i = 0; i < N; i++)
    g[i] = f3[k[i]].x+f3[k[i]].y+f3[k[i]].z;
}


// use gather
void
aos2 (void)
{
  float * ff = &(f3[0].x);
  int i;
  for (i = 0; i < N; i++)
    g[i] = ff[3*k[i]]+ff[3*k[i]+1]+ff[3*k[i]+2];
}


// use gather
void
soa (void)
{
  int i;
  for (i = 0; i < N; i++)
    g[i] = fx[k[i]]+fy[k[i]]+fz[k[i]];
}

[innocent@vinavx3 vectorize]$ c++ -Ofast -Wall -march=haswell -S simpleGather.cc -fopt-info-vec
simpleGather.cc:31:17: note: loop vectorized
simpleGather.cc:41:17: note: loop vectorized
[innocent@vinavx3 vectorize]$ c++ -v
Using built-in specs.
COLLECT_GCC=c++
COLLECT_LTO_WRAPPER=/afs/cern.ch/work/i/innocent/public/w5/bin/../libexec/gcc/x86_64-pc-linux-gnu/7.0.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-trunk//configure --prefix=/afs/cern.ch/user/i/innocent/w5 -enable-languages=c,c++,lto,fortran --enable-lto -enable-libitm -disable-multilib
Thread model: posix
gcc version 7.0.1 20170326 (experimental) [trunk revision 246485] (GCC)


---


### compiler : `gcc`
### title : `Worse code generated compared to clang with modulus operation`
### open_at : `2017-03-30T08:36:03Z`
### last_modified_date : `2021-08-15T23:40:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80261
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `6.3.1`
### severity : `enhancement`
### contents :
....................
#include <cstdio>
#include <stdint.h>

typedef unsigned int uint;

uintptr_t g_array[131];
uintptr_t foo1(void *ptr)
{
    constexpr size_t size = sizeof(g_array) / sizeof(uintptr_t);
    return g_array[uint(uintptr_t(ptr)) % size];
}

uintptr_t foo2(void *ptr)
{
    constexpr size_t size = sizeof(g_array) / sizeof(uintptr_t);
    return g_array[uintptr_t(ptr) % size];
}
....................
This code produce more worse assembly code for foo2() compared to clang.

----------- x86-64 GCC 6.3(-O2 -std=c++11) -----------
foo1(void*):
        mov     eax, edi
        mov     edx, -98358029
        mul     edx
        shr     edx, 7
        imul    edx, edx, 131
        sub     edi, edx
        mov     rax, QWORD PTR g_array[0+rdi*8]
        ret
foo2(void*):
        mov     rax, rdi
        movabs  rdx, 281629680514649643
        mul     rdx
        shr     rdx
        mov     rax, rdx
        sal     rax, 6
        add     rax, rdx
        lea     rax, [rdx+rax*2]
        sub     rdi, rax
        mov     rax, QWORD PTR g_array[0+rdi*8]
        ret
g_array:
        .zero   1048

------------------------------------------------------

----------- x86-64 CLang 3.9(-O2 -std=c++11) ---------
foo1(void*):                              # @foo1(void*)
        mov     ecx, edi
        movabs  rdx, 281629680514649643
        mov     rax, rcx
        mul     rdx
        shr     rdx
        imul    rax, rdx, 131
        sub     rcx, rax
        mov     rax, qword ptr [8*rcx + g_array]
        ret

foo2(void*):                              # @foo2(void*)
        movabs  rcx, 281629680514649643
        mov     rax, rdi
        mul     rcx
        shr     rdx
        imul    rax, rdx, 131
        sub     rdi, rax
        mov     rax, qword ptr [8*rdi + g_array]
        ret

g_array:
        .zero   1048
------------------------------------------------------

https://godbolt.org/g/PEyju2


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] bad SIMD register allocation`
### open_at : `2017-04-02T08:37:02Z`
### last_modified_date : `2023-07-07T10:32:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80283
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `middle-end`
### version : `7.0.1`
### severity : `normal`
### contents :
Michael S reported the following testcase on
http://www.realworldtech.com/forum/?threadid=166719&curpostid=166719

markus@x4 tmp % cat vec_reg.c
#include <immintrin.h>

#define MM_FMADD(a, b, c) _mm256_add_ps(_mm256_mul_ps((a), (b)), (c))

void foo(const float *A, const __m256 *B, __m256 *C, int kSteps) {
  __m256 acc00 = _mm256_setzero_ps();
  __m256 acc10 = _mm256_setzero_ps();
  __m256 acc01 = _mm256_setzero_ps();
  __m256 acc11 = _mm256_setzero_ps();
  __m256 acc02 = _mm256_setzero_ps();
  __m256 acc12 = _mm256_setzero_ps();
  __m256 acc03 = _mm256_setzero_ps();
  __m256 acc13 = _mm256_setzero_ps();
  __m256 acc04 = _mm256_setzero_ps();
  __m256 acc14 = _mm256_setzero_ps();

  for (int k = 0; k < kSteps; ++k) {
    __m256 a, b0, b1;
    b0 = B[0];
    b1 = B[1];
    B += 2;

    a = _mm256_broadcast_ss(&A[4 * 0 + 0]);
    acc00 = MM_FMADD(a, b0, acc00);
    acc10 = MM_FMADD(a, b1, acc10);

    a = _mm256_broadcast_ss(&A[4 * 1 + 0]);
    acc01 = MM_FMADD(a, b0, acc01);
    acc11 = MM_FMADD(a, b1, acc11);

    a = _mm256_broadcast_ss(&A[4 * 2 + 0]);
    acc02 = MM_FMADD(a, b0, acc02);
    acc12 = MM_FMADD(a, b1, acc12);

    a = _mm256_broadcast_ss(&A[4 * 3 + 0]);
    acc03 = MM_FMADD(a, b0, acc03);
    acc13 = MM_FMADD(a, b1, acc13);

    a = _mm256_broadcast_ss(&A[4 * 4 + 0]);
    acc04 = MM_FMADD(a, b0, acc04);
    acc14 = MM_FMADD(a, b1, acc14);

    b0 = B[0];
    b1 = B[1];
    B += 2;

    a = _mm256_broadcast_ss(&A[4 * 0 + 1]);
    acc00 = MM_FMADD(a, b0, acc00);
    acc10 = MM_FMADD(a, b1, acc10);

    a = _mm256_broadcast_ss(&A[4 * 1 + 1]);
    acc01 = MM_FMADD(a, b0, acc01);
    acc11 = MM_FMADD(a, b1, acc11);

    a = _mm256_broadcast_ss(&A[4 * 2 + 1]);
    acc02 = MM_FMADD(a, b0, acc02);
    acc12 = MM_FMADD(a, b1, acc12);

    a = _mm256_broadcast_ss(&A[4 * 3 + 1]);
    acc03 = MM_FMADD(a, b0, acc03);
    acc13 = MM_FMADD(a, b1, acc13);

    a = _mm256_broadcast_ss(&A[4 * 4 + 1]);
    acc04 = MM_FMADD(a, b0, acc04);
    acc14 = MM_FMADD(a, b1, acc14);

    b0 = B[0];
    b1 = B[1];
    B += 2;

    a = _mm256_broadcast_ss(&A[4 * 0 + 2]);
    acc00 = MM_FMADD(a, b0, acc00);
    acc10 = MM_FMADD(a, b1, acc10);

    a = _mm256_broadcast_ss(&A[4 * 1 + 2]);
    acc01 = MM_FMADD(a, b0, acc01);
    acc11 = MM_FMADD(a, b1, acc11);

    a = _mm256_broadcast_ss(&A[4 * 2 + 2]);
    acc02 = MM_FMADD(a, b0, acc02);
    acc12 = MM_FMADD(a, b1, acc12);

    a = _mm256_broadcast_ss(&A[4 * 3 + 2]);
    acc03 = MM_FMADD(a, b0, acc03);
    acc13 = MM_FMADD(a, b1, acc13);

    a = _mm256_broadcast_ss(&A[4 * 4 + 2]);
    acc04 = MM_FMADD(a, b0, acc04);
    acc14 = MM_FMADD(a, b1, acc14);

    b0 = B[0];
    b1 = B[1];
    B += 2;

    a = _mm256_broadcast_ss(&A[4 * 0 + 3]);
    acc00 = MM_FMADD(a, b0, acc00);
    acc10 = MM_FMADD(a, b1, acc10);

    a = _mm256_broadcast_ss(&A[4 * 1 + 3]);
    acc01 = MM_FMADD(a, b0, acc01);
    acc11 = MM_FMADD(a, b1, acc11);

    a = _mm256_broadcast_ss(&A[4 * 2 + 3]);
    acc02 = MM_FMADD(a, b0, acc02);
    acc12 = MM_FMADD(a, b1, acc12);

    a = _mm256_broadcast_ss(&A[4 * 3 + 3]);
    acc03 = MM_FMADD(a, b0, acc03);
    acc13 = MM_FMADD(a, b1, acc13);

    a = _mm256_broadcast_ss(&A[4 * 4 + 3]);
    acc04 = MM_FMADD(a, b0, acc04);
    acc14 = MM_FMADD(a, b1, acc14);

    A += 4 * 5;
  }

  C[0] = acc00;
  C[1] = acc10;
  C[2] = acc01;
  C[3] = acc11;
  C[4] = acc02;
  C[5] = acc12;
  C[6] = acc03;
  C[7] = acc13;
  C[8] = acc04;
  C[9] = acc14;
}

with -O3 -mavx the resulting code looks bad.
clang generated a much nicer tight loop without extra spills.

The issue looks related to PR79826.


---


### compiler : `gcc`
### title : `Sub-optimal code with an array of structs offsetted inside a struct global on x86/x86_64 at -O2`
### open_at : `2017-04-04T06:13:41Z`
### last_modified_date : `2021-09-05T01:39:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80301
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.0.1`
### severity : `enhancement`
### contents :
Created attachment 41114
test code

gcc -fno-asynchronous-unwind-tables -S -O2 -o test.s -c test.c

The displacement should be coded in the memory access instructions, but instead a separate addition instruction is being generated.

Assembly is from 4.9.2, but the issue shows up on at least 5.3.0, 6.1.0, and a relatively recent 7.0 build.
[...]
func:
        movl    %edi, %edx
        addq    $2, %rdx
        movl    m(,%rdx,8), %eax
        cmpl    %edi, %eax
        je      .L2
        movl    m+4(,%rdx,8), %eax
.L2:
        rep ret
[...]


---


### compiler : `gcc`
### title : `Missed optimization for common standard library case`
### open_at : `2017-04-04T19:40:24Z`
### last_modified_date : `2021-12-12T12:16:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80317
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0.1`
### severity : `enhancement`
### contents :
It's a common case in standard library and in user code to move objects to new location and then destroy object at the old location. For example GCC's std::vector::reserve has the following code:

  pointer __tmp = _M_allocate_and_copy(__n,
    _GLIBCXX_MAKE_MOVE_IF_NOEXCEPT_ITERATOR(this->_M_impl._M_start),
    _GLIBCXX_MAKE_MOVE_IF_NOEXCEPT_ITERATOR(this->_M_impl._M_finish));
  std::_Destroy(this->_M_impl._M_start, this->_M_impl._M_finish,
                _M_get_Tp_allocator()); 


The problem is that traversing exactly the same data twice is not well optimized. Minimal example:

///////////////

#include <new>

template <class T>
struct shared_ptr_like {
    T* ptr;
    shared_ptr_like(shared_ptr_like&& v) noexcept : ptr(v.ptr) { v.ptr = 0; }
    ~shared_ptr_like() { if (ptr) { delete ptr; } }
};

typedef shared_ptr_like<int> test_type;
void relocate(test_type* new_buffer, test_type* old_buffer, std::size_t size) {
    for (std::size_t i = 0; i != size; ++i) {
        new (new_buffer + i) test_type{ static_cast<test_type&&>(old_buffer[i]) };
    }
    for (std::size_t i = 0; i != size; ++i) {
        old_buffer[i].~test_type();
    }
}

///////////////

Produces assembly that traverses the loop twice, writes zeros to memory and compares memory with zeros:

relocate(shared_ptr_like<int>*, shared_ptr_like<int>*, unsigned long):
        test    rdx, rdx
        je      .L15
        push    rbp
        sal     rdx, 3
        push    rbx
        lea     r8, [rdi+rdx]
        mov     rbx, rsi
        mov     rax, rsi
        sub     rsp, 8
.L3:
        mov     rcx, QWORD PTR [rax]
        add     rdi, 8
        add     rax, 8
        mov     QWORD PTR [rdi-8], rcx
        mov     QWORD PTR [rax-8], 0
        cmp     rdi, r8
        jne     .L3
        lea     rbp, [rsi+rdx]
.L5:
        mov     rdi, QWORD PTR [rbx]
        test    rdi, rdi
        je      .L4
        mov     esi, 4
        call    operator delete(void*, unsigned long)
.L4:
        add     rbx, 8
        cmp     rbp, rbx
        jne     .L5
        add     rsp, 8
        pop     rbx
        pop     rbp
        ret
.L15:
        rep ret



Meanwhile optimal assembly for that function would be


relocate(shared_ptr_like<int>*, shared_ptr_like<int>*, unsigned long):
        test    rdx, rdx
        je      .L1
        lea     rdx, [rdi+rdx*8]
.L3:
        mov     rax, QWORD PTR [rsi]
        add     rdi, 8
        add     rsi, 8
        mov     QWORD PTR [rdi-8], rax
        cmp     rdi, rdx
        jne     .L3
.L1:
        rep ret


Note: Compiling the following code produces the optimal assembly from above:

#include <new>

template <class T>
struct shared_ptr_like {
    T* ptr;
    shared_ptr_like(shared_ptr_like&& v) noexcept : ptr(v.ptr) { v.ptr = 0; }
    ~shared_ptr_like() { if (ptr) { delete ptr; } }
};

typedef shared_ptr_like<int> test_type;
void relocate(test_type* new_buffer, test_type* old_buffer, std::size_t size) {
    for (std::size_t i = 0; i != size; ++i) {
        new (new_buffer + i) test_type{ static_cast<test_type&&>(old_buffer[i]) };
        old_buffer[i].~test_type();
    }
}


Checked on GCC 7.0.1 20170330 with flags -std=c++17 -O2


---


### compiler : `gcc`
### title : `perf of copying std::optional<trivial>`
### open_at : `2017-04-05T23:42:30Z`
### last_modified_date : `2019-07-28T07:28:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80335
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `7.0.1`
### severity : `enhancement`
### contents :
I was surprised recently, while profiling some application, to see boost::optional::assign relatively high (about 4%, while I am only using it in a single place). After checking, it seems that std::optional in libstdc++ has the same issue. The point is, copy-assigning an optional<int> involves 2 conditions (is lhs initialized, is rhs initialized) while a brutal memcpy/memmove would be perfectly safe for a trivial type like int (sanitizers and valgrind might occasionally complain about reading uninitialized memory though).

#include <optional>
typedef std::optional<int> O;
void f1(O&a, O const&b){ a=b; }
void f2(O&a, O const&b){ __builtin_memmove(&a,&b,sizeof(O)); }

yields

	cmpb	$0, 4(%rdi)
	movzbl	4(%rsi), %eax
	je	.L2
	testb	%al, %al
	jne	.L8
	movb	$0, 4(%rdi)
	ret
	.p2align 4,,10
	.p2align 3
.L2:
	testb	%al, %al
	je	.L9
	movl	(%rsi), %eax
	movb	$1, 4(%rdi)
	movl	%eax, (%rdi)
	ret
	.p2align 4,,10
	.p2align 3
.L9:
	ret
	.p2align 4,,10
	.p2align 3
.L8:
	movl	(%rsi), %eax
	movl	%eax, (%rdi)
	ret

vs

	movq	(%rsi), %rax
	movq	%rax, (%rdi)
	ret

I am wondering under what conditions we could implement copying this way:
- type small enough: don't want an expensive copy for an empty optional
- type "trivial": no need to run the destructor, copy assignment, etc
- not using a sanitizer, not going to use valgrind: that cannot really be tested...


---


### compiler : `gcc`
### title : `useless outermost conversions not fully elided by genmatch generated code`
### open_at : `2017-04-06T11:32:43Z`
### last_modified_date : `2021-08-15T01:07:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80342
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.0.1`
### severity : `normal`
### contents :
if you write

(simplify
 (plus @1 @2)
 (convert (plus @1 @2)))

then genmatch generates

    *res_code = NOP_EXPR;
    {
      tree ops1[2], res;
      ops1[0] = captures[0];
      ops1[1] = captures[1];
      code_helper tem_code = PLUS_EXPR;
      tree tem_ops[3] = { ops1[0], ops1[1] };
      gimple_resimplify2 (lseq, &tem_code, TREE_TYPE (ops1[0]), tem_ops, valueize);
      res = maybe_push_res_to_seq (tem_code, TREE_TYPE (ops1[0]), tem_ops, lseq);
      if (!res) return false;
      res_ops[0] = res;
    }
    gimple_resimplify1 (lseq, res_code, type, res_ops, valueize);

which means the (plus @1 @2) is pushed to seq even if the conversion ends up
being simplified away.  This results in forwprop changes like

+  _1 = x_6(D) - _10;
+  _7 = _1;

and also in missed optimizations in SCCVN which uses seq == NULL and thus
won't survive this.

Individual patterns can be adjusted to provide special-casing for the
case of useless conversions but a genmatch solution is prefered.  Note
it won't come cheap (it will duplicate the transform code).


---


### compiler : `gcc`
### title : `Improve __builtin_shuffle on AVX512F`
### open_at : `2017-04-07T16:33:07Z`
### last_modified_date : `2021-08-10T10:42:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80355
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `7.0`
### severity : `enhancement`
### contents :
As mentioned in https://gcc.gnu.org/ml/gcc-patches/2017-04/msg00375.html
we emit inefficient code for:
typedef long long V __attribute__((vector_size (64)));
typedef int W __attribute__((vector_size (64)));
W f0 (W x) {
  return __builtin_shuffle (x, (W) { 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7 });
}
V f1 (V x) {
  return __builtin_shuffle (x, (V) { 4, 5, 6, 7, 0, 1, 2, 3 });
}
e.g.
        vmovdqa64       .LC0(%rip), %zmm1
        vpermd  %zmm0, %zmm1, %zmm0
or
        vmovdqa64       .LC1(%rip), %zmm1
        vpermq  %zmm0, %zmm1, %zmm0
while we could use vpshufi64x2 instruction instead, which has just immediate.


---


### compiler : `gcc`
### title : `non-optimal handling of copying a std::complex<double>`
### open_at : `2017-04-09T05:15:16Z`
### last_modified_date : `2023-05-15T07:25:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80372
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.0`
### severity : `enhancement`
### contents :
While copying a std::complex<double> from a memory location to another, four movsd operations are used. However it is possible to use two movups, which are faster (at least on some hardware) and need less memory (36 bytes for movsd-version, but only 16 the the movups-version).

Consider the following example:

    #include <complex>  
    void get(std::complex<double> *res){
       res[1]=res[0];
    }

is compiled to:

        movsd   (%rdi), %xmm0
        movsd   %xmm0, 16(%rdi)
        movsd   8(%rdi), %xmm0
        movsd   %xmm0, 24(%rdi)
        ret

but could be:
 
        movups  (%rdi), %xmm0
        movups  %xmm0, 16(%rdi)
        ret

That is in fact, what clang and icc17 do.


---


### compiler : `gcc`
### title : `Premature optimization with unsigned`
### open_at : `2017-04-11T18:46:57Z`
### last_modified_date : `2021-12-22T09:04:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80399
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `middle-end`
### version : `7.0`
### severity : `enhancement`
### contents :
Take the following two functions:
unsigned f(unsigned t)
{
  t = (t - 1)*16;
  t = t/16;
  return t;
}
unsigned g(unsigned t)
{
  t = t - 1;
  t = t*16;
  t = t/16;
  return t;
}


They should produce the same code but they don't.
On aarch64 for an example we produce:
f:
        mov     w1, 268435455
        add     w0, w0, w1
        and     w0, w0, w1
        ret
....
g:
        sub     w0, w0, #1
        and     w0, w0, 268435455
        ret


g is better than f here since we have only one dependent instruction instead of 2.


---


### compiler : `gcc`
### title : `Missed optimization on x86/x86_64`
### open_at : `2017-04-11T22:44:09Z`
### last_modified_date : `2021-08-03T03:39:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80402
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
Created attachment 41181
sample code

A statement like "if(!(a & 0xF) || (b & (1U << 6)))" could be compiled to a "test","bt" pair followed by a single conditional branch/move instruction, but gcc currently compiles it to a combination of two tests and conditional branch/move instructions.

From https://software.intel.com/sites/default/files/managed/ad/01/253666-sdm-vol-2a.pdf
Page 3-114, BT: "The ZF flag is unaffected."

Old versions(prior to early 2010, as far as I can tell) of the manual had the flag as being undefined, so it may be prudent to talk to Intel and AMD engineers before implementing this optimization.

Attached is sample code that includes the optimal form via inline assembly(test4() function).


---


### compiler : `gcc`
### title : `Reduced false positive test case for -Warray-bounds with -O3`
### open_at : `2017-04-12T03:07:33Z`
### last_modified_date : `2021-08-29T04:22:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80406
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `6.2.1`
### severity : `normal`
### contents :
Created attachment 41182
False positive, compile with: gcc -Warray-bounds -O3 -c gcc_6-2-1_compiler_bug.c

This is yet another report of an -Warray-bounds false positive when compiling with -O3.

Scanned possible dupes and there are obviously lots similar still open (e.g., 63213, 66974, 77291), but none that seem (to me) exactly like the case I encountered compiling BRL-CAD with 6.2.1 and 5.4.0.  I've reduced the code in question and annotated the logic flow, attached.

Warning is not issued below -O3.  Warning is not issued with a boundary test (npts <= MAX_HITS) in the "WARN PARENT LOOP".  In fact, changing nearly any aspect of this simplified case seems to make the warning go away.

Particularly interesting, a warning is not issued if "ASSURANCE B" is enabled on this simplified example, though it DOESN't quell our production code.  This is interesting because it means the optimizer is (probably) correctly identifying Assurance B as dead/unnecessary code, but then subsequently warns in the WARN LOOP.


---


### compiler : `gcc`
### title : `Improve merging of identical branches`
### open_at : `2017-04-12T09:22:00Z`
### last_modified_date : `2021-11-29T20:39:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80410
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.0.1`
### severity : `enhancement`
### contents :
Consider the following code:

  va_list ap;
  void *result;

  va_start(ap, ptr_type);

  switch (ptr_type) {
    case 0: result = va_arg(ap, int*); break;
    case 1: result = va_arg(ap, char*); break;
    case 2: result = va_arg(ap, short*); break;
    default: result = va_arg(ap, void*); break;
  }

  va_end(ap);

GCC correctly recognizes that the assembly code for the cases 0, 1, 2 is identical, which is the difficult part (the source code for the branches is different and cannot be merged by the programmer unless https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80409 is resolved). At the time of writing, the Compiler Explorer trunk version produces:

        movq    %rax, -56(%rsp)
        je      .L15
        cmpl    $2, %edi
        je      .L15
        testl   %edi, %edi
        je      .L16
.L15:
        movq    -56(%rsp), %rax
.L16:

(Compiler Explorer link: https://godbolt.org/g/0NvnSX )

It would be nice if GCC was able to omit all the conditional branches, since they all lead to doing the same thing at the assembly level.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Compiler regression for long-add case.`
### open_at : `2017-04-22T19:52:37Z`
### last_modified_date : `2023-08-03T22:08:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80491
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.0.1`
### severity : `normal`
### contents :
Simple long addition program.

struct pair {
  uint64_t low;
  uint64_t hi;
};

pair add(pair& a, pair& b) {
 pair s;
 s.low = a.low + b.low;
 s.hi = a.hi + b.hi + (s.low < a.low); //carry
 return s;
}

Old versions of GCC produced adequate code:

$ gcc -S -O3 test.cc -o-
	.file	"test.cc"
	.text
	.p2align 4,,15
	.globl	_Z3addR4pairS0_
	.type	_Z3addR4pairS0_, @function
_Z3addR4pairS0_:
.LFB4:
	.cfi_startproc
	movq	(%rdi), %rax
	movq	8(%rsi), %rdx
	xorl	%ecx, %ecx
	addq	8(%rdi), %rdx
	addq	(%rsi), %rax
	setc	%cl
	addq	%rcx, %rdx
	ret
	.cfi_endproc
.LFE4:
	.size	_Z3addR4pairS0_, .-_Z3addR4pairS0_
	.ident	"GCC: (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4"
	.section	.note.GNU-stack,"",@progbits

New version of gcc produces comparison and jump:
$ gcc -S -O3 test.cc -o-
	.file "test.cc"
	.text
	.p2align 4,,15
	.globl _Z3addR4pairS0_
	.type _Z3addR4pairS0_, @function
	_Z3addR4pairS0_:
	.LFB4:
	.cfi_startproc
	xorl %ecx, %ecx
	movq (%rsi), %rax
	addq (%rdi), %rax
	jc .L5
.L2:
	movq 8(%rsi), %rdx
	addq 8(%rdi), %rdx
	addq %rcx, %rdx
	ret
.L5:
	movl $1, %ecx
	jmp .L2
.cfi_endproc
.LFE4:
.size _Z3addR4pairS0_, .-_Z3addR4pairS0_
.ident "GCC: (GNU) 7.0.1 20170307 (experimental)"
.section .note.GNU-stack,"",@progbits


For comparison - clang produces perfect code here:
$ clang++ -S -O3 test1.cc -o- 
	.text
	.file	"test1.cc"
	.globl	_Z3addR4pairS0_
	.p2align	4, 0x90
	.type	_Z3addR4pairS0_,@function
_Z3addR4pairS0_:                        # @_Z3addR4pairS0_
	.cfi_startproc
# BB#0:
	movq	(%rsi), %rax
	movq	8(%rsi), %rdx
	addq	8(%rdi), %rdx
	addq	(%rdi), %rax
	adcq	$0, %rdx
	retq
.Lfunc_end0:
	.size	_Z3addR4pairS0_, .Lfunc_end0-_Z3addR4pairS0_
	.cfi_endproc

	.ident	"Android clang version 4.0.285906  (based on LLVM 4.0.285906)"
	.section	".note.GNU-stack","",@progbits


---


### compiler : `gcc`
### title : `[missed optimization] constant propagation through Intel intrinsics`
### open_at : `2017-04-25T13:58:53Z`
### last_modified_date : `2023-08-22T04:53:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80517
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Related: #55894

Testcase:
#include <x86intrin.h>

int f() {
    __m128i x{};
    x = _mm_cmpeq_epi16(x, x);
    return _pext_u32(_mm_movemask_epi8(x), 0xaaaa);
}

(compile with `-mbmi2 -O3 -std=c++14`)

See also https://godbolt.org/g/n92wEc

This should compile to
f():
  movl $0xff, %eax
  ret

Clang already implements constant propagation for the testcase except for `pext` (see godbolt link).

This is just a precursor to the following testcase:
#include <x86intrin.h>

auto g(__m128i x, __m128i y) {
    __m128i mask0 = _mm_cmpeq_epi16(x, y);
    auto bits = _pext_u32(_mm_movemask_epi8(mask0), 0xaaaa);
    __m128i mask = _mm_set1_epi16(bits);
    mask = _mm_and_si128(mask, _mm_setr_epi16(1, 2, 4, 8, 16, 32, 64, 128));
    mask = _mm_cmpeq_epi16(mask, _mm_setzero_si128());
    mask = _mm_xor_si128(mask, _mm_cmpeq_epi16(mask, mask));
    return mask;
}

This should compile to
f():
  vpcmpeqw %xmm0, %xmm1, %xmm0
  ret

I.e. The xmm mask `mask0` is converted to a bitmask and back to an xmm mask. Similar patterns exist for all arithmetic types for SSE and AVX. If you like, I can produce a list of testcases for all vector element types for SSE and AVX.

Motivation: An ABI-stable mask type for x86 requires a storage format that is independent of the ISA extensions available on the specific x86 CPU. In light of AVX512, the most sensible choice for such a mask storage is std::bitset<N>. This is a natural fit for AVX512 masks but requires frequent conversion to/from xmm and ymm mask when AVX/SSE registers are modified. If the above is optimized, it would go a long way to reducing the cost of using the ABI-stable types.
Reference: https://wg21.link/p0214


---


### compiler : `gcc`
### title : `optimize if(p)free(p) with -Os`
### open_at : `2017-04-25T20:53:33Z`
### last_modified_date : `2021-11-06T23:54:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80519
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
It is quite common to end up with a call to free protected by a check that the pointer is not 0. This can be done because of old implementations of free that did not handle 0 properly (I certainly hope those have been gone for a while), for optimization if the 0 case is common, because of abstraction (C++ (de)allocators are not generally specified to accept 0, so we protect the call even when the allocator is the default one), etc.

I propose to optimize out the check in some cases, and particularly when optimizing for size. It might also make sense to remove the check when the frequency of the shortcut is too low.

void f(void*p){
  if(p)
    __builtin_free(p);
}


---


### compiler : `gcc`
### title : `warning on pointer access after free`
### open_at : `2017-04-26T18:12:23Z`
### last_modified_date : `2023-10-21T10:46:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80532
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
Using the value of a pointer that points to an object whose lifetime has ended is undefined, and a common source of bugs.  In the following test case, function free_list shows an example of one such bug (courtesy of Kernighan & Ritchie) where a freed pointer is dereferenced.

Function foobar shows another example.  That example is interesting because it shows that GCC is smart enough to eliminate the first call to memset (because the lifetime of the object the pointer points ends with the subsequent call to free) but it doesn't eliminate the second call to free or the second call to memset, even though that operate on an object whose lifetime has ended and will almost certainly corrupt the heap.  GCC also doesn't warn on those calls.  It would be a useful feature if GCC did both: warn and eliminate the undefined calls/accesses.


$ cat c.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout c.c
#include <stdlib.h>
#include <string.h>

struct node { struct node *next; int value; };

void free_list (struct node *head)
{
  struct node *p;

  for (p = head; p != 0; p = p->next)
    free (p);
}

void foobar (void *p, unsigned n)
{
  memset (p, 0, n);
  free (p);
  free (p);
  memset (p, 0, n);
}

;; Function free_list (free_list, funcdef_no=14, decl_uid=2614, cgraph_uid=14, symbol_order=14)

Removing basic block 5
Removing basic block 6
Removing basic block 7
Removing basic block 8
free_list (struct node * head)
{
  struct node * p;

  <bb 2> [15.00%]:
  if (head_3(D) != 0B)
    goto <bb 3>; [85.00%]
  else
    goto <bb 4>; [15.00%]

  <bb 3> [85.00%]:
  # p_9 = PHI <p_6(3), head_3(D)(2)>
  free (p_9);
  p_6 = p_9->next;
  if (p_6 != 0B)
    goto <bb 3>; [85.00%]
  else
    goto <bb 4>; [15.00%]

  <bb 4> [15.00%]:
  return;

}



;; Function foobar (foobar, funcdef_no=15, decl_uid=2622, cgraph_uid=15, symbol_order=15)

foobar (void * p, unsigned int n)
{
  long unsigned int _1;

  <bb 2> [100.00%]:
  _1 = (long unsigned int) n_2(D);
  free (p_4(D));
  free (p_4(D));
  memset (p_4(D), 0, _1); [tail call]
  return;

}


tmp$


---


### compiler : `gcc`
### title : `missing -Wformat-overfow on POSIX directives with the apostrophe flag`
### open_at : `2017-04-26T20:10:18Z`
### last_modified_date : `2021-09-15T01:56:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80535
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
POSIX specifies that the meaning of the apostrophe flag character in a printf conversion specification is as follows:

  The integer portion of the result of a decimal conversion (%i, %d, %u, %f, %F, %g, or %G) shall be formatted with thousands' grouping characters.

That means that every valid conversion specification that uses the apostrophe must result in no fewer bytes on output than the corresponding specification without the apostrophe.  In addition, since the thousands' grouping character must be a single (possibly multibyte) character, it also places an upper bound on the bytes on output.  The upper bound can be assumed to be at most the number of digits (before the decimal point) minus one times MB_LEN_MAX.

The test case below shows that GCC doesn't take advantage of these constraints, either to detect buffer overflow, or to set the range on the return value from the sprintf function.

$ cat c.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout c.c
char d[1];

void f (void)
{
  int n = __builtin_sprintf (d, "%'d", 123456);
  if (n < 5)
    __builtin_abort ();
}
c.c: In function ‘f’:
c.c:5:33: warning: ISO C does not support the ''' printf flag [-Wformat=]
   int n = __builtin_sprintf (d, "%'d", 123456);
                                 ^~~~~

;; Function f (f, funcdef_no=0, decl_uid=1796, cgraph_uid=0, symbol_order=1)

f ()
{
  int n;

  <bb 2> [100.00%]:
  n_3 = __builtin_sprintf (&d, "%\'d", 123456);
  if (n_3 <= 4)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [0.04%]:
  __builtin_abort ();

  <bb 4> [99.96%]:
  return;

}


---


### compiler : `gcc`
### title : `missing -Wformat-overflow on POSIX %C conversion specification`
### open_at : `2017-04-26T20:22:46Z`
### last_modified_date : `2019-01-17T16:52:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80537
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
POSIX specifies that the %C conversion specification is equivalent to %lc (used to format wide characters).  The -Wformat-overflow doesn't recognize the %C directive and doesn't issue warnings for it.  Worse, the directive has the effect of disabling the checking for the rest of the format string.

$ cat c.c && gcc -O2 -S -Wall -Wextra -Wpedantic c.c
char d[1];

void f (void)
{
  __builtin_sprintf (d + 1, "%lc foo", 'a');
}

void g (void)
{
  __builtin_sprintf (d + 1, "%C bar", 'a');
}
c.c: In function ‘g’:
c.c:10:31: warning: ISO C does not support the ‘%C’ gnu_printf format [-Wformat=]
   __builtin_sprintf (d + 1, "%C bar", 'a');
                               ^
c.c: In function ‘f’:
c.c:5:30: warning: ‘%lc’ directive writing up to 1 bytes into a region of size 0 [-Wformat-overflow=]
   __builtin_sprintf (d + 1, "%lc foo", 'a');
                              ^~~
c.c:5:3: note: ‘__builtin_sprintf’ output between 5 and 6 bytes into a destination of size 0
   __builtin_sprintf (d + 1, "%lc foo", 'a');
   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


---


### compiler : `gcc`
### title : `Missed optimization: std::array data is aligned if array is aligned`
### open_at : `2017-04-29T08:23:27Z`
### last_modified_date : `2023-01-14T10:37:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80561
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `6.3.0`
### severity : `normal`
### contents :
In the following code, GCC fails to recognize that the data inside a std::array has the same alignment guarantees as the array itself.  The result is that using std::array instead of a C-style array carries a significant runtime penalty, as the alignment is checked unnecessarily and code is generated for the unaligned case which should never be used.  I tested this using:

    g++ -std=c++14 -O3 -march=haswell

GCC 6.1, 6.3 and 7 all fail to optimize this.  Clang 3.7 through 4.0 optimizes it as expected.

In the code below, you can swap the comment on the two typedefs to confirm that GCC properly optimizes the C-style array.

The optimal code is 4 vmovupd, 2 vaddpd, and 1 vzeroupper.  The suboptimal code is 73 instructions including 7 branches.

This was discussed on Stack Overflow: http://stackoverflow.com/questions/43651923

---

#include <array>

static constexpr size_t my_elements = 8;

typedef std::array<double, my_elements> Vec __attribute__((aligned(32)));
// typedef double Vec[my_elements] __attribute__((aligned(32)));

void func(Vec& __restrict__ v1, const Vec& v2)
{
    for (unsigned i = 0; i < my_elements; ++i)
    {
        v1[i] += v2[i];
    }
}


---


### compiler : `gcc`
### title : `no use of avx vmovups on ymm registry in set and copy`
### open_at : `2017-04-29T16:54:10Z`
### last_modified_date : `2021-08-03T03:38:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80566
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
in this example
#include <cstring>
int * foo() {
  int * p = new int[16];
  memset(p,0,16*sizeof(int));
  return p;
}
int * foo(int * q) {
  int * p = new int[16];
  memcpy(q,p,16*sizeof(int));
  return p;
}

gcc does not make use of vmovups on ymm registry 
( c++ -O3 -Wall -march=haswell -S)
while (according to gcc.godbolt.org) clang 4.0 does https://godbolt.org/g/qnX975


---


### compiler : `gcc`
### title : `x86 -mavx256-split-unaligned-load (and store) is affecting AVX2 code, but probably shouldn't be.`
### open_at : `2017-04-29T21:12:12Z`
### last_modified_date : `2020-01-22T22:55:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80568
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `7.0`
### severity : `normal`
### contents :
Created attachment 41285
bswap16.cc

gcc7 (or at least the gcc8 snapshot on https://godbolt.org/g/ZafCE0) is now splitting unaligned loads/stores even for AVX2 integer, where gcc6.3 didn't.

I think this is undesirable by default, because some projects probably build with -mavx2 but fail to use -mtune=haswell (or broadwell or skylake).  For now,
Intel CPUs that do well with 32B unaligned loads are probably the most common AVX2-supporting CPUs.

IDK what's optimal for Excavator or Zen.  Was this an intentional change to make those tune options work better for those CPUs?

I would suggest that -mavx2 should imply -mno-avx256-split-unaligned-load (and -store) for -mtune=generic.  Or if that's too ugly (having insn set selection affect tuning), then maybe just revert to the previous behaviour of having integer loads/store not be split the way FP loads/stores are.

 The conventional wisdom is that unaligned loads are just as fast as aligned when the data does happen to be aligned at run-time.  Splitting this way badly breaks that assumption.  It's inconvenient/impossible to portably communicate to the compiler that it should optimize for the case where the data is aligned, even if that's not guaranteed, so loadu / storeu are probably used in lots of code that normally runs on aligned data.

Also, gcc doesn't always figure out that a hand-written scalar prologue does leave the pointer aligned for a vector loop.  (And since programmers expect it not to matter, they may still use `_mm256_loadu_si256`).  I reduced some real existing code that a colleague wrote into a test-case for this: https://godbolt.org/g/ZafCE0, also attached.    If using potentially-overlapping first/last vectors instead of scalar loops, it might use loadu just to avoid duplicating a helper function.


----

For an example of affected code, consider an endian-swap function that uses this (inline) function in its inner loop.  The code inside the loop matches what we get for compiling it stand-alone, so I'll just show that:

#include <immintrin.h>
// static inline
void swap256(char* addr, __m256i mask) {
	__m256i vec = _mm256_loadu_si256((__m256i*)addr);
	vec = _mm256_shuffle_epi8(vec, mask);
	_mm256_storeu_si256((__m256i*)addr, vec);
}


gcc6.3 -O3 -mavx2:
        vmovdqu (%rdi), %ymm1
        vpshufb %ymm0, %ymm1, %ymm0
        vmovdqu %ymm0, (%rdi)

g++ (GCC-Explorer-Build) 8.0.0 20170429 (experimental)  -O3 -mavx2
        vmovdqu (%rdi), %xmm1
        vinserti128     $0x1, 16(%rdi), %ymm1, %ymm1
        vpshufb %ymm0, %ymm1, %ymm0
        vmovups %xmm0, (%rdi)
        vextracti128    $0x1, %ymm0, 16(%rdi)


---


### compiler : `gcc`
### title : `auto-vectorizing int->double conversion should use half-width memory operands to avoid shuffles, instead of load+extract`
### open_at : `2017-04-30T06:37:22Z`
### last_modified_date : `2021-09-27T03:46:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80570
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
When auto-vectorizing int->double conversion, gcc loads a full-width vector into a register and then unpacks the upper half to feed (v)cvtdq2pd.  e.g. with AVX, we get a 256b load and then vextracti128.

It's even worse with an unaligned src pointer with -mavx256-split-unaligned-load, where it does vinsertf128 -> vextractf128, without ever doing anything with the full 256b vector!

On Intel SnB-family CPUs, this will bottleneck the loop on port5 throughput, because VCVTDQ2PD reg -> reg needs a port5 uop as well as a port1 uop.  (And vextracti128 can only run on the shuffle unit on port5).

VCVTDQ2PD with a memory source operand doesn't need the shuffle port at all on Intel Haswell and later, just the FP-add unit and a load, so it's a much better choice.  (Throughput of one per clock on Sandybridge and Haswell, 2 per clock on Skylake).  It's still 2 fused-domain uops, though, so I guess it can't micro-fuse the load according to Agner Fog's testing.  (Or 3 on SnB).

I'm pretty sure using twice as many half-width memory operands is not worse on other AVX CPUs either (AMD BD-family or Zen, or KNL), vs. max-width loads and extracting the high half.


void cvti32f64_loop(double *dp, int *ip) {
// ICC avoids the mistake when it doesn't emit a prologue to align the pointers
#ifdef __GNUC__
    dp = __builtin_assume_aligned(dp, 64);
    ip = __builtin_assume_aligned(ip, 64);
#endif
    for (int i=0; i<10000 ; i++) {
        double tmp = ip[i];
        dp[i] = tmp;
    }
}

https://godbolt.org/g/329C3P
gcc.godbolt.org's "gcc7" snapshot: g++ (GCC-Explorer-Build) 8.0.0 20170429 (experimental)

gcc -O3 -march=sandybridge
cvti32f64_loop:
        xorl    %eax, %eax
.L2:
        vmovdqa (%rsi,%rax), %ymm0
        vcvtdq2pd       %xmm0, %ymm1
        vextractf128    $0x1, %ymm0, %xmm0
        vmovapd %ymm1, (%rdi,%rax,2)
        vcvtdq2pd       %xmm0, %ymm0
        vmovapd %ymm0, 32(%rdi,%rax,2)
        addq    $32, %rax
        cmpq    $40000, %rax
        jne     .L2
        vzeroupper
        ret

gcc does the same thing for -march=haswell, but uses vextracti128.  This is obviously really silly.

For comparison, clang 4.0 -O3 -march=sandybridge -fno-unroll-loops emits:
        xorl    %eax, %eax
.LBB0_1:
        vcvtdq2pd       (%rsi,%rax,4), %ymm0
        vmovaps %ymm0, (%rdi,%rax,8)
        addq    $4, %rax
        cmpq    $10000, %rax            # imm = 0x2710
        jne     .LBB0_1
        vzeroupper
        retq

This should come close to one 256b store per clock (on Haswell), even with unrolling disabled.

----

With -march=nehalem, gcc gets away with it for this simple not-unrolled loop (without hurting throughput I think), but only because this strategy effectively unrolls the loop (doing two stores per add + cmp/jne), and Nehalem can run shuffles on two execution ports (so the pshufd can run on port1, while the cvtdq2pd can run on ports 1+5).  So it's 10 fused-domain uops per 2 stores instead of 5 per 1 store.  Depending on how the loop buffer handles non-multiple-of-4 uop counts, this might be a wash.  (Of course, with any other work in the loop, or with unrolling, the memory-operand strategy is much better).

CVTDQ2PD's memory operand is only 64 bits, so even the non-AVX version doesn't fault if misaligned.

------

It's even more horrible without aligned pointers, when the sandybridge version (which splits unaligned 256b loads/stores) uses vinsertf128 to emulate a 256b load, and then does vextractf128 right away:

     inner_loop:   # gcc8 -march=sandybridge without __builtin_assume_aligned
        vmovdqu (%r8,%rax), %xmm0
        vinsertf128     $0x1, 16(%r8,%rax), %ymm0, %ymm0
        vcvtdq2pd       %xmm0, %ymm1
        vextractf128    $0x1, %ymm0, %xmm0
        vmovapd %ymm1, (%rcx,%rax,2)
        vcvtdq2pd       %xmm0, %ymm0
        vmovapd %ymm0, 32(%rcx,%rax,2)

This is obviously really really bad, and should probably be checked for and avoided in case there are things other than int->double autovec that could lead to doing this.

---

With -march=skylake-avx512, gcc does the AVX512 version of the same thing: zmm load and then extra the upper 256b


---


### compiler : `gcc`
### title : `AVX allows multiple vcvtsi2ss/sd (integer -> float/double) to reuse a single dep-breaking vxorps, even hoisting it out of loops`
### open_at : `2017-04-30T08:29:54Z`
### last_modified_date : `2019-02-22T21:30:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80571
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
See also more discussion on a clang bug about what's optimal for scalar int->fp conversion (https://bugs.llvm.org/show_bug.cgi?id=22024#c11).

The important point is that using vcvtsi2sd/ss with a src vector reg different from the destination reg allows reusing the same "safe" source to avoid false dependencies, using of one vxorps for multiple conversions.  (Even hoisted out of loops).

// see https://godbolt.org/g/c9k4SH for gcc8, clang4, icc17, and MSVC CL
void cvti64f64_loop(double *dp, long long *ip) {
    for (int i=0; i<1000 ; i++) {
        dp[i] = ip[i];
        // int64 can't vectorize without AVX512
    }
}


compiles with gcc6.3.1 and gcc8 20170429 -O3 -march=haswell:

cvti64f64_loop:
        xorl    %eax, %eax
.L6:
        vxorpd  %xmm0, %xmm0, %xmm0
        vcvtsi2sdq      (%rsi,%rax), %xmm0, %xmm0
        vmovsd  %xmm0, (%rdi,%rax)
        addq    $8, %rax
        cmpq    $8000, %rax
        jne     .L6
        ret

But it could compile to

        xorl    %eax, %eax
        vxorpd  %xmm1, %xmm1, %xmm1    #### Hoisted out of the loop
.L6:
        vcvtsi2sdq      (%rsi,%rax), %xmm1, %xmm0
        vmovsd  %xmm0, (%rdi,%rax)
        addq    $8, %rax
        cmpq    $8000, %rax
        jne     .L6
        ret

----

This trick requires AVX, of course.

If the loop needs a vector constant, you can use that instead of a specially-zeroed register.  (The upper elements don't have to be 0.0 for ...ss instructions, and the x86-64 SysV ABI allows passing scalar float/double args with garbage (not zeros) in the high bytes.  This already happens in some code, so library implementers already have to avoid assuming that high elements are always clear, even with hand-written asm).

clang already hoists vxorps out of loops it, but only by looking for "dead" registers, not by reusing a reg holding a constant.  (e.g. enabling those multiplies in the godbolt link to use up all the xmm regs gets clang to put a vxorps in the loop instead of merging into a reg holding one of them.)


Using a constant has the downside that loading the constant might cache-miss, delaying a bunch of loop-setup work from happening (and loop-setup is where int->float is probably most common).  So maybe it would be best to vxorps-zero a register and use it for int->FP conversion if several other instructions depend on that before any depend on the constant.  The zeroed register can later have a constant loaded into it or whatever, and use that as a safe source register inside the loop.

Or if there's a constant that's about to be used with the result of the int->fp conversion, it's maybe not bad to load that constant and then use that xmm reg as the merge-target for a scalar conversion.  If OOO execution can do the load early (and it doesn't miss in cache), then there's no extra latency.  Or if it does miss, then there's only an extra cycle or two of latency for that dependency chain vs. spending an instruction on vxorpd-zeroing a target to convert into, separate from loading the constant.



----

This trick works for non-loops, of course:

void cvti32f32(float *A, float *B, int x, int y) {
    *B = y;
    *A = x;
}

currently compiles to (-O3 -march=haswell)

cvti32f32:
        vxorps  %xmm0, %xmm0, %xmm0
        vcvtsi2ss       %ecx, %xmm0, %xmm0
        vmovss  %xmm0, (%rsi)
        vxorps  %xmm0, %xmm0, %xmm0
        vcvtsi2ss       %edx, %xmm0, %xmm0
        vmovss  %xmm0, (%rdi)

But could compile (with no loss of safety against false deps) to

cvti32f32:
        vxorps  %xmm1, %xmm1, %xmm1     # reused for both
        vcvtsi2ss      %ecx, %xmm1, %xmm0
        vmovss  %xmm0, (%rsi)
        vcvtsi2ss      %edx, %xmm1, %xmm0   # this convert can go into xmm1 if we want to have both in regs at once
        vmovss  %xmm0, (%rdi)
        ret


Or, with the same amount of safety and fewer fused-domain uops on Intel SnB-family CPUs, and not requiring AVX (but of course works fine with AVX):

cvti32f32:
        movd  %ecx, %xmm0         # breaks deps on xmm0
        cvtdq2ps  %xmm0, %xmm0    # 1 uop for port1
        movss     %xmm0, (%rsi)
        movd  %edx, %xmm0
        cvtdq2ps  %xmm0, %xmm0
        movss     %xmm0, (%rdi)
        ret

But this is only good for 32-bit integer -> float, not double.  (Because cvtdq2pd also takes a port5 uop, unlike conversion to ps).  The code-size is also slightly larger, taking 2 instructions per conversion instead of 1 for AVX 
 when a safe register to merge into is already available.

See full analysis of this (uop counts and stuff for Intel SnB-family) at the bottom of this comment: https://bugs.llvm.org/show_bug.cgi?id=22024#c11


---


### compiler : `gcc`
### title : `GCC fail to optimize nested ternary`
### open_at : `2017-04-30T13:30:43Z`
### last_modified_date : `2023-07-21T07:43:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80574
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `enhancement`
### contents :
Created attachment 41287
nested ternary sample

GCC fail to optimize ternary chain (nested ternary) for find maximum value with unsigned types. Check attachment


---


### compiler : `gcc`
### title : `unnecessary virtual function table support in member function call`
### open_at : `2017-04-30T14:33:02Z`
### last_modified_date : `2021-07-23T20:00:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80575
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 41288
example for ineffectiveness of final with member function pointers

The support for virtual function requires more complex code at the call site through a member function pointer.  gcc has some support to elide the handling of virtual functions.  Take the following code:

struct foo {
  int a = 0, b =0;
  int get1() const { return a; }
  int get2() const { return a + b; }
};

foo f;
int (foo::*mfp)() const = &foo::get1;

int get()
{
  return (f->*mfp)();
}

The generated code (on x86-64) for get is:

	movq	mfp+8(%rip), %rax
	leaq	f(%rax), %rdi
	jmp	*mfp(%rip)

Perfectly fine.  If now the variable 'f' is changed to a pointer the code looks like this:

	movq	mfp(%rip), %rax
	movq	mfp+8(%rip), %rdi
	addq	f(%rip), %rdi
	testb	$1, %al
	je	.L4
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L4:
	jmp	*%rax

This is due to the fact that other classes derived can be derived from 'foo' and those could have virtual functions.

To prevent this it should be possible to mark 'foo' as final.  If you do this nothing changes, though.

--- u.cc-old	2017-04-30 16:30:50.704469153 +0200
+++ u.cc	2017-04-30 16:24:56.619672469 +0200
@@ -1,4 +1,4 @@
-struct foo {
+struct foo final {
   int a = 0, b =0;
   int get1() const { return a; }
   int get2() const { return a + b; }


---


### compiler : `gcc`
### title : `dead strcpy and strncpy followed by memset not eliminated`
### open_at : `2017-04-30T17:52:40Z`
### last_modified_date : `2021-09-15T01:55:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80576
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
Similar to bug 80487, due to the subsequent memset that overwrites the contents of the destination arrays, the strcpy and strncpy calls in the following two functions constitute dead stores and could be eliminated.  The output shows that GCC does not yet take advantage of this optimization opportunity.  The first case might be related to (or the same as) bug 79716.

$ cat y.c && gcc  -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout y.c
void sink (void*);

void f (const char *s)
{
  char a[256];

  __builtin_strcpy (a, s);   // dead store
  __builtin_memset (a, 0, sizeof a); 

  sink (a);
}

void g (const char *s)
{
  char a[256];

  __builtin_strncpy (a, s, sizeof a);   // dead store
  __builtin_memset (a, 0, sizeof a);   

  sink (a);
}


;; Function f (f, funcdef_no=0, decl_uid=1795, cgraph_uid=0, symbol_order=0)

f (const char * s)
{
  char a[256];

  <bb 2> [100.00%]:
  __builtin_strcpy (&a, s_2(D));
  __builtin_memset (&a, 0, 256);
  sink (&a);
  a ={v} {CLOBBER};
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1799, cgraph_uid=1, symbol_order=1)

g (const char * s)
{
  char a[256];

  <bb 2> [100.00%]:
  __builtin_strncpy (&a, s_2(D), 256);
  __builtin_memset (&a, 0, 256);
  sink (&a);
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `Avoid using adj in member function pointers`
### open_at : `2017-04-30T18:05:40Z`
### last_modified_date : `2021-08-04T04:27:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80577
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `8.0`
### severity : `enhancement`
### contents :
Consider the following code:

struct foo final {
  int a = 0, b = 0;
  int get1() const { return a; }
  int get2() const { return a + b; }
};

foo f;
int (foo::*mfp)() const = &foo::get1;

int get()
{
  return (f.*mfp)();
}

When compiled get() looks on x86-64 like this:

	movq	mfp+8(%rip), %rax
	leaq	f(%rax), %rdi
	jmp	*mfp(%rip)

The compiler knows the type 'foo'.  It can determine that there is no multiple inheritence.  This means that the adj field in the member function pointer will always be zero.  Hence the generated code should be

        movl    $f, %esi
        jmp     *mfp(%rip)


---


### compiler : `gcc`
### title : `vsqrtss with AVX should avoid a dependency on the destination register.`
### open_at : `2017-05-01T20:03:16Z`
### last_modified_date : `2019-01-27T05:40:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80586
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
#include <math.h>
float sqrt_depcheck(float a, float b) {
    return sqrtf(b);
}

compiles to (with gcc 8.0.0 20170429  -march=haswell -O3 -fno-math-errno):

  vsqrtss %xmm1, %xmm0, %xmm0
  ret


recent clang (4.0) avoids the unwanted dependency on %xmm0 by using the source register as *both* source operands:

  vsqrtss %xmm1, %xmm1, %xmm0
  ret


This of course doesn't work when the source is a different type (e.g. memory, or for int->float conversion, an integer register.  See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80571 for a suggestion to track cold registers that can be safely used read-only without delaying OOO execution, without putting vxorps-zeroing everywhere).


float sqrt_from_mem(float *fp) {
    return sqrtf(*fp);
}

ICC17 breaks the dep on xmm0 this way:
        vmovss    (%rdi), %xmm0                                 #8.12
        vsqrtss   %xmm0, %xmm0, %xmm0                           #8.12

gcc and clang both decide to risk it with:
       vsqrtss (%rdi), %xmm0, %xmm0


code on https://godbolt.org/g/mJmjdh.


---


### compiler : `gcc`
### title : `GCC can't simplify static inline function with xor/xnor`
### open_at : `2017-05-01T20:48:37Z`
### last_modified_date : `2023-10-25T22:43:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80588
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `enhancement`
### contents :
GCC can't simplify static inline function for xor/xnor, but can, if used macro instead. Testcase:

#include <inttypes.h>

#define TYPE uint8_t

#define M_XOR(a,b) ((!!(a))^(!!(b)))
#define M_NXOR(a,b) (!((!!(a))^(!!(b))))

__attribute__((__always_inline__, const))
static inline TYPE m_xor (const TYPE a, const TYPE b)
{
    return M_XOR(a,b);
}

__attribute__((__always_inline__, const))
static inline TYPE m_xnor (const TYPE a, const TYPE b)
{
    return M_NXOR(a,b);
}

// bad assembly output
int test1b(const TYPE a, const TYPE b)
{
    return m_xor(a,b) == !m_xnor(a,b);
}

int test2b(const TYPE a, const TYPE b)
{
    return !m_xor(a,b) == m_xnor(a,b);
}

int test3b(const TYPE a, const TYPE b)
{
    return M_XOR(a,b) == !m_xnor(a,b);
}

// good assembly output
int test1g(const TYPE a, const TYPE b)
{
    return m_xor(a,b) == M_XOR(a,b);
}

int test2g(const TYPE a, const TYPE b)
{
    return M_XOR(a,b) == !M_NXOR(a,b);
}

int test3g(const TYPE a, const TYPE b)
{
    return M_XOR(a,b) != !M_NXOR(a,b);;
}


---


### compiler : `gcc`
### title : `strangely missed vectorization optimizations`
### open_at : `2017-05-04T21:45:38Z`
### last_modified_date : `2021-08-03T02:28:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80634
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `6.3.1`
### severity : `enhancement`
### contents :
Created attachment 41322
gcc 6.3.1 outputs for ELEMS=1 through ELEMS=32

(Not sure which component is the correct one for this issue).

I've noticed that the vectorizer makes some strange choices occasionally, and will turn some straightforward code into a large branchy code sequence. Take this, for example:

void saxpy(float a, float * restrict x, float * restrict y)
{
        for (int i = 0; i < ELEMS; ++i)
                y[i] = a*x[i] + y[i];
}

If I use the flags "-O3 -march=haswell" (or "-O3 -xAVX2" on ICC) and use varying definitions of ELEMS, I will sometimes get odd results with GCC 6.3.1.

Here's -DELEMS=6 with GCC 6.3.1:

saxpy:
        vshufps $0, %xmm0, %xmm0, %xmm1
        vmovups (%rsi), %xmm2
        vfmadd132ps     (%rdi), %xmm2, %xmm1
        vmovss  20(%rsi), %xmm3
        vmovups %xmm1, (%rsi)
        vmovss  16(%rdi), %xmm1
        vfmadd213ss     16(%rsi), %xmm0, %xmm1
        vfmadd132ss     20(%rdi), %xmm3, %xmm0
        vmovss  %xmm1, 16(%rsi)
        vmovss  %xmm0, 20(%rsi)
        ret

Seems reasonable.


Here's -DELEMS=7 with GCC 6.3.1:

saxpy:
        movq    %rsi, %rax
        shrq    $2, %rax
        negq    %rax
        andl    $3, %eax
        je      .L7
        vmovss  (%rdi), %xmm1
        vfmadd213ss     (%rsi), %xmm0, %xmm1
        vmovss  %xmm1, (%rsi)
        cmpl    $1, %eax
        je      .L8
        vmovss  4(%rdi), %xmm1
        vfmadd213ss     4(%rsi), %xmm0, %xmm1
        vmovss  %xmm1, 4(%rsi)
        cmpl    $3, %eax
        jne     .L9
        vmovss  8(%rdi), %xmm1
        vfmadd213ss     8(%rsi), %xmm0, %xmm1
        movl    $4, %r8d
        movl    $3, %edx
        vmovss  %xmm1, 8(%rsi)
.L3:
        movl    $7, %ecx
        movl    %eax, %r9d
        subl    %eax, %ecx
.L2:
        leaq    (%rsi,%r9,4), %rax
        vshufps $0, %xmm0, %xmm0, %xmm1
        vmovaps (%rax), %xmm3
        vfmadd132ps     (%rdi,%r9,4), %xmm3, %xmm1
        vmovaps %xmm1, (%rax)
        leal    4(%rdx), %eax
        cmpl    $4, %ecx
        je      .L19
        cltq
        addl    $5, %edx
        leaq    (%rsi,%rax,4), %rcx
        vmovss  (%rdi,%rax,4), %xmm1
        vfmadd213ss     (%rcx), %xmm0, %xmm1
        vmovss  %xmm1, (%rcx)
        cmpl    $5, %r8d
        je      .L17
        movslq  %edx, %rdx
        leaq    (%rsi,%rdx,4), %rax
        vmovss  (%rdi,%rdx,4), %xmm1
        vfmadd213ss     (%rax), %xmm0, %xmm1
        vmovss  %xmm1, (%rax)
        cmpl    $6, %r8d
        je      .L17
        vmovss  24(%rsi), %xmm2
        vfmadd132ss     24(%rdi), %xmm2, %xmm0
        vmovss  %xmm0, 24(%rsi)
        ret
.L17:
        ret
.L7:
        movl    $7, %ecx
        xorl    %r9d, %r9d
        movl    $7, %r8d
        xorl    %edx, %edx
        jmp     .L2
.L19:
        ret
.L8:
        movl    $6, %r8d
        movl    $1, %edx
        jmp     .L3
.L9:
        movl    $5, %r8d
        movl    $2, %edx
        jmp     .L3


This might be explained away by it being an odd number just short of a power of two, but ICC does an apparently better job (one packed FMA plus three single FMAs):

saxpy:

        vbroadcastss %xmm0, %xmm2
        vmovups   (%rdi), %xmm1
        vmovss    16(%rdi), %xmm3
        vmovss    20(%rdi), %xmm4
        vmovss    24(%rdi), %xmm5
        vfmadd213ps (%rsi), %xmm1, %xmm2
        vfmadd213ss 16(%rsi), %xmm0, %xmm3
        vfmadd213ss 20(%rsi), %xmm0, %xmm4
        vfmadd213ss 24(%rsi), %xmm5, %xmm0
        vmovups   %xmm2, (%rsi)
        vmovss    %xmm3, 16(%rsi)
        vmovss    %xmm4, 20(%rsi)
        vmovss    %xmm0, 24(%rsi)
        ret

The results from GCC 6.3.1 for ELEMS values 8 through 14 look fine (short branchless code sequences similar to what ICC emits), but things go to crap again for what seems to be *any* value ELEMS=15 or above.

It even misses the opportunity with ELEMS=16 to just do two packed FMAs with YMM registers:

saxpy:
        movq    %rsi, %rax
        shrq    $2, %rax
        negq    %rax
        andl    $7, %eax
        je      .L7
        vmovss  (%rdi), %xmm1
        vfmadd213ss     (%rsi), %xmm0, %xmm1
        vmovss  %xmm1, (%rsi)
        cmpl    $1, %eax
        je      .L8
        vmovss  4(%rdi), %xmm1
        vfmadd213ss     4(%rsi), %xmm0, %xmm1
        vmovss  %xmm1, 4(%rsi)
        cmpl    $2, %eax
        je      .L9
        vmovss  8(%rdi), %xmm1
        vfmadd213ss     8(%rsi), %xmm0, %xmm1
        vmovss  %xmm1, 8(%rsi)
        cmpl    $3, %eax
        je      .L10
        vmovss  12(%rdi), %xmm1
        vfmadd213ss     12(%rsi), %xmm0, %xmm1
        vmovss  %xmm1, 12(%rsi)
        cmpl    $4, %eax
        je      .L11
        vmovss  16(%rdi), %xmm1
        vfmadd213ss     16(%rsi), %xmm0, %xmm1
        vmovss  %xmm1, 16(%rsi)
        cmpl    $5, %eax
        je      .L12
        vmovss  20(%rdi), %xmm1
        vfmadd213ss     20(%rsi), %xmm0, %xmm1
        vmovss  %xmm1, 20(%rsi)
        cmpl    $7, %eax
        jne     .L13
        vmovss  24(%rdi), %xmm1
        vfmadd213ss     24(%rsi), %xmm0, %xmm1
        movl    $9, %r9d
        movl    $7, %r10d
        vmovss  %xmm1, 24(%rsi)
.L3:
        movl    $16, %ecx
        movl    %eax, %edx
        movl    $8, %r8d
        movl    $1, %r11d
        subl    %eax, %ecx
.L2:
        salq    $2, %rdx
        vbroadcastss    %xmm0, %ymm1
        leaq    (%rdi,%rdx), %rax
        addq    %rsi, %rdx
        vmovups (%rax), %ymm2
        vfmadd213ps     (%rdx), %ymm1, %ymm2
        vmovaps %ymm2, (%rdx)
        cmpl    $2, %r11d
        jne     .L4
        vmovaps 32(%rdx), %ymm4
        vfmadd132ps     32(%rax), %ymm4, %ymm1
        vmovaps %ymm1, 32(%rdx)
.L4:
        movl    %r9d, %edx
        leal    (%r8,%r10), %eax
        subl    %r8d, %edx
        cmpl    %r8d, %ecx
        je      .L29
        movslq  %eax, %r8
        leaq    (%rsi,%r8,4), %rcx
        vmovss  (%rdi,%r8,4), %xmm1
        vfmadd213ss     (%rcx), %xmm0, %xmm1
        vmovss  %xmm1, (%rcx)
        leal    1(%rax), %ecx
        cmpl    $1, %edx
        je      .L29
        movslq  %ecx, %rcx
        leaq    (%rsi,%rcx,4), %r8
        vmovss  (%rdi,%rcx,4), %xmm1
        leal    2(%rax), %ecx
        vfmadd213ss     (%r8), %xmm0, %xmm1
        vmovss  %xmm1, (%r8)
        cmpl    $2, %edx
        je      .L29
        movslq  %ecx, %rcx
        leaq    (%rsi,%rcx,4), %r8
        vmovss  (%rdi,%rcx,4), %xmm1
        leal    3(%rax), %ecx
        vfmadd213ss     (%r8), %xmm0, %xmm1
        vmovss  %xmm1, (%r8)
        cmpl    $3, %edx
        je      .L29
        movslq  %ecx, %rcx
        leaq    (%rsi,%rcx,4), %r8
        vmovss  (%rdi,%rcx,4), %xmm1
        leal    4(%rax), %ecx
        vfmadd213ss     (%r8), %xmm0, %xmm1
        vmovss  %xmm1, (%r8)
        cmpl    $4, %edx
        je      .L29
        movslq  %ecx, %rcx
        leaq    (%rsi,%rcx,4), %r8
        vmovss  (%rdi,%rcx,4), %xmm1
        leal    5(%rax), %ecx
        vfmadd213ss     (%r8), %xmm0, %xmm1
        vmovss  %xmm1, (%r8)
        cmpl    $5, %edx
        je      .L29
        movslq  %ecx, %rcx
        addl    $6, %eax
        leaq    (%rsi,%rcx,4), %r8
        vmovss  (%rdi,%rcx,4), %xmm1
        vfmadd213ss     (%r8), %xmm0, %xmm1
        vmovss  %xmm1, (%r8)
        cmpl    $6, %edx
        je      .L29
        cltq
        leaq    (%rsi,%rax,4), %rdx
        vmovss  (%rdx), %xmm3
        vfmadd132ss     (%rdi,%rax,4), %xmm3, %xmm0
        vmovss  %xmm0, (%rdx)
.L29:
        vzeroupper
        ret
.L7:
        movl    $16, %r8d
        movl    $16, %ecx
        xorl    %edx, %edx
        xorl    %r10d, %r10d
        movl    $2, %r11d
        movl    $16, %r9d
        jmp     .L2
.L13:
        movl    $10, %r9d
        movl    $6, %r10d
        jmp     .L3
.L8:
        movl    $15, %r9d
        movl    $1, %r10d
        jmp     .L3
.L9:
        movl    $14, %r9d
        movl    $2, %r10d
        jmp     .L3
.L10:
        movl    $13, %r9d
        movl    $3, %r10d
        jmp     .L3
.L11:
        movl    $12, %r9d
        movl    $4, %r10d
        jmp     .L3
.L12:
        movl    $11, %r9d
        movl    $5, %r10d
        jmp     .L3


ICC gets ELEMS=16 right:

saxpy:

        vmovups   (%rdi), %ymm1
        vmovups   32(%rdi), %ymm2
        vbroadcastss %xmm0, %ymm3
        vfmadd213ps (%rsi), %ymm3, %ymm1
        vfmadd213ps 32(%rsi), %ymm2, %ymm3
        vmovups   %ymm1, (%rsi)
        vmovups   %ymm3, 32(%rsi)
        vzeroupper 
        ret 


I'll attach the code outputs for ELEMS values 1 through 32 using GCC 6.3.1 and ICC 17.0.1.


---


### compiler : `gcc`
### title : `AVX / AVX512 register-zeroing should always use AVX 128b, not ymm or zmm`
### open_at : `2017-05-05T00:02:56Z`
### last_modified_date : `2021-06-04T01:18:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80636
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Currently, gcc compiles _mm256_setzero_ps() to vxorps %ymm0, %ymm0, %ymm0, or zmm for _mm512_setzero_ps.  And similar for pd and integer vectors, using a vector size that matches how it's going to use the register.

vxorps %xmm0, %xmm0, %xmm0 has the same effect, because AVX instructions zero the destination register out to VLMAX.

AMD Ryzen decodes the xmm version to 1 micro-op, but the ymm version to 2 micro-ops.  It doesn't detect the zeroing idiom special-case until after the decoder has split it.  (Earlier AMD CPUs (Bulldozer/Jaguar) may be similar.)

---

For zeroing a ZMM register, it also saves a byte or two to use a VEX prefix instead of EVEX, if the target register is zmm0-15.  (zmm16-31 of course always need EVEX).

---

There is no benefit, but also no downside, to using xmm-zeroing on Intel CPUs that don't split 256b or 512b vector ops.  This change could be made across the board, without adding any tuning options to control it.

References: 
http://stackoverflow.com/a/43751783/224132 Agner Fog's answer to my SO question about this.
https://bugs.llvm.org/show_bug.cgi?id=32862  the same issue for clang.


---


### compiler : `gcc`
### title : `missed optimization with with std::vector resize in loop`
### open_at : `2017-05-05T10:04:34Z`
### last_modified_date : `2023-08-09T23:15:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80641
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.0`
### severity : `normal`
### contents :
#include <vector>
int main()
{
    std::vector<int> c {1,2,3,0};
    while(c.size() > 0 && c.back() == 0)
    {
        auto sz = c.size() -1;
        c.resize(sz);
    }
    return 0;
}
 
$ c++7.1 -O3 tt.cxx
cc1plus: warning: 'void* __builtin_memset(void*, int, long unsigned int)': specified size 18446744073709551612 exceeds maximum object size 9223372036854775807  -Wstringop-overflow=]

No warning comes when I use GCC 6.1


---


### compiler : `gcc`
### title : `Member function pointer optimization affected by incompatible virtual function`
### open_at : `2017-05-07T13:38:54Z`
### last_modified_date : `2021-08-05T08:38:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80660
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `8.0`
### severity : `enhancement`
### contents :
Consider the following code:

struct foo final {
  int a = 0;
  int b = 0;
  void set_a(int p) { a = p; }
  void set_b(int p) { b = p; }
#ifdef VIRT
  virtual int get_a() const { return a; }
#endif
};

void (foo::*set)(int);

foo fobj1;

void bar1(int a) {
  (fobj1.*set)(a);
}

When compiling with optimization and VIRT not defined the code generated for bar1 does correctly so elide the test for a virtual function and saves code and time at execution time.

Adding any virtual function (such as by defining VIRT) changes this.  All of the sudden the entire member function pointer call sequence is emitted.

This is unnecessary, though, since the present virtual function is incompatible with the member function pointer 'set'.  Therefore the generated code should be the same, with or without get_a defined.


---


### compiler : `gcc`
### title : `128 bit loads generated for structure copying with gcc 7.1.0 and leads to STLF stalls in avx2 targets.`
### open_at : `2017-05-09T09:47:54Z`
### last_modified_date : `2022-01-28T17:12:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80689
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.1.0`
### severity : `normal`
### contents :
For the below test case, GCC 7.1.0 started generating 128 bit loads and stores while copying the structure elements. 

This pattern is observed in some benchmarks and leads to STLF stalls for few AVX2 targets.

typedef struct st1
{
        long unsigned int a,b;
        long int c,d;
}R;

typedef struct st2
{
        int  t;
        R  reg;
}N;

void Set (const R *region,  N *n_info );

void test(N  *n_obj ,const long unsigned int a, const long unsigned int b,  const long int c,const long int d)
{
        R reg;

        reg.a=a;
        reg.b=b;
        reg.c=c;
        reg.d=d;
        Set (&reg, n_obj);

}

void Set (const R *reg,  N *n_obj )
{
        n_obj->reg=(*reg);
}

 flag: -fno-inline -O2 

 GCC 6.3.0 

Set:
.LFB1:
        .cfi_startproc
        movq    (%rdi), %rax
        movq    %rax, 8(%rsi)
        movq    8(%rdi), %rax
        movq    %rax, 16(%rsi)
        movq    16(%rdi), %rax
        movq    %rax, 24(%rsi)
        movq    24(%rdi), %rax
        movq    %rax, 32(%rsi)
        ret
        .cfi_endproc
.LFE1:
        .size   Set, .-Set
        .p2align 4,,15
        .globl  test
        .type   test, @function
test:
.LFB0:
        .cfi_startproc
        subq    $40, %rsp
       .cfi_def_cfa_offset 48
        movq    %rsi, (%rsp)
        movq    %rdi, %rsi
        movq    %rsp, %rdi
        movq    %rdx, 8(%rsp)
        movq    %rcx, 16(%rsp)
        movq    %r8, 24(%rsp)
        call    Set
        addq    $40, %rsp
        .cfi_def_cfa_offset 8
        ret


GCC 7.1.0 

Set:
.LFB1:
        .cfi_startproc
        movdqu  (%rdi), %xmm0 <== 128 bit loads
        movups  %xmm0, 8(%rsi)
        movdqu  16(%rdi), %xmm0  <== 128 bit loads
        movups  %xmm0, 24(%rsi)
        ret
        .cfi_endproc
.LFE1:
        .size   Set, .-Set
        .p2align 4,,15
        .globl  test
        .type   test, @function
test:
.LFB0:
        .cfi_startproc
        subq    $40, %rsp
        .cfi_def_cfa_offset 48
        movq    %rsi, (%rsp)
        movq    %rdi, %rsi
        movq    %rsp, %rdi
        movq    %rdx, 8(%rsp)
        movq    %rcx, 16(%rsp)
        movq    %r8, 24(%rsp)
        call    Set
        addq    $40, %rsp
        .cfi_def_cfa_offset 8
        ret


---


### compiler : `gcc`
### title : `FRE fails to eliminate to leader dominating after unreachable edge removal`
### open_at : `2017-05-10T11:23:00Z`
### last_modified_date : `2023-05-12T04:45:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80702
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
For

int c;
int foo (int a, int b)
{
  c = a + b;
  int d = c - a;
  int e;
  if (d == b)
    c = 2 * a;
  e = 2 * a;
  return e;
}

at -O2 SCCVN sees

Marking all edges out of BB 2 but (2 -> 3) as not executable
Visiting BB 3
Value numbering _3 stmt = _3 = a_5(D) * 2;
Setting value number of _3 to _3 (changed)
...
Visiting BB 4
...
Value numbering e_11 stmt = e_11 = a_5(D) * 2;
Setting value number of e_11 to _3 (changed)

but elimination ends up with the following because _3 is not thought to
be available in BB 4 (the domwalk used during elimination pops availability
of _3 after visiting dom children):

  <bb 2> [0.00%]:
  _1 = a_5(D) + b_6(D);
  c = _1;
  _3 = a_5(D) * 2;
  c = _3;
  e_11 = a_5(D) * 2;
  return e_11;

A value-numbering rewrite should preserve the value-numbering but also eventually do better during elimination.

Within the current scheme one could try to "cleverly" put aside info
we pop after visiting all dom children and restore it from a BB with
a single executable predecessor edge src.

The end goal should be merging of early CCP, forwprop and FRE without
losing the CFG cleanup early CCP is able to do and the followup FRE
that enables.  Basically allow more CSE w/o the need to iterate though
CFG cleanup which would enable more CSE.


---


### compiler : `gcc`
### title : `-fstrict-enum ineffective, incorrect -Wtype-limits warning`
### open_at : `2017-05-13T00:19:38Z`
### last_modified_date : `2021-04-06T04:09:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80733
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `c++`
### version : `7.0`
### severity : `normal`
### contents :
The -fstrict-enums option's effect is documented as

    Allow the compiler to optimize using the assumption that a value of enumerated type can only be one of the values of the enumeration (as defined in the C++ standard; basically, a value that can be represented in the minimum number of bits needed to represent all the enumerators). 

The program below shows that GCC doesn't actually perform the optimization.  It only appears to constrain the range of values of the type to that of the underlying type, apparently disregarding the TYPE_{MIN,MAX}_VALUE set by the C++ front end in 
finish_enum_value_list in response to the option.

To add insult to injury, the -Wtype-limits warning suggests that GCC actually does perform the optimization (the "not eliminated (bug), warning (bug)" case below).

When compiled without -fstrict-enums, the emitted code stays the same.  The only thing that changes is that the first warning (on line 16) is not issued.

$ cat t.C && gcc -O2 -S -Wall -Wextra -Wpedantic -Wconversion -xc++ -fstrict-enums -fdump-tree-optimized=/dev/stdout t.C | grep -E "(^void (foo|bar)|abort)"
enum E { e0, e15 = 15 };
enum __attribute__ ((packed)) F { f0, f15 = 15 };

void foo (E e)
{
  if (e > 15) __builtin_abort ();   // not eliminated (bug)
}

void bar (E e)
{
  if (e > 255) __builtin_abort ();   // not eliminated (bug)
}

void foo (F f)
{
  if (f > 15) __builtin_abort ();   // not eliminated (bug), warning (bug)
}

void bar (F f)
{
  if (f > 255) __builtin_abort ();   // eliminated, warning (good)
}
t.C: In function ‘void foo(F)’:
t.C:16:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (f > 15) __builtin_abort ();   // not eliminated (bug), warning (bug)
       ~~^~~~
t.C: In function ‘void bar(F)’:
t.C:21:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (f > 255) __builtin_abort ();   // eliminated, warning
       ~~^~~~~
void foo(E) (E e)
  __builtin_abort ();
void bar(E) (E e)
  __builtin_abort ();
void foo(F) (F f)
  __builtin_abort ();
void bar(F) (F f)


---


### compiler : `gcc`
### title : `dead first stmt in a=0;a=b;b=0 whatever the aliasing`
### open_at : `2017-05-14T12:03:46Z`
### last_modified_date : `2020-12-14T17:34:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80738
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
In C++, with move/swap, we regularly see the following appear

void f(int&a,int&b){
  a=0;
  a=b;
  b=0;
}

If a and b do not alias, the first statement is clearly killed by the second. If a and b are the same, the last statement kills all the rest. So in all cases, the first statement is dead. (we could imagine that removing it means that in some cases, we will copy uninitialized memory to itself, but that doesn't seem so bad)

This is yet another case where the same optimization is valid whether 2 pointers alias or not, like PR 66261 or PR 80617, but the others were more about propagation and this one is about dead code...

It seems hard to detect (involves 4 gimple statements, and plenty of intermediate statements could interfere), the gain may not be worth it. If we split the paths just right: if(&a!=&b){a=0;a=b;}else{a=0;a=b;}b=0; becomes if(&a!=&b)a=b; b=0; and the if could be done unconditionally... Seems unlikely.


---


### compiler : `gcc`
### title : `Aliasing with the return value`
### open_at : `2017-05-14T13:26:38Z`
### last_modified_date : `2023-08-05T05:02:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80740
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
struct A {
  int i;
  A():i(0){}
  A(A const&a):i(a.i){}
};

A f(A&a){
  A ret;
  ret.i=a.i;
  return ret;
}

We do not manage to remove the 0 initialization because of a possible aliasing between ret and a

  ret_3(D)->i = 0;
  _1 = a_4(D)->i;
  ret_3(D)->i = _1;
  return ret_3(D);

Clang does remove it.
Aliasing would be something like

int g(){
  A x=f(x);
  return x.i;
}

which we optimize to return 0; and clang optimizes it to just return; without a warning, although their static analysis tool says
/tmp/x.cc:9:8: warning: Assigned value is garbage or undefined
  ret.i=a.i;
       ^~~~

Clang has the opposite bug report, saying that they are wrong to optimize because g is valid, but they seem to consider that the bug is in the standard for not forbidding it more clearly...
https://bugs.llvm.org/show_bug.cgi?id=11470


---


### compiler : `gcc`
### title : `isnan/isfinite/isinf value propagation`
### open_at : `2017-05-15T11:35:09Z`
### last_modified_date : `2021-10-01T03:00:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80758
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Consider the following code:

#define isnan(x) __builtin_isnan(x)
#define isfinite(x) __builtin_isfinite(x)

int f(double a, double b)
{
  if (!isfinite(a) || !isfinite(b))
    return 0;
  double c = a + b;
  return isnan(c) ? 0 : 1;
}

For x86-64 with the current trunk version (and probably all previous versions) the generated code looks something like this:

	.cfi_startproc
	vmovq	.LC0(%rip), %xmm2
	vmovapd	%xmm0, %xmm4
	vmovsd	.LC1(%rip), %xmm3
	xorl	%eax, %eax
	vandpd	%xmm2, %xmm4, %xmm4
	vucomisd	%xmm4, %xmm3
	jb	.L5
	vandpd	%xmm1, %xmm2, %xmm2
	vucomisd	%xmm2, %xmm3
	jb	.L5
	vaddsd	%xmm1, %xmm0, %xmm0
	xorl	%eax, %eax
	vucomisd	%xmm0, %xmm0
	setnp	%al
.L5:
	ret
	.cfi_endproc

The issue here is that the sum of two finite values will never be NaN.  It can be ±Inf but not NaN.  The VRP information should contain necessary information and use it in the __builtin_isnan code generation.


---


### compiler : `gcc`
### title : `suboptimal code negating a 1-bit _Bool field`
### open_at : `2017-05-15T23:06:08Z`
### last_modified_date : `2023-08-25T00:32:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80770
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `enhancement`
### contents :
For all targets I tried (powerpc64, sparcv9, and x86_64), GCC emits worse code for the negation of a 1-bit _Bool field than it does for the equivalent _Bool (non-bit) field.  The following test case shows the difference.  The code for fb1 should look the same as the code for fb2.  A _Bool bit-field is typically used as a (space) optimization so emitting bigger and slower code defeats its sole purpose.

Clang emits the same code for both function for the same targets.

$ cat a.c && gcc -O2 -S -Wall -Wextra -Wconversion -o/dev/stdout a.c
struct S {
  _Bool b1: 1;
  _Bool b;
};

void fb1 (struct S *s)
{
  s->b1 = !s->b1;
}

void fb (struct S *s)
{
  s->b = !s->b;
}

	.file	"a.c"
	.text
	.p2align 4,,15
	.globl	fb1
	.type	fb1, @function
fb1:
.LFB0:
	.cfi_startproc
	movzbl	(%rdi), %eax
	movl	%eax, %edx
	andl	$-2, %eax
	andl	$1, %edx
	xorl	$1, %edx
	orl	%edx, %eax
	movb	%al, (%rdi)
	ret
	.cfi_endproc
.LFE0:
	.size	fb1, .-fb1
	.p2align 4,,15
	.globl	fb
	.type	fb, @function
fb:
.LFB1:
	.cfi_startproc
	xorb	$1, 1(%rdi)
	ret
	.cfi_endproc
.LFE1:
	.size	fb, .-fb
	.ident	"GCC: (GNU) 7.0.1 20170418 (experimental)"
	.section	.note.GNU-stack,"",@progbits
tmp$


---


### compiler : `gcc`
### title : `-Wformat-overflow false positive for %d on integer bounded by __builtin_unreachable`
### open_at : `2017-05-16T07:43:29Z`
### last_modified_date : `2023-10-25T03:36:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80776
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `7.1.0`
### severity : `normal`
### contents :
I found this problem when compiling Emacs with GCC 7.1.0 on x86-64. Emacs uses __builtin_unreachable to let the compiler know about ranges and thereby suppress false alarms. Here is a stripped-down example of the bug:

  extern int sprintf (char *restrict, const char *restrict, ...)
    __attribute__ ((__nothrow__));
  extern int foo (void);

  int
  Fgenerate_new_buffer_name (void)
  {
    char number[2];
    int i = foo ();
    if (i < 0)
      __builtin_unreachable ();
    if (i >= 2)
      __builtin_unreachable ();
    return sprintf (number, "%d", i);
  }

Compile this with:

  gcc -c -Wformat-overflow -O2  t.i

The output is:

t.i: In function 'Fgenerate_new_buffer_name':
t.i:14:28: warning: '%d' directive writing between 1 and 10 bytes into a region\
 of size 2 [-Wformat-overflow=]
   return sprintf (number, "%d", i);
                            ^~
t.i:14:27: note: directive argument in the range [0, 2147483647]
   return sprintf (number, "%d", i);
                           ^~~~

The diagnostic is incorrect, as the directive argument I is in the range [0, 1]. Changing the two ifs to read just "if (i < 0 || i >= 2) __builtin_unreachable ();" works around this particular bug, but other variants show similar problems.


---


### compiler : `gcc`
### title : `[8/9/10 regression] test case gcc.dg/sms-1.c fail2 starting with r247885`
### open_at : `2017-05-16T21:03:35Z`
### last_modified_date : `2019-09-16T05:53:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80791
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
With this revision SMS fails when compiling this test case.

With revision r247884 and earlier we get:

SMS SC already optimized.
SMS schedule branch at cycle ii-1
crr_insn->node=0 (insn id 16), crr_insn->cycle=-1, min_cycle=-1
crr_insn->node=5 (insn id 26), crr_insn->cycle=2, min_cycle=-1
crr_insn->node=3 (insn id 22), crr_insn->cycle=2, min_cycle=-1
crr_insn->node=6 (insn id 43), crr_insn->cycle=2, min_cycle=-1 (branch)
crr_insn->node=1 (insn id 18), crr_insn->cycle=0, min_cycle=-1
crr_insn->node=2 (insn id 21), crr_insn->cycle=0, min_cycle=-1
crr_insn->node=4 (insn id 25), crr_insn->cycle=1, min_cycle=-1
changing bb of uid 64
  unscanned insn
verify found no changes in insn with uid = 43.
Edge 6->6 redirected to 7
 /home/seurer/gcc/gcc-test3/gcc/testsuite/gcc.dg/sms-1.c:24 SMS succeeded 3 2 (with ii, sc)


Starting with revision r247885 we get:

SMS SC already optimized.
SMS schedule branch at cycle ii-1
crr_insn->node=1 (insn id 11), crr_insn->cycle=-1, min_cycle=-2
crr_insn->node=2 (insn id 14), crr_insn->cycle=-1, min_cycle=-2
crr_insn->node=4 (insn id 17), crr_insn->cycle=2, min_cycle=-2
crr_insn->node=7 (insn id 36), crr_insn->cycle=2, min_cycle=-2 (branch)
crr_insn->node=5 (insn id 18), crr_insn->cycle=0, min_cycle=-2
crr_insn->node=3 (insn id 16), crr_insn->cycle=0, min_cycle=-2
crr_insn->node=0 (insn id 9), crr_insn->cycle=-2, min_cycle=-2
crr_insn->node=6 (insn id 19), crr_insn->cycle=1, min_cycle=-2
Scheduling register move INSN 57; ii = 3, min cycle = -2

...more...

Scheduled w/o split in 0
SMS SC already optimized.
SMS schedule branch at cycle ii-1
SMS failed... 
SMS sched-failed (stage-count=1, loop-count=0, trip-count=0)


There is another SMS test with a similar failure but that one has been failing for a while now:  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=69992


---


### compiler : `gcc`
### title : `constant objects can be assumed to be immutable`
### open_at : `2017-05-17T02:09:35Z`
### last_modified_date : `2022-03-15T11:46:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80794
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
Modifying a const object is undefined (1.9, p4 and 3.3, p1.1 of C++17).  Therefore, valid C++ programs can be relied on not to modify const objects or subobjects to improve the efficiency of accessing such objects.  For example, in both function foo and bar in the test case below, GCC can assume that the value of the s.i member after the call to s.f() is the same as its value before the call, and eliminate the test for the inequality.

This should also hold for C but I suspect C programs more commonly violate this constraint than C++ programs do.

$ cat t.C && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout t.C
struct S
{
  const int i;

  S (int x): i (x) { }

  void f () const;
};

const S s (123);

void foo ()
{
  int i = s.i;
  s.f ();
  int j = s.i;

  if (i != j) __builtin_abort ();
}

void bar (const S &s)
{
  int i = s.i;
  s.f ();
  int j = s.i;

  if (i != j) __builtin_abort ();
}


;; Function void foo() (_Z3foov, funcdef_no=3, decl_uid=2318, cgraph_uid=3, symbol_order=4)

void foo() ()
{
  int j;
  int i;

  <bb 2> [100.00%]:
  i_2 = s.i;
  S::f (&s);
  j_4 = s.i;
  if (i_2 != j_4)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [0.04%]:
  __builtin_abort ();

  <bb 4> [99.96%]:
  return;

}



;; Function void bar(const S&) (_Z3barRK1S, funcdef_no=4, decl_uid=2323, cgraph_uid=4, symbol_order=5)

void bar(const S&) (const struct S & s)
{
  int j;
  int i;

  <bb 2> [100.00%]:
  i_3 = s_2(D)->i;
  S::f (s_2(D));
  j_5 = s_2(D)->i;
  if (i_3 != j_5)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [0.04%]:
  __builtin_abort ();

  <bb 4> [99.96%]:
  return;

}



;; Function (static initializers for t.C) (_GLOBAL__sub_I__Z3foov, funcdef_no=6, decl_uid=2332, cgraph_uid=6, symbol_order=7) (executed once)

(static initializers for t.C) ()
{
  <bb 2> [100.00%]:
  MEM[(struct  &)&s] ={v} {CLOBBER};
  s.i = 123;
  return;

}


---


### compiler : `gcc`
### title : `out-of-line string members less efficient than they could be`
### open_at : `2017-05-18T03:37:20Z`
### last_modified_date : `2019-07-31T07:53:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80811
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `7.0`
### severity : `normal`
### contents :
Consider the following contrived example and the optimized dump of the two functions.  The first overload of cmp is optimized into a no-op, while the second, equivalent overload contains a pair of identical calls to string::compare along with a dutiful comparison of their return values.  The difference is likely thanks to the first overload of string::compare being defined inline and GCC being able to determine that it has no side-effects, while the second overload of string::compare being defined out-of-line and GCC apparently not being able to make the same determination.  But when the second overload is decorated with attribute pure, GCC is able to optimize both functions the same way.  I would expect it to be possible to make GCC recognize this and perform the optimization regardless, but until it does, it seems that there may be an opportunity to help GCC optimize many libstdc++ functions by making use of this attribute (and perhaps others as well).

$ cat t.C && g++ -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout t.ii

#include <string>

void cmp (const std::string &str, const char *s)
{
  int c1 = str.compare (s);
  int c2 = str.compare (s);

  if (c1 != c2)
    __builtin_abort ();
}

void cmp (const std::string &s1, const std::string &s2)
{
  int c1 = s1.compare (s2;
  int c2 = s2.compare (s1);

  if (c1 != c2)
    __builtin_abort ();
}


;; Function void cmp(const string&, const string&) (_Z3cmpRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES6_, funcdef_no=1005, decl_uid=24124, cgraph_uid=293, symbol_order=295)

void cmp(const string&, const string&) (const struct string & s1, const struct string & s2)
{
  <bb 2> [100.00%]:
  return;

}



;; Function void cmp(const string&, const char*) (_Z3cmpRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPKc, funcdef_no=1006, decl_uid=24130, cgraph_uid=294, symbol_order=296)

void cmp(const string&, const char*) (const struct string & str, const char * s)
{
  int c2;
  int c1;

  <bb 2> [100.00%]:
  c1_5 = std::__cxx11::basic_string<char>::compare (str_2(D), s_3(D));
  c2_7 = std::__cxx11::basic_string<char>::compare (str_2(D), s_3(D));
  if (c1_5 != c2_7)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [0.04%]:
  __builtin_abort ();

  <bb 4> [99.96%]:
  return;

}


---


### compiler : `gcc`
### title : `x86: std::vector<bool>::operator[] could be somewhat faster using BT instead of SHL`
### open_at : `2017-05-18T04:47:51Z`
### last_modified_date : `2021-07-21T13:56:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80813
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
This actually applies to all cases of testing a single bit in an array, not just vector<bool>.

#include <vector>
bool f(const std::vector<bool>& v, std::size_t x) {
    return v[x];
}

compiles with -O3 to this asm, on gcc5.x, 6.x, 7.1, and 8:

        # https://godbolt.org/g/OHsgcv
        movq    (%rdi), %rdx
        movq    %rsi, %r8
        movl    %esi, %ecx
        shrq    $6, %r8
        movl    $1, %eax
        salq    %cl, %rax
        testq   %rax, (%rdx,%r8,8)
        setne   %al
        ret

(or with -march=haswell, it uses SHLX for the 1<<x)

It would be more efficient to do 1<<x using
  xor  %eax,%eax
  bts  %rsi,%rax

but gcc doesn't do this, even with -march=haswell.  (On AMD CPUs, BTS reg,reg is 2 m-ops, but it's probably about break-even if it saves a mov instruction to copy the shift-count to ecx.  But for AMD with BMI2, mov $1,reg and shlx is probably optimal despite the larger code-size.)

---

But it's even better to load and then test the bit with BT like clang does.

There's also no need to do a 64-bit load.  We can avoid a lot of REX prefixes by doing a 32-bit load.

        # hand-crafted sequence I think is optimal, modified from clang output
        movq    (%rdi), %rax
        movl    %esi, %ecx
        shrq    $5, %rsi        # does still need to be 64-bit
        movl    (%rax,%rsi,4), %eax
        btl     %ecx, %eax
        setb    %al
        retq

Using BT is fewer instructions, and variable-count SHL is slower than BT on Intel SnB-family CPUs.  According to Agner Fog's tables, SHL %cl,%reg is 3 uops on SnB/HSW/SKL, but BT %reg,%reg is 1 uop on Intel and AMD CPUs.  (SHLX is 1 uop, so BMI2 avoids the x86 legacy baggage of leaving flags untouched if count==0.)

If we were branching on it, TEST/JZ could macro-fuse on Intel and AMD CPUs, but BT/JNC couldn't.  The BT strategy is still a speed win, or with BMI2 only a win on code-size.


----

Actually, if not branching on it, we could just isolate the bit with a shift and mask, instead of a TEST and SETCC.

        movq    (%rdi), %rax
        movl    %esi, %ecx
        shrq    $5, %rsi
        movl    (%rax,%rsi,4), %eax
        shrx    %ecx, %eax, %eax
        andl    $1, %eax
        retq

This is as cheap as the bt/setb sequence if BMI2 is available, but zero-extends the bool for free to fill a register instead of having it only in the low 8.  (OTOH, BT/SETNC lets us invert the bit for free).

The latency should be the same as with SHL/TEST/SETNZ or with BT/SETC, because TEST with a memory operand still runs as a separate load and then 1-cycle test.  The load can start before the mask operand is ready.  So there are always two 1-cycle ALU operations after the load.

Without BMI2, using shr %cl,%eax would be worse on Intel SnB-family, because it has higher latency than BT and is on the critical path (unlike when shl is used to make a mask in parallel with the load, so load-use latency hides the latency of setting up the mask unless ALU resource conflicts delay it).


---


### compiler : `gcc`
### title : `[missed optimization][x86] relaxed atomics`
### open_at : `2017-05-18T15:13:04Z`
### last_modified_date : `2022-01-10T20:17:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80817
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
Using gcc 7.1 on x86, the following

#include <atomic>

void increment_relaxed(std::atomic<uint64_t>& counter) {

 atomic_store_explicit(&counter,
          atomic_load_explicit(&counter, std::memory_order_relaxed) + 1,
          std::memory_order_relaxed);
}

compiles to:

	.cfi_startproc
	movq	(%rdi), %rax
	addq	$1, %rax
	movq	%rax, (%rdi)
	ret
	.cfi_endproc

while I would expect that 

	.cfi_startproc
	addq	$1, (%rdi)
	ret
	.cfi_endproc

would be fine and more efficient. 

I also looked at 

atomic_fetch_add_explicit(&counter, uint64_t(1), std::memory_order_relaxed); 

but that surprised me with

	.cfi_startproc
	lock addq	$1, (%rdi)
	ret
	.cfi_endproc


---


### compiler : `gcc`
### title : `_mm_set_epi64x shouldn't store/reload for -mtune=haswell, Zen should avoid store/reload, and generic should think about it.`
### open_at : `2017-05-18T20:25:53Z`
### last_modified_date : `2019-04-18T14:07:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80820
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
gcc with -mtune=generic likes to bounce through memory when moving data from integer registers to xmm for things like _mm_set_epi32.

There are 3 related tuning issues here:

* -mtune=haswell -mno-sse4 still uses one store/reload for _mm_set_epi64x.

* -mtune=znver1 should definitely favour movd/movq instead of store/reload.
  (Ryzen has 1 m-op movd/movq between vector and integer with 3c latency, shorter than store-forwarding.  All the reasons to favour store/reload on other AMD uarches are gone.)

* -mtune=generic should probably favour movd/movq.  I think it's better for a weighted-average of CPUs we care about for -mtune=generic.  Most of the text below is an attempt to back up this claim, but I don't have hardware to test with so all I can do is look at Agner Fog's tables and microarch pdf.

 movd is about break-even on Bulldozer, better on SnB-family, much better on Core2/Nehalem, and significantly worse only on AMD K8/K10.  Or maybe use a hybrid strategy that does half with movd and half with store/reload, which can actually be better than either strategy alone on Bulldozer and SnB-family.

-----------

The tune=haswell issue is maybe separate from the others, since gcc already knows that bouncing through memory isn't the optimal strategy.

#include <immintrin.h>
__m128i combine64(long long a, long long b) {
  return _mm_set_epi64x(b,a);
}

gcc8 -O3 -mtune=haswell emits:

        movq    %rsi, -16(%rsp)
        movq    %rdi, %xmm0
        movhps  -16(%rsp), %xmm0

(see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80819 for the wasted store with -msse4 -mno-avx).


I think what clang and ICC do is optimal for the SSE2-only case, for Intel CPUs and Ryzen:

        movq    %rsi, %xmm1
        movq    %rdi, %xmm0
        punpcklqdq      %xmm1, %xmm0

_mm_set_epi32(d,c,b,a) with -mtune=haswell gives us the expected movd/punpck (without SSE4), no store/reload.


-----


Using movd or movq instead of a store/reload is a code-size win: movd %eax, %xmm0 is 4 bytes (or 5 with a REX prefix for movq or high registers).  Store/reload to -0x10(%rsp) is 10, 11, or 12 bytes, depending on operand size and high register(s).

movd int->xmm is lower latency than store/reload on most CPUs, especially Intel SnB-family where it's 1c latency, and also AMD Ryzen.   On SnB family, store/reload's only advantage is rare cases where port5 is a throughput bottleneck and latency isn't important.

It replaces a store and a load uop with 1 ALU uop on Intel Core2 and later, and Atom/Silvermont/KNL.  Also 1 uop on VIA Nano.

movd int->xmm is 2 ALU uops on AMD K10/Bulldozer-family and Jaguar, and P4, and 3 on K8/Bobcat.  It never costs any more total uops for the front-end (since a movd load is 2 uops on K8/Bobcat), but decoding a multi-uop instruction can sometimes be a bottleneck (especially on K8 where a 3 m-op instruction is a "vectorpath" (microcode)).


Store/reload has one per clock throughput on every CPU, AFAIK.  On most CPUs that have much weight in -mtune=generic, movd's throughput is one-per-clock or better.  (According to Agner Fog's tables, only Bobcat, K8/K10, and P4 have throughput of one per 2 or 3 clocks for movd/movq int->xmm).  The biggest problem is K10, with something like one per 2.8c throughput (according to a couple reports from http://users.atw.hu/instlatx64/, e.g.  http://users.atw.hu/instlatx64/AuthenticAMD0100FA0_K10_Thuban_InstLatX64.txt).  Agner Fog says 3, but none of these are measuring with other instructions mixed in.

Some CPUs have better than one-per-clock throughput for movd/movq: Core2 is 0.5, and Nehalem is 0.33.  So do we hurt them a lot to help PhenomII?  I'd guess that Core2+Nehalem has somewhat more weight in tune=generic than K10.  Some AMD PhenomII CPUs are still around, though.  (But we could exclude them for code built with -mssse3)


---------

Probably the deciding factor for tune=generic is whether it hurts AMD Bulldozer-family significantly or at all.  It looks there's not much difference either way: similar throughput and latency.

However, store/reload may have an advantage when two cores in a cluster are competing for their shared vector unit.  Probably both of movd's macro-ops need to run on the shared vector unit, but for store/reload maybe only the load needs the shared resource.  IDK if this is correct or relevant, though.  Probably -mtune=bdver* should keep using store/reload, but this might not be enough of a reason to stop -mtune=generic from using movd.


Agner Fog's microarch pdf (Bulldozer section 18.11) says:

  > Nevertheless, I cannot confirm that it is faster to move data from a general purpose register
  > to a vector register through a memory intermediate, as recommended in AMD's optimization guide.

That AMD optimization guide advice may have been left over from K8/K10, where movd/movq from integer->vector has bad throughput.

As far as latency goes, scalar store -> vector reload is 10c on Bulldozer according to Agner Fog's numbers, while movd/movq is 10c on Bulldozer/Piledriver, and 5c on Steamroller.  (Steamroller also appears to have reduced the store-forwarding latency to 6c.  Agner's tables are supposed to have the store+load latencies add up to the store-forwarding latency.)

Store/reload is 2 instructions / 2 m-ops, but movd or movq is 1 instruction / 2 m-ops.  This is mostly ok for the decoders, but bdver1 can't decode in a 2-2 pattern (ver2/ver3 can).

Scheduling instructions to avoid consecutive multi-uop instructions may help decode throughput on bdver1.  But pairs of 2 m-op instructions are good on bdver2 and later.


With SSE4, pinsrd/q is probably good, because it's still only 2 m-ops on Bulldozer-family.  Indeed, -mtune=bdver1 uses 2x store/reload and 2x pinsrd for
_mm_set_epi32(d,c,b,a).

        movl    %edx, -12(%rsp)
        movd    -12(%rsp), %xmm1
        movl    %edi, -12(%rsp)
        movd    -12(%rsp), %xmm0
        pinsrd  $1, %ecx, %xmm1
        pinsrd  $1, %esi, %xmm0
        punpcklqdq      %xmm1, %xmm0


Even better would probably be

        movd    %edx, %xmm1
        movl    %edi, -12(%rsp)
        pinsrd  $1, %ecx, %xmm1    # for bdver2, schedule so it can decode in a 2-2 pattern with the other pinsrd
        movd    -12(%rsp), %xmm0
        pinsrd  $1, %esi, %xmm0
        punpcklqdq      %xmm1, %xmm0

The store/reload can happen in parallel with the direct movd int->xmm1.  This would be pretty reasonable for tune=generic, and should run well on Intel SnB-family CPUs.


-----

For -msse4 -mtune=core2, -mtune=nehalem, probably this is optimal:

        movd    %edi, %xmm0
        pinsrd  $1, %esi, %xmm0
        pinsrd  $2, %edx, %xmm0
        pinsrd  $3, %ecx, %xmm0

movd can run on any port and pinsrd is only 1 uop.  So this has a total latency of 2 + 3*1 = 5c on Core2 Wolfdale.  (First-gen core2 doesn't have SSE4.1).  Front-end bottlenecks are more common on Core2/Nehalem since they don't have a uop-cache, so fewer instructions is probably a good bet even at the expense of latency.

It might not be worth the effort to get gcc to emit this for Core2/Nehalem, since they're old and getting less relevant all the time.

It may also be good for -mtune=silvermont or KNL, though, since they also have 1 uop pinsrd/q.  But with 3c latency for pinsrd, the lack ILP may be a big problem.  Also, decode on Silvermont without VEX will stall if the pinsrd needs a REX (too many prefixes).  KNL should always use VEX or EVEX to avoid that.


---


### compiler : `gcc`
### title : `SCEV final value replacement fails to recognise sum computed by loop`
### open_at : `2017-05-22T06:58:43Z`
### last_modified_date : `2021-08-07T06:43:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80852
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c`
### version : `8.0`
### severity : `normal`
### contents :
Consider this (slightly odd) code:

int foo(int num) {
    int a = 0;
    for (int x = 0; x < num; x+=2) {
      if (!(x % 2)) {
        a += x;
      }
   }
    return a;
  }

Note that the condition !(x % 2) is always true.

In clang and -O3 -march=core-avx2 you get:

foo(int):                             # @square(int)
        test    edi, edi
        jle     .LBB0_1
        add     edi, -1
        mov     eax, edi
        shr     eax
        lea     ecx, [rax - 1]
        imul    ecx, eax
        and     ecx, -2
        and     edi, -2
        add     edi, ecx
        mov     eax, edi
        ret
.LBB0_1:
        xor     edi, edi
        mov     eax, edi
        ret

This is clever as it avoids looping altogether.

gcc however doesn't know this trick and you get:

foo(int):
        test    edi, edi
        jle     .L7
        lea     eax, [rdi-1]
        mov     ecx, eax
        shr     ecx
        add     ecx, 1
        cmp     eax, 17
        jbe     .L8
        mov     edx, ecx
        vmovdqa ymm1, YMMWORD PTR .LC0[rip]
        xor     eax, eax
        vpxor   xmm0, xmm0, xmm0
        vmovdqa ymm2, YMMWORD PTR .LC1[rip]
        shr     edx, 3
.L5:
        add     eax, 1
        vpaddd  ymm0, ymm0, ymm1
        vpaddd  ymm1, ymm1, ymm2
        cmp     eax, edx
        jb      .L5
        vpxor   xmm1, xmm1, xmm1
        mov     esi, ecx
        vperm2i128      ymm2, ymm0, ymm1, 33
        and     esi, -8
        vpaddd  ymm0, ymm0, ymm2
        lea     edx, [rsi+rsi]
        vperm2i128      ymm2, ymm0, ymm1, 33
        vpalignr        ymm2, ymm2, ymm0, 8
        vpaddd  ymm0, ymm0, ymm2
        vperm2i128      ymm1, ymm0, ymm1, 33
        vpalignr        ymm1, ymm1, ymm0, 4
        vpaddd  ymm0, ymm0, ymm1
        vmovd   eax, xmm0
        cmp     ecx, esi
        je      .L12
        vzeroupper
.L3:
        lea     ecx, [rdx+2]
        add     eax, edx
        cmp     edi, ecx
        jle     .L10
        add     eax, ecx
        lea     ecx, [rdx+4]
        cmp     ecx, edi
        jge     .L10
        add     eax, ecx
        lea     ecx, [rdx+6]
        cmp     edi, ecx
        jle     .L10
        add     eax, ecx
        lea     ecx, [rdx+8]
        cmp     edi, ecx
        jle     .L10
        add     eax, ecx
        lea     ecx, [rdx+10]
        cmp     edi, ecx
        jle     .L10
        add     eax, ecx
        lea     ecx, [rdx+12]
        cmp     edi, ecx
        jle     .L10
        add     eax, ecx
        lea     ecx, [rdx+14]
        cmp     edi, ecx
        jle     .L10
        add     eax, ecx
        add     edx, 16
        lea     ecx, [rax+rdx]
        cmp     edi, edx
        cmovg   eax, ecx
        ret
.L7:
        xor     eax, eax
.L10:
        ret
.L12:
        vzeroupper
        ret
.L8:
        xor     edx, edx
        xor     eax, eax
        jmp     .L3
.LC0:
        .long   0
        .long   2
        .long   4
        .long   6
        .long   8
        .long   10
        .long   12
        .long   14
.LC1:
        .long   16
        .long   16
        .long   16
        .long   16
        .long   16
        .long   16
        .long   16
        .long   16


---


### compiler : `gcc`
### title : `hot path is slowed down when the cold return path is merged into it`
### open_at : `2017-05-22T10:11:33Z`
### last_modified_date : `2021-10-01T03:05:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80854
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `7.1.0`
### severity : `normal`
### contents :
i see subomptimal code gen for

float foo (float x)
{
  if (__builtin_expect (x > 0, 0))
    if (x>2) return 0;
  return x*x;
}

because the return path merge causes extra register move in the hot path
https://godbolt.org/g/AZxxrR

x86_64:

foo:
        pxor    %xmm1, %xmm1
        ucomiss %xmm1, %xmm0
        ja      .L8
.L2:
        movaps  %xmm0, %xmm1  // extra reg move
        mulss   %xmm0, %xmm1
.L1:
        movaps  %xmm1, %xmm0  // extra reg move
        ret
.L8:
        ucomiss .LC1(%rip), %xmm0
        jbe     .L2
        jmp     .L1           // need not jmp back
.LC1:
        .long   1073741824


aarch64:

foo:
	fcmpe	s0, #0.0
	bgt	.L8
.L2:
	fmul	s1, s0, s0
.L1:
	fmov	s0, s1   // extra reg move
	ret
	.p2align 3
.L8:
	fmov	s2, 2.0e+0
	movi	v1.2s, #0
	fcmpe	s0, s2
	ble	.L2
	b	.L1    // need not jmp back

i wonder if gcc could do better if there is information about hot/cold paths (by not merging the hot/cold return paths in some cases).


---


### compiler : `gcc`
### title : `slow compare_exchange_weak with unintegral type`
### open_at : `2017-05-22T14:28:06Z`
### last_modified_date : `2022-01-06T04:10:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80857
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.0`
### severity : `enhancement`
### contents :
Created attachment 41403
example

Compared to gcc 6.2.0, function func2 work slower:

gcc 6.2.0
Result 49999995000000. Time 110
Result 49999995000000. Time 110
gcc 7.1.0
Result 49999995000000. Time 98
Result 49999995000000. Time 154

Build options:
-m64 -Wextra -Wall -Werror -Wpedantic -Wformat-security -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -pthread -std=c++17 -DNDEBUG -Ofast -funroll-loops -fomit-frame-pointer -Wno-misleading-indentation -g -mfpmath=sse


Function func2 call function

template<typename T>
inline void atomic_fetch_add(std::atomic<T> &obj, const T& arg) noexcept {
    T current = obj;
    while (!obj.compare_exchange_weak(current, current + arg));
}

where T == std::chrono::milliseconds


---


### compiler : `gcc`
### title : `ARM (VFPv3): Inefficient float-to-char conversion goes through memory`
### open_at : `2017-05-22T21:01:34Z`
### last_modified_date : `2021-09-27T07:12:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80861
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 41407
Input C file for triggering the bug

Consider the attached code:

$ cat tst.c
char fn1(float p1) {
  return (char) p1;
}

GCC from trunk from two weeks ago generates this code on ARM:

$ gcc tst.c -O3 -S -o -
	.arch armv7-a
	.eabi_attribute 28, 1
	.eabi_attribute 20, 1
	.eabi_attribute 21, 1
	.eabi_attribute 23, 3
	.eabi_attribute 24, 1
	.eabi_attribute 25, 1
	.eabi_attribute 26, 1
	.eabi_attribute 30, 2
	.eabi_attribute 34, 1
	.eabi_attribute 18, 4
	.file	"tst.c"
	.text
	.align	2
	.global	fn1
	.syntax unified
	.arm
	.fpu vfpv3-d16
	.type	fn1, %function
fn1:
	@ args = 0, pretend = 0, frame = 8
	@ frame_needed = 0, uses_anonymous_args = 0
	@ link register save eliminated.
	vcvt.u32.f32	s15, s0
	sub	sp, sp, #8
	vstr.32	s15, [sp, #4]	@ int
	ldrb	r0, [sp, #4]	@ zero_extendqisi2
	add	sp, sp, #8
	@ sp needed
	bx	lr
	.size	fn1, .-fn1
	.ident	"GCC: (GNU) 8.0.0 20170510 (experimental)"


Going through memory for the int-to-char truncation after the float-to-int conversion (vcvt) is excessive. For comparison, this is the entire code generated by Clang:

@ BB#0:
	vcvt.u32.f32	s0, s0
	vmov	r0, s0
	bx	lr

And this is what CompCert produces for the core of the function (stack manipulation code omitted):

	vcvt.u32.f32 s12, s0
	vmov	r0, s12
	and	r0, r0, #255


My GCC version:

Target: armv7a-eabihf
Configured with: --target=armv7a-eabihf --with-arch=armv7-a --with-fpu=vfpv3-d16 --with-float-abi=hard --with-float=hard
Thread model: single
gcc version 8.0.0 20170510 (experimental) (GCC)


---


### compiler : `gcc`
### title : `gcc does not emit cmov for minmax`
### open_at : `2017-05-24T14:07:07Z`
### last_modified_date : `2023-07-21T07:40:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80874
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Hello,
Considering the following code:
------------------
struct pair {
    int min, max;
};

pair minmax1(int x, int y) {
    if (x > y)
      return {y, x};
    else
      return {x, y};
}

#include <algorithm>

std::pair<int, int> minmax2(int x, int y) {
    return std::minmax(x, y);
}

auto minmax3(int x, int y) {
    return std::minmax(x, y);
}
-------------------
I've found that for minmax1 and minmax 2, gcc fails to emit cmov at -03. Instead it produces the following:

minmax1(int, int):
        cmp     edi, esi
        jle     .L2
        mov     eax, edi
        mov     edi, esi
        mov     esi, eax
.L2:
        mov     eax, edi
        sal     rsi, 32
        or      rax, rsi
        ret
------------ 
For minmax3, the asm should be the same (I think), but it produces a more complex code.


---


### compiler : `gcc`
### title : `Implement Windows native TLS`
### open_at : `2017-05-25T16:57:26Z`
### last_modified_date : `2021-08-22T08:00:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80881
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.1.0`
### severity : `enhancement`
### contents :
GCC 7.1.0 compiled OpenMP applications fail with invalid memory access to address 0.

Used configuration:
    ../../src/gcc-7.1.0/configure --host=x86_64-w64-mingw32 --enable-languages=c,c++ --enable-seh-exceptions --enable-threads=posix --enable-tls --disable-nls --disable-shared --enable-static --enable-fully-dynamic-string --enable-lto --enable-plugins --enable-libgomp --with-dwarf2 --disable-win32-registry --enable-version-specific-runtime-libs --prefix=/mingw64-64 --with-sysroot=/mingw64-64 --target=x86_64-w64-mingw32 --enable-targets=all --enable-checking=release --with-gmp=/usr/new-gcc/lib/gmp-5.0.5 --with-mpfr=/usr/new-gcc/lib/mpfr-2.4.2 --with-mpc=/usr/new-gcc/lib/mpc-0.9 --with-isl=/usr/new-gcc/lib/isl-0.18 --with-cloog=/usr/new-gcc/lib/cloog-0.18.4 --with-host-libstdcxx='-lstdc++ -lsupc++' --disable-cloog-version-check --enable-cloog-backend=isl
    Thread model: posix
    gcc version 7.1.0 (GCC)

Sample application:
    #include <stdlib.h>
    #include <stdio.h>
    
    #define N 1024
    
    int main() {
    	int i;
    	float var[N];
    	volatile float PI = 3.1415927;
    	
    #	pragma omp parallel for private(i)
    	for (i = 0; i < N; i++) {
    		var[i] = (1024.0f / PI) + 0.5f;
    	}
    	
    	return EXIT_SUCCESS;
    }

Error message from Dr.Memory (32-bit variant):
    Error #1: UNADDRESSABLE ACCESS: reading 0x00000000-0x00000004 4 byte(s)
    # 0 GOMP_parallel               [../../../../../src/gcc-7.1.0/libgomp/libgomp.h:677]
    # 1 main                        [h:\Temp\cpp017/test.c:11]
    Note: @0:00:00.642 in thread 6700
    Note: instruction: mov    %gs:0x00 -> %esi

Error message from Dr.Memory (64-bit variant):
    Error #1: UNADDRESSABLE ACCESS: reading 0x0000000000000000-0x0000000000000008 8 byte(s)
    # 0 GOMP_parallel               [../../../../src/gcc-7.1.0/libgomp/libgomp.h:677]
    # 1 main                        [h:\Temp\cpp017/test.c:11]
    Note: @0:00:00.170 in thread 1320
    Note: instruction: mov    %fs:0x00 -> %rdi


---


### compiler : `gcc`
### title : `ARM: Useless initialization of struct passed by value`
### open_at : `2017-05-28T17:02:44Z`
### last_modified_date : `2021-08-16T00:56:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80905
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 41432
Input C file for triggering the issue

Input program:

$ cat tst.c
struct S0 {
  int f0;
  int f1;
  int f2;
  int f3;
};

int f1(struct S0 p) {
    return p.f0;
}

int f2(struct S0 p) {
    return p.f0 + p.f3;
}



When entering the function, GCC copies the entire struct from registers to the stack, even fields that are never used. Fields that *are* used are then reloaded from the stack even if they are still available in the very same registers:

$ gcc tst.c -Wall -W -O3 -S -o -
	.arch armv7-a
	.eabi_attribute 28, 1
	.eabi_attribute 20, 1
	.eabi_attribute 21, 1
	.eabi_attribute 23, 3
	.eabi_attribute 24, 1
	.eabi_attribute 25, 1
	.eabi_attribute 26, 1
	.eabi_attribute 30, 2
	.eabi_attribute 34, 1
	.eabi_attribute 18, 4
	.file	"tst.c"
	.text
	.align	2
	.global	f1
	.syntax unified
	.arm
	.fpu vfpv3-d16
	.type	f1, %function
f1:
	@ args = 0, pretend = 0, frame = 16
	@ frame_needed = 0, uses_anonymous_args = 0
	@ link register save eliminated.
	sub	sp, sp, #16
	add	ip, sp, #16
	stmdb	ip, {r0, r1, r2, r3}
	ldr	r0, [sp]
	add	sp, sp, #16
	@ sp needed
	bx	lr
	.size	f1, .-f1
	.align	2
	.global	f2
	.syntax unified
	.arm
	.fpu vfpv3-d16
	.type	f2, %function
f2:
	@ args = 0, pretend = 0, frame = 16
	@ frame_needed = 0, uses_anonymous_args = 0
	@ link register save eliminated.
	sub	sp, sp, #16
	add	ip, sp, #16
	stmdb	ip, {r0, r1, r2, r3}
	ldr	r0, [sp]
	ldr	r3, [sp, #12]
	add	r0, r0, r3
	add	sp, sp, #16
	@ sp needed
	bx	lr
	.size	f2, .-f2
	.ident	"GCC: (GNU) 8.0.0 20170527 (experimental)"

Target: armv7a-eabihf
Configured with: --target=armv7a-eabihf --with-arch=armv7-a --with-fpu=vfpv3-d16 --with-float-abi=hard --with-float=hard
gcc version 8.0.0 20170527 (experimental) (GCC)


This seems to be specific to ARM as I cannot reproduce this behavior on x86-64 or PowerPC.


For comparison, LLVM generates the following code for ARM:

f1:
	.fnstart
@ BB#0:
	bx	lr

f2:
	.fnstart
@ BB#0:
	add	r0, r0, r3
	bx	lr


---


### compiler : `gcc`
### title : `missed bit information propagation`
### open_at : `2017-05-30T06:50:25Z`
### last_modified_date : `2023-10-12T15:56:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80917
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Take the following code:

int f(unsigned a)
{
  if ((a & 2) == 0)
    return 0;

  a += 4;

  return (a & 2) != 0;
}

The addition clearly cannot affect the repeat of the test of the second bit.  Still, with the current trunk compiler I get with -O3 on x86-64:

	xorl	%eax, %eax
	testb	$2, %dil
	je	.L1
	shrl	%edi
	movl	%edi, %eax
	andl	$1, %eax
.L1:
	ret


It might get quickly expansive but the bit set/unset information could be tracked passed the test and through arithmetic operations like additions.


---


### compiler : `gcc`
### title : `SLP vectorization does not handle induction in outer loop vectorization`
### open_at : `2017-05-31T13:24:38Z`
### last_modified_date : `2020-11-03T12:41:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80928
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.0`
### severity : `normal`
### contents :
int a[1024];
void foo (int n)
{
  for (int i = 0; i < 1020; i += 5)
    {
      a[i] = i;
      a[i+1] = i;
      a[i+2] = i;
      a[i+3] = i;
      a[i+4] = i;
    }
}

is not vectorized.

t.c:4:3: note: === vect_analyze_slp ===
t.c:4:3: note: Build SLP for a[i_17] = i_17;
t.c:4:3: note: Build SLP for a[_1] = i_17;
t.c:4:3: note: Build SLP for a[_2] = i_17;
t.c:4:3: note: Build SLP for a[_3] = i_17;
t.c:4:3: note: Build SLP for a[_4] = i_17;
t.c:4:3: note: vect_is_simple_use: operand i_17
t.c:4:3: note: def_stmt: i_17 = PHI <i_13(4), 0(2)>
t.c:4:3: note: type of def: induction
t.c:4:3: note: Build SLP failed: illegal type of def i_17

that's because we do not handle inductions (neither during SLP discovery
nor later during code-gen).

      /* Check the types of the definitions.  */
      switch (dt)
        {
        case vect_constant_def:
        case vect_external_def:
        case vect_reduction_def:
          break;

        case vect_internal_def:
          oprnd_info->def_stmts.quick_push (def_stmt);
          break;

        default:
          /* FORNOW: Not supported.  */
          if (dump_enabled_p ())
            {
              dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,
                               "Build SLP failed: illegal type of def ");
              dump_generic_expr (MSG_MISSED_OPTIMIZATION, TDF_SLIM, oprnd);
              dump_printf (MSG_MISSED_OPTIMIZATION, "\n");
            }

          return -1;
        }


---


### compiler : `gcc`
### title : `[9/10/11/12 Regression] Division with constant no more optimized to mult highpart`
### open_at : `2017-05-31T13:52:40Z`
### last_modified_date : `2021-12-23T22:35:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80929
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.1.1`
### severity : `normal`
### contents :
Since PR79665, division with known denominator are no more optimized to __umulhisi3 but transfomed to an expensive signed division instead.

unsigned scale255 (unsigned val)
{
    return val / 255;
}

$ avr-gcc -O2 -mmcu=atmega328 -S ...

Reason is that PR79655 uses rtlanal.c::seq_cost() to compute the cost of (un)signed division, and seq_cost assumes anything that's not a single_set has  the very low cost of 1.

However avr BE, represents division as a PARALLEL, not as a single_set, i.e. something like:

(insn 14 13 0 (parallel [
            (set (reg:HI 52)
                (div:HI (reg:HI 47)
                    (reg:HI 54)))
            (set (reg:HI 53)
                (mod:HI (reg:HI 47)
                    (reg:HI 54)))
            (clobber (reg:QI 21 r21))
            (clobber (reg:HI 22 r22))
            (clobber (reg:HI 24 r24))
            (clobber (reg:HI 26 r26))
        ]) "scale.c":7 -1
     (nil))


---


### compiler : `gcc`
### title : `redundant bcopy/memcpy/strcpy to a non-local object not eliminated`
### open_at : `2017-06-01T03:13:42Z`
### last_modified_date : `2021-12-14T17:49:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80937
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `enhancement`
### contents :
GCC eliminates redundant calls to bcopy and memcpy involving the same local variable but doesn't eliminate redundant calls to the same two functions when the target is the same global variable, or a pointer to some unknown variable.  It does, however, eliminate calls to bzero and memset regardless of whether of the storage duration of the target object.  This seems like a missed optimization opportunity.

$ cat t.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout t.c
void sink (void*);

char a[33];

void memcpy_global (const void *s)
{
  __builtin_memcpy (a, s, sizeof a);
  __builtin_memcpy (a, s, sizeof a); 
  sink (a);
}

void bcopy_global (const void *s)
{
  __builtin_bcopy (s, a, sizeof a);
  __builtin_bcopy (s, a, sizeof a);   
  sink (a);
}

void memset_global (void)
{
  __builtin_memset (a, 0, sizeof a);
  __builtin_memset (a, 0, sizeof a);
  sink (a);
}

void bzero_global (void)
{
  __builtin_memset (a, 0, sizeof a);
  __builtin_memset (a, 0, sizeof a);
  sink (a);
}

void memcpy_local (const void *s)
{
  char a[33];
  __builtin_memcpy (a, s, sizeof a);
  __builtin_memcpy (a, s, sizeof a);
  sink (a);
}

void bcopy_local (const void *s)
{
  char a[33];
  __builtin_bcopy (s, a, sizeof a);
  __builtin_bcopy (s, a, sizeof a);
  sink (a);
}

void memset_local (void)
{
  char a[33];
  __builtin_memset (a, 0, sizeof a);
  __builtin_memset (a, 0, sizeof a);
  sink (a);
}

void bzero_local (void)
{
  char a[33];
  __builtin_memset (a, 0, sizeof a);
  __builtin_memset (a, 0, sizeof a);
  sink (a);
}


;; Function memcpy_global (memcpy_global, funcdef_no=0, decl_uid=1796, cgraph_uid=0, symbol_order=1)

memcpy_global (const void * s)
{
  <bb 2> [100.00%]:
  MEM[(char * {ref-all})&a] = MEM[(char * {ref-all})s_2(D)];
  MEM[(char * {ref-all})&a] = MEM[(char * {ref-all})s_2(D)];
  sink (&a); [tail call]
  return;

}



;; Function bcopy_global (bcopy_global, funcdef_no=1, decl_uid=1799, cgraph_uid=1, symbol_order=2)

bcopy_global (const void * s)
{
  <bb 2> [100.00%]:
  __builtin_bcopy (s_2(D), &a, 33);
  __builtin_bcopy (s_2(D), &a, 33);
  sink (&a); [tail call]
  return;

}



;; Function memset_global (memset_global, funcdef_no=2, decl_uid=1802, cgraph_uid=2, symbol_order=3)

memset_global ()
{
  <bb 2> [100.00%]:
  __builtin_memset (&a, 0, 33);
  sink (&a); [tail call]
  return;

}



;; Function bzero_global (bzero_global, funcdef_no=9, decl_uid=1805, cgraph_uid=3, symbol_order=4)

bzero_global ()
{
  <bb 2> [100.00%]:
  memset_global (); [tail call]
  return;

}



;; Function memcpy_local (memcpy_local, funcdef_no=4, decl_uid=1808, cgraph_uid=4, symbol_order=5)

memcpy_local (const void * s)
{
  char a[33];

  <bb 2> [100.00%]:
  MEM[(char * {ref-all})&a] = MEM[(char * {ref-all})s_2(D)];
  sink (&a);
  a ={v} {CLOBBER};
  return;

}



;; Function bcopy_local (bcopy_local, funcdef_no=11, decl_uid=1812, cgraph_uid=5, symbol_order=6)

bcopy_local (const void * s)
{
  <bb 2> [100.00%]:
  memcpy_local (s_2(D)); [tail call]
  return;

}



;; Function memset_local (memset_local, funcdef_no=6, decl_uid=1816, cgraph_uid=6, symbol_order=7)

memset_local ()
{
  char a[33];

  <bb 2> [100.00%]:
  __builtin_memset (&a, 0, 33);
  sink (&a);
  a ={v} {CLOBBER};
  return;

}



;; Function bzero_local (bzero_local, funcdef_no=13, decl_uid=1820, cgraph_uid=7, symbol_order=8)

bzero_local ()
{
  <bb 2> [100.00%]:
  memset_local (); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `-Wreturn-type "control reaches end of non-void function" false positive with -fsanitize=address`
### open_at : `2017-06-02T12:46:41Z`
### last_modified_date : `2022-11-17T02:50:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80959
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `c`
### version : `7.1.0`
### severity : `normal`
### contents :
Created attachment 41458
Reproducer

I stumbled on this while building the binutils-gdb repo with -fsanitize=address with gcc 7.1.0 and reproduced it with gcc master from today.  It doesn't happen with my distro compiler (5.4.0-6ubuntu1~16.04.4).

gcc reports that the control can reach the end of the function, but I think it's not the case.  This is the smallest reproducer I managed to find.  Note that the problem disappears if you enable optimizations (-O > 0) or remove -fsanitize=address.

Compile the attached file with:

$ /opt/gcc/git/bin/gcc -Wreturn-type -Werror -O0 -fsanitize=address -c test.c
test.c: In function 'foo':
test.c:23:1: error: control reaches end of non-void function [-Werror=return-type]
 }
 ^
cc1: all warnings being treated as errors


---


### compiler : `gcc`
### title : `-Os generates larger code than -O1 because loop header copying is not performend and thus loop is not removed`
### open_at : `2017-06-05T10:15:38Z`
### last_modified_date : `2022-01-05T10:25:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80980
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
$ cat foo.f90
program main
  integer :: i
  do i=1,1
    print *,i
  end do
end program main
$ gfortran -o os.s -S -Os foo.f90 
$ gfortran -o o1.s -S -O1 foo.f90 
$ wc -l os.s
82 os.s
$ wc -l o1.s
63 o1.s

This is probably due to -Os not removing the loop.
The -fdump-tree-optimized dump shows for -Os

  <bb 2> [15.00%]:
  i = 1;

  <bb 3> [100.00%]:
  i.2_1 = i;
  if (i.2_1 > 1)
    goto <bb 5>; [15.00%]
  else
    goto <bb 4>; [85.00%]

and for -O1

  <bb 2> [15.00%]:
  i = 1;

$ gfortran -v
Es werden eingebaute Spezifikationen verwendet.
COLLECT_GCC=gfortran
COLLECT_LTO_WRAPPER=/home/ig25/lib/gcc/x86_64-pc-linux-gnu/8.0.0/lto-wrapper
Ziel: x86_64-pc-linux-gnu
Konfiguriert mit: ../trunk/configure --prefix=/home/ig25 --enable-languages=c,c++,fortran --enable-maintainer-mode
Thread-Modell: posix
gcc-Version 8.0.0 20170525 (experimental) (GCC)


---


### compiler : `gcc`
### title : `missing optimization for restricted pointers passed to functions`
### open_at : `2017-06-07T19:54:47Z`
### last_modified_date : `2023-02-24T05:22:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81008
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
While exploring ways to annotate C++ classes or member functions to indicate that accessing an object of the class does not change a different object of that class I noticed that GCC doesn't take full advantage of the basic guarantee on restrict-qualified pointers: that if one is used to access an object, the same object may only be modified through an [lvalue] based on the same pointer.

In the example below, the call to foo() is guaranteed not to modify the object at *p because p is a restricted pointer and there is no other pointer based on p can exist.

Clang emits optimal code for the test case (i.e., just a jump to foo()).

$ cat t.c && gcc -O3 -S -Wall -Wrestrict -fdump-tree-optimized=/dev/stdout t.c

void f (void);

void g (int* restrict p)
{
  int x = *p;
  f ();
  if (x != *p)
    __builtin_abort ();
}

;; Function g (g, funcdef_no=0, decl_uid=1795, cgraph_uid=0, symbol_order=0)

g (int * restrict p)
{
  int x;
  int _1;

  <bb 2> [100.00%]:
  x_4 = *p_3(D);
  f ();
  _1 = *p_3(D);
  if (_1 != x_4)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [0.04%]:
  __builtin_abort ();

  <bb 4> [99.96%]:
  return;

}


In the above, the only way f() could modify *p is like so:

  int *q;
  void f (void) { *q = 0; }

and

  void h (void)
  {
    int d = 0;
    q = &d;
    g (&d);
  }

but such a modification is undefined thanks to the access to d in f() through a restricted pointer.


---


### compiler : `gcc`
### title : `missing aliasing optimization for const restrict pointers`
### open_at : `2017-06-07T21:00:59Z`
### last_modified_date : `2022-03-15T02:15:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81009
### status : `UNCONFIRMED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
One of the constraints on the use of restrict-qualified pointers is that the const qualifier effectively imposes a binding requirement on such a pointer preventing it from being used to modify the object (this is specified in 6.7.3.1, p4: "If L is used to access the value of the object X that it designates, and X is also modified (by any means), then the following requirements apply: T shall not be const-qualified.")

This constraint doesn't exist for ordinary (not restrict-qualified) const pointers where the constness can be cast away and the result used to modify the pointed-to object (provided the object itself isn't const).

GCC could take advantage of this constraint on programs by assuming that the call to f() in g() below doesn't modify *p, thus eliminating the subsequent test for x != *p.

$ cat t.c && gcc -O3 -S -Wall -Wrestrict -fdump-tree-optimized=/dev/stdout t.c

void f (const int*);

static void g (const int* restrict p)
{
  int x = *p;
  f (p);         // f() cannot modify *p here
  if (x != *p)   // cannot be true
    __builtin_abort ();
}

void h (void)
{
  int i = 0;
  g (&i);
}

;; Function h (h, funcdef_no=1, decl_uid=1799, cgraph_uid=1, symbol_order=1)

h ()
{
  int i;
  int _4;

  <bb 2> [100.00%]:
  i = 0;
  f (&i);
  _4 = MEM[(const int *)&i];
  if (_4 != 0)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [0.04%]:
  __builtin_abort ();

  <bb 4> [99.96%]:
  i ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `ARM: Spill instead of register copy / dead store on int-to-double conversion`
### open_at : `2017-06-07T21:42:44Z`
### last_modified_date : `2021-09-27T07:05:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81012
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 41496
Input C file for triggering the issue

Input file (also in attachment):

double fn2(int p1, int p2) {
  double a = p1;
  if (744073425321881 * p2 + 5)
    a = 2;
  return a;
}

Generated code on ARMv7 for VFPv3:

$ gcc tst.c -Wall -Wextra -O3 -fomit-frame-pointer -S -o -
[...]
fn2:
	@ args = 0, pretend = 0, frame = 8
	@ frame_needed = 0, uses_anonymous_args = 0
	@ link register save eliminated.
	movw	r3, #42171
	movt	r3, 2
	push	{r4, r5}
	movw	r2, #65433
	sub	sp, sp, #8
	asr	r5, r1, #31
	movt	r2, 6195
	mvn	r4, #4
	mul	r3, r3, r1
	str	r0, [sp, #4]           // SPILL
	mla	r0, r2, r5, r3
	mvn	r5, #0
	umull	r2, r3, r1, r2
	add	r3, r0, r3
	cmp	r3, r5
	cmpeq	r2, r4
	vldreq.32	s15, [sp, #4]	@ int
	vmovne.f64	d0, #2.0e+0
	vcvteq.f64.s32	d0, s15
	add	sp, sp, #8
	@ sp needed
	pop	{r4, r5}
	bx	lr
	.size	fn2, .-fn2
	.ident	"GCC: (GNU) 8.0.0 20170606 (experimental)"

Note the store I marked "SPILL". It is a store of the integer register r0 which is reloaded on the line marked "@ int" into a floating-point register for subsequent int-to-double conversion. The spill frees r0 for other use, but it would be better to just replace the spill/reload sequence with

        vmov s15, r0

since the register is available.

Also, if the large constant 744073425321881 in the if condition is changed to something smaller like 1881 (that fits into a mov's immediate field), GCC generates this code:

fn2:
	@ args = 0, pretend = 0, frame = 8
	@ frame_needed = 0, uses_anonymous_args = 0
	@ link register save eliminated.
	movw	r3, #1881
	sub	sp, sp, #8
	mul	r1, r3, r1
	str	r0, [sp, #4]                // DEAD STORE
	cmn	r1, #5
	vmovne.f64	d0, #2.0e+0
	vmoveq	s15, r0	@ int
	vcvteq.f64.s32	d0, s15
	add	sp, sp, #8
	@ sp needed
	bx	lr

This does perform a conditional move from r0 to s15, but it also generates a dead store to the stack.

Clang and CompCert both just do a copy and don't touch the stack for this value.

$ gcc -v
[...]
Target: armv7a-eabihf
Configured with: --target=armv7a-eabihf --with-arch=armv7-a --with-fpu=vfpv3-d16 --with-float-abi=hard --with-float=hard
Thread model: single
gcc version 8.0.0 20170510 (experimental) (GCC)

Not sure if this is related to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80861 which also the stack for a float-to-char conversion. But that's the other direction, and if I understand correctly, there the problem is related to the final sign extension.


---


### compiler : `gcc`
### title : `inefficient union with long double argument on 32-bit x86`
### open_at : `2017-06-13T15:40:12Z`
### last_modified_date : `2021-09-04T13:52:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81085
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.0`
### severity : `enhancement`
### contents :
Consider the following code accessing part of the representation of a long double function argument (which is similar to what various code in glibc libm does).

union u { long double d; unsigned int u[4]; };
unsigned int f (long double x) { union u u = { .d = x }; return u.u[0]; }

On x86_64-linux-gnu, building with -m64 -O2, I get the expected code:

        movl    8(%rsp), %eax
        ret

But with -m32 -O2 I get:

        subl    $28, %esp
        .cfi_def_cfa_offset 32
        fldt    32(%esp)
        fstpt   (%esp)
        movl    (%esp), %eax
        addl    $28, %esp
        .cfi_def_cfa_offset 4
        ret

The argument is already on the stack.  There is no need to load and store a copy of it in order to access part of it as an integer.


---


### compiler : `gcc`
### title : `Complex division misses BB vectorisation opportunity`
### open_at : `2017-06-19T10:29:31Z`
### last_modified_date : `2021-08-03T20:02:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81127
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.0`
### severity : `normal`
### contents :
This report has two parts. The first is about complex float division and the second about complex double division.

--- Part 1 ---

Consider:

#include <complex.h>
complex float f(complex float x, complex float y) {
  return x/y;
}

In gcc 7.1 with -O3 -march=core-avx2 -ffast-math you get:

f:
        vmovq   QWORD PTR [rsp-16], xmm1
        vmovss  xmm5, DWORD PTR [rsp-12]
        vmovss  xmm4, DWORD PTR [rsp-16]
        vmovq   QWORD PTR [rsp-8], xmm0
        vmovss  xmm0, DWORD PTR [rsp-4]
        vmovss  xmm3, DWORD PTR [rsp-8]
        vmulss  xmm2, xmm5, xmm5
        vmulss  xmm1, xmm0, xmm5
        vfmadd231ss     xmm2, xmm4, xmm4
        vfmadd231ss     xmm1, xmm3, xmm4
        vmulss  xmm3, xmm3, xmm5
        vdivss  xmm1, xmm1, xmm2
        vfmsub132ss     xmm0, xmm3, xmm4
        vdivss  xmm0, xmm0, xmm2
        vmovss  DWORD PTR [rsp-24], xmm1
        vmovss  DWORD PTR [rsp-20], xmm0
        vmovq   xmm0, QWORD PTR [rsp-24]
        ret

Note three calls to vmulss and two calls to vdivss

ICC on the other hand gives:

f:
        vcvtps2pd xmm2, xmm1                                    #3.12
        vcvtps2pd xmm4, xmm0                                    #3.12
        vmulpd    xmm8, xmm2, xmm2                              #3.12
        vunpckhpd xmm3, xmm2, xmm2                              #3.12
        vmulpd    xmm6, xmm3, xmm4                              #3.12
        vmovddup  xmm7, xmm2                                    #3.12
        vshufpd   xmm5, xmm4, xmm4, 1                           #3.12
        vshufpd   xmm9, xmm8, xmm8, 1                           #3.12
        vfmaddsub213pd xmm7, xmm5, xmm6                         #3.12
        vaddpd    xmm11, xmm8, xmm9                             #3.12
        vshufpd   xmm10, xmm7, xmm7, 1                          #3.12
        vdivpd    xmm12, xmm10, xmm11                           #3.12
        vcvtpd2ps xmm0, xmm12                                   #3.12
        ret   

Note two calls to vmulpd and one call to vdivpd.

Just for interest,if you increase the optimisation level (using -fp-model fast=2) ICC also offers this alternative:

f:
        vmovlhps  xmm2, xmm1, xmm1                              #3.12
        vmulps    xmm8, xmm2, xmm2                              #3.12
        vshufps   xmm9, xmm8, xmm8, 177                         #3.12
        vmovlhps  xmm4, xmm0, xmm0                              #3.12
        vaddps    xmm10, xmm8, xmm9                             #3.12
        vrcpps    xmm11, xmm10                                  #3.12
        vmovshdup xmm3, xmm2                                    #3.12
        vaddps    xmm12, xmm11, xmm11                           #3.12
        vmulps    xmm6, xmm4, xmm3                              #3.12
        vmulps    xmm14, xmm11, xmm10                           #3.12
        vmovsldup xmm7, xmm2                                    #3.12
        vshufps   xmm5, xmm4, xmm4, 177                         #3.12
        vfmaddsub213ps xmm7, xmm5, xmm6                         #3.12
        vfnmadd213ps xmm14, xmm11, xmm12                        #3.12
        vshufps   xmm13, xmm7, xmm7, 177                        #3.12
        vmulps    xmm0, xmm13, xmm14                            #3.12
        ret   

Note one call to vrcpps and four calls to vmulps and zero calls to vdivpd. 

--- Part 2 ---

Consider:

#include <complex.h>
complex double f(complex double x, complex double y) {
  return x/y;
}

In gcc 7.1 with -O3 -march=core-avx2 -ffast-math you get:

f:
        vmulsd  xmm4, xmm1, xmm3
        vmovapd xmm6, xmm0
        vmulsd  xmm5, xmm3, xmm3
        vmulsd  xmm6, xmm6, xmm3
        vfmadd231sd     xmm4, xmm0, xmm2
        vfmadd231sd     xmm5, xmm2, xmm2
        vfmsub132sd     xmm1, xmm6, xmm2
        vdivsd  xmm0, xmm4, xmm5
        vdivsd  xmm1, xmm1, xmm5
        ret

In ICC you get with -fp-model fast=2:

f:
        vunpcklpd xmm4, xmm2, xmm3                              #2.54
        vunpcklpd xmm6, xmm0, xmm1                              #2.54
        vunpckhpd xmm5, xmm4, xmm4                              #3.12
        vmulpd    xmm10, xmm4, xmm4                             #3.12
        vmulpd    xmm8, xmm5, xmm6                              #3.12
        vmovddup  xmm9, xmm4                                    #3.12
        vshufpd   xmm7, xmm6, xmm6, 1                           #3.12
        vshufpd   xmm11, xmm10, xmm10, 1                        #3.12
        vfmaddsub213pd xmm9, xmm7, xmm8                         #3.12
        vaddpd    xmm13, xmm10, xmm11                           #3.12
        vshufpd   xmm12, xmm9, xmm9, 1                          #3.12
        vdivpd    xmm0, xmm12, xmm13                            #3.12
        vunpckhpd xmm1, xmm0, xmm0                              #3.12
        ret  

This reduces the number of multiplications to two and the number of divisions to one again.  

It would be great to have benchmarks for all of this but I don't have a copy of ICC to test.


---


### compiler : `gcc`
### title : `[avr] C++ for -mmcu=atmega2560 doesn't do NRVO or RVO when class inherits from empty base`
### open_at : `2017-06-20T23:23:33Z`
### last_modified_date : `2021-07-24T17:41:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81147
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `7.2.0`
### severity : `normal`
### contents :
Created attachment 41594
Source file

Compilation of the attached example generates the also attached assembly. The file was compiled with the following flags. The same file compiled for x86_64 generates optimal code and doesn't make any copies. It can be seen that many copies happen between Z and X pointer registers in ld, st instructions that follow r26 and r30 register setup.

avr-g++ -std=c++14  -I. -I../avr-traits/include -I../avr-add/include -DNDEBUG -DBOOST_SPIRIT_X3_NO_RTTI -DBOOST_EXCEPTION_DISABLE -DBOOST_NO_EXCEPTIONS -DBOOST_ASIO_USE_STD_SYSTEM_ERROR -I../../../Beast/include -I../boost/libs/utility/include -I../boost/libs/tti/include -I../boost/libs/spirit/include -I../boost/libs/mpl/include -I../boost/libs/config/include -I../boost/libs/preprocessor/include -I../boost/libs/type_traits/include -I../boost/libs/core/include -I../boost/libs/fusion/include -I../boost/libs/utility/include -I../boost/libs/static_assert/include -I../boost/libs/variant/include -I../boost/libs/type_index/include -I../boost/libs/detail/include -I../boost/libs/assert/include -I../boost/libs/move/include -I../boost/libs/functional/include -I../boost/libs/math/include -I../boost/libs/throw_exception/include -I../boost/libs/tti/include -I../boost/libs/function_types/include -I../boost/libs/concept_check/include -I../boost/libs/range/include -I../boost/libs/iterator/include -I../boost/libs/optional/include -I../boost/libs/integer/include -I../boost/libs/predef/include -I../boost/libs/smart_ptr/include -I../boost/libs/intrusive/include -I../boost/libs/lexical_cast/include -I../boost/libs/numeric/conversion/include -I../boost/libs/array/include -I../boost/libs/container/include -I../boost/libs/system/include -I../boost/libs/asio/include -I../boost/libs/logic/include -I../boost/libs/regex/include -I../contiki++/include -DBOOST_SPIRIT_STANDARD_WIDE_NOVEMBER_10_2006_0913AM -DBOOST_SPIRIT_NO_STANDARD_WIDE -DBOOST_SPIRIT_NO_REAL_NUMBERS -fno-exceptions -fno-rtti -Wno-unused-local-typedefs -fno-use-cxa-atexit -save-temps -fverbose-asm -dap -fdump-tree-all-all -DCONTIKI=1 -DCONTIKI_TARGET_AVR_ATMEGA2560=1 -Wall -mmcu=atmega2560 -fno-strict-aliasing -I../contiki/platform/avr-atmega2560 -I. -I../contiki/core -I../contiki/cpu/avr  -DF_CPU=16000000UL -DAUTO_CRC_PADDING=2 -Os -ffunction-sections -fdata-sections  -I. -Isrc -I../contiki/platform/avr-atmega2560/. -I../contiki/platform/avr-atmega2560/apps -I../contiki/platform/avr-atmega2560/net -I../contiki/platform/avr-atmega2560/loader -I../contiki/cpu/avr/. -I../contiki/cpu/avr/dev -I../contiki/cpu/avr/radio/rf230bb -I../contiki/core/dev -I../contiki/core/lib -I../contiki/core/net -I../contiki/core/net/llsec -I../contiki/core/net/mac -I../contiki/core/net/rime -I../contiki/core/net/rpl -I../contiki/core/sys -I../contiki/core/cfs -I../contiki/core/ctk -I../contiki/core/lib/ctk -I../contiki/core/loader -I../contiki/core/. -I../contiki/core/sys -I../contiki/core/dev -I../contiki/core/lib -I../contiki/platform/avr-atmega2560/ -I../contiki -DCONTIKI_VERSION_STRING=\"Contiki-1d456a24b\" -MMD -c src/config-process.cpp


---


### compiler : `gcc`
### title : `poor code concatenating bitfields`
### open_at : `2017-06-21T19:19:41Z`
### last_modified_date : `2023-07-19T04:12:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81161
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Created attachment 41604
example

The example code is trying to use a set of adjacent single bit fields as a wider value.  (Think TREE_LANG_FLAG_{N,N+2} to explain why I can't just make a 3-bit bitfield).  We don't spot this is just extrating a wider bitfield[*]  here's the x86_64 code I get:
	movzbl	(%rdi), %edx
	movl	%edx, %eax
	movl	%edx, %ecx
	shrb	$2, %dl
	shrb	$4, %al
	shrb	$3, %cl
	andl	$1, %edx
	andl	$1, %eax
	andl	$1, %ecx
	sall	$2, %eax
	addl	%ecx, %ecx
	orl	%ecx, %eax
	orl	%edx, %eax
	ret

which is roughly doing:
   t1 = (val >> 2) & 1
   t2 = (val >> 3) & 1
   t3 = (val >> 4) & 1
   r = (t3 << 2) | (t2 + t1) | t1

optimal code would be something like:
   movzbl	(%rdi), %eax
   shrb $2,%al
   andl $7,%eax

this is similar to 68360, but looks sufficiently different to warrant a new report.

[*] where bitfield packing was from the other end of the object, one might want to swap the bitfields around to get a nice ordering.  Let's just assume little-endian packing for the sake of argument.


---


### compiler : `gcc`
### title : `bswap not recognized in |= statement`
### open_at : `2017-06-22T11:46:16Z`
### last_modified_date : `2021-12-08T18:56:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81174
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.0`
### severity : `enhancement`
### contents :
Created attachment 41610
bswap-issue.cc

In writting a big-endian bitfield accessor I noticed that bswap was not always recognized.

It appears the problem triggers together with |= statements, at least replacing the |= statement with += solves the issue.

I have attached a test case. The faulty one is the first, the two second ones work.


---


### compiler : `gcc`
### title : `missed tail recursion`
### open_at : `2017-06-26T10:37:39Z`
### last_modified_date : `2021-08-03T21:21:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81206
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
We do not handle the tail-recursion in gcc.dg/torture/pr81203.c because there's
a stmt in the way (see PR81203).  A proper fix is to re-do tail-recursion elimination isolating the individual paths to the return I think.


---


### compiler : `gcc`
### title : `No ipa-sra optimization for small struct / class`
### open_at : `2017-06-29T06:24:38Z`
### last_modified_date : `2019-09-19T02:50:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81248
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.1`
### severity : `normal`
### contents :
In the following minimal example, avr-g++ optimizes the call of f(n1) with ipa-sra, that is it replaces the pointer-registers with a value-register. The same should be possible for f(n2), but in this case avr-g++ doesn't make the same optimization.


#include <cstdint>
#include <type_traits>

struct A {
    A() = default;
    A(const A& o) = default;
    A(const volatile A& o) : m1(o.m1) {} 
    uint8_t m1{0};
};

volatile uint8_t v;

template<typename T>
void f(const T& x) __attribute__((noinline));
template<typename T>
void f(const T& x) {
    if constexpr(std::is_same<std::remove_cv_t<T>, A>::value) {
        v = x.m1;
    }
    else {
        v = x;
    }
}

uint8_t n1;
A n2;

int main() {
    f(n1);
    f(n2);
}


---


### compiler : `gcc`
### title : `x86 optimizer emits unnecessary LEA instruction when using AVX intrinsics`
### open_at : `2017-07-02T06:27:27Z`
### last_modified_date : `2021-12-14T21:52:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81274
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
When AVX intrinsics are used in a function, the x86-32 optimizer emits unnecessary LEA instructions that clobber a register, forcing it to be preserved at additional expense.


Test Code:
----------

        #include <immintrin.h>

        __m256 foo(const float *x)
        {
           __m256 ymmX = _mm256_load_ps(&x[0]);
           return _mm256_addsub_ps(ymmX, ymmX);
        }


Compile with: "-m32 -mtune=generic -mavx -O2"

This is also reproduced at -O1 and -O3, and when tuning for any architecture that supports AVX (not specific to the "generic" target).

It also does not matter whether the code is compiled in C or C++ mode.

This behavior is exhibited by *all* versions of GCC that support AVX targeting, from at least 4.9.0 through the 8.0.0 (20170701).

The code compiles warning-free, of course.

See it live on Godbolt: https://godbolt.org/g/NDDgsA


Actual Disassembly:
-------------------

foo:                                    # -O2 or -O3
        pushl      %ecx
        movl       8(%esp), %eax
        leal       8(%esp), %ecx
        vmovaps    (%eax), %ymm0
        popl       %ecx
        vaddsubps  %ymm0, %ymm0, %ymm0
        ret

The LEA instruction performs a redundant load of the parameter from the stack into ECX, and then promptly discards that value. The load of ECX also has spill-over effects, requiring that additional code be emitted to preserve the original value of this register (PUSH+POP).

The same bug is observed at -O1, but the ordering of the instructions is slightly different and the load of ECX is actually used to load EAX, further lengthening the dependency chain for no benefit whatsoever.

foo:                                    # -O1
        pushl      %ecx
        leal       8(%esp), %ecx
        movl       (%ecx), %eax
        vmovaps    (%eax), %ymm0
        vaddsubps  %ymm0, %ymm0, %ymm0
        popl       %ecx
        ret


Expected Disassembly:
---------------------

foo:
        movl       8(%esp), %eax
        vmovaps    (%eax), %ymm0
        vaddsubps  %ymm0, %ymm0, %ymm0
        ret


Or better yet:

foo:
        vmovaps    8(%esp), %ymm0
        vaddsubps  %ymm0, %ymm0, %ymm0
        ret


The correct code shown above is already generated for x86-64 builds (-m64), so this optimization deficiency affects only x86-32 builds (-m32).


---


### compiler : `gcc`
### title : `missing strlen optimization with intervening memcpy`
### open_at : `2017-07-05T21:50:16Z`
### last_modified_date : `2020-11-12T22:40:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81330
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
GCC eliminates redundant calls to strlen() with intervening calls to strcpy but it misses an opportunity to do the same when the intervening call is to memcpy instead.  In the test case below, since both strcpy and memcpy require that the copies do not overlap, it's safe to assume that neither call modifies any part of the source string, including the terminating nul, and so the second strlen call can be replaced with the result of the first in both functions, not just in f().  The strlen dump shows that GCC doesn't take advantage of this guarantee in the memcpy case.

(The second strlen call in g is replaced with the result of the first in g when memcpy is passed n0 + 1 as the size but I don't see why that should make a difference.)

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-strlen=/dev/stdout a.c
void f (char* d, const char* s)
{
  __SIZE_TYPE__ n0 = __builtin_strlen (s);

  __builtin_strcpy (d, s);

  __SIZE_TYPE__ n1 = __builtin_strlen (s);

  if (n0 != n1)
    __builtin_abort ();   // optimized
}

void g (char* d, const char* s)
{
  __SIZE_TYPE__ n0 = __builtin_strlen (s);

  __builtin_memcpy (d, s, n0);

  __SIZE_TYPE__ n1 = __builtin_strlen (s);

  if (n0 != n1)
    __builtin_abort ();   // not optimized
}


;; Function f (f, funcdef_no=0, decl_uid=1816, cgraph_uid=0, symbol_order=0)

f (char * d, const char * s)
{
  long unsigned int n1;
  long unsigned int n0;
  long unsigned int _8;

  <bb 2> [100.00%] [count: INV]:
  n0_3 = __builtin_strlen (s_2(D));
  _8 = n0_3 + 1;
  __builtin_memcpy (d_4(D), s_2(D), _8);
  n1_6 = n0_3;
  if (n0_3 != n1_6)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1822, cgraph_uid=1, symbol_order=1)

g (char * d, const char * s)
{
  long unsigned int n1;
  long unsigned int n0;

  <bb 2> [100.00%] [count: INV]:
  n0_3 = __builtin_strlen (s_2(D));
  __builtin_memcpy (d_4(D), s_2(D), n0_3);
  n1_6 = __builtin_strlen (s_2(D));
  if (n0_3 != n1_6)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  return;

}


---


### compiler : `gcc`
### title : `missing strlen optimization with intervening strcat of unknown strings`
### open_at : `2017-07-06T17:14:40Z`
### last_modified_date : `2021-09-12T08:46:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81343
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
GCC is able to eliminate the second of a pair of strlen(src) with an unknown string src even with an intervening call to strcpy(dst, src) with an unknown dst.  This is possible because the strcpy call can be assumed not to modify the source string.  However, GCC doesn't perform the same simplification when the intervening call is one to strcat(dst, src), even though that call too can be assumed not to modify str.  The test case below shows the difference.

From stepping through the code it looks to me like this is caused by the handle_builtin_strcat() function in tree-ssa-strlen.c returning early in this case, without retrieving the strinfo for the source string and setting the dont_invalidate flag on it.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout t.c
typedef __SIZE_TYPE__ size_t;

void f (char *d, const char *s)
{
   size_t n0 = __builtin_strlen (s);

   __builtin_strcpy (d, s);

   size_t n1 = __builtin_strlen (s);   // call eliminated

   if (n0 != n1)
     __builtin_abort ();
}

void g (char *d, const char *s)
{
   size_t n0 = __builtin_strlen (s);

   __builtin_strcat (d, s);

   size_t n1 = __builtin_strlen (s);   // call not eliminated

   if (n0 != n1)
     __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1817, cgraph_uid=0, symbol_order=0)

f (char * d, const char * s)
{
  size_t n0;
  long unsigned int _8;

  <bb 2> [100.00%] [count: INV]:
  n0_3 = __builtin_strlen (s_2(D));
  _8 = n0_3 + 1;
  __builtin_memcpy (d_4(D), s_2(D), _8); [tail call]
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1823, cgraph_uid=1, symbol_order=1)

g (char * d, const char * s)
{
  size_t n1;
  size_t n0;

  <bb 2> [100.00%] [count: INV]:
  n0_3 = __builtin_strlen (s_2(D));
  __builtin_strcat (d_4(D), s_2(D));
  n1_6 = __builtin_strlen (s_2(D));
  if (n0_3 != n1_6)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  return;

}


---


### compiler : `gcc`
### title : `Extra mov for zero extend of add`
### open_at : `2017-07-08T05:29:40Z`
### last_modified_date : `2022-10-16T17:51:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81357
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Take:
unsigned long long d;
unsigned int test1(unsigned int fParm)
{
  d = fParm + 1;
  return fParm + 1;
}

--- CUT ---
Currently this produces:
test1:
        add     w2, w0, 1
        adrp    x1, d
        str     x2, [x1, #:lo12:d]
        mov     x0, x2
        ret

But w0 is dead after the add, so why not use w0 instead of x2.  This should allow the removal of the mov at the end of the function.

This looks like it only shows up with function returns.


---


### compiler : `gcc`
### title : `pragma omp simd reduce(max:m) not vectorizing`
### open_at : `2017-07-09T17:41:57Z`
### last_modified_date : `2021-05-04T12:21:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81366
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Compiling this code:

###########################################################
double max(double* x, int n) {
  double m = 0;
  int i;
#pragma omp simd linear (i) reduction(max:m)
  for (i=0; i<n; ++i) 
    m = std::max(x[i], m);
  return m;
}
###########################################################

with g++ -fopenmp-simd -march=haswell -O3 -S main.cpp

g++ -v
Using built-in specs.
COLLECT_GCC=/usr/local/bin/g++
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-apple-darwin15.0.0/8.0.0/lto-wrapper
Target: x86_64-apple-darwin15.0.0
Configured with: ../gcc/configure --disable-multilib --enable-languages=c++ --with-gmp=/opt/local --with-libiconv-prefix=/opt/local
Thread model: posix
gcc version 8.0.0 20170610 (experimental) (GCC)

Produces the below unvectorized assembly. Similar code with "+" will work.

############################################################
LFE1116:
	.align 4,0x90
	.globl __Z3maxPdi
__Z3maxPdi:
LFB1117:
	leaq	8(%rsp), %r10
LCFI6:
	andq	$-32, %rsp
	pushq	-8(%r10)
	pushq	%rbp
LCFI7:
	movq	%rsp, %rbp
	pushq	%r10
LCFI8:
	vmovsd	lC1(%rip), %xmm1
	vmovsd	%xmm1, -48(%rbp)
	testl	%esi, %esi
	jle	L13
	leal	-1(%rsi), %eax
	leaq	8(%rdi,%rax,8), %rax
	.align 4,0x90
L14:
	vmovsd	(%rdi), %xmm0
	vucomisd	%xmm0, %xmm1
	jbe	L20
	addq	$8, %rdi
	cmpq	%rax, %rdi
	jne	L14
L13:
	vmovsd	-48(%rbp), %xmm2
	vmaxsd	lC0(%rip), %xmm2, %xmm0
	popq	%r10
LCFI9:
	popq	%rbp
	leaq	-8(%r10), %rsp
LCFI10:
	ret
	.align 4,0x90
L20:
LCFI11:
	addq	$8, %rdi
	vmovsd	%xmm0, -48(%rbp)
	cmpq	%rax, %rdi
	je	L13
	vmovapd	%xmm0, %xmm1
	jmp	L14
############################################################


---


### compiler : `gcc`
### title : `unnecessary cast before comparison`
### open_at : `2017-07-10T11:15:19Z`
### last_modified_date : `2021-08-07T19:57:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81376
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
Take the following code:

typedef double c_t;
typedef int a_t;
int f(a_t a1, a_t a2) {
  return (c_t) a1 < (c_t) a2;
}

With IEEE 754 double we have a 52 bits mantissa which is wide enough to represent all of the 'int' values exactly.  No possibility for imprecision and especially not for Inf or NaN.

Still gcc (trunk as of today but likely older versions as well) generate the code for the conversion.  This is for x86-64 with -O2:

f:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	xorl	%eax, %eax
	vcvtsi2sd	%edi, %xmm0, %xmm0
	vcvtsi2sd	%esi, %xmm1, %xmm1
	vucomisd	%xmm0, %xmm1
	seta	%al
	ret

A simple

        xorl	%eax, %eax
	cmpl	%esi, %edi
	setl	%al
	ret

is sufficient, just as if c_t above would be defined as 'int'.


---


### compiler : `gcc`
### title : `Inefficient loops generated from range-v3 code`
### open_at : `2017-07-12T09:21:54Z`
### last_modified_date : `2021-12-12T12:18:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81409
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Created attachment 41728
Preprocessed file for run_range()

The range-v3 (https://github.com/ericniebler/range-v3) function
  long run_range(std::vector<int> const &lengths, long to_find)
  {
    auto const found_index = ranges::distance(lengths
            | ranges::view::transform(ranges::convert_to<long>{})
            | ranges::view::partial_sum()
            | ranges::view::take_while([=](auto const i) {
                  return !(to_find < i);
              }));
    return found_index;
  }
is generated as slow code with GCC, needing 3x the time to run compared to the code generated by LLVM (when compiled with "-O3 -std=c++14 -DNDEBUG"). The calculation done in run_range() is the equivalent of
  long run_forloop(std::vector<int> const &vec, long to_find)
  {
    long len = vec.end() - vec.begin();
    const int *p = &vec[0];
    long i, acc = 0;
    for (i = 0; i < len; i++) {
      acc += p[i];
      if (to_find < acc)
          break;
    }
    return i;
  }
and LLVM manages to generate similar code for both functions, while GCC seems to be confused by the run_range() loop and generates extra comparisions and a somewhat messy code flow...


---


### compiler : `gcc`
### title : `missing strlen optimization for strncpy`
### open_at : `2017-07-13T19:08:55Z`
### last_modified_date : `2020-01-14T15:37:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81433
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
GCC can track the length of some strings created dynamically by calling strcpy (among other functions) but it's missing the same optimization for the related strncpy function.

The test case below shows two equivalent functions, where the first is optimized while the second is not.  The tree-sssa-strlen pass has no support for strncpy.  Since strncpy is often used as a replacement for strcpy, adding such support where possible (i.e., in cases where the length of the source string is known to be less than the specified size) is a potentially significant optimization opportunity.

$ cat a.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout a.c
void f (void)
{
  char a[7];
  __builtin_strcpy (a, "123");
  if (__builtin_strlen (a) != 3)
    __builtin_abort ();
}

void g (void)
{
  char a[7];
  __builtin_strncpy (a, "123", sizeof a);
  if (__builtin_strlen (a) != 3)
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1814, cgraph_uid=0, symbol_order=0)

f ()
{
  <bb 2> [100.00%] [count: INV]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1818, cgraph_uid=1, symbol_order=1)

g ()
{
  char a[7];
  long unsigned int _1;

  <bb 2> [100.00%] [count: INV]:
  __builtin_strncpy (&a, "123", 7);
  _1 = __builtin_strlen (&a);
  if (_1 != 3)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `missing strlen optimization for strcat past the beginning of clear array`
### open_at : `2017-07-13T21:52:11Z`
### last_modified_date : `2021-09-12T08:40:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81435
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
The tree-ssa-strlen pass is able to track the lengths of some dynamically created and modified strings by calls to strcpy and strcat.  But in at least one case where the strcat optimization could achieve parity with strcpy it does not: when strcat is called to copy a string of known length pasty the beginning of a zeroed-out buffer as is done in function g1() below the same optimization done for strcpy is not done, and the pass loses track of the length of the appended (copied) string.

$ cat a.c && gcc -O2 -S -Wall -Wextra  -fdump-tree-strlen=/dev/stdout a.c
void f0 (void)
{
  char a[4] = "234";
  char b[5] = "1";

  __builtin_strcpy (b + 1, a);

  if (__builtin_strlen (b + 1) != 3)   // optimized into 3
    __builtin_abort ();
}

void f1 (void)
{
  char a[4] = "234";
  char b[5] = "";

  __builtin_strcpy (b + 1, a);

  if (__builtin_strlen (b + 1) != 3)   // optimized into 3
    __builtin_abort ();
}

void g0 (void)
{
  char a[4] = "234";
  char b[5] = "1";

  __builtin_strcat (b + 1, a);

  if (__builtin_strlen (b + 1) != 3)   // optimized into 3
    __builtin_abort ();
}

void g1 (void)
{
  char a[4] = "234";
  char b[5] = "";

  __builtin_strcat (b + 1, a);

  if (__builtin_strlen (b + 1) != 3)   // not optimized
    __builtin_abort ();
}


;; Function f0 (f0, funcdef_no=0, decl_uid=1815, cgraph_uid=0, symbol_order=0)

f0 ()
{
  char b[5];
  char a[4];
  long unsigned int _1;

  <bb 2> [100.00%] [count: INV]:
  a = "234";
  b = "1";
  __builtin_memcpy (&MEM[(void *)&b + 1B], &a, 4);
  _1 = 3;
  if (_1 != 3)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=1820, cgraph_uid=1, symbol_order=1)

f1 ()
{
  char b[5];
  char a[4];
  long unsigned int _1;

  <bb 2> [100.00%] [count: INV]:
  a = "234";
  b = "";
  __builtin_memcpy (&MEM[(void *)&b + 1B], &a, 4);
  _1 = 3;
  if (_1 != 3)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return;

}



;; Function g0 (g0, funcdef_no=2, decl_uid=1825, cgraph_uid=2, symbol_order=2)

g0 ()
{
  char b[5];
  char a[4];
  long unsigned int _1;

  <bb 2> [100.00%] [count: INV]:
  a = "234";
  b = "1";
  __builtin_memcpy (&MEM[(void *)&b + 1B], &a, 4);
  _1 = 3;
  if (_1 != 3)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return;

}



;; Function g1 (g1, funcdef_no=3, decl_uid=1830, cgraph_uid=3, symbol_order=3)

g1 ()
{
  char b[5];
  char a[4];
  long unsigned int _1;

  <bb 2> [100.00%] [count: INV]:
  a = "234";
  b = "";
  __builtin_strcat (&MEM[(void *)&b + 1B], &a);
  _1 = __builtin_strlen (&MEM[(void *)&b + 1B]);
  if (_1 != 3)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `expmed.c:init_expmed_one_mode uses wrong mode for widening cost computations`
### open_at : `2017-07-14T12:21:48Z`
### last_modified_date : `2021-09-15T08:21:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81444
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.1.1`
### severity : `normal`
### contents :
expmed.c:init_expmed_one_mode uses a wrong mode for widening cost computations.  For example, on avr this function will compute the costs for set_mul_highpart_cost as follows:

Almost correct (TImode is strange, this is due to init_expmed_one_conv clobbering modes):

init_expmed_one_mode[:pass=?]: 0, (truncate:QI (lshiftrt:HI (mult:HI (zero_extend:HI (reg:TI 42))
            (zero_extend:HI (reg:TI 42)))
        (const_int 8 [0x8]))) = 20

Wrong: mult_highpart will use SImode (2*HImode).  This target is special as it provides a 24-bit mode (PSI) between HI and SI.  The cost of "-88" will be used in the remainder where (truncate:HI (lshiftrt:SI (mult:SI... is the actual RTX.

init_expmed_one_mode[:pass=?]: 0, (truncate:HI (lshiftrt:PSI (mult:PSI (zero_extend:PSI (reg:TI 42))
            (zero_extend:PSI (reg:TI 42)))
        (const_int 16 [0x10]))) = -88

Following is not used.  It would be unusable for sensible widening operation because SI <  2 * PSI.

init_expmed_one_mode[:pass=?]: 0, (truncate:PSI (lshiftrt:SI (mult:SI (zero_extend:SI (reg:TI 42))
            (zero_extend:SI (reg:TI 42)))
        (const_int 24 [0x18]))) = 32

Correct again but still strange TImode:

init_expmed_one_mode[:pass=?]: 0, (truncate:SI (lshiftrt:DI (mult:DI (zero_extend:DI (reg:TI 42))
            (zero_extend:DI (reg:TI 42)))
        (const_int 32 [0x20]))) = 64

...

Instead, what expmed computes costs for should read:

init_expmed_one_mode[:pass=?]: 0, (truncate:QI (lshiftrt:HI (mult:HI (zero_extend:HI (reg:QI 42))
            (zero_extend:HI (reg:QI 42)))
        (const_int 8 [0x8]))) = 20

init_expmed_one_mode[:pass=?]: 0, (truncate:HI (lshiftrt:SI (mult:SI (zero_extend:SI (reg:HI 42))
            (zero_extend:SI (reg:HI 42)))
        (const_int 16 [0x10]))) = 32

init_expmed_one_mode[:pass=?]: 0, (truncate:SI (lshiftrt:DI (mult:DI (zero_extend:DI (reg:SI 42))
            (zero_extend:DI (reg:SI 42)))
        (const_int 32 [0x20]))) = 64

etc.


---


### compiler : `gcc`
### title : `Dynamic stack allocation not optimized into static allocation`
### open_at : `2017-07-14T13:41:18Z`
### last_modified_date : `2022-03-15T10:16:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81445
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Many compilers optimize small dynamic allocations into static allocation. This is more efficient as it allows multiple small dynamic allocations to be merged into a single static allocation. In some cases it may remove all dynamic allocations, avoiding the need for a frame pointer (which frees up an extra register on various targets in addition to reducing prolog/epilog overheads).

Obviously allocations cannot be in a loop (including tail-recursion), and the total size should be limited to avoid increasing stack size by too much.

void f(void*);
void alloca (int x)
{
  if (x < 100)
    f (__builtin_alloca (x));
  f (__builtin_alloca (16));
}


---


### compiler : `gcc`
### title : `missing strcmp optimization and warning on duplicate call with an unknown string`
### open_at : `2017-07-15T23:55:21Z`
### last_modified_date : `2021-03-26T00:09:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81454
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The second call to strcmp() in the following function is redundant.: either the first call returns zero and the second call isn't reached, or it returns a non-zero value in which case so must the second.  The second call can be safely eliminated.  This is a missed optimization opportunity.

This type of duplicate comparison is often a logic error.  In addition to optimizing the redundant call, GCC should issue a warning pointing the redundancy out.

$ cat x.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout x.c

const char foo[] = "123";
const char bar[] = "123";

int f (const char *s)
{
  if (!__builtin_strcmp (s, foo))
    return 0;
  if (!__builtin_strcmp (s, bar))   // redundant, can be eliminated
    return 1;

  return -1;
}

;; Function f (f, funcdef_no=0, decl_uid=1817, cgraph_uid=0, symbol_order=2)

Removing basic block 4
Removing basic block 6
f (const char * s)
{
  int _1;
  int _2;
  int _3;

  <bb 2> [100.00%] [count: INV]:
  _1 = __builtin_strcmp (s_5(D), &foo);
  if (_1 == 0)
    goto <bb 5>; [34.00%] [count: INV]
  else
    goto <bb 3>; [66.00%] [count: INV]

  <bb 3> [66.00%] [count: INV]:
  _2 = __builtin_strcmp (s_5(D), &bar);
  if (_2 == 0)
    goto <bb 4>; [96.19%] [count: INV]
  else
    goto <bb 5>; [3.81%] [count: INV]

  <bb 4> [63.49%] [count: INV]:

  <bb 5> [100.00%] [count: INV]:
  # _3 = PHI <_1(2), 1(4), -1(3)>
  return _3;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] x86-64 optimizer makes wrong decision when optimizing for size`
### open_at : `2017-07-16T10:40:58Z`
### last_modified_date : `2023-07-07T10:32:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81456
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Consider the following code (doesn't matter if you compile it as C or C++):

#include <stdlib.h>
int Bounce(int a, int b)
{
    int mod = abs(a % b);
    return (a/b % 2) ? b-mod : mod;
}

When optimizing for speed (whether -O1, -O2, or -O3), this is compiled to the following x86-64 machine code:

Bounce(int, int):
        movl    %edi, %eax

        cltd
        idivl   %esi
        movl    %edx, %ecx
        sarl    $31, %ecx
        xorl    %ecx, %edx
        subl    %ecx, %edx

        subl    %edx, %esi
        testb   $1, %al
        cmovne  %esi, %edx
        movl    %edx, %eax
        ret

That output is observed as far back as (at least) GCC 4.8, and all the way up to the current 8.0 preview I have (8.0.0 20170716).

However, when optimizing for size (-Os), the same function produces this output:

Bounce:
        movl    %edi, %eax

        cltd
        idivl   %esi
        movl    %edx, %ecx
        sarl    $31, %ecx
        xorl    %ecx, %edx
        subl    %ecx, %edx

        testb   $1, %al
        je      .L1
        subl    %edx, %esi
        movl    %esi, %edx
.L1:
        movl    %edx, %eax
        ret

This defies expectations because it is actually *larger* (more bytes) than the version optimized for speed. The JE instruction in this version is 2 bytes, as is each MOVL instruction, making that section 6 bytes total. However, in the version optimized for speed, the CMOVNE instruction is 3 bytes, plus a 2-byte MOVL, for 5 bytes total. (The SUBL instruction there is required either way.)

Now, one byte obviously isn't a big deal in terms of total size, except that the CMOV version is more performant, so even if these two versions were exactly the same size, it should be used in preference to the branching version! (The optimizer has no reason to suspect that the quotient in the division will be predictably odd or even, so a non-branching conditional move is most appropriate to get the best worst-case performance.)

I notice that this is a regression post-GCC 6.3. In other words, GCC 6.3 generates the same code for -Os and -O1/-O2/-O3. I don't have GCC 7.0 available, so GCC 7.1 is the first version I have available that reproduces the described behavior. It continues to be there, as I said, in GCC 8.

This also is *not* observed when targeting 32-bit x86. You get conditional moves when the target architecture supports them (P6 and later). So this affects only x86-64, where conditional moves are *always* available.


---


### compiler : `gcc`
### title : `Optimization for removing same variable comparisons in loop: while(it != end1 && it != end2)`
### open_at : `2017-07-17T07:43:58Z`
### last_modified_date : `2021-12-18T18:35:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81461
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.0`
### severity : `enhancement`
### contents :
Simple iteration by std::deque elements produces suboptimal code. For example 

#include <deque>

unsigned sum(std::deque<unsigned> cont) {
    unsigned sum = 0;
    for (unsigned v : cont)
        sum += v;

    return sum;
}


produces the following loop:


.L2:
        cmp     rcx, rdx
        je      .L1
        add     eax, DWORD PTR [rdx]
        add     rdx, 4
        cmp     rdx, rsi
        jne     .L2
        mov     rdx, QWORD PTR [r8+8]
        add     r8, 8
        lea     rsi, [rdx+512]
        jmp     .L2

The loop has two comparisons in it and behaves as the following C code:

unsigned sum_like0(unsigned** chunks, unsigned* end) {
    unsigned sum = 0;
    
    for (unsigned* it = *chunks; it != end; it = *(++chunks)) {
        for (;it != end && it != *chunks + 128; ++it) {
            sum += *it;
        }
    }

    return sum;
}


Note the `it != end && it != *chunks + 128` condition. It could be simplified: if `end` belongs to `[it, *chunks + 128]` change the condition to `it != end` and use the condition `it != *chunks + 128` otherwise. Such optimization removes the cmp from the loop and produces a much more faster loop:

.L15:
        add     eax, DWORD PTR [rdx]
        add     rdx, 4
        cmp     rdx, rcx
        jne     .L15

Synthetic tests show up to 2 times better performance. Assembly outputs: https://godbolt.org/g/L7Mr4M


---


### compiler : `gcc`
### title : `AVX load from adjacent memory location followed by concatenation`
### open_at : `2017-07-20T17:00:42Z`
### last_modified_date : `2021-08-19T06:24:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81496
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
With -O2 -mavx{,2,512f}, we get on the following testcase:

typedef __int128 V __attribute__((vector_size (32)));
typedef long long W __attribute__((vector_size (32)));
typedef int X __attribute__((vector_size (16)));
typedef __int128 Y __attribute__((vector_size (64)));
typedef long long Z __attribute__((vector_size (64)));

W f1 (__int128 x, __int128 y) { return (W) ((V) { x, y }); }
W f2 (__int128 x, __int128 y) { return (W) ((V) { y, x }); }

        movq    %rdi, -16(%rsp)
        movq    %rsi, -8(%rsp)
        movq    %rdx, -32(%rsp)
        movq    %rcx, -24(%rsp)
        vmovdqa -32(%rsp), %xmm0
        vmovdqa -16(%rsp), %xmm1
        vinserti128     $0x1, %xmm0, %ymm1, %ymm0
for f1, which I'm afraid is hard to do anything about, because RA didn't see the usefulness to spill in different order, but for f2:
        movq    %rdx, -32(%rsp)
        movq    %rcx, -24(%rsp)
        vmovdqa -32(%rsp), %xmm0
        movq    %rdi, -16(%rsp)
        movq    %rsi, -8(%rsp)
        vinserti128     $0x1, -16(%rsp), %ymm0, %ymm0
Before scheduling, the movdqa is next to vinserti128 from the adjacent mem; in that case it might be a win to use a vmovdqa -32(%rsp), %ymm0 instead.
Though, the MEM has just A128 in the rtl dump, so maybe we need to use vmovdqu instead, unless we can prove it is 256-bit aligned (it is in this case, but not generally).


---


### compiler : `gcc`
### title : `mulitple calls to __tls_get_addr() with -fPIC`
### open_at : `2017-07-20T21:10:26Z`
### last_modified_date : `2022-09-23T13:18:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81501
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.1.0`
### severity : `enhancement`
### contents :
I only tested this on amd64, but see for yourself:

+ cat t.cc
struct foo {
    foo();
    ~foo();
};

foo *test() {
    static thread_local foo foo_tls;
    return &foo_tls;
}
+ g++-7 -std=c++14 -v -pthread -fPIC -shared -O2 -o gcc.so t.cc
Using built-in specs.
COLLECT_GCC=/usr/bin/g++-7
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Debian 7.1.0-9' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 7.1.0 (Debian 7.1.0-9) 
COLLECT_GCC_OPTIONS='-std=c++14' '-v' '-pthread' '-fPIC' '-shared' '-O2' '-o' 'gcc.so' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-linux-gnu/7/cc1plus -quiet -v -imultiarch x86_64-linux-gnu -D_GNU_SOURCE -D_REENTRANT t.cc -quiet -dumpbase t.cc -mtune=generic -march=x86-64 -auxbase t -O2 -std=c++14 -version -fPIC -o /tmp/ccdUrCDS.s
GNU C++14 (Debian 7.1.0-9) version 7.1.0 (x86_64-linux-gnu)
	compiled by GNU C version 7.1.0, GMP version 6.1.2, MPFR version 3.1.5, MPC version 1.0.3, isl version isl-0.18-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
ignoring duplicate directory "/usr/include/x86_64-linux-gnu/c++/7"
ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/7/../../../../x86_64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/include/c++/7
 /usr/include/x86_64-linux-gnu/c++/7
 /usr/include/c++/7/backward
 /usr/lib/gcc/x86_64-linux-gnu/7/include
 /usr/local/include
 /usr/lib/gcc/x86_64-linux-gnu/7/include-fixed
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
GNU C++14 (Debian 7.1.0-9) version 7.1.0 (x86_64-linux-gnu)
	compiled by GNU C version 7.1.0, GMP version 6.1.2, MPFR version 3.1.5, MPC version 1.0.3, isl version isl-0.18-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 3681302eda59faba4e53a905eca4bf72
COLLECT_GCC_OPTIONS='-std=c++14' '-v' '-pthread' '-fPIC' '-shared' '-O2' '-o' 'gcc.so' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 as -v --64 -o /tmp/ccI2B3TO.o /tmp/ccdUrCDS.s
GNU assembler version 2.28 (x86_64-linux-gnu) using BFD version (GNU Binutils for Debian) 2.28
COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/7/:/usr/lib/gcc/x86_64-linux-gnu/7/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/7/:/usr/lib/gcc/x86_64-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/7/:/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/7/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-std=c++14' '-v' '-pthread' '-fPIC' '-shared' '-O2' '-o' 'gcc.so' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-linux-gnu/7/collect2 -plugin /usr/lib/gcc/x86_64-linux-gnu/7/liblto_plugin.so -plugin-opt=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper -plugin-opt=-fresolution=/tmp/cc9S0zbL.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lpthread -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s --sysroot=/ --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu -shared -o gcc.so /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. /tmp/ccI2B3TO.o -lstdc++ -lm -lgcc_s -lpthread -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o
COLLECT_GCC_OPTIONS='-std=c++14' '-v' '-pthread' '-fPIC' '-shared' '-O2' '-o' 'gcc.so' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
+ gdb -q -ex disassemble test -ex quit gcc.so
Reading symbols from gcc.so...(no debugging symbols found)...done.
Dump of assembler code for function _Z4testv:
   0x00000000000007f0 <+0>:	push   %rbx
   0x00000000000007f1 <+1>:	sub    $0x10,%rsp
   0x00000000000007f5 <+5>:	lea    0x2007cc(%rip),%rdi        # 0x200fc8
   0x00000000000007fc <+12>:	callq  0x6e0 <__tls_get_addr@plt>
   0x0000000000000801 <+17>:	cmpb   $0x0,0x0(%rax)
   0x0000000000000808 <+24>:	jne    0x840 <_Z4testv+80>
   0x000000000000080a <+26>:	lea    0x8(%rax),%rbx
   0x0000000000000811 <+33>:	mov    %rax,0x8(%rsp)
   0x0000000000000816 <+38>:	mov    %rbx,%rdi
   0x0000000000000819 <+41>:	callq  0x6d0 <_ZN3fooC1Ev@plt>
   0x000000000000081e <+46>:	mov    0x8(%rsp),%rax
   0x0000000000000823 <+51>:	mov    0x2007b6(%rip),%rdi        # 0x200fe0
   0x000000000000082a <+58>:	lea    0x2007ff(%rip),%rdx        # 0x201030
   0x0000000000000831 <+65>:	mov    %rbx,%rsi
   0x0000000000000834 <+68>:	movb   $0x1,0x0(%rax)
   0x000000000000083b <+75>:	callq  0x6f0 <__cxa_thread_atexit@plt>
   0x0000000000000840 <+80>:	lea    0x200781(%rip),%rdi        # 0x200fc8
   0x0000000000000847 <+87>:	callq  0x6e0 <__tls_get_addr@plt>
   0x000000000000084c <+92>:	add    $0x10,%rsp
   0x0000000000000850 <+96>:	add    $0x8,%rax
   0x0000000000000856 <+102>:	pop    %rbx
   0x0000000000000857 <+103>:	retq   
End of assembler dump.


As you can see after the first call to __tls_get_addr() jne jumps to a second call to __tls_get_addr(). It should really only need to get the address once here, like clang does:

+ clang++ -std=c++14 -pthread -fPIC -shared -O2 -o clang.so t.cc
+ gdb -q -ex disassemble test -ex quit clang.so
Reading symbols from clang.so...(no debugging symbols found)...done.
Dump of assembler code for function _Z4testv:
   0x00000000000007a0 <+0>:	push   %r14
   0x00000000000007a2 <+2>:	push   %rbx
   0x00000000000007a3 <+3>:	push   %rax
   0x00000000000007a4 <+4>:	lea    0x20081d(%rip),%rdi        # 0x200fc8
   0x00000000000007ab <+11>:	callq  0x690 <__tls_get_addr@plt>
   0x00000000000007b0 <+16>:	mov    %rax,%rbx
   0x00000000000007b3 <+19>:	mov    0x1(%rax),%al
   0x00000000000007b9 <+25>:	and    $0x1,%al
   0x00000000000007bb <+27>:	jne    0x7ef <_Z4testv+79>
   0x00000000000007bd <+29>:	mov    %rbx,%rax
   0x00000000000007c0 <+32>:	lea    0x0(%rax),%r14
   0x00000000000007c7 <+39>:	mov    %r14,%rdi
   0x00000000000007ca <+42>:	callq  0x680 <_ZN3fooC1Ev@plt>
   0x00000000000007cf <+47>:	mov    0x20080a(%rip),%rdi        # 0x200fe0
   0x00000000000007d6 <+54>:	lea    0x200853(%rip),%rdx        # 0x201030
   0x00000000000007dd <+61>:	mov    %r14,%rsi
   0x00000000000007e0 <+64>:	callq  0x6a0 <__cxa_thread_atexit@plt>
   0x00000000000007e5 <+69>:	mov    %rbx,%rax
   0x00000000000007e8 <+72>:	movb   $0x1,0x1(%rax)
   0x00000000000007ef <+79>:	mov    %rbx,%rax
   0x00000000000007f2 <+82>:	lea    0x0(%rax),%rax
   0x00000000000007f9 <+89>:	add    $0x8,%rsp
   0x00000000000007fd <+93>:	pop    %rbx
   0x00000000000007fe <+94>:	pop    %r14
   0x0000000000000800 <+96>:	retq   
End of assembler dump.

This has some performance overhead which I'd like to avoid.


---


### compiler : `gcc`
### title : `g++ pops up a constructor for objects that could be initialized at load-time`
### open_at : `2017-07-24T13:41:35Z`
### last_modified_date : `2021-08-02T04:23:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81533
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 41817
foo.cpp: C++ test case

When compiling the following module with g++ from v8 trunk

$ g++ foo.cpp -S -O2 -save-temps

then it pops a constructor to initialize s:

struct S { int id; const char *labl; };

const S* get_S_B()
{
  static const S s =
    { 456,
      (__extension__({
          static char ccc[] = "TextB";
          &ccc[0];
	})) };
  return &s;
}


The generated assembly reads (x86_64):

_Z7get_S_Bv:
.LFB0:
	.cfi_startproc
	movzbl	_ZGVZ7get_S_BvE1s(%rip), %eax
	testb	%al, %al
	je	.L13
	movl	$_ZZ7get_S_BvE1s, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L13:
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	$_ZGVZ7get_S_BvE1s, %edi
	call	__cxa_guard_acquire
	testl	%eax, %eax
	jne	.L14
	movl	$_ZZ7get_S_BvE1s, %eax
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret


The code could be just as smart as with the following test case

struct S { int id; const char *labl; };

const S* get_S_A()
{
  static const S s = { 123, "TextA" };
  return &s;
}

which generates:

_Z7get_S_Av:
	movl	$_ZZ7get_S_AvE1s, %eax
	ret

	.section	.rodata.str1.1,"aMS",@progbits,1
.LC0:
	.string	"TextA"

	.section	.rodata
_ZZ7get_S_AvE1s:
	.long	123
	.zero	4
	.quad	.LC0
	.ident	"GCC: (GNU) 8.0.0 20170724 (experimental)"


The first version with the "TextB" assignment to ccc is useful when ccc needs a variable attribute.


---


### compiler : `gcc`
### title : `tree-switch-conversion leads to code bloat`
### open_at : `2017-07-24T19:38:06Z`
### last_modified_date : `2021-08-29T20:47:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81540
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 41821
cswtch.c: C test case

The following code gets blown by tree-switch-conversion by a factor of 4:

typedef __UINT64_TYPE__ T;

T func64 (int i)
{
    T x = (T) -1;
    switch (i)
    {
        case 1: x = 2; break;
        case 2: x = 2; break;
        case 25: x = 0x12345678abcd; break;
        default: break;
    }
    return x;
}


$ avr-gcc cswtch.c -Os -c -fno-ident
$ avr-size -A cswtch.o
cswtch.o  :
section   size   addr
.text       62      0
.rodata    200      0
Total      262


$ avr-gcc cswtch.c -Os -c -fno-ident -fno-tree-switch-conversion
$ avr-size -A cswtch.o
cswtch.o  :
section   size   addr
.text     66      0
Total     66

Compiler sources are v8 trunk from around 2017-07-18

Target: avr
Configured with: ../../gcc.gnu.org/trunk/configure --target=avr --disable-nls --prefix=/local/gnu/install/gcc-8-avr-mingw32 --host=i686-w64-mingw32 --build=x86_64-linux-gnu --enable-languages=c,c++,lto --with-gnu-as --with-gnu-ld --disable-shared --with-dwarf2 --enable-checking=release


---


### compiler : `gcc`
### title : `PHI node should be eliminated if loop iterates enough times.`
### open_at : `2017-07-25T13:10:22Z`
### last_modified_date : `2023-06-09T17:37:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81549
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Given below test:

int a[10000], b[10000], c[10000];
int f(void)
{
  int i, n = 100;
  int t0 = a[0];
  int t1 = a[1];
     for (i = 0; i < n; i++)
       {
         a[i] = 1;
         int t2 = 2;
         t0 = t1;
         t1 = t2;
       }
     a[n] = t0;
     a[n+1] = t1;
  return 0;
}
Compile it at Ofast by default, the optimized dump is as:

  <bb 2> [1.00%] [count: INV]:
  t1_8 = a[1];
  ivtmp.9_17 = (unsigned long) &a;
  _16 = ivtmp.9_17 + 400;

  <bb 3> [99.00%] [count: INV]:
  # t1_20 = PHI <2(3), t1_8(2)>
  # ivtmp.9_2 = PHI <ivtmp.9_1(3), ivtmp.9_17(2)>
  _15 = (void *) ivtmp.9_2;
  MEM[base: _15, offset: 0B] = 1;
  ivtmp.9_1 = ivtmp.9_2 + 4;
  if (ivtmp.9_1 != _16)
    goto <bb 3>; [98.99%] [count: INV]
  else
    goto <bb 4>; [1.01%] [count: INV]

  <bb 4> [1.00%] [count: INV]:
  a[100] = t1_20;
  a[101] = 2;
  return 0;

We now eliminate one phi and leave another behind.  It is vrp1/dce2 when the phi is eliminated.


---


### compiler : `gcc`
### title : `Loop not vectorized`
### open_at : `2017-07-26T06:03:00Z`
### last_modified_date : `2023-07-21T23:22:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81558
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
For the testcase:

struct I
{
  int opix_x;
  int opix_y;
};

//#define R 
#define R __restrict__
extern struct I * R img;
extern unsigned short ** R imgY_org;
extern unsigned short orig_blocks[256];

void foo1 (int n)
{
  int x = 1, y = 1;
  unsigned short *orgptr=orig_blocks;
  // Vectorized
  for (y = 0; y < img->opix_y; y++)
    for (x = 0; x < img->opix_x; x++)
      *orgptr++ = imgY_org [y][x];
}

void foo2 (int n)
{
  int x = 1, y = 1;
  unsigned short *orgptr=orig_blocks;
  // Not vectorized
  for (y = img->opix_y; y < img->opix_y+16; y++)
    for (x = img->opix_x; x < img->opix_x+16; x++)
      *orgptr++ = imgY_org [y][x];
}

Loop in foo2 is not vectorized.

In the *.156t.vect, I see:
Creating dr for *_40
analyze_innermost: failed: evolution of base is not affine.
	base_address: 
	offset from base address: 
	constant offset from base address: 
	step: 
	aligned to: 
	base_object: *_40


LLVM seems to be able to vectorize this.


---


### compiler : `gcc`
### title : `Unnecessary zero-extension after 16 bit popcnt`
### open_at : `2017-07-28T13:04:32Z`
### last_modified_date : `2021-08-03T03:43:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81602
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `7.1.1`
### severity : `enhancement`
### contents :
GCC misses an optimization on this:

 #include <cstdint>
 #include "immintrin.h"

 void test(std::uint16_t* mask, std::uint16_t* data) {
 for (int i = 0; i < 1024; ++i) {
 *data = 0;
 unsigned tmp = *mask++;
 unsigned step = _mm_popcnt_u32(tmp);
 data += step;
 }
 }

g++ -O3 -Wall -std=c++14 -march=skylake generates:

 test(unsigned short*, unsigned short*):
 leaq 2048(%rdi), %rdx
 .L2:
 xorl %eax, %eax
 addq $2, %rdi
 movw %ax, (%rsi)
 popcntw -2(%rdi), %ax
 movzwl %ax, %eax
 leaq (%rsi,%rax,2), %rsi
 cmpq %rdx, %rdi
 jne .L2
 ret

The rax register is known to be zero at the time of `popcntw -2(%rdi), %ax`. Anyway gcc still clears the upper bits using `movzwl %ax, %eax` afterwards.

While clang uses 32 bit popcnt and `movzwl (%rdi,%rax,2), %ecx` it correctly recognises that there's no need to clear the upper bits.

clang -O3 -Wall -std=c++14 -march=skylake -fno-unroll-loops generates:

 test(unsigned short*, unsigned short*): 
 xorl %eax, %eax
 .LBB0_1: 
 movw $0, (%rsi)
 movzwl (%rdi,%rax,2), %ecx
 popcntl %ecx, %ecx
 leaq (%rsi,%rcx,2), %rsi
 addq $1, %rax
 cmpl $1024, %eax # imm = 0x400
 jne .LBB0_1
 retq

See https://godbolt.org/g/kgQ7VS


---


### compiler : `gcc`
### title : `[8 Regression] gcc un-learned loop / post-increment optimization`
### open_at : `2017-07-29T12:16:01Z`
### last_modified_date : `2020-06-04T07:32:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81611
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
C test case:

void func1 (unsigned char x, char *str)
{
    do {
        *str++ = '0' + (x & 1);
        x = x / 2;
    } while (x);
    *str = 0;
}

$ avr-gcc-8 foo.c -S -mmcu=atmega8 -O2

foo.s:

func1:
    movw r30,r22    ; ok, need the address in some
                    ;     address register (here Z=r30)
    rjmp .L2        ; ???
.L3:
    movw r30,r18     ; what the heck? Moving address back...
.L2:
    movw r18,r30;    ; ...and forth to some fresh register, just to increment
    subi r18,-1      ; the address from the *PREVIOUS* loop
    sbci r19,-1      ; slow, bloat, increases reg pressure
    mov r25,r24      ; ok
    andi r25,lo8(1)  ; ok
    subi r25,lo8(-(48)) ; ok
    st Z,r25         ; Why not just "st Z+, r25" ???
    lsr r24          ; ok
    brne .L3
    std Z+1,__zero_reg__
    ret


Just for reference the code from 4.7:

* Using 5 instructions less
* Occupying 2 registers less
* Loop consumes 4 cycles less (8 instead of 12).

func1:
    movw r30,r22
.L2:
    mov r25,r24
    andi r25,lo8(1)
    subi r25,lo8(-(48))
    st Z+,r25
    lsr r24
    brne .L2
    st Z,__zero_reg__
    ret

That's code I expect from a 3rd millenium compiler!! ;-)


---


### compiler : `gcc`
### title : `Harmful SLP vectorization`
### open_at : `2017-08-02T14:22:44Z`
### last_modified_date : `2021-07-21T03:28:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81673
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.0`
### severity : `normal`
### contents :
Revision r238364 has changes the cost of vec_construct in
ix86_builtin_vectorization_cost.  On one hand, the new code correctly
models the number of vecotr inserts, but on the other it has lead to
SLP vectorization in the following testcase which is extracted from
538.imagick_r in which it leads to run-time regressions (depending on
the HW you use) up to 6% at -O3 and -Ofast optimisation levels.

----------------------------------------------------------------------
typedef unsigned long size_t;
typedef long ssize_t;

typedef struct _RectangleInfo
{
  size_t
    width,
    height;

  ssize_t
    x,
    y;
} RectangleInfo;

void bar (RectangleInfo *region);


void
foo (void *ua, void *ub, const ssize_t x,const ssize_t y,
     const size_t columns,const size_t rows)
{
  RectangleInfo region;

  region.x=x;
  region.y=y;
  region.width=columns;
  region.height=rows;

  bar (&region);
}
----------------------------------------------------------------------

SLP2 converts this into:

  vect_cst__14 = {x_2(D), y_4(D)};
  vect_cst__12 = {columns_6(D), rows_8(D)};
  MEM[(long int *)&region + 16B] = vect_cst__14;
  MEM[(long unsigned int *)&region] = vect_cst__12;
  bar (&region);

which is then finally compiled to:

	.cfi_startproc
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	movq	%rdx, 24(%rsp)
	movq	%rcx, 8(%rsp)
	leaq	32(%rsp), %rdi
	movq	24(%rsp), %xmm0
	movq	%r9, 16(%rsp)
	movhps	8(%rsp), %xmm0
	movq	%r8, 8(%rsp)
	movaps	%xmm0, 48(%rsp)
	movq	8(%rsp), %xmm0
	movhps	16(%rsp), %xmm0
	movaps	%xmm0, 32(%rsp)
	call	bar
	addq	$72, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc

as opposed to the output of the previous revision:

	.cfi_startproc
	subq	$40, %rsp
	.cfi_def_cfa_offset 48
	movq	%rsp, %rdi
	movq	%rdx, 16(%rsp)
	movq	%rcx, 24(%rsp)
	movq	%r8, (%rsp)
	movq	%r9, 8(%rsp)
	call	bar
	addq	$40, %rsp
	.cfi_def_cfa_offset 8
	ret

The moves from GPRs into XMM registersh through stack are the thing
that can cost a lot of time (as shown by perf in the case of
538.imagick_r).


---


### compiler : `gcc`
### title : `[6 Regression] attribute(noreturn) of destructor in :? not honored`
### open_at : `2017-08-02T14:54:55Z`
### last_modified_date : `2023-03-01T15:03:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81675
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `c++`
### version : `6.2.0`
### severity : `normal`
### contents :
cat > t.cc << EOF
struct S
{
    ~S() __attribute__((noreturn));
    int a;
};
int foo()
{
  false ? 5 : S().a;
}
EOF
g++ -c -Wall t.cc


GCC 6.2.0 prints:
t.cc: In function 'int foo()':
t.cc:9:1: warning: control reaches end of non-void function [-Wreturn-type]

GCC 5.3.0 seems to detect correctly that foo() will never return.


---


### compiler : `gcc`
### title : `use attribute unused on function arguments as an optimization hint`
### open_at : `2017-08-02T22:10:13Z`
### last_modified_date : `2021-04-10T17:58:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81679
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
This is a proposal for an enhancement to attribute unused in two ways:

1) When attribute unused is specified on a function argument of pointer type in a declaration of a function, GCC could use that as an indication that the argument is, in fact, not used by the implementation of the function and assume that the object the pointer points to is unchanged by the function call.

2) In addition, the attribute could also be used to issue a -Wunused-variable warning when the variable whose address is passed to a function decorated with it is otherwise unused.

The test case below shows examples where the possible improvements could be made:

$ cat b.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout b.c
void f (void* __attribute__ ((unused)));

void g (void)
{
  int i = 1;   // issue -Wunused-variable here
  f (&i);
}

void h (void)
{
  int i = 1;
  f (&i);
  if (i != 1)   // assume this can not be true
    __builtin_abort ();
}

;; Function g (g, funcdef_no=0, decl_uid=1817, cgraph_uid=0, symbol_order=0)

g ()
{
  int i;

  <bb 2> [100.00%] [count: INV]:
  i = 1;
  f (&i);
  i ={v} {CLOBBER};
  return;

}



;; Function h (h, funcdef_no=1, decl_uid=1821, cgraph_uid=1, symbol_order=1)

h ()
{
  int i;
  int i.0_1;

  <bb 2> [100.00%] [count: INV]:
  i = 1;
  f (&i);
  i.0_1 = i;
  if (i.0_1 != 1)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  i ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `Compile-time snprintf optimization`
### open_at : `2017-08-08T16:01:01Z`
### last_modified_date : `2021-09-05T08:22:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81772
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `5.4.0`
### severity : `enhancement`
### contents :
snprintf is slow. This is caused not only by format string handling, but there is also some internal fault with string handling - see https://sourceware.org/bugzilla/show_bug.cgi?id=21905 . Because of this it would be nice if gcc could perform some compile-time optimization. The best would be to parse format string at compile time and generate appropriate code for it, so there would be no format parsing at runtime at all. However any partial optimization here would be helpful. I found that sprintf with %s format is replaced with strcpy. snprintf with %s format could be replaced with strlcpy - unfortunately is is not supported yet by gcc/glibc, so other approach is needed (or implement strlcpy/strlcat and use them :))


---


### compiler : `gcc`
### title : `missing sprintf optimization due to pointer escape analysis`
### open_at : `2017-08-09T02:26:21Z`
### last_modified_date : `2021-04-16T15:35:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81776
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
When a built-in function like memcpy or strcpy is called with a local buffer GCC knows that the call doesn't clobber other local buffers.  But the analysis seems to be missing a case to make the same determination for calls to sprintf or snprintf.  As a result, as the following case shows, when sprintf if transformed to strcpy, GCC successfully optimizes away the test and subsequent call to abort in g1().  But when the same sprintf to strcpy transformation is defeated by using the '-' flag in the "%-s" directive, GCC fails to make the same optimization in g2().

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout b.c
void f (void*);

struct S { char *a, *b; };

void g1 (struct S *p, const char *s, unsigned n)
{
  p->a = __builtin_malloc (n + 1);
  p->a[0] = 123;

  p->b = __builtin_malloc (n + 1);

  __builtin_sprintf (p->b, "%s", s);

  if (p->a[0] != 123)     // can never be true
    __builtin_abort ();   // eliminated

  __builtin_sprintf (p->b, "%s", s);

  f (p);
}

void g2 (struct S *p, const char *s, unsigned n)
{
  p->a = __builtin_malloc (n + 1);
  p->a[0] = 123;

  p->b = __builtin_malloc (n + 1);

  __builtin_sprintf (p->b, "%-s", s);

  if (p->a[0] != 123)     // can never be true
    __builtin_abort ();   // not eliminated

  __builtin_sprintf (p->a, "%-s", s);

  f (p);
}

;; Function g1 (g1, funcdef_no=0, decl_uid=1822, cgraph_uid=0, symbol_order=0)

g1 (struct S * p, const char * s, unsigned int n)
{
  unsigned int _1;
  long unsigned int _2;
  void * _3;
  void * _4;

  <bb 2> [100.00%] [count: INV]:
  _1 = n_8(D) + 1;
  _2 = (long unsigned int) _1;
  _3 = __builtin_malloc (_2);
  p_11(D)->a = _3;
  MEM[(char *)_3] = 123;
  _4 = __builtin_malloc (_2);
  p_11(D)->b = _4;
  __builtin_strcpy (_4, s_16(D));
  __builtin_strcpy (_4, s_16(D));
  f (p_11(D)); [tail call]
  return;

}



;; Function g2 (g2, funcdef_no=1, decl_uid=1827, cgraph_uid=1, symbol_order=1)

g2 (struct S * p, const char * s, unsigned int n)
{
  unsigned int _1;
  long unsigned int _2;
  void * _3;
  void * _4;
  char * _5;
  char _6;

  <bb 2> [100.00%] [count: INV]:
  _1 = n_7(D) + 1;
  _2 = (long unsigned int) _1;
  _3 = __builtin_malloc (_2);
  p_10(D)->a = _3;
  MEM[(char *)_3] = 123;
  _4 = __builtin_malloc (_2);
  p_10(D)->b = _4;
  __builtin_sprintf (_4, "%-s", s_15(D));
  _5 = p_10(D)->a;
  _6 = *_5;
  if (_6 != 123)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  __builtin_sprintf (_5, "%-s", s_15(D));
  f (p_10(D)); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `local restricted pointer can be relied on in alias analysis`
### open_at : `2017-08-09T20:26:19Z`
### last_modified_date : `2023-02-24T05:21:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81786
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
I've been looking for opportunities to improve code generation by relying on explicit guarantees about aliasing properties that commonly hold but that GCC cannot otherwise rely on due to the lack of contextual information.  For instance, it's common for a function that takes a non-const pointer argument (say Q) to modify the object it points to (say X), then call some other function (say FOO), and then read X's value again via *Q.  Unless GCC knows the semantics of FOO it must assume that every call to it changes the value of X, even though it's quite rare in practice for that to happen.  It would be nice to be able to express this guarantee to GCC to let it generate more efficient code.

As it turns out, according to the C aliasing rules, locally declaring a restrict-qualified pointer P as a copy of an unknown pointer Q (such as a function argument) can be used to express the guarantee that the object X the pointers point to isn't modified during P's lifetime.  (This is 6.7.3.1, p4.)  So with this simple notation, GCC could rely on this guarantee to optimize the program below as indicated in the comments.  Note that neither functions need to make use of the restrict qualifier in its interface.  This is helpful because relatively few functions do.

$ cat x.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout x.c
void foo (void);

void bar (int *q)
{ 
  int* restrict p = q;
  *p = 123; 

  foo ();                 // must not modify *P

  if (*p != 123)          // cannot be true
    __builtin_abort ();   // can be eliminated
}

int x;

void baz (void)
{
  x = 789;
  bar (&x);
}

;; Function bar (bar, funcdef_no=0, decl_uid=1817, cgraph_uid=0, symbol_order=0)

bar (int * q)
{
  int _1;

  <bb 2> [100.00%] [count: INV]:
  *q_2(D) = 123;
  foo ();
  _1 = *q_2(D);
  if (_1 != 123)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  return;

}



;; Function baz (baz, funcdef_no=1, decl_uid=1822, cgraph_uid=1, symbol_order=2)

baz ()
{
  int _2;

  <bb 2> [100.00%] [count: INV]:
  x = 123;
  foo ();
  _2 = x;
  if (_2 != 123)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  return;

}


---


### compiler : `gcc`
### title : `address of a local variable escapes too early`
### open_at : `2017-08-10T00:14:57Z`
### last_modified_date : `2020-08-26T18:14:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81788
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
This is similar to bug 81776.

It seems that the alias analysis considers the address of a local variable to escape if a call to an unknown function is made after the latter variable has been declared, that takes the address of another local variable even if the variable isn't used until after the call.

Clang emits optimal code for both functions below (though other compilers suffer  the same limitation).

$ cat x.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout x.c
void f (void*);

void g0 (void)
{
  int a = 123;

  f (&a);

  int b = 456;

  if (b != 456)           // can never be true
    __builtin_abort ();   // eliminated

  f (&b);
}

void g1 (void)
{
  int a = 123;
  int b = 456;

  f (&a);

  if (b != 456)           // can never be true
    __builtin_abort ();   // not eliminated

  f (&b);
}

;; Function g0 (g0, funcdef_no=0, decl_uid=1817, cgraph_uid=0, symbol_order=0)

g0 ()
{
  int b;
  int a;

  <bb 2> [100.00%] [count: INV]:
  a = 123;
  f (&a);
  b = 456;
  f (&b);
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return;

}



;; Function g1 (g1, funcdef_no=1, decl_uid=1822, cgraph_uid=1, symbol_order=1)

g1 ()
{
  int b;
  int a;
  int b.1_1;

  <bb 2> [100.00%] [count: INV]:
  a = 123;
  b = 456;
  f (&a);
  b.1_1 = b;
  if (b.1_1 != 456)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  f (&b);
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `unused strcpy to a local buffer not eliminated`
### open_at : `2017-08-10T20:29:55Z`
### last_modified_date : `2019-08-23T15:48:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81810
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
GCC successfully eliminates the call to memcpy in f() whose result is unused below but fails to eliminate the equally pointless call to strcpy() in g() or the one to strncpy() in h().  This is a missed optimization opportunity.

$ cat a.c && gcc -O2 -S -Wall -Wextra -Wpedantic a.c
void f (const void *p, unsigned n)
{
  char a[8];
  __builtin_memcpy (a, p, n);
}

void g (const char *s)
{
  char a[8];
  __builtin_strcpy (a, s);
}

void h (const char *s)
{
  char a[8];
  __builtin_strncpy (a, s, sizeof a);
}


;; Function f (f, funcdef_no=0, decl_uid=1816, cgraph_uid=0, symbol_order=0)

f (const void * p, unsigned int n)
{
  <bb 2> [100.00%] [count: INV]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1820, cgraph_uid=1, symbol_order=1)

g (const char * s)
{
  char a[8];

  <bb 2> [100.00%] [count: INV]:
  __builtin_strcpy (&a, s_2(D));
  a ={v} {CLOBBER};
  return;

}



;; Function h (h, funcdef_no=2, decl_uid=1824, cgraph_uid=2, symbol_order=2)

h (const char * s)
{
  char a[8];

  <bb 2> [100.00%] [count: INV]:
  __builtin_strncpy (&a, s_2(D), 8);
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `missing -Wreturn-local-addr returning strcpy result`
### open_at : `2017-08-10T20:34:17Z`
### last_modified_date : `2020-02-09T21:55:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81811
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
Modifying the test case from bug 81810 to have each of the test functions return the address of the local array by returning each of the built-in's return value exposes a defect in the implementation of -Wreturn-local-addr.  The warning successfully detects the bug in the case of memcpy but fails to detect the same problem involving strcpy or strncpy.

$ cat a.c && gcc -O2 -S -Wall  -fdump-tree-optimized=/dev/stdout a.c
void* f (const void *p, unsigned n)
{
  char a[8];
  return __builtin_memcpy (a, p, n);   // -Wreturn-local-addr (good)
}

char* g (const char *s)
{
  char a[8];
  return __builtin_strcpy (a, s);   // missing -Wreturn-local-addr
}

char* h (const char *s)
{
  char a[8];
  return __builtin_strncpy (a, s, sizeof a);   // missing -Wreturn-local-addr
}

a.c: In function ‘f’:
a.c:4:10: warning: function returns address of local variable [-Wreturn-local-addr]
   return __builtin_memcpy (a, p, n);   // -Wreturn-local-addr (good)
          ^~~~~~~~~~~~~~~~~~~~~~~~~~
a.c:3:8: note: declared here
   char a[8];
        ^

;; Function f (f, funcdef_no=0, decl_uid=1816, cgraph_uid=0, symbol_order=0)

f (const void * p, unsigned int n)
{
  <bb 2> [100.00%] [count: INV]:
  return 0B;

}



;; Function g (g, funcdef_no=1, decl_uid=1820, cgraph_uid=1, symbol_order=1)

g (const char * s)
{
  char a[8];
  char * _4;

  <bb 2> [100.00%] [count: INV]:
  _4 = __builtin_strcpy (&a, s_2(D));
  a ={v} {CLOBBER};
  return _4;

}



;; Function h (h, funcdef_no=2, decl_uid=1824, cgraph_uid=2, symbol_order=2)

h (const char * s)
{
  char a[8];
  char * _4;

  <bb 2> [100.00%] [count: INV]:
  _4 = __builtin_strncpy (&a, s_2(D), 8);
  a ={v} {CLOBBER};
  return _4;

}


---


### compiler : `gcc`
### title : `Inefficient stack pointer adjustment`
### open_at : `2017-08-11T02:31:32Z`
### last_modified_date : `2021-08-15T13:37:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81813
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.1.0`
### severity : `normal`
### contents :
Created attachment 41968
fs_pin.i

With GCC 7.1, the kernel's object code static analysis tool (objtool) found this unusual method of adjusting the stack pointer in the kernel:

   2cc:   48 8d 4c 24 08          lea    0x8(%rsp),%rcx
   2d1:   48 89 cc                mov    %rcx,%rsp

The value in %rcx was never used afterwards.  It would be faster and more straightforward to just add 8 to %rsp.

To recreate:

  gcc -mno-sse -mpreferred-stack-boundary=3 -O2 -c -o fs_pin.o fs_pin.i


---


### compiler : `gcc`
### title : `loop with increment conditional on IV is not optimized out`
### open_at : `2017-08-12T21:52:45Z`
### last_modified_date : `2021-12-17T06:47:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81834
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.1.1`
### severity : `enhancement`
### contents :
The following code:

int main() {
    for(int i=0; i += i < 1000, i < 1000;);
}

optimizes on x86_64 to the following code:

main:
.LFB0:
    xorl    %eax, %eax
.L2:
    xorl    %edx, %edx
    cmpl    $999, %eax
    setle   %dl
    addl    %edx, %eax
    cmpl    $999, %eax
    jle .L2
    xorl    %eax, %eax
    ret

I would expect the loop to be removed completely.


---


### compiler : `gcc`
### title : `[7 Regression] bogus warnings with -Wmaybe-uninitialized -O3`
### open_at : `2017-08-18T13:50:15Z`
### last_modified_date : `2022-02-23T14:33:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81889
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `7.1.0`
### severity : `normal`
### contents :
Please consider this simple Fortran test case:


module m

   type t
      integer, dimension(:), pointer :: list
   end type

contains

   subroutine s(n, p, Y)
      integer, intent(in) :: n
      type(t) :: p
      real, dimension(:) :: Y

      real, dimension(1:16) :: xx

      if (n > 3) then
         xx(1:n) = 0.
         print *, xx(1:n)
      else
         xx(1:n) = Y(p%list(1:n))
         print *, sum(xx(1:n))
      end if

   end subroutine

end module



When compiled with "gfortran-7 -Wmaybe-uninitialized -O3 -c", it yields:

maybe_uninit.f90:21:0:

          print *, sum(xx(1:n))
 
Warning: ‘xx[3]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[4]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[5]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[6]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[7]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[8]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[9]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[10]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[11]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[12]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[13]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[14]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
maybe_uninit.f90:21:0: Warning: ‘xx[15]’ may be used uninitialized in this function [-Wmaybe-uninitialized]


Obviously no value that is used is actually uninitialized. gfortran 6 and earlier does not show these warnings. Also decreasing the optimization level makes them disappear.


---


### compiler : `gcc`
### title : `false-positive -Warray-bounds`
### open_at : `2017-08-19T18:48:32Z`
### last_modified_date : `2021-07-22T20:32:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81901
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `7.0`
### severity : `normal`
### contents :
When switching from gcc-6 to gcc-7 my application triggers some new compiler warnings (with -Wall). I believe these warnings are false-positives. But I don't see an easy way to work around them. I also tested with a gcc-8 snapshot (2017/08/19) and the problem persists.

I managed to reduce my program to the following:

        int a[8];
        
        extern void f2();
        
        void f() {
            for (int i = 0x3F; i >= 0x10; --i) {
                switch (i & 0xF0) {
                    case 0x00:
                        if ((i & 0x0F) < 8) a[i] = 0;
                        break;
                    case 0x10:
                        f2();
                }
            }
        }

Compile with
    g++ -O2 -Wall test.cpp

warning: array subscript is above array bounds [-Warray-bounds]


---


### compiler : `gcc`
### title : `FMA and addsub instructions`
### open_at : `2017-08-20T11:28:34Z`
### last_modified_date : `2023-08-02T06:50:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81904
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
(asked in https://stackoverflow.com/questions/45298855/how-to-write-portable-simd-code-for-complex-multiplicative-reduction/45401182#comment77780455_45401182 )

Intel has instructions like vfmaddsubps. Gcc manages, under certain circumstances, to merge mult and plus or mult and minus into FMA, but not mult and this strange addsub mix.

#include <x86intrin.h>
__m128d f(__m128d x, __m128d y, __m128d z){
  return _mm_addsub_pd(_mm_mul_pd(x,y),z);
}
__m128d g(__m128d x, __m128d y, __m128d z){
  return _mm_fmaddsub_pd(x,y,z);
}

(the order of the arguments is probably not right)

My first guess as to how this could be implemented without too much trouble is in ix86_gimple_fold_builtin: for IX86_BUILTIN_ADDSUBPD and others, check that we are late enough in the optimization pipeline (roughly where "widening_mul" is), that contractions are enabled, and that the first (?) argument is a single-use MULT_EXPR.

I didn't check what the situation is with the vectorizer (which IIRC can now generate code that ends up as addsub).


---


### compiler : `gcc`
### title : `[7 Regression] gcc 7.1 generates branch for code which was branchless in earlier gcc version`
### open_at : `2017-08-21T11:40:31Z`
### last_modified_date : `2019-11-14T10:20:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81914
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.1.0`
### severity : `normal`
### contents :
Code:

#include <stdint.h>

int cmp(int64_t a, int64_t b)
{
    return a < b ? -1 : a > b;
}

Above function compiled with gcc 4.5 or above and -O2 is compiled in following way. It is branchless:

cmp(long, long):
  xor eax, eax
  cmp rdi, rsi
  mov edx, -1
  setg al
  cmovl eax, edx
  ret

gcc 7.1 generates different code. It has branch:

cmp(long, long):
  cmp rdi, rsi
  jl .L3
  setg al
  movzx eax, al
  ret
.L3:
  mov eax, -1
  ret


---


### compiler : `gcc`
### title : `Code sinking increases register pressure`
### open_at : `2017-08-23T16:38:16Z`
### last_modified_date : `2023-08-04T17:33:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81953
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
void bar();
int j;
void foo(int a, int b, int c, int d, int e, int f)
{
  int l;
  l = a + b + c + d +e + f;
  if (a != 5)
    {
      bar();
      j = l;
    }
}


The whole expression "l = a + b + c..." is moved past the call to bar(), which means we now need to use 6 callee-saved regs to hold the parm values across the call.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] spurious -Wmaybe-uninitialized warning in gcc-8, or with -O1`
### open_at : `2017-08-23T22:26:45Z`
### last_modified_date : `2023-08-03T22:40:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81958
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
building many random configurations of the linux kernel using a gcc-8.0.0 snapshot (20170821) found a couple of bugs with -Wmaybe-uninitialized, but also added one warning for code that does not appear to have any uninitialized use, reduced test case follows:

__attribute__ ((__cold__)) int printk();
struct lpfc_queue {
        int queue_id;
        struct lpfc_queue *hba_eq;
} *cq_phba;
void lpfc_debug_dump_all_queues(unsigned maxidx)
{
        struct lpfc_queue *eq;
        unsigned eqidx;
        printk();
        for (eqidx = 0; eqidx < maxidx; eqidx++) {
                eq = &cq_phba->hba_eq[eqidx];
                if (eq->queue_id)
                        break;
        }
        if (eqidx == maxidx)
                eq = &cq_phba->hba_eq[0];
        printk(eq);
}

$ x86_64-linux-gcc-8.0.0 -Wall -Werror=maybe-uninitialized -Werror -O2 -c lpfc_debugfs.c
lpfc_debugfs-e-21.i: In function 'lpfc_debug_dump_all_queues':
lpfc_debugfs-e-21.i:22:10: error: 'eq' may be used uninitialized in this function [-Werror=maybe-uninitialized]
          printk(eq);
          ^~~~~~~~~~

Older versions warn for -O1 and -Os but not for -O2, while gcc-8.0.0 currently warns for all optimization levels.


---


### compiler : `gcc`
### title : `Improve data tracking for simple conditional code`
### open_at : `2017-08-24T16:37:20Z`
### last_modified_date : `2021-08-02T04:24:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81972
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
[code]
__attribute__((nonnull(1)))
void f1(int*);

void f2(int* n)
{
    int* ptr = nullptr;
    if (*n > 2)
        ptr = n;
    f1(ptr);
}

void f3(int* n)
{
    if (*n > 2)
        f1(n);
    else
        f1(nullptr);
}
[/code]

Functions f2 and f3 are equivalent. Above code compiled by gcc 7.2 (with -O3 -Wall -Wextra) generates following assembler code:

[code]
f2(int*):
  cmp DWORD PTR [rdi], 2
  mov eax, 0
  cmovle rdi, rax
  jmp f1(int*)
f3(int*):
  cmp DWORD PTR [rdi], 2
  jg .L7
  xor edi, edi
.L7:
  jmp f1(int*)
[/code]

There are two issues here:
- both functions contains bug, they try to pass NULL pointer to f1. However gcc reports this for f3 only. gcc is able to track data for optimization purposes, so it should be possible to generate warning for f2.
- f3 is compiled to code with branch, so most probably it will be slower a bit. It could be optimized to get the same code as f2 (branchless).


---


### compiler : `gcc`
### title : `Missed optimization of char_traits::length() on constant string`
### open_at : `2017-08-27T20:03:12Z`
### last_modified_date : `2021-07-22T20:43:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82000
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.0`
### severity : `normal`
### contents :
In the following code, the calculation of the string length is not folded, despite the input string being visible to the compiler:

  template <char ...c>
  struct string_constant {
    static constexpr char internal_buffer[sizeof...(c)+1] = {c..., '\0'};
    constexpr char const* c_str() const { return internal_buffer; }
    constexpr std::size_t size() const { return sizeof...(c); }
  };

  auto const text = string_constant<'s', 'o', 'm', 'e', 'c', 'o', 'm', 'p', 'i', 'l', 'e', 't', 'i', 'm', 'e', 't', 'e', 'x', 't'>{};

  int main() {
    std::string_view sv1{text.c_str()}; // calculation of length not folded

    std::string_view sv2{text.c_str(), text.size()}; // no work required
  }

I tracked it down to `char_traits<char>::length()` not being folded. Note that Clang folds that without problem. Here's a link on Godbolt so you can reproduce the issue easily and look at the assembly: https://godbolt.org/g/sF3enh


---


### compiler : `gcc`
### title : `missing strlen optimization for chained mempcpy calls`
### open_at : `2017-08-28T21:34:06Z`
### last_modified_date : `2021-09-12T08:46:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82017
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
GCC is able to track the length of the string created by the two calls to memcpy in f() below but it's fails to do the same for the chained calls to mempcpy in g().  Since mempcpy is specifically designed for such chained copying it's even more likely to benefit from the strlen optimization than memcpy.

$ cat t.c && gcc -O2 -S -Wall -Wextra -Wpedantic -fdump-tree-optimized=/dev/stdout t.c
unsigned f (char *d)
{
  __builtin_memcpy (d, "abc", 3);
  __builtin_memcpy (d + 3, "def", 4);

  return __builtin_strlen (d);   // strlen call folded into a constant
}

unsigned g (char *d)
{
  __builtin_mempcpy (__builtin_mempcpy (d, "abc", 3), "def", 4);
  return __builtin_strlen (d);   // strlen call not folded
}

;; Function f (f, funcdef_no=0, decl_uid=1815, cgraph_uid=0, symbol_order=0)

f (char * d)
{
  char * _1;

  <bb 2> [100.00%] [count: INV]:
  __builtin_memcpy (d_4(D), "abc", 3);
  _1 = d_4(D) + 3;
  __builtin_memcpy (_1, "def", 4);
  return 6;

}



;; Function g (g, funcdef_no=1, decl_uid=1818, cgraph_uid=1, symbol_order=1)

g (char * d)
{
  void * _1;
  long unsigned int _2;
  unsigned int _7;

  <bb 2> [100.00%] [count: INV]:
  _1 = __builtin_mempcpy (d_4(D), "abc", 3);
  __builtin_mempcpy (_1, "def", 4);
  _2 = __builtin_strlen (d_4(D));
  _7 = (unsigned int) _2;
  return _7;

}


---


### compiler : `gcc`
### title : `SMMLAR pattern not detected on ARMv7-M`
### open_at : `2017-08-30T12:23:46Z`
### last_modified_date : `2018-12-20T14:37:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82034
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
CMSIS-DSP contains the following macro in arm_math.h:

#define multAcc_32x32_keep32_R(a, x, y) \
    a = (q31_t) (((((q63_t) a) << 32) + ((q63_t) x * y) + 0x80000000LL ) >> 32)

This signature should ideally be translated to SMMLAR instruction but it is not.
I noticed a speedup when I replaced this macro with a call to the instrinsic SMMLAR.

Can this macro be translated into SMMLAR, and if so can this be implemented?
This it a feature request I guess.

Sorry if this was filed in the wrong category.


---


### compiler : `gcc`
### title : `Very poor optimization of constant multiply on ARM Cortex-M7`
### open_at : `2017-08-30T15:36:07Z`
### last_modified_date : `2020-04-10T11:01:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82038
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `7.2.0`
### severity : `normal`
### contents :
Consider the following source code:

#include <stdint.h>

int64_t f(int32_t x) {
	return x * 16384LL;
}

int64_t g(int32_t x) {
	return static_cast<int64_t>(x) << 14;
}

Compile it with the following command:

armv7m-none-eabihf-g++ -ffreestanding -Wall -Wextra -O2 -mcpu=cortex-m7
-std=c++17 -c test.cpp

It produces the following code:

00000000 <_Z1fl>:
   0:	b430      	push	{r4, r5}
   2:	17c5      	asrs	r5, r0, #31
   4:	4603      	mov	r3, r0
   6:	0380      	lsls	r0, r0, #14
   8:	03a9      	lsls	r1, r5, #14
   a:	bc30      	pop	{r4, r5}
   c:	ea41 4193 	orr.w	r1, r1, r3, lsr #18
  10:	4770      	bx	lr
  12:	bf00      	nop

00000014 <_Z1gl>:
  14:	4601      	mov	r1, r0
  16:	0380      	lsls	r0, r0, #14
  18:	1489      	asrs	r1, r1, #18
  1a:	4770      	bx	lr

The implementation of f could be the same as g, yet it’s really quite awful.

Changing -mcpu=cortex-m7 to -mcpu=cortex-m4 doesn’t affect g. It yields rather better code for f than the M7 build, but still not as good as g.

I could just use g, but that isn’t really a good option because left-shifting a negative number is UB.


---


### compiler : `gcc`
### title : `[aarch64] vmlsq_f32 compiled into 2 instructions`
### open_at : `2017-09-01T14:28:22Z`
### last_modified_date : `2021-02-10T12:43:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82074
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `7.2.0`
### severity : `enhancement`
### contents :
Created attachment 42100
simplest example showing the bug

On aarch64, the Neon intrinsic "vmlsq_f32" is compiled into:
    fneg  v1.4s, v1.4s
    fmla  v0.4s, v1.4s, v2.4s

instead of:
    fmls  v0.4s, v1.4s, v2.4s

The same output is produced by all the following expressions:
    vmlsq_f32(a, b, c)
    a - b*c
    vsubq_f32(a, vmulq_f32(b, c))


The example has been compiled with gcc -O3
I tested on GCC 4.8.5, GCC 6.3.0 and GCC 7.2.0. All of them has the bug.
The bug is also present at -O1, but with a slightly different output:
    fmul    v1.4s, v1.4s, v2.4s
    fsub    v0.4s, v0.4s, v1.4s

If it can help, here is a godbolt link that shows the bug: https://godbolt.org/g/jWvmxS

Sometimes, depending on the surrounding, it is successfully converted into the FMLS instruction, but never on the attached example.


---


### compiler : `gcc`
### title : `Bogus warning: ‘magic_p’ may be used uninitialized in this function [-Wmaybe-uninitialized]`
### open_at : `2017-09-03T16:29:14Z`
### last_modified_date : `2021-12-03T05:50:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82090
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `7.2.1`
### severity : `normal`
### contents :
[hjl@gnu-efi-2 tmp]$ cat foo.c
typedef int __libc_lock_t;
void malloc_printerr(const char *str) __attribute__ ((noreturn));
extern  __libc_lock_t foo;

static void *
mem2chunk_check (void *mem, unsigned char **magic_p)
{
  if (!mem)
    return (( void *)0) ;
  if (magic_p)
    *magic_p = (unsigned char *) mem;
  return mem;
}

void *
realloc_check (void *oldmem, unsigned int bytes, const void *caller)
{
  unsigned char *magic_p;

  void * oldp = mem2chunk_check (oldmem, &magic_p);
  (void) ({ int ignore; if (__builtin_constant_p (0) && (0) == 0) __asm __volatile ("cmpl $0, __libc_multiple_threads(%%rip)\n\t" "je 0f\n\t" "lock; decl %0\n\t" "jne 1f\n\t" "jmp 24f\n\t" "0:\tdecl %0\n\t" "je 24f\n\t" "1:\tlea %0, %%" "rdi" "\n" "2:\tsub $128, %%" "rsp" "\n" ".cfi_adjust_cfa_offset 128\n" "3:\tcallq __lll_unlock_wake_private\n" "4:\tadd $128, %%" "rsp" "\n" ".cfi_adjust_cfa_offset -128\n" "24:" : "=m" (foo), "=&D" (ignore) : "m" (foo) : "ax", "cx", "r11", "cc", "memory"); else __asm __volatile ("cmpl $0, __libc_multiple_threads(%%rip)\n\t" "je 0f\n\t" "lock; decl %0\n\t" "jne 1f\n\t" "jmp 24f\n\t" "0:\tdecl %0\n\t" "je 24f\n\t" "1:\tlea %0, %%" "rdi" "\n" "2:\tsub $128, %%" "rsp" "\n" ".cfi_adjust_cfa_offset 128\n" "3:\tcallq __lll_unlock_wake\n" "4:\tadd $128, %%" "rsp" "\n" ".cfi_adjust_cfa_offset -128\n" "24:" : "=m" (foo), "=&D" (ignore) : "m" (foo), "S" (0) : "ax", "cx", "r11", "cc", "memory"); });
  if (!oldp)
    malloc_printerr ("realloc(): invalid pointer");

  *magic_p ^= 0xFF;

  return oldp;
}
[hjl@gnu-efi-2 tmp]$ gcc  -Wall  foo.c -S -O3
foo.c: In function ‘realloc_check’:
foo.c:25:12: warning: ‘magic_p’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   *magic_p ^= 0xFF;
   ~~~~~~~~~^~~~~~~
[hjl@gnu-efi-2 tmp]$


---


### compiler : `gcc`
### title : `missed warning for uninitialized value on the loop entry edge`
### open_at : `2017-09-04T19:35:17Z`
### last_modified_date : `2021-12-22T10:43:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82101
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `7.1.1`
### severity : `normal`
### contents :
Created attachment 42118
sample code

An uninitialized variable in a for loop cannot be detected by g++ 7.1.1
Here's the code


---


### compiler : `gcc`
### title : `Missed constant propagation through possible unsigned wraparound, with std::align() variable pointer, constant everything else.`
### open_at : `2017-09-07T18:54:27Z`
### last_modified_date : `2021-12-13T00:58:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82135
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The code in this report is easiest to look at here: https://godbolt.org/g/DffP3J, with asm output.

When g++ inlines this (copied version of std::align from include/c++/memory), it fails to optimize to just rounding up to the next power of 2 when align=size=64 and space=1024, but ptr is variable.

(If __ptr is also constant, it's fine.)

#include <cstdint>
#include <stddef.h>
inline void*
libalign(size_t __align, size_t __size, void*& __ptr, size_t& __space) noexcept
{
  const auto __intptr = reinterpret_cast<uintptr_t>(__ptr);
  const auto __aligned = (__intptr - 1u + __align) & -__align;
//    if (__aligned < __size)   __builtin_unreachable();
  const auto __diff = __aligned - __intptr;
//    if (__diff > __size)  __builtin_unreachable();
  if ((__size + __diff) > __space)
    return (void*)123456; //nullptr;   // non-zero constant is obvious in the asm
  else
    {
      __space -= __diff;
      return __ptr = reinterpret_cast<void*>(__aligned);
    }
}

void *libalign64(void *voidp) {
    std::size_t len = 1024;
             //if (voidp+len < voidp) __builtin_unreachable();   // doesn't help
    voidp = 
      libalign(64, 64, voidp, len);
    return voidp;
}

g++ -O3 -std=c++14  -Wall -Wextra  (trunk 8.0.0 20170906)

        # x86-64.  Other targets do the same compare/cmov or branch
        leaq    63(%rdi), %rax
        andq    $-64, %rax
        movq    %rax, %rdx
        subq    %rdi, %rdx
        addq    $65, %rdx
        cmpq    $1025, %rdx
        movl    $123456, %edx
        cmovnb  %rdx, %rax
        ret


libalign64 gives exactly the same result as just rounding up to the next power of 2 (including wrapping around to zero with addresses very close to the top).  But gcc doesn't spot this, I think getting confused about what can happen with unsigned wraparound.

char *roundup2(char *p) {
    auto t = (uintptr_t)p;
    t = (t+63) & -64;
    return (char*)t;
}

        leaq    63(%rdi), %rax
        andq    $-64, %rax
        ret

For easy testing, I made wrappers that call with a constant pointer, so I can test that it really does wrap around at exactly the same place as roundup2().  (It does: libalign64(-64) = -64, libalign64(-64) = 0.)  So it can safely be compiled to 2 instructions on targets where unsigned integer wraparound works normally, without all that adding constants and comparing against constants.

static char* const test_constant = (char*)-63ULL;

char *test_roundup2() {
    return roundup2(test_constant);
}
void *test_libalign() {
    return libalign64(test_constant);
}


Uncommenting this line I added:
   if (__diff > __size)  __builtin_unreachable();

lets it compile to just two instructions, but that condition isn't really always true.  __diff will be huge when __aligned wraps around.

clang, icc, and msvc also fail to make this optimization.  IDK if it's particularly useful in real life for anything other than abusing std::align as a simple round-up function.


---


### compiler : `gcc`
### title : `unnecessary movapd with _mm_castsi128_pd to use BLENDPD on __m128i results`
### open_at : `2017-09-08T05:47:52Z`
### last_modified_date : `2021-09-06T06:09:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82139
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
#include <immintrin.h>
#include <stdint.h>
// stripped down from a real function that did something more useful
void foo(uint64_t blocks[]) {
    for (int i = 0 ; i<10240 ; i+=2) {
        __m128i v = _mm_loadu_si128((__m128i*)&blocks[i]);
        __m128i t1 = _mm_add_epi32(v, _mm_set1_epi32(1));
        __m128i t2 = _mm_add_epi32(v, _mm_set1_epi32(-1));
        __m128d blend = _mm_blend_pd(_mm_castsi128_pd(t1),
                             _mm_castsi128_pd(t2), 2);
          // is this even aliasing-safe?  Could cast back to __m128i
        _mm_storeu_pd((double*)(__m128d*)&blocks[i], blend);
    }
}

https://godbolt.org/g/im1kcc for source and gcc-trunk asm output (and the slightly larger version of this function that I simplified).

blendpd/blendps have better throughput than pblendw on Intel CPUs, so I played with that in this function I was looking at.

gcc4.8 and later waste a MOVAPD for no reason instead of clobbering one of the PADDD results with the blend.  (The larger version of this function, pairs_u64_sse2 in the godbolt link, avoids the extra MOVAPD with gcc4.9.4 and earlier, but not in foo().  So maybe it's just by chance, or maybe 4.8 changed something.  Anyway, still present in 7.2 and 8.0-trunk, and with -O2 or -O3

(GCC-Explorer-Build) 8.0.0 20170907 -xc -std=gnu99 -O3 -Wall -msse4 -mno-avx

foo:
        pcmpeqd %xmm2, %xmm2
        leaq    81920(%rdi), %rax
        movdqa  .LC0(%rip), %xmm3
.L6:
        movdqa  %xmm3, %xmm1
        addq    $16, %rdi
        movdqu  -16(%rdi), %xmm0
        paddd   %xmm0, %xmm1
        movapd  %xmm1, %xmm4
        paddd   %xmm2, %xmm0
        blendpd $2, %xmm0, %xmm4
        movups  %xmm4, -16(%rdi)
        cmpq    %rdi, %rax
        jne     .L6
        rep ret

Notice that BLENDPD's operands aren't the two output registers from the PADDD instructions.  Different versions/options (like -mtune=skylake) put the extra MOVAPD between the PADDD instructions, or right before BLENDPD, so don't let it fool you. :P

With the function even simpler (like only one _mm_add_epi32), blending between the original load result and the add result didn't appear to have an extra MOVAPD


---


### compiler : `gcc`
### title : `struct zeroing should use wide stores instead of avoiding overwriting padding`
### open_at : `2017-09-08T07:20:22Z`
### last_modified_date : `2021-10-10T20:06:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82142
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
(not sure if tree-optimization is the right "component", please check.)

Zeroing a struct (by assignment from an all-zero struct) is inefficient for structs too small to inline rep stos or memset, if they have padding.  Store coalescing happens (yay, huge improvement over gcc6!), but only if it won't write any padding.  I assume assignment from non-zero struct constants is like this, too.

Note also that gcc zeros two registers for two 16-bit stores, instead of reusing a zeroed reg.

Also note that Haswell doesn't have LCP stalls for a `mov` with 16-bit immediate, only for ALU, so there's no need to avoid it.  But if you *are* going to zero a register, you should use it as the source for all the integer mov instructions to save code-size.  (And avoid filling up space in the uop cache with immediates).  Sandybridge-family no longer has register-read-port stalls (P6 / Core2 / Nehalem), and even there a recently-written register is fine for several cycles.

https://godbolt.org/g/AT7yfs

typedef struct {
    int a,b,c;
    char j,k, k1;
        // 1B of padding
    int l,m,n[8];
    char c1,c2;
        // 2B of padding
}foo;
int sf = sizeof(foo);

sf:
        .long   60   # bytes long

void assignzero(foo *p) {
    foo tmp = {};
    *p = tmp;
}

(GCC-Explorer-Build) 8.0.0 20170907  -xc -std=c11 -O3 -march=haswell


assignzero:
        pushq   %rbp
        xorl    %eax, %eax            # zero a reg to avoid mov $imm16, (mem)
        vpxor   %xmm0, %xmm0, %xmm0
        xorl    %edx, %edx            # and another one, instead of reusing eax??
        movq    $0, (%rdi)
        movl    $0, 8(%rdi)
        movq    %rsp, %rbp      # make a stack frame because of the ymm?  At least it doesn't do something with %r10 like gcc7.2
        movw    %ax, 12(%rdi)
        movb    $0, 14(%rdi)
                           # avoiding a store to 1B of padding
        vmovdqu %ymm0, 16(%rdi)       # bunch of int members
        movq    $0, 48(%rdi)
        movw    %dx, 56(%rdi)
        vzeroupper
        popq    %rbp
        ret

So 1B of padding cost us 4 narrow stores instead of one 16B store at the front of the struct.  This also uses AVX for a single 256b store; very likely not worth it.


// what we would like to see, more or less
void charzero(char *p) {
    __builtin_memset(p, 0, sizeof(foo));
}
charzero:
        vpxor   %xmm0, %xmm0, %xmm0
        movq    $0, 48(%rdi)
        vmovups %xmm0, (%rdi)
        vmovups %xmm0, 16(%rdi)
        vmovups %xmm0, 32(%rdi)
        movl    $0, 56(%rdi)
        ret

This chooses not to use 256b AVX, so no vzeroupper, and no slow-mode while warming up the upper-half of execution units / data bus. (Agner's description sounds like it applies to stores: http://agner.org/optimize/blog/read.php?i=415).  Also no triggering a lower turbo threshold.  All of this is probably good for a tiny function.

We could save some code-size by replacing the integer mov $0 stores with vmovq %xmm0, 48(%rdi).  That's probably not good for Bulldozer-family (shared vector unit between two integer cores), so maybe only enable that with a -march=znver1 or sandybridge-family CPU.  (xor-zeroing takes zero cycles on Intel SnB-family, so xmm0 is ready as a source for the store in the same cycle that vpxor issues.  I guess it's possible that an integer store can execute 1 cycle sooner than an AVX 8-byte store, so if there weren't already any stores waiting to execute, and store-port throughput was about to become a bottleneck in later code, it makes sense.  Maybe only do it for the last store, using a 4-byte vmovd %xmm0, 56(%rdi))

Or better (on Haswell and especially Skylake): use overlapping vector stores like clang has been doing since 3.7, and like glibc memcpy does for small copies:

   # clang (trunk) -O3 -march=haswell
assignzero:                             # same code for clang's memset
        vxorps  %xmm0, %xmm0, %xmm0
        vmovups %ymm0, 28(%rdi)         # last 32 bytes
        vmovups %ymm0, (%rdi)           # first 32 bytes, overlapping by 4
        vzeroupper
        retq

Having to use vzeroupper here is questionable.

4 xmm stores (with the last one overlapping) might be a better choice here, but I haven't tried to measure the effect of scattered uses of AVX.  When tuning for Sandybridge, clang only uses 128b stores here.


---


### compiler : `gcc`
### title : `Autovectorization for insertion is slower than done manually`
### open_at : `2017-09-08T17:24:52Z`
### last_modified_date : `2021-08-03T02:32:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82151
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Take:
void f(float *restrict a, float * restrict b, float * restrict c, int s)
{
  for(int i = 0; i< s;i++)
    {
      c[i*4] = a[i*2];
      c[i*4+1] = a[i*2+1];
      c[i*4+2] = b[i*2];
      c[i*4+3] = b[i*2+1];
    }
}

--- CUT ---
This currently vectorizes using 2xld2 followed by st4.  On some (most?) micro-arch, not vecotrizing is better or vectorizing using 64bit (2xS):
ldr d0, [a, index]
ldr d1, [b, index]
stp d0, d1, [c, index]
is better.

Or even:
ldr d0, [a, index]
ldr d1, [b, index]
ins v0.2d[1], d1
sdr q0, [c, index]
is better than using ld2/st3.

That is just do SLP vectorization and not loop aware SLP here.


---


### compiler : `gcc`
### title : `missed optimization: double truncating`
### open_at : `2017-09-08T20:48:47Z`
### last_modified_date : `2023-08-22T05:01:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82153
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.2.0`
### severity : `enhancement`
### contents :
#include <math.h>

int roundme(double A)
{
        return floor(A * 4.3);
}

leads to

0000000000000000 <roundme>:
   0:   f2 0f 59 05 00 00 00    mulsd  0x0(%rip),%xmm0        # 8 <roundme+0x8>
   7:   00
   8:   66 0f 3a 0b c0 09       roundsd $0x9,%xmm0,%xmm0
   e:   f2 0f 2c c0             cvttsd2si %xmm0,%eax
  12:   c3                      retq

both roundsd $0x9 and cvttsd2si truncate (floor) their argument, so gcc is doing redundant work here; roundsd is +/- 8 cycles which is not cheap.


---


### compiler : `gcc`
### title : `gcc optimizes int range-checking poorly on x86-64`
### open_at : `2017-09-10T22:16:20Z`
### last_modified_date : `2023-06-13T06:17:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82170
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.3.1`
### severity : `enhancement`
### contents :
Created attachment 42148
source code that is poorly optimized on x86-64

GCC on Fedora 26 x86-64 (GCC 7.1.1 20170622 (Red Hat 7.1.1-3) generates poorly-optimized machine instructions for C code that tests in the obvious way whether a 'long long' fits in 'int'. If n is long long, the expression (INT_MIN <= n && n <= INT_MAX) should generate code that is no worse than (!__builtin_add_overflow_p (n, 0, 0)). However, with -O2 optimization before the conditional branch, the former expression generates four instructions containing two literal constants, whereas the latter generates just two instructions that reference only registers.

For the code that prompted this bug report we will likely use an "#if __GNUC__ < 7" that uses __builtin_add_overflow_p for GCC 7 and later, and the portable code otherwise. It'd be nicer, though, if we could just use the portable code.  See:

https://sourceware.org/ml/libc-alpha/2017-09/msg00411.html

Attached are a source program inrange.c and an assembly-language file inrange.s produced by 'gcc -O2 -S' that illustrates the problem. Akthough the two functions checked_arg_GCC7 and checked_arg_portable should have the same machine code, the former is more efficient than the latter.


---


### compiler : `gcc`
### title : `missed PRE at -O3 (-O2 in GCC 12+)`
### open_at : `2017-09-12T08:11:53Z`
### last_modified_date : `2021-11-22T09:01:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82187
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `normal`
### contents :
I have this code: main.c:

int main(int c, char **v){
        char *s = v[0];
        while (*s++ != 0) {
                if ((*s == 'a') && (*s != 'b')) {
                        return 1;
                }
        }
        return 0;
}

which I compile generating the assembly code:

gcc -S -masm=intel -O3 main.c


main:
.LFB0:
        .cfi_startproc
        mov     rax, QWORD PTR [rsi]
        jmp     .L2
        .p2align 4,,10
        .p2align 3
.L4:
        cmp     BYTE PTR [rax], 97
        je      .L5
.L2:
        add     rax, 1
        cmp     BYTE PTR -1[rax], 0
        jne     .L4
        xor     eax, eax
        ret
.L5:
        mov     eax, 1
        ret


There are two accesses to the memory, it could be optimized to one access to the memory.

When I set 'volatile char *s = v[0];', there are 3 accesses to the memory which is good.


---


### compiler : `gcc`
### title : `Missed optimization opportunity for constant folding`
### open_at : `2017-09-12T09:08:37Z`
### last_modified_date : `2023-08-08T06:53:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82188
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
We compiled two programs (A1.c, A2.c) by GCC-8.0.0 with -O3 option.
Though the two programs are equivalent, GCC outputs different assembly codes.


+-------------------------------------+---------------------------------+
| A1.c                                | A2.c                            |
+-------------------------------------+---------------------------------+
|int main (void)                      |int main (void)                  |
|{                                    |{                                |
|  volatile int x = 1;                |  volatile int x = 1;            |
|                                     |                                 |
|  long t1 = 1/(2U-x);                |  long t1 = 1/(2U-x);            |
|  int t2 = 0!=(((int)(2%(2+t1)))/2); |  int t2 = 0!=(int)t1;           |
|                                     |                                 |
|  if (t1 != 1) __builtin_abort();    |  if (t1 != 1) __builtin_abort();|
|  if (t2 != 1) __builtin_abort();    |  if (t2 != 1) __builtin_abort();|
|                                     |                                 |
|  return 0;                          |  return 0;                      |
|}                                    |}                                |
+-------------------------------------+---------------------------------+

+-------------------------------------+-------------------------------------+
|    A1.s (gcc-8.0.0 A1.c -O3 -S)     |    A2.s (gcc-8.0.0 A2.c -O3 -S)     |
+-------------------------------------+-------------------------------------+
|main:                                |main:                                |
|.LFB0:                               |.LFB0:                               |
|   .cfi_startproc                    |   .cfi_startproc                    |
|   subq   $24, %rsp                  |   subq   $24, %rsp                  |
|   .cfi_def_cfa_offset 32            |   .cfi_def_cfa_offset 32            |
|   movl   $2, %esi                   |   movl   $2, %ecx                   |
|   xorl   %edx, %edx                 |   xorl   %edx, %edx                 |
|   movl   $1, 12(%rsp)               |   movl   $1, 12(%rsp)               |
|   movl   12(%rsp), %eax             |   movl   12(%rsp), %eax             |
|   subl   %eax, %esi                 |   subl   %eax, %ecx                 |
|   movl   $1, %eax                   |   movl   $1, %eax                   |
|   divl   %esi                       |   divl   %ecx                       |
|   movl   %eax, %esi                 |   testl  %eax, %eax                 |
|   movl   $2, %eax                   |   je     .L3                        |
|   movq   %rsi, %rcx                 |                                     |
|   cqto                              |                                     |
|   addq   $2, %rsi                   |                                     |
|   idivq  %rsi                       |                                     |
|   cmpl   $1, %ecx                   |                                     |
|   jne    .L3                        |                                     |
|   cmpq   $2, %rdx                   |                                     |
|   jne    .L3                        |                                     |
|   xorl   %eax, %eax                 |   xorl   %eax, %eax                 |
|   addq   $24, %rsp                  |   addq   $24, %rsp                  |
|   .cfi_def_cfa_offset 8             |   .cfi_def_cfa_offset 8             |
|   ret                               |   ret                               |
|   .cfi_endproc                      |   .cfi_endproc                      |
|   .section   .text.unlikely         |   .section   .text.unlikely         |
|   .cfi_startproc                    |   .cfi_startproc                    |
|   .type   main.cold.0, @function    |   .type   main.cold.0, @function    |
|main.cold.0:                         |main.cold.0:                         |
|.L3:                                 |.L3:                                 |
|   .cfi_def_cfa_offset 32            |   .cfi_def_cfa_offset 32            |
|   call   abort                      |   call   abort                      |
|   .cfi_endproc                      |   .cfi_endproc                      |
|.LFE0:                               |.LFE0:                               |
|   .section   .text.startup          |   .section   .text.startup          |
|   .size   main, .-main              |   .size   main, .-main              |
|   .section   .text.unlikely         |   .section   .text.unlikely         |
|   .size  main.cold.0, .-main.cold.0 |   .size  main.cold.0, .-main.cold.0 |
+-------------------------------------+-------------------------------------+


FYI, LLVM/clang generates the same same assembly codes for A1.c and A2.c.

+---------------------------------+---------------------------------+
|  A1.s (clang-6.0.0 A1.c -O3 -S) |  A2.s (clang-6.0.0 A2.c -O3 -S) |
+---------------------------------+---------------------------------+
|main:              # @main       |main:              # @main       |
|   .cfi_startproc                |   .cfi_startproc                |
|# BB#0:                          |# BB#0:                          |
|   pushq   %rax                  |   pushq   %rax                  |
|.Lcfi0:                          |.Lcfi0:                          |
|   .cfi_def_cfa_offset 16        |   .cfi_def_cfa_offset 16        |
|   movl   $1, 4(%rsp)            |   movl   $1, 4(%rsp)            |
|   movl   4(%rsp), %eax          |   movl   4(%rsp), %eax          |
|   cmpl   $1, %eax               |   cmpl   $1, %eax               |
|   jne   .LBB0_2                 |   jne   .LBB0_2                 |
|# BB#1:                          |   # BB#1:                       |
|   xorl   %eax, %eax             |   xorl   %eax, %eax             |
|   popq   %rcx                   |   popq   %rcx                   |
|   retq                          |   retq                          |
|.LBB0_2:                         |.LBB0_2:                         |
|   callq   abort                 |   callq   abort                 |
|.Lfunc_end0:                     |.Lfunc_end0:                     |
|   .size   main, .Lfunc_end0-main|   .size   main, .Lfunc_end0-main|
+---------------------------------+---------------------------------+

gcc-8.0 (GCC) 8.0.0 20170828 (experimental)
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

clang version 6.0.0 (trunk 308582) (llvm/trunk 308581)
Target: x86_64-unknown-linux-gnu
Thread model: posix


---


### compiler : `gcc`
### title : `Two stage SLP needed`
### open_at : `2017-09-12T09:26:03Z`
### last_modified_date : `2021-08-11T06:57:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82189
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Take:
void f(float *restrict a, float * restrict b, float * restrict c, float t)
{
  int i = 0 ;
  a[i] = b[i]/t;
  a[i+1] = b[i+1]/t;
  a[i+2] = c[i]/t;
  a[i+3] = c[i+1]/t;
}

Right now we do SLP once (at -O3) and produce:
f:
        dup     v2.2s, v0.s[0]
        ldr     d1, [x1]
        ldr     d0, [x2]
        fdiv    v1.2s, v1.2s, v2.2s
        fdiv    v0.2s, v0.2s, v2.2s
        stp     d1, d0, [x0]
        ret

But it might be better do:
f:
        dup     v2.4s, v0.s[0]
        ldr     d0, [x1]
        ldr     d1, [x2]
        ins     v0.2d[1], v1.2d[0]
        fdiv    v0.4s, v0.4s, v2.4s
        str     q0, [x0]
        ret

Mainly because two div is usually not pipelined.


---


### compiler : `gcc`
### title : `__builtin_shuffle sometimes should produce zip1 rather than TBL`
### open_at : `2017-09-13T08:06:21Z`
### last_modified_date : `2020-07-21T10:03:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82199
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
Take:
#define vector __attribute__((vector_size(16) ))

vector float f(vector float a, vector float b)
{
  return __builtin_shuffle  (a, b, (vector int){0, 1, 4,5});
}
---- CUT ---

Currently this produces TBL but really we should be able to produce (for little-endian):
f:
  ins v0.2d[1], v1.2d[0]
  ret

--- CUT ---

X86_64 is able to produce:
f:
        movlhps %xmm1, %xmm0
        ret

Which is what I had expected.

There is most likely many more __builtin_shuffle which can be optimized for aarch64 without using TBL which we are not currently doing.


---


### compiler : `gcc`
### title : `GCC7's LTO underperforms compared to GCC6`
### open_at : `2017-09-16T14:29:19Z`
### last_modified_date : `2019-05-15T18:40:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82229
### status : `WAITING`
### tags : `lto, missed-optimization`
### component : `lto`
### version : `7.2.0`
### severity : `normal`
### contents :
A small SFML project on github:
https://github.com/Hopson97/Empire
Takes huge advantage of -flto flag on GCC6, but after upgrading to GCC7, the difference is negligible.
Here are some of my test results:
gcc6.3 no flto: 24-25 fps
gcc7.2 no flto: 24 fps
gcc6.3 with flto: 40-42fps
gcc7.2 with flto: 26-27fps


---


### compiler : `gcc`
### title : `[AArch64] Destructive operations result in poor register allocation after scheduling`
### open_at : `2017-09-18T13:25:26Z`
### last_modified_date : `2021-07-25T00:59:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82237
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `8.0`
### severity : `normal`
### contents :
A destructive operation is one in which an input operand is both read and written. For example, in the vector FMLA instruction in AArch64:

  FMLA v0.4s, v1.4s, v2.4s

The first operand is used for the accumulator value (the operation is v0 = v0 + v1 * v2) and is both read and written by the instruction.

In RTL terms, this is:

  (define_insn "fma<mode>4"
    [(set (match_operand:VHSDF 0 "register_operand" "=w")
         (fma:VHSDF (match_operand:VHSDF 1 "register_operand" "w")
	  	  (match_operand:VHSDF 2 "register_operand" "w")
		    (match_operand:VHSDF 3 "register_operand" "0")))]
    "TARGET_SIMD"
   "fmla\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>"
    [(set_attr "type" "neon_fp_mla_<stype><q>")]
  )

from config/aarch64/aarch64-simd.md .

We can get suboptimal code where a read/write operand is used both by a destructive operation, and a non-destructive operation, and the destructive operation is scheduled before the non-destructive operation. For example, with this auto-vectorizable code (with trunk, -O3 -mcpu=cortex-a57):

  void
  foo (float* __restrict__ in1, float* __restrict__ in2,
       float* __restrict__ out1, float* __restrict__ out2)
  {
    for (int i = 0; i < 1024; i++)
      {
        float t = out1[i];
        out1[i] = t + in1[i] * in2[i];
        out2[i] = t + in1[i];
      }
  }

	ldr	q1, [x2, x4]
	ldr	q0, [x0, x4]
	ldr	q2, [x1, x4]
	mov	v3.16b, v1.16b          // <<<<<< 1)
	fmla	v3.4s, v2.4s, v0.4s     // <<<<<< 2)
	fadd	v0.4s, v0.4s, v1.4s     // <<<<<< 3)
	str	q3, [x2, x4]
	str	q0, [x3, x4]


The scheduling of 2) before 3) forces a reload from v1 in to v3 at 1). With an improved schedule, this could be:

	ldr	q1, [x2, x4]
	ldr	q0, [x0, x4]
	ldr	q2, [x1, x4]
	fadd	v4.4s, v0.4s, v1.4s     // <<<<<< 3)
	fmla	v3.4s, v2.4s, v0.4s     // <<<<<< 2)
	str	q3, [x2, x4]
	str	q4, [x3, x4]

In larger loops, we can end up in this situation more frequently than we would like - the cost of the extra move instructions can be high.


---


### compiler : `gcc`
### title : `Vectorizer cost model overcounts cost of some vectorized loads`
### open_at : `2017-09-19T13:14:21Z`
### last_modified_date : `2023-07-13T05:42:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82255
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Consider the following testcase, compiled at -O3 on powerpc64le:

extern int abs (int __x) __attribute__ ((__nothrow__, __leaf__)) __attribute__ \
((__const__));

static int
foo (unsigned char *w, int i, unsigned char *x, int j)
{
  int tot = 0;
  for (int a = 0; a < 16; a++)
    {
      for (int b = 0; b < 16; b++)
        tot += abs (w[b] - x[b]);  (*)
      w += i;
      x += j;
    }
  return tot;
}

void
bar (unsigned char *w, unsigned char *x, int i, int *result)
{
  *result = foo (w, 16, x, i);
}

The salient feature here is that we process 16 characters from "w" at
a time, and each time through the "a" loop we increment the "w"
pointer by 16; and we process 16 characters from "x" at a time, but
here each time through the "a" loop we increment the "x" pointer by an
unknown but unchanging value.

Since we don't know the alignment of either "w" or "x", it is logical
to assume that vectorizing this loop on a machine with a 128-bit
vector size will treat both "w" and "x" the same.  The 16 loads of
characters from each array can be combined into a single unaligned
load of a 16-byte vector.  And indeed, this is the code that the
vectorizer produces.  It is only the stride *between* vector loads
that differs; one is constant and the other is variable.

However, the vector cost model does NOT produce the same results for
"w" and "x".  For "w", the cost model represents the combined loads as
a single "unaligned_load" vect_cost_for_stmt.  For "x", the cost model
represents them as an "unaligned_load" followed by a "vec_construct".
This wrongly makes the load of "x" look a good deal more expensive
than the load of "w".

Now, how does this happen?  Prior to vectorization, the "b" loop is
completely unrolled, exposing the main computation (*) to SLP
vectorization.  In tree-vect-slp.c:vect_analyze_slp_cost_1, we find
that for "x" (but not "w"), STMT_VINFO_STRIDED_P (stmt_info) is TRUE,
and this causes us to assume a memory access type of
VMAT_STRIDED_SLP.  That value is passed to tree-vect-stmts.c:
vect_model_load_cost, which first records the cost of the load, and
then (because of the memory access type VMAT_STRIDED_SLP) also records
a vec_construct cost.

This would be appropriate if the loads making up the vectorized load
didn't already form a full vector, so that a strided load of elements
were being loaded.  For example, if we loaded 4 characters at a time
and then skipped ahead by an unknown stride amount to get the next 4,
a strided load of four 32-bit elements would indeed require a
vec_construct.  But that's not the case here.

Going back a bit further, STMT_VINFO_STRIDED_P (stmt_info) is set to
TRUE in tree-vect-data-refs.c:vect_analyze_data_refs, because the
group of data references appears within a loop, and the DR_STEP for
the group is not a constant.  Note that the analysis is done for the
outer loop, but is consumed when doing SLP vectorization on the
unrolled inner loop (on the loop body of the outer loop).

Looking at tree-vect-stmts.c:vectorizable_load, we don't actually
generate the constructor unless the number of loads we're going to
generate is greater than 1.  So we're missing the corresponding logic
in the analysis phase.

I'm working on a patch.


---


### compiler : `gcc`
### title : `missed optimization: use LEA to add 1 to flip the low bit when copying before AND with 1`
### open_at : `2017-09-19T16:27:18Z`
### last_modified_date : `2023-06-23T03:47:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82259
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `enhancement`
### contents :
bool bt_signed(int x, unsigned bit) {
        bit = 13;
        return !(x & (1<<bit));
}
// https://godbolt.org/g/rzdtzm
        movl    %edi, %eax
        sarl    $13, %eax
        notl    %eax
        andl    $1, %eax
        ret

This is pretty good, but we could do better by using addition instead of a separate NOT.  (XOR is add-without-carry.  Adding 1 will always flip the low bit).

        sarl    $13, %edi
        lea     1(%edi), %eax
        andl    $1, %eax
        ret

If partial-registers aren't a problem, this will be even better on most CPUs:

        bt      $13, %edi
        setz    %al
        ret

related: bug 47769 about missed BTR peepholes.  That probably covers the missed BT.

But *this* bug is about the LEA+AND vs. MOV+NOT+AND optimization.  This might be relevant for other 2-operand ISAs with mostly destructive instructions, like ARM Thumb.


Related:

bool bt_unsigned(unsigned x, unsigned bit) {
        //bit = 13;
        return !(x & (1<<bit));  // 1U avoids test/set
}

        movl    %esi, %ecx
        movl    $1, %eax
        sall    %cl, %eax
        testl   %edi, %eax
        sete    %al
        ret

This is weird.  The code generated with  1U << bit  is like the bt_signed code above and has identical results, so gcc should emit whatever is optimal for both cases.  There are similar differences on ARM32.

(With a fixed count, it just makes the difference between NOT vs. XOR $1.)

If we're going to use setcc, it's definitely *much* better to use  bt  instead of a variable-count shift + test.

        bt      %esi, %edi
        setz    %al
        ret


---


### compiler : `gcc`
### title : `[x86] Unnecessary use of 8-bit registers with -Os.  slightly slower and larger code`
### open_at : `2017-09-19T16:39:45Z`
### last_modified_date : `2021-08-29T20:54:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82260
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
int shift(int x, int c) {
        return x >> c;
}
// https://godbolt.org/g/waovLu

gcc8 20170915 -Os -mtune=haswell:
        movl    %edi, %eax
        movb    %sil, %cl       # bad
        sarl    %cl, %eax
        ret

-O3:
        movl    %edi, %eax
        movl    %esi, %ecx      # good
        sarl    %cl, %eax
        ret

The 8-bit MOV needs a REX prefix to access %sil, and has a false dependency on the old value of RCX.  Haswell/Skylake don't rename low8 partial registers, only high8.  https://stackoverflow.com/q/45660139/224132.  P6 and Sandybridge do, but an 8-bit mov is definitely *not* better when a 32-bit mov is also an option.

So -Os makes code that's larger and also potentially slower.


---


### compiler : `gcc`
### title : `x86: missing peephole for SHLD / SHRD`
### open_at : `2017-09-19T17:09:34Z`
### last_modified_date : `2022-04-09T07:00:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82261
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
unsigned shld(unsigned a, unsigned b, unsigned n){
        //n=13;
        a <<= n;
        b >>= (32-n); //&31;
        return a|b;
}
// https://godbolt.org/g/3jbgbR

g++ (GCC-Explorer-Build) 8.0.0 20170919 -O3 -march=haswell
        movl    $32, %eax
        subl    %edx, %eax          # missed optimization: NEG would work
        shrx    %eax, %esi, %eax
        shlx    %edx, %edi, %esi
        orl     %esi, %eax
        ret

Intel has efficient SHLD/SHRD, so this should be compiled similar to what clang does:

        movl    %edx, %ecx
        movl    %edi, %eax           # move first so we overwrite a mov-elimination result right away
        shldl   %cl, %esi, %eax
        retq

Without SHLD, there's another missed optimization: shifts mask their count, and 32 & 31 is 0, so we could just NEG instead of setting up a constant 32.

        shlx    %edx, %edi, %eax
        neg     %edx
        shrx    %edx, %esi, %esi
        orl     %esi, %eax
        ret

This *might* be worth it on AMD, where SHLD is 7 uops and one per 3 clock throughput/latency.  Without BMI2, though, it may be good to just use SHLD anyway.

There are various inefficiencies (extra copying of the shift count) in the non-BMI2 output, but this bug report is supposed to be about the SHRD/SHLD peephole.  (I didn't check for SHRD).


---


### compiler : `gcc`
### title : `Missed optimization opportunity constexpr/inline data copy not elided`
### open_at : `2017-09-21T13:56:06Z`
### last_modified_date : `2021-08-22T23:47:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82280
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Consider the C++ code:

```
#include <array>
#include <algorithm>

constexpr auto get_tokens()
{
  return std::array<char, 53>{"This is my string of possible tokens to match."};
}

struct S {
#ifndef LOCAL
  static constexpr auto m_tokens = get_tokens();
#endif

  constexpr bool has_token(const char c) const {
#ifdef LOCAL
    const constexpr auto m_tokens = get_tokens();
#endif

    for (const auto val : m_tokens) {
      if (val == c) { return true; }
    }
    return false;
  }
};

int main(const int argc, const char *[]) {
  constexpr S s;
  return s.has_token(char(argc));
}
```

Compiled with `-O3 -std=c++1z`, the `m_tokens` is a struct-level static constexpr data member. Adding `-DLOCAL` makes it a const constexpr local variable within the `has_token` routine.

When compiled with local, the result of the `get_tokens()` call is copied to stack, and is used from there. The copy introduces 10 instructions. Given that the source is immutable, the copy can be elided and the original data referred to instead. See, for example: https://godbolt.org/g/ZVZMi6

Clang avoids the copy (see https://godbolt.org/g/j5P8Uv - NB this is clang 5.0 with `-O2` to prevent some of its other optimizations from confusing the issue further).


---


### compiler : `gcc`
### title : `Array of objects with constexpr constructors initialized from space-inefficient memory image`
### open_at : `2017-09-22T08:33:17Z`
### last_modified_date : `2019-01-08T10:22:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82294
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `7.2.0`
### severity : `normal`
### contents :
Consider the following code:

$ cat test.cpp
struct S {
	int x;

	explicit constexpr S();
};

constexpr S::S() : x{7} {}

struct T {
	S objects[256];

	explicit T();
};

T::T() {}


Compile it as follows:

$ g++ -Wall -Wextra -march=native -std=c++17 -Os -c test.cpp


Dumping the resulting object file reveals that T::T() is implemented by memcpying (depending on the target architecture sometimes inline or sometimes by an actual memcpy call) a kilobyte of data from .rodata into the array being initialized. For two or three S objects that might be a good, efficient solution. For 256 of them, a kilobyte of memory entirely filled with sevens to serve as an initialization image is rather ridiculous compared to just using a loop to store seven into the objects one by one. This is especially egregious considering I was asking to optimize for size!

Making S::S() non-constexpr improves the situation, though it’s hardly an ideal solution.

This might be related to any of #12245, #56671, #59659, #63728, #68399, or #71165, but none of them describe quite the same problem. The first five are about memory or CPU time usage during compilation (not about the generated code), and #71165 is specifically about aggregate initialization and appears to be an unrolled loop in code rather than a huge data blob.


---


### compiler : `gcc`
### title : `x86 BMI: no peephole for BZHI`
### open_at : `2017-09-22T16:30:21Z`
### last_modified_date : `2021-09-04T22:01:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82298
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
gcc never seems to emit BZHI on its own.

// exact BZHI behaviour for all inputs (with no C UB)
unsigned bzhi_exact(unsigned x, unsigned c) {
    c &= 0xff;
    if (c <= 31) {
      x &= ((1U << c) - 1);
      // 1ULL defeats clang's peephole, but is a convenient way to avoid UB for count=32.
    }
    return x;
}
// https://godbolt.org/g/tZKnV3

unsigned long bzhi_l(unsigned long x, unsigned c) {
    return x & ((1UL << c) - 1);
}

Out-of-range shift UB allows peepholing to BZHI for the simpler case, so these (respectively) should compile to

        bzhil   %esi, %edi, %edi
        bzhiq   %rsi, %rdi, %rax

But we actually get (gcc8 -O3 -march=haswell (-mbmi2))

        movq    $-1, %rax
        shlx    %rsi, %rax, %rdx
        andn    %rdi, %rdx, %rax
        ret

Or that with a test&branch for bzhi_exact.  Clang succeeds at peepholing BZHI here, but it still does the &0xff and the test&branch to skip BZHI when it would do nothing.  It's easy to imagine cases where the source would use a conditional to avoid UB when it wants to leave x unmodified for c==32, and the range is 1 to 32:

unsigned bzhi_1_to_32(unsigned x, unsigned c) {
    if (c != 32)
        x &= ((1U << c) - 1);
    return x;
}


BZHI is defined to saturate the index to OperandSize, so it copies src1 unmodified when the low 8 bits of src2 are >= 32 or >= 64.  (See the Operation section of http://felixcloutier.com/x86/BZHI.html.  The text description is wrong, claiming it saturates to OperandSize-1, which would zero the high bit.)

Other ways to express it (which clang fails to peephole to BZHI, like gcc):

unsigned bzhi2(unsigned x, unsigned c) {
    //  c &= 0xff;
    //  if(c < 32) {
      x &= (0xFFFFFFFFUL >> (32-c));
    //  }
    return x;
}

unsigned bzhi3(unsigned long x, unsigned c) {
    // c &= 0xff;
    return x & ~(-1U << c);
}



Related: pr65871 suggested this, but was really about taking advantage of flags set by __builtin_ia32_bzhi_si so it is correctly closed.  pr66872 suggested transforming x & ((1 << t) - 1); to x & ~(-1 << t); to enable ANDN.  Compiling both to BZHI when BMI2 is available was mentioned, but the the main subject of that bug either.


---


### compiler : `gcc`
### title : `Better PIE/PIC code generation for kernel code (x86_64 & arm64)`
### open_at : `2017-09-22T23:52:14Z`
### last_modified_date : `2019-06-15T00:43:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82303
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
The current PIE/PIC code generation is not optimal for kernel code.

It makes inferences about the execution environment which do not hold for freestanding executables such as the Linux kernel, regarding the need to avoid text relocations, to minimize the footprint of CoWed pages, and to always refer to exported symbols via the GOT so they can be preempted. None of these concerns apply to freestanding binaries.

Having a separate flag (like mcmodel=kernel-pie or -fkernel-pie) would allow better code optimization for PIE/PIC kernel code, notably:

- Select the right segment register for TLS on kernel code (For example x86_64 use gs instead of fs [1]).
- No need for GOT or PLT.
- Re-enable code optimizations disabled for COW pages support, trying to reduce relocations to code sections. For example, switch are not folded for PIE/PIC code to avoid relocations [2].

Note that arm64 PIE uses the small or tiny mcmodel based on UEFI so it should be taken in considerations for this architecture.

For reference the discussion on Linux kernel x86_64 PIE RFC: http://www.openwall.com/lists/kernel-hardening/2017/09/21/16

[1] https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81708
[2] https://github.com/gcc-mirror/gcc/blob/7977b0509f07e42fbe0f06efcdead2b7e4a5135f/gcc/tree-switch-conversion.c#L828


---


### compiler : `gcc`
### title : `worse code generated compared to clang when using a constexpr array`
### open_at : `2017-09-25T22:23:43Z`
### last_modified_date : `2023-01-19T04:24:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82325
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.2.0`
### severity : `normal`
### contents :
While testing some functions I'm writing for a deflate compressor I've noticed that the following code is translated differently between gcc 7.2 and clang 5.0

	#include <array>

	struct code_value
	{
		uint16_t base;
		uint8_t bits;
	};

	constexpr std::array<code_value, 29> al = {{
		{  3, 0}, {  4, 0}, {  5, 0}, {  6, 0}, {  7, 0}, {  8, 0}, {  9, 0}, {  10, 0},
		{ 11, 1}, { 13, 1}, { 15, 1}, { 17, 1},
		{ 19, 2}, { 23, 2}, { 27, 2}, { 31, 2},
		{ 35, 3}, { 43, 3}, { 51, 3}, { 59, 3},
		{ 67, 4}, { 83, 4}, { 99, 4}, {115, 4},
		{131, 5}, {163, 5}, {195, 5}, {227, 5},
		{258, 0}
	}};


	code_value f(int v) {
		size_t index = 0;
		while (index < al.size()) {
			auto mi = al[index].base;
			auto mx = al[index].base + (1 << al[index].bits);
			if (mi <= v && v < mx)
				break;
			index++;
		}
		return al[index];
	}
	
On gcc (with -O3 and -funroll-loop) every iteration is (more or less):

	.L4:
	  movzx ecx, BYTE PTR al[2+rax*4]
	  movzx r9d, WORD PTR al[0+rax*4]
	  mov r10d, esi
	  sal r10d, cl
	  add r10d, r9d
	  cmp r10d, edi
	  jle .L5
	  cmp r9d, edi
	  jle .L2

while on clang 

	.LBB0_4:
	  cmp edi, 13
	  jge .LBB0_6
	  mov eax, 8
	  mov eax, dword ptr [4*rax + al]
	  ret

It looks like the latter is able to infer at compilet time the values of `al[index].base + (1 << al[index].bits);`

godbolt link: https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAKxAEZSBnVAV2OUxAHIBSAJgGY8AO2QAbZlgDU3fgGEAhsWLyAnjOzcADAEEt2hgWLNkBSWiwB9AG7zxmPdwDsAIQeaAnM2EFaANgumAEbyDPb8rjpant4AHAGSgYQMMhG6jgAiKW7aaEIGmAAeAA7EkgboICCKympy5pjWtsyYpJK87uqSttL86dIuTqlRg5KS/K2aTumtI5IALBNTMy6jAKyLGcvOo74b0/3bko57W6MxJwej7heztJonbh0rtPSStEuXtONvHyO06z9Np9joDpo8/tc2r8VgJWrxodteCD4UCRvxXiiwZEPGiAfwEfNvvjUStVq9ifsRqtIRTSOCVr4QXMCTFvsySdt3JD2ZSXC8ATy6dins4vq9Vgi/N8JRzaO4ATLec5eEjWoqhbocS5eKtzpJJhkHIbDeFsnp6o07JIAGYQbySKwASgOjwYeAAXg1TMIsAUen1JqbhQB3BB4USYSR2oS%2BnqyLqiAB0bs9EEdzsG4I88mYREkAFs8P6E9xVs4fYVS%2BlE8FQllhVEc3n836ZH1bKXyzHK6tq7XI3xthBaHGZPGO2WKwUqzWko765qOh48Nao4XR70HdJeL4%2BL4t2OCwV5w2cR5AsRMPIANYL4bL7vT3iuZ93rWZYWXgisIQlyePqs7wyThHVIUQuFWThSCELhNCg1AuFkQdBzKFg2AHARaCgghYJA0DrxAfh%2BETIjSLI8jdnAzg5igmDODg0gEM4KCGBAe4cPokDSDgWAkDQfMinDTAyAoCB%2BMEiNiBAYBHF4UhrXDAhhNYiBAlw0hEiERQVC4LDSH4/NMCEAgAHkhFEHTONILB83kIRgAjdT8EvEw8CsTBWKswpMGQXMOE4PTvEwKiGMMPB81w0DRDwQJWMgUDUCKAg8FQPIuAAWnKNtkGQ59aHdSR0pM/hCutZghGIVBRFEdLRFQRKGBYtD2DoKKINo9SmIKGJfHS3w5kkYBkGQI5E14KNcEIEht3RVpZFQAShNKPh0UdbDIvwwjiPInbSMorgaOgzquBYtjSA4uDQJ4xAUAWiThPIShxKWlBRDs4BVk0e4FNEJTiBUtSrM07TdKggyjNM8zLIYmz3scqznJ85L3M8hjvN8pTQfIYzgvUsKIs4qKYritNGKSlK0s4TKCHQbLctFAqiv4JrWBa2g2s4SCjqsrqer6gaxHeyRVkTTRRYm/AiGWzC5rupaZt4Nbzo20gCKIkjdp2sCDo6nmTsYM6Lrw7XOF4XWGKY9bCdA9z/opkA5iAA%3D


---
