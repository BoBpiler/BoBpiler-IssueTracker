### Total Bugs Detected: 4649
### Current Chunk: 25 of 30
### Bugs in this Chunk: 160 (From bug 3841 to 4000)
---


### compiler : `gcc`
### title : `Missed optimization: shifting signed to unsigned range before comparison not removed`
### open_at : `2022-05-29T16:17:37Z`
### last_modified_date : `2023-05-17T21:31:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105768
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following translation unit:

```
#include <limits.h>

inline unsigned to_unsigned(int value) {
	return (unsigned)value + (unsigned)INT_MIN;
}

bool f(int lhs, int rhs) {
	return to_unsigned(lhs) < to_unsigned(rhs);
}
```

when compiled with `-O3` optimizes to

```
f(int, int):
        add     esi, -2147483648
        add     edi, -2147483648
        cmp     edi, esi
        setb    al
        ret
```

I would expect this to optimize to

```
f(int, int):
        cmp     edi, esi
        setl    al
        ret
```

Essentially, I want gcc to recognize that a signed value + minimum signed value, as an unsigned, has the same comparison semantics as just comparing the original signed value.

This code pattern comes up in implementations of radix sort (specifically, ska_sort) when it falls back to std::sort (for instance, because the range is small).

See it live: https://godbolt.org/z/Gn4rxr3nY


---


### compiler : `gcc`
### title : `[Aarch64] Failure to optimize and+cmp to tst`
### open_at : `2022-05-30T13:29:04Z`
### last_modified_date : `2023-09-21T12:49:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105773
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
int
baz (unsigned long x, unsigned long y)
{
  return (int) (x & y) > 0;
}

With -O3, AArch64 GCC outputs this:

baz(unsigned long, unsigned long):
        and     w0, w0, w1
        cmp     w0, 0
        cset    w0, gt
        ret

whereas LLVM outputs this:

baz(unsigned long, unsigned long):
        tst     w1, w0
        cset    w0, gt
        ret

It seems to me as though using tst should be faster (unless Aarch64 processors are extremely weird).


---


### compiler : `gcc`
### title : `Failure to recognize __builtin_mul_overflow pattern`
### open_at : `2022-05-30T19:22:43Z`
### last_modified_date : `2023-09-21T12:45:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105776
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
int f4(unsigned x, unsigned y)
{
    if (x == 0)
        return 1;
    return ((int)(x * y) / (int)x) == y;
}

can be optimized to

int f4(unsigned x, unsigned y)
{
    int z;
    return !__builtin_mul_overflow((int)x, (int)y, &z);
}

This transformation is done by LLVM, but not by GCC.

Note that this derivates from another function written as such:

int
f3 (unsigned x, unsigned y)
{
  unsigned int r = x * y;
  return !x || ((int) r / (int) x) == (int) y;
}

which does optimize correctly on x86 but not on aarch64 (where it generates tree-optimized GIMPLE corresponding to the code above)


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_mul_overflow with constant operand to add+cmp check`
### open_at : `2022-05-30T20:37:11Z`
### last_modified_date : `2023-09-21T12:36:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105777
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
int f17(unsigned x)
{
    int z;
    return __builtin_mul_overflow((int)x, 35, &z);
}

This can be optimized to `return (x + 0xFC57C57C) < 0xF8AF8AF9;` (and I'd assume the same pattern with other constants than 35 should be optimizable in the same way). LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Shift by register --- unnecessary AND instruction`
### open_at : `2022-05-30T21:21:30Z`
### last_modified_date : `2022-10-24T17:44:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105778
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `enhancement`
### contents :
Created attachment 53053
Sample code

With -O2, some x86 shift-by-register instructions are preceded by an unnecessary AND instruction.  The AND instruction is unnecessary because the shift-by-register instructions already mask the register containing the variable shift.

In the sample code, the #if 0 branch produces the code

        mov     rax, rdi
        mov     ecx, edi
        shr     rax, cl
        ret

but the #if 1 branch produces the code

        mov     rcx, rdi
        mov     rax, rdi
        and     ecx, 63
        shr     rax, cl
        ret

even though the code has the same behavior.  Note that the and ecx, 63 is unnecessary here because shr rax, cl will already operate on the bottom 6 bits of ecx anyway, as per the Intel manual.

As notated in the code's comments, some explicit masks other than 0x3f may produce even more inefficient code, e.g.:

        movabs  rcx, 35184372088831
        mov     rax, rdi
        and     rcx, rdi
        shr     rax, cl
        ret

while some other masks like 0xff and 0xffff eliminate the explicit and altogether.

Found with gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0.  Verified with godbolt for all gcc versions from 9.4.0 through trunk.

For the sake of completeness, I could not get clang to reproduce this problem.  The latest classic ICC compiler available in Godbolt (2021.5.0) can emit code with MOVABS as above.  However, the newer ICX Intel compiler behaves like clang (this seems reasonable).


---


### compiler : `gcc`
### title : `GCC does not vectorise filling array of integers with a value on sse2`
### open_at : `2022-05-30T22:31:03Z`
### last_modified_date : `2022-05-31T00:41:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105780
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
The following code snippet I believe should be auto-vectorised even on sse2

```
void fill(int* f, int* l, int v) {
    while (f != l) {
        *f++ = v;
    }
}
```

Clang does it: https://godbolt.org/z/3bEeE8E8n


---


### compiler : `gcc`
### title : `GCC does not unroll auto-vectorized loops.`
### open_at : `2022-05-30T22:37:54Z`
### last_modified_date : `2022-06-01T11:49:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105781
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
Even when vectorizing very simple loops, gcc uses unroll factor of 1.
Clang unrolls 8, for context.

Example: 

void double_elements(int* f, int* l, int v) {
    while (f != l) {
        *f = *f + *f;
        ++f;
    }
}


https://godbolt.org/z/hTx84essY

In many measurements unrolling such code by a factor of 4 is beneficial.


---


### compiler : `gcc`
### title : `emission of inefficient movxtod/movdtox with -mvis3`
### open_at : `2022-05-30T23:22:51Z`
### last_modified_date : `2022-10-14T22:41:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105782
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
Created attachment 53055
The problematic function, adapted for standalone compilation

Hello, I found out that the blake2b implementation in monocypher runs much slower on a SPARC T4 when compiled with `-O3 -mvis3`, as opposed to plain `-O3`:

With plain -O3:  Blake2b : 184 megabytes  per second
With -O3 -mvis3: Blake2b : 118 megabytes  per second

(Results are from monocypher's `make speed` benchmark)

Looking at the generated assembly, it seems that when the code is compiled with -mvis3, GCC emits a lot of questionable `movxtod`/`movdtox` instructions?

I'm using sparc64-linux-gnu-gcc (GCC) 12.1.0.


---


### compiler : `gcc`
### title : `Missed vectorisation with conditional-select inside loop`
### open_at : `2022-05-31T14:55:20Z`
### last_modified_date : `2022-10-24T16:40:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105793
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
The code:
#define N 1024

float f(const float in[N], unsigned int n) {
    float a = 0.0f;

    for (unsigned  i = 0; i < N; ++i) {
        float b = in[i];
        if (b < 10.f)
            a += b;
        else
            a -= b;
    }

    return a;
}

with -Ofast does not vectorise (on aarch64, for example):
f:
        movi    v0.2s, #0
        add     x1, x0, 4096
        fmov    s3, 1.0e+1
.L5:
        ldr     s1, [x0], 4
        fsub    s2, s0, s1
        fcmpe   s1, s3
        fadd    s0, s0, s1
        fcsel   s0, s0, s2, mi
        cmp     x1, x0
        bne     .L5
        ret

whereas clang can and does. Commenting out the "else a -=b;" line allows GCC to vectorise it:
f:
        movi    v0.4s, 0
        add     x1, x0, 4096
        fmov    v3.4s, 1.0e+1
.L2:
        ldr     q2, [x0], 16
        fcmgt   v1.4s, v3.4s, v2.4s
        and     v1.16b, v1.16b, v2.16b
        fadd    v0.4s, v0.4s, v1.4s
        cmp     x1, x0
        bne     .L2
        faddp   v0.4s, v0.4s, v0.4s
        faddp   v0.4s, v0.4s, v0.4s
        ret

Examples at https://gcc.godbolt.org/z/qbn6T73qE


---


### compiler : `gcc`
### title : `Miss optimization to simplify (v + A) * B + C -> B * v + ABC in rtl.`
### open_at : `2022-06-01T05:39:38Z`
### last_modified_date : `2022-06-02T06:47:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105799
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
This case is from pr53533

int foo (int a)
{
  int tmp = a + 12345;
  tmp *= 914237;
  tmp += 12332;
  tmp *= 914237;
  tmp += 12332;
  tmp *= 914237;
  tmp -= 13;
  tmp *= 8000;
  return tmp;
}

with -O2, gcc generates

        add     edi, 12345
        imul    edi, edi, -1564285888
        lea     eax, [rdi-333519936]

llvm generates:

        imul    eax, edi, -1564285888
        add     eax, -1269844480

It looks like rtl simplifies (v * A + B ) * C -> AC * v + BC
but not (v + A) * B + C -> B * v + ABC.


---


### compiler : `gcc`
### title : `Missed CCP with -ftrivial-auto-var-init=zero`
### open_at : `2022-06-01T10:24:19Z`
### last_modified_date : `2022-12-13T14:18:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105801
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.1`
### severity : `normal`
### contents :
int foo (int flag)
{
  int i;
  if (flag)
    i = 1;
  return i;
}

CCP used to optimize this to 'return 1;' but with -ftrivial-auto-var-init=zero
only FRE does this now.


---


### compiler : `gcc`
### title : `[x86] Extend ix86_vectorize_vec_perm_const which now accept different input and output modes by r13-745`
### open_at : `2022-06-02T06:52:34Z`
### last_modified_date : `2022-06-02T07:49:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105815
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :



---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -O3 (trunk vs. 12.1.0)`
### open_at : `2022-06-03T09:59:46Z`
### last_modified_date : `2023-10-25T22:19:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105832
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
cat case.c #2
void foo(void);

static struct {
    short a;
    char b;
} c;

static char d;

int main() {
    char g = c.b > 4U ? c.b : c.b << 2;
    for (int h = 0; h < 5; h++) {
        d = (g >= 2 || 1 >> g) ? g : g << 1;
        if (d && 1 == g)
            foo();
        c.a = 0;
    }
}

`gcc-1982fe2692b6c3b7f969ffc4edac59f9d4359e91 (trunk) -O3` can not eliminate `foo` but `gcc-releases/gcc-12.1.0 -O3` can.

`gcc-1982fe2692b6c3b7f969ffc4edac59f9d4359e91 (trunk) -O3 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movsbl	c+2(%rip), %edx
	movl	%edx, %eax
	sall	$2, %edx
	cmpb	$5, %al
	cmovb	%edx, %eax
	cmpb	$1, %al
	jle	.L3
.L13:
	xorl	%eax, %eax
	movw	%ax, c(%rip)
	xorl	%eax, %eax
	ret
.L3:
	movsbl	%al, %edx
	addl	%edx, %edx
	testb	%al, %al
	cmove	%eax, %edx
	testb	%dl, %dl
	je	.L13
	subb	$1, %al
	jne	.L13
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$5, %ebx
.L6:
	call	foo
	movw	$0, c(%rip)
	subl	$1, %ebx
	jne	.L6
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.1.0 -O3 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	xorl	%eax, %eax
	movw	%ax, c(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=a1c9f779f75283427316b5c670c1e01ff8ce9ced


---


### compiler : `gcc`
### title : `[13 Regression] Dead Code Elimination Regression at -O2 (trunk vs. 12.1.0)`
### open_at : `2022-06-03T13:13:25Z`
### last_modified_date : `2023-02-21T16:27:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105833
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
cat case.c #4
void foo();

static int b;
static int **c;
static int ***d = &c;
static int ****e = &d;

static char(a)(char g, int h) { return h >= 2 ? g : g << h; }

int main() {
  **e == 0;
  b = b ? 4 : 0;
  if (!a(*e != 0, b))
    foo();
}

`gcc-1982fe2692b6c3b7f969ffc4edac59f9d4359e91 (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.1.0 -O2` can.

`gcc-1982fe2692b6c3b7f969ffc4edac59f9d4359e91 (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movl	b(%rip), %ecx
	xorl	%eax, %eax
	testl	%ecx, %ecx
	setne	%al
	sall	$2, %eax
	cmpq	$0, d(%rip)
	movl	%eax, b(%rip)
	je	.L8
	xorl	%eax, %eax
	ret
.L8:
	pushq	%rax
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.1.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movl	b(%rip), %edx
	xorl	%eax, %eax
	testl	%edx, %edx
	setne	%al
	sall	$2, %eax
	movl	%eax, b(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=b7501739f3b14ac7749aace93f636d021fd607f7


---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -O2 (trunk vs. 12.1.0)`
### open_at : `2022-06-03T13:25:44Z`
### last_modified_date : `2023-07-27T09:23:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105834
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
cat case.c
static int a, b;

void foo();

int main() {
    for (int c = 0; c < 2; c = c + (unsigned)3)
        if (a)
            for (;;)
                if (c > 0)
                    b = 0;
    if (b)
        foo();
}

`gcc-1982fe2692b6c3b7f969ffc4edac59f9d4359e91 (trunk) -O2` can not eliminate `foo` but `gcc-releases/gcc-12.1.0 -O2` can.

`gcc-1982fe2692b6c3b7f969ffc4edac59f9d4359e91 (trunk) -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movl	b(%rip), %ecx
	testl	%ecx, %ecx
	jne	.L8
	xorl	%eax, %eax
	ret
.L8:
	pushq	%rax
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------


`gcc-releases/gcc-12.1.0 -O2 -S -o /dev/stdout case.c`
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------


Bisects to: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=98e475a8f58ca3ba6e9bd5c9276efce4236f5d26


---


### compiler : `gcc`
### title : `missed optimization - vectorization -fsanitize=signed-integer-overflow`
### open_at : `2022-06-05T09:57:17Z`
### last_modified_date : `2022-06-06T08:48:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105855
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
It would be nice if -fsanitize=signed-integer-overflow
would impact optimization less, so it could be used
in production more often.


In the following example, using this flag prevents
vectorization:


void f(int i, float * restrict a, float * restrict b) 
{     
    for (int j = i; j < i + 4; ++j)
        a[j] = b[j] + 1.;
}


https://godbolt.org/z/raqdd794x


---


### compiler : `gcc`
### title : `storing nullptr_t to memory should not generate any instructions`
### open_at : `2022-06-06T17:16:02Z`
### last_modified_date : `2022-06-21T08:15:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105864
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.1.0`
### severity : `normal`
### contents :
Currently storing a nullptr_t to memory causes 0 to be written to that memory. As there is no way to read this value back without invoking undefined behavior I believe GCC can omit storing it.

This will make nullptr_t behave more similar to an empty struct that has only padding bytes in it.

using nullptr_t = decltype(nullptr);

void test(nullptr_t* p)
{
    *p = nullptr;
}

struct empty
{};

void test(empty* p)
{
    *p = empty();
}

test(decltype(nullptr)*):
        mov     QWORD PTR [rdi], 0
        ret
test(empty*):
        ret


---


### compiler : `gcc`
### title : `Toggling an atomic_bool is inefficient`
### open_at : `2022-06-07T15:10:30Z`
### last_modified_date : `2023-05-17T23:32:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105875
### status : `NEW`
### tags : `missed-optimization`
### component : `c`
### version : `12.1.0`
### severity : `normal`
### contents :
Consider this C code:

#include <stdatomic.h>

atomic_bool b;
atomic_char c;
_Bool b2;

void f1(void) {
    b ^= 1;
}

void f2(void) {
    c ^= 1;
}

void f3(void) {
    b2 ^= 1;
}

At -O3, those functions compile into this:

f1:
        movzbl  b(%rip), %eax
.L5:
        movb    %al, -1(%rsp)
        xorl    $1, %eax
        movl    %eax, %edx
        movzbl  -1(%rsp), %eax
        lock cmpxchgb   %dl, b(%rip)
        jne     .L5
        ret
f2:
        lock xorb       $1, c(%rip)
        ret
f3:
        xorb    $1, b2(%rip)
        ret

The code generated for f1 is inefficient. It should have just done a "lock xorb       $1, b(%rip)".


---


### compiler : `gcc`
### title : `Memcmp folded only when size is a power of two`
### open_at : `2022-06-08T07:24:58Z`
### last_modified_date : `2022-06-14T07:18:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105883
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Compiled with trunk and -O2, on x86-64, the testcase bellow shows, the calls to memcmp are folded just when the size is a power of two. Other archs seems to produce unoptimal code too.

If the arrays are declared const, everything is optimized as expected.


-------
template <int I, typename T>
bool equals() {
    T a[16] = {1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
    T b[16] = {1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
    static_assert(I <= sizeof a);
    return !__builtin_memcmp(a, b, I);
}

template <int I, typename T>
bool not_equals() {
    T a[16] = {1,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
    T b[16] = {0,2,3,4,5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
    static_assert(I <= sizeof a);
    return !__builtin_memcmp(a, b, I);
}

#define TEST \
template bool equals<1, char>(); \
template bool equals<2, char>(); \
template bool equals<3, char>(); \
template bool equals<4, char>(); \
template bool equals<5, char>(); \
template bool equals<7, char>(); \
template bool equals<8, char>(); \
template bool equals<15, char>(); \
template bool equals<16, char>(); \
template bool equals<1, long long>(); \
template bool equals<2, long long>(); \
template bool equals<3, long long>(); \
template bool equals<4, long long>(); \
template bool equals<5, long long>(); \
template bool equals<7, long long>(); \
template bool equals<8, long long>(); \
template bool equals<16, long long>(); \
template bool equals<17, long long>(); \
template bool equals<31, long long>(); \
template bool equals<32, long long>(); \
template bool equals<63, long long>(); \
template bool equals<64, long long>(); \
template bool equals<127, long long>(); \
template bool equals<128, long long>();

TEST

#define equals not_equals
TEST


---


### compiler : `gcc`
### title : `Missed optimization for __synth3way`
### open_at : `2022-06-08T23:43:20Z`
### last_modified_date : `2023-08-02T13:38:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105903
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Consider this example:

#include <compare>
#include <vector>

inline constexpr auto synth3way = std::__detail::__synth3way;

struct Iterator {
    std::vector<int>::iterator it;

    constexpr bool operator<(Iterator const& rhs) const {
        return it < rhs.it;
    }
    constexpr bool operator>(Iterator const& rhs) const {
        return it > rhs.it;
    }    
};

bool less(Iterator const& a, Iterator const& b) {
    return a < b;
}

bool less3way(Iterator const& a, Iterator const& b) {
    return synth3way(a, b) < 0;
}

Here, synth3way(a, b) < 0 produces identical code to a < b (compiling with gcc 12.1 --std=c++20 -O3), which is great. The compiler recognizes that it doesn't have to do the second synthesized comparison. 

However, if we instead compared (sorry):

bool greater(Iterator const& a, Iterator const& b) {
    return a > b;
}

bool greater3way(Iterator const& a, Iterator const& b) {
    return synth3way(a, b) > 0;
}

This is now much worse:

greater(Iterator const&, Iterator const&):
        mov     rax, QWORD PTR [rdi]
        cmp     QWORD PTR [rsi], rax
        setb    al
        ret
greater3way(Iterator const&, Iterator const&):
        mov     rdx, QWORD PTR [rdi]
        mov     rcx, QWORD PTR [rsi]
        xor     eax, eax
        cmp     rdx, rcx
        jb      .L3
        cmp     rcx, rdx
        setb    al
.L3:
        ret

Interestingly, if we write this out:

bool greater3way(Iterator const& a, Iterator const& b) {
    auto const cmp = [&]{
        if (a < b) return std::weak_ordering::less;
        if (b < a) return std::weak_ordering::greater;
        return std::weak_ordering::equivalent;
    }();
    return cmp > 0;
}

bool greater3way_fold(Iterator const& a, Iterator const& b) {
    return [&]{
        if (a < b) return false;
        if (b < a) return true;
        return false;
    }();
}

The greater3way_fold implementation generates the same code as >, which definitely suggests to me that the greater3way version is simply a missed optimization. 

On compiler explorer: https://godbolt.org/z/1xvfsMrnf

Because synth3way is only used in contexts that require a weak ordering anyway, it would be a valid optimization to replace synth3way(a, b) > 0 with b < a, synth3way(a, b) <= 0 with !(b < a) and synth3way(a, b) >= 0 with !(a < b). These replacements aren't generally true (because partial orders), but the precondition on this type is that we have a weak order, so we should be able to do better.


---


### compiler : `gcc`
### title : `Predicated mov r0, #1 with opposite conditions could be hoisted, between 1 and 1<<n in opposite sides of a branch`
### open_at : `2022-06-09T07:33:26Z`
### last_modified_date : `2023-05-17T20:48:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105904
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <bit>  // using the libstdc++ header
unsigned roundup(unsigned x){
    return std::bit_ceil(x);
}

https://godbolt.org/z/Px1fvWaex

GCC's version is somewhat clunky, including MOV r0, #1 in either "side":

roundup(unsigned int):
        cmp     r0, #1
        itttt   hi
        addhi   r3, r0, #-1
        movhi   r0, #1            @@ here
        clzhi   r3, r3
        rsbhi   r3, r3, #32
        ite     hi
        lslhi   r0, r0, r3
        movls   r0, #1            @@ here
        bx      lr

Even without spotting the other optimizations that clang finds, we can combine to a single unconditional MOV r0, #1.  But only if we avoid setting flags, so it requires a 4-byte encoding, not MOVS.  Still, it's one fewer instruction to execute.

This is not totally trivial: it requires seeing that we can move it across the conditional LSL.  So it's really a matter of folding the 1s between 1<<n and 1  in opposite sides of an if-converted branch.

        cmp     r0, #1
        ittt    hi
        addhi   r3, r0, #-1
        clzhi   r3, r3
        rsbhi   r3, r3, #32
        mov     r0, #1            @@ now unconditional
        it      hi
        lslhi   r0, r0, r3
        bx      lr



clang makes rather nice asm for ARMv7 -mcpu=cortex-a53 as discussed in PR104773 which covers a different missed optimization in the same asm.

roundup(unsigned int):                @@ clang's version.
        subs    r0, r0, #1
        clz     r0, r0
        rsb     r1, r0, #32         @ 32-clz
        mov     r0, #1
        lslhi   r0, r0, r1          @ using flags set by SUBS
        bx      lr                  @ 1<<(32-clz) or just 1

Folding the mov r0, #1 from either side is only a couple steps away from making the clz and rsb unconditional, and keeping only the LSL conditional.


---


### compiler : `gcc`
### title : `[AArch64] 64-bit constants with same high/low halves can use ADD lsl 32 (-Os at least)`
### open_at : `2022-06-11T20:19:57Z`
### last_modified_date : `2023-09-18T12:33:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105928
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
void foo(unsigned long *p) {
    *p = 0xdeadbeefdeadbeef;
}

cleverly compiles to https://godbolt.org/z/b3oqao5Kz

        mov     w1, 48879
        movk    w1, 0xdead, lsl 16
        stp     w1, w1, [x0]
        ret

But producing the value in a register uses more than 3 instructions:

unsigned long constant(){
    return 0xdeadbeefdeadbeef;
}

        mov     x0, 48879
        movk    x0, 0xdead, lsl 16
        movk    x0, 0xbeef, lsl 32
        movk    x0, 0xdead, lsl 48
        ret

At least with -Os, and maybe at -O2 or -O3 if it's efficient, we could be doing a shifted ADD or ORR to broadcast a zero-extended 32-bit value to 64-bit.

        mov     x0, 48879
        movk    x0, 0xdead, lsl 16
        add     x0, x0, x0, lsl 32

Some CPUs may fuse sequences of movk, and shifted operands for ALU ops may take extra time in some CPUs, so this might not actually be optimal for performance, but it is smaller for -Os and -Oz.

We should also be using that trick for stores to _Atomic or volatile long*, where we currently do MOV + 3x MOVK, then an STR, with ARMv8.4-a which guarantees atomicity.


---

ARMv8.4-a and later guarantees atomicity for ldp/stp within an aligned 16-byte chunk, so we should use MOV/MOVK / STP there even for volatile or __ATOMIC_RELAXED.  But presumably that's a different part of GCC's internals, so I'll report that separately.


---


### compiler : `gcc`
### title : `[AArch64] armv8.4-a allows atomic stp. 64-bit constants can use 2 32-bit halves with _Atomic or volatile`
### open_at : `2022-06-11T20:21:33Z`
### last_modified_date : `2022-11-05T08:53:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105929
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
void foo(unsigned long *p) {
    *p = 0xdeadbeefdeadbeef;
}
// compiles nicely:  https://godbolt.org/z/8zf8ns14K
        mov     w1, 48879
        movk    w1, 0xdead, lsl 16
        stp     w1, w1, [x0]
        ret

But even with -Os -march=armv8.4-a   the following doesn't:
void foo_atomic(_Atomic unsigned long *p) {
    __atomic_store_n(p, 0xdeadbeefdeadbeef, __ATOMIC_RELAXED);
}

        mov     x1, 48879
        movk    x1, 0xdead, lsl 16
        movk    x1, 0xbeef, lsl 32
        movk    x1, 0xdead, lsl 48
        stlr    x1, [x0]
        ret

ARMv8.4-a and later guarantees atomicity for aligned ldp/stp, according to ARM's architecture reference manual: ARM DDI 0487H.a - ID020222, so we could use the same asm as the non-atomic version.

> If FEAT_LSE2 is implemented, LDP, LDNP, and STP instructions that access fewer than 16 bytes are single-copy atomic when all of the following conditions are true:
> • All bytes being accessed are within a 16-byte quantity aligned to 16 bytes.
> • Accesses are to Inner Write-Back, Outer Write-Back Normal cacheable memory

(FEAT_LSE2 is the same CPU feature that gives 128-bit atomicity for aligned ldp/stp x,x,mem)

Prior to that, apparently it wasn't guaranteed that stp of 32-bit halves merged into a single 64-bit store. So without -march=armv8.4-a it wasn't a missed optimization to construct the constant in a single register for _Atomic or volatile.

But with ARMv8.4, we should use MOV/MOVK + STP.

Since there doesn't seem to be a release-store version of STP, 64-bit release and seq_cst stores should still generate the full constant in a register, instead of using STP + barriers.


(Without ARMv8.4-a, or with a memory-order other than relaxed, see PR105928 for generating 64-bit constants in 3 instructions instead of 4, at least for -Os, with add x0, x0, x0, lsl 32)


---


### compiler : `gcc`
### title : `[12/13 Regression] Excessive stack spill generation on 32-bit x86`
### open_at : `2022-06-11T20:51:44Z`
### last_modified_date : `2022-08-19T18:29:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105930
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.1`
### severity : `normal`
### contents :
Created attachment 53121
Test-case extracted from the generic blake2b kernel code

Gcc-12 seems to generate a huge number of stack spills on this blake2b test-case, to the point where it overflows the allowable kernel stack on 32-bit x86.

This crypto thing has two 128-byte buffers, so a stack frame a bit larger than 256 is expected when the dataset doesn't fit in the register set.

Just as an example, on this code, clang-.14.0.0 generates a stack frame that is 296 bytes. 

In contrast, gcc-12.1.1 generates a stack frame that is almost an order of magnitude(!) larger, at 2620 bytes.

The trivial Makefile I used for this test-case is

   # The kernel cannot just randomly use FP/MMX/AVX
    CFLAGS := -mno-sse -mno-mmx -mno-sse2 -mno-3dnow -mno-avx
    CFLAGS += -m32
    CFLAGS += -O2

    test:
        gcc $(CFLAGS) -Wall -S blake2b.c
        grep "sub.*%[er]sp" blake2b.s

to easily test different flags and the end result, but as can be seen from above, it really doesn't need any special flags except the ones that disable MMX/AVX code generation.

And the generated code looks perfectly regular, except for the fact that it uses almost 3kB of stack space.

Note that "-m32" is required to trigger this - the 64-bit case does much better, presumably because it has more registers and this needs fewer spills. It gets worse with some added debug flags we use in the kernel, but not that kind of "order of magnitude" worse.

Using -O1 or -Os makes no real difference.

This is presumably due to some newly triggered optimization in gcc-12, but I can't even begin to guess at what we'd need to disable (or enable) to avoid this horrendous stack growth. Some very aggressive instruction scheduling thing that spreads out all the calculations and always wants to spill-and-reload the subepxressions that it CSE'd? I dunno. 

Pls advice. The excessive stack literally causes build failures due to us using -Werror-frame-larger-than= to make sure stack use remains sanely bounded. The kernel stack is a rather limited resource.


---


### compiler : `gcc`
### title : `[12 Regression] maybe-uninitialized with std::optional`
### open_at : `2022-06-12T22:12:38Z`
### last_modified_date : `2023-07-11T16:27:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105937
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, needs-reduction`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
Created attachment 53125
Test case extracted from original code

Commit https://gcc.gnu.org/g:6feb628a706e86eb3f303aff388c74bdb29e7381 "Improve warning suppression for inlined functions [PR98512]. " introduced a regression when using std::optional.

Ever since that commit, when building optimized code compiler complains:

/home/mijn/workspace/openrct2/src/openrct2/drawing/Drawing.String.cpp: In function ‘void ttf_process_string_literal(OpenRCT2::IContext*, std::string_view)’:
/home/mijn/workspace/openrct2/src/openrct2/drawing/Drawing.String.cpp:215:62: error: ‘*(long unsigned int*)((char*)&ttfRunIndex + offsetof(std::optional<long unsigned int>,std::optional<long unsigned int>::<unnamed>.std::_Optional_base<long unsigned int, true, true>::<unnamed>))’ may be used uninitialized in this function [-Werror=maybe-uninitialized]
  215 |                 auto len = it.GetIndex() - ttfRunIndex.value();
      |                                                              ^


gcc -v:

Using built-in specs.
COLLECT_GCC=/home/mijn/workspace/gcc-install-master/bin/c++
COLLECT_LTO_WRAPPER=/home/mijn/workspace/gcc-install-master/libexec/gcc/x86_64-pc-linux-gnu/12.0.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc/configure --prefix=/home/mijn/workspace/gcc-install-master --enable-languages=c++
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20210702 (experimental) (GCC)



Provided an extracted test case for the issue mentioned.
Original code: https://github.com/OpenRCT2/OpenRCT2/blob/b0ffa9d28e62b862f68a37788931265cb4528835/src/openrct2/drawing/Drawing.String.cpp#L839
Original bug report for OpenRCT2: https://github.com/OpenRCT2/OpenRCT2/issues/17371
Test case on godbolt: https://godbolt.org/z/6Y3cn1Ks1

c++ -O2 -c -o foo test.cpp -Werror=maybe-uninitialized


---


### compiler : `gcc`
### title : `[11 Regression] x86: single-element vectors don't have scalar FMA insns used anymore`
### open_at : `2022-06-14T06:31:26Z`
### last_modified_date : `2023-07-07T10:43:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105965
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
While this used to work fine up to gcc8, gcc9 and newer use vmuls[sdh]+vadds[sdh] instead. No similar issue exists when operating on scalars, or when operating on multi-element vectors not matching any available register size (so my guess of "target" as the component may not be correct).

This has regressed the test harness of the Xen Project's insn emulator [1], which no longer exercises any scalar FMA insns because of the compiler not emitting any. (Note that using intrinsics is not really an option, as the primary goal is to test insns with memory operands. Yet the intrinsics don't lend themselves to such because of using 128-bit parameter types.)

The issue is uniform for FMA, FMA4, AVX512F, and AVX512-FP16. It can be easily seen by compiling

T test(T x, T y, T z) {
	return x * y + z;
}

#define TEST(n) \
typedef T __attribute__((vector_size(n * sizeof(T)))) v##n##_t; \
v##n##_t test##n(v##n##_t x, v##n##_t y, v##n##_t z) { \
	return x * y + z; \
}

TEST(1)
TEST(2)
TEST(4)
TEST(8)
TEST(16)
TEST(32)
TEST(64)

with e.g. "-mfpmath=sse -O3 -c -mfma -DT=float", but obvious other option combinations similarly demonstrate the issue.

[1] https://xenbits.xen.org/gitweb/?p=xen.git;a=tree;f=tools/tests/x86_emulator


---


### compiler : `gcc`
### title : `x86: operations on certain few-element vectors yield very inefficient code`
### open_at : `2022-06-14T06:56:32Z`
### last_modified_date : `2023-07-22T03:09:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105966
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
Respective operations on vectors with more than one element but less than enough elements to fill minimal available register width are decomposed into scalar FMA insns. While this may be on-par for small element counts, it certainly generates absurd code for e.g. AVX512-FP16 with, say, 128- or 256-bit vectors but AVX512VL not enabled. This would be far more efficient by zero-extending the vectors to 512 bits (to avoid exceptions on the unused elements), emitting the FMA insn on %zmm registers, and then using just the low part of the result. (The same likely applies to e.g. plain addition, subtraction, and multiplication.)

If necessary the example code from bug 105965 can be re-used to easily see the odd behavior.


---


### compiler : `gcc`
### title : `Wrong branch prediction for if (COND) { if(x) noreturn1(); else noreturn2(); }`
### open_at : `2022-06-14T11:46:47Z`
### last_modified_date : `2023-03-16T14:04:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105973
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Given this code:

__attribute__((noreturn)) void throw1();
__attribute__((noreturn)) void throw2();

typedef decltype(sizeof(0)) size_t;

#if defined LIKELY
# define PREDICT(C) __builtin_expect(C,1)
#elif defined UNLIKELY
# define PREDICT(C) __builtin_expect(C,0)
#else
# define PREDICT(C) (C)
#endif

template<typename T>
T* allocate(size_t n)
{
  if (PREDICT(n > (__PTRDIFF_MAX__ / sizeof(T))))
  {
    if (n > (__SIZE_MAX__ / sizeof(T)))
      throw1();
    throw2();
  }

  return (T*) ::operator new(n * sizeof(T));
}

int* alloc_int(size_t n)
{
  return allocate<int>(n);
}


The condition decorated with PREDICT is compiled to different code with -DLIKELY and -DUNLIKELY, as expected.

However with neither macro defined, the result is the same as -DLIKELY (for any optimization level > -O0). i.e. the calls to throw1 and throw1 come first and the return statement requires a branch:

_Z9alloc_intm:
.LFB1:
	.cfi_startproc
	movq	%rdi, %rax
	shrq	$61, %rax
	je	.L2
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	shrq	$62, %rdi
	je	.L3
	call	_Z6throw1v
	.p2align 4,,10
	.p2align 3
.L3:
	call	_Z6throw2v
	.p2align 4,,10
	.p2align 3
.L2:
	.cfi_def_cfa_offset 8
	salq	$2, %rdi
	jmp	_Znwm
	.cfi_endproc


Surely this is wrong?

If calling a noreturn function is considered unlikely, then surely entering a block that always calls a noreturn function should also be unlikely?

Clang gets this right, generating the same code as UNLIKELY by default, and only requiring a branch for the return value when LIKELY is defined.


This code is reduced from std::allocator in libstdc++ and I thought I should be able to remove a redundant __builtin_expect, but it's needed due to this.


---


### compiler : `gcc`
### title : `Failure to optimize (b != 0) && (a >= b) as well as the same pattern with binary and`
### open_at : `2022-06-14T21:37:26Z`
### last_modified_date : `2023-09-21T12:35:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105983
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
bool f(unsigned a, unsigned b)
{
    return (b != 0) && (a >= b);
}

This can be optimized to `return (b != 0) & (a >= b);`, which is itself optimized to `return (b - 1) > a;`. GCC outputs code equivalent to `return (b != 0) & (a >= b);` (at least on x86) whereas if that code is compiled it would output `return (b - 1) > a;`, while LLVM has no trouble directly outputting the optimal code.


---


### compiler : `gcc`
### title : `Coroutine frame space for temporaries in a co_await expression is not reused`
### open_at : `2022-06-15T09:03:18Z`
### last_modified_date : `2022-08-26T09:08:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105989
### status : `UNCONFIRMED`
### tags : `C++-coroutines, missed-optimization`
### component : `c++`
### version : `12.1.0`
### severity : `enhancement`
### contents :
Created attachment 53142
Minimal reproduction of the described issue.

Space in a coroutine frame that is allocated for temporaries in a co_await expression (in particular the awaiter object) is not reused for subsequent co_await-s. Writing the same co_await expression multiple times results in a larger coroutine frame than using a for loop, where the space is only allocated once. This is a bummer for me, as I would like to use coroutines for embedded systems with limited memory, and every byte counts.

I would expect that the space in the coroutine frame for the temporaries of two subsequent co_await expressions would overlap. This appears to work on Clang (allthough it has a similar issue with HALO, where the space for the subroutine frames do not overlap).

The attached minimal example shows the issue by logging allocations of the coroutine frames; change the number of co_await expressions in coro_1() and the size of the dummy array in the awaiter object to see that the space for the awaiter is not reclaimed. Change it to 1 co_await in a for loop, and the temporaries are only allocated once.

Compilation command (the issue is on all optimization levels, not just -O3):
g++ -std=c++20 -fcoroutines -O3 --save-temps report.cpp

gcc -v output:
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-pc-linux-gnu/12.1.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: /build/gcc/src/gcc/configure --enable-languages=c,c++,ada,fortran,go,lto,objc,obj-c++ --enable-bootstrap --prefix=/usr --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --with-linker-hash-style=gnu --with-system-zlib --enable-__cxa_atexit --enable-cet=auto --enable-checking=release --enable-clocale=gnu --enable-default-pie --enable-default-ssp --enable-gnu-indirect-function --enable-gnu-unique-object --enable-linker-build-id --enable-lto --enable-multilib --enable-plugin --enable-shared --enable-threads=posix --disable-libssp --disable-libstdcxx-pch --disable-werror --with-build-config=bootstrap-lto --enable-link-serialization=1
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.1.0 (GCC)


---


### compiler : `gcc`
### title : `Miss vectorization for complex type copy.`
### open_at : `2022-06-17T04:37:50Z`
### last_modified_date : `2022-07-22T03:23:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106010
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
This is from PR105923

void
foo (_Complex double *p, _Complex double* q)
{
    for (int i = 0; i != 100000; i++)
      p[i] = q[i];
}

gcc generates

foo(double _Complex*, double _Complex*):
        xor     eax, eax
.L2:
        vmovsd  xmm1, QWORD PTR [rsi+rax]
        vmovsd  xmm0, QWORD PTR [rsi+8+rax]
        vmovsd  QWORD PTR [rdi+rax], xmm1
        vmovsd  QWORD PTR [rdi+8+rax], xmm0
        add     rax, 16
        cmp     rax, 1600000
        jne     .L2
        ret

llvm generates:

foo(double _Complex*, double _Complex*):                           # @foo(double _Complex*, double _Complex*)
        xor     eax, eax
.LBB0_1:                                # =>This Inner Loop Header: Depth=1
        movups  xmm0, xmmword ptr [rsi + rax]
        movups  xmmword ptr [rdi + rax], xmm0
        add     rax, 16
        cmp     rax, 1600000
        jne     .LBB0_1
        ret

vectorizer failed because get_related_vectype_for_scalar_type failed for complex type.


---


### compiler : `gcc`
### title : `Surprising SLP failure on trivial code`
### open_at : `2022-06-17T17:51:00Z`
### last_modified_date : `2022-06-22T10:29:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106019
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
In the following code, 'f' is not SLP-vectorized, but 'g' is. From a brief look at slp2 dump, looks like dependence analysis for p[i] vs. p[i+1] fails?

void f(double *p, long i)
{
    p[i+0] += 1;
    p[i+1] += 1;
}
void g(double *p, long i)
{
    double *q = p + i;
    q[0] += 1;
    q[1] += 1;
}


---


### compiler : `gcc`
### title : `[12 Regression] Enable vectorizer generates extra load`
### open_at : `2022-06-17T21:59:49Z`
### last_modified_date : `2023-05-08T12:24:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106022
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
[hjl@gnu-clx-1 gcc-bisect]$ cat x.c
void foo(char * c) {
    c[0] = 0;
    c[1] = 1;
    c[2] = 2;
    c[3] = 3;
}
[hjl@gnu-clx-1 gcc-bisect]$ gcc -O3 x.c -S
[hjl@gnu-clx-1 gcc-bisect]$ cat x.s 
	.file	"x.c"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movl	.LC0(%rip), %eax
	movl	%eax, (%rdi)
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.section	.rodata.cst4,"aM",@progbits,4
	.align 4
.LC0:
	.byte	0
	.byte	1
	.byte	2
	.byte	3
	.ident	"GCC: (GNU) 12.1.1 20220507 (Red Hat 12.1.1-1)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-clx-1 gcc-bisect]$ gcc -O3 x.c -S -fno-tree-vectorize
[hjl@gnu-clx-1 gcc-bisect]$ cat x.s
	.file	"x.c"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movl	$50462976, (%rdi)
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 12.1.1 20220507 (Red Hat 12.1.1-1)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-clx-1 gcc-bisect]$ /usr/gcc-11.2.1-x32/bin/gcc -S -O3 x.c 
[hjl@gnu-clx-1 gcc-bisect]$ cat x.s
	.file	"x.c"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movl	$50462976, (%rdi)
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 11.2.1 20220118"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-clx-1 gcc-bisect]$


---


### compiler : `gcc`
### title : `x86_64 vectorization of ALU ops using xmm registers prematurely`
### open_at : `2022-06-20T23:49:52Z`
### last_modified_date : `2022-07-22T01:39:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106038
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
See: https://godbolt.org/z/YxWEn6Y65

Basically in all cases where the total amount of memory touched is <= 8 bytes (word size) the vectorization pass is choosing to inefficiently use xmm registers to vectorize the unrolled loops. 

GPRs (as GCC <= 9.5 was doing) is faster / less code size.


Related to: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106022


---


### compiler : `gcc`
### title : `Inefficient constant broadcast on x86_64`
### open_at : `2022-06-23T01:59:15Z`
### last_modified_date : `2023-05-17T23:20:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106060
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
```
#include <immintrin.h>

__m256i
shouldnt_have_movabs ()
{
  return _mm256_set1_epi8 (123);
}

__m256i
should_be_cmpeq_abs ()
{
  return _mm256_set1_epi8 (1);
}

__m256i
should_be_cmpeq_add ()
{
  return _mm256_set1_epi8 (-2);
}
```

Compiled with: '-O3 -march=x86-64-v3'

Results in:
```
Disassembly of section .text:

0000000000000000 <shouldnt_have_movabs>:
   0:   48 b8 7b 7b 7b 7b 7b    movabs $0x7b7b7b7b7b7b7b7b,%rax
   7:   7b 7b 7b
   a:   c4 e1 f9 6e c8          vmovq  %rax,%xmm1
   f:   c4 e2 7d 59 c1          vpbroadcastq %xmm1,%ymm0
  14:   c3                      retq
  15:   66 66 2e 0f 1f 84 00    data16 nopw %cs:0x0(%rax,%rax,1)
  1c:   00 00 00 00

0000000000000020 <should_be_cmpeq_abs>:
  20:   48 b8 01 01 01 01 01    movabs $0x101010101010101,%rax
  27:   01 01 01
  2a:   c4 e1 f9 6e c8          vmovq  %rax,%xmm1
  2f:   c4 e2 7d 59 c1          vpbroadcastq %xmm1,%ymm0
  34:   c3                      retq
  35:   66 66 2e 0f 1f 84 00    data16 nopw %cs:0x0(%rax,%rax,1)
  3c:   00 00 00 00

0000000000000040 <should_be_cmpeq_add>:
  40:   48 b8 fe fe fe fe fe    movabs $0xfefefefefefefefe,%rax
  47:   fe fe fe
  4a:   c4 e1 f9 6e c8          vmovq  %rax,%xmm1
  4f:   c4 e2 7d 59 c1          vpbroadcastq %xmm1,%ymm0
  54:   c3                      retq
```

Compiled with: '-O3 -march=x86-64-v4'

Results in:
```
0000000000000000 <shouldnt_have_movabs>:
   0:   48 b8 7b 7b 7b 7b 7b    movabs $0x7b7b7b7b7b7b7b7b,%rax
   7:   7b 7b 7b
   a:   62 f2 fd 28 7c c0       vpbroadcastq %rax,%ymm0
  10:   c3                      retq
  11:   66 66 2e 0f 1f 84 00    data16 nopw %cs:0x0(%rax,%rax,1)
  18:   00 00 00 00
  1c:   0f 1f 40 00             nopl   0x0(%rax)

0000000000000020 <should_be_cmpeq_abs>:
  20:   48 b8 01 01 01 01 01    movabs $0x101010101010101,%rax
  27:   01 01 01
  2a:   62 f2 fd 28 7c c0       vpbroadcastq %rax,%ymm0
  30:   c3                      retq
  31:   66 66 2e 0f 1f 84 00    data16 nopw %cs:0x0(%rax,%rax,1)
  38:   00 00 00 00
  3c:   0f 1f 40 00             nopl   0x0(%rax)

0000000000000040 <should_be_cmpeq_add>:
  40:   48 b8 fe fe fe fe fe    movabs $0xfefefefefefefefe,%rax
  47:   fe fe fe
  4a:   62 f2 fd 28 7c c0       vpbroadcastq %rax,%ymm0
  50:   c3                      retq
```


All functions / targets are inoptimal.

Generating 1/2 can be done without any lane-cross broadcast.

Generating constants like 123 shouldn't first be constant broadcast
into an imm64. That makes it require an 10-byte `movabs` and wastes
spaces.


---


### compiler : `gcc`
### title : `Sub-optimal code is generated for checking bitfields via proxy functions`
### open_at : `2022-06-24T10:02:49Z`
### last_modified_date : `2023-07-19T04:10:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106076
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Consider the following struct:

#include <cstdint>
    
struct SomeClass {
    uint16_t         dummy1 : 1;
    uint16_t         cfg2 : 1;
    uint16_t         cfg3 : 1;
    uint16_t         dummy2 : 1;
    uint16_t         dummy3 : 1;
    uint16_t         dummy4 : 1;
    uint16_t         cfg1 : 1;
    uint16_t         dummy5 : 1;
    uint16_t         cfg4 : 1;

    constexpr bool checkA() const { return cfg1 || cfg2 || cfg3; }
    constexpr bool checkB() const { return cfg4; }

    constexpr bool checkA_B() const { return (cfg1 || cfg2 || cfg3) || cfg4; }
    constexpr bool checkA_B_SLOW() const { return checkA() || checkB(); }
};


For the following functions (which do the same thing) GCC generates different assembly.

bool check(const SomeClass& rt) {
    return rt.checkA_B();
}

bool check_SLOW(const SomeClass& rt) {
    return rt.checkA_B_SLOW();
}

Compiled as:

g++ -std=c++17 -S 

The assembly:

; demangled: check(SomeClass const&)
_Z5checkRK9SomeClass:
        endbr64
        testw   $326, (%rdi)
        setne   %al
        ret

; demangled: check_SLOW(SomeClass const&)
_Z10check_SLOWRK9SomeClass:
        endbr64
        movzwl  (%rdi), %edx
        movl    $1, %eax
        testb   $70, %dl
        jne     .L3
        movzbl  %dh, %eax
        andl    $1, %eax
.L3:
        ret

As we can see, during check_SLOW GCC decided to check the result on byte-by-byte basis introducing a conditional jump in between. It looks like GCC did not fully analyse the code after inlining checkA() and checkB().

FYI, the same code on Clang produces the 1st option of ASM for both functions.


---


### compiler : `gcc`
### title : `missed vectorization`
### open_at : `2022-06-24T15:56:40Z`
### last_modified_date : `2023-07-26T13:28:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106081
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
This testcase (derived from ImageMagick)

struct pixels
{
        short a,b,c,d;
} *pixels;
struct dpixels
{
        double a,b,c,d;
};

double
test(double *k)
{
        struct dpixels results={};
        for (int u=0; u<10000;u++,k--)
        {
                results.a += *k*pixels[u].a;
                results.b += *k*pixels[u].b;
                results.c += *k*pixels[u].c;
                results.d += *k*pixels[u].d;
        }
        return results.a+results.b*2+results.c*3+results.d*4;
}

gets vectorized by clang:
test:                                   # @test
        .cfi_startproc
# %bb.0:
        movq    pixels(%rip), %rax
        vxorpd  %xmm0, %xmm0, %xmm0
        xorl    %ecx, %ecx
        .p2align        4, 0x90
.LBB0_1:                                # =>This Inner Loop Header: Depth=1
        vpmovsxwd       (%rax), %xmm1
        vbroadcastsd    (%rdi,%rcx,8), %ymm2
        addq    $8, %rax
        decq    %rcx
        vcvtdq2pd       %xmm1, %ymm1
        vfmadd231pd     %ymm2, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm2) + ymm0
        cmpq    $-10000, %rcx                   # imm = 0xD8F0
        jne     .LBB0_1
# %bb.2:
        vpermilpd       $1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
        vfmadd132sd     .LCPI0_0(%rip), %xmm0, %xmm1 # xmm1 = (xmm1 * mem) + xmm0
        vextractf128    $1, %ymm0, %xmm0
        vfmadd231sd     .LCPI0_1(%rip), %xmm0, %xmm1 # xmm1 = (xmm0 * mem) + xmm1
        vpermilpd       $1, %xmm0, %xmm0        # xmm0 = xmm0[1,0]
        vfmadd132sd     .LCPI0_2(%rip), %xmm1, %xmm0 # xmm0 = (xmm0 * mem) + xmm1
        vzeroupper
        retq

but not by GCC.
Original loop is:
    0.94 :   423cb0: vmovdqu (%rsi,%rdi,8),%xmm5 // morphology.c:2984
         : 2983   if ( IsNaN(*k) ) continue;
    0.29 :   423cb5: vpermilpd $0x1,(%rcx),%xmm4
         : 2982   for (u=0; u < (ssize_t) kernel->width; u++, k--) {
    0.46 :   423cbb: add    $0x2,%rdi
    0.07 :   423cbf: add    $0xfffffffffffffff0,%rcx
         : 2984   result.red     += (*k)*k_pixels[u].red;
    0.03 :   423cc3: vpshufb %xmm12,%xmm5,%xmm6
    6.81 :   423cc8: vcvtdq2pd %xmm6,%xmm6
   13.05 :   423ccc: vfmadd231pd %xmm6,%xmm4,%xmm1
         : 2985   result.green   += (*k)*k_pixels[u].green;
   17.45 :   423cd1: vpshufb %xmm15,%xmm5,%xmm6 // morphology.c:2985
    0.33 :   423cd6: vcvtdq2pd %xmm6,%xmm6
    0.00 :   423cda: vfmadd231pd %xmm6,%xmm4,%xmm3
         : 2986   result.blue    += (*k)*k_pixels[u].blue;
   15.28 :   423cdf: vpshufb %xmm13,%xmm5,%xmm6 // morphology.c:2986
         : 2987   result.opacity += (*k)*k_pixels[u].opacity;
    0.00 :   423ce4: vpshufb %xmm8,%xmm5,%xmm5
         : 2986   result.blue    += (*k)*k_pixels[u].blue;
    0.00 :   423ce9: vcvtdq2pd %xmm6,%xmm6
         : 2987   result.opacity += (*k)*k_pixels[u].opacity;
    0.21 :   423ced: vcvtdq2pd %xmm5,%xmm5
         : 2986   result.blue    += (*k)*k_pixels[u].blue;
    0.97 :   423cf1: vfmadd231pd %xmm6,%xmm4,%xmm0
         : 2987   result.opacity += (*k)*k_pixels[u].opacity;
   19.16 :   423cf6: vfmadd231pd %xmm5,%xmm4,%xmm2 // morphology.c:2987
         : 2982   for (u=0; u < (ssize_t) kernel->width; u++, k--) {
   14.51 :   423cfb: cmp    %rdi,%rbp // morphology.c:2982
    0.00 :   423cfe: jne    423cb0 <MorphologyApply.6136+0x20c0>

Changing short to double makes it vectorized:
.L2:
        vmovupd (%rax), %ymm4
        vmovupd 64(%rax), %ymm2
        subq    $-128, %rax
        subq    $32, %rdx
        vunpcklpd       -96(%rax), %ymm4, %ymm1
        vunpckhpd       -96(%rax), %ymm4, %ymm0
        vmovupd -64(%rax), %ymm4
        vunpckhpd       -32(%rax), %ymm2, %ymm2
        vunpcklpd       -32(%rax), %ymm4, %ymm4
        vpermpd $27, 32(%rdx), %ymm3
        vpermpd $216, %ymm1, %ymm1
        vpermpd $216, %ymm0, %ymm0
        vpermpd $216, %ymm2, %ymm2
        vpermpd $216, %ymm4, %ymm4
        vunpcklpd       %ymm2, %ymm0, %ymm10
        vunpckhpd       %ymm2, %ymm0, %ymm0
        vunpckhpd       %ymm4, %ymm1, %ymm9
        vunpcklpd       %ymm4, %ymm1, %ymm1
        vpermpd $216, %ymm10, %ymm10
        vpermpd $216, %ymm0, %ymm0
        vfmadd231pd     %ymm3, %ymm10, %ymm6
        vfmadd231pd     %ymm3, %ymm0, %ymm8
        vpermpd $216, %ymm9, %ymm9
        vpermpd $216, %ymm1, %ymm1
        vfmadd231pd     %ymm3, %ymm1, %ymm5
        vfmadd231pd     %ymm3, %ymm9, %ymm7
        cmpq    %rax, %rcx
        jne     .L2

howver clang's code looks shorter:
LBB0_1:                                # =>This Inner Loop Header: Depth=1
        vbroadcastsd    (%rdi,%rcx,8), %ymm1
        vfmadd231pd     (%rax), %ymm1, %ymm0    # ymm0 = (ymm1 * mem) + ymm0
        addq    $32, %rax
        decq    %rcx
        cmpq    $-10000, %rcx                   # imm = 0xD8F0
        jne     .LBB0_1

We loop vectorize while clang slp vectorizes it seems.


---


### compiler : `gcc`
### title : `SRA scalarizes structure copies`
### open_at : `2022-06-27T15:35:41Z`
### last_modified_date : `2023-08-23T15:57:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106106
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
The following example

#include <arm_neon.h>

float32x2x2_t f2(const float *p1, const float *p2)
{
    float32x2x2_t v = vld2_f32(p1);
    return vld2_lane_f32(p2, v, 1);
}

uses a type `float32x2x2_t` which is an array consisting of two `float32x2_t` types.  This type fits within the maximum object size for SRA so it tries to scalarize it.

However doing so it makes some useless copies:

  D.22939 = __builtin_aarch64_ld2v2sf (p1_2(D));
  v = D.22939;
  __b = v;
  D.22937 = __builtin_aarch64_ld2_lanev2sf (p2_3(D), __b, 1); [tail call]

becomes

  D.22939 = __builtin_aarch64_ld2v2sf (p1_2(D));
  v$val$0_3 = D.22939.val[0];
  v$val$1_5 = D.22939.val[1];
  __b.val[0] = v$val$0_3;
  __b.val[1] = v$val$1_5;
  D.22937 = __builtin_aarch64_ld2_lanev2sf (p2_4(D), __b, 1); [tail call]

having broken the structures up it causes problem for register allocation as these types require sequential register allocation and reload is unable to consolidate all the copies resulting in superfluous register moves:

f2:
        ld2     {v2.2s - v3.2s}, [x0]
        mov     v0.8b, v2.8b
        mov     v1.8b, v3.8b
        ld2     {v0.s - v1.s}[1], [x1]
        ret

The following snippet from a real library using intrinsics shows the resulting carnage https://godbolt.org/z/xnre3Pe34.

Perhaps SRA should not scalarize a type if it's just being used in a copy? or have a way to prevent scalarization of certain types?


---


### compiler : `gcc`
### title : `Inefficient code generation: logical AND of disjoint booleans from equal and bitwise AND not optimized to constant false`
### open_at : `2022-06-29T13:28:57Z`
### last_modified_date : `2023-06-26T05:22:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106138
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
_Bool f(char x)
{
    _Bool b1 = x == 4;
    _Bool b2 = x & 0x3;
    return b1 && b2;
}

Invocation: gcc -O3

Actual generated code:
f:
        test    dil, 3
        setne   al
        cmp     dil, 4
        sete    dl
        and     eax, edx
        ret

Expected generated code:
f:
        xor     eax, eax
        ret

The Summary needs to be adjusted perhaps.


---


### compiler : `gcc`
### title : `a redundant movprfx insn compare to llvm`
### open_at : `2022-06-30T12:29:46Z`
### last_modified_date : `2022-08-20T17:21:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106146
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
* test case, gcc has a redundant movprfx insn in the kernel loop body, see detail https://gcc.godbolt.org/z/8vG4PzM18. 

```
#include <arm_sve.h>

#define ARRAY_ALIGNMENT 64
#define LEN_2D 128ll
#define LEN_1D 8000ll
#define iterations 10000
typedef float real_t;

__attribute__((aligned(ARRAY_ALIGNMENT))) real_t a[LEN_1D],b[LEN_1D];

void s113_tuned(void) {
    for (int nl = 0; nl < 4*iterations; nl++) {
        int64_t i = 1;
        svbool_t pg = svwhilelt_b32(i, LEN_1D);
        svfloat32_t a0v = svdup_f32(a[0]);
        do {
            svfloat32_t bv = svld1_f32(pg, &b[i]);
            svfloat32_t res = svadd_z(pg, bv, a0v);
            svst1(pg, &a[i], res);
            i += svcntw();
            pg = svwhilelt_b32(i, LEN_1D);
        } while (svptest_any(svptrue_b32(), pg));
    }
    return;
}
```

* gcc's kernel loop
```
.L2:
        ld1w    z0.s, p0/z, [x3, x0, lsl 2]
        movprfx z0.s, p0/z, z0.s
        fadd    z0.s, p0/m, z0.s, z1.s
        st1w    z0.s, p0, [x1, x0, lsl 2]
        incw    x0
        whilelt p0.s, x0, x2
        b.any   .L2
```

* llvm's kernel loop:
```
.LBB0_2:                                //   Parent Loop BB0_1 Depth=1
        ld1w    { z1.s }, p2/z, [x13, x14, lsl #2]
        fadd    z1.s, p2/m, z1.s, z0.s
        st1w    { z1.s }, p2, [x12, x14, lsl #2]
        add     x14, x10, x14
        whilelt p2.s, x14, x9
        b.ne    .LBB0_2
```


---


### compiler : `gcc`
### title : `Inconsistent optimization when defaulting aggregate vs non-aggregate`
### open_at : `2022-06-30T19:07:59Z`
### last_modified_date : `2022-07-13T06:30:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106151
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Consider this example:

using size_t = decltype(sizeof(0));

struct string_view {
    // string_view() : ptr(nullptr), len(0) { }
    string_view() = default;

    char const* ptr = nullptr;
    size_t len = 0;
};

struct Fields
{
    string_view a;
    string_view b;
    string_view c;
    string_view d;
    string_view e;
    string_view f;
};

struct Foo : Fields
{
    void clear();

    Foo() = default;
};

void Foo::clear() {
    *this = Foo{};
}

struct Bar : Fields
{
    // No constructor, not even defaulted.

    void clear();
};

void Bar::clear() {
    *this = Bar{};
}


On gcc 12.1 -std=c++20 -O2 -mavx2, this emits:

Foo::clear():
        mov     QWORD PTR [rdi], 0
        mov     QWORD PTR [rdi+8], 0
        mov     QWORD PTR [rdi+16], 0
        mov     QWORD PTR [rdi+24], 0
        mov     QWORD PTR [rdi+32], 0
        mov     QWORD PTR [rdi+40], 0
        mov     QWORD PTR [rdi+48], 0
        mov     QWORD PTR [rdi+56], 0
        mov     QWORD PTR [rdi+64], 0
        mov     QWORD PTR [rdi+72], 0
        mov     QWORD PTR [rdi+80], 0
        mov     QWORD PTR [rdi+88], 0
        ret
Bar::clear():
        mov     ecx, 12
        xor     eax, eax
        rep stosq
        ret

There are a lot of subtle changes you can make to change the generated code:

* if Fields has 3 members or fewer, it's a bunch of mov's either way
* if Fields has 4 or 5 members, Bar::clear uses vmov insead of mov. 
* if Fields has 6+ members, Bar::clear uses stosq
* if Fields has an array of string_view, Foo::clear and Bar::clear are identical for sizes 2, 3, and >= 9.

If string_view uses the constructor I have commented out (with the member initializer list, instead of defaulted and using the default member initializers, then:
* if Fields holds an array of string_view with at least 4 string_views, both Foo::clear and Bar::clear emit a loop.
* otherwise, it's always a bunch of mov's for both Foo and Bar, regardless of how many members. 


Feels like something is up here.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] spurious "may be used uninitialized" warning`
### open_at : `2022-07-01T01:49:32Z`
### last_modified_date : `2023-08-08T05:04:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106155
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
With "-O -Wmaybe-uninitialized", I get a spurious "may be used uninitialized" on the following code on an x86_64 Debian/unstable machine:

int *e;
int f1 (void);
void f2 (int);
long f3 (void *, long, int *);
void f4 (void *);
int *fh;

void tst (void)
{
  int status;
  unsigned char badData[3][3] = { { 7 }, { 16 }, { 23 } };
  int badDataSize[3] = { 1, 1, 1 };
  int i;

  for (i = 0; i < 3; i++)
    {
      int emax;
      if (i == 2)
        emax = f1 ();
      status = f3 (&badData[i][0], badDataSize[i], fh);
      if (status)
        {
          f1 ();
          f1 ();
          f1 ();
        }
      f4 (fh);
      *e = 0;
      f1 ();
      if (i == 2)
        f2 (emax);
    }
}

Note that even a small change such as changing "long" to "int" as the second parameter of f3 makes the warning disappear.

$ gcc-12 -O -Wmaybe-uninitialized -c -o tfpif.o tfpif.c
tfpif.c: In function ‘tst’:
tfpif.c:31:9: warning: ‘emax’ may be used uninitialized [-Wmaybe-uninitialized]
   31 |         f2 (emax);
      |         ^~~~~~~~~
tfpif.c:17:11: note: ‘emax’ was declared here
   17 |       int emax;
      |           ^~~~
$ gcc-12 --version
gcc-12 (Debian 12.1.0-5) 12.1.0
[...]

$ gcc-snapshot -O -Wmaybe-uninitialized -c -o tfpif.o tfpif.c
tfpif.c: In function 'tst':
tfpif.c:31:9: warning: 'emax' may be used uninitialized [-Wmaybe-uninitialized]
   31 |         f2 (emax);
      |         ^~~~~~~~~
tfpif.c:17:11: note: 'emax' was declared here
   17 |       int emax;
      |           ^~~~
$ gcc-snapshot --version
gcc (Debian 20220630-1) 13.0.0 20220630 (experimental) [master r13-1359-gaa1ae74711b]
[...]

No such issue with:
  gcc-9 (Debian 9.5.0-1) 9.5.0
  gcc-10 (Debian 10.4.0-1) 10.4.0
  gcc-11 (Debian 11.3.0-4) 11.3.0

I detected this issue by testing GNU MPFR. The above code is derived from "tests/tfpif.c", function check_bad.


---


### compiler : `gcc`
### title : `Dubious choice of optimization strategy`
### open_at : `2022-07-01T12:36:46Z`
### last_modified_date : `2023-07-07T07:12:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106161
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Hello,

here's a piece of C code:

...
#define	AC_NEWCEILING		16
#define	AC_NEWFLOOR		32

...
        if (newclipbounds)
        {
            int newfloorclipx = floorclipx;
            int newceilingclipx = ceilingclipx;
            uint16_t newclip;

            // rewrite clipbounds
            if (actionbits & AC_NEWFLOOR)
                newfloorclipx = low;
            if (actionbits & AC_NEWCEILING)
                newceilingclipx = high;

            newclip = (newceilingclipx << 8) + newfloorclipx;
            clipbounds[x] = newclip;
            newclipbounds[x] = newclip;
        }
...

which is compiled with -Os and results in the following set of SH-2 assembler instructions:

        if (newclipbounds)
 190:	54 fb       	mov.l	@(44,r15),r4
 192:	24 48       	tst	r4,r4
 194:	8d 11       	bt.s	1ba <_R_SegLoop+0x1ba>
 196:	e0 58       	mov	#88,r0
            if (actionbits & AC_NEWFLOOR)
 198:	05 fe       	mov.l	@(r0,r15),r5
 19a:	25 58       	tst	r5,r5
 19c:	8f 01       	bf.s	1a2 <_R_SegLoop+0x1a2>
 19e:	e0 5c       	mov	#92,r0
        floorclipx = ceilingclipx & 0x00ff;
 1a0:	67 93       	mov	r9,r7
            if (actionbits & AC_NEWCEILING)
 1a2:	00 fe       	mov.l	@(r0,r15),r0
 1a4:	20 08       	tst	r0,r0
 1a6:	8f 01       	bf.s	1ac <_R_SegLoop+0x1ac>
 1a8:	e0 40       	mov	#64,r0
            int newceilingclipx = ceilingclipx;
 1aa:	66 83       	mov	r8,r6
            clipbounds[x] = newclip;
 1ac:	00 fe       	mov.l	@(r0,r15),r0
            newclip = (newceilingclipx << 8) + newfloorclipx;
 1ae:	46 18       	shll8	r6
 1b0:	37 6c       	add	r6,r7
 1b2:	67 7d       	extu.w	r7,r7
            clipbounds[x] = newclip;
 1b4:	0c 75       	mov.w	r7,@(r0,r12)
            newclipbounds[x] = newclip;
 1b6:	50 fb       	mov.l	@(44,r15),r0
 1b8:	0c 75       	mov.w	r7,@(r0,r12)


What I find really odd is that gcc opts to cache results of bitwise AND on the stack and reload them individually instead of simply doing tst #imm1,r0 and tst #imm,r0. There are more instances of the this behavior further down the same function.

Now memory reads are really expensive on the target architecture and I would like to avoid them if possible. I'm not sure whether this behavior is triggered by some optimization setting or is inherent to the architecture, but I'd appreciate any help here.


---


### compiler : `gcc`
### title : `(a > b) & (a >= b) does not get optimized until reassoc1`
### open_at : `2022-07-01T22:39:13Z`
### last_modified_date : `2023-09-26T15:01:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106164
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
```
_Bool f(int a, int b)
{
  _Bool c = a > b;
  _Bool d = a >= b;
  return c & d;
}
```
This does not get optimized until reassoc1.
While:
```
_Bool f(int a, int b)
{
  return (a > b) & (a >= b);
}
```
Gets optimized during folding (not by match though), I have not looked into what does it though.

I noticed this while working on PR 105903 as there is not a reassoc pass after phiopt4 so nothing optimizes.


---


### compiler : `gcc`
### title : `Missed optimization (memory vs register read)`
### open_at : `2022-07-02T10:50:23Z`
### last_modified_date : `2022-07-04T06:39:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106167
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Created attachment 53238
Test case

Hello,

in the provided example, this piece of C code:

        if (newclipbounds)
        {
            int newfloorclipx = floorclipx;
            int newceilingclipx = ceilingclipx;
            uint16_t newclip;

            // rewrite clipbounds
            if (actionbits & AC_NEWFLOOR)
                newfloorclipx = low;
            if (actionbits & AC_NEWCEILING)
                newceilingclipx = high;

            newclip = (newceilingclipx << 8) + newfloorclipx;
            clipbounds[x] = newclip;
            newclipbounds[x] = newclip;
        }

is compiled to the following assembler code:

        if (newclipbounds)
 16c:	54 f7       	mov.l	@(28,r15),r4
 16e:	24 48       	tst	r4,r4
 170:	8d 10       	bt.s	194 <_R_SegLoop+0x194>
 172:	e0 54       	mov	#84,r0
            if (actionbits & AC_NEWFLOOR)
 174:	05 fe       	mov.l	@(r0,r15),r5
 176:	25 58       	tst	r5,r5
 178:	8f 01       	bf.s	17e <_R_SegLoop+0x17e>
 17a:	e0 58       	mov	#88,r0
        floorclipx = ceilingclipx & 0x00ff;
 17c:	67 a3       	mov	r10,r7
            if (actionbits & AC_NEWCEILING)
 17e:	00 fe       	mov.l	@(r0,r15),r0
 180:	20 08       	tst	r0,r0
 182:	8f 01       	bf.s	188 <_R_SegLoop+0x188>
 184:	50 fb       	mov.l	@(44,r15),r0
            int newceilingclipx = ceilingclipx;
 186:	66 93       	mov	r9,r6
            newclip = (newceilingclipx << 8) + newfloorclipx;
 188:	46 18       	shll8	r6
 18a:	37 6c       	add	r6,r7
 18c:	67 7d       	extu.w	r7,r7
            clipbounds[x] = newclip;
 18e:	08 75       	mov.w	r7,@(r0,r8)
            newclipbounds[x] = newclip;


The following lines are of particular interest:
        if (newclipbounds)
 16c:	54 f7       	mov.l	@(28,r15),r4
 16e:	24 48       	tst	r4,r4
...
            newclipbounds[x] = newclip;
 190:	50 f7       	mov.l	@(28,r15),r0
 192:	08 75       	mov.w	r7,@(r0,r8)

The compiler fails to notice that it's already holding the value of @(28,r15) in r4 and re-reads the value from stack instead of doing the mov r4,r0.

CFLAGS: -c -std=c11 -g -m2 -mb -Os -fomit-frame-pointer -Wall -Wextra -pedantic -Wno-unused-parameter -Wimplicit-fallthrough=0 -Wno-missing-field-initializers -Wnonnull

This may or may not be relevant to bug #106161


---


### compiler : `gcc`
### title : `[13 regression] Recent change causing target regressions for uninitialized objects`
### open_at : `2022-07-04T15:54:03Z`
### last_modified_date : `2022-07-05T15:51:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106186
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
This change:

commit 4e82205b68024f5c1a9006fe2b62e1a0fa7f1245
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Fri Jul 1 13:20:42 2022 +0200

    Integrate nonzero bits with irange.

    The nonzero bits and integer ranges compliment each other quite well,
    and it only makes sense to make the mask a first class citizen in the
    irange.  We do a half assed job of keeping ranges and nonzero bits
    somewhat in sync in SSA_NAME_RANGE_INFO, and the goal has always
    been to integrate them properly.  This patch does that, in preparation
    for streaming out full-resolution iranges between passes (think
    SSA_NAME_RANGE_INFO).
[ ... ]

Has caused several targets to start failing this test:

Tests that now fail, but worked before (1 tests):

cris-sim: gcc.dg/auto-init-uninit-4.c (test for excess errors)


From the gcc.log file:

Excess errors:
/home/jlaw/test/gcc/gcc/testsuite/gcc.dg/uninit-4.c:49:10: warning: 'rprio' may be used uninitialized [-Wmaybe-uninitialized]

This should be visible with just a cross compiler.


Testcase not explicitly included as it's obvious this is gcc.dg/uninit-4.c in the testsuite.


---


### compiler : `gcc`
### title : `Shrink-wrapping opportunity releated to function call`
### open_at : `2022-07-05T13:08:02Z`
### last_modified_date : `2022-07-05T22:49:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106200
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
case:https://godbolt.org/z/sc5rTzaeb
```
double advance(double dt, double dx, double dy, double dz)
{
    double dSquared = dx * dx + dy * dy + dz * dz;
    double mag = dt / (dSquared * std::sqrt(dSquared));

    return mag;        
}
```

we can the llvm Shrink-wrapping, so the spill only in the branch where *call sqrt*, so there is no spill if goto the branch where use the *insn fsqrt*.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] missing shrink wrap for simple case since r9-3594-g8d2d39587d941a40`
### open_at : `2022-07-05T22:39:02Z`
### last_modified_date : `2023-07-25T14:37:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106210
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Take:
```
int f(int);

int advance(int dz)
{
    int dz1;
    if (dz > 0)
        return (dz + dz) * dz;
    else
        return dz * f(dz);
}
```
I would have expected this to be shrink wrapped but it is not.


---


### compiler : `gcc`
### title : `[11/12/13 Regression] sinking of loads prevents vectorization`
### open_at : `2022-07-06T18:19:33Z`
### last_modified_date : `2022-07-06T21:17:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106217
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.3.1`
### severity : `normal`
### contents :
The following example

void f(int n, float * __restrict x, float *mass, float max, float &ax)
{
    float lax = 0.0f;

    for (int i = 0; i < n; ++i) {
        float f = x[i] * mass[i];
        lax += x[i] >= max ? 0 : f * x[i];
    }

  ax += lax;
  return;
}

vectorizes with GCC 10 with -Ofast but fails starting with GCC 11.
The difference is that in the sink1 pass we now sink the unconditional load of mass[i] into the conditional.

Sinking # VUSE <.MEM_14(D)>
_7 = *_6;
 from bb 3 to bb 4
Sinking _6 = mass_18(D) + _2;
 from bb 3 to bb 4

consequently this causes if-convert to no longer be able to if-convert the loop
because the load is now conditional, and we only accept unconditional loads in ifcvt_memrefs_wont_trap.

-------------------------
_6 = mass_18(D) + _2;
-------------------------
_7 = *_6;
tree could trap...

perhaps during the sinking we should mark the load as non-trapping? as it was originally unconditional anyway.


---


### compiler : `gcc`
### title : `x86-64 optimizer forgets about shrd peephole optimization pattern when faced with more than one in close proximity`
### open_at : `2022-07-06T22:05:04Z`
### last_modified_date : `2022-11-11T05:05:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106220
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
I am reporting about right shift issue, but left shift has the same issues as well.

In theory, gcc knows how to calculate lower 64 bits of the right shift of 128-bit number with a single instruction when it is provable that shift count is in range [0:63]. In practice, it does it only under very special condition.
See here: https://godbolt.org/z/fhdo8xhxW

foo1to1 is good
foo2to1 is good
foo1to2 starts well but is broken near the end but hyperactive vectorizer.
But that's a separate issue already reported in 105617.

foo2to2, foo2to3, foo3to4 - looks like compiler forgot all it knew about double-word right shifts, or, more likely, forgot that (x % 64) is always in range [0:63].

I am reporting it as a target issue despite being sure that the problem is not in the x86-64 back end itself, but somehow in interaction between various phases of optimizer. As 80+ percents of my reports.
However it's your call, not mine. In practice, an impact is most visible on x86-64, because, due to existence of shrd instruction, x86-64 is potentially very good in this sort of tasks. On ARM64 or on POWER64LE the relative slowdown is lower, because an optimal code is not as fast.

P.S
82261 sounds similar, but I am not sure it is related.


---


### compiler : `gcc`
### title : `x86 Better code squence for __builtin_shuffle`
### open_at : `2022-07-07T06:26:00Z`
### last_modified_date : `2023-07-30T09:03:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106222
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Related bug: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=54346

After I merge two VEC_PERM_EXPR in the match.pd, I found that two __builtin_shuffle actually generate better code than one __builtin_shuffle:

https://godbolt.org/z/xE9xd9ExT


---


### compiler : `gcc`
### title : `sign-extension of the result of `__builtin_ctz``
### open_at : `2022-07-08T08:55:30Z`
### last_modified_date : `2022-07-19T07:52:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106231
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
Given (godbolt: https://gcc.godbolt.org/z/hqKKW33T7):


```
long long
foo(long long x, unsigned bits)
  {
    return x + (unsigned) __builtin_ctz(bits);
  }
```

GCC output:
```
foo:
        bsf     eax, esi
        cdqe
        add     rax, rdi
        ret
```

Clang output:
```
foo:                                    # @foo
        bsf     eax, esi
        add     rax, rdi
        ret
```


The CDQE instruction is totally unnecessary, because BSF stores either a non-negative value or an undefined one into its destination operand.

It's sad that `__builtin_ctz()` yields a signed type; but even we cast it, we don't get anything better.


---


### compiler : `gcc`
### title : `Not a bug, bad performance (with GCC 11.3.0 - O3) of a small etude in C`
### open_at : `2022-07-08T14:24:47Z`
### last_modified_date : `2022-07-11T06:44:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106236
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.3.0`
### severity : `normal`
### contents :
Created attachment 53279
The full C source, along with how to compile

The following C etude is compiled not in a good way by GCC 11.3.0 -O3; the thing that is unpleasant to look on, is the unnecesary jumps GCC generates:

					// 'Magnetica' partitioning, mainloop rev.7p [
					for (;;) {
						for (;Pivot < *Jndx; Jndx--) {
						}
						// Jndx could be PR i.e. PR == Jndx
						if (PR == Jndx) break;
						//if (PR == Jndx) goto KUH; //] QS_bench_r14_CLANG_14.0.1_rev5bypass.exe Glupendxr DNA Done in 240/59 seconds.
						//PR++;                     //]<----+
						//PR = PR + !!(PR - Jndx); //]      |
						//PR = PR + (PR != Jndx);  //]------+ QS_bench_r14_CLANG_14.0.1_rev5bypass.exe Glupendxr DNA Done in 256/63 seconds.
						PR++;					
						M18_swapUnconditional (PR, Jndx);
						// Inhere Pivot is either == or > *(PR) when PR<Jndx
						if (Pivot > *(PR)) {
							*PL=*(PR); PL++; *(PR)=Pivot;									
						}
					}
					//M18_SwapConditional_ifXbY_BUGGY((uint64_t)Jndx, (uint64_t)PR, PR, Jndx); //] QS_bench_r14_CLANG_14.0.1_rev5bypass.exe Glupendxr DNA Done in 239/57 seconds.
					//PR = PR - (PR > Jndx);                                                   //]
					//PR = PR + M18_SwapConditional_ifXbY_BUGGY_DidWeSwap((uint64_t)Jndx, (uint64_t)PR, PR, Jndx); //] QS_bench_r14_CLANG_14.0.1_rev5bypass.exe Glupendxr DNA Done in 233/57 seconds.
					KUH:;
					// 'Magnetica' partitioning, mainloop rev.7p ]

// QS_bench_r14_CLANG_14.0.1_rev5bypass.exe Glupendxr DNA; Done in 236/33 seconds.
// QS_bench_r14_GCC11.3.0_rev5bypass.exe Glupendxr DNA; Done in 239/37 seconds.
// QS_bench_r14_ICL19.0_rev5bypass.exe Glupendxr DNA; Done in 244/32 seconds.

// clang_14.0.1 -O3 -mavx2 -S -fverbose-asm
/*
// 'Magnetica' partitioning, mainloop rev.7p [
.LBB181_7:                         ]<-----+
	movq	%rax, (%rsi)              |
	movq	%rsi, %rdi                |
	.p2align	4, 0x90           |
.LBB181_8:                        ]<--+   |
	addq	$8, %rcx              |   |
	.p2align	4, 0x90       |   |
.LBB181_9:                       ]<-+ |   |
	movq	-8(%rcx), %rbx      | |   |
	addq	$-8, %rcx           | |   |
	cmpq	%rbx, %rax          | |   |
	jb	.LBB181_9        ]--+ |   |
	cmpq	%rdi, %rcx            |   |
	je	.LBB181_13            |   |
	leaq	8(%rdi), %rsi         |   |
	movq	8(%rdi), %r14         |   |
	movq	%rbx, 8(%rdi)         |   |
	movq	%r14, (%rcx)          |   |
	movq	8(%rdi), %rbx         |   |
	movq	%rsi, %rdi            |   |
	cmpq	%rbx, %rax            |   |
	jbe	.LBB181_8         ]---+   |
	movq	%rbx, (%r11)              |
	addq	$8, %r11                  |
	jmp	.LBB181_7          ]------+
// 'Magnetica' partitioning, mainloop rev.7p ]
*/

// gcc_11.3.0 -S -O3 -mavx2 -m64 -static -fomit-frame-pointer
/*
// 'Magnetica' partitioning, mainloop rev.7p [
.L5597:                        ]<----------------------+<-+
	cmpq	%rcx, %r8                              |  |
	jb	.L5598          ]-------------------+  |  |
.L5609:                       ]<--+                 |  |  |
	leaq	8(%rax), %r11     |                 |  |  |
	cmpq	%rax, %rdx        |                 |  |  |
	je	.L5599            |                 |  |  |
	movq	8(%rax), %r10     |                 |  |  |
	movq	%rcx, 8(%rax)     |                 |  |  |
	movq	%r10, (%rdx)      |                 |  |  |
	movq	8(%rax), %rcx     |                 |  |  |
	cmpq	%r8, %rcx         |                 |  |  |
	jnb	.L5605            | ]---+           |  |  |
	movq	%rcx, (%r9)       |     |           |  |  |
	addq	$8, %r9           |     |           |  |  |
	movq	%r8, 8(%rax)      |     |           |  |  |
	movq	(%rdx), %rcx      |     |           |  |  |
	movq	%r11, %rax        |     |           |  |  |
	cmpq	%rcx, %r8         |     |           |  |  |
	jnb	.L5609        ]---+     |           |  |  |
.L5598:                                 | ]<--------+  |  |
	movq	-8(%rdx), %rcx          |              |  |
	subq	$8, %rdx                |              |  |
	jmp	.L5597                  | ]------------+  |
	.p2align 4,,10                  |                 |
	.p2align 3                      |                 |
.L5605:                        ]<-------+                 |
	movq	%r10, %rcx                                |
	movq	%r11, %rax                                |
	jmp	.L5597         ]--------------------------+
// 'Magnetica' partitioning, mainloop rev.7p ]
*/


---


### compiler : `gcc`
### title : `[12/13/14 regression] Inline optimization causes dangling pointer warning on "include/c++/12.1.0/bits/stl_tree.h"`
### open_at : `2022-07-08T21:32:37Z`
### last_modified_date : `2023-08-16T07:38:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106238
### status : `NEW`
### tags : `diagnostic, missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
Created attachment 53282
Preprocessed file to reroduce the issue

When compiling the code below, we get a dangling pointer warning.

====================================================================
#include <map>

struct sysDLoc {                        /* Coordinates (in DBU) */
  double        x, y;
};

std::map<int*, sysDLoc> static_copy;

void realSwap()
{
  std::map<int*, sysDLoc> local_copy;
  extern void getLocalCopy(std::map<int*, sysDLoc>&);
  getLocalCopy(local_copy);
  local_copy.swap(static_copy);
}
====================================================================

Compilation command:
g++ -Wdangling-pointer -c -O2 bug.cpp


Warning log:
In file included from /grid/common/test/gcc-v12.1.0d2rh74_lnx86/include/c++/12.1.0/map:60,
                 from bug.cpp:1:
In member function 'void std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::swap(std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>&) [with _Key = int*; _Val = std::pair<int* const, sysDLoc>; _KeyOfValue = std::_Select1st<std::pair<int* const, sysDLoc> >; _Compare = std::less<int*>; _Alloc = std::allocator<std::pair<int* const, sysDLoc> >]',
    inlined from 'void std::map<_Key, _Tp, _Compare, _Alloc>::swap(std::map<_Key, _Tp, _Compare, _Alloc>&) [with _Key = int*; _Tp = sysDLoc; _Compare = std::less<int*>; _Alloc = std::allocator<std::pair<int* const, sysDLoc> >]' at /grid/common/test/gcc-v12.1.0d2rh74_lnx86/include/c++/12.1.0/bits/stl_map.h:1172:18,
    inlined from 'void realSwap()' at bug.cpp:14:18:
/grid/common/test/gcc-v12.1.0d2rh74_lnx86/include/c++/12.1.0/bits/stl_tree.h:2090:32: warning: storing the address of local variable 'local_copy' in '*MEM[(struct _Rb_tree_node_base * &)&local_copy + 16].std::_Rb_tree_node_base::_M_paren
' [-Wdangling-pointer=]
 2090 |           _M_root()->_M_parent = _M_end();
      |           ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~
bug.cpp: In function 'void realSwap()':
bug.cpp:11:27: note: 'local_copy' declared here
   11 |   std::map<int*, sysDLoc> local_copy;
      |                           ^~~~~~~~~~
bug.cpp:11:27: note: 'local_copy.std::map<int*, sysDLoc, std::less<int*>, std::allocator<std::pair<int* const, sysDLoc> > >::_M_t.std::_Rb_tree<int*, std::pair<int* const, sysDLoc>, std::_Select1st<std::pair<int* const, sysDLoc> >, std::less<int*>, std::allocator<std::pair<int* const, sysDLoc> > >::_M_impl.std::_Rb_tree<int*, std::pair<int* const, sysDLoc>, std::_Select1st<std::pair<int* const, sysDLoc> >, std::less<int*>, std::allocator<std::pair<int* const, sysDLoc> > >::_Rb_tree_impl<std::less<int*>, true>::<unnamed>.std::_Rb_tree_header::_M_header.std::_Rb_tree_node_base::_M_parent' declared here

This warning only happens on GCC v12.1. Using -O1, "-fno-inline-small-functions”, or  “-fno-inline-functions” prevents the error from happening.

Regards,
Rogerio


---


### compiler : `gcc`
### title : `[13/14 Regression] missed vectorization opportunity (cond move) on mips since r13-707-g68e0063397ba82`
### open_at : `2022-07-09T03:45:50Z`
### last_modified_date : `2023-07-27T09:23:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106240
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
This can be see on the mipsisa32r2-linux-gnu target with a cross compiler.

Starting a couple months ago these tests started failing on mipsisa32r2-linux-gnu:

Tests that now fail, but worked before (24 tests):

gcc.target/mips/mips-ps-5.c   -O1   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-5.c   -O1   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-5.c   -O1   scan-assembler \tmov[tf]\\.ps\t
gcc.target/mips/mips-ps-5.c   -O2   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-5.c   -O2   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-5.c   -O2   scan-assembler \tmov[tf]\\.ps\t
gcc.target/mips/mips-ps-5.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-5.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-5.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions   scan-assembler \tmov[tf]\\.ps\t
gcc.target/mips/mips-ps-5.c   -O3 -g   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-5.c   -O3 -g   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-5.c   -O3 -g   scan-assembler \tmov[tf]\\.ps\t
gcc.target/mips/mips-ps-7.c   -O1   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-7.c   -O1   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-7.c   -O1   scan-assembler \tmov[tf]\\.ps\t
gcc.target/mips/mips-ps-7.c   -O2   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-7.c   -O2   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-7.c   -O2   scan-assembler \tmov[tf]\\.ps\t
gcc.target/mips/mips-ps-7.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-7.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-7.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions   scan-assembler \tmov[tf]\\.ps\t
gcc.target/mips/mips-ps-7.c   -O3 -g   scan-assembler \tadd\\.ps\t
gcc.target/mips/mips-ps-7.c   -O3 -g   scan-assembler \tc\\.eq\\.ps\t
gcc.target/mips/mips-ps-7.c   -O3 -g   scan-assembler \tmov[tf]\\.ps\t

This change is the trigger:
commit 68e0063397ba820e71adc220b2da0581dce29ffa
Author: Richard Biener <rguenther@suse.de>
Date:   Mon Apr 11 13:36:53 2022 +0200

    Force the selection operand of a GIMPLE COND_EXPR to be a register

    This goes away with the selection operand allowed to be a GENERIC
    tcc_comparison tree.  It keeps those for vectorizer pattern recog,
    those are short lived and removing this instance is a bigger task.

    The patch doesn't yet remove dead code and functionality, that's
    left for a followup.  Instead the patch makes sure to produce
    valid GIMPLE IL and continue to optimize COND_EXPRs where the
    previous IL allowed and the new IL showed regressions in the testsuite.

[ ... ]


Basically before this change we were able to vectorize the loop and after that change we no longer vectorize the loop.

Testcase:

/* { dg-do compile } */
/* { dg-options "-mpaired-single -mgp64 -ftree-vectorize forbid_cpu=octeon.*" } */
/* { dg-skip-if "requires vectorization" { *-*-* } { "-O0" "-Os" } { "" } } */

extern float a[] __attribute__ ((aligned (8)));
extern float b[] __attribute__ ((aligned (8)));
extern float c[] __attribute__ ((aligned (8)));

NOMIPS16 void
foo (void)
{
  int i;
  for (i = 0; i < 16; i++)
    a[i] = b[i] == c[i] + 1 ? b[i] : c[i];
}

/* { dg-final { scan-assembler "\tadd\\.ps\t" } } */
/* { dg-final { scan-assembler "\tc\\.eq\\.ps\t" } } */
/* { dg-final { scan-assembler "\tmov\[tf\]\\.ps\t" } } */


Compilation line:

/home/jlaw/test/obj/mipsisa32r2-linux-gnu/obj/gcc/gcc/xgcc -B/home/jlaw/test/obj/mipsisa32r2-linux-gnu/obj/gcc/gcc/ /home/jlaw/test/gcc/gcc/testsuite/gcc.target/mips/mips-ps-5.c -fdiagnostics-plain-output -O2 -DNOMIPS16="__attribute__((nomips16))" -DNOMICROMIPS="__attribute__((nomicromips))" -DNOCOMPRESSION="__attribute__((nocompression))" -mabi=o64 -mips64r2 -mhard-float -mdouble-float -mfp64 -mgp64 -mlong32 -mpaired-single -modd-spreg -ftree-vectorize -fno-ident -S -o mips-ps-5.s -fdump-tree-all-details


As one would expect this patch changes the form of the COND_EXPRs and ultimately we're unable to vectorize as a result.  I haven't dug any deeper than that.


---


### compiler : `gcc`
### title : `Failure to optimize (0 - x) & 1 on gimple level (including vector types)`
### open_at : `2022-07-10T08:40:08Z`
### last_modified_date : `2023-09-21T12:32:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106243
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
#include <stdint.h>

typedef int64_t v2i64 __attribute__((vector_size(16)));

v2i64 f(v2i64 x)
{
    return (0 - x) & 1;
}

This can be optimized to `return x & 1;`. LLVM does this transformation, but GCC does not


---


### compiler : `gcc`
### title : `Failure to optimize (1 << x) & 1 to `x == 0` if separated into multiple statements`
### open_at : `2022-07-10T08:56:31Z`
### last_modified_date : `2023-09-21T12:31:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106244
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

int8_t f(int8_t x)
{
    int8_t sh = 1 << x;
    return sh & 1;
}

This can be optimized to `return x == 0;`. This transformation is done by LLVM, but not by GCC.

PS: For some reason GCC manages to do this optimization if I replace `f` with `return (1 << x) & 1;` instead of having it spelled out in 2 statements.


---


### compiler : `gcc`
### title : `Failure to optimize (u8)(a << 7) >> 7 pattern to a & 1`
### open_at : `2022-07-10T09:30:02Z`
### last_modified_date : `2023-10-20T23:08:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106245
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
#include <stdint.h>

int8_t f(int8_t a)
{
    return (uint8_t)(a << 7) >> 7;
}

This can be optimized to `return a & 1;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `RISC-V SPEC2017 507.cactu code bloat due to address generation`
### open_at : `2022-07-11T23:13:53Z`
### last_modified_date : `2023-08-04T22:30:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106265
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
SPEC2017 FP benchmark 507.cactu: ML_BSSN_RHS.cc:ML_BSSN_RHS_Body() has terrible codegen. A recurring pattern which shows up in hotspot analysis is

  380a1a:       lui     a6,0x1     # LI 4096 before linker relaxation
  380a1c:       addi    t2,sp,32
  380a20:       addi    a6,a6,-1192 # b58
  380a24:       add     a6,a6,t2
  380a26:       sd      t4,0(a6)

The first 4 instructions help calculate the destination of the last SD instruction. There were 27 such distinct instances in the hottest top block.

Part of this is the ISA not having a single instruction to do set a reg with 32-bit const and/or the limited addressing modes. However the compiler is not helping either. All those 27 instances have the first instruction to set register with 4096, sometimes with the register still being live with the exact same value.

Using creduce I was able to create a small'ish (not ideal small) test case which shows 14 instances of li <reg>,4096

Built as riscv64-unknown-linux-gnu-g++ -O3 -static -ffast-math -funroll-loops -march=rv64gc_zba_zbb_zbc_zbs -c  -std=c++03 

This is with trunk as of June 14 (commit 6abe341558ab)

--->8-----
void *a();
int b, c, d, l, o, q;
double *e, *f, *p, *r, *s = 0;
long g, h, k;
double m, n, aa;
void y() {
  double ag, ai, aj, ap, ar, at, aw(01.0 / 0);
  double *am((double *)a);
  double *an;
  double *ao((double *)a());
  long av = sizeof(double) * g;
  for (; l; ++l)
    for (int j = d; j < q; ++j)
      for (int i = 1; i < o; i++) {
        long az = i * j + l;
        double ba, bb, bc, bd, be, bf, bg, bh, bi, bj, bk, bl, bm, bn, bo, bp,
            bq, br, bs, bv(-ar);
        switch (b)
        case 4: {
          *(double *)((char *)0)[2] = ((char *)0)[2];
          ba = ((char *)0)[k + av] + ((char *)0)[2] + ((char *)0)[k] +
               *(double *)((char *)0)[av] +
               ((char *)0)[av * 2] * ((char *)0)[k * av];
          bb = (&am[az])[h] + ((char *)0)[k * h] +
               ((char *)&am[az])[1] *
                   (((char *)&am[az])[h] + *(&am)[h] + (&am[az])[k]) +
               (&am[az])[2] - ((char *)0)[h] + (&am[az])[k * av];
          bc = 4 * 8 + *(double *)&((char *)0)[2];
          bd = 4 * ((char *)0)[k + 1] +
               (&an[az])[k * h] * (((char *)0)[-1] + (&an[az])[k + h]) +
               8 * ((&an[az])[k * 2] + (&an[az])[av + h * -2] + (&an[az])[av] +
                    (&an[az])[av * 2 + h]) -
               8 * ((&an[az])[av * 2] + (&an[az])[k] + (&an[az])[av * 21] +
                    (&an[az])[av + h]) +
               *(&an)[h] - (&an[az])[av * 22] * (&an[az])[h] +
               (&an[az])[av * 2 * 2];
          bf = (&az)[0] * (((char *)&ao[az])[0] + ao[av] + (&ao[az])[av * 2] +
                           *(double *)((char *)ao)[k]);
          bg = (&ao[az])[0] * *(&ao)[av] + *(&ao)[2];
          bh = *((char **)0)[av] * (&ao[az])[av - h] +
               ((char *)0)[1] * *(double *)((char *)0)[1] *
                   (*(double *)((char *)&ao)[k] + *(double *)((char *)0)[12] +
                    ((char *)&ao[az])[av]) +
               *(&ao)[av * 2] - *(&ao)[k] - (&ao[az])[2] +
               (&ao[az])[k * av * 2];
          bi = *(&ao)[av * h] *
                   (ao[h] + ((char *)0)[k * 2] + (&ao[az])[k * av] +
                    ((char *)0)[21]) *
                   (((char *)0)[h] + *(double *)((char *)0)[h * 2] +
                    *(double *)((char *)&ao[az])[k] + *(&ao)[h]) +
               *(&ao)[av * 22] - (&ao[az])[2 * h * 2] + ((char *)0)[22];
          bj = 4 * ((&ao[az])[av] + (&ao[az])[k * av * h] * (&ao[az])[av * -1] +
                    (&ao[az])[av * h]) +
               ((&ao[az])[av * 12] + (&ao[az])[k] + ((char *)&ao)[h] +
                (&ao[az])[av * 2 + h * 0]) *
                   (((char *)0)[2] + *(double *)((char *)&ao[az])[av * 2] +
                    (&ao[az])[h] + (&ao[az])[av + h]) +
               (&ao[az])[av * 22] - ((char *)&ao[az])[h] - (&ao[az])[av * -2] +
               (&ao[az])[av + h * 2];
          bk = 8 * (&ap)[1];
          bm = ((char *)0)[1] * (&e[az])[2];
          bn = -(&az)[0];
          bo = (&e[az])[h] - (&e[az])[h * 2] * aw;
          bq = (&at)[h];
          br = 8 * 8 * ((&az)[1] + (&f[az])[-2] - (&f[az])[2]);
          bs = (&f[az])[h * -2] - (&f[az])[h * 2];
          n = ((char *)&s)[h * 2];
        }
          double bt(e[az] * f[az] - at * at);
        double bu(p[az] * at * az);
        double bw(f[az] - p[az]);
        double bx = 01.0 / 0;
        double by = 01.0 / 0 * 0;
        double ca;
        double cb = -bl;
        double cd;
        double cf = 0.5 * bn;
        double ch;
        double cj = bp - 0.5 * bo;
        double cm = ch * bv;
        double cn = bk * cd * ch;
        double co = bk + cd * by;
        double cq = bm + bx;
        double ct = ca + bx;
        double cu = cb * bt + cf * bu + cj * bv;
        double cz = bt + bq * bv;
        double db = br * bq * by;
        double dc = cm * cu * bw + cz * by;
        double dd = cn * bt + cq * ct + bs * by;
        double df(c == 1 ? s[az] : 0);
        double dg = df * n;
        double dj = bu + ai * bv;
        double dk = ag * ai * bu + aj * bv;
        double dl = bw + aj * bx;
        double dm;
        double dn = bu * bv;
        double t = bu + dj;
        double u = dk * by;
        double v = ag * bv + dl * by;
        double w = bx + dm;
        double x = bf + 0.333333333333333333333333333333 * bc + bi + ba + bj +
                   by * (bb + bd +
                         bg * bh *
                             (6 * w * dg + dn * co * t * bv + u * aa * cj * v +
                              w * db)) -
                   m * dc * dd + (be + 0.666666666666666666666666666667) * co;
        r[az] = x;
      }
}
--->8-----


---


### compiler : `gcc`
### title : `[suboptimal] Remove unnecessary loops releated to fortran compare to ifort`
### open_at : `2022-07-12T09:38:52Z`
### last_modified_date : `2022-07-12T12:16:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106268
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For the kernel inner loop body, gcc generate an loop, while icc doesn't, see detail in https://godbolt.org/z/G77nKnf8W.
```
    DO i = 1, 10000
      do l = 1, ADM_lall
        rhogw_vm(:,ADM_kmin  ,l) = 0.D0
        rhogw_vm(:,ADM_kmax+1,l) = 0.D0
      enddo
    enddo
```


---


### compiler : `gcc`
### title : `missed-optimization: redundant movzx`
### open_at : `2022-07-13T02:16:52Z`
### last_modified_date : `2022-07-22T06:42:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106277
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
I came across this when examining a loop that runs slower than I expected. It involves explicit and implicit conversions between 8-bit and 32/64-bit values, and as I looked through the generated assembly using Godbolt compiler explorer, I found lots of movzx instructions that don't seem to break dependency or play a role in correctness, not to mention many use the same register like "movzx eax al", which cannot be eliminated.

I then tried some simple examples on Godbolt with X86-64 GCC 12.1, and found that this behavior is persistent and easily reproducible, even when I specify "-march=skylake". Here's an example:

#include <stdint.h>

int add2bytes(uint8_t* a, uint8_t* b) {
    return uint8_t(*a + *b);
}
gcc -O3 gives:
add2bytes(unsigned char*, unsigned char*):
        movzx   eax, BYTE PTR [rsi]
        add     al, BYTE PTR [rdi]
        movzx   eax, al
        ret

The first movzx here breaks dependency on old eax value, but what is the second movzx doing? I don't think there's any dependency it can break, and it shouldn't affect the result either.

I also asked this on Stack Overflow and [Peter Cordes] has a great response (https://stackoverflow.com/a/72953035/14730360) explaining how this extra movzx is bad for the vast majority of X86-64 processors. IMHO newer versions of GCC should give newer processors more weight in performance tradeoff. Probably -mtune=generic in a later GCC shouldn't care about P6-family partial-register stalls. Practically there should be so few still using those CPUs to run latest compiled softwares.

Godbolt link with code for examples: https://godbolt.org/z/4n6ezaav7
Here's another example closer to what I was originally examining:

int foo(uint8_t* a, uint8_t i, uint8_t j) {
    return a[a[i] | a[j]];
}
gcc -O3 gives:
foo(unsigned char*, unsigned char, unsigned char):
        movzx   esi, sil
        movzx   edx, dl
        movzx   eax, BYTE PTR [rdi+rsi]
        or      al, BYTE PTR [rdi+rdx]
        movzx   eax, al
        movzx   eax, BYTE PTR [rdi+rax]
        ret

As was discussed in the Stack Overflow post, the first 2 movzx should be changed to use different registers so that some CPUs can have the benefit from mov elimination.

The "movzx   eax, al" just seems unnecessary. The upper bits of RAX should already be cleared, and the dependency of RAX on the "or" is not something that "movzx eax al" can break. So I think it's better to just do "movzx   eax, byte ptr [rdi + rax]" after the "or". Or maybe even better, just use "mov   eax, byte ptr [rdi + rax]" since EAX should already be free and cleaned in upper bits at this point.


---


### compiler : `gcc`
### title : `[13 regression] 456.hmmer at -Ofast -march=native regressed by 19% on zen2 and zen3 in July 2022`
### open_at : `2022-07-14T09:08:42Z`
### last_modified_date : `2023-08-10T16:01:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106293
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
The benchmark 456.hmmer from SPECINT 2006 suite has regressed on zen2 when compiled with -Ofast -march=native, with or without LTO. See:

  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=301.180.0
  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=289.180.0

On zen3, LNT only reported a similar regression with LTO:

  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=476.180.0


There may be some effect also on the Kabylake:
  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=2.180.0

On Zen2 (with LTO), I have manually bisected the regression to:
  d2a898666609452ef79a14feae1cadc3538e4b45 is the first bad commit
  commit d2a898666609452ef79a14feae1cadc3538e4b45
  Author: Richard Biener <rguenther@suse.de>
  Date:   Tue Jun 21 16:17:58 2022 +0200

    Put virtual operands into loop-closed SSA
    
    When attempting to manually update SSA form after high-level loop
    transforms such as loop versioning it is helpful when the loop-closed
    SSA form includes virtual operands.  While we have the special
    rewrite_virtuals_into_loop_closed_ssa function that doesn't
    presently scale, invoking update_ssa by itself.  So the following
    makes the regular loop-closed SSA form also cover virtual operands.
    For users of loop_version this allows to use cheaper
    TODO_update_ssa_no_phi, skipping dominance frontier compute
    (for the whole function) and iterated dominance frontiers for each
    copied def.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] stringop-overflow misbehaviour on atomic since r12-4725-g88b504b7a8c5affb`
### open_at : `2022-07-14T14:05:53Z`
### last_modified_date : `2023-05-08T12:25:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106297
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
repro steps

git clone https://github.com/haproxy/haproxy
cd haproxy

export CC=/path/to/gcc
make CC=$CC ERR=1 TARGET=linux-glibc 

error reported:

src/haproxy.c: In function ‘run_poll_loop’:
include/haproxy/atomic.h:428:39: error: ‘__atomic_load_8’ writing 8 bytes into a region of size 0 overflows the destination [-Werror=stringop-overflow=]
  428 | #define _HA_ATOMIC_LOAD(val)          __atomic_load_n(val, __ATOMIC_RELAXED)
      |                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
src/haproxy.c:2843:46: note: in expansion of macro ‘_HA_ATOMIC_LOAD’
 2843 |                                         if ((_HA_ATOMIC_LOAD(&ha_tgroup_ctx[i].stopping_threads) & ha_tgroup_info[i].threads_enabled) !=
      |                                              ^~~~~~~~~~~~~~~
compilation terminated due to -Wfatal-errors.




error was reviewed by Willy Tarreau in https://github.com/haproxy/haproxy/issues/1767 and it is considered as false positive.

I bisected gcc, breaking change is: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=88b504b7a8c5affb0ffa97990d22af2b199e36ed


---


### compiler : `gcc`
### title : `[13/14 Regression] 7.8% increased codesize on specfp 507.cactuBSSN_r`
### open_at : `2022-07-15T14:55:06Z`
### last_modified_date : `2023-07-27T09:23:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106315
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
when compiled with march_native_ofast_lto (-march=native -Ofast -funroll-loops -flto) on IceLake,CascadeLake, SkylakeW, Zen3 Server/Client,
r13-1268-g8c99e307b20c50 results 7.8%-7.9% codesize increment.

On aarch64 codesize looks ok.


---


### compiler : `gcc`
### title : `[Suboptimal] memcmp(s1, s2, n) == 0 expansion on AArch64 compare to llvm`
### open_at : `2022-07-16T09:55:02Z`
### last_modified_date : `2022-12-06T13:03:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106323
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
test case, see detail https://gcc.godbolt.org/z/PM3jxEM9M

```
#include <string.h>

int src(char* s1, char* s2) { 
  return memcmp(s1, s2, 3) == 0; 
}
```

* llvm doesn't emit branch with instruction cset
```
src:                                    // @src
        ldrh    w8, [x0]
        ldrh    w9, [x1]
        ldrb    w10, [x0, #2]
        ldrb    w11, [x1, #2]
        eor     w8, w8, w9
        eor     w9, w10, w11
        orr     w8, w8, w9
        cmp     w8, #0
        cset    w0, eq
        ret
```

* gcc
```
src:
        ldrh    w3, [x0]
        ldrh    w2, [x1]
        cmp     w3, w2
        beq     .L5
.L2:
        mov     w0, 1
        eor     w0, w0, 1
        ret
.L5:
        ldrb    w2, [x0, 2]
        ldrb    w0, [x1, 2]
        cmp     w2, w0
        bne     .L2
        mov     w0, 0
        eor     w0, w0, 1
        ret
```


---


### compiler : `gcc`
### title : `ptrue not reused between vector instructions and predicate instructions`
### open_at : `2022-07-16T13:05:50Z`
### last_modified_date : `2022-07-18T13:29:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106324
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
The following code has two use of `svptrue_b64()`s and none of the instructions using them should be clearning it so only one `ptrue` instruction should be needed.

```
svfloat64_t test(svbool_t pg, svfloat64_t a, svfloat64_t b)
{
    auto d = svdiv_m(svptrue_b64(), a, b);
    return svmul_m(svnot_z(svptrue_b64(), pg), d, d);
}
```

However, the code generated is,

```
        ptrue   p2.b, all
        ptrue   p1.d, all
        fdiv    z0.d, p2/m, z0.d, z1.d
        not     p0.b, p1/z, p0.b
        fmul    z0.d, p0/m, z0.d, z0.d
        ret
```

which has an extra `ptrue`.

OTOH, clang generates,

```
        ptrue   p1.d
        fdiv    z0.d, p1/m, z0.d, z1.d
        not     p0.b, p1/z, p0.b
        fmul    z0.d, p0/m, z0.d, z0.d
        ret
```

and the same `ptrue` is reused in both instructions.

This seems to be caused by gcc insisting on using `svptrue_b8` for the svnot which does not seem necessary here especially since _b64 is explicitly requested. Changing svptrue_b64 to svptrue_b8 in the code fixes the issue.


---


### compiler : `gcc`
### title : `_m and _z version of SVE instrinsics not optimized to predicate-free version`
### open_at : `2022-07-16T14:21:45Z`
### last_modified_date : `2023-04-14T11:33:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106326
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
The following code should generate a predicate-free fadd instruction since all the predicates are true.

```
svfloat64_t test(svfloat64_t a, svfloat64_t b)
{
    return svadd_m(svptrue_b64(), a, b);
}
```

but gcc instead generates an all-tree predicate and use that instead, i.e.

```
        ptrue   p0.b, all
        fadd    z0.d, p0/m, z0.d, z1.d
```

The same happens for the `_z` version as well with even worse code generated.

```
        ptrue   p0.b, all
        movprfx z0.d, p0/z, z0.d
        fadd    z0.d, p0/m, z0.d, z1.d
```

This optimization is only done for the `_x` variance. Clang optimizes this for all variance.


---


### compiler : `gcc`
### title : `side-effect-free _x variance not optimized to unpredicated instruction`
### open_at : `2022-07-16T20:11:38Z`
### last_modified_date : `2022-08-31T11:38:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106327
### status : `WAITING`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
Related to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106326 .

According to the Arm C Language Extension for SVE, when the _x predicate is used,

> The compiler can then pick whichever form of instruction seems to give the best code. This includes using unpredicated instructions, where available and suitable

Because of this, I'm expecting the following to be optimized to a single add instruction, as if a `svptrue_b64()` predicate is used.

```
svfloat64_t add(svfloat64_t a, svfloat64_t b)
{
    auto und_ok = svcmpge(svptrue_b64(), a, b);
    return svadd_x(und_ok, a, b);
}
```

However, gcc compiles this as _m and generates

```
        ptrue   p0.b, all
        fcmge   p0.d, p0/z, z0.d, z1.d
        fadd    z0.d, p0/m, z0.d, z1.d
```

In general, is there any reason not to treat an `add_x` (also other side-effect-free functions) with an unknown predicate as unpredicated one?


---


### compiler : `gcc`
### title : `No optimization for SVE pfalse predicate`
### open_at : `2022-07-16T20:16:58Z`
### last_modified_date : `2023-04-14T11:32:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106329
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
If a known-all-false predicate is used on an SVE intrinsic, the result should be fully no-op, undefined, zeroing and no actual instruction (other than potentially returning a zero) should be generated. This does not seem to be happening even when a `svpfalse_b()` is explicitly passed in as the predicate.

As an example,

```
svfloat64_t add(svfloat64_t a, svfloat64_t b)
{
    return svadd_x(svpfalse_b(), a, b);
}
```

is being compiled to
```
        pfalse  p0.b
        fadd    z0.d, p0/m, z0.d, z1.d
        ret
```

when it could simply be an empty function.


---


### compiler : `gcc`
### title : `flag set from SVE svwhilelt intrinsic not reused in loop`
### open_at : `2022-07-18T13:01:58Z`
### last_modified_date : `2022-07-20T14:35:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106340
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
I'm experimenting with manually writing VLA loops and trying to match the assembly code I expect/from autovectorizer. One of the main area I can't get it to work is when setting the loop predicate using the svwhilelt intrinsics. The instruction it corresponds to set the flags and can be directly used to terminate the loop. Indeed, when using the autovectorizer, this is exactly what happens.

```
void set1(uint32_t *__restrict__ out, size_t m)
{
    for (size_t i = 0; i < m; i++) {
        out[i] = 1;
    }
}
```

compiles to

```
        cbz     x1, .L1
        mov     x2, 0
        cntw    x3
        whilelo p0.s, xzr, x1
        mov     z0.s, #1
        .p2align 3,,7
.L3:
        st1w    z0.s, p0, [x0, x2, lsl 2]
        add     x2, x2, x3
        whilelo p0.s, x2, x1
        b.any   .L3
.L1:
        ret
```

(Here I believe the flag set from the loop header whilelo could also be used for the jump but that doesn't same much in this case.)

However, no matter how I trie to replicate this using manually written code using the sve intrinsics, there is always an additional cmp instruction generated. The closest I can get is by replicating the structure of the auto-vectorized loop as much as possible with,

```
void set2(uint32_t *__restrict__ out, size_t m)
{
    auto svelen = svcntw();
    auto v = svdup_u32(1);
    if (m != 0) {
        auto pg = svwhilelt_b32(0ul, m);
        for (size_t i = 0; i < m; i += svelen, pg = svwhilelt_b32(i, m)) {
            svst1(pg, &out[i], v);
        }
    }
}
```

which is compiled to

```
        cbz     x1, .L9
        mov     x2, 0
        cntw    x3
        whilelo p0.s, xzr, x1
        mov     z0.s, #1
        .p2align 3,,7
.L11:
        st1w    z0.s, p0, [x0, x2, lsl 2]
        add     x2, x2, x3
        whilelo p0.s, x2, x1
        cmp     x1, x2
        bhi     .L11
.L9:
        ret
```

which is literally the same code down to register allocation except that the branch following the `whilelo` instruction is replaced with another comparison and branch.


---


### compiler : `gcc`
### title : `SLP does not support no-op case`
### open_at : `2022-07-18T15:51:09Z`
### last_modified_date : `2022-07-25T11:41:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106343
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Created attachment 53316
Does not vectorize

The following test case:

  void foo (uint32_t dst[8], uint8_t src1[8], uint8_t src2[8])
  {
    uint16_t diff_e0 = src1[0] - src2[0];
    uint16_t diff_e1 = src1[1] - src2[1];
    uint16_t diff_e2 = src1[2] - src2[2];
    uint16_t diff_e3 = src1[3] - src2[3];
    uint16_t diff_e4 = src1[4] - src2[4];
    uint16_t diff_e5 = src1[5] - src2[5];
    uint16_t diff_e6 = src1[6] - src2[6];
    uint16_t diff_e7 = src1[7] - src2[7];

    uint32_t a0 = diff_e0 + 1;
    uint32_t a1 = diff_e1 + 3;
    uint32_t a2 = diff_e2 + 4;
    uint32_t a3 = diff_e3 + 2;
    uint32_t a4 = diff_e4 + 12;
    uint32_t a5 = diff_e5 + 11;
    uint32_t a6 = diff_e6 + 9;
    uint32_t a7 = diff_e7 + 3;

    dst[0] = a0;
    dst[1] = a1;
    dst[2] = a2;
    dst[3] = a3;
    dst[4] = a4;
    dst[5] = a5;
    dst[6] = a6;
    dst[7] = a7;
  }

Produces nice vectorized code on aarch64:

  ldr     d2, [x2]
  adrp    x3, .LC0
  ldr     d0, [x1]
  ldr     q1, [x3, #:lo12:.LC0]
  usubl   v0.8h, v0.8b, v2.8b
  uaddl   v2.4s, v0.4h, v1.4h
  uaddl2  v0.4s, v0.8h, v1.8h
  stp     q2, q0, [x0]
  ret

But if any of the constants is replaced with zero instead then scalar code is produced:

  ldrb    w4, [x2, 1]
  ldrb    w8, [x1, 1]
  ldrb    w3, [x2, 3]
  ldrb    w7, [x1, 3]
  sub     w8, w8, w4
  ldrb    w6, [x1, 4]
  and     w8, w8, 65535
  ldrb    w4, [x2, 4]
  sub     w7, w7, w3
  ldrb    w5, [x1, 5]
  and     w7, w7, 65535
  ldrb    w3, [x2, 5]
  sub     w6, w6, w4
  ldrb    w9, [x2, 6]
  and     w6, w6, 65535
  ldrb    w4, [x1, 6]
  sub     w5, w5, w3
  ldrb    w10, [x2, 7]
  and     w5, w5, 65535
  ldrb    w3, [x1, 7]
  sub     w4, w4, w9
  ldrb    w11, [x2]
  and     w4, w4, 65535
  ldrb    w9, [x1]
  sub     w3, w3, w10
  ldrb    w2, [x2, 2]
  add     w8, w8, 3
  ldrb    w10, [x1, 2]
  sub     w9, w9, w11
  and     w1, w3, 65535
  and     w9, w9, 65535
  sub     w10, w10, w2
  add     w3, w5, 11
  add     w2, w4, 9
  add     w7, w7, 2
  add     w6, w6, 12
  add     w1, w1, 3
  add     w4, w9, 1
  and     w5, w10, 65535
  stp     w4, w8, [x0]
  stp     w5, w7, [x0, 8]
  stp     w6, w3, [x0, 16]
  stp     w2, w1, [x0, 24]
  ret

It would be possible to produce the same vectorized code as above but with zero in the constants. If I understand correctly, the identity element of addition is not taken into consideration in the SLP vectorizer, which could be improved. The same happens with subtraction.

I can reproduce this in any recent version of GCC (e.g. >= 10).

Vectorized case: https://godbolt.org/z/5sbb1an89
Scalar case:     https://godbolt.org/z/v8jPT9jEe


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Potential regression on vectorization of left shift with constants since r11-5160-g9fc9573f9a5e94`
### open_at : `2022-07-18T18:28:10Z`
### last_modified_date : `2023-08-04T13:58:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106346
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53317
Does not vectorize on GCC > 10.3

The following test case:

  void foo (uint32_t dst[8], uint8_t src1[8], uint8_t src2[8])
  {
    uint16_t diff_e0 = src1[0] - src2[0];
    uint16_t diff_e1 = src1[1] - src2[1];
    uint16_t diff_e2 = src1[2] - src2[2];
    uint16_t diff_e3 = src1[3] - src2[3];
    uint16_t diff_e4 = src1[4] - src2[4];
    uint16_t diff_e5 = src1[5] - src2[5];
    uint16_t diff_e6 = src1[6] - src2[6];
    uint16_t diff_e7 = src1[7] - src2[7];

    uint32_t a0 = diff_e0 << 1;
    uint32_t a1 = diff_e1 << 3;
    uint32_t a2 = diff_e2 << 4;
    uint32_t a3 = diff_e3 << 2;
    uint32_t a4 = diff_e4 << 12;
    uint32_t a5 = diff_e5 << 11;
    uint32_t a6 = diff_e6 << 9;
    uint32_t a7 = diff_e7 << 3;

    dst[0] = a0;
    dst[1] = a1;
    dst[2] = a2;
    dst[3] = a3;
    dst[4] = a4;
    dst[5] = a5;
    dst[6] = a6;
    dst[7] = a7;
  }

Compiles at -O3 to nice vectorized code by loading the constants from memory in GCC 10.3:

  ldr     d0, [x1]
  adrp    x3, .LC0
  ldr     d1, [x2]
  adrp    x1, .LC1
  ldr     q3, [x3, #:lo12:.LC0]
  usubl   v0.8h, v0.8b, v1.8b
  ldr     q2, [x1, #:lo12:.LC1]
  uxtl    v1.4s, v0.4h
  uxtl2   v0.4s, v0.8h
  sshl    v1.4s, v1.4s, v3.4s
  sshl    v0.4s, v0.4s, v2.4s
  stp     q1, q0, [x0]
  ret

But this has regressed in later releases, with GCC still loading the constants from memory but also emitting a lot of scalar code before that. For example GCC 13 produces:

  adrp    x3, .LC0
  ldrb    w6, [x1, 4]
  fmov    d0, x6
  ldrb    w7, [x1]
  ldr     q5, [x3, #:lo12:.LC0]
  fmov    d1, x7
  ldrb    w3, [x1, 5]
  ldrb    w4, [x1, 1]
  ldrb    w8, [x2, 4]
  ldrb    w5, [x2, 5]
  ins     v0.h[1], w3
  ldrb    w6, [x2]
  fmov    d2, x8
  ldrb    w3, [x2, 1]
  fmov    d3, x6
  ins     v2.h[1], w5
  ins     v1.h[1], w4
  ldrb    w9, [x1, 2]
  ins     v3.h[1], w3
  ldrb    w8, [x1, 6]
  ldrb    w7, [x2, 2]
  ldrb    w6, [x2, 6]
  ins     v1.h[2], w9
  ins     v0.h[2], w8
  ldrb    w5, [x1, 3]
  ins     v3.h[2], w7
  ldrb    w4, [x1, 7]
  ins     v2.h[2], w6
  ldrb    w1, [x2, 7]
  ldrb    w3, [x2, 3]
  ins     v1.h[3], w5
  ins     v0.h[3], w4
  ins     v2.h[3], w1
  ins     v3.h[3], w3
  adrp    x1, .LC1
  ldr     q4, [x1, #:lo12:.LC1]
  sub     v1.4h, v1.4h, v3.4h
  sub     v0.4h, v0.4h, v2.4h
  uxtl    v1.4s, v1.4h
  uxtl    v0.4s, v0.4h
  sshl    v1.4s, v1.4s, v5.4s
  sshl    v0.4s, v0.4s, v4.4s
  stp     q1, q0, [x0]
  ret

Interestingly, this happens only with left shift and not with right shift.

GCC 10.3 vs trunk comparison: https://godbolt.org/z/xWbfGdfen


---


### compiler : `gcc`
### title : `Miss to handle ifn .LEN_STORE in FRE`
### open_at : `2022-07-20T05:25:26Z`
### last_modified_date : `2022-07-21T11:24:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106365
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
In regression testing for the patch to add unroll factor suggestion to vectorizer for port rs6000, one failure got exposed on Power10 (with partial vector in length supported). 

The test case is gcc/testsuite/gcc.dg/tree-ssa/pr84512.c

The option can be: -O3 -mcpu=power10 -fno-vect-cost-model

The resulted IR in optimized:

  <bb 2> [local count: 97603129]:
  MEM <vector(4) int> [(int *)&a] = { 0, 1, 4, 9 };
  MEM <vector(4) int> [(int *)&a + 16B] = { 16, 25, 36, 49 };
  .LEN_STORE (&MEM <int[10]> [(void *)&a + 32B], 128B, 8, { 64, 0, 0, 0, 81, 0, 0, 0, 100, 0, 0, 0, 121, 0, 0, 0 }, 0);
  vect__2.10_6 = MEM <vector(4) int> [(int *)&a];
  vect__2.10_30 = MEM <vector(4) int> [(int *)&a + 16B];
  vect_res_10.11_31 = vect__2.10_6 + vect__2.10_30;
  _33 = VEC_PERM_EXPR <vect_res_10.11_31, { 0, 0, 0, 0 }, { 2, 3, 4, 5 }>;
  _34 = vect_res_10.11_31 + _33;
  _35 = VEC_PERM_EXPR <_34, { 0, 0, 0, 0 }, { 1, 2, 3, 4 }>;
  _36 = _34 + _35;
  stmp_res_10.12_37 = BIT_FIELD_REF <_36, 32, 0>;
  _13 = a[8];
  res_3 = _13 + stmp_res_10.12_37;
  _8 = a[9];
  res_23 = res_3 + _8;
  a ={v} {CLOBBER(eol)};
  return res_23;

instead of:

int foo ()
{
  <bb 2> [local count: 97603129]:
  return 285;

}


---


### compiler : `gcc`
### title : `[13 regreesion] Lowering complex type mov regressed loop distribution for memset/memcpy/memmov.`
### open_at : `2022-07-21T03:21:17Z`
### last_modified_date : `2023-07-27T09:23:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106375
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
void
foo (_Complex double* p, _Complex double* __restrict q)
{
  for (int i = 0; i != 1000; i++)
    p[i] = q[i];
}

before lowering complex type move


  <bb 2> [local count: 10737416]:
  __builtin_memcpy (p_10(D), q_9(D), 16000);
  return;


After r13-1762-gf9d4c3b45c5ed5f45c8089c990dbd4e181929c3d, it's 

18  <bb 3> [local count: 1063004409]:
19  # i_15 = PHI <i_12(5), 0(2)>
20  # ivtmp_13 = PHI <ivtmp_5(5), 1000(2)>
21  _1 = (long unsigned int) i_15;
22  _2 = _1 * 16;
23  _3 = q_9(D) + _2;
24  _4 = p_10(D) + _2;
25  _7 = REALPART_EXPR <*_3>;
26  _6 = IMAGPART_EXPR <*_3>;
27  REALPART_EXPR <*_4> = _7;
28  IMAGPART_EXPR <*_4> = _6;
29  i_12 = i_15 + 1;
30  ivtmp_5 = ivtmp_13 - 1;
31  if (ivtmp_5 != 0)
32    goto <bb 5>; [99.00%]
33  else


---


### compiler : `gcc`
### title : `Miss to handle ifn .LEN_STORE in DSE`
### open_at : `2022-07-21T07:54:54Z`
### last_modified_date : `2022-07-21T11:24:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106378
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #106365 +++

In PR106365, Richi has fixed the miss optimization on {MASK/LEN}_STORE by teaching VN about these IFNs. But it further exposes one miss optimization in DSE for LEN_STORE.

With the option -O3 -mcpu=power10 -fno-vect-cost-model, check the optimized dumpings.

--------------

for the case gcc/testsuite/gcc.dg/tree-ssa/pr84512.c, we expect to get:

int foo ()
{
  <bb 2> [local count: 97603129]:
  return 285;
}

but it gets now:

int foo ()
{
  int a[10];

  <bb 2> [local count: 97603129]:
  .LEN_STORE (&MEM <int[10]> [(void *)&a + 32B], 128B, 8, { 64, 0, 0, 0, 81, 0, 0, 0, 100, 0, 0, 0, 121, 0, 0, 0 }, 0);
  a ={v} {CLOBBER(eol)};
  return 285;

}

--------------

for the case Richi posted:

int __attribute__((noinline,noclone))
foo ()
{
  int out[10];
  int i;
  for (i = 0; i < 10; ++i)
    {
      out[i] = i;
    }
  return out[9];
}

it gets:

__attribute__((noclone, noinline))
int foo ()
{
  int out[10];

  <bb 2> [local count: 97603129]:
  .LEN_STORE (&MEM <int[10]> [(void *)&out + 32B], 128B, 8, { 8, 0, 0, 0, 9, 0, 0, 0, 10, 0, 0, 0, 11, 0, 0, 0 }, 0);
  out ={v} {CLOBBER(eol)};
  return 9;

}


---


### compiler : `gcc`
### title : `DCE depends on order`
### open_at : `2022-07-21T09:13:15Z`
### last_modified_date : `2023-10-25T22:42:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106379
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
In some cases, the compiler's ability to eliminate dead code depends on the order of the sub-expressions within the if.

GCC detects that the if expression in the following code snippet evaluates to false and thus removes the dead code:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(bool c, bool s) {
    if (((!c == !s) && !(c) && s)) {
        DCEMarker0_();
    }
}

In the following snippet the expressions are swapped (the semantics stay the same). However, GCC cannot eliminate the dead code anymore:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(bool c, bool s) {
    if ((!s == !c) && s && !(c)) {
        DCEMarker0_();
    }
}


This can also be seen via the following Compiler Explorer link: https://godbolt.org/z/vTqhc46qY


---


### compiler : `gcc`
### title : `DCE depends on datatype used (bool vs unsigned)`
### open_at : `2022-07-21T09:23:41Z`
### last_modified_date : `2023-10-25T22:45:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106380
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
In some cases, the compiler's ability to eliminate dead code depends on the datatype used (bool vs unsigned).

GCC detects that the if expression in the following code snippet evaluates to false and thus removes the dead code:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(bool s, bool c) {
    if (((!s == !c) && c && !(s))) {
        DCEMarker0_();
    }
}

In the following snippet the datatype is switched to unsigned (the semantics stay the same). However, GCC cannot eliminate the dead code anymore:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(unsigned s, unsigned c) {
    if (((!s == !c) && c && !(s))) {
        DCEMarker0_();
    }
}


This can also be seen via the following Compiler Explorer link:  https://godbolt.org/z/aMcEvEaYa

(Might be related to 106379)


---


### compiler : `gcc`
### title : `DCE depends on used programming language (C vs C++)`
### open_at : `2022-07-21T09:33:07Z`
### last_modified_date : `2023-10-25T22:43:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106381
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
In some cases, the compiler's ability to eliminate dead code depends on the used language (C vs C++).

GCC detects that the if expression in the following code snippet evaluates to false and thus removes the dead code. The code is compiled as C++ code:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(unsigned s, unsigned c) {
    if (((!s == !c) && c && !(s))) {
        DCEMarker0_();
    }
}

However, if the same code is compiled as C code, GCC cannot eliminate the dead code anymore.

This can also be seen via the following Compiler Explorer link: https://godbolt.org/z/a9ned9Exb

(Might be related to 106379 and 106380)


---


### compiler : `gcc`
### title : `loop-ivopts prevents correct usage of dbra with 16-bit loop counters on m68k`
### open_at : `2022-07-22T17:32:05Z`
### last_modified_date : `2022-08-01T23:44:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106415
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
Created attachment 53338
C file that reproduces the problem.

When targeting m68k and compiling certain loops with 16-bit counters that should trivially generate a DBRA instruction, GCC's optimization passes end up converting the IV to 32-bit, which  requires extra logic to check the upper half. More specifically, these are loops where the number of iterations is known at compile time.

This additional code is completely useless since we know the loop count fits in 16 bits.

I am using GCC 11.2.0 hosted on ARM64 macOS and targeting m68k. All code snippets were compiled with `-O3 -std=c99 -march=68000 -mtune=68000`.

Consider the following function:
void dbra_test1(short i) {
    do {
        foo(i);
    } while(--i != -1);
}

As expected, the generated body is a tiny loop consisting solely of call setup, the call itself, call cleanup, and a DBRA:
.L2:
        movew %d2,%a0
        movel %a0,%sp@-
        jsr %a2@
        addql #4,%sp
        dbra %d2,.L2

Now consider this function, where we change the initial value of the loop count to be a constant:
void dbra_test2(void) {
    short i = 15;
    do {
        foo(i);
    } while(--i != -1);
}

GCC generates the following code for the body of the loop:
.L7:
        movel %d2,%sp@-
        jsr %a2@
        addql #4,%sp
        dbra %d2,.L7
        clrw %d2
        subql #1,%d2
        jcc .L7

Note the extraneous clr/subq/jcc.

During ivcanon, GCC transforms the second loop to run from 16 to 0 instead of 15 to -1. Later during ivopts, it transforms back into 15 to -1 form, but promotes the variable from short to int. Future transformations are no longer able to optimize around the short variable, and we end up with extraneous checks inserted during codegen.

I've attached a simple file that reproduces the problem. GCC 2.95.3 performed the operation correctly, but it's been broken since at least 4.3.2, possibly earlier.

Thanks
--UD2


---


### compiler : `gcc`
### title : `Missed optimization for comparisons`
### open_at : `2022-07-23T09:25:36Z`
### last_modified_date : `2022-07-25T02:35:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106420
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53339
Sample code

When comparing different variables to the same constants, in some cases the compiler could first combine the variables and then do a single compare.  In the sample given, two variables are compared against 7.  In the slow path, GCC produces the following with -O2.

        cmp     edi, 7
        setg    al
        cmp     esi, 7
        setg    dl
        or      eax, edx
        movzx   eax, al
        ret

In the fast path, GCC produces this instead.

        or      edi, esi
        xor     eax, eax
        cmp     edi, 7
        setg    al
        ret

Although the expression a > 7 || b > 7 is the same as (a | b) > 7, the latter is better because it results in fewer instructions.  A quick experiment shows the latter also runs quite faster.

Verified with Godbolt for GCC trunk.  Clang, ICC, and MSVC latest versions also miss this opportunity as per Godbolt.


---


### compiler : `gcc`
### title : `RISC-V suboptimal codegen for large constants`
### open_at : `2022-07-25T20:25:51Z`
### last_modified_date : `2022-07-25T20:39:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106439
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
As part of experiments to create smaller tests for  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106265, ran into a slightly different issue involving large constants (> S12) and stack access.

RISC-V codegen for stack accesses with offsets > S12 is terrible. 

long foo(void)
{
        long a[512];
        return a[500];
}

output for rv64 -O2 : gcc trunk as of June 14 (commit 6abe341558ab)

foo:
	li	t0,-4096
	li	a5,4096
	add	sp,sp,t0
	addi	a5,a5,-96
	add	a5,a5,sp
	li	t0,4096
	ld	a0,0(a5)
	add	sp,sp,t0
	jr	ra

Compare the same code to aarch64:

foo:
	sub	sp, sp, #4096
	ldr	x0, [sp, 4000]
	add	sp, sp, 4096

Of course part of the problem is limited S12 offset encoding in RV LD, but the compiler can definitely do much better.


---


### compiler : `gcc`
### title : `Redundant zero extension after crc32q`
### open_at : `2022-07-27T09:55:09Z`
### last_modified_date : `2022-09-05T18:04:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106453
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
On 64-bit x86, straightforward use of SSE 4.2 crc instruction looks like

#include <immintrin.h>
#include <stdint.h>

uint32_t f(uint32_t c, uint64_t *p, size_t n)
{
    for (size_t i = 0; i < n; i++)
        c = _mm_crc32_u64(c, p[i]);
    return c;
}

On the ISA level, the crc32q instruction takes 64-bit operands, and resulting assembly is (gcc -O2 -msse4.2):

f:
        mov     eax, edi
        test    rdx, rdx
        je      .L1
        lea     rdx, [rsi+rdx*8]
.L3:
        mov     eax, eax
        add     rsi, 8
        crc32   rax, QWORD PTR [rsi-8]
        cmp     rdx, rsi
        jne     .L3
.L1:
        ret

Note zero-extension of 'eax' (which is usually not move-eliminated since destination is the same as source).

The crc32q instruction zero-extends rax from the 32-bit result (it also ignores high 32 bits when reading the destination operand), so I think it should be possible to model zero extension in the .md pattern, allowing to eliminate the explicit extension.

A source-level workaround is using a 64-bit variable in the loop, so the extension happens just once before the loop.


---


### compiler : `gcc`
### title : `-march=generic might want to remove X86_TUNE_AVOID_FALSE_DEP_FOR_BMI now`
### open_at : `2022-07-28T23:37:50Z`
### last_modified_date : `2023-05-19T01:53:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106471
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.1`
### severity : `normal`
### contents :
So this actually started out as a clang issue report about bad inline asm input argument behavior at

   https://github.com/llvm/llvm-project/issues/56789

but as part of that I was looking at __builtin_ctzl() and while gcc DTRT for the inline asm version we use in the kernel, the builtin acts very oddly indeed.

IOW, this code:

        unsigned long test(unsigned long arg)
        {
                return __builtin_ctzl(arg);
        }

generates this odd result with 'gcc -O2 -S':

	xorl	%eax, %eax
	rep bsfq	%rdi, %rax
	cltq
	ret

where the xor and the cltq both just confuse me.


---


### compiler : `gcc`
### title : `DCE depends on how if expressions are nested`
### open_at : `2022-07-29T08:24:57Z`
### last_modified_date : `2022-08-02T20:01:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106474
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
In some cases, the compiler's ability to eliminate dead code depends on how if expressions are nested.

GCC detects that the if expressions in the following code snippet evaluate to false and thus removes the dead code:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(bool s, bool c) {
    if (!c == !s) {
        if (!c && s) {
            DCEMarker0_();
        }
    }
}

In the following snippet the !c term is moved into the outer if expression. However, GCC cannot eliminate the dead code anymore:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(bool s, bool c) {
    if ((!c == !s) && !c) {
        if (s) {
            DCEMarker0_();
        }
    }
}

In the following snippet, only the outer if statement is kept and all terms are moved there. GCC is now able to eliminate the code again:

#include <stdio.h>
#include <stdbool.h>

static void __attribute__ ((noinline)) DCEMarker0_() {printf("DCE2.0");}

void f(bool s, bool c) {
    if (((!c == !s) && !c) && s) {
        DCEMarker0_();
    }
}

This can also be seen via the following Compiler Explorer link: https://godbolt.org/z/9WPn5cG1W


---


### compiler : `gcc`
### title : `Failure to optimize uint64_t/constant division on ARM32`
### open_at : `2022-07-30T18:38:29Z`
### last_modified_date : `2023-07-08T18:10:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106484
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `enhancement`
### contents :
The following test function compiles into a call to __aeabi_uldivmod, even though the divisor is a constant. Here's an example function:

    #include <stdint.h>
    uint64_t ns_to_s( uint64_t ns64 )
    {
    return ns64 / 1000000000ULL;
    }

CortexM4(-O3 -Wall -Wextra -mcpu=cortex-m4) assembly:

ns_to_s(unsigned long long):
        push    {r3, lr}
        adr     r3, .L4
        ldrd    r2, [r3]
        bl      __aeabi_uldivmod
        pop     {r3, pc}
.L4:
        .word   1000000000
        .word   0

Interestingly, gcc 12.1 for aarch64 compiles the above C function by implementing division by a constant with scaled multiplication by the inverse using the umulh instruction(not present on 32-bit ARM). (-O3 -Wall -Wextra):

ns_to_s(unsigned long):
        mov     x1, 23123
        lsr     x0, x0, 9
        movk    x1, 0xa09b, lsl 16
        movk    x1, 0xb82f, lsl 32
        movk    x1, 0x44, lsl 48
        umulh   x0, x0, x1
        lsr     x0, x0, 11
        ret

Instead, if something like __umulh could be added to libgcc, then GCC could use the constants generated in the aarch64 logic to implement uint64_t/constant division. Example umulh approach is attached. On Cortex-M4, the umulh-based approach is significantly faster, although this depends on the specific libc __aeabi_uldivmod linked against as well as the numerator.


---


### compiler : `gcc`
### title : `Three calls to __attribute__((const)) function`
### open_at : `2022-08-01T23:04:58Z`
### last_modified_date : `2022-08-04T06:28:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106502
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `13.0`
### severity : `normal`
### contents :
#include <system_error>

int main()
{
  __builtin_puts(std::system_category().name());
}

With gcc 12 the resulting assembly is:

	.file	"cat.C"
	.text
	.globl	main
	.type	main, @function
main:
.LFB1309:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$8, %rsp
	.cfi_offset 3, -24
	call	_ZNSt3_V215system_categoryEv
	call	_ZNSt3_V215system_categoryEv
	movq	(%rax), %rax
	addq	$16, %rax
	movq	(%rax), %rbx
	call	_ZNSt3_V215system_categoryEv
	movq	%rax, %rdi
	call	*%rbx
	movq	%rax, %rdi
	call	puts
	movl	$0, %eax
	movq	-8(%rbp), %rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1309:
	.size	main, .-main
	.ident	"GCC: (GNU) 12.1.1 20220507 (Red Hat 12.1.1-1)"
	.section	.note.GNU-stack,"",@progbits


Why is the std::system_category() function called three times?


---


### compiler : `gcc`
### title : `DCE depends on whether if or else is used`
### open_at : `2022-08-02T11:38:35Z`
### last_modified_date : `2023-10-25T22:43:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106505
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
In some cases, the compiler's ability to eliminate dead code depends on whether the if expression is left as it is or negated and the body moved to the else block. 

GCC detects that the if expressions in the following code snippet evaluate to false and thus removes the dead code:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(bool s, bool c) {
    if (!c == !s) {
        if (s && !c) {
            DCEMarker0_();
        }
    }
}

In the following snippet, the inner if has been negated and the body has been moved to the else block. However, GCC cannot eliminate the dead code anymore:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(bool s, bool c) {
    if (!c == !s) {
        if (!(s && !c)) {}
        else {
            DCEMarker0_();
        }
    }
}

This can also be seen via the following Compiler Explorer link: https://godbolt.org/z/P4a61nsdq

If s and c in the inner if expression are swapped, it is the other way round. Without the else, the compiler cannot eliminate the dead code:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(bool s, bool c) {
    if (!c == !s) {
        if (c && !s) {
            DCEMarker0_();
        }
    }
}

However, it suddenly can optimise the code when the if expression is negated and the body is moved to the else:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(bool s, bool c) {
    if (!c == !s) {
        if (!(c && !s)) {}
        else {
            DCEMarker0_();
        }
    }
}

This can also be seen via the following Compiler Explorer link: https://godbolt.org/z/95jo3Gv45

Due to the order issue this might be related to the following bug https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106379

However, this example only works with C as a source language. With C++, it is fine. Thus, this might be related to the following bug https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106381

Another example is given below. In this case, only one variable is needed. However, this only works with the datatype unsigned instead of bool. Therefore, it might be related to the following bug https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106380

Version with dead code eliminated:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(unsigned s) {
    if (!s == !!s) {
        DCEMarker0_();
    }
}

Version with dead code not eliminated:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(unsigned s) {
    if (!(!s == !!s)) {}
    else {
        DCEMarker0_();
    }
}

This can also be seen via the following Compiler Explorer link: https://godbolt.org/z/zYfoTs3PE


---


### compiler : `gcc`
### title : `[13 Regression] g++.dg/opt/pr94589-2.C FAILS after enabling floats in VRP`
### open_at : `2022-08-02T13:02:36Z`
### last_modified_date : `2022-08-12T11:44:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106506
### status : `RESOLVED`
### tags : `missed-optimization, xfail`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
As mentioned in this thread, enabling frange operators in range-op-float.cc causes g++.dg/opt/pr94589-2.C to fail:

https://gcc.gnu.org/pipermail/gcc/2022-July/239207.html

I've marked it as XFAIL.

It's all yours Jakub ;-).


---


### compiler : `gcc`
### title : `[13/14 Regression] New -Werror=maybe-uninitialized since r13-1268-g8c99e307b20c502e`
### open_at : `2022-08-03T09:14:02Z`
### last_modified_date : `2023-08-25T23:37:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106511
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53404
test-case

Knowing the warning has a significant false-positive rate, but still, it may be an interesting test-case. It's reduced from xen package:

$ gcc bunzip.i -Werror=maybe-uninitialized -O1
bunzip2.c: In function ‘get_next_block’:
bunzip2.c:261:27: error: ‘length’ may be used uninitialized [-Werror=maybe-uninitialized]
bunzip2.c:224:17: note: ‘length’ declared here
cc1: some warnings being treated as errors


---


### compiler : `gcc`
### title : `RISC-V: Inefficient Generated Code for Floating Point to Integer Rounds`
### open_at : `2022-08-03T17:32:12Z`
### last_modified_date : `2022-08-04T09:28:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106517
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
RISC-V has a handful of floating-point conversion instructions that we don't appear to be taking advantage of.  For example

long f(double in)
{
    return __builtin_floor(in);
}

generates a call to the floor() library routine, while I believe we can implement in via just a "fcvt.l.d a0, fa0, rdn" (RISC-V clang and arm64 GCC).  There are a bunch of similar patterns, the aarch64 test suite seems to have pretty good coverage of them.

We should port those tests over to RISC-V, figure out which conversions we can implement directly, and then fix whatever's broken.  I started poking around a bit and found that even some of the conversions where we have MD file patterns aren't behaving as expected, so there might be some deeper issue going on.

This has come up in a handful of forums lately and while we're still hoping to find some time to look into it, I figured it'd be best to open at least a basic bug so at least we can have one place to track the issues.


---


### compiler : `gcc`
### title : `Exchange/swap aware register allocation (generate xchg in reload)`
### open_at : `2022-08-03T20:14:11Z`
### last_modified_date : `2023-04-26T08:11:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106518
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
This enhacement request is a proposal for improving/tweaking GCC's register allocation, but assuming/making use of a register exchange/swap operation as a useful abstraction.  Currently reload/lra is (solely) "move"-based, so when the contents of regA need to be placed in regB and the original contents of regB need to be placed in regA, they make use of a temporary register (or a spill) and generate the classic sequence: tmp=regA; regA=regB; regB=tmp.

A small improvement is to tweak register allocation to assume, as a higher level abstraction, the existence of an exchange/swap instruction, like x86's xchg, much like is assummed/used during the reg-stack pass (with i387's fxch).  [https://gcc.gnu.org/legacy-ml/gcc-patches/2004-12/msg00815.html]

During early register allocation, we introduce virtual exchange operations, that on can be lowered as a later pass, either to real exchange operations on targets that support them, or to the standard three-move shuffle sequence above, if there's a spare suitable temporary register, or alternatively to the sequence regA^=regB; regB^=regA; regA^=regB, which implements an exchange using three fast instructions without requiring an additional register.  These three alternatives guarantee that register allocation is no worse than current, but has the flexibility to use fewer registers and perhaps fewer instructions.
On modern hardware, xchg is sometimes zero latency (using register renaming), and on older architectures, a three xor sequence has the same latency as three moves, but requires on less register, helpfully reducing register pressure.

An example application/benefit of this PR rtl-optimization/97756, which demonstrates that the x86_64 ABI frequently places (TImode double word)
registers in locations that then neeed the high and low parts to be swapped
(or moved) to place them in the (reg X) and (reg X+1) locations required by
GCC's multi-word register allocation requirements.

Interestingly, GCC's middle-end doesn't have a standard named pattern for an exchange/swap instruction, i.e. an optab, so currently it has no (easy) way of deciding whether a target has an xchg-like instruction, which helps explain why it doesn't currently use/generate them.


---


### compiler : `gcc`
### title : `s390: Inefficient branchless conditionals for unsigned long long`
### open_at : `2022-08-04T13:15:44Z`
### last_modified_date : `2022-08-05T08:25:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106525
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
Created attachment 53409
source code

1)  -(a > b)

        clgr    %r2,%r3
        lhi     %r2,0
        alcr    %r2,%r2
        sllg    %r2,%r2,63
        srag    %r2,%r2,63

Last 2 could be merged to LCDFR. But optimal is:

        slgrk %r2,%r3,%r2
        slbgr %r2,%r2
        lgfr  %r2,%r2
Note: lgfr is not required => 2 instructions only.

2) -(a <= b)

        slgr    %r3,%r2
        lhi     %r2,0
        alcr    %r2,%r2
        sllg    %r2,%r2,63
        srag    %r2,%r2,63

Last 2 could be merged to LCDFR. But optimal is:

        clgr %r2,%r3
        slbgr %r2,%r2
        lgfr    %r2,%r2

Note: lgfr is not required => 2 instructions only.

3) unsigned 64-bit compare for qsort (a > b) - (a < b)

        clgr    %r2,%r3
        lhi     %r1,0
        alcr    %r1,%r1
        clgr    %r3,%r2
        lhi     %r2,0
        alcr    %r2,%r2
        srk     %r2,%r1,%r2
        lgfr    %r2,%r2

Optimal:
        slgrk %r1,%r2,%r3
        slgrk 0,%r3,%r2
        slbgr %r2,%r3
        slbgr %r1,%r2
        lgfr  %r2,%r1

Note: lgfr not required => 4 instructions only


---


### compiler : `gcc`
### title : `loop distribution not distributing inner loop (to memcpy) when perfect loop nest`
### open_at : `2022-08-05T07:22:36Z`
### last_modified_date : `2022-08-16T16:48:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106533
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53415
test case

When tinkering with a slightly modified version of stream benchmark [1] observed that Loop distribution is not distributing a nested copy loop into "0 loop and 1 libcall (memcpy)".

This is with test built with -O2, mainline gcc, as of June 14, 2022: commit 6abe341558ab

Actual test is attached but the loops look like following. Loop 7 (copy) is distributed to memcpy in general case - but not if benchmark built with #define COPYONLY (which elides loops 8,9,10 from compilation).

-->8---
    for (j=0; j<10000000; j++) {                            // 1
        a[j] = 1.0;
        b[j] = 2.0;
        c[j] = 0.0;
    }

    for (j = 0; j < 10000000; j++)                          // 2
        a[j] = 2.0E0 * a[j];

    for (k=0; k<10; k++)                                    // 3
    {
        for (j=0; j<10000000; j++) c[j] = a[j];             // 7
#ifndef COPYONLY
        for (j=0; j<10000000; j++) b[j] = scalar*c[j];      // 8
        for (j=0; j<10000000; j++) c[j] = a[j]+b[j];        // 9
        for (j=0; j<10000000; j++) a[j] = b[j]+scalar*c[j]; // 10
#endif
    }

    for (k=1; k<10; k++)
	for (j=0; j<4; j++)			            // 6
	    avgtime[j] = avgtime[j] + times[j][k];
            ..

    for (j=0; j<4; j++)	                                    // 5
	avgtime[j] = avgtime[j]/(double)(NTIMES-1);
            ..

-->8---


---


### compiler : `gcc`
### title : `P9: gcc does not detect setb pattern`
### open_at : `2022-08-05T10:17:54Z`
### last_modified_date : `2022-11-16T03:18:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106536
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
int compare2(unsigned long long a, unsigned long long b)
{
    return (a > b ? 1 : (a < b ? -1 : 0));
}

Output:
_Z8compare2yy:
        cmpld 0,3,4
        bgt 0,.L5
        mfcr 3,128
        rlwinm 3,3,1,1
        neg 3,3
        blr
.L5:
        li 3,1
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0

clang generates:

_Z8compare2yy:                          # @_Z8compare2yy
        cmpld   3, 4
        setb 3, 0
        extsw 3, 3
        blr
        .long   0
        .quad   0


---


### compiler : `gcc`
### title : `peephole.md seems like it should not exist`
### open_at : `2022-08-06T04:59:23Z`
### last_modified_date : `2023-05-19T05:14:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106545
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
There is one peephole2 in there for:
Simplify (unsigned long)(unsigned int)a << const

But that should really be handled in generic code in simplify-rtx/combine already.
If it is not, then it should be added.


---


### compiler : `gcc`
### title : `[rs6000] sub-optimal 64bit constant generation for P10`
### open_at : `2022-08-08T02:34:18Z`
### last_modified_date : `2022-09-15T07:31:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106550
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
There is 'pli' which supports a 34bits immediate, so to generate a 64bits constant we just need 3 instructions at most.

void
foo (unsigned long long *a)
{
  *a = 0x020805006106003;
}

On the trunk, below asm is generated:

        .file   "test.c"
        .machine power10
        .abiversion 2
        .section        ".text"
        .align 2
        .p2align 4,,15
        .globl foo
        .type   foo, @function
foo:
.LFB0:
        .cfi_startproc
        .localentry     foo,1
        lis 9,0x20
        ori 9,9,0x8050
        sldi 9,9,32
        oris 9,9,0x610
        ori 9,9,0x6003
        std 9,0(3)
        blr
        .long 0
        .byte 0,0,0,0,0,0,0,0
        .cfi_endproc
.LFE0:
        .size   foo,.-foo
        .ident  "GCC: (GNU) 13.0.0 20220729 (experimental)"
        .section        .note.GNU-stack,"",@progbits


The compiling command: gcc -O2 -std=c99 test.c -S -mcpu=power10


---


### compiler : `gcc`
### title : `pre-register allocation scheduler is not RMW aware`
### open_at : `2022-08-08T10:21:52Z`
### last_modified_date : `2023-06-30T14:54:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106553
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.3.1`
### severity : `normal`
### contents :
The following example is minimized from the math routines in glibc:

#include <arm_neon.h>

typedef float32x4_t v_f32_t;

static inline v_f32_t
v_fma_f32 (v_f32_t x, v_f32_t y, v_f32_t z)
{
  return vfmaq_f32 (z, x, y);
}

v_f32_t
__v_sinf (v_f32_t x,v_f32_t z, v_f32_t n, v_f32_t r)
{
  v_f32_t r2, y;
  r2 = r * r;
  y = v_fma_f32 (n, r2, x);
  y = v_fma_f32 (y, r2, x);
  r = v_fma_f32 (y, r2, z);
  y = v_fma_f32 (y, r2, x);
  y = v_fma_f32 (y * r2, r, r);

  return y;
}

here we generate at -O2

__v_sinf(__Float32x4_t, __Float32x4_t, __Float32x4_t, __Float32x4_t):
        fmul    v3.4s, v3.4s, v3.4s
        mov     v5.16b, v0.16b
        mov     v4.16b, v0.16b
        fmla    v5.4s, v2.4s, v3.4s
        fmla    v4.4s, v5.4s, v3.4s
        fmla    v0.4s, v4.4s, v3.4s
        mov     v2.16b, v0.16b
        mov     v0.16b, v1.16b
        fmla    v0.4s, v4.4s, v3.4s
        fmul    v3.4s, v3.4s, v2.4s
        fmla    v0.4s, v3.4s, v0.4s
        ret

the 3rd move is there because the pre-register allocation scheduler created a false dependency by scheduling the the fmul after the fmla. This forces reload to have to create a reload to keep `v0` alive after the destructive operation.

with -O2  -fno-schedule-insns we get

__v_sinf(__Float32x4_t, __Float32x4_t, __Float32x4_t, __Float32x4_t):
        fmul    v3.4s, v3.4s, v3.4s
        mov     v4.16b, v0.16b
        fmla    v0.4s, v2.4s, v3.4s
        mov     v2.16b, v4.16b
        fmla    v2.4s, v0.4s, v3.4s
        mov     v0.16b, v1.16b
        fmla    v4.4s, v2.4s, v3.4s
        fmla    v0.4s, v2.4s, v3.4s
        fmul    v3.4s, v3.4s, v4.4s
        fmla    v0.4s, v3.4s, v0.4s
        ret

In glibc these additional moves cost double digit performance by breaking up the fmla chains.


Should we perhaps use a special RMW scheduling attribute to make it treat the last input as an output too?


---


### compiler : `gcc`
### title : `PRU: Inefficient code for zero check of 64-bit (boolean) AND result`
### open_at : `2022-08-08T19:41:42Z`
### last_modified_date : `2023-09-21T14:37:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106562
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
GCC generates inefficient code for the following C snippet:

char test(uint64_t a, uint64_t b)
{
        return a && b;
}

test:
        or      r0.b0, r14.b0, r14.b1
        or      r0.b0, r0.b0, r14.b2
        or      r0.b0, r0.b0, r14.b3
        or      r0.b0, r0.b0, r15.b0
        or      r0.b0, r0.b0, r15.b1
        or      r0.b0, r0.b0, r15.b2
        or      r0.b0, r0.b0, r15.b3
        qbeq    .L4, r0.b0, 0
        mov     r14, r16
        mov     r15, r17
        sub     r2, r2, 2
        rsb     r0, r16, 0
        rsc     r1, r17, 0
        mov     r17, r14
        mov     r18, r15
        sbbo    r3.b2, r2, 0, 2
        ldi     r16.b0, (63) & 0xffff
        or      r14, r0, r17
        or      r15, r1, r18
        call    %label(__pruabi_lsrll)
        lbbo    r3.b2, r2, 0, 2
        add     r2, r2, 2
        ret


---


### compiler : `gcc`
### title : `PRU: Inefficient zero-extend from 32-bit to 64-bit unsigned values`
### open_at : `2022-08-08T19:49:41Z`
### last_modified_date : `2022-08-22T19:42:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106564
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
Compiler uses 8-bit constant loads instead of 32-bit constant load to zero-extend 32-bit unsigned values.

uint64_t test(uint32_t a)
{
        return a;
}

test:
        mov     r0, r14
        ldi     r1.b0, (0) & 0xffff
        ldi     r1.b1, (0) & 0xffff
        ldi     r1.b2, (0) & 0xffff
        ldi     r1.b3, (0) & 0xffff
        mov     r14, r0
        mov     r15, r1
        ret


---


### compiler : `gcc`
### title : `[12/13/14 Regression] DCE sometimes fails with depending if statements since r12-2305-g398572c1544d8b75`
### open_at : `2022-08-09T11:35:38Z`
### last_modified_date : `2023-10-25T22:42:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106570
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
Sometimes, DCE fails when multiple if statements are used. 

For example, GCC detects that the following if statements always evaluate to false and thus removes the dead code:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(bool s, bool c) {
    if (!c == !s) {
        if (s && !c) {
            DCEMarker0_();
        }
    }
}

In the next snippet, the if statements are used to set a variable. This variable is then used in the next if statement. However, GCC now fails to detect and eliminate the dead code:

#include <stdio.h>
#include <stdbool.h>

void DCEMarker0_();

void f(bool s, bool c) {
    int intermediate_result = 0;
    if (!c == !s) {
        if (s && !c) {
            intermediate_result = 1;
        }
    }
    if (((!c == !s) && (s && !c)) || intermediate_result) {
        DCEMarker0_();
    }
}

This is actually a regression: It works fine until GCC 11.3.

This can also be seen via the following Compiler Explorer link: https://godbolt.org/z/n9dKMfqsd


---


### compiler : `gcc`
### title : `Suboptimal immediate generation on aarch64`
### open_at : `2022-08-11T09:01:33Z`
### last_modified_date : `2022-10-31T19:21:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106583
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
A simple codegen issue:
unsigned long long
foo (void)
{
  return 0x7efefefefefefeff;
}

generates at -O2
foo:
        mov     x0, 65279
        movk    x0, 0xfefe, lsl 16
        movk    x0, 0xfefe, lsl 32
        movk    x0, 0x7efe, lsl 48
        ret

whereas LLVM can do:
foo:                                    // @foo
        mov     x0, #-72340172838076674
        movk    x0, #65279
        movk    x0, #32510, lsl #48
        ret

Should be a matter of just making aarch64_internal_mov_immediate in aarch64.cc a bit smarter


---


### compiler : `gcc`
### title : `RISC-V: Mis-optimized code gen for zbs`
### open_at : `2022-08-11T15:36:00Z`
### last_modified_date : `2023-04-28T22:46:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106585
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
Command:
$ riscv64-unknown-elf-gcc foo.c -o - -S -O3 -march=rv64imafdc_zbb_zbs

```c
int foo(int rs1, int rs2) {
  return (rs1 & ~(1<<rs2));
}
```

Current code gen:
```asm
foo:
        li      a5,1
        sllw    a5,a5,a1
        andn    a0,a0,a5
        sext.w  a0,a0
        ret
```

And even worth if compile without zbb
```asm
foo:
        li      a5,1
        sllw    a5,a5,a1
        andn    a0,a0,a5
        sext.w  a0,a0
        ret
```

clang code gen:
```
foo:
        bclr    a0, a0, a1
        sext.w  a0, a0
        ret
```


---


### compiler : `gcc`
### title : `s390: Inefficient branchless conditionals for long long`
### open_at : `2022-08-12T08:28:28Z`
### last_modified_date : `2022-08-12T09:14:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106592
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
Created attachment 53443
source code

long long gtRef(long long a, long long b)
{
   return a > b;
}

Generates:

        cgr     %r2,%r3
        lghi    %r1,0
        lghi    %r2,1
        locgrnh %r2,%r1

Better sequence:
        cgr %r2,%r3
        lghi %r2,0
        alcgr %r2,%r2


long long leMaskRef(long long a, long long b)
{
   return -(a <= b);
}

Generates:

        cgr     %r2,%r3
        lhi     %r1,0
        lhi     %r2,1
        locrnle %r2,%r1
        sllg    %r2,%r2,63
        srag    %r2,%r2,63

Better sequence:

        cgr %r2,%r3
        slbgr %r2,%r2

long long gtMaskRef(long long a, long long b)
{
   return -(a > b);
}

Generates:
        cgr     %r2,%r3
        lhi     %r1,0
        lhi     %r2,1
        locrnh  %r2,%r1
        sllg    %r2,%r2,63
        srag    %r2,%r2,63

Better sequence:
        cgr   %r2,%r3
        lghi  %r2,0
        alcgr %r2,%r2
        lcgr  %r2,%r2


---


### compiler : `gcc`
### title : `[13/14 Regression] sign-extensions no longer merged into addressing mode`
### open_at : `2022-08-12T11:23:55Z`
### last_modified_date : `2023-07-27T09:23:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106594
### status : `NEW`
### tags : `missed-optimization, patch`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Since around the 5th of August (need to bisect) we no longer generate addressing modes on load merging the sign extends.

The following example:

extern const int constellation_64qam[64];

void foo(int nbits,
         const char *p_src,
         int *p_dst) {

  while (nbits > 0U) {
    char first = *p_src++;

    char index1 = ((first & 0x3) << 4) | (first >> 4);

    *p_dst++ = constellation_64qam[index1];

    nbits--;
  }
}

used to generate

        orr     w3, w4, w3, lsr 4
        ldr     w3, [x6, w3, sxtw 2]

and now generates:

        orr     w0, w4, w0, lsr 4
        sxtw    x0, w0
        ldr     w0, [x6, x0, lsl 2]

at -O2 (-O1 seems to still be fine).  This is causing a regression in perf in some of our libraries.

Looks like there's a change in how the operation is expressed.  It used to be

  first_17 = *p_src_28;
  _1 = (int) first_17;
  _2 = _1 << 4;
  _3 = (signed char) _2;

where the shift is done as an int, whereas now it's

  first_16 = *p_src_27;
  first.1_1 = (signed char) first_16;
  _2 = first.1_1 << 4;


---


### compiler : `gcc`
### title : `s390: Inefficient branchless conditionals for int`
### open_at : `2022-08-12T15:38:34Z`
### last_modified_date : `2022-08-15T06:42:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106598
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
int lt(int a, int b)
{
    return a < b;
}

generates:
        cr      %r2,%r3
        lhi     %r1,1
        lhi     %r2,0
        locrnl  %r1,%r2
        lgfr    %r2,%r1
        br      %r14

int ltOpt(int a, int b)
{
    long long x = a;
    long long y = b;
    return ((unsigned long long)(x - y)) >> 63;
}

better:
        sgr     %r2,%r3
        srlg    %r2,%r2,63
        br      %r14

int ltMask(int a, int b)
{
    return -(a < b);
}

generates:
        cr      %r2,%r3
        lhi     %r1,1
        lhi     %r2,0
        locrnl  %r1,%r2
        sllg    %r1,%r1,63
        srag    %r2,%r1,63


int ltMaskOpt(int a, int b)
{
    long long x = a;
    long long y = b;
    return (x - y) >> 63;
}

better:
        sgr     %r2,%r3
        srag    %r2,%r2,63
        br      %r14

int leMask(int a, int b)
{
    return -(a <= b);
}

generates:
        cr      %r2,%r3
        lhi     %r1,1
        lhi     %r2,0
        locrnle %r1,%r2
        sllg    %r1,%r1,63
        srag    %r2,%r1,63
        br      %r14

int leMaskOpt(int a, int b)
{
   int c;
   __asm__("cr %1,%2\n\tslbgr %0,%0":"=r"(c):"r"(a),"r"(b):"cc");
   // slbgr create a 64-bit mask => lgfr would not be required
   return c;
}

better:
        cr %r2,%r3
        slbgr %r2,%r2
        lgfr    %r2,%r2 <= not necessary
        br      %r14


int le(int a, int b)
{
    return a <= b;
}

generates:
        cr      %r2,%r3
        lhi     %r1,1
        lhi     %r2,0
        locrnle %r1,%r2
        lgfr    %r2,%r1
        br      %r14

int leOpt(int a, int b)
{
   unsigned long long c;
   __asm__("cr %1,%2\n\tslbgr %0,%0":"=r"(c):"r"(a),"r"(b):"cc");
   return (c >> 63);
}

better:
        cr %r2,%r3
        slbgr %r2,%r2
        srlg    %r2,%r2,63
        br      %r14


---


### compiler : `gcc`
### title : `__builtin_bswap32 is not hooked up for ZBB`
### open_at : `2022-08-12T19:56:35Z`
### last_modified_date : `2022-08-24T18:32:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106600
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Testcase:
```
int f(int a)
{
  return __builtin_bswap32(a);
}
```
The reason is because the check for TARGET_64BIT is there even though it should not be:
(define_insn "bswap<mode>2"
  [(set (match_operand:X 0 "register_operand" "=r")
        (bswap:X (match_operand:X 1 "register_operand" "r")))]
  "TARGET_64BIT && TARGET_ZBB"
  "rev8\t%0,%1"
  [(set_attr "type" "bitmanip")])


---


### compiler : `gcc`
### title : `__builtin_bswap16 code gen could be improved with ZBB enabled`
### open_at : `2022-08-12T20:09:13Z`
### last_modified_date : `2022-08-24T18:32:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106601
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
unsigned short t(unsigned short a)
{
  return __builtin_bswap16(a);
}

This could just be rev8 followed by a srli (I think).


---


### compiler : `gcc`
### title : `riscv: suboptimal codegen for zero_extendsidi2_shifted w/o bitmanip`
### open_at : `2022-08-12T20:34:25Z`
### last_modified_date : `2023-04-19T02:41:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106602
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
This came up when investigating https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106545. The test to supposedly induce the peephole doesn't, but generates far inferior code.

unsigned long foo2(unsigned long a)
{
	return (unsigned long)(unsigned int)a << 6;
}

-march=rv64gc -O2   # no bitmanip

foo2:
	li	a5,1
	slli	a5,a5,38
	addi	a5,a5,-64
	slli	a0,a0,6
	and	a0,a0,a5
	ret

llvm generates expected

foo:
	slli	a0, a0, 32
	srli	a0, a0, 26
	ret


---


### compiler : `gcc`
### title : `redundant load and store introduced in if-true-branch`
### open_at : `2022-08-14T15:48:42Z`
### last_modified_date : `2022-08-21T17:40:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106615
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
Given the code:

int g_36 = 0x0B137D37L;
int *g_35 = &g_36;
int g_44[2] = {0x964071C1L,0x964071C1L};

unsigned char  func_1(void);
int * func_16(int * p_17, unsigned long int  p_18);
int * func_19(int * p_20);


unsigned char func_1() { func_16(func_19(g_35), g_44[1]); }
int* func_16(int *a, unsigned long int b) {
	unsigned int c=1;
	*a = g_44;
	if ((g_44[0] = c) <= b)
		;
	else
		*a = 0;
}
int* func_19(int32_t *d) {
	g_44[1] |= g_36 ;
	return d;
}

when it's compiled on gcc-12.1 with option -O1, the generated asm code of func_16 will be:

0000000000401186 <func_16>:
  401186:	b8 60 40 40 00       	mov    $0x404060,%eax
  40118b:	89 07                	mov    %eax,(%rdi)
  40118d:	c7 05 c9 2e 00 00 01 	movl   $0x1,0x2ec9(%rip)        # 404060 <g_44>
  401194:	00 00 00 
  401197:	b8 00 00 00 00       	mov    $0x0,%eax
  40119c:	48 85 f6             	test   %rsi,%rsi
  40119f:	74 02                	je     4011a3 <func_16+0x1d>
  4011a1:	8b 07                	mov    (%rdi),%eax    
 # rdi keep the address of g_36
  4011a3:	89 07                	mov    %eax,(%rdi)
  4011a5:	c3                   	retq  

We can see g_36 will be loaded to %eax and stored back as is when the if-condition is true. This operation should be redundant?


---


### compiler : `gcc`
### title : `Missed folding with COMPOUND_EXPR due to C frontend bugs`
### open_at : `2022-08-15T13:57:05Z`
### last_modified_date : `2022-08-15T16:04:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106628
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `c`
### version : `13.0`
### severity : `normal`
### contents :
Doing

diff --git a/gcc/fold-const.cc b/gcc/fold-const.cc
index 4f4ec81c8d4..698f115507a 100644
--- a/gcc/fold-const.cc
+++ b/gcc/fold-const.cc
@@ -12595,14 +12595,9 @@ fold_binary_loc (location_t loc, enum tree_code code, tree type,
       return NULL_TREE;
 
     case COMPOUND_EXPR:
-      /* When pedantic, a compound expression can be neither an lvalue
-        nor an integer constant expression.  */
-      if (TREE_SIDE_EFFECTS (arg0) || TREE_CONSTANT (arg1))
+      if (TREE_SIDE_EFFECTS (arg0))
        return NULL_TREE;
-      /* Don't let (0, 0) be null pointer constant.  */
-      tem = integer_zerop (arg1) ? build1_loc (loc, NOP_EXPR, type, arg1)
-                                : fold_convert_loc (loc, type, arg1);
-      return tem;
+      return op1;
 
     case ASSERT_EXPR:
       /* An ASSERT_EXPR should never be passed to fold_binary.  */


results in missed diagnostics:

FAIL: gcc.dg/array-5.c bad vla handling (test for bogus messages, line 40)
FAIL: gcc.dg/c99-const-expr-12.c (test for excess errors)
FAIL: gcc.dg/pr61096-1.c  (test for errors, line 29)
FAIL: gcc.dg/pr61096-1.c (test for excess errors)


---


### compiler : `gcc`
### title : `FSM threading doesn't handle computed goto`
### open_at : `2022-08-17T09:22:05Z`
### last_modified_date : `2022-08-17T14:47:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106663
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For the following B->C isn't threaded but it seems the code is/was halfway there.  With ranger we won't ever get &C as range for the goto destination
though, so whether the thread copier handles things correctly is unknown.

unsigned g;
void FSM (int start)
{
  void *states[] = { &&A, &&B, &&C, &&E };
  void *state = states[start];

  do {
  goto *state;

A:
  g += 1;
  state = g & 1 ? &&B : &&E;
  continue;

B:
  g += 2;
  state = &&C;
  continue;

C:
  g += 3;
  state = states[g & 3];
  continue;

E:
  break;
  } while (1);
}


---


### compiler : `gcc`
### title : `Abstraction overhead with std::views::join`
### open_at : `2022-08-18T19:34:58Z`
### last_modified_date : `2023-08-25T18:34:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106677
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
(from https://stackoverflow.com/q/73407636/1918193 )

#include <array>
#include <vector>
#include <ranges>

struct Foo {
    auto join() const { return m_array | std::views::join; }
    auto direct() const { return std::views::all(m_array[0]); }
    std::array<std::vector<int*>, 1> m_array;
};
__attribute__((noinline)) int sum_array(const Foo& foo)
{
    int result = 0;
    for (int* val : foo.join())
        result += *val;
    return result;
}
__attribute__((noinline)) int sum_vec(const Foo& foo)
{
    int result = 0;
    for (int* val : foo.direct())
        result += *val;
    return result;
}

I am using a snapshot from 20220719 with -std=gnu++2b -O3 and looking at .optimized dumps.

sum_vec gets relatively nice, short code. sum_array gets something uglier.

  _18 = &foo_5(D)->m_array;
  _6 = foo_5(D) + 24;
  if (_6 != _18)

Err, x != x+24 should be folded to false? Let's add

  if(foo.m_array.begin()==foo.m_array.end())__builtin_unreachable();

to move forward.

  _16 = MEM[(int * const * const &)foo_4(D)];
  _17 = MEM[(int * const * const &)foo_4(D) + 8];
  if (_16 != _17)
    goto <bb 4>; [5.50%]
  else
    goto <bb 3>; [94.50%]

why are we guessing that the vector is probably empty? Let's look at more code

  <bb 2> [local count: 853673669]:
  _10 = &MEM[(const struct array *)foo_4(D)]._M_elems;
  _7 = foo_4(D) + 24;
  _16 = MEM[(int * const * const &)foo_4(D)];
  _17 = MEM[(int * const * const &)foo_4(D) + 8];
  if (_16 != _17)
    goto <bb 4>; [5.50%]
  else
    goto <bb 3>; [94.50%]

  <bb 3> [local count: 806721618]:
  _18 = foo_4(D) + 24;

  <bb 4> [local count: 96636762]:
  # SR.89_28 = PHI <_10(2), _18(3)>
  # SR.90_41 = PHI <_16(2), 0B(3)>
  goto <bb 9>; [100.00%]

  <bb 9> [local count: 923031551]:
  # result_2 = PHI <0(4), result_12(8)>
  # SR.89_13 = PHI <SR.89_28(4), SR.89_54(8)>
  # SR.90_30 = PHI <SR.90_41(4), SR.90_27(8)>
  if (_7 == SR.89_13)
    goto <bb 10>; [30.00%]
  else
    goto <bb 12>; [70.00%]

  <bb 10> [local count: 276909463]:
  if (SR.90_30 == 0B)
    goto <bb 11>; [16.34%]
  else
    goto <bb 12>; [83.66%]

  <bb 11> [local count: 96636764]:
  # result_31 = PHI <result_2(10), result_12(7), result_12(5)>
  return result_31;

(why not _18 = _7 towards the beginning?)
It would be nice if threading could isolate the case of an empty vector: 2 -> 3 -> 4 -> 9 -> 10 -> 11: just return 0, and the rest of the code may become easier to optimize.

Let me add

  if(foo.m_array[0].begin()==foo.m_array[0].end())__builtin_unreachable();

to avoid the empty vector case as well. This looks better, at least the inner loop looks normal, but we are still iterating on the elements of m_array, when we should be able to tell that it has exactly 1 element.


---


### compiler : `gcc`
### title : `Inefficiency in long integer multiplication`
### open_at : `2022-08-18T19:58:40Z`
### last_modified_date : `2022-08-18T20:06:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106678
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
The code from PR 103109

#include <stdint.h>

void Long_multiplication( uint64_t multiplicand[],
                          uint64_t multiplier[],
                          uint64_t sum[],
                          uint64_t ilength, uint64_t jlength )
{
  uint64_t acarry, mcarry, product;

  for( uint64_t i = 0;
       i < (ilength + jlength);
       i++ )
    sum[i] = 0;

  acarry = 0;
  for( uint64_t j = 0; j < jlength; j++ )
    {
      mcarry = 0;
      for( uint64_t i = 0; i < ilength; i++ )
        {
          __uint128_t mcarry_prod;
          __uint128_t acarry_sum;
          mcarry_prod = ((__uint128_t) multiplicand[i]) * ((__uint128_t) multiplier[j])
            + (__uint128_t) mcarry;
          mcarry = mcarry_prod >> 64;
          product = mcarry_prod;
          acarry_sum = ((__uint128_t) sum[i+j]) + ((__uint128_t) acarry) + product;
          sum[i+j] += acarry_sum;
          acarry = acarry_sum >> 64;
          //      {mcarry, product} = multiplicand[i]*multiplier[j]
          //                            + mcarry;
          //      {acarry,sum[i+j]} = {sum[i+j]+acarry} + product;
          
        }
    }
}

still shows some inefficiency after r13-2107.

Compiling the function with gcc 13.0.0 20220818, with

$ gcc  -mcpu=power9 -O3 -c loop.c

and disassembling the output (for easier reading) gives (looking only
at the main part)

  7c:   00 00 80 38     li      r4,0
  80:   00 00 80 3b     li      r28,0
  84:   00 00 60 38     li      r3,0
  88:   00 00 00 38     li      r0,0
  8c:   ff ff c0 38     li      r6,-1
  90:   00 00 e0 38     li      r7,0
  94:   20 00 c1 fa     std     r22,32(r1)
  98:   28 00 e1 fa     std     r23,40(r1)
  9c:   60 00 c1 fb     std     r30,96(r1)
  a0:   68 00 e1 fb     std     r31,104(r1)
  a4:   00 00 00 60     nop
  a8:   00 00 00 60     nop
  ac:   00 00 42 60     ori     r2,r2,0
  b0:   a6 03 49 7f     mtctr   r26
  b4:   78 c3 0c 7f     mr      r12,r24
  b8:   14 22 b9 7c     add     r5,r25,r4
  bc:   00 00 00 39     li      r8,0
  c0:   09 00 6c e9     ldu     r11,8(r12)
  c4:   2a 20 5d 7d     ldx     r10,r29,r4
  c8:   09 00 25 e9     ldu     r9,8(r5)
  cc:   33 52 cb 13     maddld  r30,r11,r10,r8
  d0:   31 52 eb 13     maddhdu r31,r11,r10,r8
  d4:   38 30 d6 7f     and     r22,r30,r6
  d8:   38 38 f7 7f     and     r23,r31,r7
  dc:   78 fb e8 7f     mr      r8,r31
  e0:   14 48 56 7d     addc    r10,r22,r9
  e4:   14 01 77 7d     adde    r11,r23,r0
  e8:   14 18 4a 7d     addc    r10,r10,r3
  ec:   14 52 29 7d     add     r9,r9,r10
  f0:   94 01 6b 7c     addze   r3,r11
  f4:   00 00 25 f9     std     r9,0(r5)
  f8:   c8 ff 00 42     bdnz    c0 <Long_multiplication+0xc0>
  fc:   01 00 9c 3b     addi    r28,r28,1
 100:   08 00 84 38     addi    r4,r4,8
 104:   40 e0 3b 7c     cmpld   r27,r28
 108:   a8 ff 82 40     bne     b0 <Long_multiplication+0xb0>
 
In these two nested loops, r6 is not changed, so it is always -1.

  d4:   38 30 d6 7f     and     r22,r30,r6

just assigns r30 to r22, so r30 could have been used instead of
r22.

Similarly,

  d8:   38 38 f7 7f     and     r23,r31,r7

just sets r23 to zero because r7 is always zero.


---


### compiler : `gcc`
### title : `[13 regression] gcc.dg/tree-prof/cmpsf-1.c fails after r13-2098-g5adfb6540db95d`
### open_at : `2022-08-18T21:15:32Z`
### last_modified_date : `2022-10-05T10:17:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106679
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
g:5adfb6540db95da5faf1f77fbe9ec38b4cf8eb1f, r13-2098-g5adfb6540db95d
make  -k check-gcc RUNTESTFLAGS="tree-prof.exp=gcc.dg/tree-prof/cmpsf-1.c"
FAIL: gcc.dg/tree-prof/cmpsf-1.c scan-tree-dump-not dom2 "Invalid sum"
# of expected passes		4
# of unexpected failures	1
# of unsupported tests		1

commit 5adfb6540db95da5faf1f77fbe9ec38b4cf8eb1f (HEAD, refs/bisect/bad)
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Wed Aug 17 17:47:21 2022 +0200

    Reset root oracle from path_oracle::reset_path.


---


### compiler : `gcc`
### title : `out of ssa Coalescing sometimes chooses the wrong thing causing an extra move`
### open_at : `2022-08-19T19:41:17Z`
### last_modified_date : `2022-08-23T17:20:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106688
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
For the following testcase, gcc -O2

unsigned foo(const unsigned char *buf, long size);
unsigned bar(const unsigned char *buf, long size)
{
        typedef char  i8v8  __attribute__((vector_size(8)));
        typedef short i16v8 __attribute__((vector_size(16)));
        long chunk_sz = 15*16;
        for (; size >= chunk_sz; size -= chunk_sz) {
                i16v8 vs1 = { 0 };
                const unsigned char *end = buf + chunk_sz;
                for (; buf != end; buf += 16) {
                        i16v8 b;
                        asm("pmovzxbw %1, %0" : "=x"(b) : "m"(*(i8v8*)buf));
                        vs1 += b;
                        asm("pmovzxbw %1, %0" : "=x"(b) : "m"(*(i8v8*)(buf+8)));
                        vs1 += b;
                }
                asm("" :: "x"(vs1));
        }
        return foo(buf, size);
}

(asms needed due to PR 31667)

generates

bar:
        cmp     rsi, 239
        jle     .L2
        lea     rdx, [rdi+240]
.L4:
        lea     rax, [rdx-240]
        pxor    xmm0, xmm0
.L3:
        pmovzxbw QWORD PTR [rax], xmm1
        add     rax, 16
        paddw   xmm0, xmm1

        mov     rdi, rdx ; <<< ehhh

        pmovzxbw QWORD PTR [rax-8], xmm1
        paddw   xmm0, xmm1
        cmp     rax, rdx
        jne     .L3
        sub     rsi, 240
        add     rdx, 240
        cmp     rsi, 239
        jg      .L4
.L2:
        jmp     foo

It looks as if going out of SSA places in the loop a register copy corresponding to a phi node which is outside of the loop. Strangely, RTL optimizations do not clean it up either.


---


### compiler : `gcc`
### title : `Section anchors is not efficient for riscv`
### open_at : `2022-08-19T22:41:23Z`
### last_modified_date : `2022-08-19T23:48:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106691
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
static int a, b;

int f(void)
{
  return a+b;
}

void g(int c, int d)
{
 a = c;
 b = d;
}
--- CUT ---
Right now riscv32-linux-gnu produces:
f:
        lui     a5,%hi(a)
        lw      a0,%lo(a)(a5)
        lui     a5,%hi(b)
        lw      a5,%lo(b)(a5)
        add     a0,a0,a5
        ret
g:
        lui     a5,%hi(a)
        sw      a0,%lo(a)(a5)
        lui     a5,%hi(b)
        sw      a1,%lo(b)(a5)
        ret

While aarch64 produces:
f:
        adrp    x0, .LANCHOR0
        add     x2, x0, :lo12:.LANCHOR0
        ldr     w1, [x0, #:lo12:.LANCHOR0]
        ldr     w0, [x2, 4]
        add     w0, w1, w0
        ret
g:
        adrp    x2, .LANCHOR0
        add     x3, x2, :lo12:.LANCHOR0
        str     w0, [x2, #:lo12:.LANCHOR0]
        str     w1, [x3, 4]
        ret

One less instruction.


---


### compiler : `gcc`
### title : `Redundant move instructions in ARM SVE intrinsics use cases`
### open_at : `2022-08-20T02:50:58Z`
### last_modified_date : `2023-04-02T18:55:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106694
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
Here is the example: https://godbolt.org/z/4zPK7j8vT

The codes:
#include "arm_sve.h"

int coalesce (svbool_t pg, int64_t* base, int n, int32_t *in1, int64_t *in2, int64_t*out)
{
  svint64x4_t result = svld4_s64 (pg, base);
  svint64_t v0 = svget4_s64(result, 0);
  svint64_t v1 = svget4_s64(result, 1);
  svint64_t v2 = svget4_s64(result, 2);
  svint64_t v3 = svget4_s64(result, 3);

  for (int i = 0; i < n; i += 1)
    {
        svint64_t v18 = svld1_s64(pg, in1);
        svint64_t v19 = svld1_s64(pg, in2);
        v0 = svmad_s64_z(pg, v0, v18, v19);
        v1 = svmad_s64_z(pg, v1, v18, v19);
        v2 = svmad_s64_z(pg, v2, v18, v19);
        v3 = svmad_s64_z(pg, v3, v18, v19);
    }
  svst1_s64(pg, out+0,v0);
  svst1_s64(pg, out+1,v1);
  svst1_s64(pg, out+2,v2);
  svst1_s64(pg, out+3,v3);
}

Assembly:
coalesce:
        ld4d    {z24.d - z27.d}, p0/z, [x0]
        mov     z5.d, z24.d
        mov     z4.d, z25.d
        mov     z3.d, z26.d
        mov     z2.d, z27.d
        cmp     w1, 0
        ble     .L2
        mov     w0, 0
        ld1d    z1.d, p0/z, [x2]
        ld1d    z0.d, p0/z, [x3]
.L3:
        add     w0, w0, 1
        movprfx z5.d, p0/z, z5.d
        mad     z5.d, p0/m, z1.d, z0.d
        movprfx z4.d, p0/z, z4.d
        mad     z4.d, p0/m, z1.d, z0.d
        movprfx z3.d, p0/z, z3.d
        mad     z3.d, p0/m, z1.d, z0.d
        movprfx z2.d, p0/z, z2.d
        mad     z2.d, p0/m, z1.d, z0.d
        cmp     w1, w0
        bne     .L3
.L2:
        add     x3, x4, 8
        add     x2, x4, 16
        add     x1, x4, 24
        st1d    z5.d, p0, [x4]
        st1d    z4.d, p0, [x3]
        st1d    z3.d, p0, [x2]
        st1d    z2.d, p0, [x1]
        ret

The "mov" instructions are redundant. I think the issue is that GCC doesn't
have the register coalescing like LLVM.

The subreg can not propagate accorss basic blocks.

Can someone implement register coalesing or subreg forwarding across basic blocks?
 
Thanks.


---


### compiler : `gcc`
### title : `Compiler does not take into account number range limitation to avoid subtract from immediate`
### open_at : `2022-08-21T07:12:10Z`
### last_modified_date : `2022-08-24T15:02:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106701
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
unsigned long long subfic(unsigned long long a)
{
    if (a > 15) __builtin_unreachable();
    return 15 - a;
}

With clang on x86 subtract from immediate gets translated to xor:
_Z6subficy:                             # @_Z6subficy
        mov     rax, rdi
        xor     rax, 15
        ret

Platforms like 390 and x86 which have no subtract from immediate would benefit from this optimization:

gcc currently generates:
_Z6subficy:
        lghi    %r1,15
        sgr     %r1,%r2
        lgr     %r2,%r1
        br      %r14


---


### compiler : `gcc`
### title : `[12/13 Regression] avx intrinsic not generating vblendvps instruction with -mavx`
### open_at : `2022-08-21T19:31:45Z`
### last_modified_date : `2023-01-08T09:09:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106704
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.1.0`
### severity : `normal`
### contents :
I'm getting some really strange code generation on x86_64 when using the _mm256_blendv_ps intrinsic in gcc 12 with just the -mavx flag. 

https://godbolt.org/z/YbEK4nvEd

It should generate a single vblendvps instruction, but instead creates a lot of branchy scaler code.

gcc 11 doesn't appear to exhibit this behavior.
Also using -mavx2 on gcc 12 generates the single instruction, just not -mavx for whatever reason. 

#include <immintrin.h>

__m256 bend_stuff( __m256 a, __m256 b, __m256 mask)
{
    return _mm256_blendv_ps(a, b, mask);
}

gcc -O2 -mavx -c blend.c -masm=intel -S -o out.s

in GCC 12 it generates:

bend_stuff:
        vmovd   eax, xmm2
        vmovaps ymm3, ymm0
        vmovdqa xmm4, xmm2
        test    eax, eax
        jns     .L3
        vmovaps xmm0, xmm1
.L3:
        vpextrd eax, xmm4, 1
        vshufps xmm9, xmm3, xmm3, 85
        test    eax, eax
        jns     .L5
        vshufps xmm9, xmm1, xmm1, 85
.L5:
        vpextrd eax, xmm4, 2
        vunpckhps       xmm5, xmm3, xmm3
        test    eax, eax
        jns     .L7
        vunpckhps       xmm5, xmm1, xmm1
.L7:
        vpextrd eax, xmm4, 3
        vshufps xmm7, xmm3, xmm3, 255
        test    eax, eax
        jns     .L9
        vshufps xmm7, xmm1, xmm1, 255
.L9:
        vextractf128    xmm2, ymm2, 0x1
        vextractf128    xmm4, ymm3, 0x1
        vmovd   eax, xmm2
        test    eax, eax
        jns     .L11
        vextractf128    xmm4, ymm1, 0x1
.L11:
        vextractf128    xmm8, ymm3, 0x1
        vpextrd eax, xmm2, 1
        vshufps xmm8, xmm8, xmm8, 85
        test    eax, eax
        jns     .L13
        vextractf128    xmm8, ymm1, 0x1
        vshufps xmm8, xmm8, xmm8, 85
.L13:
        vextractf128    xmm6, ymm3, 0x1
        vpextrd eax, xmm2, 2
        vunpckhps       xmm6, xmm6, xmm6
        test    eax, eax
        jns     .L15
        vextractf128    xmm6, ymm1, 0x1
        vunpckhps       xmm6, xmm6, xmm6
.L15:
        vextractf128    xmm3, ymm3, 0x1
        vpextrd eax, xmm2, 3
        vshufps xmm3, xmm3, xmm3, 255
        test    eax, eax
        jns     .L17
        vextractf128    xmm1, ymm1, 0x1
        vshufps xmm3, xmm1, xmm1, 255
.L17:
        vunpcklps       xmm6, xmm6, xmm3
        vunpcklps       xmm4, xmm4, xmm8
        vunpcklps       xmm5, xmm5, xmm7
        vunpcklps       xmm0, xmm0, xmm9
        vmovlhps        xmm4, xmm4, xmm6
        vmovlhps        xmm0, xmm0, xmm5
        vinsertf128     ymm0, ymm0, xmm4, 0x1
        ret

in GCC 11.3 it generates:

bend_stuff:
        vblendvps       ymm0, ymm0, ymm1, ymm2
        ret


---


### compiler : `gcc`
### title : `constant prop is not undone for switches`
### open_at : `2022-08-21T21:17:41Z`
### last_modified_date : `2022-08-26T07:51:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106705
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Given code like:

#define do_sd(a, b, c) do { b[c] = a; } while (0)

void sd(unsigned len, unsigned long *mem)
{
	const unsigned long val = 2415920512;

	switch (len) {
	case 7:
		do_sd(val, mem, -7);
	case 6:
		do_sd(val, mem, -6);
	case 5:
		do_sd(val, mem, -5);
	case 4:
		do_sd(val, mem, -4);
	case 3:
		do_sd(val, mem, -3);
	case 2:
		do_sd(val, mem, -2);
	case 1:
		do_sd(val, mem, -1);
	}
}

(reduced from code using an out-of-tree intrinsic for a vendor-specific
machine instruction) we get 64-bit assembly at -O2 with a constant load
replicated across all the switch cases, terribly suboptimal.

E.g. for RISC-V this gets compiled to:

	.text
	.align	1
	.globl	sd
	.type	sd, @function
sd:
	li	a5,7
	bgtu	a0,a5,.L1
	lui	a5,%hi(.L4)
	addi	a5,a5,%lo(.L4)
	slli	a0,a0,2
	add	a0,a0,a5
	lw	a5,0(a0)
	jr	a5
	.section	.rodata
	.align	2
	.align	2
.L4:
	.word	.L1
	.word	.L10
	.word	.L9
	.word	.L8
	.word	.L7
	.word	.L6
	.word	.L5
	.word	.L3
	.text
.L3:
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	sd	a5,-56(a1)
.L5:
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	sd	a5,-48(a1)
.L6:
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	sd	a5,-40(a1)
.L7:
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	sd	a5,-32(a1)
.L8:
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	sd	a5,-24(a1)
.L9:
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	sd	a5,-16(a1)
.L10:
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	sd	a5,-8(a1)
.L1:
	ret
	.size	sd, .-sd

and similarly e.g. for Alpha:

[...]
$L3:
	lda $1,9($31)
	sll $1,28,$1
	lda $1,1408($1)
	stq $1,-56($17)
$L5:
	lda $1,9($31)
	sll $1,28,$1
	lda $1,1408($1)
	stq $1,-48($17)
[...]

MIPS:

[...]
.L3:
	li	$2,18874368			# 0x1200000
	daddiu	$2,$2,11
	dsll	$2,$2,7
	sd	$2,-56($5)
.L5:
	li	$2,18874368			# 0x1200000
	daddiu	$2,$2,11
	dsll	$2,$2,7
	sd	$2,-48($5)
[...]

POWER:

[...]
.L3:
	lis 9,0x9000
	ori 9,9,0x580
	rldicl 9,9,0,32
	std 9,-56(4)
.L5:
	lis 9,0x9000
	ori 9,9,0x580
	rldicl 9,9,0,32
	std 9,-48(4)
[...]

or x86 even:

[...]
.L3:
	movl	$2415920512, %eax
	movq	%rax, -56(%rsi)
.L5:
	movl	$2415920512, %eax
	movq	%rax, -48(%rsi)
[...]

I understand this is due forward constant propagation, and it used to
be possible to circumvent it at least at -Os by disabling several of
the tree passess (with GCC 9 this has been originally discovered with),
but apparently not anymore.  Not that it would be useful for regular
compilations.  In that case good RISC-V code was produced (except for
some needless sign-extensions, etc. coming from -Os inefficiency):

	.text
	.align	1
	.globl	sd
	.type	sd, @function
sd:
	addiw	a0,a0,-1
	sext.w	a4,a0
	li	a5,6
	bgtu	a4,a5,.L1
	slli	a0,a0,32
	lui	a5,%hi(.L4)
	addi	a5,a5,%lo(.L4)
	srli	a0,a0,30
	add	a0,a0,a5
	lw	a4,0(a0)
	li	a5,9
	slli	a5,a5,28
	addi	a5,a5,1408
	jr	a4
	.section	.rodata
	.align	2
	.align	2
.L4:
	.word	.L10
	.word	.L9
	.word	.L8
	.word	.L7
	.word	.L6
	.word	.L5
	.word	.L3
	.text
	.align	2
.L3:
	sd	a5,-56(a1)
	.align	2
.L5:
	sd	a5,-48(a1)
	.align	2
.L6:
	sd	a5,-40(a1)
	.align	2
.L7:
	sd	a5,-32(a1)
	.align	2
.L8:
	sd	a5,-24(a1)
	.align	2
.L9:
	sd	a5,-16(a1)
	.align	2
.L10:
	sd	a5,-8(a1)
	.align	2
.L1:
	ret
	.size	sd, .-sd


---


### compiler : `gcc`
### title : `[rs6000] 64bit constant generation with oris xoris`
### open_at : `2022-08-22T09:37:54Z`
### last_modified_date : `2023-10-08T02:30:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106708
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
For code: t.c
void foo (long *arg)
{
  *arg++ = 0x98765432ULL;
  *arg++ = 0xffffffff7cdeab55ULL;
  *arg++ = 0xffffffff65430000ULL;
}

gcc -O2 -S t.c
        lis 10,0xffff
        lis 8,0x9876
        ori 10,10,0x7cde
        lis 9,0xffff
        ori 8,8,0x5432
        sldi 10,10,16
        ori 9,9,0x6543
        rldicl 8,8,0,32
        ori 10,10,0xab55
        sldi 9,9,16

Below sequences would be better:
        li 8,21554
        li 10,-21675
        lis 9,0xe543
        oris 8,8,0x9876
        xoris 10,10,0x8321
        xoris 9,9,0x8000


---


### compiler : `gcc`
### title : `logical-op-non-short-circuit maybe should be 1`
### open_at : `2022-08-23T15:18:16Z`
### last_modified_date : `2022-08-31T07:17:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106724
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Take:
int f(int a, int b, int c, int d)
{
  return a > b && c > d;
}
---- CUT ---
Currently on riscv32 at -O2 produces:
f:
        ble     a0,a1,.L3
        sgt     a0,a2,a3
        ret
.L3:
        li      a0,0
        ret

But add --param logical-op-non-short-circuit=1, produces:
f:
        sgt     a0,a0,a1
        sgt     a2,a2,a3
        and     a0,a0,a2
        ret

Which is much better, especially on in-order cores.


---


### compiler : `gcc`
### title : `Missed fold / canonicalization for checking if a number is a power of 2`
### open_at : `2022-08-24T02:51:21Z`
### last_modified_date : `2023-09-21T13:56:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106727
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.1.1`
### severity : `enhancement`
### contents :
These two are equivalent but generate different code:

bool foo(unsigned n) {
    return std::popcount(n) <= 1;
}
bool bar(unsigned n) {
    return (n & (n - 1)) == 0;
}

https://godbolt.org/z/crrxnfWo7

For systems that don't have popcount instructions it would be very beneficial to transform foo -> bar. For systems that do have popcount instructions it still would be beneficial to canonicalize the two.


---


### compiler : `gcc`
### title : `[OpenMP] reverse offload – avoid emitting 'nohost' variant on the host / cleanup`
### open_at : `2022-08-26T10:26:14Z`
### last_modified_date : `2022-08-26T10:26:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106752
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
For reverse offload, GCC generates a
  <...>._omp_fn.0.nohost
function on all devices and on the host it also generates
  <...>._omp_fn.0

However, the 'nohost' version is not used on the host and could be removed.
(Cleanup + size optimization.) - Or in other words:


(Pre-remark: DECL_IGNORE_P is set in omp-expand.cc's expand_omp_target
while the quote below is from omp-offload.cc execute_omp_device_lower.)

From https://gcc.gnu.org/pipermail/gcc-patches/2022-August/600387.html:

> +	  case IFN_GOMP_TARGET_REV:
...
> +	      /* FIXME: Find a way to actually prevent outputting the empty-body
> +		 old_decl as debug symbol + function in the assembly file.  */

The debug stuff ought to be through DECL_IGNORED_P on the FUNCTION_DECL.
If you want it set just on one side and clear on the other side, perhaps set
or clear it during lto streaming it in in offload lto1?

As for emitting it, perhaps turning it into an external declaration from
definition afterwards?


Cf. also original patch:
  https://gcc.gnu.org/pipermail/gcc-patches/2022-July/598662.html
albeit the longer description is in
  https://gcc.gnu.org/pipermail/gcc-patches/2022-July/598654.html

This patch was committed as r13-2218.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Incorrect "writing 1 byte into a region of size 0" on a vectorized loop`
### open_at : `2022-08-26T18:32:13Z`
### last_modified_date : `2023-07-03T19:53:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106757
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Created attachment 53515
Source code (gcc-bug.c) for the repro

GCC 11.2.0 is happy with this code (and I believe it is correct).  Neither GCC 12.1.0 nor GCC 12.2.0 are happy with this code (and I believe this is a bug).  There are no preprocessor directives in the source code.

$ /usr/gcc/v12.2.0/bin/gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/work1/gcc/v12.2.0/bin/../libexec/gcc/x86_64-pc-linux-gnu/12.2.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-12.2.0/configure --prefix=/usr/gcc/v12.2.0 CC=/usr/bin/gcc CXX=/usr/bin/g++
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 12.2.0 (GCC) 
$

Compilation:

$ /usr/gcc/v11.2.0/bin/gcc -c -std=c99 -O3 -Wall -Werror -pedantic -Wextra  gcc-bug.c
$ /usr/gcc/v12.2.0/bin/gcc -c -std=c99 -O3 -Wall -Werror -pedantic -Wextra  gcc-bug.c
gcc-bug.c: In function ‘pqr_scanner’:
gcc-bug.c:16:24: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
   16 |             tmpchar[k] = mbs[k];
      |             ~~~~~~~~~~~^~~~~~~~
gcc-bug.c:14:14: note: at offset 4 into destination object ‘tmpchar’ of size 4
   14 |         char tmpchar[MBC_MAX];
      |              ^~~~~~~
gcc-bug.c:16:24: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
   16 |             tmpchar[k] = mbs[k];
      |             ~~~~~~~~~~~^~~~~~~~
gcc-bug.c:14:14: note: at offset 5 into destination object ‘tmpchar’ of size 4
   14 |         char tmpchar[MBC_MAX];
      |              ^~~~~~~
cc1: all warnings being treated as errors
$

The -Wall, -Wextra, -pedantic options are not necessary to generate the warning; the -Werror gives an error instead of a warning, of course.

$ cat gcc-bug.i
# 0 "gcc-bug.c"
# 0 "<built-in>"
# 0 "<command-line>"
# 1 "/usr/include/stdc-predef.h" 1 3 4
# 0 "<command-line>" 2
# 1 "gcc-bug.c"
enum { MBC_MAX = 4 };

extern int pqr_scanner(char *mbs);
extern int pqr_mbc_len(char *mbs, int n);
extern void pqr_use_mbs(const char *mbs, int len);
extern char *pqr_mbs_nxt(char *mbs);

int
pqr_scanner(char *mbs)
{
    while (mbs != 0 && *mbs != '\0')
    {
        int len = pqr_mbc_len(mbs, MBC_MAX);
        char tmpchar[MBC_MAX];
        for (int k = 0; k < len; k++)
            tmpchar[k] = mbs[k];
        pqr_use_mbs(tmpchar, len);

        mbs = pqr_mbs_nxt(mbs);
    }

    return 0;
}
$

The source code contains a comment noting that if I replace `mbs = pqr_nbs_nxt(mbs);` with `mbs += len;`, the bug does not reproduce.

In the original code (which was doing work with multi-byte characters and strings), the analogue of pqr_mbc_len() returns either -1 or a value 1..MBC_MAX.    The code for the pqr_mbc_len() function was not part of the TU.  There was a test for `if (len < 0) return -1;` after the call to pqr_mbc_len() but it wasn't needed for the repro.

Just in case - GCC 11.2.0 specs and output from uname -a:

$ /usr/gcc/v11.2.0/bin/gcc -v
Using built-in specs.
COLLECT_GCC=/usr/gcc/v11.2.0/bin/gcc
COLLECT_LTO_WRAPPER=/work1/gcc/v11.2.0/bin/../libexec/gcc/x86_64-pc-linux-gnu/11.2.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-11.2.0/configure --prefix=/usr/gcc/v11.2.0 CC=/usr/bin/gcc CXX=/usr/bin/g++
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.2.0 (GCC)
$ uname -a
Linux njdc-ldev04 3.10.0-693.el7.x86_64 #1 SMP Thu Jul 6 19:56:57 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux
$

The original function was 100 lines of code in a file of 2600 lines, including 20 headers directly.


---


### compiler : `gcc`
### title : `PPCLE: vec_extract(vector unsigned int) unnecessary rldicl after mfvsrwz`
### open_at : `2022-08-29T06:57:28Z`
### last_modified_date : `2023-10-09T06:38:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106769
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
#include <altivec.h>

unsigned int extr(vector unsigned int v)
{
   return vec_extract(v, 2);
}

Generates:

_Z4extrDv4_j:
.LFB1:
        .cfi_startproc
        mfvsrwz 3,34
        rldicl 3,3,0,32
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0
        .cfi_endproc


The rldicl is not necessary as mfvsrwz already wiped out the upper 32 bits of the register.


---


### compiler : `gcc`
### title : `powerpc64le: Unnecessary xxpermdi before mfvsrd`
### open_at : `2022-08-29T07:52:26Z`
### last_modified_date : `2023-03-03T06:02:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106770
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
#include <altivec.h>

int cmp2(double a, double b)
{
    vector double va = vec_promote(a, 1);
    vector double vb = vec_promote(b, 1);
    vector long long vlt = (vector long long)vec_cmplt(va, vb);
    vector long long vgt = (vector long long)vec_cmplt(vb, va);
    vector signed long long vr = vec_sub(vlt, vgt);

    return vec_extract(vr, 1);
}

Generates:

_Z4cmp2dd:
.LFB1:
        .cfi_startproc
        xxpermdi 1,1,1,0
        xxpermdi 2,2,2,0
        xvcmpgtdp 33,2,1
        xvcmpgtdp 32,1,2
        vsubudm 0,1,0
        xxpermdi 0,32,32,3
        mfvsrd 3,0
        extsw 3,3
        blr

The unnecessary xxpermdi for vec_promote are already reported in another bugzilla case.

mfvsrd can access all 64 vector registers directly and xxpermdi is not required.
mfvsrd 3,32 expected instead xxpermdi 0,32,32,3 + mfvsrd 3,0


---


### compiler : `gcc`
### title : `Add __is_convertible built-in`
### open_at : `2022-08-31T10:41:10Z`
### last_modified_date : `2023-04-29T17:44:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106784
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `13.0`
### severity : `enhancement`
### contents :
This is supported by Clang already, and would be very beneficial for libstdc++. The std::is_convertible (and is_nothrow_convertible) type trait is used widely in the std::lib, but is currently implemented in pure C++ as a class template. A built-in would improve compile times.

We already have __is_constructible and __is_assignable, and the nothrow forms of those.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] SRA regression causes extra instructions sometimes since r12-1529-gd7deee423f993bee`
### open_at : `2022-08-31T11:16:48Z`
### last_modified_date : `2023-05-08T12:25:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106786
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
I noticed a regression when using the builtin for sbb instruction (__builtin_ia32_sbb_u64).

typedef unsigned long long u64;

struct R {
    u64 value;
    bool carry;
};

inline R subc(u64 x, u64 y, bool carry) noexcept {
    u64 d;
    const u64 carryout = __builtin_ia32_sbb_u64(carry, x, y, &d);
    return {d, carryout != 0};
}

bool bad(u64 x, u64 y) {
    const R z = subc(x, y, false);
    R a = subc(x, y, z.carry);
    return a.carry;
}

https://godbolt.org/z/f41KKe19q

The expected assembly is
        cmp     rdi, rsi
        sbb     rdi, rsi

But GCC 12.2.0 and trunk produces
        cmp     rdi, rsi
        setb    al
        movzx   eax, al
        add     al, -1
        sbb     rdi, rsi

The regression is in 12.2.0, the 11.3.0 optimizes properly.

There are simple changes which will bring back the expected optimization:
- change `const R z` to `R z`,
- change `bool carry` to `u64 carry`.

This may be related to calling convention / ABI because I noticed in one of the tree optimization outputs for 12.2.0 that the `bool carry` is forced to be in memory: `MEM <unsigned char> [(struct R *)&z + 8B]`.

https://godbolt.org/z/7zh7GxraK


---


### compiler : `gcc`
### title : `Poor codegen for selecting and incrementing value behind a reference`
### open_at : `2022-09-01T17:11:58Z`
### last_modified_date : `2022-10-02T16:02:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106804
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.2.0`
### severity : `enhancement`
### contents :
Godbolt: https://godbolt.org/z/e9ePs7Ece

For the following source code:

    void increment_largest(int& a, int& b) {
        ++(a > b ? a : b);
    }

gcc 12 with -O2 produces the following asm:

increment_largest(int&, int&):
        mov     edx, DWORD PTR [rdi]
        mov     eax, DWORD PTR [rsi]
        cmp     edx, eax
        jle     .L2
        add     edx, 1
        mov     DWORD PTR [rdi], edx
        ret
.L2:
        add     eax, 1
        mov     DWORD PTR [rsi], eax
        ret

For equivalent code using pointers:

    void increment_largest(int* a, int* b) {
        ++*(*a > *b ? a : b);
    }

gcc with -O2 gives something slightly different:

increment_largest(int*, int*):
        mov     edx, DWORD PTR [rdi]
        mov     eax, DWORD PTR [rsi]
        cmp     edx, eax
        jle     .L2
        mov     eax, edx
        mov     rsi, rdi
.L2:
        add     eax, 1
        mov     DWORD PTR [rsi], eax
        ret

If one rewrites code with references to assign the selected reference to a variable:

    void increment_largest(int& a, int& b) {
        auto& tgt = (a > b ? a : b);
        ++tgt;
    }

it gives exactly the same asm as the version with pointers. Anyway it is seemingly worse than what clang-14 -O2 produces for all three sources:

increment_largest(int&, int&):
        mov     eax, dword ptr [rdi]
        cmp     eax, dword ptr [rsi]
        cmovg   rsi, rdi
        add     dword ptr [rsi], 1
        ret

Likely to be related to PR94006.


---


### compiler : `gcc`
### title : `"Boolean-or" optimization opportunity not consistently used`
### open_at : `2022-09-03T22:48:44Z`
### last_modified_date : `2022-09-07T10:44:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106822
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
Hello

Assume the following code is given:

-----------------------------
#include <stdio.h>
#include <stdbool.h>

typedef struct {
    bool a;
    bool b;
} MyStruct;

bool f(const MyStruct *s)
{
    return s->a || s->b;
}

int main()
{
}
-----------------------------

If the code is compiled with -O3 I would assume it does not include any jumps, because it should be valid to always read s->b (even if s->a is already true). However, the function compiles into this:

-----------------------------
f:
        movzx   eax, BYTE PTR [rdi]
        test    al, al
        jne     .L1
        movzx   eax, BYTE PTR [rdi+1]
.L1:
        ret
-----------------------------
(see: https://godbolt.org/z/KdnzWb9Wh)


Interestingly, gcc will produce code without any jumps if MyStruct is slightly modified:

-----------------------------
#include <stdio.h>
#include <stdbool.h>

typedef struct {
    int x;
    bool a;
    bool b;
} MyStruct;

bool f(const MyStruct *s)
{
    return s->a || s ->b;
}

int main()
{
}
-----------------------------

Generated code:

-----------------------------
f:
        cmp     WORD PTR [rdi+4], 0
        setne   al
        ret
-----------------------------
(see: https://godbolt.org/z/P78daxhhe)

It's not a bug, but I think it's a missed optimization opportunity.

Thanks


---


### compiler : `gcc`
### title : `[12/13/14 Regression] misleading warning : iteration X invokes undefined behavior`
### open_at : `2022-09-05T21:43:22Z`
### last_modified_date : `2023-05-08T12:25:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106842
### status : `NEW`
### tags : `diagnostic, missed-optimization, wrong-code`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
Hello,

I get the following misleading warning with gcc 12.2 with -O2/-O3.
11.3 seems fine.

gcc -O2 main.c 
main.c: In function ‘main’:
main.c:16:38: warning: iteration 9 invokes undefined behavior [-Waggressive-loop-optimizations]
   16 |            for(int64_t k =0; k<i1 ; k++)
      |                                     ~^~
main.c:16:31: note: within this loop
   16 |            for(int64_t k =0; k<i1 ; k++)
      |                              ~^~~

cat main.c
#include "stdio.h"
#include "stdint.h"

int main(int argc, char** argv)
{
   int64_t i1=0;
   int64_t i3=0,i2=0; // warning with this declaration order 
   //int64_t i2=0,i3=0; // but fine (no warning) with this order
   
   for ( i1 = 0; i1<10 ; i1++)
   {
       for ( ; i2<10 ;i2++ )         
           printf("L2\n");
       
       for ( ; i3<10 ; i3++ )
           for(int64_t k =0; k<i1 ; k++)
               printf("L3\n");
   }
   
   printf("i1 %lu i2 %lu i3 %lu\n",i1,i2,i3);  
   return 0;
}


Could someone reproduce ?
The weirdest part of this warning : it depends on the index declaration order.
Even if gcc has a bad(?) way of computing internal range/validity to deduce such issues, I would expect it in both order.

Regards,


---


### compiler : `gcc`
### title : `Unexpected capture of "constexpr int" variable inside lambda`
### open_at : `2022-09-06T19:51:34Z`
### last_modified_date : `2022-10-21T03:31:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106864
### status : `UNCONFIRMED`
### tags : `missed-optimization, rejects-valid`
### component : `c++`
### version : `13.0`
### severity : `normal`
### contents :
void bad() {
  constexpr int x = 1;
  auto Outer = [&]() {
    auto L = [=]() {
      for (int i = 0; i < x; ++i) {
      }
    };
    static_assert(sizeof(L) == 1); // fails
  };
}

void good() {
  constexpr int x = 1;
  auto L = [=]() {
    for (int i = 0; i < x; ++i) {
    }
  };
  static_assert(sizeof(L) == 1);  // passes
}

Not sure if that's a bug from the standard conformance point of view, but I'd expect the L not to capture constexpr variable in both cases.

Sizeof is one in both cases with clang and (MSVC v19.latest + "/std:c++latest /c") on godbolt.


---


### compiler : `gcc`
### title : `addcarry pattern`
### open_at : `2022-09-06T19:54:51Z`
### last_modified_date : `2022-09-07T08:58:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106865
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
clang provides builtins like __builtin_addcll to deal with addcarry in a generic way. However, i believe we can provide pattern matching in GCC so existing programs may get benefit from the code, instead of adding new builtins.

https://godbolt.org/z/3j18bPq8b

Take riscv as example:

https://godbolt.org/z/KWxfPWGz4

https://godbolt.org/z/b4r63oGqj

This proves GCC and clang can generate identical code EVEN without using adc builtins.

I suggest to add pattern matching for code like this in GCC:

template<typename T>
inline constexpr T add_carry_no_carry_in(T a,T b,T& carryout) noexcept
{
    T res{a+b};
    carryout=res<a;
    return res;
}

template<typename T>
inline constexpr T add_carry(T a,T b,T carryin,T& carryout) noexcept
{
    if(carryin>1)
    {
        __builtin_unreachable();
    }
    a=add_carry_no_carry_in(carryin,a,carryout);
    a=add_carry_no_carry_in(a,b,carryin);
    carryout+=carryin;
    return a;
}

This should correctly identify carries for GCC without adding new builtins while it keeps the same interface as clang's builtins.


---


### compiler : `gcc`
### title : `roundss not vectorized`
### open_at : `2022-09-12T16:15:16Z`
### last_modified_date : `2022-09-22T07:52:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106910
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
GCC 7 and newer optimize `std::floor(float)` into `vroundss` with -O2 and -march=skylake, which is great.

However, I can see in Compiler Explorer that the following example:

```
#include <cmath>

struct TVec { float x, y; };

struct TKey { int i, j; };

class TDom
{
private:
  static int Floor(float const x)
    { return static_cast<int>(std::floor(x)); }

public:
  TKey CalcKey(TVec const &) const;
};

TKey TDom::CalcKey(TVec const &r) const
  { return {Floor(r.x), Floor(r.y)}; }
```

produces:

```

vxorps      %xmm1, %xmm1, %xmm1
vroundss    $9, (%rsi), %xmm1, %xmm0
vroundss    $9, 4(%rsi), %xmm1, %xmm1
vunpcklps   %xmm1, %xmm0, %xmm0
vcvttps2dq  %xmm0, %xmm2
vmovq       %xmm2, %rax
ret
```

Couldn’t the pair of `vroundss` have been merged into a single `vroundps`?


---


### compiler : `gcc`
### title : `[12 Regression] Bogus uninitialized warning on boost::optional<<std::vector<std::string>>>, missed FRE`
### open_at : `2022-09-13T08:18:53Z`
### last_modified_date : `2023-08-18T02:26:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106922
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
Created attachment 53568
Reproducer for maybe uninitialized warning with gcc 12

In our production code when upgrading to gcc 12 from gcc 11 we have faced bogus warning
"may be used uninitialized" on bit complex use of boost::optional vector of strings used
in a loop with unrelated string stream redirect.

> In file included from <..>/include/c++/12.0.0/vector:67,
>                  from reproduce.cpp:13:
> In destructor ‘std::vector<_Tp, _Alloc>::~vector() [with _Tp = std::__cxx11::basic_string<char>; _Alloc = std::allocator<std::__cxx11::basic_string<char> >]’,
>     inlined from ‘void boost::optional_detail::optional_base<T>::destroy_impl() [with T = std::vector<std::__cxx11::basic_string<char> >]’ at /usr/include/boost/optional/optional.hpp:771:50,
>     inlined from ‘void boost::optional_detail::optional_base<T>::destroy() [with T = std::vector<std::__cxx11::basic_string<char> >]’ at /usr/include/boost/optional/optional.hpp:757:21,
>     inlined from ‘void boost::optional_detail::optional_base<T>::assign(const boost::optional_detail::optional_base<T>&) [with T = std::vector<std::__cxx11::basic_string<char> >]’ at /usr/include/boost/optional/optional.hpp:264:21,
>     inlined from ‘boost::optional_detail::optional_base<T>& boost::optional_detail::optional_base<T>::operator=(const boost::optional_detail::optional_base<T>&) [with T = std::vector<std::__cxx11::basic_string<char> >]’ at /usr/include/boost/optional/optional.hpp:241:19,
>     inlined from ‘boost::optional<T>& boost::optional<T>::operator=(const boost::optional<T>&) [with T = std::vector<std::__cxx11::basic_string<char> >]’ at /usr/include/boost/optional/optional.hpp:1040:15, 
>     inlined from ‘void test()’ at reproduce.cpp:55:13:
> <..>/include/c++/12.0.0/bits/stl_vector.h:680:22: error: ‘*(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >*)((char*)&external + offsetof(boost::Optional<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >,boost::optional<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::<unnamed>.boost::optional_detail::optional_base<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::m_storage.boost::optional_detail::aligned_storage<std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::dummy_)).std::vector<std::__cxx11::basic_string<char> >::<anonymous>.std::_Vector_base<std::__cxx11::basic_string<char>, std::allocator<std::__cxx11::basic_string<char> > >::_M_impl.std::_Vector_base<std::__cxx11::basic_string<char>, std::allocator<std::__cxx11::basic_string<char> > >::_Vector_impl::<anonymous>.std::_Vector_base<std::__cxx11::basic_string<char>, std::allocator<std::__cxx11::basic_string<char> > >::_Vector_impl_data::_M_finish’ may be used uninitialized [-Werror=maybe-uninitialized]
>   680 |         std::_Destroy(this->_M_impl._M_start, this->_M_impl._M_finish,
>       |         ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>   681 |                       _M_get_Tp_allocator());
>       |                       ~~~~~~~~~~~~~~~~~~~~~~
> reproduce.cpp: In function ‘void test()’:
> reproduce.cpp:45:40: note: ‘external’ declared here
>    45 |     Optional<std::vector<std::string>> external;
>       |                                        ^~~~~~~~

I have tested this with gcc 12 (12.1.1, 12.2.1 and release/gcc-12 commit
https://gcc.gnu.org/git/?p=gcc.git&a=commit;h=4ce316ca54c863cf0fd4257ba0ab768ab83c62e5") 
with boost 1.69.0, 1.75.0, 1.76.0 and 1.80.0. All fails.

With any boost version and gcc 11 (11.3.1 and release/gcc-11 tip commit
https://gcc.gnu.org/git/?p=gcc.git&a=commit;h=7e356c3083c79473c941bc92d61f755e923bc86c)
the warning doesn't appear.

To reproduce:

> $ g++ -Werror -std=c++20 -O2 -Wall -o reproduce.o -c reproduce.cpp

I was not able to bump the reproducer to simpliear case. In all modification where loop
is removed the warning disappears.

The problem doesn't occur when std::optional is used.

Only workaround I found is to add following pragma at top of the module:

> #pragma GCC diagnostic push
> #pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
> #include <vector>
> #pragma GCC diagnostic pop

I have bisected the problem to start with 
https://gcc.gnu.org/git/?p=gcc.git&a=commit;h=5b8b1522e04adc20980f396571be1929a32d148a

commit 5b8b1522e04adc20980f396571be1929a32d148a (HEAD, refs/bisect/bad)
Author: Richard Biener <rguenther@suse.de>
Date:   Mon Sep 27 12:01:38 2021 +0200

    tree-optimization/100112 - VN last_vuse and redundant store elimination
    
    This avoids the last_vuse optimization hindering redundant store
    elimination by always also recording the original VUSE that was
    in effect on the load.
    
    In stage3 gcc/*.o we have 3182752 times recorded a single
    entry and 903409 times two entries (that's ~20% overhead).
    
    With just recording a single entry the number of hashtable lookups
    done when walking the vuse->vdef links to find an earlier access
    is 28961618.  When recording the second entry this makes us find
    that earlier for donwnstream redundant accesses, reducing the number
    of hashtable lookups to 25401052 (that's a ~10% reduction).
    
    2021-09-27  Richard Biener  <rguenther@suse.de>
    
            PR tree-optimization/100112
            * tree-ssa-sccvn.c (visit_reference_op_load): Record the
            referece into the hashtable twice in case last_vuse is
            different from the original vuse on the stmt.
    
            * gcc.dg/tree-ssa/ssa-fre-95.c: New testcase.


---


### compiler : `gcc`
### title : `Missed PRE full redundancy without post-loop use`
### open_at : `2022-09-15T12:45:11Z`
### last_modified_date : `2022-09-22T11:12:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106950
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
when we have the situation of PR106922, aka g++.dg/tree-ssa/pr106922.C which is
XFAILed on !lp64 because of this there's

  mem = 0;
  do
    {
       if (mem)
         mem = 0;
       if (mem) // as 'char' instead of 'bool', maybe relevant
         bar ();
       if (i++ != n)
         break;
    }
  while (1);
  .. = mem;

with the use of 'mem' after the loop we get that antic-in in the loop
exit condition block and eventually figure the full redundancy in
the conditional redundant set to zero during PRE insertion.  When that
'mem' after the loop is _not_ there this isn't triggered (possibly
because of the 'char' vs. 'bool' here, but then one can eventually
elide the whole if (mem) bar () code).


---


### compiler : `gcc`
### title : `Missed optimization: x < y ? x : y not lowered to minss`
### open_at : `2022-09-15T15:30:30Z`
### last_modified_date : `2023-07-21T08:18:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106952
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53580
Assembly from gcc -O3 -S bug.c

The following is an implementation of a ray/axis-aligned box intersection test:

struct ray {
    float origin[3];
    float dir_inv[3];
};

struct box {
    float min[3];
    float max[3];
};

static inline float min(float x, float y) {
    return x < y ? x : y;
}

static inline float max(float x, float y) {
    return x < y ? x : y;
}

_Bool intersection(const struct ray *ray, const struct box *box) {
    float tmin = 0.0, tmax = 1.0 / 0.0;

    for (int i = 0; i < 3; ++i) {
        float t1 = (box->min[i] - ray->origin[i]) * ray->dir_inv[i];
        float t2 = (box->max[i] - ray->origin[i]) * ray->dir_inv[i];

        tmin = min(max(t1, tmin), max(t2, tmin));
        tmax = max(min(t1, tmax), min(t2, tmax));
    }

    return tmin < tmax;
}

However, gcc -O3 doesn't use minss/maxss for every min()/max().  Instead, some of them are lowered to conditional jumps which regresses performance significantly since the branches are unpredictable.

Simpler variants like

        tmin = max(tmin, min(t1, t2));
        tmax = min(tmax, max(t1, t2));

get the desired codegen, but that behaves differently if t1 or t2 is NaN.

"Bisecting" with godbolt.org, it seems this is an old regression: 4.8.5 was good, but 4.9.0 was bad.


---


### compiler : `gcc`
### title : `GCC fail to vectorize and clang succeed`
### open_at : `2022-09-20T23:05:36Z`
### last_modified_date : `2022-09-22T15:56:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106989
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
https://godbolt.org/z/v5arbjh3n

This case ARM-clang can vectorize but ARM-GCC failed.
Can anyone fix it? Or give me some guideline to fix it?


code:
typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
real_t flat_2d_array[LEN_2D*LEN_2D];

real_t x[LEN_1D];

real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D],
bb[LEN_2D][LEN_2D],cc[LEN_2D][LEN_2D],tt[LEN_2D][LEN_2D];

int indx[LEN_1D];

real_t* __restrict__ xx;
real_t* yy;
real_t s243(void)
{
    for (int nl = 0; nl < iterations; nl++) {
        for (int i = 0; i < LEN_1D-1; i++) {
            a[i] = b[i] + c[i  ] * d[i];
            b[i] = a[i] + d[i  ] * e[i];
            a[i] = b[i] + a[i+1] * d[i];
        }
    }
}


---


### compiler : `gcc`
### title : `[13 Regression] ICE in expand_LOOP_VECTORIZED, at internal-fn.cc:2720  with -O2 since r13-1598-g0a7e721a6499a42f`
### open_at : `2022-09-21T07:59:05Z`
### last_modified_date : `2022-11-29T11:59:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106995
### status : `RESOLVED`
### tags : `ice-on-valid-code, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
% gcc-tk -v
Using built-in specs.
COLLECT_GCC=gcc-tk
COLLECT_LTO_WRAPPER=/zdata/shaoli/compilers/ccbuilder-compilers/gcc-1e4c7e870e2a3a059568565196a8f3b8c9de1fa4/libexec/gcc/x86_64-pc-linux-gnu/13.0.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --disable-multilib --disable-bootstrap --enable-languages=c,c++ --prefix=/zdata/shaoli/compilers/ccbuilder-compilers/gcc-1e4c7e870e2a3a059568565196a8f3b8c9de1fa4
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 13.0.0 20220915 (experimental) (GCC)
%
% gcc-tk -w -O1 a.c
during RTL pass: expand
simple.c: In function ‘i’:
simple.c:4:1: internal compiler error: in expand_LOOP_VECTORIZED, at internal-fn.cc:2729
    4 | i() {
      | ^
0x2033eee internal_error(char const*, ...)
	???:0
0x998592 fancy_abort(char const*, int, char const*)
	???:0
Please submit a full bug report, with preprocessed source (by using -freport-bug).
Please include the complete backtrace with any bug report.
See <https://gcc.gnu.org/bugs/> for instructions.
%
% cat a.c
a, b, c, d;
static unsigned long *e = &b, *f = &b;
g(h) { return a >= 2 ? 0 : h >> a; }
i() {
  for (; c;) {
    if (*f ^ 11)
      for (;;)
        ;
    d = g(0 >= 0);
    *e = d;
  }
}
main() {}
%

Compiler explorer: https://godbolt.org/z/4a5WYn4fM


---


### compiler : `gcc`
### title : `gcc not exploiting undefined behavior to optimize away the result of division`
### open_at : `2022-09-21T21:41:39Z`
### last_modified_date : `2022-09-22T20:04:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107005
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
#include <limits.h>
int main() { return INT_MIN / -1; }

gcc -O2

main:
        mov     eax, -2147483648
        ret


clang -O2

main:                                   # @main
        ret



https://godbolt.org/z/Tjxx3KGdK


---


### compiler : `gcc`
### title : `Missing optimization: common idiom for external data`
### open_at : `2022-09-22T02:27:43Z`
### last_modified_date : `2022-09-23T06:11:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107006
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
Created attachment 53602
C test case source

The only *portable* way in C to deal with external data structures containing data of specific endianness, possibly unaligned, is to operate on them as byte (char) arrays.

At least on x86 (which supports arbitrarily aligned loads), gcc *sometimes* recognize these as single loads, but sometimes not.

In the included test cases, there is a plain C implementation and an implementation wrapped in a C++ class.

Compiling the former with:

gcc -std=c2x -g -O3 -W -Wall -[cSE] -o bswap.[osi] bswap.c

... recognizes the load idiom for 16-bit numbers but not for 32- or 64-bit numbers.

Compiling the latter with:

gcc -std=c++20 -g -O3 -E -Wall -[cSE] -o bswapcc.[osi] bswapcc.cc

... *additionally* recognizes the 32-bit load, *but only in the bigendian case* (that is, it generates a load and a bswap instruction); whereas in the littleendian -- native -- case, this does not happen!

I am familiar with the used of packed arrays and __builtin_bswap*() for these accesses, but unfortunately these are gcc-specific.


---


### compiler : `gcc`
### title : `[13 Regression] massive unnecessary code blowup in vectorizer`
### open_at : `2022-09-22T18:49:10Z`
### last_modified_date : `2022-09-26T17:25:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107009
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Given an annotated saxpy function:

#include <cstdlib>

void saxpy(size_t n, float* __restrict res, float a, const float* __restrict x, const float* __restrict y)
{
  if (n == 0 || n % 8 != 0)
    __builtin_unreachable();
  res = (float*)__builtin_assume_aligned(res, 32);
  x = (const float*) __builtin_assume_aligned(x, 32);
  y = (const float*) __builtin_assume_aligned(y, 32);
  for (size_t i = 0; i < n; ++i)
    res[i] = a * x[i] + y[i];
}


Compiling this with the gcc 12.2.1 version from Fedora 36 leads to the expected, guided result (although the shrq isn't necessary…) with -O3:

_Z5saxpymPffPKfS1_:
	.cfi_startproc
	shrq	$3, %rdi
	vbroadcastss	%xmm0, %ymm0
	xorl	%eax, %eax
	salq	$5, %rdi
	.p2align 4
	.p2align 3
.L2:
	vmovaps	(%rdx,%rax), %ymm1
	vfmadd213ps	(%rcx,%rax), %ymm0, %ymm1
	vmovaps	%ymm1, (%rsi,%rax)
	addq	$32, %rax
	cmpq	%rdi, %rax
	jne	.L2
	vzeroupper
	ret

The the current trunk gcc the result is massively bigger and given the guidance in the sources none of the extra code is necessary.

_Z5saxpymPffPKfS1_:
.LFB22:
	.cfi_startproc
	movq	%rdi, %r8
	movq	%rdx, %rdi
	movq	%rcx, %rdx
	leaq	-1(%r8), %rax
	cmpq	$6, %rax
	jbe	.L7
	movq	%r8, %rcx
	vbroadcastss	%xmm0, %ymm2
	xorl	%eax, %eax
	shrq	$3, %rcx
	salq	$5, %rcx
	.p2align 4
	.p2align 3
.L3:
	vmovaps	(%rdi,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm2, %ymm1
	vmovaps	%ymm1, (%rsi,%rax)
	addq	$32, %rax
	cmpq	%rcx, %rax
	jne	.L3
	movq	%r8, %rax
	andq	$-8, %rax
	testb	$7, %r8b
	je	.L18
	vzeroupper
.L2:
	movq	%r8, %rcx
	subq	%rax, %rcx
	leaq	-1(%rcx), %r9
	cmpq	$2, %r9
	jbe	.L5
	vmovaps	(%rdx,%rax,4), %xmm3
	vshufps	$0, %xmm0, %xmm0, %xmm1
	movq	%rcx, %r9
	vfmadd132ps	(%rdi,%rax,4), %xmm3, %xmm1
	andq	$-4, %r9
	vmovaps	%xmm1, (%rsi,%rax,4)
	addq	%r9, %rax
	andl	$3, %ecx
	je	.L16
.L5:
	vmovss	(%rdi,%rax,4), %xmm1
	leaq	0(,%rax,4), %rcx
	leaq	1(%rax), %r9
	vfmadd213ss	(%rdx,%rax,4), %xmm0, %xmm1
	vmovss	%xmm1, (%rsi,%rcx)
	cmpq	%r8, %r9
	jnb	.L16
	vmovss	4(%rdi,%rcx), %xmm1
	addq	$2, %rax
	vfmadd213ss	4(%rdx,%rcx), %xmm0, %xmm1
	vmovss	%xmm1, 4(%rsi,%rcx)
	cmpq	%r8, %rax
	jnb	.L16
	vmovss	8(%rdx,%rcx), %xmm4
	vfmadd132ss	8(%rdi,%rcx), %xmm4, %xmm0
	vmovss	%xmm0, 8(%rsi,%rcx)
.L16:
	ret
	.p2align 4
	.p2align 3
.L18:
	vzeroupper
	ret
	.p2align 4
	.p2align 3
.L7:
	xorl	%eax, %eax
	jmp	.L2


---


### compiler : `gcc`
### title : `range information not used in popcount`
### open_at : `2022-09-26T20:10:20Z`
### last_modified_date : `2023-07-12T21:19:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107043
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
This code could be compiled to a simple return of the value 1 but it isn't because the range information for n does not survive long enough.

int g(int n)
{
  n &= 0x8000;
  if (n == 0)
    return 1;
  return __builtin_popcount(n);
}

The code generated today is lengthy:

   0:	81 e7 00 80 00 00    	and    $0x8000,%edi
   6:	ba 01 00 00 00       	mov    $0x1,%edx
   b:	89 f8                	mov    %edi,%eax
   d:	c1 e8 0f             	shr    $0xf,%eax
  10:	85 ff                	test   %edi,%edi
  12:	0f 44 c2             	cmove  %edx,%eax
  15:	c3                   	ret


---


### compiler : `gcc`
### title : `load introduced of struct/union fields after assigning it to a local variable`
### open_at : `2022-09-27T07:25:49Z`
### last_modified_date : `2022-10-26T08:31:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107047
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.1.0`
### severity : `normal`
### contents :
for gcc-12.1.0, with this code:

struct S6 {
   short  f0;
   char  f1;
   int  f2;
};

struct S6 g_36 = {2UL,4L,0x32B3D10AL};
int g_48 = (-1L);


void func_30() {
    struct S6 f[4][7];
    f[1][6] = g_36;
    if (f[1][6].f0 || f[1][6].f1)
        g_48 = f[1][6].f1;
}

when compiled with -O1 option, generated binaries will be like:

0000000000401186 <func_30>:
  401186:	48 83 ec 70          	sub    $0x70,%rsp
  40118a:	f7 05 d4 2e 00 00 ff 	testl  $0xffffff,0x2ed4(%rip)        # 404068 <g_36>
  401191:	ff ff 00 
  401194:	74 0d                	je     4011a3 <func_30+0x1d>
  401196:	0f be 05 cd 2e 00 00 	movsbl 0x2ecd(%rip),%eax        # 40406a <g_36+0x2>
  40119d:	89 05 bd 2e 00 00    	mov    %eax,0x2ebd(%rip)        # 404060 <g_48>
  4011a3:	48 83 c4 70          	add    $0x70,%rsp
  4011a7:	c3                   	retq   

we can see the use of f[1][6] is replaced by g_36 in the if-condition and "g_48 = f[1][6].f1", and the f is not optimized away.


---


### compiler : `gcc`
### title : `duplicate load of return value when facing multiple branches`
### open_at : `2022-09-27T08:53:41Z`
### last_modified_date : `2022-09-27T19:53:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107050
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
given this code:

int g_286 = (-5L);
int p;
int f = 1;

void func_58();

func_31(int c, int d) {
	if (c) {
		if (f){
			if (d)
				func_58();
			return g_286;
		}
		g_286 = 0;
	}
	return 0;
}
func_58() {
	int arr[30];
	p = arr[0];
}

when compiled with gcc-12.1.0 (-O1), it will generate:

0000000000401186 <func_58>:
  401186:	48 83 ec 10          	sub    $0x10,%rsp
  40118a:	8b 44 24 88          	mov    -0x78(%rsp),%eax
  40118e:	89 05 f4 8c 00 00    	mov    %eax,0x8cf4(%rip)        # 409e88 <p>
  401194:	48 83 c4 10          	add    $0x10,%rsp
  401198:	c3                   	retq   

0000000000401199 <func_31>:
  401199:	89 f8                	mov    %edi,%eax
  40119b:	85 ff                	test   %edi,%edi
  40119d:	74 1f                	je     4011be <func_31+0x25>
  40119f:	8b 05 bb 2e 00 00    	mov    0x2ebb(%rip),%eax        # 404060 <f>
  4011a5:	85 c0                	test   %eax,%eax
  4011a7:	75 0b                	jne    4011b4 <func_31+0x1b>
  4011a9:	c7 05 b1 2e 00 00 00 	movl   $0x0,0x2eb1(%rip)        # 404064 <g_286>
  4011b0:	00 00 00 
  4011b3:	c3                   	retq   
  4011b4:	8b 05 aa 2e 00 00    	mov    0x2eaa(%rip),%eax        # 404064 <g_286>
  4011ba:	85 f6                	test   %esi,%esi
  4011bc:	75 01                	jne    4011bf <func_31+0x26>
  4011be:	c3                   	retq   
  4011bf:	48 83 ec 08          	sub    $0x8,%rsp
  4011c3:	b8 00 00 00 00       	mov    $0x0,%eax
  4011c8:	e8 b9 ff ff ff       	callq  401186 <func_58>
  4011cd:	8b 05 91 2e 00 00    	mov    0x2e91(%rip),%eax        # 404064 <g_286>
  4011d3:	48 83 c4 08          	add    $0x8,%rsp
  4011d7:	c3                   	retq 

we can see in the func_31, compiler choose to load g_286 before judge whether d != 0, and if it's true, %eax will be used and func_58 is called. Before return, g_286 will be loaded again to %eax


---


### compiler : `gcc`
### title : `redundant loads when copying a union`
### open_at : `2022-09-27T11:25:33Z`
### last_modified_date : `2022-09-27T13:18:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107051
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.1.0`
### severity : `normal`
### contents :
for this code:

union U2 {
   unsigned  f0;
   char * f1;
};

union U2 g_284[2] = {{0UL},{0xC2488F72L}};

int e;
void func_1() {
	union U2 c = {7};
	int32_t *d[2];
	for (; e;)
		*d[1] = 0;
	g_284[0] = c = g_284[1];
}

compile it with gcc-12.1.0 -O1, and generate:

0000000000401186 <func_1>:
  401186:	83 3d fb 8c 00 00 00 	cmpl   $0x0,0x8cfb(%rip)        # 409e88 <e>
  40118d:	74 02                	je     401191 <func_1+0xb>
  40118f:	eb fe                	jmp    40118f <func_1+0x9>
  401191:	8b 15 d1 2e 00 00    	mov    0x2ed1(%rip),%edx        # 404068 <g_284+0x8>
  401197:	48 b8 00 00 00 00 ff 	movabs $0xffffffff00000000,%rax
  40119e:	ff ff ff 
  4011a1:	48 23 05 c0 2e 00 00 	and    0x2ec0(%rip),%rax        # 404068 <g_284+0x8>
  4011a8:	48 09 d0             	or     %rdx,%rax
  4011ab:	48 89 05 ae 2e 00 00 	mov    %rax,0x2eae(%rip)        # 404060 <g_284>
  4011b2:	c3                   	retq 

I don't understand why clearing the low 4 bytes of g_284[1].f1 and then or it with g_284[1].f0, because it should be equal?

and for the next example, we can see the both fields of g_303 have been loaded and written to g:

union U0 {
    short  f0;
    int  f3;
};

union U0 g_303 = {0x9B86L};
union U0 g;

int a,b;
void func_1() {
    union U0 d[1] = {1};
    for (; a;)
      for (; b;)
        ;
    g = d[0] = g_303;
}

under gcc-12.1.0 -O1:

0000000000401186 <func_1>:
  401186:	83 3d ff 8c 00 00 00 	cmpl   $0x0,0x8cff(%rip)        # 409e8c <a>
  40118d:	74 02                	je     401191 <func_1+0xb>
  40118f:	eb fe                	jmp    40118f <func_1+0x9>
  401191:	8b 05 c9 2e 00 00    	mov    0x2ec9(%rip),%eax        # 404060 <g_303>
  401197:	66 8b 05 c2 2e 00 00 	mov    0x2ec2(%rip),%ax        # 404060 <g_303>
  40119e:	89 05 ec 8c 00 00    	mov    %eax,0x8cec(%rip)        # 409e90 <g>
  4011a4:	c3                   	retq


---


### compiler : `gcc`
### title : `Range of __builtin_popcount can be improved with nonzerobits`
### open_at : `2022-09-27T15:18:28Z`
### last_modified_date : `2022-10-05T12:23:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107052
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
```
void link_failure();
void f(int a)
{
    a &= 0x300;
    int b =  __builtin_popcount(a);
    if (b > 3)
        link_failure();
}

```
The if statement should be optimized away as the only values for popcount here is 0-3.


---


### compiler : `gcc`
### title : `ones bits is not tracked and popcount is not tracked`
### open_at : `2022-09-27T15:24:07Z`
### last_modified_date : `2023-07-12T21:56:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107053
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
```
void link_failure();
void f(int a)
{
    a |= 0x300;
    int b =  __builtin_popcount(a);
    if (b < 3)
        link_failure();
}
```
The if statement should be optimized away.
The reason is after the |, at least 3 bits are set in a.


---


### compiler : `gcc`
### title : `[12 Regression] bits/stl_algobase.h:431: warning: 'void* __builtin_memcpy(void*, const void*, unsigned int)' reading between 8 and 2147483644 bytes from a region of size 4 [-Wstringop-overread]`
### open_at : `2022-09-29T14:35:46Z`
### last_modified_date : `2023-05-08T12:25:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107087
### status : `REOPENED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 53640
Gzipped preprocessed output

There are bogus warnings when compiling the attached file using:

g++ -O2 3.ii -m32 -c

/usr/include/c++/12/bits/stl_algobase.h:431:30: warning: ‘void* __builtin_memcpy(void*, const void*, unsigned int)’ reading between 8 and 2147483644 bytes from a region of size 4 [-Wstringop-overread]
  431 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/usr/include/c++/12/bits/stl_algobase.h:431:30: warning: ‘void* __builtin_memcpy(void*, const void*, unsigned int)’ reading between 8 and 2147483644 bytes from a region of size 4 [-Wstringop-overread]
  431 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


The warnings are still there on trunk, but -Warray-bounds instead of -Wstringop-overread.


---


### compiler : `gcc`
### title : `[aarch64] sequence logic should be combined with mul and umulh`
### open_at : `2022-09-29T17:51:35Z`
### last_modified_date : `2022-10-29T01:54:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107090
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
* test case: https://godbolt.org/z/x5jMhqW8s
```
#  define BN_BITS4        32
#  define BN_MASK2        (0xffffffffffffffffL)
#  define BN_MASK2l       (0xffffffffL)
#  define BN_MASK2h       (0xffffffff00000000L)
#  define BN_MASK2h1      (0xffffffff80000000L)
#  define LBITS(a)        ((a)&BN_MASK2l)
#  define HBITS(a)        (((a)>>BN_BITS4)&BN_MASK2l)
#  define L2HBITS(a)      (((a)<<BN_BITS4)&BN_MASK2)

void mul64(unsigned long in0, unsigned long in1,
           unsigned long &l, unsigned long &h) {
    unsigned long m, m1, lt, ht, bl, bh;
    lt = LBITS(in0);
    ht = HBITS(in0);
    bl = LBITS(in1);
    bh = HBITS(in1);
    m  = bh * lt;
    lt = bl * lt;
    m1 = bl * ht;
    ht = bh * ht;
    m  = (m + m1) & BN_MASK2;
    if (m < m1) ht += L2HBITS((unsigned long)1);
    ht += HBITS(m);
    m1 = L2HBITS(m);
    lt = (lt + m1) & BN_MASK2; if (lt < m1) ht++;
    l  = lt;
    h  = ht;
}
```
* The above source is equel to an mull operater for two 64bits integer vaules, so it should be fold to similar assemble
```
   mul   x8,x1,x0
   umulh x9,x0,x1
   str   x8,[x2]
   str   x9,[x3]
   ret
```


---


### compiler : `gcc`
### title : `AVX512 mask operations not simplified in fully masked loop`
### open_at : `2022-09-30T09:06:43Z`
### last_modified_date : `2023-07-24T08:21:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107093
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Trying to implement WHILE_ULT for AVX512 I run into optimization issues.  Consider

double a[1024], b[1024];

void foo (int n)
{
  for (int i = 0; i < n; ++i)
    a[i] = b[i] * 3.;
}

compiled with -O3 -march=cascadelake --param vect-partial-vector-usage=2

I get snippets like

        kxnorb  %k1, %k1, %k1
        kortestb        %k1, %k1
        je      .L11

or

        kxorb   %k1, %k1, %k1
        kxnorb  %k1, %k1, %k1

where we fail to simplify the operations.  Looking at the RTL it looks
like missed jump threading, but I do see the ops being

(insn 18 72 74 5 (parallel [
            (set (reg:QI 69 k1 [orig:86 loop_mask_15 ] [86])
                (not:QI (xor:QI (reg:QI 69 k1 [orig:86 loop_mask_15 ] [86])
                        (reg:QI 69 k1 [orig:86 loop_mask_15 ] [86]))))
            (unspec [
                    (const_int 0 [0])
                ] UNSPEC_MASKOP)
        ]) 1912 {kxnorqi}
     (expr_list:REG_EQUAL (const_int -1 [0xffffffffffffffff])
        (nil)))

thus having an UNSPEC in them.  When emitting a SET from constm1 I end up
with mask<->GPR moves and if-converted code which isn't optimal either.
When doing -fno-if-conversion I get

.L7:
        vmovapd b(%rax), %ymm1{%k1}
        addl    $4, %ecx
        movl    %edi, %edx
        vmulpd  %ymm2, %ymm1, %ymm0
        subl    %ecx, %edx
        vmovapd %ymm0, a(%rax){%k1}
        kxnorb  %k1, %k1, %k1
        cmpl    $4, %edx
        jge     .L5
        vpbroadcastd    %edx, %xmm0
        vpcmpd  $1, %xmm0, %xmm3, %k1
.L5:
        addq    $32, %rax
        kortestb        %k1, %k1
        jne     .L7

which also doesn't have the desired short-cut from the cmpl $4, %edx.


---


### compiler : `gcc`
### title : `uncprop a bit too eager`
### open_at : `2022-09-30T14:41:32Z`
### last_modified_date : `2022-10-06T08:53:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107099
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For the following testcase

#include <immintrin.h>

__attribute__((target("avx")))
int f(__m128i a[], long n)
{
    for (long i = 0; i < n; i++)
        if (!_mm_testz_si128(a[i], a[i]))
            return 0;
    return 1;
}

gcc -O2 generates

f:
        test    rsi, rsi
        jle     .L4
        xor     eax, eax
        jmp     .L3
.L10:
        add     rax, 1
        cmp     rsi, rax
        je      .L4
.L3:
        mov     rdx, rax
        sal     rdx, 4
        vmovdqa xmm0, XMMWORD PTR [rdi+rdx]
        xor     edx, edx
        vptest  xmm0, xmm0
        sete    dl
        je      .L10
        mov     eax, edx
        ret
.L4:
        mov     edx, 1
        mov     eax, edx
        ret

Note the redundant assignments to edx in the loop and compare with gcc -O2 -fdisable-tree-uncprop1

Also note that generally uncprop adds a data dependency where only a control dependency existed, hurting speculative execution (hence more appropriate for -Os than -O2).


---


### compiler : `gcc`
### title : `SVE function fails to realize it doesn't need the frame-pointer in the tail call.`
### open_at : `2022-09-30T16:37:56Z`
### last_modified_date : `2022-10-06T00:33:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107102
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
The following example:

#include <arm_sve.h>

void foo(svbool_t p, svbool_t q);

void bar() {
    foo(svptrue_b32(), svptrue_b8());
}

generates at -O2 -march=armv8-a+sve2:

bar:
        stp     x29, x30, [sp, -16]!
        ptrue   p1.b, all
        ptrue   p0.s, all
        mov     x29, sp
        bl      foo
        ldp     x29, x30, [sp], 16
        ret

The tail-call gets de-optimized and GCC fails to realize it doesn't need a frame pointer nor the stack pointer at all.

In RTL before frame layout there is no stack usage at all however this looks to happen because in the expansion there's a clobber on the LR:

(call_insn 7 6 10 2 (parallel [
            (call (mem:DI (symbol_ref:DI ("foo") [flags 0x41]  <function_decl 0x7fac39ef9e00 foo>) [0 foo S8 A8])
                (const_int 0 [0]))
            (unspec:DI [
                    (const_int 2 [0x2])
                ] UNSPEC_CALLEE_ABI)
            (clobber (reg:DI 30 x30))
        ]) "example.c":6:5 46 {*call_insn}

and it, it looks like the expansion missed the sibcall?


---


### compiler : `gcc`
### title : `[13 Regression] Failure to discover range results in bogus warning`
### open_at : `2022-10-01T17:59:31Z`
### last_modified_date : `2023-02-21T13:51:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107114
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
After this change (from me):

aa360fbf68b11e54017e8fa5b1bdb87ce7c19188

I'm seeing a case on arc-elf where VRP/Ranger is no longer identifying the range of one object as not including zero.  As a result a test later in the CFG isn't simplified and we get a bogus warning.

What's really interesting here is my change simplifies the CFG by eliminating a handful of blocks in the affected loop (including a sub-loop).

Here's the testcase:
/* { dg-do compile } */

short a;
long b;
void fn1()
{
  int c = a = 1;
  for (; a; a++)
    {
      for (; 9 <= 8;)
        for (;;) {
            a = 20;
            for (; a <= 35; a++)
              ;
line:;
        }
      if ((c += 264487869) == 9)
        {
          unsigned *d = 0;
          for (; b;)
            d = (unsigned *)&c;
          if (d)
            for (;;)
              ;
        }
    }
  goto line;
}



Compiled on arc-elf with -Os -Wall:
[jlaw@X10DRH-iT gcc]$ ./cc1 -Os -Wall k.c -quiet
k.c: In function ‘fn1’:
k.c:17:14: warning: iteration 8 invokes undefined behavior [-Waggressive-loop-optimizations]
   17 |       if ((c += 264487869) == 9)
      |              ^~
k.c:8:10: note: within this loop
    8 |   for (; a; a++)
      |          ^

If we look at the .vrp2 dump before my change we have this:

Global Exported: a_lsm.14_26 = [irange] short int [1, 9] NONZERO 0xf

This is key because we have this in the CFG:

;;   basic block 7, loop depth 1, count 21262216 (estimated locally), maybe hot
;;    prev block 6, next block 1, flags: (NEW, REACHABLE, VISITED)
;;    pred:       2 [always]  count:1346238 (estimated locally) (FALLTHRU,EXECUTABLE) k.c:8:3
;;                6 [always]  count:20092794 (estimated locally) (FALLTHRU,DFS_BACK,EXECUTABLE)
  # a_lsm.14_26 = PHI <1(2), _11(6)>
  # a_lsm_flag.15_28 = PHI <0(2), 1(6)>
  # c_lsm.16_29 = PHI <1(2), _6(6)>
  if (a_lsm.14_26 != 0) 
    goto <bb 6>; [94.50%]
  else 
    goto <bb 3>; [5.50%]


We really want to simplify that condition to a compile-time constant.  That avoids the incorrect warning.

After my change we do not discover the range for a_lsm.14_26 in vfp2 and naturally conditional above isn't simplified and the warning gets triggered.



Maybe I'm missing something subtle, but it looks like the simplifications done in dom3 are resulting in vrp2 missing discovery of the key range.  It's not clear to me why that's that's happening though.

Thoughts?


---


### compiler : `gcc`
### title : `(unsigned)-(int)(bool_var) should be optimized to -(unsigned)bool_var`
### open_at : `2022-10-03T19:40:05Z`
### last_modified_date : `2023-09-05T21:16:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107137
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
```
unsigned f(_Bool a)
{
  int t = a;
  t = -t;
  return t;
}
```

Currently we get:
```
  t_2 = (int) a_1(D);
  t_3 = -t_2;
  _4 = (unsigned int) t_3;
```

But we can do better than that with just:
```
  _ = (unsigned int) a_1(D);
  _4 = -_;
```
Noticed that while looking into https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107131.

Note even bool_var could be an unsigned type or a type which whos size is bigger than the outer type really.


---


### compiler : `gcc`
### title : `[12/13/14 regression] std::variant<std::string, ...> triggers false-positive 'may be used uninitialized' warning`
### open_at : `2022-10-03T20:11:28Z`
### last_modified_date : `2023-07-13T20:38:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107138
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `c++`
### version : `12.2.0`
### severity : `normal`
### contents :
Created attachment 53660
.ii file as per bug writing guidelines

Compiling the following code using an std::string inside an std::variant compiles without warnings on g++ 10.4.0 and 11.3.0 as well as clang++ 14.0.6 with -std=c++17 -Wall -Wextra -O2 -fsanitize=undefined:

```
#include <variant>
#include <vector>
#include <string>

class Toml {
public:
    class Array;

    typedef std::variant<std::string, int, double, bool, Array> value_t;

    class Array {
    public:
        std::vector<value_t> data;

        Array(const std::vector<double> &data)
        {
            this->data.reserve(data.size());
            for (const double &elem : data) {
                this->data.push_back(elem);
            }
        }
    };
};

int main(int argc, char *argv[])
{
    (void) argc;
    (void) argv;

    std::vector<double> a = { 42., 42. };
    auto array = Toml::Array(a);

    return 0;
}
```

However on g++ 12.2.0 as well as gcc 13 trunk (https://godbolt.org/z/6MqGWcPhr) it triggers what I believe are several false positive 'may be used uninitialized' warnings. Removing the std::string from the std::variant resolves the issue.

```
Using built-in specs.
COLLECT_GCC=g++-12
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/12/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none:amdgcn-amdhsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Debian 12.2.0-4' --with-bugurl=file:///usr/share/doc/gcc-12/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-12 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --enable-cet --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-12-zSjAmm/gcc-12-12.2.0/debian/tmp-nvptx/usr,amdgcn-amdhsa=/build/gcc-12-zSjAmm/gcc-12-12.2.0/debian/tmp-gcn/usr --enable-offload-defaulted --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.2.0 (Debian 12.2.0-4) 
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-std=c++17' '-Wall' '-Wextra' '-O2' '-o' 'gcc_bug' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-linux-gnu/12/cc1plus -E -quiet -v -imultiarch x86_64-linux-gnu -D_GNU_SOURCE gcc_bug.cpp -mtune=generic -march=x86-64 -std=c++17 -Wall -Wextra -O2 -fpch-preprocess -fasynchronous-unwind-tables -o gcc_bug.ii
ignoring duplicate directory "/usr/include/x86_64-linux-gnu/c++/12"
ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/12/include-fixed"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/12/../../../../x86_64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/include/c++/12
 /usr/include/x86_64-linux-gnu/c++/12
 /usr/include/c++/12/backward
 /usr/lib/gcc/x86_64-linux-gnu/12/include
 /usr/local/include
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-std=c++17' '-Wall' '-Wextra' '-O2' '-o' 'gcc_bug' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-linux-gnu/12/cc1plus -fpreprocessed gcc_bug.ii -quiet -dumpbase gcc_bug.cpp -dumpbase-ext .cpp -mtune=generic -march=x86-64 -O2 -Wall -Wextra -std=c++17 -version -fasynchronous-unwind-tables -o gcc_bug.s
GNU C++17 (Debian 12.2.0-4) version 12.2.0 (x86_64-linux-gnu)
	compiled by GNU C version 12.2.0, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.25-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
GNU C++17 (Debian 12.2.0-4) version 12.2.0 (x86_64-linux-gnu)
	compiled by GNU C version 12.2.0, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.1, isl version isl-0.25-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 2f2982b7a0eb82900d513759f8d84f81
In file included from /usr/include/c++/12/string:53,
                 from gcc_bug.cpp:3:
In member function ‘std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::length() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’,
    inlined from ‘std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::basic_string(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/include/c++/12/bits/basic_string.h:676:22,
    inlined from ‘constexpr std::__detail::__variant::_Uninitialized<_Type, false>::_Uninitialized(std::in_place_index_t<0>, _Args&& ...) [with _Args = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >}; _Type = std::__cxx11::basic_string<char>]’ at /usr/include/c++/12/variant:283:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<0>, _Args&& ...) [with _Args = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >}; _First = std::__cxx11::basic_string<char>; _Rest = {int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:385:4,
    inlined from ‘void std::_Construct(_Tp*, _Args&& ...) [with _Tp = __detail::__variant::_Variadic_union<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>; _Args = {const in_place_index_t<0>&, __cxx11::basic_string<char, char_traits<char>, allocator<char> >}]’ at /usr/include/c++/12/bits/stl_construct.h:119:7,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)> mutable [with auto:4 = std::__cxx11::basic_string<char>; auto:5 = std::integral_constant<long unsigned int, 0>]’ at /usr/include/c++/12/variant:605:23,
    inlined from ‘constexpr _Res std::__invoke_impl(__invoke_other, _Fn&&, _Args&& ...) [with _Res = void; _Fn = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {__cxx11::basic_string<char, char_traits<char>, allocator<char> >, integral_constant<long unsigned int, 0>}]’ at /usr/include/c++/12/bits/invoke.h:61:36,
    inlined from ‘constexpr typename std::__invoke_result<_Functor, _ArgTypes>::type std::__invoke(_Callable&&, _Args&& ...) [with _Callable = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {__cxx11::basic_string<char, char_traits<char>, allocator<char> >, integral_constant<long unsigned int, 0>}]’ at /usr/include/c++/12/bits/invoke.h:96:40,
    inlined from ‘static constexpr decltype(auto) std::__detail::__variant::__gen_vtable_impl<std::__detail::__variant::_Multi_array<_Result_type (*)(_Visitor, _Variants ...)>, std::integer_sequence<long unsigned int, __indices ...> >::__visit_invoke(_Visitor&&, _Variants ...) [with _Result_type = std::__detail::__variant::__variant_idx_cookie; _Visitor = std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>&&; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&}; long unsigned int ...__indices = {0}]’ at /usr/include/c++/12/variant:1020:17,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = __detail::__variant::__variant_idx_cookie; _Visitor = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {variant<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:1783:105,
    inlined from ‘constexpr void std::__detail::__variant::__raw_idx_visit(_Visitor&&, _Variants&& ...) [with _Visitor = _Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:184:44,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:600:28,
    inlined from ‘std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>::_Copy_assign_base(std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:665:7,
    inlined from ‘std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>::_Move_assign_base(std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:719:7,
    inlined from ‘std::__detail::__variant::_Variant_base<_Types>::_Variant_base(std::__detail::__variant::_Variant_base<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:750:7,
    inlined from ‘std::variant<_Types>::variant(std::variant<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:1404:7,
    inlined from ‘void std::__new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/new_allocator.h:175:4,
    inlined from ‘static void std::allocator_traits<std::allocator<_Tp1> >::construct(allocator_type&, _Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/alloc_traits.h:516:17,
    inlined from ‘std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::emplace_back(_Args&& ...) [with _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/vector.tcc:117:30,
    inlined from ‘void std::vector<_Tp, _Alloc>::push_back(value_type&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:1294:21,
    inlined from ‘Toml::Array::Array(const std::vector<double>&)’ at gcc_bug.cpp:19:37:
/usr/include/c++/12/bits/basic_string.h:1071:16: warning: ‘*(const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)((char*)&<unnamed> + offsetof(std::value_type, std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_storage<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_M_u)).std::__cxx11::basic_string<char>::_M_string_length’ may be used uninitialized [-Wmaybe-uninitialized]
 1071 |       { return _M_string_length; }
      |                ^~~~~~~~~~~~~~~~
gcc_bug.cpp: In constructor ‘Toml::Array::Array(const std::vector<double>&)’:
gcc_bug.cpp:19:37: note: ‘<anonymous>’ declared here
   19 |                 this->data.push_back(elem);
      |                 ~~~~~~~~~~~~~~~~~~~~^~~~~~
In member function ‘std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::length() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’,
    inlined from ‘std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::basic_string(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/include/c++/12/bits/basic_string.h:687:11,
    inlined from ‘constexpr std::__detail::__variant::_Uninitialized<_Type, false>::_Uninitialized(std::in_place_index_t<0>, _Args&& ...) [with _Args = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >}; _Type = std::__cxx11::basic_string<char>]’ at /usr/include/c++/12/variant:283:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<0>, _Args&& ...) [with _Args = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >}; _First = std::__cxx11::basic_string<char>; _Rest = {int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:385:4,
    inlined from ‘void std::_Construct(_Tp*, _Args&& ...) [with _Tp = __detail::__variant::_Variadic_union<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>; _Args = {const in_place_index_t<0>&, __cxx11::basic_string<char, char_traits<char>, allocator<char> >}]’ at /usr/include/c++/12/bits/stl_construct.h:119:7,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)> mutable [with auto:4 = std::__cxx11::basic_string<char>; auto:5 = std::integral_constant<long unsigned int, 0>]’ at /usr/include/c++/12/variant:605:23,
    inlined from ‘constexpr _Res std::__invoke_impl(__invoke_other, _Fn&&, _Args&& ...) [with _Res = void; _Fn = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {__cxx11::basic_string<char, char_traits<char>, allocator<char> >, integral_constant<long unsigned int, 0>}]’ at /usr/include/c++/12/bits/invoke.h:61:36,
    inlined from ‘constexpr typename std::__invoke_result<_Functor, _ArgTypes>::type std::__invoke(_Callable&&, _Args&& ...) [with _Callable = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {__cxx11::basic_string<char, char_traits<char>, allocator<char> >, integral_constant<long unsigned int, 0>}]’ at /usr/include/c++/12/bits/invoke.h:96:40,
    inlined from ‘static constexpr decltype(auto) std::__detail::__variant::__gen_vtable_impl<std::__detail::__variant::_Multi_array<_Result_type (*)(_Visitor, _Variants ...)>, std::integer_sequence<long unsigned int, __indices ...> >::__visit_invoke(_Visitor&&, _Variants ...) [with _Result_type = std::__detail::__variant::__variant_idx_cookie; _Visitor = std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>&&; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&}; long unsigned int ...__indices = {0}]’ at /usr/include/c++/12/variant:1020:17,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = __detail::__variant::__variant_idx_cookie; _Visitor = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {variant<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:1783:105,
    inlined from ‘constexpr void std::__detail::__variant::__raw_idx_visit(_Visitor&&, _Variants&& ...) [with _Visitor = _Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:184:44,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:600:28,
    inlined from ‘std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>::_Copy_assign_base(std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:665:7,
    inlined from ‘std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>::_Move_assign_base(std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:719:7,
    inlined from ‘std::__detail::__variant::_Variant_base<_Types>::_Variant_base(std::__detail::__variant::_Variant_base<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:750:7,
    inlined from ‘std::variant<_Types>::variant(std::variant<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:1404:7,
    inlined from ‘void std::__new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/new_allocator.h:175:4,
    inlined from ‘static void std::allocator_traits<std::allocator<_Tp1> >::construct(allocator_type&, _Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/alloc_traits.h:516:17,
    inlined from ‘std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::emplace_back(_Args&& ...) [with _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/vector.tcc:117:30,
    inlined from ‘void std::vector<_Tp, _Alloc>::push_back(value_type&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:1294:21,
    inlined from ‘Toml::Array::Array(const std::vector<double>&)’ at gcc_bug.cpp:19:37:
/usr/include/c++/12/bits/basic_string.h:1071:16: warning: ‘*(const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)((char*)&<unnamed> + offsetof(std::value_type, std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_storage<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_M_u)).std::__cxx11::basic_string<char>::_M_string_length’ may be used uninitialized [-Wmaybe-uninitialized]
 1071 |       { return _M_string_length; }
      |                ^~~~~~~~~~~~~~~~
gcc_bug.cpp: In constructor ‘Toml::Array::Array(const std::vector<double>&)’:
gcc_bug.cpp:19:37: note: ‘<anonymous>’ declared here
   19 |                 this->data.push_back(elem);
      |                 ~~~~~~~~~~~~~~~~~~~~^~~~~~
In file included from /usr/include/c++/12/vector:64,
                 from gcc_bug.cpp:2:
In constructor ‘std::_Vector_base<_Tp, _Alloc>::_Vector_impl_data::_Vector_impl_data(std::_Vector_base<_Tp, _Alloc>::_Vector_impl_data&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’,
    inlined from ‘std::_Vector_base<_Tp, _Alloc>::_Vector_impl::_Vector_impl(std::_Vector_base<_Tp, _Alloc>::_Vector_impl&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:152:68,
    inlined from ‘std::_Vector_base<_Tp, _Alloc>::_Vector_base(std::_Vector_base<_Tp, _Alloc>&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:335:7,
    inlined from ‘std::vector<_Tp, _Alloc>::vector(std::vector<_Tp, _Alloc>&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:615:7,
    inlined from ‘Toml::Array::Array(Toml::Array&&)’ at gcc_bug.cpp:11:11,
    inlined from ‘constexpr std::__detail::__variant::_Uninitialized<_Type, false>::_Uninitialized(std::in_place_index_t<0>, _Args&& ...) [with _Args = {Toml::Array}; _Type = Toml::Array]’ at /usr/include/c++/12/variant:283:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<0>, _Args&& ...) [with _Args = {Toml::Array}; _First = Toml::Array; _Rest = {}]’ at /usr/include/c++/12/variant:385:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 1; _Args = {Toml::Array}; _First = bool; _Rest = {Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 2; _Args = {Toml::Array}; _First = double; _Rest = {bool, Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 3; _Args = {Toml::Array}; _First = int; _Rest = {double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 4; _Args = {Toml::Array}; _First = std::__cxx11::basic_string<char>; _Rest = {int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘void std::_Construct(_Tp*, _Args&& ...) [with _Tp = __detail::__variant::_Variadic_union<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>; _Args = {const in_place_index_t<4>&, Toml::Array}]’ at /usr/include/c++/12/bits/stl_construct.h:119:7,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)> mutable [with auto:4 = Toml::Array; auto:5 = std::integral_constant<long unsigned int, 4>]’ at /usr/include/c++/12/variant:605:23,
    inlined from ‘constexpr _Res std::__invoke_impl(__invoke_other, _Fn&&, _Args&& ...) [with _Res = void; _Fn = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {Toml::Array, integral_constant<long unsigned int, 4>}]’ at /usr/include/c++/12/bits/invoke.h:61:36,
    inlined from ‘constexpr typename std::__invoke_result<_Functor, _ArgTypes>::type std::__invoke(_Callable&&, _Args&& ...) [with _Callable = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {Toml::Array, integral_constant<long unsigned int, 4>}]’ at /usr/include/c++/12/bits/invoke.h:96:40,
    inlined from ‘static constexpr decltype(auto) std::__detail::__variant::__gen_vtable_impl<std::__detail::__variant::_Multi_array<_Result_type (*)(_Visitor, _Variants ...)>, std::integer_sequence<long unsigned int, __indices ...> >::__visit_invoke(_Visitor&&, _Variants ...) [with _Result_type = std::__detail::__variant::__variant_idx_cookie; _Visitor = std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>&&; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&}; long unsigned int ...__indices = {4}]’ at /usr/include/c++/12/variant:1020:17,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = __detail::__variant::__variant_idx_cookie; _Visitor = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {variant<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:1787:105,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = __detail::__variant::__variant_idx_cookie; _Visitor = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {variant<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:1729:5,
    inlined from ‘constexpr void std::__detail::__variant::__raw_idx_visit(_Visitor&&, _Variants&& ...) [with _Visitor = _Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:184:44,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:600:28,
    inlined from ‘std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>::_Copy_assign_base(std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:665:7,
    inlined from ‘std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>::_Move_assign_base(std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:719:7,
    inlined from ‘std::__detail::__variant::_Variant_base<_Types>::_Variant_base(std::__detail::__variant::_Variant_base<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:750:7,
    inlined from ‘std::variant<_Types>::variant(std::variant<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:1404:7,
    inlined from ‘void std::__new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/new_allocator.h:175:4,
    inlined from ‘static void std::allocator_traits<std::allocator<_Tp1> >::construct(allocator_type&, _Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/alloc_traits.h:516:17,
    inlined from ‘std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::emplace_back(_Args&& ...) [with _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/vector.tcc:117:30,
    inlined from ‘void std::vector<_Tp, _Alloc>::push_back(value_type&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:1294:21,
    inlined from ‘Toml::Array::Array(const std::vector<double>&)’ at gcc_bug.cpp:19:37:
/usr/include/c++/12/bits/stl_vector.h:106:49: warning: ‘*(std::_Vector_base<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>, std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> > >::_Vector_impl_data*)((char*)&<unnamed> + offsetof(std::value_type, std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_storage<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_M_u)).std::_Vector_base<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>, std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> > >::_Vector_impl_data::_M_finish’ may be used uninitialized [-Wmaybe-uninitialized]
  106 |         : _M_start(__x._M_start), _M_finish(__x._M_finish),
      |                                             ~~~~^~~~~~~~~
gcc_bug.cpp: In constructor ‘Toml::Array::Array(const std::vector<double>&)’:
gcc_bug.cpp:19:37: note: ‘<anonymous>’ declared here
   19 |                 this->data.push_back(elem);
      |                 ~~~~~~~~~~~~~~~~~~~~^~~~~~
In constructor ‘std::_Vector_base<_Tp, _Alloc>::_Vector_impl_data::_Vector_impl_data(std::_Vector_base<_Tp, _Alloc>::_Vector_impl_data&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’,
    inlined from ‘std::_Vector_base<_Tp, _Alloc>::_Vector_impl::_Vector_impl(std::_Vector_base<_Tp, _Alloc>::_Vector_impl&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:152:68,
    inlined from ‘std::_Vector_base<_Tp, _Alloc>::_Vector_base(std::_Vector_base<_Tp, _Alloc>&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:335:7,
    inlined from ‘std::vector<_Tp, _Alloc>::vector(std::vector<_Tp, _Alloc>&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:615:7,
    inlined from ‘Toml::Array::Array(Toml::Array&&)’ at gcc_bug.cpp:11:11,
    inlined from ‘constexpr std::__detail::__variant::_Uninitialized<_Type, false>::_Uninitialized(std::in_place_index_t<0>, _Args&& ...) [with _Args = {Toml::Array}; _Type = Toml::Array]’ at /usr/include/c++/12/variant:283:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<0>, _Args&& ...) [with _Args = {Toml::Array}; _First = Toml::Array; _Rest = {}]’ at /usr/include/c++/12/variant:385:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 1; _Args = {Toml::Array}; _First = bool; _Rest = {Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 2; _Args = {Toml::Array}; _First = double; _Rest = {bool, Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 3; _Args = {Toml::Array}; _First = int; _Rest = {double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘constexpr std::__detail::__variant::_Variadic_union<_First, _Rest ...>::_Variadic_union(std::in_place_index_t<_Np>, _Args&& ...) [with long unsigned int _Np = 4; _Args = {Toml::Array}; _First = std::__cxx11::basic_string<char>; _Rest = {int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:391:4,
    inlined from ‘void std::_Construct(_Tp*, _Args&& ...) [with _Tp = __detail::__variant::_Variadic_union<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>; _Args = {const in_place_index_t<4>&, Toml::Array}]’ at /usr/include/c++/12/bits/stl_construct.h:119:7,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)> mutable [with auto:4 = Toml::Array; auto:5 = std::integral_constant<long unsigned int, 4>]’ at /usr/include/c++/12/variant:605:23,
    inlined from ‘constexpr _Res std::__invoke_impl(__invoke_other, _Fn&&, _Args&& ...) [with _Res = void; _Fn = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {Toml::Array, integral_constant<long unsigned int, 4>}]’ at /usr/include/c++/12/bits/invoke.h:61:36,
    inlined from ‘constexpr typename std::__invoke_result<_Functor, _ArgTypes>::type std::__invoke(_Callable&&, _Args&& ...) [with _Callable = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Args = {Toml::Array, integral_constant<long unsigned int, 4>}]’ at /usr/include/c++/12/bits/invoke.h:96:40,
    inlined from ‘static constexpr decltype(auto) std::__detail::__variant::__gen_vtable_impl<std::__detail::__variant::_Multi_array<_Result_type (*)(_Visitor, _Variants ...)>, std::integer_sequence<long unsigned int, __indices ...> >::__visit_invoke(_Visitor&&, _Variants ...) [with _Result_type = std::__detail::__variant::__variant_idx_cookie; _Visitor = std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>&&; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&}; long unsigned int ...__indices = {4}]’ at /usr/include/c++/12/variant:1020:17,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = __detail::__variant::__variant_idx_cookie; _Visitor = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {variant<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:1787:105,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = __detail::__variant::__variant_idx_cookie; _Visitor = __detail::__variant::_Move_ctor_base<false, __cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {variant<__cxx11::basic_string<char, char_traits<char>, allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:1729:5,
    inlined from ‘constexpr void std::__detail::__variant::__raw_idx_visit(_Visitor&&, _Variants&& ...) [with _Visitor = _Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>&&)::<lambda(auto:4&&, auto:5)>; _Variants = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}]’ at /usr/include/c++/12/variant:184:44,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>::_Move_ctor_base(std::__detail::__variant::_Move_ctor_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:600:28,
    inlined from ‘std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>::_Copy_assign_base(std::__detail::__variant::_Copy_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:665:7,
    inlined from ‘std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>::_Move_assign_base(std::__detail::__variant::_Move_assign_base<<anonymous>, _Types>&&) [with bool <anonymous> = false; _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:719:7,
    inlined from ‘std::__detail::__variant::_Variant_base<_Types>::_Variant_base(std::__detail::__variant::_Variant_base<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:750:7,
    inlined from ‘std::variant<_Types>::variant(std::variant<_Types>&&) [with _Types = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array}]’ at /usr/include/c++/12/variant:1404:7,
    inlined from ‘void std::__new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/new_allocator.h:175:4,
    inlined from ‘static void std::allocator_traits<std::allocator<_Tp1> >::construct(allocator_type&, _Up*, _Args&& ...) [with _Up = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>]’ at /usr/include/c++/12/bits/alloc_traits.h:516:17,
    inlined from ‘std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::emplace_back(_Args&& ...) [with _Args = {std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>}; _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/vector.tcc:117:30,
    inlined from ‘void std::vector<_Tp, _Alloc>::push_back(value_type&&) [with _Tp = std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>; _Alloc = std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> >]’ at /usr/include/c++/12/bits/stl_vector.h:1294:21,
    inlined from ‘Toml::Array::Array(const std::vector<double>&)’ at gcc_bug.cpp:19:37:
/usr/include/c++/12/bits/stl_vector.h:107:33: warning: ‘*(std::_Vector_base<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>, std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> > >::_Vector_impl_data*)((char*)&<unnamed> + offsetof(std::value_type, std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_assign_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Move_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Copy_ctor_base<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::<unnamed>.std::__detail::__variant::_Variant_storage<false, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>::_M_u)).std::_Vector_base<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array>, std::allocator<std::variant<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, double, bool, Toml::Array> > >::_Vector_impl_data::_M_end_of_storage’ may be used uninitialized [-Wmaybe-uninitialized]
  107 |           _M_end_of_storage(__x._M_end_of_storage)
      |                             ~~~~^~~~~~~~~~~~~~~~~
gcc_bug.cpp: In constructor ‘Toml::Array::Array(const std::vector<double>&)’:
gcc_bug.cpp:19:37: note: ‘<anonymous>’ declared here
   19 |                 this->data.push_back(elem);
      |                 ~~~~~~~~~~~~~~~~~~~~^~~~~~
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-std=c++17' '-Wall' '-Wextra' '-O2' '-o' 'gcc_bug' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 as -v --64 -o gcc_bug.o gcc_bug.s
GNU assembler version 2.39 (x86_64-linux-gnu) using BFD version (GNU Binutils for Debian) 2.39
COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/12/:/usr/lib/gcc/x86_64-linux-gnu/12/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/12/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/12/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-std=c++17' '-Wall' '-Wextra' '-O2' '-o' 'gcc_bug' '-shared-libgcc' '-mtune=generic' '-march=x86-64' '-dumpdir' 'gcc_bug.'
 /usr/lib/gcc/x86_64-linux-gnu/12/collect2 -plugin /usr/lib/gcc/x86_64-linux-gnu/12/liblto_plugin.so -plugin-opt=/usr/lib/gcc/x86_64-linux-gnu/12/lto-wrapper -plugin-opt=-fresolution=gcc_bug.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu --as-needed -dynamic-linker /lib64/ld-linux-x86-64.so.2 -pie -o gcc_bug /usr/lib/gcc/x86_64-linux-gnu/12/../../../x86_64-linux-gnu/Scrt1.o /usr/lib/gcc/x86_64-linux-gnu/12/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/12/crtbeginS.o -L/usr/lib/gcc/x86_64-linux-gnu/12 -L/usr/lib/gcc/x86_64-linux-gnu/12/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/12/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/12/../../.. gcc_bug.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/lib/gcc/x86_64-linux-gnu/12/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/12/../../../x86_64-linux-gnu/crtn.o
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-std=c++17' '-Wall' '-Wextra' '-O2' '-o' 'gcc_bug' '-shared-libgcc' '-mtune=generic' '-march=x86-64' '-dumpdir' 'gcc_bug.'
```


---


### compiler : `gcc`
### title : `gcc doesn't constant fold member if any other member is mutable`
### open_at : `2022-10-05T17:33:56Z`
### last_modified_date : `2022-10-06T12:39:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107161
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.2.0`
### severity : `enhancement`
### contents :
On this code:

struct mytype
{
    int a;
    mutable int b;
};

extern mytype const p = {1, 2};

int foo()
{
    return p.a + 10;
}

int bar()
{
    return p.b + 10;
}

GCC -O2 generates:
foo():
        mov     eax, DWORD PTR p[rip]
        add     eax, 10
        ret
bar():
        mov     eax, DWORD PTR p[rip+4]
        add     eax, 10
        ret

While clang folds "p.a + 10" into 11:
foo():                                # @foo()
        mov     eax, 11
        ret
bar():                                # @bar()
        mov     eax, dword ptr [rip + p+4]
        add     eax, 10
        ret

I think GCC should do the same.


---


### compiler : `gcc`
### title : `It looks like GCC wastes registers on trivial computations when result can be cached`
### open_at : `2022-10-06T05:33:59Z`
### last_modified_date : `2022-10-07T01:41:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107167
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
I do not know whether it is a big issue or not with targets that provide tons of available registers (like aarch64 or loongarch64). However, this looks like a big issue for x86_64 which only provides 16 general purpose registers (plus %rsp is reserved, so 15 available registers)
Take the example like this:

https://godbolt.org/z/77rEsr1PG

#include<bit>

unsigned Sigma1(unsigned x) noexcept
{
    return std::rotr(x,6)^std::rotr(x,11)^std::rotr(x,25);
}


GCC generates code like this to avoid dependencies.
Sigma1m(unsigned int):
        movl    %edi, %eax
        movl    %edi, %edx
        roll    $7, %edi
        rorl    $6, %eax
        rorl    $11, %edx
        xorl    %edx, %eax
        xorl    %edi, %eax
        ret

However:
mySigma1m(unsigned int):
	movl    %edi, %eax
	rorl    $6, %edi
	rorl    $11, %eax
	xorl	%edi, %eax
	rorl	$19, %edi
	xorl	%edi, %eax
	ret

Saves one register in this task. That becomes a huge problem when tons of computation are involved where registers are in a position of shortage.

1st one also generates 1 more instruction and it can affect the code cache.

Aggressively utilizing all registers may not give the best results. Local maximum =/= Global maximum.
I don't know.


---


### compiler : `gcc`
### title : `[aarch64] regression with optimization -fexpensive-optimizations`
### open_at : `2022-10-09T10:33:16Z`
### last_modified_date : `2023-05-19T01:22:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107190
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
This case is simplify from https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107090, and we can see that the codegen of function `test_m` has some regression with optimization -fexpensive-optimizations, https://gcc.godbolt.org/z/zbKrEox4j

This is because the pass 208t.widening_mul is controlled by -fexpensive-optimizations (default on at -O3), it conversion

```
  m_12 = m_9 + m1_10;
  if (m1_10 > m_12)
```
into

```
  _17 = .ADD_OVERFLOW (m_9, m1_10);
  m_12 = REALPART_EXPR <_17>;
  _18 = IMAGPART_EXPR <_17>;
  if (_18 != 0)``

```


---
