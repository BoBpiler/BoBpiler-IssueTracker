### Total Bugs Detected: 4649
### Current Chunk: 30 of 30
### Bugs in this Chunk: 9 (From bug 4641 to 4649)
---


### compiler : `gcc`
### title : ``a - b < 0` is not transformed into `a < b` for signed types`
### open_at : `2023-10-21T21:13:59Z`
### last_modified_date : `2023-10-23T15:19:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111910
### status : `ASSIGNED`
### tags : `FIXME, missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
From match.pd:

/* Transform comparisons of the form X - Y CMP 0 to X CMP Y.
   ??? The transformation is valid for the other operators if overflow
   is undefined for the type, but performing it here badly interacts
   with the transformation in fold_cond_expr_with_comparison which
   attempts to synthetize ABS_EXPR.  */

I am looking into that fix that for either GCC 14 or 15.


---


### compiler : `gcc`
### title : `RISC-V: Use vsetvl insn replace csrr vlenb insn`
### open_at : `2023-10-23T03:36:03Z`
### last_modified_date : `2023-10-24T03:00:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111926
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
We can use: 
        vsetvl a5, zero, e8, mf8, ta, ta
replace:
        csrr    a4,vlenb
        srli    a4,a4,3

The reason for this is that the performance of the vsetvl instruction tends to be better optimised than the csrr instruction.

#include <riscv_vector.h>

#define exhaust_vector_regs()                                                  \
  asm volatile("#" ::                                                          \
		 : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", \
		   "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17",     \
		   "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25",     \
		   "v26", "v27", "v28", "v29", "v30", "v31");
           
void
spill_1 (int8_t *in, int8_t *out)
{
  vint8mf8_t v1 = *(vint8mf8_t*)in;
  exhaust_vector_regs ();
  *(vint8mf8_t*)out = v1;
}

spill_1(signed char*, signed char*):
        csrr    a4,vlenb
        srli    a4,a4,3
        csrr    t0,vlenb
        slli    a3,a4,3
        sub     sp,sp,t0
        sub     a3,a3,a4
        add     a3,a3,sp
        vsetvli a5,zero,e8,mf8,ta,ma
        vle8.v  v1,0(a0)
        vse8.v  v1,0(a3)
        csrr    a4,vlenb
        srli    a4,a4,3
        slli    a3,a4,3
        sub     a3,a3,a4
        add     a3,a3,sp
        vle8.v  v1,0(a3)
        csrr    t0,vlenb
        vse8.v  v1,0(a1)
        add     sp,sp,t0
        jr      ra


https://godbolt.org/z/TcKxbjnoh


---


### compiler : `gcc`
### title : `RISC-V: Trivial optimization of VSETVL PASS`
### open_at : `2023-10-23T09:26:47Z`
### last_modified_date : `2023-10-23T17:53:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111931
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include "riscv_vector.h"

void foo9 (int8_t *base, int8_t* out, size_t vl, size_t m)
{
    vint8mf8_t v0;
    size_t avl = __riscv_vsetvl_e8mf8 (vl);

    for (size_t i = 0; i < m; i++)
    {
        v0 = __riscv_vle8_v_i8mf8 (base + i, avl);
        __riscv_vse8_v_i8mf8 (out + i, v0, avl);
    }
}

ASM:

        vsetvli a2,a2,e8,mf8,ta,ma
        beq     a3,zero,.L8
        add     a3,a0,a3
.L3:
        vle8.v  v1,0(a0)
        addi    a0,a0,1
        vse8.v  v1,0(a1)
        addi    a1,a1,1
        bne     a0,a3,.L3
.L8:
        ret

The vsetvl should be optimized into "vsetvl zero, a2" instead of "vsetvl a2,a2"

The reason we failed to optimize it is because we set the entry block as unknown block when computing reaching_def. So preds_has_same_avl_p is false.

As long as there are predecessors block before the user vsetvl, we can optimize it.

This is an trivial issue. I will fix it if I find the time.


---


### compiler : `gcc`
### title : `memcpy on Xtensa not optimized when n == sizeof(uint32_t) or sizeof(uint64_t)`
### open_at : `2023-10-23T11:16:12Z`
### last_modified_date : `2023-10-23T13:54:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111933
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.2.0`
### severity : `normal`
### contents :
This issue is about what I think being a missing optimization on ESP32 Xtensa GCC compiler. 

I tested the same issue on versions between gcc 8.4.0 and 11.2.0 with Xtensa ESP32/ESP32-S2/ESP32-S3 GCC. 

I'm writing some functions for unaligned memory access and I've been checking them with Compiler Explorer (https://godbolt.org/) and I'm getting some (I think) sub-optimal outputs.

As far as I understood on ESP32 Xtensa a 32-bit unaligned memory access is faster than 4 8-bit accesses, however I'm getting the following results (using -O2) and the following snippets of code:

Function that calls the inline from_unaligned_u32:
bool test2(uint32_t *in)
{
    uint32_t got = from_unaligned_u32(in);
    if (got > 5) {
        return true;
    }

    return false;
}

A:
uint32_t from_unaligned_u32(uint32_t *unaligned)
{
    uint32_t tmp;
    tmp = *unaligned;
    return tmp;
}

generates:
test2(unsigned int*):
        entry   sp, 32
        l32i.n  a8, a2, 0
        movi.n  a2, 1
        bgeui   a8, 6, .L2
        movi.n  a2, 0
.L2:
        extui   a2, a2, 0, 1
        retw.n
        

B:
inline uint32_t from_unaligned_u32(uint32_t *unaligned)
{
    uint32_t tmp;
    memcpy(&tmp, unaligned, sizeof(tmp));
    return tmp;
}

generates:
test2(unsigned int*):
        entry   sp, 48
        l8ui    a8, a2, 2
        l8ui    a10, a2, 0
        l8ui    a9, a2, 1
        l8ui    a2, a2, 3
        s8i     a10, sp, 0
        s8i     a2, sp, 3
        s8i     a9, sp, 1
        s8i     a8, sp, 2
        l32i.n  a8, sp, 0
        movi.n  a2, 1
        bgeui   a8, 6, .L2
        movi.n  a2, 0
.L2:
        extui   a2, a2, 0, 1
        retw.n

My assumption here is that unaligned access on Xtensa ESP32 is faster than calling memcpy or multiple 1-byte loads (please let me know if I am wrong), so from my point of view is a missing optimization.

I would expect both A and B generating the same assembly code like on other archs.

Also interstingly the uint64_t "B" version (that is similar to the previous), generates a call to memcpy instead of some inline code.


---


### compiler : `gcc`
### title : `combine split points are not so good with targets that have (and (not x) y)`
### open_at : `2023-10-24T06:01:49Z`
### last_modified_date : `2023-10-24T06:01:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111949
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
bool f1(int a, bool b)
{
        int c = a & b;
        return (c ^ a)&1;
}
```
Currently GCC produces:
```
        and     w1, w1, 255
        bic     w0, w0, w1
        and     w0, w0, 1
```

Notice how there are 2 and.

If we look at combine dumps we will see:
Trying 3, 8 -> 10:
    3: r98:SI=zero_extend(x1:QI)
      REG_DEAD x1:QI
    8: r101:SI=~r98:SI&r103:SI
      REG_DEAD r98:SI
      REG_DEAD r103:SI
   10: r102:SI=r101:SI&0x1
      REG_DEAD r101:SI
Failed to match this instruction:
(set (reg:SI 102)
    (and:SI (and:SI (not:SI (reg:SI 1 x1 [ b ]))
            (reg:SI 103))
        (const_int 1 [0x1])))
Successfully matched this instruction:
(set (reg:SI 101)
    (not:SI (reg:SI 1 x1 [ b ])))
Failed to match this instruction:
(set (reg:SI 102)
    (and:SI (and:SI (reg:SI 101)
            (reg:SI 103))
        (const_int 1 [0x1])))

The first part is good but the second part is not so good and shows that combine not finding a good split point and using:
(and:SI (not:SI (reg:SI 1 x1 [ b ])) (reg:SI 103))
as the point how to split the above instruction.

(note I don't know if this should be a generic change or a target specific one off hand, just filing it to keep track of what missed optimization I found).


---


### compiler : `gcc`
### title : ``a ? abs(a) : 0` is not simplified to just abs(a)`
### open_at : `2023-10-24T16:15:02Z`
### last_modified_date : `2023-10-25T03:43:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111957
### status : `ASSIGNED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(int a)
{
  if (a)
  return a > 0 ? a : -a;
  return 0;
}
```

This should just simplify to ` return a > 0 ? a : -a;` or rather `return abs(a);`


---


### compiler : `gcc`
### title : `tree_single_nonnegative_warnv_p could use tree_nonzero_bits for SSA_NAMES and int types`
### open_at : `2023-10-24T16:25:45Z`
### last_modified_date : `2023-10-25T03:51:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111959
### status : `NEW`
### tags : `internal-improvement, missed-optimization, patch`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
I Noticed this while looking into PR 101590.

tree_single_nonnegative_warnv_p  currently does:
    case SSA_NAME:
      /* Limit the depth of recursion to avoid quadratic behavior.
	 This is expected to catch almost all occurrences in practice.
	 If this code misses important cases that unbounded recursion
	 would not, passes that need this information could be revised
	 to provide it through dataflow propagation.  */
      return (!name_registered_for_update_p (t)
	      && depth < param_max_ssa_name_query_depth
	      && gimple_stmt_nonnegative_warnv_p (SSA_NAME_DEF_STMT (t),
						  strict_overflow_p, depth));

But we could/should use tree_nonzero_bits first to see if the info is already cached somewhere.  This might speed up things.

An example of where this would improve is:
```
int f(int a, int b)
{
  if (a & ~0xff) __builtin_unreachable();
  return a / (1<<b);
}
int f1(int a, int b)
{
  if (a & ~0xff) __builtin_unreachable();
  return a >>b;
}
```

There are other examples too.


---


### compiler : `gcc`
### title : `[14 regression] missed vectorzation for bool a = j != 1; j = (long int)a;`
### open_at : `2023-10-25T08:14:43Z`
### last_modified_date : `2023-10-25T16:46:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111972
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
cat test.c

double
foo() {
  long n3 = 3450000, xtra = 7270;
  long i,ix;
  long j;
  double Check;

  /* Section 3, Conditional jumps */
  j = 0;
  {
    for (ix=0; ix<xtra; ix++)
      {
	for(i=0; i<n3; i++)
	  {
	    if(j==1)       j = 2;
	    else           j = 3;
	    if(j>2)        j = 0;
	    else           j = 1;
	    if(j<1)        j = 1;
	    else           j = 0;
	  }
      }
  }
  Check = Check + (double)j;
  return Check;
}

The different between gcc 13 dump and gcc14 dump is
GCC13 we have

  <bb 3> [local count: 1063004411]:
  # i_16 = PHI <i_13(8), 0(5)>
  # j_18 = PHI <_7(8), j_21(5)>
  # ivtmp_15 = PHI <ivtmp_6(8), 3450000(5)>
  _7 = j_18 ^ 1;
  i_13 = i_16 + 1;
  ivtmp_6 = ivtmp_15 - 1;
  if (ivtmp_6 != 0)
    goto <bb 8>; [99.00%]
  else
    goto <bb 4>; [1.00%]

GCC14 we have

  <bb 3> [local count: 1063004410]:
  # i_17 = PHI <i_13(8), 0(5)>
  # j_19 = PHI <_14(8), j_22(5)>
  # ivtmp_16 = PHI <ivtmp_15(8), 3450000(5)>
  _9 = j_19 != 1;
  _14 = (long int) _9;
  i_13 = i_17 + 1;
  ivtmp_15 = ivtmp_16 - 1;
  if (ivtmp_15 != 0)
    goto <bb 8>; [98.99%]
  else
    goto <bb 4>; [1.01%]

Vectorizer can handle 
  
  _7 = j_18 ^ 1; 

but not

  _9 = j_19 != 1;
  _14 = (long int) _9;


../test.C:11:18: note:   vect_is_simple_use: operand j_19 != 1, type of def: internal
../test.C:11:18: note:   mark relevant 2, live 0: _9 = j_19 != 1;
../test.C:11:18: note:   worklist: examine stmt: _9 = j_19 != 1;
../test.C:11:18: note:   vect_is_simple_use: operand j_19 = PHI <_14(8), j_22(5)>, type of def: unknown
../test.C:11:18: missed:   Unsupported pattern.
../test.C:15:6: missed:   not vectorized: unsupported use in stmt.
../test.C:11:18: missed:  unexpected pattern.


The difference comes from phiopt2.


---


### compiler : `gcc`
### title : ```(1 >> X) != 0` pattern should be extended to support other constants`
### open_at : `2023-10-25T22:17:52Z`
### last_modified_date : `2023-10-25T22:42:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=112090
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
We could extend it to support power of 2 constants or more.

Example:
```
int f(int a)
{
  return 2 != (2 >> a);
}
```

This could be optimized to `a != 0`


---
