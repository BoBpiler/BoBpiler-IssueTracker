### Total Bugs Detected: 4649
### Current Chunk: 23 of 30
### Bugs in this Chunk: 160 (From bug 3521 to 3680)
---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] extra mov with int->double conversion and addition (incoming arguments and return)`
### open_at : `2021-10-13T21:08:08Z`
### last_modified_date : `2023-09-21T07:49:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102737
### status : `NEW`
### tags : `missed-optimization, ra, ssemmx`
### component : `target`
### version : `12.0`
### severity : `minor`
### contents :
double foo(int s, double a)
{
  return s + a;
}

With -O3, GCC outputs this on AMD64:

foo(int, double):
        movapd  xmm1, xmm0
        pxor    xmm0, xmm0
        cvtsi2sd        xmm0, edi
        addsd   xmm0, xmm1
        ret

LLVM outputs this:

foo(int, double):
        cvtsi2sd        xmm1, edi
        addsd   xmm0, xmm1
        ret

The movapd in GCC's version can be optimized out.


---


### compiler : `gcc`
### title : `Failure to optimize (signed) right shift if the range is already known to be [-1, 0]`
### open_at : `2021-10-14T00:36:37Z`
### last_modified_date : `2023-09-21T07:49:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102738
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
int a(__int128 f, int g)
{
    return (f >> 127) >> g;
}

This can be optimized to `return f >> 127;`. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[12 Regression] 433.milc regressed by 10% on AMD zen2 at -Ofast -march=native -flto after r12-3893-g6390c5047adb75`
### open_at : `2021-10-14T16:50:25Z`
### last_modified_date : `2022-03-23T08:51:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102750
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
I have bisected an AMD zen2 10% performance regression of SPEC 2006 FP
433.milc benchmark when compiled with -Ofast -march=native -flto to
commit r12-3893-g6390c5047adb75.  See also:

  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=412.70.0&plot.1=289.70.0&

As Richi asked, I am filing this bug even though I cannot reproduce the
regression neither on an AMD zen3 machine nor on Intel CascadeLake, because
the history of the benchmark performance and because I know milc can be
sensitive to conditions outside our control.  OTOH, the regression
reproduces reliably for me.

Some relevant perf data:

BEFORE:
# Samples: 585K of event 'cycles:u'
# Event count (approx.): 472738682838
#
# Overhead       Samples  Command          Shared Object           Symbol
# ........  ............  ...............  ......................  .........................................
# 
    24.59%        140397  milc_peak.mine-  milc_peak.mine-lto-nat  [.] u_shift_fermion
    18.47%        105497  milc_peak.mine-  milc_peak.mine-lto-nat  [.] add_force_to_mom
    15.97%         96343  milc_peak.mine-  milc_peak.mine-lto-nat  [.] mult_su3_na
    15.29%         90027  milc_peak.mine-  milc_peak.mine-lto-nat  [.] mult_su3_nn
     5.55%         35114  milc_peak.mine-  milc_peak.mine-lto-nat  [.] path_product
     4.75%         27693  milc_peak.mine-  milc_peak.mine-lto-nat  [.] compute_gen_staple
     2.76%         16109  milc_peak.mine-  milc_peak.mine-lto-nat  [.] mult_su3_an
     2.42%         14255  milc_peak.mine-  milc_peak.mine-lto-nat  [.] imp_gauge_force.constprop.0
     2.02%         11561  milc_peak.mine-  milc_peak.mine-lto-nat  [.] mult_adj_su3_mat_4vec

AFTER:
# Samples: 634K of event 'cycles:u'
# Event count (approx.): 513635733685
#
# Overhead       Samples  Command          Shared Object           Symbol                                   
# ........  ............  ...............  ......................  .........................................
#
    24.04%        149010  milc_peak.mine-  milc_peak.mine-lto-nat  [.] add_force_to_mom
    23.76%        147370  milc_peak.mine-  milc_peak.mine-lto-nat  [.] u_shift_fermion
    14.19%         90929  milc_peak.mine-  milc_peak.mine-lto-nat  [.] mult_su3_nn
    14.14%         92912  milc_peak.mine-  milc_peak.mine-lto-nat  [.] mult_su3_na
     4.90%         33846  milc_peak.mine-  milc_peak.mine-lto-nat  [.] path_product
     3.89%         24621  milc_peak.mine-  milc_peak.mine-lto-nat  [.] mult_su3_an
     3.62%         22831  milc_peak.mine-  milc_peak.mine-lto-nat  [.] compute_gen_staple
     2.05%         13215  milc_peak.mine-  milc_peak.mine-lto-nat  [.] imp_gauge_force.constprop.0


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Complete unrolling is too senative to PRE; c-c++-common/torture/vector-compare-2.c`
### open_at : `2021-10-15T00:16:14Z`
### last_modified_date : `2023-05-08T12:22:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102756
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The visium-elf port is a bit broken in that any code which calls abort will fail to link.   This has turned out to be useful in that it has pointed out cases where the quality of our code generation has suffered.

The change to turn on the vectorizer by default at -O2 is yet another example.

c-c++-common/torture/vector-compare-2.c before the vectorizer change compiled down to this code at -O2:



        .file   "j.c"
        .text
        .align  4
        .p2align 8
        .global foo
        .type   foo, @function
foo:
        moviu   r9,65535
        movil   r9,65533
        write.l (r1),r9
        write.l 1(r1),r9
        write.l 2(r1),r9
        bra     tr,r21,r0               ;return
         write.l 3(r1),r9
        .size   foo, .-foo
        .section        .text.startup,"ax",@progbits
        .align  4
        .p2align 8
        .global main
        .type   main, @function
main:
        bra     tr,r21,r0               ;return
         moviq   r1,0           ;movsi  r  J
        .size   main, .-main
        .ident  "GCC: (GNU) 12.0.0 20211008 (experimental)"


Of particular note "main" does _not_ call abort.  The optimizers have figured everything out and realized that it should never abort.

After enabling the vectorizer at -O2 we get:
        .file   "j.c"
        .text
        .align  4
        .p2align 8
        .global foo
        .type   foo, @function
foo:
        moviu   r9,65535
        movil   r9,65533
        write.l (r1),r9
        write.l 1(r1),r9
        write.l 2(r1),r9
        bra     tr,r21,r0               ;return
         write.l 3(r1),r9
        .size   foo, .-foo
        .section        .text.startup,"ax",@progbits
        .align  4
        .p2align 8
        .global main
        .type   main, @function
main:
        subi    sp,36
        moviq   r10,23          ;movsi  r  J
        write.l (sp),fp
        move.l  fp,sp           ;stack_save
        add.l   r8,fp,r10
        lsr.l   r8,r8,4
        asl.l   r8,r8,4
        moviu   r9,65535
        write.l 1(sp),r21
        movil   r9,65533
        write.l (r8),r9
        write.l 1(r8),r9
        write.l 2(r8),r9
        write.l 3(r8),r9
        move.l  r10,r8
        moviq   r8,16           ;movsi  r  J
        add.l   r7,r10,r8
.L5:
        read.l  r8,(r10)
        cmp.l   r8,r9
        brr     ne,.L8
         addi    r10,4
        cmp.l   r10,r7
        brr     ne,.L5
         moviq   r1,0           ;movsi  r  J
        read.l  fp,(sp)
        read.l  r21,1(sp)
        bra     tr,r21,r0               ;return
         addi    sp,36          ;stack pop
.L8:
        moviu   r10,%u abort
        movil   r10,%l abort
        bra     tr,r10,r21
         nop            ;call
        .size   main, .-main
        .ident  "GCC: (GNU) 12.0.0 20211008 (experimental)"


Note how there's a call to abort starting at the label .L8.  And more generally the code in "main" is considerably larger and more complex.

This is a code quality regression.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Failure to use registers optimally with return values (2 operands related and subreg)`
### open_at : `2021-10-15T02:24:30Z`
### last_modified_date : `2023-09-21T07:48:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102758
### status : `NEW`
### tags : `missed-optimization, needs-bisection, ra, ssemmx`
### component : `target`
### version : `12.0`
### severity : `minor`
### contents :
#include <stdint.h>

typedef int64_t v2i64 __attribute__((vector_size(16)));
typedef uint16_t v8u16 __attribute__((vector_size(16)));

v2i64 f(v8u16 make_b_xxm1, v2i64 b)
{
    return (v2i64)((v8u16)b + (v8u16){1});
}

With -O3, GCC outputs this:

f(unsigned short __vector(8), long __vector(2)):
        movdqa  xmm2, XMMWORD PTR .LC0[rip]
        paddw   xmm2, xmm1
        movdqa  xmm0, xmm2
        ret

LLVM outputs this:

f(unsigned short __vector(8), long __vector(2)):
        movdqa  xmm0, xmm1
        paddw   xmm0, xmmword ptr [rip + .LCPI0_0]
        ret

It should be possible to optimize out the last `movdqa`. This seems to be directly related to the usage of differing types here (even though the conversion is cost-free) as replacing all usage of `v2i64` with `v8u16` makes this be better optimized.


---


### compiler : `gcc`
### title : `AArch64: sequential comparisons with equal conditional blocks don't use ccmp`
### open_at : `2021-10-15T21:07:50Z`
### last_modified_date : `2023-08-26T07:27:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102793
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following code:

int ccmp(uint64_t* s1, uint64_t* s2, int(*foo)(void))
{
    uint64_t d1, d2, bar;

    d1 = *s1++;
    d2 = *s2++;
    bar = (d1 ^ d2) & 0xabcd;
    if (bar == 0 || d1 != d2)
      return foo();
    return 0;
}

int noccmp(uint64_t* s1, uint64_t* s2, int(*foo)(void))
{
    uint64_t d1, d2, bar;

    d1 = *s1++;
    d2 = *s2++;
    bar = (d1 ^ d2) & 0xabcd;
    if (bar == 0)
      return foo();
    if (d1 != d2)
      return foo();
    return 0;
}

...produces (GCC master or earlier, ARM64, with -O3):

ccmp:
        ldr     x3, [x0]
        mov     x4, 43981
        ldr     x0, [x1]
        eor     x1, x3, x0
        tst     x1, x4
        ccmp    x3, x0, 0, ne
        bne     .L5
        mov     w0, 0
        ret
.L5:
        mov     x16, x2
        br      x16
noccmp:
        ldr     x3, [x0]
        mov     x4, 43981
        ldr     x0, [x1]
        eor     x1, x3, x0
        tst     x1, x4
        beq     .L8
        cmp     x3, x0
        beq     .L9
.L8:
        mov     x16, x2
        br      x16
.L9:
        mov     w0, 0
        ret

Since both conditional blocks do exactly the same (they call foo()), both functions could use the ccmp instruction.
I just observed this and have not analysed this any further.


---


### compiler : `gcc`
### title : `[12 Regression] missing vrp in evrp dealing with casts and ands`
### open_at : `2021-10-16T04:49:55Z`
### last_modified_date : `2021-10-22T21:39:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102794
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
void foo(void);
int main() {
  int b = 0;
  int a = -100;
  for (; a; ++a) {
    unsigned short d = a;
    if (!(b | d) && d)
      foo();
  }
  return 0;
}

--- CUT ----
This used to be optimized during evrp in 11.1.0/11.2.0 but trunk evrp does not.
This is reduced from PR 102703 but is related issue though different.


---


### compiler : `gcc`
### title : `RFE: recognize !! vs branch similarity better`
### open_at : `2021-10-16T08:25:12Z`
### last_modified_date : `2023-06-09T16:19:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102795
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Input
=====

unsigned long p1(unsigned long x)
{
        return x - x / 10 - !!(x % 10);
}
unsigned long p2(unsigned long x)
{
        if (x % 10 == 0)
                return x - x / 10;
        return x - x / 10 - 1;
}
unsigned long p3(unsigned long x)
{
        unsigned long z = x - x / 10;
        if (x % 10)
                --z;
        return z;
}

Outcome
=======

► ./host-x86_64-pc-linux-gnu/gcc/xg++ -B ./host-x86_64-pc-linux-gnu/gcc -c -O3 -v t.cpp
GNU C++17 (GCC) version 12.0.0 20211016 (experimental) (x86_64-pc-linux-gnu)
        compiled by GNU C version 11.2.1 20210816 [revision 056e324ce46a7924b5cf10f61010cf9dd2ca10e9], GMP version 6.2.1, MPFR version 4.1.0-p7, MPC version 1.2.1, isl version none
GGC heuristics: --param ggc-min-expand=30 --param ggc-min-heapsize=4096

► objdump -Mintel -d t.o
0000000000000000 <_Z2p1m>:
...
  24:   0f 95 c2                setne  dl
  27:   0f b6 d2                movzx  edx,dl
  2a:   48 29 d0                sub    rax,rdx
...

0000000000000030 <_Z2p2m>:
...
  4f:   48 29 d6                sub    rsi,rdx
  52:   48 29 d0                sub    rax,rdx
  55:   48 01 c9                add    rcx,rcx
  58:   48 39 cf                cmp    rdi,rcx
  5b:   48 0f 44 c6             cmove  rax,rsi
...

0000000000000060 <_Z2p3m>:
...
  81:   48 29 d7                sub    rdi,rdx
  84:   48 83 ff 01             cmp    rdi,0x1
  88:   48 83 d0 ff             adc    rax,0xffffffffffffffff
...


Expected outcome
================

I would have hoped that the optimizer were able to reduce p1, p2 and p3 to the same asm.


---


### compiler : `gcc`
### title : `Incorrect UB warning with aggressive-loop-optimizations`
### open_at : `2021-10-17T03:49:52Z`
### last_modified_date : `2023-05-12T06:36:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102800
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `rtl-optimization`
### version : `11.1.0`
### severity : `normal`
### contents :
I found this spurious warning occurs since gcc 11.1.  It does not occur in 10.3 or previous versions that I tested.

I believe this is a different bug from PR100801 because that bug started appearing in 9.1.

This bug seems to be very sensitive to changes from this minimal code:
----------------[ Source ]-----------------
/* Compiled with: gcc-11 -O2 -c test.c   */
void bar(char x);
void foo(char * begin)
{
    // end = ALIGN(begin, 8);
    char * end = begin + 7 - ((((unsigned long)begin) - 1) % 8);

    if (begin != end) {
        long long x = end - begin;
        do bar(x);
        while(--x != 0);
    }
}

static char buff[1024] = { 0 };
void test(int i)
{
    foo(buff + i * 8);
}

------------[ Compiler output ]-------------

In function 'void foo(char*)',
    inlined from 'void test(int)' at <source>:16:8:
<source>:9:19: warning: iteration 0x8000000000000000 invokes undefined behavior [-Waggressive-loop-optimizations]
    9 |         while(--x != 0);
      |               ~~~~^~~~
<source>:9:19: note: within this loop
--------------------------------------------

Notice we don't enter the loop unless x will be greater than zero because of the `begin != end` check, and because end >= begin due to the ALIGN math.

The compiled code appears to be correct. Only the spurious warning is a problem. (In fact, the compiled code is optimized out where the warning is generated because the inlined function has nothing to do. But the code is called from elsewhere, so size and correctness matter, of course.)

I found several workarounds:

> Change x to unsigned.  Produces exact same obj output but no warning:
        unsigned long long x = end - begin;

> Change x to short (int16_t). Produces exact same obj output but no warning:
        short x = end - begin;

> Compare with > 0 instead of != 0. Compiled output is 1 opcode longer:
        while(--x > 0);

> Use calculated x value to determine whether to enter loop.
    long long x = end - begin;
    if (x > 0)
        do bar(x);
        while(--x != 0);

----

I just noticed this bug (spurious warning) appears in 9.3 and earlier if I change this line from != to <:

    if (begin < end) {

I guess it is this subtle difference that I think is a new bug. But maybe this is a dup of PR100801 after all.


---


### compiler : `gcc`
### title : `at -O2, spurious stringop-overflow warning writing to std::vector::back()`
### open_at : `2021-10-17T22:47:31Z`
### last_modified_date : `2022-12-12T17:18:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102805
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `normal`
### contents :
g++ -O2 -Wextra -Wall -Werror -Wextra vectorBackWrite.cpp

#include <vector>
#include <stddef.h>
#include <iostream>

extern FILE* f;
void triggerBug(uint64_t start, uint64_t end) {
  if (f && end > start) {
    std::vector<char> data(end - start + 1);
    auto res = fread(&data[0], end-start, 1, f);
    if (res == 1) {
      data.back() = 0;
    }
  }
}

error is:

vectorBackWrite.cpp: In function ‘void triggerBug(uint64_t, uint64_t)’:
vectorBackWrite.cpp:11:19: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
   11 |       data.back() = 0;
      |       ~~~~~~~~~~~~^~~
In file included from /opt/gcc-11.2.0/include/c++/11.2.0/x86_64-linux-gnu/bits/c++allocator.h:33,
                 from /opt/gcc-11.2.0/include/c++/11.2.0/bits/allocator.h:46,
                 from /opt/gcc-11.2.0/include/c++/11.2.0/vector:64,
                 from vectorBackWrite.cpp:1:
/opt/gcc-11.2.0/include/c++/11.2.0/ext/new_allocator.h:127:48: note: at offset [0, 9223372036854775806] into destination object of size [2, 9223372036854775807] allocated by ‘operator new’
  127 |         return static_cast<_Tp*>(::operator new(__n * sizeof(_Tp)));
      |                                  ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~
cc1plus: all warnings being treated as errors


Interestingly, this also fails w/ the same error:
*(data.rbegin()) = 0;

but this is accepted:
data[data.size()-1] = 0;

code works in gcc7 & gcc9 on the same platform, & works on gcc11 with -O1
It also seems to require the fread to be present

Version: 11.2.0

system: CentOS Linux 7.7.1908 on Intel Xeon


---


### compiler : `gcc`
### title : `[x86] Suboptimal codegen for v4hi vector concat under -mavx512bw and -mavx512vl`
### open_at : `2021-10-18T06:09:40Z`
### last_modified_date : `2021-10-18T06:53:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102806
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
For

typedef short v8hi __attribute__((vector_size (16)));
typedef short v4hi __attribute__((vector_size (8))); 

v8hi foov (v4hi a, v4hi b)                                       
{                                                               
 return __builtin_shufflevector (a, b, 0, 1, 2, 3, 4, 5, 6, 7);
}

gcc -O2 -mavx512vl -mavx512bw:

        vmovq   %xmm0, %xmm2
        vmovq   %xmm1, %xmm1
        vmovdqa .LC0(%rip), %xmm0
        vpermi2w        %xmm1, %xmm2, %xmm0
        ret

While clang with same option:

        vmovlhps        %xmm1, %xmm0, %xmm0             # xmm0 = xmm0[0],xmm1[0]
        retq

It looks like expand order of permutation should be adjusted


---


### compiler : `gcc`
### title : `[12 regression] sve/mask_gather_load_1.c fails since g:9b2ad21ab3ebc21a3408108327fa1a7cbedaf217`
### open_at : `2021-10-18T07:33:27Z`
### last_modified_date : `2021-12-03T16:43:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102808
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
Hi,

Since g:9b2ad21ab3ebc21a3408108327fa1a7cbedaf217, I have noticed a few regressions in the aarch64 tests:

FAIL: gcc:gcc.target/aarch64/sve/aarch64-sve.exp=gcc.target/aarch64/sve/mask_gather_load_1.c scan-assembler-times \\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+\\.d, lsl 3\\]\\n 9
FAIL: gcc:gcc.target/aarch64/sve/aarch64-sve.exp=gcc.target/aarch64/sve/mask_gather_load_2.c scan-assembler-times \\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+\\.d, lsl 3\\]\\n 9
FAIL: gcc:gcc.target/aarch64/sve/aarch64-sve.exp=gcc.target/aarch64/sve/mask_gather_load_3.c scan-assembler-times \\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+\\.d\\]\\n 9
FAIL: gcc:gcc.target/aarch64/sve/aarch64-sve.exp=gcc.target/aarch64/sve/mask_gather_load_4.c scan-assembler-times \\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+\\.d\\]\\n 9
FAIL: gcc:gcc.target/aarch64/sve/aarch64-sve.exp=gcc.target/aarch64/sve/mask_gather_load_7.c scan-assembler-times \\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+\\.d, lsl 3\\]\\n 36
FAIL: gcc:gcc.target/aarch64/sve/aarch64-sve.exp=gcc.target/aarch64/sve/mask_scatter_store_1.c scan-assembler-times \\tst1d\\tz[0-9]+\\.d, p[0-7], \\[x[0-9]+, z[0-9]+\\.d, lsl 3\\]\\n 9
FAIL: gcc:gcc.target/aarch64/sve/aarch64-sve.exp=gcc.target/aarch64/sve/mask_scatter_store_2.c scan-assembler-times \\tst1d\\tz[0-9]+\\.d, p[0-7], \\[x[0-9]+, z[0-9]+\\.d, lsl 3\\]\\n 9


---


### compiler : `gcc`
### title : `[12 Regression] Some TSVC benchmarks are slower after r12-4195-gec0124e0acb556cd`
### open_at : `2021-10-18T10:14:45Z`
### last_modified_date : `2021-11-05T13:55:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102809
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Tested with https://github.com/UoB-HPC/TSVC_2/pull/2 where one can run a single benchmark.

Happens with -Ofast -march=native on znver1 machine:

gcc-bisect.py 'gcc *.c -lm -Ofast -march=native -fuse-ld=bfd && timeout 100.7 ./a.out va' -l -e r12-4194-g1f51e9af7b615838 -s r12-4195-gec0124e0acb556cd

Bisecting latest revisions
  ec0124e0acb556cd(05 Oct 2021 18:24)(aldyh@redhat.com): [took: 7.050s] result: OK
Loop 	Time(sec) 	Checksum
   va	     3.719	1.644725
  1f51e9af7b615838(05 Oct 2021 16:47)(jwakely@redhat.com): [took: 5.433s] result: OK
Loop 	Time(sec) 	Checksum
   va	     2.006	1.644725

and can be seen here:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=14.920.0
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=14.887.0
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=14.943.0


---


### compiler : `gcc`
### title : `vcvtph2ps and vcvtps2ph should be used to convert _Float16 to SFmode with -mf16c`
### open_at : `2021-10-18T10:58:52Z`
### last_modified_date : `2021-12-01T22:15:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102811
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
The following testcase:

_Float16 test (_Float16 a, _Float16 b)
{
  return a + b;
}

compiles with -O2 -mf16c to:

--cut here--
        subq    $24, %rsp
        pextrw  $0, %xmm1, 14(%rsp)
        call    __extendhfsf2
        pinsrw  $0, 14(%rsp), %xmm1
        vmovss  %xmm0, 8(%rsp)
        vmovss  %xmm1, %xmm1, %xmm0
        call    __extendhfsf2
        vaddss  8(%rsp), %xmm0, %xmm0
        call    __truncsfhf2
        addq    $24, %rsp
        ret
--cut here--

Instead of calling __extendhfsf2 and __truncsfhf2, we should use vcvtph2ps and vcvtps2ph (with zeroed elements 1..3) for -m16c targets.


---


### compiler : `gcc`
### title : `Redundant null check after atomic load from that address`
### open_at : `2021-10-19T08:17:58Z`
### last_modified_date : `2021-10-26T00:01:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102829
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following source code
----
struct d {
  long b;

  d *e() {
    __atomic_load_n(&b, 0);
    return this;
  }
};

d *j;

void i();

void k() {
  auto l = j->e();
  if (l)
    i();
}
----
produces the following bit of assembly with -O3 on x86_64 (https://godbolt.org/z/G1ThvnMWP):
...
        mov     rdx, QWORD PTR [rax]
        test    rax, rax
        je      .L1
...

It first dereferences the address at RAX, and later checks whether RAX == 0. Since we already tried accessing memory at that address, the nullptr check seems redundant. Moreover, since the address happens to be of 'this' of variable j, it being equal to nullptr is UB anyway and may be assumed to be != nullptr?

Clang tests show the redundant test being present up to version 11 inclusive (https://godbolt.org/z/rT3ha4enb) and absent from version 12 onwards (https://godbolt.org/z/Gfb9EWMfq)


---


### compiler : `gcc`
### title : `Missed optimization with __builtin_shuffle and zero vector on ppc`
### open_at : `2021-10-21T06:08:29Z`
### last_modified_date : `2021-10-29T02:31:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102868
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Similar to PR94680 and PR100165, PPC currently generates inefficient instructions for below case:

typedef float V __attribute__((vector_size(16)));
typedef int VI __attribute__((vector_size(16)));
V foo (V x)
{
return __builtin_shuffle (x, (V) { 0, 0, 0, 0 }, (VI) {0, 1, 4, 5});
}


foo:
.LFB0:
        .cfi_startproc
.LCF0:
0:      addis 2,12,.TOC.-.LCF0@ha
        addi 2,2,.TOC.-.LCF0@l
        .localentry     foo,.-foo
        addis %r9,%r2,.LC0@toc@ha
        xxspltib %vs32,0
        addi %r9,%r9,.LC0@toc@l
        lxv %vs33,0(%r9)
        xxperm %vs34,%vs32,%vs33
        blr



It will be better to produce:

foo:
.LFB0:
        .cfi_startproc
        vspltisw %v0,0
        xxpermdi %vs34,%vs32,%vs34,3


---


### compiler : `gcc`
### title : `If statement is always false but not figured out at gimple level`
### open_at : `2021-10-21T08:25:31Z`
### last_modified_date : `2022-11-28T22:41:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102872
### status : `RESOLVED`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
void foo(void);

static int a, b;
int main() {
  for (; a; ++a) {
    unsigned short d = a;
    if (!(b | d) && d)
      foo();
  }
}
---- CUT ---
This is the reduced testcase of PR 102703 but without the store to a static variable which then was removed.

In VRP1 we have:
  d_10 = (short unsigned int) a.3_5;
  _9 = a.3_5 & 65535;
  if (_9 == 0)
    goto <bb 4>; [50.00%]
  else
    goto <bb 6>; [50.00%]

  <bb 4> [local count: 477815112]:
  if (d_10 != 0)
    goto <bb 5>; [33.00%]
  else
    goto <bb 6>; [67.00%]

  <bb 5> [local count: 157678986]:
  foo ();

_9 and d_10 have the same value though different types.


---


### compiler : `gcc`
### title : `GCC fails to use constant initialization even when it knows the value to initialize`
### open_at : `2021-10-21T11:48:45Z`
### last_modified_date : `2022-10-17T22:34:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102876
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.2.0`
### severity : `enhancement`
### contents :
See: https://godbolt.org/z/KnKv9j8b9

#include <chrono>
using namespace std::literals;
/*not constexpr*/ inline std::chrono::sys_days compute_days() {
    return 1776y/7/4;
}
std::chrono::sys_days BURN_IN_TO_BINARY = compute_days();

Clang and MSVC both correctly burn BURN_IN_TO_BINARY into the binary image with the correct value. Even with -O3, gcc zero-initializes it then uses a dynamic initializer to complete the initialization. Both are valid implementation strategies according to https://eel.is/c++draft/basic.start.static#3, however, I think the strategy used by clang and MSVC is clearly superior QoI here.


---


### compiler : `gcc`
### title : `missed optimization: memcpy produces lots more asm than otherwise`
### open_at : `2021-10-21T12:33:26Z`
### last_modified_date : `2021-10-21T13:03:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102877
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.2.1`
### severity : `normal`
### contents :
Input (C++)
===========
struct GLOBCNT { unsigned char ab[6]; };
unsigned long long gc_to_num(GLOBCNT gc)
{
        unsigned long long value;
        auto v = reinterpret_cast<unsigned char *>(&value);
        v[0] = 0;
        v[1] = 0;
#ifdef WITH_MEMCPY
        __builtin_memcpy(v + 2, gc.ab, 6);
#else
        v[2] = gc.ab[0]; v[3] = gc.ab[1]; v[4] = gc.ab[2];
        v[5] = gc.ab[3]; v[6] = gc.ab[4]; v[7] = gc.ab[5];
#endif
        if (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
                value = __builtin_bswap64(value);
        return value;
}

I hope this is UB-free.


Observed behavior
=================
The use of memcpy/__builtin_memcpy produces a function with 28 instructions/0x5c bytes long.

► g++ -O2 -c t3.cpp -Wall -DWITH_MEMCPY -v
Target: x86_64-suse-linux
gcc version 11.2.1 20210816 [revision 056e324ce46a7924b5cf10f61010cf9dd2ca10e9] (SUSE Linux)

► objdump -Mintel -d t3.o
0000000000000000 <_Z9gc_to_num7GLOBCNT>:
   0:   89 f8                   mov    eax,edi
   2:   89 f9                   mov    ecx,edi
   4:   89 fa                   mov    edx,edi
   6:   44 0f b6 c7             movzx  r8d,dil
   a:   c1 e9 10                shr    ecx,0x10
   d:   0f b6 f4                movzx  esi,ah
  ...
  5c:   c3                      ret    


Expected behavior
=================
► g++ -O2 -c t3.cpp -Wall -UWITH_MEMCPY
► objdump -Mintel -d t3.o
0000000000000000 <_Z9gc_to_num7GLOBCNT>:
   0:   0f b7 c7                movzx  eax,di
   3:   48 c1 ef 10             shr    rdi,0x10
   7:   48 c1 e7 20             shl    rdi,0x20
   b:   48 c1 e0 10             shl    rax,0x10
   f:   48 09 f8                or     rax,rdi
  12:   48 0f c8                bswap  rax
  15:   c3                      ret    


Other notes
===========
In a twist, clang 13.0.0 produces the short version for memcpy (even shorter than gcc), and produces a long version for non-memcpy case (even longer than gcc).

► clang++ -O2 -c t3.cpp -Wall -DWITH_MEMCPY; objdump -Mintel -d t3.o
0000000000000000 <_Z9gc_to_num7GLOBCNT>:
   0:   48 89 f8                mov    rax,rdi
   3:   48 c1 e0 10             shl    rax,0x10
   7:   48 0f c8                bswap  rax
   a:   c3                      ret    

► clang++ -O2 -c t3.cpp -Wall -UWITH_MEMCPY; objdump -Mintel -d t3.o
0000000000000000 <_Z9gc_to_num7GLOBCNT>:
   0:   48 89 f8                mov    rax,rdi
   3:   48 b9 ff ff ff ff ff    movabs rcx,0xffffffffffff
   a:   ff 00 00 
 ...
  6c:   c3                      ret


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Dead Code Elimination Regression at -O3`
### open_at : `2021-10-21T12:51:18Z`
### last_modified_date : `2023-05-08T12:22:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102879
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c
void foo(void);

static int b, c, d;
static char a(char e, int f) { return e < 0 ? e : e >> f; }

int main() {
  c = 2;
  for (int g = 0; g < 2; g++) {
    for (int h = 0; h < 3; h++)
      d = a(c, d == b);
    if (!d)
      foo();
  }
}


11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:


gcc-11 -O3 -S test.c -o /dev/stdout
...
main:
.LFB1:
	.cfi_startproc
	movl	$2, c(%rip)
	xorl	%eax, %eax
	movl	$2, d(%rip)
	ret
	.cfi_endproc

gcc-trunk test.c -S -O3 -o /dev/stdout
...
main:
.LFB1:
	.cfi_startproc
	movl	$2, %edx
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	d(%rip), %ecx
	movl	$2, %ebx
	movl	$2, c(%rip)
	movsbl	%dl, %eax
	testb	%dl, %dl
	js	.L9
.L4:
	testl	%ecx, %ecx
	movl	%eax, %edx
	sete	%cl
	sarl	%cl, %edx
	testl	%edx, %edx
	movl	%eax, %edx
	sete	%cl
	sarl	%cl, %edx
	testl	%edx, %edx
	sete	%cl
	sarl	%cl, %eax
	movl	%eax, d(%rip)
	testl	%eax, %eax
	jne	.L5
	call	foo
.L5:
	cmpl	$1, %ebx
	je	.L3
.L10:
	movl	c(%rip), %edx
	movl	d(%rip), %ecx
	movl	$1, %ebx
	movsbl	%dl, %eax
	testb	%dl, %dl
	jns	.L4
.L9:
	movl	%eax, d(%rip)
	cmpl	$1, %ebx
	jne	.L10
.L3:
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc


gcc-trunk -v   
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211021 (experimental) (GCC)

Started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=2e96b5f14e4025691b57d2301d71aa6092ed44bc


---


### compiler : `gcc`
### title : `[12 regression] gcc.dg/tree-ssa/sra-18.c fails starting with r12-4607`
### open_at : `2021-10-21T17:35:24Z`
### last_modified_date : `2021-11-02T10:19:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102886
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
g:701ee067807b80957c65bd7ff94b6099a27181de, r12-4607

make  -k check-gcc RUNTESTFLAGS="tree-ssa.exp=gcc.dg/tree-ssa/sra-18.c"
FAIL: gcc.dg/tree-ssa/sra-18.c scan-tree-dump-times esra "Removing load: a = \\*.?L.?C.?.?.?0;" 1
FAIL: gcc.dg/tree-ssa/sra-18.c scan-tree-dump-times esra "SR[.$][0-9_]+ = \\*.?L.?C.?.?.?0\\.b\\[0\\]\\.f\\[0\\]\\.x" 1
FAIL: gcc.dg/tree-ssa/sra-18.c scan-tree-dump-times esra "SR[.$][0-9_]+ = \\*.?L.?C.?.?.?0\\.b\\[0\\]\\.f\\[1\\]\\.x" 1
FAIL: gcc.dg/tree-ssa/sra-18.c scan-tree-dump-times esra "SR[.$][0-9_]+ = \\*.?L.?C.?.?.?0\\.b\\[1\\]\\.f\\[0\\]\\.x" 1
FAIL: gcc.dg/tree-ssa/sra-18.c scan-tree-dump-times esra "SR[.$][0-9_]+ = \\*.?L.?C.?.?.?0\\.b\\[1\\]\\.f\\[1\\]\\.x" 1
# of expected passes		2
# of unexpected failures	5


commit 701ee067807b80957c65bd7ff94b6099a27181de (HEAD, refs/bisect/bad)
Author: Martin Jambor <mjambor@suse.cz>
Date:   Thu Oct 21 14:26:45 2021 +0200

    sra: Fix corner case of total scalarization with virtual inheritance (PR 102505)


---


### compiler : `gcc`
### title : `missing case for combining / and % into one operation`
### open_at : `2021-10-21T20:51:04Z`
### last_modified_date : `2021-11-04T17:43:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102888
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Normally GCC combines a/b and a%b into one operation when they are computed in the same basic-block. The example below has two functions. For one GCC is able to combine the operations and for other not (presumably because of complicated control-flow). I believe the two functions are functionally equivalent. 

unsigned long long reduce(unsigned long long a, unsigned long long b)
{
    while ((a % b) == 0)
        a /= b;

    return a;
}

unsigned long long reduce_opt(unsigned long long a, unsigned long long b)
{
    for (;;)
    {
        unsigned long long quot = a / b;
        unsigned long long rem = a % b;
        if (rem != 0)
            break;
        a = quot;
    }

    return a;
}

reduce.L3:
        mov     rax, r8
        xor     edx, edx
        div     rsi
        xor     edx, edx
        mov     r8, rax
        div     rsi
        test    rdx, rdx
        je      .L3

reduce_opt.L8:
        xor     edx, edx
        mov     r8, rax
        div     rsi
        test    rdx, rdx
        je      .L8

https://godbolt.org/z/9dqs8avE5

It would be great if GCC generated the same code for both of these functions.


---


### compiler : `gcc`
### title : `[12/13 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-10-22T09:39:03Z`
### last_modified_date : `2022-10-19T09:43:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102892
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c
static long b[2][1] = {0};
void bar(void);
void foo(void);
int main() {
  long c = 0;
  for (long a; a < 1; ++a)
    for (; c <= 1; c++) {
      bar();
      if (1 == b[c][0])
        foo();
    }
}

11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11 -O3 -S test.c -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	call	bar
	call	bar
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc

gcc-trunk-O3 -S test.c -o /dev/stdout
main:
.LFB0:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	xorl	%ebx, %ebx
	call	bar
	cmpq	$1, b(,%rbx,8)
	je	.L11
	.p2align 4,,10
	.p2align 3
.L3:
	cmpq	$1, %rbx
	je	.L12
.L5:
	call	bar
	movl	$1, %ebx
	cmpq	$1, b(,%rbx,8)
	jne	.L3
.L11:
	call	foo
	cmpq	$1, %rbx
	jne	.L5
.L12:
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc

gcc-trunk -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211022 (experimental) (GCC)

Introduced with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=01b5038718056b024b370b74a874fbd92c5bbab3


---


### compiler : `gcc`
### title : `[10 Regression] CDDCE does not detect empty infinite nested loops`
### open_at : `2021-10-22T09:40:37Z`
### last_modified_date : `2023-09-17T06:42:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102893
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.5.0`
### severity : `normal`
### contents :
The following example

int main() {
  while(1) {
    for(int i=0; i<9000000; i++){}
  }
}

In GCC 7 compiled to

main:
.L2:
        b       .L2

but since GCC 8 compiles to

main:
.L3:
        mov     w0, 21568
        movk    w0, 0x89, lsl 16
.L2:
        subs    w0, w0, #1
        bne     .L2
        b       .L3

The difference seems to be in cddce which previously was able to detect the code as dead:

;; 3 loops found
;;
;; Loop 0
;;  header 0, latch 1
;;  depth 0, outer -1
;;  nodes: 0 1 2 3 4 5 6
;;
;; Loop 1
;;  header 3, latch 6
;;  depth 1, outer 0
;;  nodes: 3 6 5 4
;;
;; Loop 2
;;  header 5, latch 4
;;  depth 2, outer 1
;;  nodes: 5 4
;; 2 succs { 3 }
;; 3 succs { 5 }
;; 4 succs { 5 }
;; 5 succs { 4 6 }
;; 6 succs { 3 }
can not prove finiteness of loop 1
Removing basic block 4
Merging blocks 3 and 5
Removing basic block 6

but since GCC 8 does

;; 3 loops found
;;
;; Loop 0
;;  header 0, latch 1
;;  depth 0, outer -1
;;  nodes: 0 1 2 3 4 5 6
;;
;; Loop 1
;;  header 3, latch 6
;;  depth 1, outer 0
;;  nodes: 3 6 5 4
;;
;; Loop 2
;;  header 5, latch 4
;;  depth 2, outer 1
;;  nodes: 5 4
;; 2 succs { 3 }
;; 3 succs { 5 }
;; 4 succs { 5 }
;; 5 succs { 4 6 }
;; 6 succs { 3 }
cannot prove finiteness of loop 1


---


### compiler : `gcc`
### title : `[12 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-10-22T09:49:25Z`
### last_modified_date : `2021-10-30T18:08:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102895
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c                                                                                      master
static int a, b, c;
void foo(void);
int main() {
  for (a = 0; a <= 1; ++a)
    if (c <= a) {
      for (b = 0; b <= 1; ++b)
        ;
    } else
      foo();
}

11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11 -O3 -S test.c -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L2:
	movl	$2, b(%rip)
	addl	$1, %eax
	movl	%eax, a(%rip)
	cmpl	$1, %eax
	je	.L2
	xorl	%eax, %eax
	ret
	.cfi_endproc

gcc-trunk -O3 -S test.c -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	movl	$0, a(%rip)
	.p2align 4,,10
	.p2align 3
.L5:
	testl	%eax, %eax
	jns	.L9
	call	foo
	.p2align 4,,10
	.p2align 3
.L7:
	movl	a(%rip), %eax
	addl	$1, %eax
	movl	%eax, a(%rip)
	cmpl	$1, %eax
	jle	.L5
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L9:
	.cfi_restore_state
	movl	$2, b(%rip)
	jmp	.L7
	.cfi_endproc


gcc-trunk -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211022 (experimental) (GCC)

Introduced with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=d8e1f1d24179690fd9c0f63c27b12e030010d9ea


---


### compiler : `gcc`
### title : `[12 regression] gcc.target/arm/ivopts-4.c fails since r12-4526`
### open_at : `2021-10-22T23:09:49Z`
### last_modified_date : `2021-11-14T10:39:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102906
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Since r12-4526 (g:d8edfadfc7a9795b65177a50ce44fd348858e844) I have noticed regressions on some arm targets for gcc.target/arm/ivopts-4.c

For instance on arm-none-linux-gnueabihf --with-arch armv7-a+mp+sec+neon-fp16
FAIL: gcc.target/arm/ivopts-4.c object-size text <= 36

But this depends on the actual target & runtestflags, see:
https://people.linaro.org/~christophe.lyon/cross-validation/gcc/trunk/r12-4526-gd8edfadfc7a9795b65177a50ce44fd348858e844/report-build-info.html


---


### compiler : `gcc`
### title : `TLS register value is spilled to the stack instead of reloaded from the system register`
### open_at : `2021-10-25T11:35:14Z`
### last_modified_date : `2021-10-25T15:24:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102926
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
The code below uses the hardware TLS register on ARM, and instead of reloading its value directly from the system register, it spills its value to the stack. This is suboptimal, and given that the TLS register is being proposed as an alternative reference for the stack protector canary, it is also a security concern, as an attacker that controls the stack may be able to control both sides of the equation in the stack protector check occurring at the end of the function.

Instead, I would expect any subsequent uses of the thread pointer to simply issue the MRC again, which doesn't touch memory.

$ cat /tmp/spill.c 

    int foo(int);
    int bar(void)
    {
	int *l = __builtin_thread_pointer();

	return foo(l[0]) + l[1];
    }

$ arm-linux-gnueabihf-gcc -o - -S /tmp/spill.c -O3 -mtp=cp15 -ffixed-r4 -ffixed-r5 -ffixed-r6 -ffixed-r7 -ffixed-r8 -ffixed-r9 -ffixed-r10 -fno-omit-frame-pointer
	.cpu arm10tdmi
	.arch armv5t
	.fpu softvfp
	.eabi_attribute 20, 1
	.eabi_attribute 21, 1
	.eabi_attribute 23, 3
	.eabi_attribute 24, 1
	.eabi_attribute 25, 1
	.eabi_attribute 26, 2
	.eabi_attribute 30, 2
	.eabi_attribute 34, 0
	.eabi_attribute 18, 4
	.file	"spill.c"
	.text
	.align	2
	.global	bar
	.syntax unified
	.arm
	.type	bar, %function
bar:
	@ args = 0, pretend = 0, frame = 8
	@ frame_needed = 1, uses_anonymous_args = 0
	push	{fp, lr}
	add	fp, sp, #4
	sub	sp, sp, #8
	mrc	p15, 0, r3, c13, c0, 3	@ load_tp_hard
	ldr	r0, [r3]
	str	r3, [fp, #-8]
	bl	foo
	ldr	r3, [fp, #-8]
	ldr	r3, [r3, #4]
	add	r0, r0, r3
	sub	sp, fp, #4
	@ sp needed
	pop	{fp, pc}
	.size	bar, .-bar
	.ident	"GCC: (GNU) 12.0.0 20211024 (experimental)"
	.section	.note.GNU-stack,"",%progbits


---


### compiler : `gcc`
### title : `Failure to optimize series of if-else to use array when possible`
### open_at : `2021-10-25T11:41:01Z`
### last_modified_date : `2023-09-21T07:47:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102927
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
int foo(int i) {
  if (i == 0)
    return 52;
  else if (i == 1)
    return 77;
  else if (i == 2)
    return 91;
  else if (i == 3)
    return 10;
  else
    return 42;
}

int bar(int i) {
  switch (i) {
  case 0:
    return 52;
  case 1:
    return 77;
  case 2:
    return 91;
  case 3:
    return 10;
  default:
    return 42;
  }
}

int baz(int i)
{
    static const int results[] = {52, 77, 91, 10};
    if (__builtin_expect_with_probability((unsigned)i < 4, 1, 0.5))
        return results[(unsigned)i];
    return 42;
}

foo can be optimized to be equivalent to baz (like bar is). This optimization is done by LLVM, but not by GCC.

PS: I've observed that making the if-else chain longer triggers the optimization. Is GCC considering the if-else chain to be faster than an array access ? Because in that case, it seems like bar should be optimized to an if-else chain (perhaps along with bar).


---


### compiler : `gcc`
### title : `[missed optimization] two ways to rounddown-to-next-multiple`
### open_at : `2021-10-25T12:53:36Z`
### last_modified_date : `2021-11-17T00:55:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102929
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.1`
### severity : `enhancement`
### contents :
Input
=====
unsigned long calc(unsigned long x, unsigned long y)
{
        return x/y*y;
}
unsigned long calc2(unsigned long x, unsigned long y)
{
        return x - x % y;
}

Observed
========
» g++ -O3 -c x.c; objdump -Mintel -d x.o
gcc version 11.2.1 20210816 [revision 056e324ce46a7924b5cf10f61010cf9dd2ca10e9] (SUSE Linux) x86_64
0000000000000000 <_Z4calcmm>:
   0:   48 89 f8                mov    rax,rdi
   3:   31 d2                   xor    edx,edx
   5:   48 f7 f6                div    rsi
   8:   48 0f af c6             imul   rax,rsi
   c:   c3                      ret    
   d:   0f 1f 00                nop    DWORD PTR [rax]

0000000000000010 <_Z5calc2mm>:
  10:   48 89 f8                mov    rax,rdi
  13:   31 d2                   xor    edx,edx
  15:   48 f7 f6                div    rsi
  18:   48 89 f8                mov    rax,rdi
  1b:   48 29 d0                sub    rax,rdx
  1e:   c3                      ret    

Expected
========
I do not see any obvious differences in the outcome of the two C functions, so I would expect that, ideally, both should lead to the same asm. (Either by making calc use div-mov-sub, or by making calc2 using div-imul; whichever happens to be determined more beneficial as per the machine descriptions).


---


### compiler : `gcc`
### title : `Implicit copy constructor not elided`
### open_at : `2021-10-26T08:49:10Z`
### last_modified_date : `2021-10-28T14:05:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102942
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.2.0`
### severity : `normal`
### contents :
This is seen on every GCC version. The copy-constructor for a class with a data buffer only gets elided if the copy-constructor is user provided. So if I have a copy-constructor that explicitly calls memcpy, the compiler will elide it when it can (and thus NOT call memcpy), but if I do not declare any such constructor, the compiler *will* call mempcy even if the call could be elided.

https://godbolt.org/z/5YvWTE847

#include <cstring>

struct StructWithImplicitCpyCtr
{
    StructWithImplicitCpyCtr();
    float data[5000];
};

struct StructWithExplicitMemCpyCpyCtr
{
    StructWithExplicitMemCpyCpyCtr();
    StructWithExplicitMemCpyCpyCtr(StructWithExplicitMemCpyCpyCtr const& other)
    { std::memcpy(data, other.data, sizeof(data)); }
    float data[5000];
};

void take(StructWithImplicitCpyCtr);
void take(StructWithExplicitMemCpyCpyCtr);

void foobar()
{
    // The copy constructor explicitly calls memcpy, but copy elision removes it
    take(StructWithExplicitMemCpyCpyCtr());

    // Here there is no explicit copy constructor to elide, we should expect to see
    // similar code generated, instead we see a call to memcpy!!!
    take(StructWithImplicitCpyCtr());
}


---


### compiler : `gcc`
### title : `[11/12 Regression] Dead Code Elimination Regression at -O3 (11.2.0 vs 10.3.0)`
### open_at : `2021-10-26T15:21:16Z`
### last_modified_date : `2022-11-03T19:01:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102950
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c
void foo(void);

static char a;
static short d(unsigned e) {
  char b;
  short c;
  a = b = e;
  if (b)
    return 0;
  if (1 >= e) {
    c = e == 0;
    if (c)
      foo();
  }
  return 0;
}
int main() { d(a ^ 233); }

10.3.0 at -O3 can eliminate the call to foo but neither trunk nor 11.2.0 at -O3 can:

gcc-10 -O3 -S test.c -o /dev/stdout
main:
.LFB1:
	.cfi_startproc
	xorb	$-23, a(%rip)
	xorl	%eax, %eax
	ret
	.cfi_endproc

gcc-11 -O3 -S test.c -o /dev/stdout
...
main:
.LFB1:
	.cfi_startproc
	movsbl	a(%rip), %eax
	xorb	$-23, %al
	movb	%al, a(%rip)
	cmpl	$1, %eax
	ja	.L10
	testb	%al, %al
	je	.L14
.L10:
	xorl	%eax, %eax
	ret
.L14:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc


gcc-trunk -O3 -S test.c -o /dev/stdout
main:
.LFB1:
	.cfi_startproc
	movsbl	a(%rip), %eax
	xorb	$-23, %al
	movb	%al, a(%rip)
	cmpl	$1, %eax
	ja	.L10
	testb	%al, %al
	je	.L14
.L10:
	xorl	%eax, %eax
	ret
.L14:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc


gcc-trunk -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211022 (experimental) (GCC)

Introduced with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=fcae5121154d1c3382b056bcc2c563cedac28e74


---


### compiler : `gcc`
### title : `failure to optimize MIN_EXPR of subobject addresses of the same object`
### open_at : `2021-10-26T15:24:15Z`
### last_modified_date : `2021-10-28T18:11:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102951
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC fails to fold the MIN_EXPR below to the lower address.  The test case affects C because the C front end emits a MIN_EXPR but not C++ because the C++ front end emits an if statement.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
extern int a[];

void f (void)
{
  int *p1 = &a[1];
  int *p2 = &a[2];

  int *d = p1 < p2 ? p1 : p2;
  *d = 0;
}

;; Function f (f, funcdef_no=0, decl_uid=1979, cgraph_uid=1, symbol_order=0)

void f ()
{
  int * d;

  <bb 2> [local count: 1073741824]:
  d_1 = MIN_EXPR <&a[2], &a[1]>;   // the two statements can be folded into
  *d_1 = 0;                        //   MEM[(int *)&a + 4B] = 0
  return;

}


---


### compiler : `gcc`
### title : `std::u8string suboptimal compared to std::string, triggers warnings`
### open_at : `2021-10-26T22:37:25Z`
### last_modified_date : `2022-12-12T17:22:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102958
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
The two functions below are equivalent and would ideally both result in optimally efficient code, but as the dump shows, only the one with std::string does.  The other one that uses uchar8_t doesn't.  In addition, the latter also triggers a bogus warning.  This was discussed in https://gcc.gnu.org/pipermail/gcc-patches/2021-October/582416.html and is the reason for the -Wstringop-overread suppression in libstdc++-v3/testsuite/27_io/filesystem/path/factory/u8path-char8_t.cc.

$ cat x.C && gcc -O2 -S -std=c++17 -fchar8_t -fdump-tree-optimized=/dev/stdout x.C

int f ()
{
  std::string s = "123456789";
  std::string t = s;
  return t.length ();
}

int g ()
{
  std::u8string s = u8"123456789";
  std::u8string t = s;
  return t.length ();
}


;; Function f (_Z1fv, funcdef_no=1222, decl_uid=34814, cgraph_uid=307, symbol_order=346)

int f ()
{
  <bb 2> [local count: 1073741824]:
  return 9;

}



;; Function g (_Z1gv, funcdef_no=1223, decl_uid=34822, cgraph_uid=308, symbol_order=347)

int g ()
{
  void * D.41167;
  size_t __i;
  struct u8string t;
  struct u8string s;
  int _7;
  char8_t * _19;
  char8_t _23;
  char8_t * _34;
  char8_t * _41;
  char8_t * _51;
  char8_t * _52;
  char8_t * _61;
  char8_t _64;
  char8_t * _65;
  long unsigned int _67;
  long unsigned int _68;
  long unsigned int _71;
  long unsigned int _72;
  char8_t * _73;
  long unsigned int _75;
  long unsigned int _76;
  char8_t * _83;
  long unsigned int _85;
  char8_t * _102;
  void * _129;
  char8_t * _130;
  char8_t * _134;
  long unsigned int _135;

  <bb 2> [local count: 1073741824]:
  MEM[(struct basic_string *)&s] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&s] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&s]._M_p = &s.D.29743._M_local_buf;

  <bb 3> [local count: 8687547547]:
  # __i_35 = PHI <__i_21(3), 0(2)>
  __i_21 = __i_35 + 1;
  _23 = MEM[(const char_type &)"123456789" + __i_21 * 1];
  if (_23 != 0)
    goto <bb 3>; [89.00%]
  else
    goto <bb 4>; [11.00%]

  <bb 4> [local count: 1073741824]:
  if (__i_21 > 15)
    goto <bb 5>; [33.00%]
  else
    goto <bb 8>; [67.00%]

  <bb 5> [local count: 354334802]:
  if (__i_21 > 4611686018427387903)
    goto <bb 6>; [0.04%]
  else
    goto <bb 7>; [99.96%]

  <bb 6> [local count: 141736]:
  std::__throw_length_error ("basic_string::_M_create");

  <bb 7> [local count: 354193066]:
  _85 = __i_35 + 2;
  _41 = operator new (_85);
  s._M_dataplus._M_p = _41;
  s.D.29743._M_allocated_capacity = __i_21;
  __builtin_memcpy (_41, "123456789", __i_21);
  s._M_string_length = __i_21;
  _19 = s._M_dataplus._M_p;
  _130 = _19 + __i_21;
  MEM[(char_type &)_130] = 0;
  MEM[(struct basic_string *)&t] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&t] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&t]._M_p = &t.D.29743._M_local_buf;
  if (_19 == 0B)
    goto <bb 10>; [30.00%]
  else
    goto <bb 11>; [70.00%]

  <bb 8> [local count: 703255365]:
  if (__i_21 == 1)
    goto <bb 9>; [50.19%]
  else
    goto <bb 22>; [49.81%]

  <bb 9> [local count: 352981469]:
  s._M_string_length = 1;
  MEM <unsigned short> [(char_type &)&s + 16] = 49;
  MEM[(struct basic_string *)&t] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&t] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&t]._M_p = &t.D.29743._M_local_buf;
  _64 = MEM[(const char_type &)&s + 16];
  MEM[(char_type &)&t + 16] = _64;
  goto <bb 14>; [100.00%]

  <bb 10> [local count: 124582]:
  std::__throw_logic_error ("basic_string::_M_construct null not valid");

  <bb 11> [local count: 342421520]:
  _135 = __i_35 + 2;
  _61 = operator new (_135);

  <bb 12> [local count: 308179374]:
  t._M_dataplus._M_p = _61;
  t.D.29743._M_allocated_capacity = __i_21;

  <bb 13> [local count: 323794238]:
  # _34 = PHI <&t.D.29743._M_local_buf(22), _61(12)>
  # _134 = PHI <_102(22), _19(12)>
  __builtin_memcpy (_34, _134, __i_21);

  <bb 14> [local count: 1003677033]:
  t._M_string_length = __i_21;
  _51 = t._M_dataplus._M_p;
  _52 = _51 + __i_21;
  MEM[(char_type &)_52] = 0;
  _7 = (int) __i_21;
  if (&t.D.29743._M_local_buf != _51)
    goto <bb 15>; [53.47%]
  else
    goto <bb 16>; [46.53%]

  <bb 15> [local count: 536666103]:
  _71 = t.D.29743._M_allocated_capacity;
  _72 = _71 + 1;
  operator delete (_51, _72);

  <bb 16> [local count: 1003677033]:
  t ={v} {CLOBBER};
  _65 = s._M_dataplus._M_p;
  if (&s.D.29743._M_local_buf != _65)
    goto <bb 17>; [53.47%]
  else
    goto <bb 18>; [46.53%]

  <bb 17> [local count: 536666103]:
  _67 = s.D.29743._M_allocated_capacity;
  _68 = _67 + 1;
  operator delete (_65, _68);

  <bb 18> [local count: 1003677033]:
  s ={v} {CLOBBER};
  s ={v} {CLOBBER};
  t ={v} {CLOBBER};
  return _7;

  <bb 19> [count: 0]:
<L6>:
  _73 = s._M_dataplus._M_p;
  if (&s.D.29743._M_local_buf != _73)
    goto <bb 20>; [0.00%]
  else
    goto <bb 21>; [0.00%]

  <bb 20> [count: 0]:
  _75 = s.D.29743._M_allocated_capacity;
  _76 = _75 + 1;
  operator delete (_73, _76);

  <bb 21> [count: 0]:
  s ={v} {CLOBBER};
  _129 = __builtin_eh_pointer (7);
  __builtin_unwind_resume (_129);

  <bb 22> [local count: 0]:
  __builtin_memcpy (&s.D.29743._M_local_buf, "123456789", __i_21);
  s._M_string_length = __i_21;
  _102 = s._M_dataplus._M_p;
  _83 = _102 + __i_21;
  MEM[(char_type &)_83] = 0;
  MEM[(struct basic_string *)&t] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&t] ={v} {CLOBBER};
  MEM[(struct _Alloc_hider *)&t]._M_p = &t.D.29743._M_local_buf;
  if (_102 == 0B)
    goto <bb 10>; [30.00%]
  else
    goto <bb 13>; [70.00%]

}


In file included from /build/gcc-master/x86_64-pc-linux-gnu/libstdc++-v3/include/string:40,
                 from x.C:1:
In static member function ‘static std::char_traits<char>::char_type* std::char_traits<char>::copy(std::char_traits<char>::char_type*, const std::char_traits<char>::char_type*, std::size_t)’,
    inlined from ‘static void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_S_copy(_CharT*, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /build/gcc-master/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/basic_string.h:361:21,
    inlined from ‘static void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_S_copy(_CharT*, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char8_t; _Traits = std::char_traits<char8_t>; _Alloc = std::allocator<char8_t>]’ at /build/gcc-master/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/basic_string.h:356:7,
    inlined from ‘static void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_S_copy_chars(_CharT*, const _CharT*, const _CharT*) [with _CharT = char8_t; _Traits = std::char_traits<char8_t>; _Alloc = std::allocator<char8_t>]’ at /build/gcc-master/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/basic_string.h:408:16,
    inlined from ‘void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_construct(_InIterator, _InIterator, std::forward_iterator_tag) [with _FwdIterator = const char8_t*; _CharT = char8_t; _Traits = std::char_traits<char8_t>; _Alloc = std::allocator<char8_t>]’ at /build/gcc-master/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/basic_string.tcc:225:25,
    inlined from ‘std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::basic_string(const _CharT*, const _Alloc&) [with <template-parameter-2-1> = std::allocator<char8_t>; _CharT = char8_t; _Traits = std::char_traits<char8_t>; _Alloc = std::allocator<char8_t>]’ at /build/gcc-master/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/basic_string.h:541:14,
    inlined from ‘int g()’ at x.C:12:21:
/build/gcc-master/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/char_traits.h:355:56: warning: ‘void* __builtin_memcpy(void*, const void*, long unsigned int)’ reading between 16 and 4611686018427387903 bytes from a region of size 10 [-Wstringop-overread]
  355 |         return static_cast<char_type*>(__builtin_memcpy(__s1, __s2, __n));
      |                                        ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~


---


### compiler : `gcc`
### title : `GCC cannot understand >>32 pattern`
### open_at : `2021-10-27T20:29:19Z`
### last_modified_date : `2021-10-28T08:19:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102971
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
https://godbolt.org/z/eeG8fnY1z

I want to seperate a uint_least64_t to 2x uint_least32_t or merge 2x uint_least32_t to uint_least64_t. GCC just cannot understand the pattern at all here.

The assembly it generates is very bad for things like this.


---


### compiler : `gcc`
### title : `GCC optimization is very poor for add carry and multiplication combos`
### open_at : `2021-10-27T21:17:23Z`
### last_modified_date : `2023-06-06T17:19:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102974
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
GCC:
https://godbolt.org/z/6sc6v5YcG

clang:
https://godbolt.org/z/eP8fTrWzd
msvc:
https://godbolt.org/z/snzoEe5he
GCC generates 16 more instructions than msvc and clang for carry flag optimizations.


---


### compiler : `gcc`
### title : `[12 Regression] vectorizer failed to use armv8.3-a complex fma`
### open_at : `2021-10-28T01:34:36Z`
### last_modified_date : `2021-10-29T11:51:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102977
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
#include<complex.h>

#include<complex.h>

void
foo (_Complex _Float16* __restrict a, _Complex _Float16* b, _Complex _Float16 *c)
{
    for (int i =0 ; i != 8; i++)
      a[i] += b[i] * c[i];
}


gcc11.2 generate 

foo:
        mov     x3, 16
        ptrue   p1.b, all
        whilelo p0.h, xzr, x3
        ld1h    z2.h, p0/z, [x1]
        ld1h    z1.h, p0/z, [x2]
        ld1h    z0.h, p0/z, [x0]
        fcmla   z0.h, p1/m, z1.h, z2.h, #0
        fcmla   z0.h, p1/m, z1.h, z2.h, #90
        st1h    z0.h, p0, [x0]
        cntb    x4
        cnth    x5
        add     x0, x0, x4
        add     x1, x1, x4
        add     x2, x2, x4
        whilelo p0.h, x5, x3
        b.none  .L1
        ld1h    z2.h, p0/z, [x1]
        ld1h    z1.h, p0/z, [x2]
        ld1h    z0.h, p0/z, [x0]
        fcmla   z0.h, p1/m, z1.h, z2.h, #0
        fcmla   z0.h, p1/m, z1.h, z2.h, #90
        st1h    z0.h, p0, [x0]
.L1:
        ret


current trunk

foo:
        ptrue   p1.h, vl8
        ptrue   p0.b, all
        ld2h    {z2.h - z3.h}, p1/z, [x1]
        ld2h    {z0.h - z1.h}, p1/z, [x2]
        ld2h    {z16.h - z17.h}, p1/z, [x0]
        fmul    z6.h, z0.h, z3.h
        movprfx z7, z16
        fmla    z7.h, p0/m, z0.h, z2.h
        fmla    z6.h, p0/m, z1.h, z2.h
        movprfx z4, z7
        fmls    z4.h, p0/m, z1.h, z3.h
        fadd    z5.h, z6.h, z17.h
        st2h    {z4.h - z5.h}, p1, [x0]
        ret


options: -Ofast -march=armv8.3-a+sve+fp16
refer to https://godbolt.org/z/4PPKnWvc1


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-10-28T09:38:56Z`
### last_modified_date : `2023-05-08T12:22:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102981
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c    
void foo(void);
void bar(void);

static int a;
static short b[2][2][2] = {1};
int main() {
  int c = 0;
  short d = 0;
  for (; d <= 1; d++) {
    if (c)
      foo();
    for (; a < 0; a++) {
      bar();
      if (!b[d][d][d])
        c = 1;
    }
  }
}


11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11 test.c -O3 -S -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	movl	a(%rip), %eax
	testl	%eax, %eax
	jns	.L6
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	.p2align 4,,10
	.p2align 3
.L3:
	call	bar
	addl	$1, a(%rip)
	js	.L3
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
.L6:
	xorl	%eax, %eax
	ret
	.cfi_endproc

gcc-trunk test.c -O3 -S -o /dev/stdout
main:
.LFB0:
	.cfi_startproc
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	movl	$1, %r12d
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	movl	$b, %ebp
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	xorl	%ebx, %ebx
.L2:
	movl	a(%rip), %eax
	testl	%eax, %eax
	jns	.L4
	.p2align 4,,10
	.p2align 3
.L6:
	call	bar
	cmpw	$0, 0(%rbp)
	cmove	%r12d, %ebx
	addl	$1, a(%rip)
	js	.L6
.L4:
	addq	$14, %rbp
	cmpq	$b+28, %rbp
	je	.L14
	testl	%ebx, %ebx
	je	.L2
	call	foo
	jmp	.L2
	.p2align 4,,10
	.p2align 3
.L14:
	popq	%rbx
	.cfi_def_cfa_offset 24
	xorl	%eax, %eax
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc

gcc-trunk -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211028 (experimental) (GCC)

It started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=d8edfadfc7a9795b65177a50ce44fd348858e844


---


### compiler : `gcc`
### title : `[12 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-10-28T09:56:08Z`
### last_modified_date : `2021-10-29T14:46:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102983
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat test.c
void foo(void);

static int a = 1;

int main() {
  int c = 0;
  for (int b = 0; b <= 0; b++) {
    if (!a)
      foo();
    if (b > c){
      if (c)
        continue;
      a = 0;
    }
    c = 1;
  }
}

11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc-11 test.c -O3 -S -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	xorl	%eax, %eax
	ret
	.cfi_endproc


gcc-trunk test.c -O3 -S -o /dev/stdout
...
main:
.LFB0:
	.cfi_startproc
	movl	a(%rip), %ecx
	testl	%ecx, %ecx
	je	.L8
	xorl	%eax, %eax
	ret
.L8:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc


gcc-trunk  -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211028 (experimental) (GCC)


Started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=398572c1544d8b7541862401b985ae7e855cb8fb


---


### compiler : `gcc`
### title : `[12 Regression] 45% 454.calculix regression with LTO+PGO -march=native -Ofast on Zen since r12-4526-gd8edfadfc7a9795b65177a50ce44fd348858e844`
### open_at : `2021-10-29T10:05:39Z`
### last_modified_date : `2021-11-12T16:05:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102997
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
This is seen on zen3
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=474.170.0
zen2
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=288.170.0
zen1
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=8.170.0

Changes in range:
commit 6fca1761a16c68740f875fc487b98b6bde8e9be7
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Wed Oct 20 07:15:17 2021 +0200

    Remove unused back_threader destructor.
    
    Tested on x86-64 Linux.
    
    gcc/ChangeLog:
    
            * tree-ssa-threadbackward.c (back_threader::~back_threader): Remove.

commit 8b7f9c40ef42411b6f51b508d41a580d4682069e
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Tue Oct 19 10:26:47 2021 +0200

    Remove superflous debugging messages from the threading registry.
    
    These are some random obvious cleanups to the threading dumps, since
    it seems I'm not the only one looking at dumps these days.
    
    The "just threaded" debugging message is redundant since there's
    already an equivalent "Registering jump thread" message.
    
    The "about to thread" message is actually confusing, because the source
    block doesn't match the IL, since the CFG update is mid-flight.
    
    Tested on x86-64 Linux.
    
    gcc/ChangeLog:
    
            * tree-ssa-threadupdate.c (back_jt_path_registry::adjust_paths_after_duplication):
            Remove superflous debugging message.
            (back_jt_path_registry::duplicate_thread_path): Same.

commit 18606d776642a876a787c37491c52b81c30ebc83
Author: Bob Duff <duff@adacore.com>
Date:   Sat Oct 16 15:30:45 2021 -0400

    [Ada] Remove unnecessary call to No_Uint_To_0
    
    gcc/ada/
    
            * gcc-interface/decl.c (gnat_to_gnu_entity): Remove unnecessary
            call to No_Uint_To_0.

commit 4afb464e1f76d63d89c4034f78d5ebb3400eaf3c
Author: Richard Kenner <kenner@adacore.com>
Date:   Thu Oct 14 15:31:38 2021 -0400

    [Ada] Never treat intrinsic subprograms as nested
    
    gcc/ada/
    
            * exp_unst.adb (Visit_Node, when N_Subprogram_Call): Never treat
            instrinsic subprograms as nested.

commit bd2560b726fa93b61060a9f469ad288c512961f3
Author: Yannick Moy <moy@adacore.com>
Date:   Mon Aug 30 16:33:00 2021 +0200

    [Ada] Proof of the runtime support for attribute 'Width
    
    gcc/ada/
    
            * libgnat/s-widlllu.ads: Mark in SPARK.
            * libgnat/s-widllu.ads: Likewise.
            * libgnat/s-widuns.ads: Likewise.
            * libgnat/s-widthu.adb: Add ghost code and a
            pseudo-postcondition.

commit c5742a0e1191365c57bc06fdbf1ff5da1028f127
Author: Yannick Moy <moy@adacore.com>
Date:   Fri Oct 15 12:00:16 2021 +0200

    [Ada] Provide dummy body for big integers library used in reduced runtimes
    
    gcc/ada/
    
            * libgnat/a-nbnbin__ghost.adb (Signed_Conversions,
            Unsigned_Conversions): Mark subprograms as not imported.
            * libgnat/a-nbnbin__ghost.ads: Provide a dummy body.

commit 723d09e8895733f065200fa1b54c84243cf96f69
Author: Eric Botcazou <ebotcazou@adacore.com>
Date:   Thu Oct 14 15:44:48 2021 +0200

    [Ada] Fix problematic conversion of real literal in static context
    
    gcc/ada/
    
            * sem_eval.adb (Eval_Type_Conversion): If the target subtype is
            a static floating-point subtype and the result is a real literal,
            consider its machine-rounded value to raise Constraint_Error.
            (Test_In_Range): Turn local variables into constants.

commit f6f8b3f95e55084b59ecc8fbe0f0cfd485d58c39
Author: Doug Rupp <rupp@adacore.com>
Date:   Thu Oct 14 08:41:56 2021 -0700

    [Ada] Delete unused runtime files
    
    gcc/ada/
    
            * libgnat/g-io-put__vxworks.adb: Remove (unused)
            * libgnat/s-parame__ae653.ads: Likewise.
            * libgnat/s-thread.ads: Likewise.
            * libgnat/s-thread__ae653.adb: Likewise.

commit 60440d3cf51acb9cb63543d5bb71fd50cfdd9470
Author: Eric Botcazou <ebotcazou@adacore.com>
Date:   Wed Oct 13 20:50:28 2021 +0200

    [Ada] Factor out machine rounding operations
    
    gcc/ada/
    
            * sem_eval.ads (Machine_Number): New inline function.
            * sem_eval.adb (Machine_Number): New function body implementing
            the machine rounding operation specified by RM 4.9(38/2).
            (Check_Non_Static_Context): Call Machine_Number and set the
            Is_Machine_Number flag consistently on the resulting node.
            * sem_attr.adb (Eval_Attribute) <Attribute_Machine>: Likewise.
            * checks.adb (Apply_Float_Conversion_Check): Call Machine_Number.
            (Round_Machine): Likewise.

commit 931d4819f740ade9707436447b6d7a1148d65d54
Author: Johannes Kliemann <kliemann@adacore.com>
Date:   Fri Oct 8 15:55:33 2021 +0000

    [Ada] Define __wrs_rtp_base in linker spec
    
    gcc/ada/
    
            * vxworks7-cert-rtp-link.spec: Add the definition of
            __wrs_rtp_base.

commit 6cf01c9277b81b81e8d09fe770b5fafe25fd880f
Author: Piotr Trojanek <trojanek@adacore.com>
Date:   Wed Oct 13 13:40:28 2021 +0200

    [Ada] Reject boxes in delta record aggregates
    
    gcc/ada/
    
            * sem_aggr.adb (Resolve_Delta_Record_Aggregate): Reject boxes in
            record delta aggregates.

commit 50cdd660b2ac54abb2659c7a88200d4c2fd1f195
Author: Justin Squirek <squirek@adacore.com>
Date:   Thu Sep 23 11:04:25 2021 -0400

    [Ada] Missing accessibility check when returning discriminated types
    
    gcc/ada/
    
            * sem_ch6.adb (Check_Return_Construct_Accessibility): Modify
            generation of accessibility checks to be more consolidated and
            get triggered properly in required cases.
            * sem_util.adb (Accessibility_Level): Add extra check within
            condition to handle aliased formals properly in more cases.

commit 9267014b351edf5aa0d0951545ec405edec5e3f5
Author: Justin Squirek <squirek@adacore.com>
Date:   Tue Oct 12 14:04:16 2021 -0400

    [Ada] Crash on object of protected type with defaulted access component
    
    gcc/ada/
    
            * exp_ch7.adb (Make_Final_Call): Detect expanded protected types
            and use original protected type in order to calculate
            appropriate finalization routine.

commit 0f074aa4aa248e9602765155acff57604c1d9778
Author: Johannes Kliemann <kliemann@adacore.com>
Date:   Thu Sep 30 13:41:13 2021 +0200

    [Ada] Add ghost code version of Ada.Numerics.Big_Numbers.Big_Integers
    
    gcc/ada/
    
            * libgnat/a-nbnbin__ghost.ads: Add ghost package.

commit 2aa814cb27fe3f61adfe894d52f01a4c377263fe
Author: Piotr Trojanek <trojanek@adacore.com>
Date:   Wed Oct 13 13:48:42 2021 +0200

    [Ada] Refine type of a counter function for record delta aggregate
    
    gcc/ada/
    
            * sem_aggr.adb (Variant_Depth): Refine type from Integer to
            Natural.

commit 36e38022125f2f336e5d281fb3e5e66191d21e73
Author: Bob Duff <duff@adacore.com>
Date:   Wed Oct 6 09:03:53 2021 -0400

    [Ada] tech debt: Clean up Uint fields, such as Esize
    
    gcc/ada/
    
            * atree.ads: Comment improvements. How is a "completely new
            node" different from a "new node"? Document default values
            corresponding to field types.
            * exp_ch7.adb (Process_Tagged_Type_Declaration): Use
            higher-level Scope_Depth instead of Scope_Depth_Value.  Remove
            confusing comment: not clear what a "true" library level package
            is.
            * uintp.adb (Image_Out): Print No_Uint in a more readable way.
            * gen_il-gen.adb, gen_il-gen-gen_entities.adb,
            gen_il-gen-gen_nodes.adb, gen_il-types.ads: Tighten up the
            subtypes of fields whose type is Uint, where possible; use
            more-constrained subtypes such as Unat.
            * einfo-utils.adb, einfo-utils.ads, exp_attr.adb,
            exp_ch3.adb, exp_intr.adb, exp_unst.adb, exp_util.adb,
            freeze.adb, repinfo.adb, sem.adb, sem_ch12.adb, sem_ch13.adb,
            sem_ch3.adb, sem_ch8.adb, sem_util.adb, sprint.adb, treepr.adb:
            No longer use Uint_0 to indicate "unknown" or "not yet known"
            for various fields whose type is Uint. Use No_Uint for that,
            except in a small number of legacy cases that cause test
            failures. Protect many queries of such fields with calls to
            Known_... functions. Improve comments.
            * exp_aggr.adb: Likewise.
            (Is_OK_Aggregate): Check whether Csiz is present.
            (Aggr_Assignment_OK_For_Backend): Ensure we do not access an
            uninitialized size.
            * exp_strm.adb (Build_Elementary_Input_Call,
            Build_Elementary_Write_Call): Check whether P_Size is present.
            * cstand.adb: Leave Component_Size of Any_Composite unknown.
            Similar for RM_Size of Standard_Exception_Type.  These should
            not be used.
            * einfo.ads: Comment improvements.
            * exp_disp.ads: Minor.
            * gen_il-internals.ads, gen_il-internals.adb: Minor.
            * sinfo-utils.adb: Take advantage of full-coverage rules.
            * types.h: Minor.

commit 749e01a5f310f2c4327f030d425aa6e23afbbbd5
Author: Gary Dismukes <dismukes@adacore.com>
Date:   Fri Oct 8 17:57:37 2021 -0400

    [Ada] Warning on nonmatching subtypes in fully conforming subprogram specs and bodies
    
    gcc/ada/
    
            * sem_ch6.adb: Add with and use of Warnsw.
            (Check_Conformance): Report a warning when subtypes or
            designated subtypes of formal parameters or result subtypes
            denote different declarations between the spec and body of the
            (Subprogram_Subtypes_Have_Same_Declaration): New function nested
            within Check_Conformance that determines whether two subtype
            entities referenced in a subprogram come from the same
            declaration. Returns True immediately if the subprogram is in a
            generic instantiation, or the subprogram is marked Is_Internal
            or is declared in an internal (GNAT library) unit, or GNAT_Mode
            is enabled, otherwise compares the nonlimited views of the
            entities (or their designated subtypes' nonlimited views in the
            anonymous access cases).
            (Nonlimited_View_Of_Subtype): New function nested within
            function Subprogram_Subtypes_Have_Same_Declaration that returns
            Non_Limited_View of a type or subtype that is an incomplete or
            class-wide type that comes from a limited of a
            package (From_Limited_With is True for the entity), or returns
            Full_View when the nonlimited view is an incomplete type.
            Otherwise returns the entity passed in.
            * warnsw.ads (Warn_On_Pedantic_Checks): New warning flag.
            (type Warning_Record): New component Warn_On_Pedantic_Checks.
            * warnsw.adb (All_Warnings): Set Warn_On_Pedantic_Checks from
            parameter Setting.
            (Restore_Warnings): Restore the value of the
            Warn_On_Pedantic_Checks flag.
            (Save_Warnings): Save the value of the Warn_On_Pedantic_Checks
            flag.
            (Set_Underscore_Warning_Switch): Add settings of the
            Warn_On_Pedantic flag according to the switch ("-gnatw_p" vs.
            "-gnatw_P").
            * doc/gnat_ugn/building_executable_programs_with_gnat.rst: Add
            documentation of new switches -gnatw_p and -gnatw_P (warnings
            for pedantic checks).
            * gnat_ugn.texi: Regenerate.
            * usage.adb: Add Warn_On_Pedantic_Checks.

commit ff2746728050429684bf62729df798189cc1d396
Author: Piotr Trojanek <trojanek@adacore.com>
Date:   Tue Oct 12 17:42:05 2021 +0200

    [Ada] Prevent use of an uninitialized AST field with universal integer
    
    gcc/ada/
    
            * exp_spark.adb (Expand_SPARK_N_Attribute_Reference): Guard
            against equality of an uninitialized RM_Size field.

commit d24e5767fe780653d5601b69d981f33e2a62e47e
Author: Eric Botcazou <ebotcazou@adacore.com>
Date:   Mon Oct 11 16:16:41 2021 +0200

    [Ada] Expose and use type-generic GCC atomic builtins
    
    gcc/ada/
    
            * sem_ch12.adb (Analyze_Subprogram_Instantiation): Also propagate an
            interface name on an intrinsic subprogram.  Remove obsolete comment.
            * libgnat/s-atopri.ads (Atomic_Load): New generic intrinsic function
            (Atomic_Load_8): Rewrite into instantiation.
            (Atomic_Load_16): Likewise.
            (Atomic_Load_32): Likewise.
            (Atomic_Load_64): Likewise.
            (Sync_Compare_And_Swap): New generic intrinsic function.
            (Sync_Compare_And_Swap_8): Rewrite into instantiation.
            (Sync_Compare_And_Swap_16): Likewise.
            (Sync_Compare_And_Swap_32): Likewise.
            (Sync_Compare_And_Swap_64): Likewise.
            (Lock_Free_Read): New generic inline function.
            (Lock_Free_Read_8): Rewrite into instantiation.
            (Lock_Free_Read_16): Likewise.
            (Lock_Free_Read_32): Likewise.
            (Lock_Free_Read_64): Likewise.
            (Lock_Free_Try_Write): New generic inline function.
            (Lock_Free_Try_Write_8): Rewrite into instantiation.
            (Lock_Free_Try_Write_16): Likewise.
            (Lock_Free_Try_Write_32): Likewise.
            (Lock_Free_Try_Write_64): Likewise.
            * libgnat/s-atopri.adb (Lock_Free_Read): New function body.
            (Lock_Free_Read_8): Delete.
            (Lock_Free_Read_16): Likewise.
            (Lock_Free_Read_32): Likewise.
            (Lock_Free_Read_64): Likewise.
            (Lock_Free_Try_Write): New function body.
            (Lock_Free_Try_Write_8): Delete.
            (Lock_Free_Try_Write_16): Likewise.
            (Lock_Free_Try_Write_32): Likewise.
            (Lock_Free_Try_Write_64): Likewise.
            * libgnat/s-aoinar.adb (Atomic_Fetch_And_Add): Use type-generic GCC
            atomic builtin and tidy up implementation.
            (Atomic_Fetch_And_Subtract): Likewise.
            * libgnat/s-aomoar.adb (Atomic_Fetch_And_Add): Likewise.
            (Atomic_Fetch_And_Subtract): Likewise.
            * libgnat/s-atopex.adb (Atomic_Exchange): Likewise.
            (Atomic_Compare_And_Exchange): Likewise.

commit abb540a70b05556da1b318e67f82d668b93d0824
Author: Eric Botcazou <ebotcazou@adacore.com>
Date:   Tue Oct 12 11:56:46 2021 +0200

    [Ada] Rewrite tests on Convention_Intrinsic
    
    gcc/ada/
    
            * gcc-interface/decl.c (gnat_to_gnu_entity) <E_Subprogram_Type>:
            Replace test on Convention_Intrinsic with Is_Intrinsic_Subprogram.
            (gnat_to_gnu_param): Likewise.
            (gnat_to_gnu_subprog_type): Likewise.
            * gcc-interface/trans.c (elaborate_all_entities_for_package): Ditto.

commit 4b39bab947436640c20624ec58b61830c8ab2d71
Author: Eric Botcazou <ebotcazou@adacore.com>
Date:   Mon Oct 11 16:56:06 2021 +0200

    [Ada] Small cleanup in Eval_Integer_Literal
    
    gcc/ada/
    
            * sem_eval.ads (Check_Non_Static_Context): Update documentation.
            * sem_eval.adb (In_Any_Integer_Context): Change parameter type,
            adjust accordingly and remove unreachable case.
            (Eval_Integer_Literal): Consider the node kind throughout and
            trim down verbose condition.

commit 0a8dcff179804d928b20ea8c2a40bc73035acaf3
Author: Doug Rupp <rupp@adacore.com>
Date:   Mon Sep 20 10:31:02 2021 -0700

    [Ada] Get rid of Frontend_Exceptions refs
    
    gcc/ada/
    
            * Makefile.rtl: Remove references to system-vxworks-ppc.ads
            and system-vxworks-x86.ads.
            * libgnat/system-vxworks-ppc.ads: Remove.
            * libgnat/system-vxworks-ppc-ravenscar.ads: Likewise.
            * libgnat/system-vxworks-x86.ads: Likewise.

commit 8c73de6ea4c05d11b278626e170d5932d7955095
Author: Yannick Moy <moy@adacore.com>
Date:   Thu Oct 7 09:05:45 2021 +0200

    [Ada] Issue warning on unused quantified expression
    
    gcc/ada/
    
            * sem_ch4.adb (Analyze_QUantified_Expression): Issue warning on
            conjunct/disjunct sub-expression of the full expression inside a
            quantified expression, when it does not reference the quantified
            variable.

commit d9fe0e53d8dbc7cae3170cd6ad783100ec3a704a
Author: Marc Poulhiès <poulhies@adacore.com>
Date:   Fri Oct 8 10:02:11 2021 +0200

    [Ada] Fix type conversion handling in validity checks
    
    gcc/ada/
    
            * checks.adb (Insert_Valid_Check): in case of checked type
            conversion, update Typ to match Exp's type and add call to
            Analyze_And_Resolve.

commit b47b5438b9b95cea90f8d925518e893259255a50
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Wed Oct 20 09:06:18 2021 +0200

    Remove unused back_threader_registry::m_threaded_paths.
    
    Tested on x86-64 Linux.
    
    gcc/ChangeLog:
    
            * tree-ssa-threadbackward.c (back_threader_registry::back_threader_registry):
            Remove.
            (back_threader_registry::register_path): Remove m_threaded_paths.

commit 82cd78f2c31db1664ca154d7fcd24e9eaee1427f
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Wed Oct 20 09:05:23 2021 +0200

    Restore --param=max-fsm-thread-length
    
    The removal of --param=max-fsm-thread-length is causing code
    explosion.  I thought that --param=max-fsm-thread-path-insns was a
    better gague for path profitability than raw BB length, but it turns
    out that we don't take into account PHIs when estimating the number of
    statements.
    
    In this PR, we have a sequence of very large PHIs that have us
    traversing extremely large paths that blow up the compilation.
    
    We could fix this a couple of different ways.  We could avoid
    traversing more than a certain number of PHI arguments, or ignore
    large PHIs altogether.  The old implementation certainly had this
    knob, and we could cut things off before we even got to the ranger.
    We could also adjust the instruction estimation to take into account
    PHIs, but I'm sure we'll mess something else in the process ;-).
    
    The easiest thing to do is just restore the knob.
    
    At a later time we could tweak this further, for instance,
    disregarding empty blocks in the count.  BTW, this is the reason I
    didn't chop things off in the lowlevel registry for all threaders: the
    forward threader can't really explore too deep paths, but it could
    theoretically get there while threading over empty blocks.
    
    This fixes 102814, 102852, and I bet it solves the Linux kernel cross
    compile issue.
    
    Tested on x86-64 Linux.
    
    gcc/ChangeLog:
    
            PR tree-optimization/102814
            * doc/invoke.texi: Document --param=max-fsm-thread-length.
            * params.opt: Add --param=max-fsm-thread-length.
            * tree-ssa-threadbackward.c
            (back_threader_profitability::profitable_path_p): Fail on paths
            longer than max-fsm-thread-length.

commit 972ee845f54839e9bd2e4611bb268d75440f3845
Author: Eric Botcazou <ebotcazou@adacore.com>
Date:   Wed Oct 20 10:42:56 2021 +0200

    Fix PR middle-end/102764
    
    This is a regression present on the mainline in the form of -fcompare-debug
    failure at -O3 on a compiler-generated testcase.  Fixed by disregarding a
    debug statement in the last position of a basic block to reset the current
    location for the outgoing edges.
    
    gcc/
            PR middle-end/102764
            * cfgexpand.c (expand_gimple_basic_block): Disregard a final debug
            statement to reset the current location for the outgoing edges.
    
    gcc/testsuite/
            * gcc.dg/pr102764.c: New test.

commit 8fe93cc664ded8cc1952da28b23f3fc68504a73e
Author: Arnaud Charlet <charlet@adacore.com>
Date:   Wed Oct 20 10:23:40 2021 +0200

    Avoid exception propagation during bootstrap
    
    This addresses PR ada/100486, which is the bootstrap failure of GCC 11 for
    32-bit Windows in the MSYS setup.  The PR shows that we cannot rely on
    exception propagation being operational during the bootstrap, at least on
    the 11 branch, so fix this by removing the problematic raise statement.
    
    gcc/ada/
            PR ada/100486
            * sem_prag.adb (Check_Valid_Library_Unit_Pragma): Do not raise an
            exception as part of the bootstrap.

commit c7abdf46fb7ac9a0c37f120feff3fcc3a752584f
Author: Jakub Jelinek <jakub@redhat.com>
Date:   Wed Oct 20 09:34:51 2021 +0200

    openmp: Fix up struct gomp_work_share handling [PR102838]
    
    If GOMP_HAVE_EFFICIENT_ALIGNED_ALLOC is not defined, the intent was to
    treat the split of the structure between first cacheline (64 bytes)
    as mostly write-once, use afterwards and second cacheline as rw just
    as an optimization.  But as has been reported, with vectorization enabled
    at -O2 it can now result in aligned vector 16-byte or larger stores.
    When not having posix_memalign/aligned_alloc/memalign or other similar API,
    alloc.c emulates it but it needs to allocate extra memory for the dynamic
    realignment.
    So, for the GOMP_HAVE_EFFICIENT_ALIGNED_ALLOC not defined case, this patch
    stops using aligned (64) attribute in the middle of the structure and instead
    inserts padding that puts the second half of the structure at offset 64 bytes.
    
    And when GOMP_HAVE_EFFICIENT_ALIGNED_ALLOC is defined, usually it was allocated
    as aligned, but for the orphaned case it could still be allocated just with
    gomp_malloc without guaranteed proper alignment.
    
    2021-10-20  Jakub Jelinek  <jakub@redhat.com>
    
            PR libgomp/102838
            * libgomp.h (struct gomp_work_share_1st_cacheline): New type.
            (struct gomp_work_share): Only use aligned(64) attribute if
            GOMP_HAVE_EFFICIENT_ALIGNED_ALLOC is defined, otherwise just
            add padding before lock to ensure lock is at offset 64 bytes
            into the structure.
            (gomp_workshare_struct_check1, gomp_workshare_struct_check2):
            New poor man's static assertions.
            * work.c (gomp_work_share_start): Use gomp_aligned_alloc instead of
            gomp_malloc if GOMP_HAVE_EFFICIENT_ALIGNED_ALLOC.

commit d4044db034b40c275b5f287d5854a102d22e07c0
Author: Tobias Burnus <tobias@codesourcery.com>
Date:   Wed Oct 20 08:32:16 2021 +0200

    gfortran.dg/bind-c-contiguous-5.c: Big-endian fix
    
    gcc/testsuite/
    
            PR fortran/102815
            * gfortran.dg/bind-c-contiguous-5.c (do_call, reset_var): Handle
            big andian.

commit 424945258d1778617b5d3d5273f6e1c10e718f80
Author: Jakub Jelinek <jakub@redhat.com>
Date:   Wed Oct 20 08:38:58 2021 +0200

    c++: Fix up push_local_extern_decl_alias error recovery [PR102642]
    
    My recent push_local_extern_decl_alias change broke error-recovery,
    do_pushdecl can return error_mark_node and set_decl_tls_model can't be
    called on that.  There are other code paths that store error_mark_node
    into DECL_LOCAL_DECL_ALIAS, with the intent to differentiate the cases
    where we haven't yet tried to push it into the namespace scope (NULL)
    and one where we have tried it but it failed (error_mark_node), but looking
    around, there are other spots where we call functions or do processing
    which doesn't tolerate error_mark_node.
    
    So, the first hunk with the testcase fixes the testcase, the others
    fix what I've spotted and the fix was easy to figure out (there are I think
    3 other spots mainly for function multiversioning).
    
    2021-10-20  Jakub Jelinek  <jakub@redhat.com>
    
            PR c++/102642
            * name-lookup.c (push_local_extern_decl_alias): Don't call
            set_decl_tls_model on error_mark_node.
            * decl.c (make_rtl_for_nonlocal_decl): Don't call
            set_user_assembler_name on error_mark_node.
            * parser.c (cp_parser_oacc_declare): Ignore DECL_LOCAL_DECL_ALIAS
            if it is error_mark_node.
            (cp_parser_omp_declare_target): Likewise.
    
            * g++.dg/tls/pr102642.C: New test.

commit d8edfadfc7a9795b65177a50ce44fd348858e844
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Mon Oct 4 09:47:02 2021 +0200

    Disallow loop rotation and loop header crossing in jump threaders.
    
    There is a lot of fall-out from this patch, as there were many threading
    tests that assumed the restrictions introduced by this patch were valid.
    Some tests have merely shifted the threading to after loop
    optimizations, but others ended up with no threading opportunities at
    all.  Surprisingly some tests ended up with more total threads.  It was
    a crapshoot all around.
    
    On a postive note, there are 6 tests that no longer XFAIL, and one
    guality test which now passes.
    
    I felt a bit queasy about such a fundamental change wrt threading, so I
    ran it through my callgrind test harness (.ii files from a bootstrap).
    There was no change in overall compilation, DOM, or the VRP threaders.
    
    However, there was a slight increase of 1.63% in the backward threader.
    I'm pretty sure we could reduce this if we incorporated the restrictions
    into their profitability code.  This way we could stop the search when
    we ran into one of these restrictions.  Not sure it's worth it at this
    point.
    
    Tested on x86-64 Linux.
    
    Co-authored-by: Richard Biener <rguenther@suse.de>
    
    gcc/ChangeLog:
    
            * tree-ssa-threadupdate.c (cancel_thread): Dump threading reason
            on the same line as the threading cancellation.
            (jt_path_registry::cancel_invalid_paths): Avoid rotating loops.
            Avoid threading through loop headers where the path remains in the
            loop.
    
    libgomp/ChangeLog:
    
            * testsuite/libgomp.graphite/force-parallel-5.c: Remove xfail.
    
    gcc/testsuite/ChangeLog:
    
            * gcc.dg/Warray-bounds-87.c: Remove xfail.
            * gcc.dg/analyzer/pr94851-2.c: Remove xfail.
            * gcc.dg/graphite/pr69728.c: Remove xfail.
            * gcc.dg/graphite/scop-dsyr2k.c: Remove xfail.
            * gcc.dg/graphite/scop-dsyrk.c: Remove xfail.
            * gcc.dg/shrink-wrap-loop.c: Remove xfail.
            * gcc.dg/loop-8.c: Adjust for new threading restrictions.
            * gcc.dg/tree-ssa/ifc-20040816-1.c: Same.
            * gcc.dg/tree-ssa/pr21559.c: Same.
            * gcc.dg/tree-ssa/pr59597.c: Same.
            * gcc.dg/tree-ssa/pr71437.c: Same.
            * gcc.dg/tree-ssa/pr77445-2.c: Same.
            * gcc.dg/tree-ssa/ssa-dom-thread-4.c: Same.
            * gcc.dg/tree-ssa/ssa-dom-thread-7.c: Same.
            * gcc.dg/vect/bb-slp-16.c: Same.
            * gcc.dg/tree-ssa/ssa-dom-thread-6.c: Remove.
            * gcc.dg/tree-ssa/ssa-dom-thread-18.c: Remove.
            * gcc.dg/tree-ssa/ssa-dom-thread-2a.c: Remove.
            * gcc.dg/tree-ssa/ssa-thread-invalid.c: New test.

commit f36240f8c835d792f788b6724e272fc0a4a4f26f
Author: Jeff Law <jeffreyalaw@gmail.com>
Date:   Wed Oct 20 00:26:59 2021 -0400

    Trivial fix to gil-1.c when analyzer is not enabled
    
    gcc/testsuite
            * gcc.dg/plugin/gil-1.c: Add dg-require-effective-target marker.

commit 9fbb6fa123be81c55e888e5d117e63d05780f0ed
Author: Siddhesh Poyarekar <siddhesh@gotplt.org>
Date:   Tue Oct 19 09:36:35 2021 +0530

    tree-object-size: Make unknown a computation
    
    Compute the unknown size value as a function of the min/max bit of
    object_size_type.  This transforms into a neat little branchless
    sequence on x86_64:
    
            movl    %edi, %eax
            sarl    %eax
            xorl    $1, %eax
            negl    %eax
            cltq
    
    which should be faster than loading the value from memory.  A quick
    unscientific test using
    
    `time make check-gcc RUNTESTFLAGS="dg.exp=builtin*"`
    
    shaves about half a second off execution time with this.  Also simplify
    implementation of unknown_object_size.
    
    gcc/ChangeLog:
    
            * tree-object-size.c (unknown): Make into a function.  Adjust
            all uses.
            (unknown_object_size): Simplify implementation.
    
    Signed-off-by: Siddhesh Poyarekar <siddhesh@gotplt.org>

commit 3c8d8c0be95e99dc0cba7f6fad2429243582119f
Author: liuhongt <hongtao.liu@intel.com>
Date:   Thu Oct 14 09:31:03 2021 +0800

    Adjust testcase for O2 vectorization.
    
    As discussed in [1], this patch add xfail/target selector to those
    testcases, also make a copy of them so that they can be tested w/o
    vectorization.
    
    Newly added xfail/target selectors are used to check the vectorization
    capability of continuous byte/double bytes storage, these scenarios
    are exactly the part of the testcases that regressed after O2
    vectorization.
    
    [1] https://gcc.gnu.org/pipermail/gcc-patches/2021-October/581456.html.
    
    2021-10-19  Hongtao Liu  <hongtao.liu@intel.com>
                Kewen Lin  <linkw@linux.ibm.com>
    
    gcc/ChangeLog
    
            * doc/sourcebuild.texi (Effective-Target Keywords): Document
            vect_slp_v2qi_store, vect_slp_v4qi_store, vect_slp_v8qi_store,
            vect_slp_v16qi_store, vect_slp_v2hi_store,
            vect_slp_v4hi_store, vect_slp_v2si_store, vect_slp_v4si_store.
    
    gcc/testsuite/ChangeLog
    
            PR middle-end/102722
            PR middle-end/102697
            PR middle-end/102462
            PR middle-end/102706
            PR middle-end/102744
            * c-c++-common/Wstringop-overflow-2.c: Adjust testcase with new
            xfail/target selector.
            * gcc.dg/Warray-bounds-51.c: Ditto.
            * gcc.dg/Warray-parameter-3.c: Ditto.
            * gcc.dg/Wstringop-overflow-14.c: Ditto.
            * gcc.dg/Wstringop-overflow-21.c: Ditto.
            * gcc.dg/Wstringop-overflow-68.c: Ditto.
            * gcc.dg/Wstringop-overflow-76.c: Ditto.
            * gcc.dg/Warray-bounds-48.c: Ditto.
            * gcc.dg/Wzero-length-array-bounds-2.c: Ditto.
            * lib/target-supports.exp (check_vect_slp_aligned_store_usage):
            New function.
            (check_effective_target_vect_slp_v2qi_store): Ditto.
            (check_effective_target_vect_slp_v4qi_store): Ditto.
            (check_effective_target_vect_slp_v8qi_store): Ditto.
            (check_effective_target_vect_slp_v16qi_store): Ditto.
            (check_effective_target_vect_slp_v2hi_store): Ditto.
            (check_effective_target_vect_slp_v4hi_store): Ditto.
            (check_effective_target_vect_slp_v2si_store): Ditto.
            (check_effective_target_vect_slp_v4si_store): Ditto.
            * c-c++-common/Wstringop-overflow-2-novec.c: New test.
            * gcc.dg/Warray-bounds-51-novec.c: New test.
            * gcc.dg/Warray-bounds-48-novec.c: New test.
            * gcc.dg/Warray-parameter-3-novec.c: New test.
            * gcc.dg/Wstringop-overflow-14-novec.c: New test.
            * gcc.dg/Wstringop-overflow-21-novec.c: New test.
            * gcc.dg/Wstringop-overflow-76-novec.c: New test.
            * gcc.dg/Wzero-length-array-bounds-2-novec.c: New test.

commit 19472fc3fc0cabcee5b8a5073e8128d21a1ed6f2
Author: GCC Administrator <gccadmin@gcc.gnu.org>
Date:   Wed Oct 20 00:16:43 2021 +0000

    Daily bump.

commit 5566f3c6b46cf053ae4b918513e318561b7af053
Author: Patrick Palka <ppalka@redhat.com>
Date:   Tue Oct 19 18:07:19 2021 -0400

    libstdc++: Implement LWG 3580 change to ranges::iota_view
    
    libstdc++-v3/ChangeLog:
    
            * include/std/ranges (iota_view::_Iterator::operator+): Adjust
            definition as per LWG 3580.
            (iota_view::_Iterator::operator-): Likewise.

commit bed1892f5b1a6601caa93d54c156a4d04d76ee7b
Author: Patrick Palka <ppalka@redhat.com>
Date:   Tue Oct 19 18:07:16 2021 -0400

    libstdc++: Implement LWG 3568 change to ranges::basic_istream_view
    
    libstdc++-v3/ChangeLog:
    
            * include/std/ranges (basic_istream_view::_M_object): Value
            initialize as per LWG 3568.

commit 98af6b86bc6cac705474c14bb3f9748f6866c859
Author: Patrick Palka <ppalka@redhat.com>
Date:   Tue Oct 19 18:07:05 2021 -0400

    libstdc++: Implement LWG 3470 change to ranges::subrange
    
    libstdc++-v3/ChangeLog:
    
            * include/bits/ranges_util.h
            (__detail::__uses_nonqualification_pointer_conversion): Define
            and use it ...
            (__detail::__convertible_to_nonslicing): ... here, as per LWG 3470.
            * testsuite/std/ranges/subrange/1.cc: New test.

commit 861440a77b62756d200ae356c4fdfd9653902e77
Author: Patrick Palka <ppalka@redhat.com>
Date:   Tue Oct 19 17:54:24 2021 -0400

    libstdc++: Implement LWG 3523 changes to ranges::iota_view
    
    libstdc++-v3/ChangeLog:
    
            * include/std/ranges (iota_view::_Iterator): Befriend iota_view.
            (iota_view::_Sentinel): Likewise.
            (iota_view::iota_view): Add three overloads, each taking an
            iterator/sentinel pair as per LWG 3523.
            * testsuite/std/ranges/iota/iota_view.cc (test06): New test.

commit 53b1c382d5a6fe8dec394a7ff820d77cda02af81
Author: Patrick Palka <ppalka@redhat.com>
Date:   Tue Oct 19 17:50:56 2021 -0400

    libstdc++: Implement LWG 3549 changes to ranges::enable_view
    
    This patch also reverts r11-3504 since that workaround is now obsolete
    after this resolution.
    
    libstdc++-v3/ChangeLog:
    
            * include/bits/ranges_base.h (view_interface): Forward declare.
            (__detail::__is_derived_from_view_interface_fn): Declare.
            (__detail::__is_derived_from_view_interface): Define as per LWG 3549.
            (enable_view): Adjust as per LWG 3549.
            * include/bits/ranges_util.h (view_interface): Don't derive from
            view_base.
            * include/std/ranges (filter_view): Revert r11-3504 change.
            (transform_view): Likewise.
            (take_view): Likewise.
            (take_while_view): Likewise.
            (drop_view): Likewise.
            (drop_while_view): Likewise.
            (join_view): Likewise.
            (lazy_split_view): Likewise.
            (split_view): Likewise.
            (reverse_view): Likewise.
            * testsuite/std/ranges/adaptors/sizeof.cc: Update expected sizes.
            * testsuite/std/ranges/view.cc (test_view::test_view): Remove
            this default ctor since views no longer need to be default initable.
            (test01): New test.

commit c6a1fdd6dde3a95997731c8339d70970aca67594
Author: Jonathan Wakely <jwakely@redhat.com>
Date:   Tue Oct 19 20:37:53 2021 +0100

    doc: Fix typo in name of PowerPC __builtin_cpu_supports built-in
    
    gcc/ChangeLog:
    
            * doc/extend.texi (Basic PowerPC Built-in Functions): Fix typo.

commit 58f339fc5eaae7db9526f81ab91f282ad4a9b8cc
Author: Jonathan Wakely <jwakely@redhat.com>
Date:   Tue Oct 19 12:31:06 2021 +0100

    libstdc++: Implement std::random_device::entropy() for other sources
    
    Currently this function only returns a non-zero value for /dev/random
    and /dev/urandom. When a hardware instruction such as RDRAND is in use
    it should (in theory) be perfectly random and produce 32 bits of entropy
    in each 32-bit result. Add a helper function to identify the source of
    randomness from the _M_func and _M_file data members, and return a
    suitable value when RDRAND or RDSEED is being used.
    
    libstdc++-v3/ChangeLog:
    
            * src/c++11/random.cc (which_source): New helper function.
            (random_device::_M_getentropy()): Use which_source and return
            suitable values for sources other than device files.
            * testsuite/26_numerics/random/random_device/entropy.cc: New test.

commit 3cfbe5dc08b574bccc398256946cc03e2a767329
Author: Paul A. Clarke <pc@us.ibm.com>
Date:   Mon Aug 9 13:08:25 2021 -0500

    rs6000: Guard some x86 intrinsics implementations
    
    Some compatibility implementations of x86 intrinsics include
    Power intrinsics which require POWER8.  Guard them.
    
    emmintrin.h:
    - _mm_cmpord_pd: Remove code which was ostensibly for pre-POWER8,
      but which indeed depended on POWER8 (vec_cmpgt(v2du)/vcmpgtud).
      The "POWER8" version works fine on pre-POWER8.
    - _mm_mul_epu32: vec_mule(v4su) uses vmuleuw.
    pmmintrin.h:
    - _mm_movehdup_ps: vec_mergeo(v4su) uses vmrgow.
    - _mm_moveldup_ps: vec_mergee(v4su) uses vmrgew.
    smmintrin.h:
    - _mm_cmpeq_epi64: vec_cmpeq(v2di) uses vcmpequd.
    - _mm_mul_epi32: vec_mule(v4si) uses vmuluwm.
    - _mm_cmpgt_epi64: vec_cmpgt(v2di) uses vcmpgtsd.
    tmmintrin.h:
    - _mm_sign_epi8: vec_neg(v4si) uses vsububm.
    - _mm_sign_epi16: vec_neg(v4si) uses vsubuhm.
    - _mm_sign_epi32: vec_neg(v4si) uses vsubuwm.
      Note that the above three could actually be supported pre-POWER8,
      but current GCC does not support them before POWER8.
    - _mm_sign_pi8: depends on _mm_sign_epi8.
    - _mm_sign_pi16: depends on _mm_sign_epi16.
    - _mm_sign_pi32: depends on _mm_sign_epi32.
    
    sse4_2-pcmpgtq.c:
    - _mm_cmpgt_epi64: vec_cmpeq(v2di) uses vcmpequd.
    
    2021-10-19  Paul A. Clarke  <pc@us.ibm.com>
    
    gcc
            PR target/101893
            PR target/102719
            * config/rs6000/emmintrin.h: Guard POWER8 intrinsics.
            * config/rs6000/pmmintrin.h: Same.
            * config/rs6000/smmintrin.h: Same.
            * config/rs6000/tmmintrin.h: Same.
    
    gcc/testsuite
            * gcc.target/powerpc/sse4_2-pcmpgtq.c: Tighten dg constraints
            to minimally Power8.

commit ce8add4b0e086e671a7e08503408356ad6beee7f
Author: Paul A. Clarke <pc@us.ibm.com>
Date:   Mon Oct 18 19:12:12 2021 -0500

    rs6000: Add nmmintrin.h to extra_headers
    
    Fix an omission in commit 29fb1e831bf1c25e4574bf2f98a9f534e5c67665.
    
    2021-10-19  Paul A. Clarke  <pc@us.ibm.com>
    
    gcc
            * config.gcc (extra_headers): Add nmmintrin.h.

commit 04d392e8430ca66a3f12b7db4f3cb84788269a48
Author: Jonathan Wakely <jwakely@redhat.com>
Date:   Tue Oct 19 16:00:13 2021 +0100

    libstdc++: Fix doxygen generation to work with relative paths
    
    In r12-826 I tried to remove some redundant steps from the doxygen
    build, but they are needed when configure is run as a relative path. The
    use of pwd is to resolve the relative path to an absolute one.
    
    libstdc++-v3/ChangeLog:
    
            * doc/Makefile.am (stamp-html-doxygen, stamp-html-doxygen)
            (stamp-latex-doxygen, stamp-man-doxygen): Fix recipes for
            relative ${top_srcdir}.
            * doc/Makefile.in: Regenerate.

commit ff0eec94e87dfb7dc387f120ca5ade2707aecf50
Author: Tobias Burnus <tobias@codesourcery.com>
Date:   Tue Oct 19 16:38:56 2021 +0200

    Fortran: Fix 'fn spec' for deferred character length
    
    Shows now up with gfortran.dg/deferred_type_param_6.f90 due to more ME
    optimizations, causing fails without this commit.
    
    gcc/fortran/ChangeLog:
    
            * trans-types.c (create_fn_spec): For allocatable/pointer
            character(len=:), use 'w' not 'R' as fn spec for the length dummy
            argument.

commit 7ef0cc444488e0bfa9b63d46307105e78ffc17a6
Author: Martin Liska <mliska@suse.cz>
Date:   Tue Oct 19 16:12:42 2021 +0200

    Make file utf8 valid input.
    
    liboffloadmic/ChangeLog:
    
            * include/coi/source/COIBuffer_source.h: Convert 2 chars to
            unicode.

commit 93bd0213885739a1073f8c98911f8a00c0eb5597
Author: Richard Biener <rguenther@suse.de>
Date:   Mon Oct 18 15:55:22 2021 +0200

    Refactor vect_supportable_dr_alignment
    
    This refactors vect_supportable_dr_alignment to get the misalignment
    as input parameter which allows us to elide modifying/restoring
    of DR_MISALIGNMENT during alignment peeling analysis which eventually
    makes it more straight-forward to split out the negative step
    handling.
    
    2021-10-19  Richard Biener  <rguenther@suse.de>
    
            * tree-vectorizer.h (vect_supportable_dr_alignment): Add
            misalignment parameter.
            * tree-vect-data-refs.c (vect_get_peeling_costs_all_drs):
            Do not change DR_MISALIGNMENT in place, instead pass the
            adjusted misalignment to vect_supportable_dr_alignment.
            (vect_peeling_supportable): Likewise.
            (vect_peeling_hash_get_lowest_cost): Adjust.
            (vect_enhance_data_refs_alignment): Likewise.
            (vect_vfa_access_size): Likewise.
            (vect_supportable_dr_alignment): Add misalignment
            parameter and simplify.
            * tree-vect-stmts.c (get_negative_load_store_type): Adjust.
            (get_group_load_store_type): Likewise.
            (get_load_store_type): Likewise.

commit 5a8832b1659e311437d25b7ec8b078be27ae54b8
Author: Jonathan Wakely <jwakely@redhat.com>
Date:   Tue Oct 19 11:53:27 2021 +0100

    libstdc++: Change std::variant union member to empty struct
    
    This more clearly expresses the intent (a completely unused, trivial
    type) than using char. It's also consistent with the unions in
    std::optional.
    
    libstdc++-v3/ChangeLog:
    
            * include/std/variant (_Uninitialized): Use an empty struct
            for the unused union member, instead of char.

commit c4ecb11e4f7ea15f636e463248c8b14083bef05d
Author: Jonathan Wakely <jwakely@redhat.com>
Date:   Tue Oct 19 11:38:26 2021 +0100

    libstdc++: Fix std::stack deduction guide
    
    libstdc++-v3/ChangeLog:
    
            * include/bits/stl_stack.h (stack(Iterator, Iterator)): Remove
            non-deducible template parameter from deduction guide.
            * testsuite/23_containers/stack/deduction.cc: Check new C++23
            deduction guides.

commit 82b2e4f8cf5a01c6724fe3f465a77ee03cfcaae2
Author: Jonathan Wakely <jwakely@redhat.com>
Date:   Tue Oct 19 11:06:56 2021 +0100

    libstdc++: Implement monadic operations for std::optional (P0798R8)
    
    Another new addition to the C++23 working draft.
    
    The new member functions of std::optional are only defined for C++23,
    but the new members of _Optional_payload_base are defined for C++20 so
    that they can be used in non-propagating-cache in <ranges>. The
    _Optional_payload_base::_M_construct member can also be used in
    non-propagating-cache now, because it's constexpr since r12-4389.
    
    There will be an LWG issue about the feature test macro, suggesting that
    we should just bump the value of __cpp_lib_optional instead. I haven't
    done that here, but it can be changed once consensus is reached on the
    change.
    
    libstdc++-v3/ChangeLog:
    
            * include/std/optional (_Optional_payload_base::_Storage): Add
            constructor taking a callable function to invoke.
            (_Optional_payload_base::_M_apply): New function.
            (__cpp_lib_monadic_optional): Define for C++23.
            (optional::and_then, optional::transform, optional::or_else):
            Define for C++23.
            * include/std/ranges (__detail::__cached): Remove.
            (__detail::__non_propagating_cache): Remove use of __cached for
            contained value. Use _Optional_payload_base::_M_construct and
            _Optional_payload_base::_M_apply to set the contained value.
            * include/std/version (__cpp_lib_monadic_optional): Define.
            * testsuite/20_util/optional/monadic/and_then.cc: New test.
            * testsuite/20_util/optional/monadic/or_else.cc: New test.
            * testsuite/20_util/optional/monadic/or_else_neg.cc: New test.
            * testsuite/20_util/optional/monadic/transform.cc: New test.
            * testsuite/20_util/optional/monadic/version.cc: New test.

commit 6920d5a1a2834e9c62d441b8f4c6186b01107d13
Author: Tobias Burnus <tobias@codesourcery.com>
Date:   Tue Oct 19 15:16:01 2021 +0200

    Fortran: Fix "str" to scalar descriptor conversion [PR92482]
    
            PR fortran/92482
    gcc/fortran/ChangeLog:
    
            * trans-expr.c (gfc_conv_procedure_call): Use TREE_OPERAND not
            build_fold_indirect_ref_loc to undo an ADDR_EXPR.
    
    gcc/testsuite/ChangeLog:
    
            * gfortran.dg/bind-c-char-descr.f90: Remove xfail; extend a bit.

commit e3ef92e79f9f1c4275a9e19652439089a310627d
Author: Clément Chigot <clement.chigot@atos.net>
Date:   Thu Oct 14 09:03:13 2021 +0200

    aix: ensure reference to __tls_get_addr is in text section.
    
    The garbage collector of AIX linker might remove the reference to
    __tls_get_addr if it's added inside an unused csect, which can be
    the case of .data with very simple programs.
    
    gcc/ChangeLog:
    2021-10-19  Clément Chigot  <clement.chigot@atos.net>
    
            * config/rs6000/rs6000.c (rs6000_xcoff_file_end): Move
            __tls_get_addr reference to .text csect.

commit 6b34f5c5ec75823d656b6882f12d46248402a2aa
Author: Martin Liska <mliska@suse.cz>
Date:   Tue Oct 19 11:11:16 2021 +0200

    target: Support whitespaces in target attr/pragma.
    
            PR target/102375
    
    gcc/ChangeLog:
    
            * config/aarch64/aarch64.c (aarch64_process_one_target_attr):
            Strip whitespaces.
    
    gcc/testsuite/ChangeLog:
    
            * gcc.target/aarch64/pr102375.c: New test.

commit 5f5baf79927dfa8208f540cd1a441a56b0dfdb38
Author: Clément Chigot <clement.chigot@atos.net>
Date:   Tue Oct 19 13:20:14 2021 +0200

    MAINTAINERS: Add myself for write after approval
    
    ChangeLog:
    2021-10-19  Clément Chigot  <clement.chigot@atos.net>
    
            * MAINTAINERS: Add myself for write after approval.

commit 793d2549b173a0a2da6dd20ffc27acb9fd2de73e
Author: Richard Biener <rguenther@suse.de>
Date:   Tue Oct 19 12:40:59 2021 +0200

    Refactor load/store costing
    
    This passes down the already available alignment scheme and
    misalignment to the load/store costing routines, removing
    redundant queries.
    
    2021-10-19  Richard Biener  <rguenther@suse.de>
    
            * tree-vectorizer.h (vect_get_store_cost): Adjust signature.
            (vect_get_load_cost): Likewise.
            * tree-vect-data-refs.c (vect_get_data_access_cost): Get
            alignment support scheme and misalignment as arguments
            and pass them down.
            (vect_get_peeling_costs_all_drs): Compute that info here
            and note that we shouldn't need to.
            * tree-vect-stmts.c (vect_model_store_cost): Get
            alignment support scheme and misalignment as arguments.
            (vect_get_store_cost): Likewise.
            (vect_model_load_cost): Likewise.
            (vect_get_load_cost): Likewise.
            (vectorizable_store): Pass down alignment support scheme
            and misalignment to costing.
            (vectorizable_load): Likewise.

commit 9890b12c72c02828c691f22198c3e0afd8678991
Author: Jonathan Wakely <jwakely@redhat.com>
Date:   Tue Oct 19 09:16:56 2021 +0100

    libstdc++: Fix mem-initializer in std::move_only_function [PR102825]
    
    libstdc++-v3/ChangeLog:
    
            PR libstdc++/102825
            * include/bits/mofunc_impl.h (move_only_function): Remove
            invalid base initializer.
            * testsuite/20_util/move_only_function/cons.cc: Instantiate
            constructors to check bodies.

commit 476ca5ade8522d566ffffeab0bece6a64aefeecd
Author: Richard Biener <rguenther@suse.de>
Date:   Tue Oct 19 11:36:13 2021 +0200

    Compute negative offset in get_load_store_type
    
    This moves the computation of a negative offset that needs to be
    applied when we vectorize a negative stride access to
    get_load_store_type alongside where we compute the actual access
    method.
    
    2021-10-19  Richard Biener  <rguenther@suse.de>
    
            * tree-vect-stmts.c (get_negative_load_store_type): Add
            offset output parameter and initialize it.
            (get_group_load_store_type): Likewise.
            (get_load_store_type): Likewise.
            (vectorizable_store): Use offset as computed by
            get_load_store_type.
            (vectorizable_load): Likewise.

commit d996799a507f9f4c379b55b004233be92fa63380
Author: Richard Biener <rguenther@suse.de>
Date:   Tue Oct 19 10:19:12 2021 +0200

    tree-optimization/102827 - avoid stmts in preheader
    
    The PR shows that when carefully crafting the runtime alias
    condition in the vectorizer we might end up using defs from
    the loop preheader but will end up inserting the condition
    before the .LOOP_VECTORIZED call.  So the following makes
    sure to insert invariants before that when we versioned the
    loop, preserving the invariant the vectorizer relies on.
    
    2021-10-19  Richard Biener  <rguenther@suse.de>
    
            PR tree-optimization/102827
            * tree-if-conv.c (predicate_statements): Add pe parameter
            and use that edge to insert invariant stmts on.
            (combine_blocks): Pass through pe.
            (tree_if_conversion): Compute the edge to insert invariant
            stmts on and pass it along.
    
            * gcc.dg/pr102827.c: New testcase.

commit f98359ba9d3775319fb3181009be7d3dafe9ba15
Author: Roger Sayle <roger@nextmovesoftware.com>
Date:   Tue Oct 19 11:00:10 2021 +0100

    PR target/102785: Correct addsub/subadd patterns on bfin.
    
    This patch resolves PR target/102785 where my recent patch to constant
    fold saturating addition/subtraction exposed a latent bug in the bfin
    backend.  The patterns used for blackfin's V2HI ssaddsub and sssubadd
    instructions had the indices/operations swapped.  This was harmless
    until we started evaluating these expressions at compile-time, when
    the mismatch was caught by the testsuite.
    
    2021-10-19  Roger Sayle  <roger@nextmovesoftware.com>
    
    gcc/ChangeLog
            PR target/102785
            * config/bfin/bfin.md (addsubv2hi3, subaddv2hi3, ssaddsubv2hi3,
            sssubaddv2hi3):  Swap the order of operators in vec_concat.

commit 0910c516a3d72af048af27308349167f25c406c2
Author: Xionghu Luo <luoxhu@linux.ibm.com>
Date:   Tue Oct 19 04:02:04 2021 -0500

    rs6000: Remove unspecs for vec_mrghl[bhw]
    
    vmrghb only accepts permute index {0, 16, 1, 17, 2, 18, 3, 19, 4, 20,
    5, 21, 6, 22, 7, 23} no matter for BE or LE in ISA, similarly for vmrglb.
    Remove UNSPEC_VMRGH_DIRECT/UNSPEC_VMRGL_DIRECT pattern as vec_select
    + vec_concat as normal RTL.
    
    Tested pass on P8LE, P9LE and P8BE{m32}.
    
    gcc/ChangeLog:
    
    2021-10-19  Xionghu Luo  <luoxhu@linux.ibm.com>
    
            * config/rs6000/altivec.md (*altivec_vmrghb_internal): Delete.
            (altivec_vmrghb_direct): New.
            (*altivec_vmrghh_internal): Delete.
            (altivec_vmrghh_direct): New.
            (*altivec_vmrghw_internal): Delete.
            (altivec_vmrghw_direct_<mode>): New.
            (altivec_vmrghw_direct): Delete.
            (*altivec_vmrglb_internal): Delete.
            (altivec_vmrglb_direct): New.
            (*altivec_vmrglh_internal): Delete.
            (altivec_vmrglh_direct): New.
            (*altivec_vmrglw_internal): Delete.
            (altivec_vmrglw_direct_<mode>): New.
            (altivec_vmrglw_direct): Delete.
            * config/rs6000/rs6000-p8swap.c (rtx_is_swappable_p): Adjust.
            * config/rs6000/rs6000.c (altivec_expand_vec_perm_const):
            Adjust.
            * config/rs6000/vsx.md (vsx_xxmrghw_<mode>): Adjust.
            (vsx_xxmrglw_<mode>): Adjust.
    
    gcc/testsuite/ChangeLog:
    
    2021-10-19  Xionghu Luo  <luoxhu@linux.ibm.com>
    
            * gcc.target/powerpc/builtins-1.c: Update instruction counts.

commit d2161caffbb56a434776608af4e4491b59e508c8
Author: Aldy Hernandez <aldyh@redhat.com>
Date:   Tue Oct 19 09:33:34 2021 +0200

    Change threading comment before pass_ccp pass.
    
    gcc/ChangeLog:
    
            * passes.def: Change threading comment before pass_ccp pass.

commit 91419baf4d0075d11e3667b816c83687288163fd
Author: Haochen Gui <guihaoc@gcc.gnu.org>
Date:   Tue Oct 19 16:28:31 2021 +0800

    Optimize the builtin vec_xl_sext
    
    gcc/
            * config/rs6000/rs6000-call.c (altivec_expand_lxvr_builtin):
            Modify the expansion for sign extension. All extensions are done
            within VSX registers.
    
    gcc/testsuite/
            * gcc.target/powerpc/p10_vec_xl_sext.c: New test.

commit 6b4c18b98127087d7f14062b81bc678f0589cd36
Author: prathamesh.kulkarni <prathamesh.kulkarni@linaro.org>
Date:   Tue Oct 19 13:51:51 2021 +0530

    [sve] PR93183 - Add support for conditional neg.
    
    gcc/testsuite/ChangeLog:
            PR target/93183
            * gcc.target/aarch64/sve/pr93183.c: Remove -mcpu=generic+sve from dg-options.

commit d19d90289d1343e4fb0550eb1151db6da8a0d1ce
Author: Richard Biener <rguenther@suse.de>
Date:   Mon Oct 18 14:59:54 2021 +0200

    Add misalignment output parameter to get_load_store_type
    
    This makes us compute the misalignment alongside the alignment support
    scheme in get_load_store_type, removing some out-of-place calls to
    the DR alignment API.
    
    2021-10-18  Richard Biener  <rguenther@suse.de>
    
            * tree-vect-stmts.c (get_group_load_store_type): Add
            misalignment output parameter and initialize it.
            (get_group_load_store_type): Likewise.
            (vectorizable_store): Remove now redundant queries.
            (vectorizable_load): Likewise.

commit f45610a45236e97616726ca042898d6ac46a082e
Author: Jakub Jelinek <jakub@redhat.com>
Date:   Tue Oct 19 09:24:57 2021 +0200

    c++: Don't reject calls through PMF during constant evaluation [PR102786]
    
    The following testcase incorrectly rejects the c initializer,
    while in the s.*a case cxx_eval_* sees .__pfn reads etc.,
    in the s.*&S::foo case get_member_function_from_ptrfunc creates
    expressions which use INTEGER_CSTs with type of pointer to METHOD_TYPE.
    And cxx_eval_constant_expression rejects any INTEGER_CSTs with pointer
    type if they aren't 0.
    Either we'd need to make sure we defer such folding till cp_fold but the
    function and pfn_from_ptrmemfunc is used from lots of places, or
    the following patch just tries to reject only non-zero INTEGER_CSTs
    with pointer types if they don't point to METHOD_TYPE in the hope that
    all such INTEGER_CSTs with POINTER_TYPE to METHOD_TYPE are result of
    folding valid pointer-to-member function expressions.
    I don't immediately see how one could create such INTEGER_CSTs otherwise,
    cast of integers to PMF is rejected and would have the PMF RECORD_TYPE
    anyway, etc.
    
    2021-10-19  Jakub Jelinek  <jakub@redhat.com>
    
            PR c++/102786
            * constexpr.c (cxx_eval_constant_expression): Don't reject
            INTEGER_CSTs with type POINTER_TYPE to METHOD_TYPE.
    
            * g++.dg/cpp2a/constexpr-virtual19.C: New test.

commit caab0139761b04226fab14d87c4a4f981d942bbf
Author: Richard Biener <rguenther@suse.de>
Date:   Mon Oct 18 15:55:22 2021 +0200

    Remove check_aligned parameter from vect_supportable_dr_alignment
    
    There are two calls with true as parameter, one is only relevant
    for the case of the misalignment being unknown which means the
    access is never aligned there, the other is in the peeling hash
    insert code used conditional on the unlimited cost model which
    adds an artificial count.  But the way it works right now is
    that it boosts the count if the specific misalignment when not peeling
    is unsupported - in particular when the access is currently aligned
    we'll query the backend with a misalign value of zero.  I've
    changed it to boost the peeling when unknown alignment is not
    supported instead and noted how we could in principle improve this.
    
    2021-10-19  Richard Biener  <rguenther@suse.de>
    
            * tree-vectorizer.h (vect_supportable_dr_alignment): Remove
            check_aligned argument.
            * tree-vect-data-refs.c (vect_supportable_dr_alignment):
            Likewise.
            (vect_peeling_hash_insert): Add supportable_if_not_aligned
            argument and do not call vect_supportable_dr_alignment here.
            (vect_peeling_supportable): Adjust.
            (vect_enhance_data_refs_alignment): Compute whether the
            access is supported with different alignment here and
            pass that down to vect_peeling_hash_insert.
            (vect_vfa_access_size): Adjust.
            * tree-vect-stmts.c (vect_get_store_cost): Likewise.
            (vect_get_load_cost): Likewise.
            (get_negative_load_store_type): Likewise.
            (get_group_load_store_type): Likewise.
            (get_load_store_type): Likewise.

commit df592811f950301ed3b10a08e476dad0f2eff26a
Author: Martin Liska <mliska@suse.cz>
Date:   Mon Oct 4 14:06:14 2021 +0200

    target: support spaces in target attribute.
    
            PR target/102374
    
    gcc/ChangeLog:
    
            * config/i386/i386-options.c (ix86_valid_target_attribute_inner_p): Strip whitespaces.
            * system.h (strip_whilespaces): New function.
    
    gcc/testsuite/ChangeLog:
    
            * gcc.target/i386/pr102374.c: New test.

commit 38f6ee6bfc4633175ca6f6d29e597d379ccae820
Author: dianhong xu <dianhong.xu@intel.com>
Date:   Sat Oct 9 18:23:35 2021 +0800

    AVX512FP16: Add *_set1_pch intrinsics.
    
    Add *_set1_pch (_Float16 _Complex A) intrinsics.
    
    gcc/ChangeLog:
    
            * config/i386/avx512fp16intrin.h:
            (_mm512_set1_pch): New intrinsic.
            * config/i386/avx512fp16vlintrin.h:
            (_mm256_set1_pch): New intrinsic.
            (_mm_set1_pch): Ditto.
    
    gcc/testsuite/ChangeLog:
    
            * gcc.target/i386/avx512fp16-set1-pch-1a.c: New test.
            * gcc.target/i386/avx512fp16-set1-pch-1b.c: New test.
            * gcc.target/i386/avx512fp16vl-set1-pch-1a.c: New test.
            * gcc.target/i386/avx512fp16vl-set1-pch-1b.c: New test.


---


### compiler : `gcc`
### title : `Missed loop unrolling opportunity`
### open_at : `2021-10-29T19:11:02Z`
### last_modified_date : `2021-11-02T08:14:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103002
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
#define C 3


struct node {
    struct node *next;
    int payload;
};

static int count_nodes(const node* p) {
    int size = 0;
    while (p) {
        p = p->next;
        size++;
    }
    return size;
}

bool has_one_node(const node* p) {
    return count_nodes(p) == 1;
}


bool has_C_nodes(const node* p) {
    return count_nodes(p) == C;
}

has_one_node(node const*):                # @has_one_node(node const*)
        test    rdi, rdi
        je      .LBB0_1
        mov     eax, 1
.LBB0_3:                                # =>This Inner Loop Header: Depth=1
        mov     rdi, qword ptr [rdi]
        add     eax, -1
        test    rdi, rdi
        jne     .LBB0_3
        test    eax, eax
        sete    al
        ret
.LBB0_1:
        xor     eax, eax
        ret
has_C_nodes(node const*):                 # @has_C_nodes(node const*)
        test    rdi, rdi
        je      .LBB1_1
        mov     eax, 3
.LBB1_3:                                # =>This Inner Loop Header: Depth=1
        mov     rdi, qword ptr [rdi]
        add     eax, -1
        test    rdi, rdi
        jne     .LBB1_3
        test    eax, eax
        sete    al
        ret
.LBB1_1:
        xor     eax, eax
        ret


has_C_nodes is simple with some kind of loop deletion pass, but generally, these loops can be unrolled for some reasonable C values.


https://godbolt.org/z/do656c17b


---


### compiler : `gcc`
### title : `poor inlined builtin_fmod on x86_64`
### open_at : `2021-10-30T18:51:33Z`
### last_modified_date : `2022-02-14T07:35:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103008
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
Created attachment 51706
ggl.f90

This is from looking at a Fortran benchmark set
<https://www.fortran.uk/fortran-compiler-comparisons/>, but presumably
isn't Fortran-specific.

One of the cases in that set (ac.f90) gets bottlenecked on a random
number routine (which may be rubbish, but it's there).  It uses DMOD,
which gets compiled to __builtin_fmod according to the tree dump, and
is inlined.  However, the benchmark performance is still 50% worse
with gfortran than Intel ifort, and if I replace DMOD with its
definition, gfortran is much closer to ifort.

I'll attach files ggl.f90, the original, and gglx.f90 which avoids the
call to the intrinsic, along with assembler from each.  The assembler
is from GCC 11.2.0, run (on SKX) as

  gfortran -Ofast -march=native

(I note that the generated fmod isn't inlined with -O3, which looks to
me like a Fortran miss that I should report.)

I only take benchmarks too seriously for understanding the results
but, at least with PDO, GCC is pretty much on a par with ifort on the
bottom line of that set, despite also #40770, and another poor case. :-)


---


### compiler : `gcc`
### title : `Extra move to x0 for non-POD returns`
### open_at : `2021-10-31T00:59:43Z`
### last_modified_date : `2021-11-02T13:53:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103010
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Take the following C++ code:
void ll() noexcept(true);
struct k {
  ~k() noexcept(true);
};
k ice(k *a)
{
  k v;
  ll();
  return v;
}
---- CUT ----
Currently GCC produces:
        stp     x29, x30, [sp, -32]!
        mov     x29, sp
        str     x19, [sp, 16]
        mov     x19, x8
        bl      _Z2llv
        mov     x0, x19
        ldr     x19, [sp, 16]
        ldp     x29, x30, [sp], 32

But the ABI does say x0 need to be set here at all.
So really the call to _Z2llv could even be a tail call.


---


### compiler : `gcc`
### title : `Missed optimization for symmetric subset: (a & b) == a || (a & b) == b`
### open_at : `2021-11-03T22:45:11Z`
### last_modified_date : `2021-11-04T09:19:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103071
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.2.0`
### severity : `enhancement`
### contents :
This is a bit of a long shot, but I'll file it anyway :-)

I have this function in a hot path (of course, in the real project, it's inlined):

#include <stdbool.h>
#include <stdint.h>

void foo();
void bar();

void EitherIsSubset(uint64_t v0, uint64_t v1) {
        if ((v0 & v1) == v0 || (v0 & v1) == v1) {
                foo();
        } else {
                bar();
        }
}

It is intended to treat v0 and v1 as bit sets, and then test whether either v0 or v1 is a subset of each other (or that they are equal). (An equivalent formulation happens to be replacing & with |.)

GCC compiles (with -O2, x86-64) this to:

EitherIsSubset:
	movq	%rdi, %rax
	andq	%rsi, %rax
	cmpq	%rsi, %rax
	je	.L4
	cmpq	%rdi, %rax
	je	.L4
	xorl	%eax, %eax
	jmp	bar@PLT
.L4:
	xorl	%eax, %eax
	jmp	foo@PLT

This is pretty straight-forward, but feels like it's using two (relatively hard-to-predict) branches where it should be possible to deal with one. And indeed, GNU superopt (!) found this amazing sequence instead, with v0 in eax and v1 in edx (this is, of course, trivially portable to 64-bit):

14:	mov 	%edx,%ecx
	or 	%eax,%edx
	cmp 	%edx,%eax
	sbb 	%ebx,%ebx
	sbb 	%ecx,%edx
	adc 	$1,%ebx

I can't claim to understand fully what it does, but after this, ebx contains either 0 or 1 with the right answer, and one would assume that after this, the zero flag is also usable to branch on (leaving us with one branch instead of two, in all).

Is it possible to teach GCC this sequence? I tried using it as inline assembler, and while it works, it seems it becomes suboptimal and slower, because I can't return a condition code (so I get a redundant test):

inline bool EitherIsSubsetAsm(uint64_t v0, uint64_t v1) {
        uint64_t tmp = v0 | v1;
        bool result;
        asm("cmp %1, %2 ; sbb %0, %0 ; sbb %3, %1 ; adc $1, %0"
            : "=r"(result), "+&r"(tmp)
            : "r"(v0), "r"(v1)
            : "cc");
        return result;
}

void EitherIsSubset(uint64_t v0, uint64_t v1) {
        if (EitherIsSubsetAsm(v0, v1)) {
                foo();
        } else {
                bar();
        }
}


---


### compiler : `gcc`
### title : `Folding common switch code`
### open_at : `2021-11-04T00:01:26Z`
### last_modified_date : `2021-11-29T20:35:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103072
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
In the following code, GCC can optimize foo but not bar:

void foo(int x) {
    switch(x) {
        case 1: func1(); break;
        case 2: func1(); break;
        case 3: func1(); break;
        case 4: func1(); break;
        case 5: func1(); break;
        default: __builtin_unreachable();
    }
}

void bar(int x) {
    switch(x) {
        case 1: func2(x); break;
        case 2: func2(x); break;
        case 3: func2(x); break;
        case 4: func2(x); break;
        case 5: func2(x); break;
        default: __builtin_unreachable();
    }
}

https://godbolt.org/z/3eqnG71f5 (LLVM optimizes both)


---


### compiler : `gcc`
### title : `slp vectorizer failed to try smaller size for generic vectors with word_mode`
### open_at : `2021-11-04T05:27:49Z`
### last_modified_date : `2021-11-12T20:12:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103076
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
struct Ax
{
  int n;
  short a[4];
};
void
foo4_2 (struct Ax *p)
{
  p->a[0] = 0;
  p->a[1] = 1;
  p->a[2] = 2;
  p->a[3] = 3;
}

Struct is 4-byte aligned, and gcc failed to vectorize it since s390 backend doesn't support unaligned vector(4) short store. But it can be splitted into 2 vector(2) short store, i.e.

struct Ax
{
  int n;
  short a[4];
};
void
foo4_2 (struct Ax *p)
{
  p->a[0] = 0;
  p->a[1] = 1;
}


void
foo4_3 (struct Ax *p)
{
  p->a[2] = 2;
  p->a[3] = 3;
}


Gcc can vectorize both foo4_2 and foo4_3 with 
add new stmt: MEM <vector(2) short int> [(short int *)vectp.5_5] = { 0, 1 };
add new stmt: MEM <vector(2) short int> [(short int *)vectp.10_5] = { 2, 3 }


---


### compiler : `gcc`
### title : `[i386] GCC should use the SF and ZF flags in some atomic_fetch_op sequences`
### open_at : `2021-11-04T21:24:03Z`
### last_modified_date : `2021-11-10T18:32:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103090
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Disclaimer: I don't know this code actually exists anywhere. But I've just come up with it.

In Bug 102566, we optimised:

bool tbit(std::atomic<int> &i)
{
    return i.fetch_or(1, std::memory_order_relaxed) & 1;
}

To emit LOCK BTS. Similarly, fetch_xor got LOCK BTC and fetch_and got LOCK BTR. These all work because CF is set by the bit-test-and-op instructions.

It occurs to me that LOCK AND, LOCK OR and LOCK XOR reliably set the SF, ZF, and PF flags according to the result, so they may be used too. I can't think of any time the PF flag would be useful and obviously ZF will not be set after a LOCK OR (unless you OR'ed zero, but why would you do that?).

So possibilities are:

static constexpr int signbit = 0x80000000;
bool tsign1(std::atomic<int> &i)
{
    int bit = 1; // any one or more bits, except for a constant sign bit
    return i.fetch_or(bit, std::memory_order_relaxed) & signbit;
}
bool tsign2(std::atomic<int> &i)
{
    int bit = 1; // any one or more bits, except for a constant sign bit
    return i.fetch_xor(bit, std::memory_order_relaxed) & signbit;
}
bool tzero1(std::atomic<int> &i)
{
    int bits = 1; // any one or more bits
    return i.fetch_and(bit, std::memory_order_relaxed) == 0;
}
bool tzero2(std::atomic<int> &i)
{
    int bits = 1; // any one or more bits
    return i.fetch_xor(bit, std::memory_order_relaxed) == 0;
}

all of the above can be negated too (op != 0 and (op & signbit) == 0).


---


### compiler : `gcc`
### title : `madd not used for multiply add on POWER9`
### open_at : `2021-11-06T18:56:26Z`
### last_modified_date : `2023-02-15T09:13:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103109
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
The following code

#include <stdint.h>

void Long_multiplication( uint64_t multiplicand[],
                          uint64_t multiplier[],
                          uint64_t sum[],
                          uint64_t ilength, uint64_t jlength )
{
  uint64_t acarry, mcarry, product;

  for( uint64_t i = 0;
       i < (ilength + jlength);
       i++ )
    sum[i] = 0;

  acarry = 0;
  for( uint64_t j = 0; j < jlength; j++ )
    {
      mcarry = 0;
      for( uint64_t i = 0; i < ilength; i++ )
        {
          __uint128_t mcarry_prod;
          __uint128_t acarry_sum;
          mcarry_prod = ((__uint128_t) multiplicand[i]) * ((__uint128_t) multiplier[j])
            + (__uint128_t) mcarry;
          mcarry = mcarry_prod >> 64;
          product = mcarry_prod;
          acarry_sum = ((__uint128_t) sum[i+j]) + ((__uint128_t) acarry) + product;
          sum[i+j] += acarry_sum;
          acarry = acarry_sum >> 64;
          //      {mcarry, product} = multiplicand[i]*multiplier[j]
          //                            + mcarry;
          //      {acarry,sum[i+j]} = {sum[i+j]+acarry} + product;
          
        }
    }
}

is translated by

$ gcc -mcpu=power9 -mtune=power9 -S -O3 big_int.c

to (assembler output of the loop)

.L4:
        mtctr 25
        mr 12,23
        add 3,24,4
        li 5,0
        .p2align 4,,15
.L5:
        ldu 10,8(12)
        ldx 11,29,4
        ldu 9,8(3)
        mulld 8,10,11
        mulhdu 10,10,11
        addc 30,8,5
        addze 31,10
        and 21,30,6
        and 22,31,7
        addc 10,21,9
        mr 5,31
        adde 8,22,28
        addc 10,10,0
        add 9,9,10
        addze 0,8
        std 9,0(3)
        bdnz .L5
        addi 27,27,1
        addi 4,4,8
        cmpld 0,26,27
        bne 0,.L4

For the idiom to calculate mcarry_prod, I would have expected
a pair of maddhdu and maddld instructions.

This is with

gcc-Version 12.0.0 20211028 (experimental) (GCC)


---


### compiler : `gcc`
### title : `PPC: "mr" instruction is unnecessary when extending DI to V1TI`
### open_at : `2021-11-08T03:20:57Z`
### last_modified_date : `2022-01-18T01:15:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103124
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
//test.c
vector __int128 init (long long a)
{
  vector __int128 b;
  b = (vector __int128) {a};
  return b;
}

gcc -O2 -s test.c -mcpu=power9

//p9 assembly
        mr 10,3
        sradi 11,3,63
        mtvsrdd 34,11,10

The first mr is unnecessary if the last one is changed to "mtvsrdd 34,11,3".


---


### compiler : `gcc`
### title : `stack adjustment with struct with int element on aarch64be-*-*`
### open_at : `2021-11-08T07:02:55Z`
### last_modified_date : `2021-11-10T13:16:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103125
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
demo:

typedef struct {
    int a;
} S;

void foo(S s)
{
    return;
}

compile with gcc demo.c -mbig-endian -O2 -S

output:

foo:
        sub     sp, sp, #16
        add     sp, sp, 16
        ret

I think the function must not use the stack, just like compiling with LLVM. is this gcc bug? 
https://gcc.godbolt.org/z/r35a3PbxE


---


### compiler : `gcc`
### title : `Miss vectorization for bit_and/bit_ior/bit_xor reduction`
### open_at : `2021-11-08T08:06:27Z`
### last_modified_date : `2021-11-10T08:29:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103126
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Cat test.c

#include<stdint.h>

void xor_bit_arr_nolcd (uint64_t *__restrict mat, uint64_t* a,uint64_t* b, uint64_t *__restrict ans,
    int64_t n)
{
  int64_t i;
  uint64_t vec1, sum1;
  uint64_t vec2, sum2;

  while (n > 0) {
    sum1 = 0;
    vec1 = a[0];
    sum2 = 0;
    vec2 = b[0];

    for (i = 0; i < 64; i++) {
      uint64_t tmp = mat[i]; // always safe to load
      uint64_t vec1_i = (vec1 >> i);
      uint64_t vec2_i = (vec2 >> i);
      sum1 ^= (vec1_i & 1) ? tmp : 0;
      if (vec2_i&1) sum2 ^= tmp;
    }
    *ans++ ^= sum1;  n--;
    *ans++ ^= sum2;  n--;
  }
}


vectorizer failed exactly the same reason as PR98365 #c3

(In reply to Richard Biener from comment #3)
> The issue is that we hit
> 
>   /* If this isn't a nested cycle or if the nested cycle reduction value
>      is used ouside of the inner loop we cannot handle uses of the reduction
>      value.  */
>   if (nlatch_def_loop_uses > 1 || nphi_def_loop_uses > 1)
>     {
>       if (dump_enabled_p ())
>         dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,
>                          "reduction used in loop.\n");
>       return NULL;
>     }
> 
> because cnt_21 is used in both the update and the COND_EXPR.  The reduction
> doesn't fit the cond reductions we support but is a blend of a cond and
> regular reduction.  Making the COND-reduction support handle this case
> should be possible though.
> 
> Using 'int' we arrive at handled IL:
> 
>   # cnt_19 = PHI <cnt_8(7), 0(15)>
>   _ifc__32 = _4 == _7 ? 1 : 0;
>   cnt_8 = cnt_19 + _ifc__32;
> 
> so adjusting if-conversion can indeed help.

I'm working on a patch to extend ifcvt(is_cond_scalar_reduction) to handle bit_and/bit_ior/bit_xor operation.


---


### compiler : `gcc`
### title : `vectorizer failed to recognize shift>>=1 in loop as shift>>i`
### open_at : `2021-11-09T02:07:48Z`
### last_modified_date : `2022-09-07T00:57:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103144
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
#include<stdint.h>

void
foo1 (uint64_t* __restrict pdst, uint64_t* psrc, uint64_t shift)
{
    for (int64_t i = 0; i != 64; i++)
    {
      uint64_t shift_i = shift >> i;
      pdst[i] = psrc[i] + shift_i;
    }
}

void
foo (uint64_t* __restrict pdst, uint64_t* psrc, uint64_t shift)
{
    for (int64_t i = 0; i != 64; i++)
    {
      pdst[i] = psrc[i] + shift;
      shift >>= 1;
    }
}

gcc can vectorize foo1 but not foo since there's cross-iteration dep for shift >>= 1, but shift >>= 1 is just shift >> i in loop.

Where should it be handled, can vect_recog_pattern handle this?


---


### compiler : `gcc`
### title : `Structure return is not optimized on 32-bit targets`
### open_at : `2021-11-09T07:18:45Z`
### last_modified_date : `2021-11-09T20:29:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103150
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the following test case:

struct timespec {
  long long tv_sec;
  long tv_nsec;
};

void  a(struct timespec *);
void  b(struct timespec *);

static inline struct timespec c(void)
{
  struct timespec t;

  a(&t);

  return t;
}

void f(void)
{
  struct timespec ts = c();
  b(&ts);
}

For the 64-bit targets (I checked x86_64, aarch64, riscv, powerpc) this is optimized to:

aarch64-rtems6-gcc -O2 -S -o - getptr.c 
        .arch armv8-a
        .file   "getptr.c"
        .text
        .align  2
        .p2align 4,,11
        .global f
        .type   f, %function
f:
        stp     x29, x30, [sp, -32]!
        mov     x29, sp
        add     x0, sp, 16
        bl      a
        add     x0, sp, 16
        bl      b
        ldp     x29, x30, [sp], 32
        ret
        .size   f, .-f
        .ident  "GCC: (GNU) 10.3.1 20210409 (RTEMS 6, RSB 889cf95db0122bd1a6b21598569620c40ff2069d, Newlib eb03ac1)"

The pointer used to get the data from a() is directly passed to the consumer b().

However, for 32-bit targets this test case seems to be not optimized and the data is copied on the stack frame. For example:

arm-rtems6-gcc -O2 -S -o - getptr.c 
        .cpu arm7tdmi
        .eabi_attribute 20, 1
        .eabi_attribute 21, 1
        .eabi_attribute 23, 3
        .eabi_attribute 24, 1
        .eabi_attribute 25, 1
        .eabi_attribute 26, 2
        .eabi_attribute 30, 2
        .eabi_attribute 34, 0
        .eabi_attribute 18, 4
        .file   "getptr.c"
        .text
        .align  2
        .global f
        .arch armv4t
        .syntax unified
        .arm
        .fpu softvfp
        .type   f, %function
f:
        @ Function supports interworking.
        @ args = 0, pretend = 0, frame = 32
        @ frame_needed = 0, uses_anonymous_args = 0
        push    {r4, lr}
        sub     sp, sp, #32
        add     r4, sp, #16
        mov     r0, r4
        bl      a
        mov     ip, sp
        ldm     r4, {r0, r1, r2, r3}
        stm     ip, {r0, r1, r2, r3}
        mov     r0, ip
        bl      b
        add     sp, sp, #32
        @ sp needed
        pop     {r4, lr}
        bx      lr
        .size   f, .-f
        .ident  "GCC: (GNU) 10.3.1 20210409 (RTEMS 6, RSB 889cf95db0122bd1a6b21598569620c40ff2069d, Newlib eb03ac1)"

riscv-rtems6-gcc -O2 -S -o - getptr.c 
        .file   "getptr.c"
        .option nopic
        .text
        .align  1
        .globl  f
        .type   f, @function
f:
        addi    sp,sp,-48
        addi    a0,sp,16
        sw      ra,44(sp)
        call    a
        lw      a5,16(sp)
        mv      a0,sp
        sw      a5,0(sp)
        lw      a5,20(sp)
        sw      a5,4(sp)
        lw      a5,24(sp)
        sw      a5,8(sp)
        lw      a5,28(sp)
        sw      a5,12(sp)
        call    b
        lw      ra,44(sp)
        addi    sp,sp,48
        jr      ra
        .size   f, .-f
        .ident  "GCC: (GNU) 10.3.1 20210409 (RTEMS 6, RSB 889cf95db0122bd1a6b21598569620c40ff2069d, Newlib eb03ac1)"


---


### compiler : `gcc`
### title : `Value numbering for PRE of pure functions can be improved`
### open_at : `2021-11-10T11:15:23Z`
### last_modified_date : `2021-11-24T12:06:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103168
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
In the following testcase the call to test is loop invariant because memory modified is not escaping. While value numbering of pure functions we however add value number of the vdef corresponding to vuse.

I think we want to walk the chain to aliasing store?

__attribute__((const)) int test();
int
main()
{
        int ret;
        int a[10];
        for (int i=0; i<10;i++)
          if (test())
                  a[i]++;
        for (int i=0; i<10;i++)
                ret *= a[i];
        return ret;
}


---


### compiler : `gcc`
### title : `[12/13 Regression] tfft2 text grows by 70% with -Ofast since r12-5113-gd70ef65692fced7a`
### open_at : `2021-11-11T17:44:13Z`
### last_modified_date : `2022-07-26T13:10:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103195
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
This is seen at LTN here

https://lnt.opensuse.org/db_default/v4/CPP/24179
https://lnt.opensuse.org/db_default/v4/CPP/graph?highlight_run=24179&plot.0=14.799.4

and similarly on other testers.  Performance is not improved so it looks like bit extreme even for -Ofast. It would be nice to know why such growth happens.


---


### compiler : `gcc`
### title : `[10/11] ppc inline expansion of memcpy/memmove should not use lxsibzx/stxsibx for a single byte`
### open_at : `2021-11-11T18:27:48Z`
### last_modified_date : `2022-08-16T23:38:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103197
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.3.1`
### severity : `normal`
### contents :
This got broken sometime in gcc 10 timeframe. For this test case:

#include <string.h>
void m(char *a, char *b)
{
  memcpy(a,b,9);
}

AT13 (gcc 9.3.1) produces:

m:
.LFB0:
        .cfi_startproc
        ld 10,0(4)
        lbz 9,8(4)
        std 10,0(3)
        stb 9,8(3)
        blr
        .long 0
        .byte 0,0,0,0,0,0,0,0
        .cfi_endproc

which is the expected code to copy 9 bytes.

AT14 (gcc 10.3.1), gcc 11, and current trunk all produce:

m:
.LFB0:
        .cfi_startproc
        addi 10,4,8
        ld 9,0(4)
        lxsibzx 0,0,10
        std 9,0(3)
        addi 9,3,8
        stxsibx 0,0,9
        blr
        .long 0
        .byte 0,0,0,0,0,0,0,0
        .cfi_endproc

which is really bad, mixing gpr and vsx. The inline expansion code in expand_block_move() does not attempt to generate vsx code at all unless the size is at least 16 bytes.


---


### compiler : `gcc`
### title : `[12 Regression] FAIL: gcc.target/i386/pr91333.c scan-assembler-times vmovapd|vmovsd 3 since r12-5177-g494bdadf28d0fb35`
### open_at : `2021-11-11T20:16:24Z`
### last_modified_date : `2021-11-12T13:14:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103200
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
I see the following failure in an x86_64-linux build with today's top of trunk:

Executing on host: /build/gcc-master/gcc/xgcc -B/build/gcc-master/gcc/ /src/gcc/master/gcc/testsuite/gcc.target/i386/pr91333.c    -fdiagnostics-plain-output  -O2 -mavx -ffat-lto-objects -fno-ident -S -o pr91333.s    (timeout = 300)
spawn -ignore SIGHUP /build/gcc-master/gcc/xgcc -B/build/gcc-master/gcc/ /src/gcc/master/gcc/testsuite/gcc.target/i386/pr91333.c -fdiagnostics-plain-output -O2 -mavx -ffat-lto-objects -fno-ident -S -o pr91333.s
PASS: gcc.target/i386/pr91333.c (test for excess errors)
gcc.target/i386/pr91333.c: vmovapd|vmovsd found 1 times
FAIL: gcc.target/i386/pr91333.c scan-assembler-times vmovapd|vmovsd 3


---


### compiler : `gcc`
### title : `missed optimization, phiopt/vrp?`
### open_at : `2021-11-13T00:25:35Z`
### last_modified_date : `2023-10-14T04:45:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103216
### status : `ASSIGNED`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
signed char f(unsigned char a)
{
  unsigned char b;
  signed char c, d, v;
  b = a & 127;
  c = (signed char) b;
  d = (signed char) a;
  v = c;
  if (d < 0)
    v = c | -128;
  return v;
}

This should just be optimized to return (signed char)a;

Reduced from the testcase provided by https://twitter.com/__phantomderp/status/1459247957904080901 .


---


### compiler : `gcc`
### title : `(a < 0) << signbit is not always optimized to a & signbitmask at the gimple level`
### open_at : `2021-11-13T01:40:38Z`
### last_modified_date : `2021-11-16T15:10:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103218
### status : `RESOLVED`
### tags : `missed-optimization, patch, TREE`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(signed char a)
{
  signed char t = a < 0;
  return (unsigned char)(t << 7);
}

At the gimple level we get:
int f(signed char a)
{
  signed char t = a < 0;
  return (unsigned char)(t << 7);
}
But combine is able to it:
Trying 9 -> 10:
    9: {r89:QI=r91:SI#0 0>>0x7;clobber flags:CC;}
      REG_DEAD r91:SI
      REG_UNUSED flags:CC
   10: {r90:QI=r89:QI<<0x7;clobber flags:CC;}
      REG_DEAD r89:QI
      REG_UNUSED flags:CC
Successfully matched this instruction:
(parallel [
        (set (reg:QI 90)
            (and:QI (subreg:QI (reg:SI 91) 0)
                (const_int -128 [0xffffffffffffff80])))
        (clobber (reg:CC 17 flags))
    ])
allowing combination of insns 9 and 10
original costs 4 + 4 = 8
replacement cost 4
deferring deletion of insn with uid = 9.
modifying insn i3    10: {r90:QI=r91:SI#0&0xffffffffffffff80;clobber flags:CC;}
      REG_DEAD r91:SI
      REG_UNUSED flags:CC
deferring rescan insn with uid = 10.

If we had wrote the testcase like:
int f(signed char a)
{
  return (a < 0) << 7;
}

GCC does optimize it to:
(int) NON_LVALUE_EXPR <a> & 128


---


### compiler : `gcc`
### title : `missed casting issue with |/&?`
### open_at : `2021-11-13T04:48:28Z`
### last_modified_date : `2023-04-14T21:54:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103220
### status : `RESOLVED`
### tags : `missed-optimization, patch, TREE`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
signed char f(unsigned char a)
{
  unsigned char b = a & 127;
  signed char c = (signed char) b;
  signed char d = (signed char) a;
  signed char e = d & -128;
  signed char h = c | e;
  return h;
}

---- CUT ----
GCC can optimize this at the RTL level to just return a but misses out at the gimple level.
Found this while working on PR 103216.


---


### compiler : `gcc`
### title : `evrp removes |SIGN but does not propagate the ssa name`
### open_at : `2021-11-13T06:54:27Z`
### last_modified_date : `2022-01-12T20:59:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103221
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
signed char f(signed char a)
{
  signed char v;
  v = a;
  if (a < 0)
    v = a | -128;
  return v;
}
---- CUT ----
EVRP is able to remove the | -128 part but still left with:
  if (a_2(D) < 0)
    goto <bb 3>; [INV]
  else
    goto <bb 4>; [INV]

  <bb 3> :
  v_4 = a_2(D);

  <bb 4> :
  # v_1 = PHI <a_2(D)(2), v_4(3)>
  return v_1;

But the PHI should be removed and the function should have been at this point just "return a_2(D);".

Note this is not a regression and even the original VRP had a similar issue too.


---


### compiler : `gcc`
### title : `[9/10/11/12 Regression] missed optimization with |^ at the gimple level`
### open_at : `2021-11-14T06:30:16Z`
### last_modified_date : `2021-11-17T23:40:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103228
### status : `RESOLVED`
### tags : `missed-optimization, patch, TREE`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Take:
int f(int a, int b)
{
  b|=1u;
  b|=2;
  return b;
}

----- CUT ---
This no longer gets optimized at the gimple level but it did in 4.7.x.
This is also true for ^.

I Noticed this while writing testcases for PR 103216.


---


### compiler : `gcc`
### title : `Failure to detect abs pattern using multiplication with slightly different casts for unsigned case`
### open_at : `2021-11-15T10:06:39Z`
### last_modified_date : `2023-05-28T00:18:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103245
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
While working on PR 103228, I got the testcase for PR 94785 failing.
It fails if I change the casts even:
unsigned int f4 (int v)
{
  unsigned int d_6;
  int b_5;
  int a_4;
  _Bool _1;
  unsigned int v1_2;
  unsigned int _7;
  int _9;

  _1 = v < 0;
  a_4 = (int) _1;
  b_5 = -a_4;
  _9 = b_5 | 1;
  d_6 = (unsigned int) _9;
  v1_2 = (unsigned int) v;
  _7 = v1_2 * d_6;
  return _7;
}
---- CUT ---


---


### compiler : `gcc`
### title : `questionable codegen with kmovd`
### open_at : `2021-11-15T14:58:15Z`
### last_modified_date : `2021-11-24T05:47:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103252
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
Created attachment 51798
preprocessed source

gcc 11.2 with -march=native is generating avx512 code that seems less efficient than the non avx512 case, for inet_aton_end in glibc 2.34.

Non-avx512 (-march=skylake):

mov     ecx, ds:(__libc_tsd_CTYPE_B_tpoff - _GLOBAL_OFFSET_TABLE_)[ebx]
mov     ecx, gs:[ecx]

Avx512:

kmovd   k0, ds:(__libc_tsd_CTYPE_B_tpoff - _GLOBAL_OFFSET_TABLE_)[ebx]
kmovd   edx, k0
kmovd   k0, dword ptr gs:[edx]
kmovd   edx, k0

Command line is:

thinkpad /var/tmp/portage/sys-libs/glibc-2.34-r2/work/glibc-2.34/resolv # x86_64-pc-linux-gnu-gcc -m32 -march=native -pipe -O2 -Wl,-O1 -Wl,--as-needed inet_addr.c -c -std=gnu11 -fgnu89-inline  -march=native -pipe -O2 -Wall -Wwrite-strings -Wundef -fmerge-all-constants -frounding-math -fstack-protector-strong -fno-common -Wstrict-prototypes -Wold-style-definition -fmath-errno    -Wa,-mtune=i686   -ftls-model=initial-exec   -U_FORTIFY_SOURCE   -I../include -I/var/tmp/portage/sys-libs/glibc-2.34-r2/work/build-x86-x86_64-pc-linux-gnu-nptl/resolv  -I/var/tmp/portage/sys-libs/glibc-2.34-r2/work/build-x86-x86_64-pc-linux-gnu-nptl  -I../sysdeps/unix/sysv/linux/i386/i686  -I../sysdeps/i386/i686/nptl  -I../sysdeps/unix/sysv/linux/i386  -I../sysdeps/unix/sysv/linux/x86/include -I../sysdeps/unix/sysv/linux/x86  -I../sysdeps/x86/nptl  -I../sysdeps/i386/nptl  -I../sysdeps/unix/sysv/linux/include -I../sysdeps/unix/sysv/linux  -I../sysdeps/nptl  -I../sysdeps/pthread  -I../sysdeps/gnu  -I../sysdeps/unix/inet  -I../sysdeps/unix/sysv  -I../sysdeps/unix/i386  -I../sysdeps/unix  -I../sysdeps/posix  -I../sysdeps/i386/i686/fpu/multiarch  -I../sysdeps/i386/i686/fpu  -I../sysdeps/i386/i686/multiarch  -I../sysdeps/i386/i686  -I../sysdeps/i386/fpu  -I../sysdeps/x86/fpu  -I../sysdeps/i386  -I../sysdeps/x86/include -I../sysdeps/x86  -I../sysdeps/wordsize-32  -I../sysdeps/ieee754/float128  -I../sysdeps/ieee754/ldbl-96/include -I../sysdeps/ieee754/ldbl-96  -I../sysdeps/ieee754/dbl-64  -I../sysdeps/ieee754/flt-32  -I../sysdeps/ieee754  -I../sysdeps/generic  -I.. -I../libio -I. -nostdinc -isystem /usr/lib/gcc/x86_64-pc-linux-gnu/11.2.0/include -isystem /usr/lib/gcc/x86_64-pc-linux-gnu/11.2.0/include-fixed -isystem /usr/include -D_LIBC_REENTRANT -include /var/tmp/portage/sys-libs/glibc-2.34-r2/work/build-x86-x86_64-pc-linux-gnu-nptl/libc-modules.h -DMODULE_NAME=libc -include ../include/libc-symbols.h  -DPIC     -DTOP_NAMESPACE=glibc -o /var/tmp/portage/sys-libs/glibc-2.34-r2/work/build-x86-x86_64-pc-linux-gnu-nptl/resolv/inet_addr.o -MD -MP -MF /var/tmp/portage/sys-libs/glibc-2.34-r2/work/build-x86-x86_64-pc-linux-gnu-nptl/resolv/inet_addr.o.dt -MT /var/tmp/portage/sys-libs/glibc-2.34-r2/work/build-x86-x86_64-pc-linux-gnu-nptl/resolv/inet_addr.o

And the preprocessed -E source is attached. This was noticed during investigating https://www.sourceware.org/bugzilla/show_bug.cgi?id=28595 , which seems likely to be a binutils or glibc bug instead. But nonetheless, the code gen here seems surprising.


---


### compiler : `gcc`
### title : `Unused COND_MUL isn't removed by DCE even with -fno-trapping-math`
### open_at : `2021-11-15T17:05:47Z`
### last_modified_date : `2023-05-08T12:23:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103253
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
gcc-12.0.0-alpha20111114 snapshot (g:3057f1ab737582a9fb37a3fb967ed8bf3659f2f4) ICEs when compiling the following testcase, reduced from libgomp/testsuite/libgomp.c/examples-4/simd-8.c, w/ -march=knl -O2 -fexceptions -fopenmp -fno-delete-dead-exceptions -fno-trapping-math:

double
do_work (double do_work_pri)
{
  int i;

#pragma omp simd
  for (i = 0; i < 17; ++i)
    do_work_pri = (!i ? 0.5 : i) * 2.0;

  return do_work_pri;
}

% x86_64-unknown-linux-gnu-gcc-12.0.0 -march=knl -O2 -fexceptions -fopenmp -fno-delete-dead-exceptions -fno-trapping-math -c gdl8z9xi.c
during GIMPLE pass: widening_mul
gdl8z9xi.c: In function 'do_work':
gdl8z9xi.c:2:1: internal compiler error: Segmentation fault
    2 | do_work (double do_work_pri)
      | ^~~~~~~
0xeac96f crash_signal
	/var/tmp/portage/sys-devel/gcc-12.0.0_alpha20211114/work/gcc-12-20211114/gcc/toplev.c:322
0x104eecf contains_struct_check(tree_node*, tree_node_structure_enum, char const*, int, char const*)
	/var/tmp/portage/sys-devel/gcc-12.0.0_alpha20211114/work/gcc-12-20211114/gcc/tree.h:3554
0x104eecf convert_mult_to_fma
	/var/tmp/portage/sys-devel/gcc-12.0.0_alpha20211114/work/gcc-12-20211114/gcc/tree-ssa-math-opts.c:3227
0x105056c after_dom_children
	/var/tmp/portage/sys-devel/gcc-12.0.0_alpha20211114/work/gcc-12-20211114/gcc/tree-ssa-math-opts.c:4673
0x1b9ed62 dom_walker::walk(basic_block_def*)
	/var/tmp/portage/sys-devel/gcc-12.0.0_alpha20211114/work/gcc-12-20211114/gcc/domwalk.c:352
0x10436e2 execute
	/var/tmp/portage/sys-devel/gcc-12.0.0_alpha20211114/work/gcc-12-20211114/gcc/tree-ssa-math-opts.c:4718


---


### compiler : `gcc`
### title : `[9/10/11/12 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-11-15T18:21:24Z`
### last_modified_date : `2023-06-17T01:40:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103257
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat case.c

void foo(void);
unsigned b, c;
static short a(short e, short f) { return e * f; }
int main() {
  if (a(1  ^ ((0, 0) ^ 1 && b) <= b, c))
    foo();
  c = 0;
}

11.2.0 at -O3 can eliminate the call to foo but trunk at -O3 cannot:

gcc11  case.c -S -O3 -o /dev/stdout
main:
.LFB1:
	.cfi_startproc
	movl	b(%rip), %eax
	xorl	%edx, %edx
	testl	%eax, %eax
	setne	%dl
	cmpl	%edx, %eax
	movl	$0, %eax
	cmovb	c(%rip), %ax
	testw	%ax, %ax
	jne	.L11
	movl	$0, c(%rip)
	xorl	%eax, %eax
	ret
.L11:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%edx, %edx
	xorl	%eax, %eax
	movl	%edx, c(%rip)
	popq	%rcx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc


gcc-trunk -v
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211115 (experimental) (GCC)

It started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=0288527f47cec6698b31ccb3210816415506009e


---


### compiler : `gcc`
### title : `Enable ZMM in MOVE_MAX and STORE_MAX_PIECES without -mprefer-vector-width=512`
### open_at : `2021-11-15T23:57:43Z`
### last_modified_date : `2021-12-04T12:56:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103269
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Need a way to enable ZMM in MOVE_MAX and STORE_MAX_PIECES without
-mprefer-vector-width=512.


---


### compiler : `gcc`
### title : `failure to use vld20/vld21 to vectorize for ARM MVE`
### open_at : `2021-11-16T10:07:48Z`
### last_modified_date : `2021-11-16T12:49:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103272
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
With current GCC trunk with -mcpu=cortex-m55 -mfpu=auto

#include <stdint.h>

typedef struct {
  int16_t v1;
  int16_t v2;
} data;

void test (data* restrict d, data* restrict x,
           data* restrict y, uint32_t L) {
  for (uint32_t i = 0; i < L*16; i++) {
     d[i].v1 = x[i].v1*y[i].v1;
     d[i].v2 = x[i].v2*y[i].v2;
   }
}

we generate:
test:
        lsls    r3, r3, #4
        beq     .L9
        lsls    r3, r3, #2
        push    {lr}
        sub     lr, r3, #16
        lsr     lr, lr, #4
        add     lr, lr, #1
        dls     lr, lr
.L3:
        vldrh.16        q3, [r2], #16
        vldrh.16        q2, [r1], #16
        vmul.i16        q3, q3, q2
        vstrh.16        q3, [r0], #16
        le      lr, .L3
        ldr     pc, [sp], #4
.L9:
        bx      lr


while LLVM generates:
test:
        push    {r7, lr}
        mov     r7, sp
        mov.w   r12, #0
        cmp.w   r12, r3, lsl #4
        it      eq
        popeq   {r7, pc}
        mvn     r12, #7
        add.w   r12, r12, r3, lsl #4
        movs    r3, #1
        add.w   lr, r3, r12, lsr #3
.LBB0_2:
        vld20.16        {q0, q1}, [r1]
        vld20.16        {q2, q3}, [r2]
        vld21.16        {q0, q1}, [r1]!
        vld21.16        {q2, q3}, [r2]!
        vmul.i16        q0, q2, q0
        vmul.i16        q1, q3, q1
        vst20.16        {q0, q1}, [r0]
        vst21.16        {q0, q1}, [r0]!
        le      lr, .LBB0_2
        pop     {r7, pc}


OTOH, GCC vectorizes better the samples included in the testsuite (gcc.target/arm/simd/mve-vld2.c)


---


### compiler : `gcc`
### title : `[12 Regression] Recent change to cddce inhibits switch optimization`
### open_at : `2021-11-16T17:16:21Z`
### last_modified_date : `2021-11-30T13:48:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103278
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
On iq2000-elf this change:

commit 045206450386bcd774db3bde0c696828402361c6
Author: Richard Biener <rguenther@suse.de>
Date:   Fri Nov 12 10:21:22 2021 +0100

    tree-optimization/102880 - improve CD-DCE
[ ... ]

Is inhibiting switch optimization for tree-ssa/if-to-switch-3.c from converting an if statement into a switch statement:

/* { dg-do compile } */
/* { dg-options "-O2 -fdump-tree-iftoswitch-optimized" } */

int IsMySuperRandomChar(int aChar)
{
  return aChar == 0x0009 || aChar == 0x000A ||
         aChar == 0x000C || aChar == 0x000D ||
         aChar == 0x0020 || aChar == 0x0030;
}

/* { dg-final { scan-tree-dump "Condition chain with \[^\n\r]\* BBs transformed into a switch statement." "iftoswitch" } } */


After today's cd-dce change we no longer turn that into a switch:

Before:

;; Canonical GIMPLE case clusters: 9-10 12 13 32 48
;; JT can be built: JT(values:6 comparisons:10 range:40 density: 25.00%):9-48
j.c:8:26: optimized: Condition chain with 3 BBs transformed into a switch statement.


After:
;; Canonical GIMPLE case clusters: 9-10 12-13 32 48


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-11-16T18:31:51Z`
### last_modified_date : `2023-08-08T15:46:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103281
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat case.c
void foo(void);

static unsigned b;

int main() {
  for (; b < 3; b++) {
    char c = b;
    char a = c ? c : c << 1;
    if (!(a < 1 ^ b))
      foo();
  }
}

trunk cannot eliminate the call to foo but 11.2.0 can:

gcc-11.2.0 -O3 -S -o /dev/stdout case.c
main:
.LFB0:
	.cfi_startproc
	movl	b(%rip), %eax
	cmpl	$2, %eax
	ja	.L6
	testl	%eax, %eax
	movl	$1, %edx
	cmove	%edx, %eax
.L8:
	addl	$1, %eax
	movl	%eax, b(%rip)
	cmpl	$2, %eax
	je	.L8
.L6:
	xorl	%eax, %eax
	ret
	.cfi_endproc


gcc-trunk -O3 -S -o /dev/stdout case.c
main:
.LFB0:
	.cfi_startproc
	movl	b(%rip), %eax
	cmpl	$2, %eax
	ja	.L13
	.p2align 4,,10
	.p2align 3
.L12:
	xorl	%edx, %edx
	testb	%al, %al
	setle	%dl
	cmpl	%eax, %edx
	je	.L16
	addl	$1, %eax
	movl	%eax, b(%rip)
	cmpl	$3, %eax
	jne	.L12
.L13:
	xorl	%eax, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L16:
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
.L14:
	call	foo
	movl	b(%rip), %eax
	addl	$1, %eax
	movl	%eax, b(%rip)
	cmpl	$2, %eax
	ja	.L7
.L2:
	xorl	%edx, %edx
	testb	%al, %al
	setle	%dl
	cmpl	%eax, %edx
	je	.L14
	addl	$1, %eax
	movl	%eax, b(%rip)
	cmpl	$3, %eax
	jne	.L2
.L7:
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc


gcc-trunk -v
Using built-in specs.
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211116 (experimental) (GCC)

Started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=7d6979197274a662da7bdc564314afe8415865c1


---


### compiler : `gcc`
### title : `[11 Regression] #pragma gcc visibility is lost for external decls declared inside a function`
### open_at : `2021-11-16T22:01:18Z`
### last_modified_date : `2022-03-28T19:19:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103291
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.2.1`
### severity : `normal`
### contents :
GCC 11 has a regression with:

```
#pragma GCC visibility push(hidden)                                             
                                                                                
int hidden_fetch(void) {                                                        
  extern const int hidden_global;                                               
  return hidden_global;                                                         
}
```

when compiled with -fpic. GCC 10 would always avoid the GOT for this case. GCC 11 avoids the GOT when it's compiled as C, but uses the GOT when it's compiled as C++. Moving the extern decl outside the function makes it dtrt again in C++ as well.

Reproduced on trunk at 4cdf7db9a39d18bd536d816a5751d4d3cf23808b and on 11 branch at b52e2254b30445f3cd667ae0f0d99b183394e37b.


---


### compiler : `gcc`
### title : `PowerPC: Gimple folding of int128 comparisons produces suboptimal code`
### open_at : `2021-11-18T15:59:17Z`
### last_modified_date : `2022-06-16T09:02:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103316
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
While working on replacing the old builtins infrastructure for the rs6000 back end, I observed that we generate poor code for 128-bit comparisons if we enable gimple folding on such builtins.  These included (old internal names):

  P10V_BUILTIN_VCMPEQUT
  P10V_BUILTIN_CMPNET
  P10V_BUILTIN_CMPGE_1TI
  P10V_BUILTIN_CMPGE_U1TI
  P10V_BUILTIN_VCMPGTUT
  P10V_BUILTIN_VCMPGTST
  P10V_BUILTIN_CMPLE_1TI
  P10V_BUILTIN_CMPLE_U1TI

I disabled gimple folding for these with the new support, but we need to look into why we generated bad code here.  I suspect it's something as simple as missing optab entries.

This isn't a regression, just a potential improvement.


---


### compiler : `gcc`
### title : `1 << -1 is never reduced to a constant during gimple`
### open_at : `2021-11-19T01:35:51Z`
### last_modified_date : `2023-08-21T23:18:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103325
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int main() {
  return 1 >> (-1);
}

GCC is the only compiler I have tried (MSVC, ICC and clang/LLVM) which does not remove the shift.
Yes this is undefined behavior but really I think it is best to reduce to something rather than keeping around the shift. Even converting it to __builtin_unreachable will be ok in my book.


---


### compiler : `gcc`
### title : `mulshift does not work when divisor is larger than 100 on 32 bits target.`
### open_at : `2021-11-21T10:14:11Z`
### last_modified_date : `2021-11-22T09:18:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103344
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
https://godbolt.org/z/x3zx738he


---


### compiler : `gcc`
### title : `missed optimization: add/xor individual bytes to form a word`
### open_at : `2021-11-21T11:17:03Z`
### last_modified_date : `2021-11-30T10:37:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103345
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
All code generated with godbolt's idea of 'trunk'. See https://godbolt.org/z/Wcj61PKKG

Source:

#include <stdint.h>

uint32_t load_le_32_or(const uint8_t *ptr)
{
  return ((uint32_t)ptr[0]) | ((uint32_t)ptr[1] << 8) | ((uint32_t)ptr[2] << 16) | ((uint32_t)ptr[3] << 24);
}

uint32_t load_le_32_add(const uint8_t *ptr)
{
  return ((uint32_t)ptr[0]) + ((uint32_t)ptr[1] << 8) + ((uint32_t)ptr[2] << 16) + ((uint32_t)ptr[3] << 24);
}


uint32_t load_le_32_xor(const uint8_t *ptr)
{
  return ((uint32_t)ptr[0]) ^ ((uint32_t)ptr[1] << 8) ^ ((uint32_t)ptr[2] << 16) ^ ((uint32_t)ptr[3] << 24);
}

The ^ version is admittedly a bit of an odd choice but the + version is a reasonably natural way to write the code.


Code on gcc -O2:

load_le_32_or:
        mov     eax, DWORD PTR [rdi]
        ret
load_le_32_add:
        movzx   eax, BYTE PTR [rdi+1]
        movzx   edx, BYTE PTR [rdi+2]
        sal     eax, 8
        sal     edx, 16
        add     eax, edx
        movzx   edx, BYTE PTR [rdi]
        add     eax, edx
        movzx   edx, BYTE PTR [rdi+3]
        sal     edx, 24
        add     eax, edx
        ret
load_le_32_xor:
        movzx   eax, BYTE PTR [rdi+1]
        movzx   edx, BYTE PTR [rdi+2]
        sal     eax, 8
        sal     edx, 16
        xor     eax, edx
        movzx   edx, BYTE PTR [rdi]
        xor     eax, edx
        movzx   edx, BYTE PTR [rdi+3]
        sal     edx, 24
        xor     eax, edx
        ret


Code on clang -O2:

load_le_32_or:                          # @load_le_32_or
        mov     eax, dword ptr [rdi]
        ret
load_le_32_add:                         # @load_le_32_add
        mov     eax, dword ptr [rdi]
        ret
load_le_32_xor:                         # @load_le_32_xor
        mov     eax, dword ptr [rdi]
        ret


---


### compiler : `gcc`
### title : `missed optimization with & and | and compares`
### open_at : `2021-11-22T02:17:19Z`
### last_modified_date : `2023-03-13T17:53:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103354
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int h(int a, int b, int c, int d)
{
  return (c & -(a==b)) | (d & -(a!=b));
}

---- CUT ---
We currently don't optimize the above down to just:
int h(int a, int b, int c, int d)
{
  return (a==b) ? c : d;
}

This is from https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92342#c4


---


### compiler : `gcc`
### title : `bool0 == ~bool1 should simplify to bool1 ^ bool0`
### open_at : `2021-11-22T11:06:06Z`
### last_modified_date : `2022-11-26T18:01:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103356
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f(bool a, bool b)
{
  b = !b;
  return a==b;
}

---- CUT ---
LLVM produces:
        mov     eax, edi
        xor     eax, esi
        ret

While GCC produces:
        xor     esi, 1
        cmp     sil, dil
        sete    al
        ret

GCC should be able to produce the xor case.


---


### compiler : `gcc`
### title : `[12 Regression] Dead Code Elimination Regression at -O3 (trunk vs 11.2.0)`
### open_at : `2021-11-22T15:34:16Z`
### last_modified_date : `2021-11-25T13:45:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103359
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat case.c
void foo();
static char a, c;
static int d, e;
static short b(short f, short g) { return f * g; }
int main() {
  short h = 4;
  for (; d;)
    if (h)
      if(e) {
        if (!b(a & 1 | h, 3))
          c = 0;
        h = 1;
      }
  if (c)
    foo();
}

trunk cannot eliminate the call to foo but 11.2.0 can:

gcc-11.2.0 -O3 -S -o /dev/stdout case.c
main:
.LFB1:
	.cfi_startproc
	xorl	%eax, %eax
	ret
	.cfi_endproc


gcc-trunk -O3 -S -o /dev/stdout case.c
main:
.LFB1:
	.cfi_startproc
	cmpb	$0, c(%rip)
	jne	.L8
	xorl	%eax, %eax
	ret
.L8:
	pushq	%rax
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc

gcc-trunk -v
Using built-in specs.
Target: x86_64-pc-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211122 (experimental) (GCC)

Started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=408579c9c9b8fee20e1d8114489ce2b93872767c


---


### compiler : `gcc`
### title : `[12/13/14 Regression] missed optimization for dead code elimination at -O3 (trunk vs 11.2.0)`
### open_at : `2021-11-23T18:21:17Z`
### last_modified_date : `2023-05-08T12:23:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103388
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
cat case.c
void foo(void);
void bar(void);

static int b, d, e, *c, *f = &d, *h = &b;

int main() {
  int **i = &c;
  if (e) {
    e = *f;
    bar();
    if (!(((i && d) + *h >= 1 ^ d & b) <= 4 | d))
      foo();
  }
}

trunk cannot eliminate the call to foo but 11.2.0 can:

gcc-11.2.0 -O3 -S -o /dev/stdout case.c
main:
.LFB0:
	.cfi_startproc
	movl	e(%rip), %ecx
	testl	%ecx, %ecx
	jne	.L8
	xorl	%eax, %eax
	ret
.L8:
	pushq	%rax
	.cfi_def_cfa_offset 16
	movl	d(%rip), %eax
	movl	%eax, e(%rip)
	call	bar
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:

gcc-trunk -O3 -S -o /dev/stdout case.c
main:
.LFB0:
	.cfi_startproc
	movl	e(%rip), %esi
	testl	%esi, %esi
	jne	.L10
	xorl	%eax, %eax
	ret
.L10:
	pushq	%rcx
	.cfi_def_cfa_offset 16
	movl	d(%rip), %eax
	movl	%eax, e(%rip)
	call	bar
	movl	d(%rip), %edx
	movl	b(%rip), %eax
	cmpl	$1, %edx
	movl	%eax, %ecx
	sbbl	$-1, %ecx
	testl	%ecx, %ecx
	setg	%cl
	andl	%edx, %eax
	movzbl	%cl, %ecx
	xorl	%ecx, %eax
	cmpl	$4, %eax
	setle	%al
	movzbl	%al, %eax
	orl	%edx, %eax
	je	.L11
.L3:
	xorl	%eax, %eax
	popq	%rdx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L11:
	.cfi_restore_state
	call	foo
	jmp	.L3
	.cfi_endproc
.LFE0:

gcc-trunk -v
Using built-in specs.
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211123 (experimental) (GCC)

Started with https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=4b3a325f07acebf47e82de227ce1d5ba62f5bcae


---


### compiler : `gcc`
### title : `[12 Regression] Generating 256bit register usage with -mprefer-avx128 -mprefer-vector-width=128`
### open_at : `2021-11-23T19:20:47Z`
### last_modified_date : `2022-03-31T07:31:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103393
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `normal`
### contents :
gcc -v
Using built-in specs.
COLLECT_GCC=/gcc_build/bin/gcc
COLLECT_LTO_WRAPPER=/gcc_build/bin/../libexec/gcc/x86_64-pc-linux-gnu/12.0.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../configure --prefix=/gcc_build --include=/gcc_build/include --disable-multilib --enable-rpath --enable-__cxa_atexit --enable-nls --disable-checking --disable-libunwind-exceptions --enable-bootstrap --enable-shared --enable-static --enable-threads=posix --with-gcc --with-gnu-as --with-gnu-ld --with-system-zlib --enable-languages=c,c++,fortran,go,objc,obj-c++ --enable-lto --enable-stage1-languages=c
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 12.0.0 20211123 (experimental) (GCC)

Branch: trunk, w/ a latest commit of 721d8b9e26bf8205c1f2125c2626919a408cdbe4

===========
=TEST CODE=
===========
# cat test.cpp
struct TestData {
  float arr[8];
};
void cpy( TestData& s1, TestData& s2 ) {
  for(int i=0; i<8; ++i) {
    s1.arr[i] = s2.arr[i];
  }
}

===========
=cmd      =
===========
gcc -S -masm=intel -O2 -mavx -mprefer-avx128 -mprefer-vector-width=128 -Wall -Wextra test.cpp -o test.s

===========
=BAD ASM  =
= GCC 12  =
===========
cat test.s
        .file   "test.cpp"
        .intel_syntax noprefix
        .text
        .p2align 4
        .globl  _Z3cpyR8TestDataS0_
        .type   _Z3cpyR8TestDataS0_, @function
_Z3cpyR8TestDataS0_:
.LFB0:
        .cfi_startproc
        vmovdqu ymm0, YMMWORD PTR [rsi]
        vmovdqu YMMWORD PTR [rdi], ymm0
        vzeroupper
        ret
        .cfi_endproc
.LFE0:
        .size   _Z3cpyR8TestDataS0_, .-_Z3cpyR8TestDataS0_
        .ident  "GCC: (GNU) 12.0.0 20211123 (experimental)"
        .section        .note.GNU-stack,"",@progbits

===========
= GCC 11  = (GCC 10 generates identical asm)
===========
cat test.s
        .file   "test.cpp"
        .intel_syntax noprefix
        .text
        .p2align 4
        .globl  _Z3cpyR8TestDataS0_
        .type   _Z3cpyR8TestDataS0_, @function
_Z3cpyR8TestDataS0_:
.LFB0:
        .cfi_startproc
        mov     edx, 32
        jmp     memmove
        .cfi_endproc
.LFE0:
        .size   _Z3cpyR8TestDataS0_, .-_Z3cpyR8TestDataS0_
        .ident  "GCC: (GNU) 11.2.0"
        .section        .note.GNU-stack,"",@progbits

=========
= GCC 9 =
=========
cat test.s
        .file   "test.cpp"
        .intel_syntax noprefix
        .text
        .p2align 4
        .globl  _Z3cpyR8TestDataS0_
        .type   _Z3cpyR8TestDataS0_, @function
_Z3cpyR8TestDataS0_:
.LFB0:
        .cfi_startproc
        xor     eax, eax
        .p2align 4,,10
        .p2align 3
.L2:
        vmovss  xmm0, DWORD PTR [rsi+rax]
        vmovss  DWORD PTR [rdi+rax], xmm0
        add     rax, 4
        cmp     rax, 32
        jne     .L2
        ret
        .cfi_endproc
.LFE0:
        .size   _Z3cpyR8TestDataS0_, .-_Z3cpyR8TestDataS0_
        .ident  "GCC: (GNU) 9.3.0"
        .section        .note.GNU-stack,"",@progbits




The auto vectorizer is generating YMM / 256-bit vector instructions with -mprefer-avx128 and -mprefer-vector-width=128 flags specified.  This is an issue for low latency software. Using registers 256-bit and wider causes jitter CPU problems on sky lake / cascade lake / ice lake chips.  This is true even in cases where the instructions used are considered avx256-light instructions due to a "mix of instructions" being used to determine the power levels (this is also mentioned in intel's optimization manual).

Auto vectorizer needs to respect the prefer width flags.  Enabling/using newer instruction sets i.e. AVX/AVX2/AVX512 does not require usage of the wider register types.


---


### compiler : `gcc`
### title : `[12 Regression] 48% tramp3d regression between g:df1a0d526e2e4c75 and g:9e026da720091704 with  -Ofast -march=native at Zen`
### open_at : `2021-11-25T10:54:25Z`
### last_modified_date : `2021-12-08T13:56:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103425
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
visible at https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=171.576.0
and with LTO https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=178.576.0

It lasts for 2 runs already so probably not a noise.  Curiously I do not see it on other testers (perhaps they need to run yet)


---


### compiler : `gcc`
### title : `Alignment of C++ references and 'this' pointer not used by optimizer`
### open_at : `2021-11-25T12:51:31Z`
### last_modified_date : `2022-10-25T17:18:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103427
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <stdint.h>

void* f(int& i)
{
    // should be no-op, &i is aligned to alignof(int)
    return (void*)((((uintptr_t)&i) + 3) & ~3);
}

struct My
{
    int value;

    void* Test1();
    void* Test2();
};

void* My::Test1()
{
    return this;
}

void* My::Test2()
{
    // should be no-op, 'this' is aligned to alignof(My)
    return (void*)((((uintptr_t)this) + 3) & ~3);
}


GCC fails to optimize away the redundant arithmetic to "fix" the alignment:

f(int&):
        lea     rax, [rdi+3]
        and     rax, -4
        ret
My::Test1():
        mov     rax, rdi
        ret
My::Test2():
        lea     rax, [rdi+3]
        and     rax, -4
        ret

Since https://reviews.llvm.org/D99790 Clang optimizes it:





Although an int* might not actually point to a valid int, and so could be misaligned, and int& must be bound to a valid object, which means it cannot be misaligned. It would be undefined to bind a reference to a misaligned object.

Similarly, although a My* could contain an arbitrary address, inside a member function the 'this' pointer must point to a valid object, which cannot be misaligned. It would be undefined to 

Pragmas and attributes and -fpack-struct=n can break those rules, but by default we should be able to assume they're true.

This will break some code which has undefined behaviour (and would be diagnosed by -fsanitize=alignment) but currently "works"  e.g. https://github.com/dotnet/runtime/issues/61671 was caused by the Clang change. So maybe there should be a specific flag to enable/disable this optimization, like we do for -fdelete-null-pointer-checks etc.


---


### compiler : `gcc`
### title : `Optimization of Auto-generated condition chain is not giving good lookup tables.`
### open_at : `2021-11-25T14:55:45Z`
### last_modified_date : `2023-07-27T09:22:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103429
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
I've got come generated condition chains (using recursive templates) and am getting some odd/suboptimal optimization results. Code is provided below and with a godbolt link.

In the first case (without a force inline), the compiler inlines the functions but does not perform condition chain optimization. In the second case (identical code but with force inline), it will optimize condition chains but only with exactly 5 elements. Otherwise it will end up with an if-else structure indexing optimized 5 element condition chains, and an if-else chain for anything spare.

It only attempts the optimization from gcc 11 onwards, I checked on trunk too.


Example:
https://godbolt.org/z/c9xbPqq7r

Here's the code:
template<int I> void f();

constexpr int N=5;

template<int I=0> 
static inline void f_dispatch(int i){
    if constexpr (I == N)
        return;
    else if(i == I)
        f<I>();
    else
        f_dispatch<I+1>(i);
}

template<int I=0> __attribute__((always_inline)) 
static inline void f_dispatch_always_inline(int i){
    if constexpr (I == N)
        return;
    else if(i == I)
        f<I>();
    else
        f_dispatch_always_inline<I+1>(i);
}

void run(int i){
    f_dispatch<>(i);
}

void run_inline(int i){
    f_dispatch_always_inline<>(i);
}


---


### compiler : `gcc`
### title : `boolean operations on bit-fields are not merged`
### open_at : `2021-11-28T19:58:59Z`
### last_modified_date : `2023-07-27T09:22:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103457
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
~~~c
#include <stdbool.h>

typedef struct GNodeFlagsS {
	bool remake:1;
	bool childMade:1;
	bool force:1;
	bool doneWait:1;
	bool doneOrder:1;
	bool fromDepend:1;
	bool doneAllsrc:1;
	bool cycle:1;
	bool doneCycle:1;
} GNodeFlags;

bool
GNodeFlags_IsNone(GNodeFlags flags)
{
	return !flags.remake
	       && !flags.childMade
	       && !flags.force
	       && !flags.doneWait
	       && !flags.doneOrder
	       && !flags.fromDepend
	       && !flags.doneAllsrc
	       && !flags.cycle
	       && !flags.doneCycle;
}
~~~

On x86_64, GCC 11.2 generates:

~~~asm
GNodeFlags_IsNone(GNodeFlagsS):
        mov     eax, edi
        and     eax, 1
        jne     .L6
        test    dil, 2
        jne     .L1
        mov     eax, edi
        shr     ax, 2
        and     eax, 1
        jne     .L6
        test    dil, 8
        jne     .L1
        mov     eax, edi
        shr     ax, 4
        and     eax, 1
        jne     .L6
        test    dil, 32
        jne     .L1
        mov     eax, edi
        shr     ax, 6
        and     eax, 1
        jne     .L6
        test    dil, dil
        js      .L1
        shr     di, 8
        mov     eax, edi
        and     eax, 1
        xor     eax, 1
        ret
.L6:
        xor     eax, eax
.L1:
        ret
~~~

ICC 2021.3.0 generates shorter code:

~~~asm
        test      edi, 1                                        #18.10
        jne       ..B1.10       # Prob 50%                      #18.10
        test      edi, 2                                        #19.13
        jne       ..B1.10       # Prob 50%                      #19.13
        test      edi, 4                                        #20.13
        jne       ..B1.10       # Prob 50%                      #20.13
(and so on)
~~~

Many other compilers fail to see the potential for optimizing this code as well.

Clang is better, it generates:

~~~asm
GNodeFlags_IsNone(GNodeFlagsS):     # @GNodeFlags_IsNone(GNodeFlagsS)
        test    edi, 511
        sete    al
        ret
~~~


---


### compiler : `gcc`
### title : `GCC failed to reduce bit clear in loop.`
### open_at : `2021-11-29T05:15:19Z`
### last_modified_date : `2022-05-26T08:28:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103462
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
the testcase is from pr47769

unsigned long cfunc_one(unsigned long tmp) {
    for (unsigned long bit = 0; bit < 64; bit += 3) {
        tmp &= ~(1UL << bit);
    }
    return tmp;
}

with -O3 -march=skylake -funroll-loops
gcc generates:
cfunc_one:
        mov     rax, rdi
        xor     edx, edx
.L2:
        lea     rcx, [rdx+3]
        btr     rax, rdx
        lea     rsi, [rdx+6]
        btr     rax, rcx
        lea     rdi, [rdx+9]
        btr     rax, rsi
        btr     rax, rdi
        lea     r8, [rdx+12]
        lea     r9, [rdx+15]
        btr     rax, r8
        lea     r10, [rdx+18]
        btr     rax, r9
        lea     r11, [rdx+21]
        btr     rax, r10
        lea     rcx, [rdx+24]
        btr     rax, r11
        lea     rsi, [rdx+27]
        btr     rax, rcx
        lea     rdi, [rdx+30]
        btr     rax, rsi
        add     rdx, 33
        btr     rax, rdi
        cmp     rdx, 66
        jne     .L2
        ret

while clang generates:

cfunc_one(unsigned long):                          # @cfunc_one(unsigned long)
        movabs  rax, 7905747460161236406
        and     rax, rdi
        ret

7905747460161236406 is bit clear for bit {0, 3, 6, 9, ..., 63}.


---


### compiler : `gcc`
### title : `Unnecessary ANDI instruction is generated for MIPS target`
### open_at : `2021-11-30T02:02:06Z`
### last_modified_date : `2021-11-30T07:06:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103482
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.3.0`
### severity : `normal`
### contents :
Created attachment 51903
Generated assembly file

There is unnecessary (useless) instruction ADDI is generated after the LHU instruction.

GCC configuration and version:
Target: mipsisa32-elf
Configured with: ../configure --enable-languages=c,c++ --target=mipsisa32-elf ----with-float=soft --with-newlib --with-gnu-as --with-gnu-ld --disable-shared --disable-libssp
Thread model: single
Supported LTO compression algorithms: zlib
gcc version 10.3.0 20210408 (GCC) 

Command line:
$ mipsisa32-elf-gcc -Os -fcommon -fomit-frame-pointer -ffunction-sections       -frename-registers -fweb -fno-builtin -c -o andi.o andi.c -v -save-temps -msoft-float


Source file addi.c:

unsigned short cF;
unsigned short tC;

unsigned int  func(void)
{
  unsigned int totCell;

  unsigned short cellFill = cF;
  totCell = tC; 
  totCell = ((totCell & 0xFF) << 8) | (totCell >> 8);
  return totCell*cellFill;
}

Disassembled object file andi.o:
andi.o:     file format elf32-bigmips


Disassembly of section .text.func:

00000000 <func>:
   0:	97820000 	lhu	v0,0(gp)
   4:	97880000 	lhu	t0,0(gp)
   8:	3044ffff 	andi	a0,v0,0xffff ## unnecessary clearing upper bits
                                             ## lhu does not do sign extension 
   c:	00021a00 	sll	v1,v0,0x8
  10:	00042a02 	srl	a1,a0,0x8
  14:	00653025 	or	a2,v1,a1
  18:	30c7ffff 	andi	a3,a2,0xffff
  1c:	03e00008 	jr	ra
  20:	70e81002 	mul	v0,a3,t0


---


### compiler : `gcc`
### title : `vectorizer bool pattern recog does not handle cycles correctly`
### open_at : `2021-11-30T13:13:40Z`
### last_modified_date : `2021-12-10T02:34:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103495
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
As can be seen in PR103489 bool pattern recog is defunct for cycle PHIs when we manage to visit the "wrong" def first.


---


### compiler : `gcc`
### title : `Spec 2017 imagick_r is 2.62% slower on Power10 with pc-relative addressing compared to not using pc-relative addressing`
### open_at : `2021-11-30T16:14:08Z`
### last_modified_date : `2023-06-02T04:40:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103498
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
I was doing some Spec 2017 rate runs on a single power10 little endian 64-bit CPU.  One of the runs disabled pc-relative addressing.  One benchmark (imagick_r) was faster if PC-relative addressing was disabled.

For this run I used the options:
    -DSPEC \
    -DNDEBUG \
    -I. \
    -DSPEC_AUTO_SUPPRESS_OPENMP \
    -g \
    -save-temps=obj \
    -Ofast \
    -mcpu=power10 \
    -mrecip \
    -funroll-loops \
    -msave-toc-indirect \
    -mno-pcrel \
    -fgnu89-inline \
    -Wno-multichar \
    -DSPEC_LP64 \
    -frandom-seed=spec2017


---


### compiler : `gcc`
### title : `((-1u >> t) & b) != 0 is not optimized to b != 0`
### open_at : `2021-11-30T23:35:00Z`
### last_modified_date : `2023-09-21T11:40:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103509
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
bool f1(bool a, int t)
{
    unsigned tt = -1;
    tt >>= t;
    return (a&tt) != 0;
}

bool f2(bool a, int t)
{
    unsigned tt = -1;
    tt >>= t;
    return (tt) != 0;
}
As shown by the above, we know that (-1u>>t)!=0 is always true Therefore we should be able to optimize f1 to just a!=0


---


### compiler : `gcc`
### title : `Missing XOR-EQ-AND Optimization`
### open_at : `2021-12-01T07:33:16Z`
### last_modified_date : `2022-01-31T09:32:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103514
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
We are not optimizing &&-^-== combination, LLVM does it [1]:

// Proof of correctness https://alive2.llvm.org/ce/z/a4tuWF
bool
src (bool a, bool b)
{
    return (a && b) == (a ^ b);
}

bool
tgt (bool a, bool b)
{
    return !(a || b);
}


// Proof of correctness https://alive2.llvm.org/ce/z/w-iotd
bool
src (bool a, bool b)
{
     return (a && b) ^ (a == b);
}

bool
tgt (bool a, bool b)
{
    return !(a || b);
}


I will be sending a patch for this. This will solve it, I have to run the testsuite and write a few tests:

/* (a && b) first_op (a second_op b) -> !(a || b) */
(for first_op (bit_xor eq)
 (for second_op (bit_xor eq)
 (simplify 
  (first_op:c (bit_and:c truth_valued_p@0 truth_valued_p@1) (second_op:c @0 @1))
   (if (first_op != second_op)
    (bit_not (bit_ior @0 @1))))))



1) https://compiler-explorer.com/z/WqTxYhG3s


---


### compiler : `gcc`
### title : `[12 regression] Spurious -Wstringop-overflow warning with std::string concatencation`
### open_at : `2021-12-02T19:47:55Z`
### last_modified_date : `2021-12-13T16:11:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103534
### status : `RESOLVED`
### tags : `alias, diagnostic, missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `normal`
### contents :
hi -

With a recent checkout of gcc12 (20211201) on a x86_64-pc-linux-gnu host,
compiling the following source with -O --std=c++20 gives a bogus
-Wstringop-overflow warning:

-- x.cc ---------------------------------------------------
#include <string>

std::string foo()
{
  return std::string("1234567890123456")  + std::string("");
}
-----------------------------------------------------------

$ g++ -c -O --std=c++20 x.cc
In file included from /usr/local/gcc/include/c++/12.0.0/string:40,
                 from x.cc:1:
In static member function ‘static constexpr std::char_traits<char>::char_type* std::char_traits<char>::copy(std::char_traits<char>::char_type*, const std::char_traits<char>::char_type*, std::size_t)’,
    inlined from ‘static constexpr void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_S_copy(_CharT*, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/local/gcc/include/c++/12.0.0/bits/basic_string.h:423:21,
    inlined from ‘constexpr std::__cxx11::basic_string<_CharT, _Traits, _Allocator>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_append(const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/local/gcc/include/c++/12.0.0/bits/basic_string.tcc:417:19,
    inlined from ‘constexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::append(const std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/local/gcc/include/c++/12.0.0/bits/basic_string.h:1385:25,
    inlined from ‘constexpr std::__cxx11::basic_string<_CharT, _Traits, _Allocator> std::operator+(std::__cxx11::basic_string<_CharT, _Traits, _Allocator>&&, std::__cxx11::basic_string<_CharT, _Traits, _Allocator>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/local/gcc/include/c++/12.0.0/bits/basic_string.h:3530:23,
    inlined from ‘std::string foo()’ at x.cc:5:59:
/usr/local/gcc/include/c++/12.0.0/bits/char_traits.h:426:56: warning: ‘void* __builtin_memcpy(void*, const void*, long unsigned int)’ specified bound between 18446744073709551600 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [-Wstringop-overflow=]
  426 |         return static_cast<char_type*>(__builtin_memcpy(__s1, __s2, __n));
      |                                        ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
$ 



Perhaps interestingly, the warning goes away if the first string literal
is less than 16 characters long.


---


### compiler : `gcc`
### title : `[missed optimization] remainder-of-2 with subtract-1`
### open_at : `2021-12-02T22:17:36Z`
### last_modified_date : `2021-12-02T22:40:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103535
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.1`
### severity : `enhancement`
### contents :
gcc version 11.2.1 20210816 [revision 056e324ce46a7924b5cf10f61010cf9dd2ca10e9] (SUSE Linux)  x86_64

== input ==
unsigned int fff(unsigned int p)
{
        // equivalent to  return p & ~1;
        if (p % 2 != 0)
                --p;
        return p;
}

== Observed output ==
» gcc -O2 -c x.cpp; objdump -Mintel -d x.o
   0:   89 fa                   mov    edx,edi
   2:   89 f8                   mov    eax,edi
   4:   83 e2 01                and    edx,0x1
   7:   83 fa 01                cmp    edx,0x1
   a:   83 d0 ff                adc    eax,0xffffffff
   d:   c3                      ret    

== Expected output ==
   0:   89 f8                   mov    eax,edi
   2:   83 e0 fe                and    eax,0xfffffffe
   5:   c3                      ret


---


### compiler : `gcc`
### title : `Suboptimal codegen for && and || combination.`
### open_at : `2021-12-03T01:37:50Z`
### last_modified_date : `2023-09-05T21:17:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103536
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
We do have suboptimal codegen for this pattern:

https://compiler-explorer.com/z/dszaK49WT

bool
src_1 (bool a, bool b)
{
    return (a || b) && (a && b);
}

bool
src_2 (bool a, bool b) // no problem here
{
    return (a && b) && (a || b);
}

bool
tgt (bool a, bool b) // what we want
{
    return (a && b);
}


In the case of src_2, it is the "ethread" pass which will remove the extra basicblock. But in case of src_1 since the operation is spread over multiple basicblocks, the match.pd is not able to detect and solve this.

1) In limited situations, it is possible to detect pattern in GENERIC and fix this:

As proof of concept, I tried with something like this and was able to fix this in GENERIC:
(for firstAnd (truth_and truth_andif)
 (for secondAnd (truth_and truth_andif)
  (for firstOr (truth_or truth_orif)
   (simplify
    (firstAnd (firstOr @0 @1) (secondAnd truth_valued_p@0 truth_valued_p@1))
    (truth_and @0 @1)))))

Because of GENERIC limitation, this is not going to work on something like this:
bool src_12 (bool a, bool b)
{
    int z = a || b;
    return z && (a && b);
}



2) A better approach which I haven't looked at, is not to spread this over multiple basic blocks.


---


### compiler : `gcc`
### title : `unnecessary spills around const functions calls`
### open_at : `2021-12-03T15:16:13Z`
### last_modified_date : `2023-02-09T21:48:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103541
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
While looking into reasons why modref causes some code size increases I noticed that we produce unnecessary spill on x86-64 here:

float a;

__attribute__((const)) float foo (float);

float
test()
{
        return a + foo(a) + a;
}

we load "a" into register and then spill it to stack because all SSE registers are clobbered by the call. This seems to happen somewhere between gcc 4.1 and 4.6.  It is caused by:

          /* We can combine a reg def from one insn into a reg use in
             another over a call if the memory is readonly or the call
             const/pure.  However, we can't set reg_equiv notes up for
             reload over any call.  The problem is the equivalent form
             may reference a pseudo which gets assigned a call
             clobbered hard reg.  When we later replace REG with its
             equivalent form, the value in the call-clobbered reg has
             been changed and all hell breaks loose.  */
          ret = valid_combine;
          if (!MEM_READONLY_P (memref)
              && !RTL_CONST_OR_PURE_CALL_P (insn))
            return valid_none;

in ira.c:validate_equiv_mem

If I read the comment correctly it is afraid of the address of memory reading being altered by the call (using call clobbered registers). But here it is a constant, so perhaps we can just rule this out when MEM rtx does not mention registers or does not mention any callee clobbered registers?


---


### compiler : `gcc`
### title : `[10/11/12 Regregression] bogus -Warray-bounds while index is limited by switch/case`
### open_at : `2021-12-03T15:33:43Z`
### last_modified_date : `2022-11-30T13:42:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103542
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `normal`
### contents :
gcc 11.2.0 reports the following on a reduced test case:

$ powerpc-linux-gcc -c array-bounds-fruit.c -O2 -Wall -Werror

array-bounds-fruit.c: In function 'get_default_config.part.0':
array-bounds-fruit.c:69:37: error: array subscript 4 is above array bounds of 'struct fruit_config[4]' [-Werror=array-bounds]
   69 |             do_something(id, &config[id].num_lemons);
      |                               ~~~~~~^~~~
array-bounds-fruit.c:19:28: note: while referencing 'config'
   19 | static struct fruit_config config[4];
      |                            ^~~~~~
cc1: all warnings being treated as errors


Above is for powerpc, but I have the same problem with ARM.

The offending line is inside a switch/case, within the block where 'id' is tested to be 0, 1, 2, or 3.
gcc/g++ is considering a case where 'id' becomes 4, which is not possible in this code.

If I make any more changes (even seemingly unrelated changes) to the test case, the error disappears.

Test code:

#include <stddef.h>
#include <stdbool.h>
#include <stdint.h>

enum {
    ID_0 = 0,
    ID_1 = 1,
    ID_2 = 2,
    ID_3 = 3,
    MAX_IDS,
};
#define MAX_ENTRIES 256

struct fruit_config {
    uint32_t num_apples;
    uint32_t num_lemons;
    uint32_t * lemons;
};
static struct fruit_config config[4];

static uint32_t unrelated_table[MAX_IDS][MAX_ENTRIES];

uint32_t do_something(const uint32_t id, uint32_t * number_of_entries)
{
    uint32_t error = 0;

    switch (id) {
        /* merging these case statements with identical body removes the issue */
        case ID_0: {
            *number_of_entries = 0;
            break;
        }
        case ID_1: {
            *number_of_entries = 0;
            break;
        }
        case ID_2: {
            *number_of_entries = 0;
            break;
        }
        case ID_3: {
            *number_of_entries = 0;
            break;
        }
        default: {
            error = 0xff;
            *number_of_entries = 0;
            break;
        }
    }
    return error;
}

struct fruit_config * get_default_config(const uint32_t id)
{
    switch (id) {
        case ID_0:
        case ID_1:
        case ID_2:
        case ID_3:
        {
            uint32_t entry = 0;
            for (entry = 0; entry <config[id].num_apples ; entry++) {
                   unrelated_table[0][0] = 0;
            }

            config[id].num_apples = 0;

            do_something(id, &config[id].num_lemons);

            /* removing following two lines removes the issue, even though
             * the error already occurs above */
            config[id].num_lemons = 0;
            config[id].lemons = NULL;

            /* removing use of error removes the issue */
            extern void foo(uint32_t arg);
            uint32_t error = 0;
            foo(error);

            break;
        }
        default: {
            break;
        }
    }

    return NULL;
}

void func_start(void)
{
    uint32_t i = 0;
    for (i = 0; i < MAX_IDS; i++) {
        get_default_config(i);
    }
}


---


### compiler : `gcc`
### title : `2 more instructions generated by gcc than clang`
### open_at : `2021-12-04T06:56:21Z`
### last_modified_date : `2022-10-06T06:47:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103550
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC
https://godbolt.org/z/Y5W3xfeao
clang
https://godbolt.org/z/8EW6v77PP

GCC generates 2 more instructions. why?


---


### compiler : `gcc`
### title : `-mavx generates worse code on scalar code`
### open_at : `2021-12-04T15:37:30Z`
### last_modified_date : `2021-12-09T03:07:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103554
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.1`
### severity : `normal`
### contents :
Test case:

struct s1 {
    long a, b, c, d, e, f, g, h;
};

s1 move(s1 in) {
    s1 ret;

    ret.a = in.d;
    ret.b = in.e;
    ret.c = in.a;
    ret.d = in.b;
    return ret;
}


-O3 generates:

move(s1):
  movq 8(%rsp), %xmm0
  movq 32(%rsp), %xmm1
  movq %rdi, %rax
  movhps 16(%rsp), %xmm0
  movhps 40(%rsp), %xmm1
  movups %xmm1, (%rdi)
  movups %xmm0, 16(%rdi)
  ret


-O3 -mavx generates:

move(s1):
        pushq   %rbp
        movq    %rdi, %rax
        movq    %rsp, %rbp
        vmovq   16(%rbp), %xmm2
        vmovq   40(%rbp), %xmm3
        vpinsrq $1, 24(%rbp), %xmm2, %xmm1
        vpinsrq $1, 48(%rbp), %xmm3, %xmm0
        vinsertf128     $0x1, %xmm1, %ymm0, %ymm0
        vmovdqu %ymm0, (%rdi)
        vzeroupper
        popq    %rbp
        ret

Clang -O3 generates this simple code, with or without -mavx (-mavx does use VEX instructions):

move(s1): # @move(s1)
  movq %rdi, %rax
  movups 32(%rsp), %xmm0
  movups %xmm0, (%rdi)
  movaps 8(%rsp), %xmm0
  movups %xmm0, 16(%rdi)
  retq


---


### compiler : `gcc`
### title : `Can't optimize away < 0 check on sqrt`
### open_at : `2021-12-04T20:58:21Z`
### last_modified_date : `2023-03-30T11:41:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103559
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
For a simple invocation of sqrt, gcc inserts a < 0 check to set math errno if needed. E.g.

float f(float x) {
    return sqrt(x);
}

Is generated as

f(float):
        vxorps  xmm1, xmm1, xmm1
        vucomiss        xmm1, xmm0
        ja      .L10
        vsqrtss xmm0, xmm0, xmm0
        ret
.L10:
        jmp     sqrtf


Unfortunately, this check is still present when the GCC is able to prove that x is non-negative:

float f(float x) {
    if(x < 0) [[unlikely]] {
        __builtin_unreachable();
    } else {
        return sqrt(x);
    }
}

LLVM suffers from the same problem, even with __builtin_assume(). https://godbolt.org/z/ddcoMj3oz

This is a very common pattern, and I'd imagine the argument for sqrt is often able to be shown to be positive. This would be a helpful enhancement.


---


### compiler : `gcc`
### title : `GCC emits more assembly than clang for carry flag`
### open_at : `2021-12-05T15:41:42Z`
### last_modified_date : `2023-06-02T14:24:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103565
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
I tried both examples. One emulation with the pattern. Another is with the x86 intrinsic. GCC emits more instructions than clang.

https://godbolt.org/z/d15WEY85T
https://godbolt.org/z/cob36P8nz

Also can GCC be able to understand the pattern of add_carry_pattern just like it understands std::rotl? 
https://github.com/gcc-mirror/gcc/blob/8d4ef2299cbf9517877dab60d48f34835758a6ee/libstdc%2B%2B-v3/include/std/bit#L135

template<typename T>
inline constexpr bool add_carry_pattern(bool carry,T a,T b,T& out) noexcept
{
	T temp{carry+a};
	out=temp+b;
	return (out < b) | (temp < a);
}

So we do not need that intrinsic anymore and the whole thing can be optimized at SSA level, rather than RTL level??


---


### compiler : `gcc`
### title : `sub-optimal vector construction with two loaded doubles on Power10`
### open_at : `2021-12-06T02:05:18Z`
### last_modified_date : `2022-03-08T16:20:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103568
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
For the test case:

vector double test(double *a, double *b) {
  return (vector double) { *a, *b };
}

On Power10, we generate the code like:

        ld 10,0(3)
        ld 9,0(4)
        mtvsrdd 34,9,10

As Power10 latency table, we can get better code with xxlor like:

        lxsd 0, 0(4)
        lxvrdx 1, 0, 3
        xxlor 34, 1, 32

As to the prerequisites "if we can assume the doubleword 1 of a vsx register after an lfd is zero", as Segher pointed out "ISA 3.1 section 7.1.1.1 says this already".

SPEC2017 510.parest_r may be one benchmark to evaluate the effect (with vectorization turned on).


---


### compiler : `gcc`
### title : `fatigue2 benchmarks on zen runs 43% faster with -fno-tree-vectorize -fno-tree-slp-vectorize`
### open_at : `2021-12-06T21:02:25Z`
### last_modified_date : `2023-07-15T19:50:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103592
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
While looking into -fno-inline-functions-called-once difference I noticed that on zen hardware I get:
 - 0m33s runtime for fatigue2 benchmark (from phoronix) when built with -Ofast -march=native -fno-slp-vectorize -fno-tree-vectorize
 - 0m57s for -Ofast -march=native binary


---


### compiler : `gcc`
### title : `GCC generates suboptimal code for SSE2/SSE4.1 64-bit integer element extraction on 32-bit x86 targets`
### open_at : `2021-12-08T00:55:07Z`
### last_modified_date : `2021-12-21T21:30:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103611
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.0`
### severity : `normal`
### contents :
Here is some code for extracting 64-bit integers from a SSE2 vector:
#include <cstdint>
#include <immintrin.h>

template<int ElemIdx>
std::int64_t SSE2ExtractInt64(__m128i vect) noexcept {
    static_assert(ElemIdx == (ElemIdx & 1), "ElemIdx must be between 0 and 1");
    
    __m128i vect2;
    if constexpr(ElemIdx == 0) {
        vect2 = _mm_shuffle_epi32(vect, 1);
    } else {
        vect2 = _mm_shuffle_epi32(vect, 3);
        vect = _mm_shuffle_epi32(vect, 2);
    }

    auto loVal = std::uint32_t(_mm_cvtsi128_si32(vect));
    auto hiVal = std::uint32_t(_mm_cvtsi128_si32(vect2));

    return std::int64_t(loVal) | std::int64_t(std::uint64_t(hiVal) << 32);
}

template std::int64_t SSE2ExtractInt64<0>(__m128i vect) noexcept;
template std::int64_t SSE2ExtractInt64<1>(__m128i vect) noexcept;

Here is the assembly code that is generated when the above C++ code is compiled with the -O2 -std=c++17 -march=nocona -mtune=skylake -m32 options:
_Z16SSE2ExtractInt64ILi0EExDv2_x:
        pushl   %ebx
        pshufd  $1, %xmm0, %xmm1
        xorl    %ebx, %ebx
        movd    %xmm1, %edx
        movd    %xmm0, %eax
        orl     %ebx, %edx
        orb     $0, %ah
        popl    %ebx
        ret
_Z16SSE2ExtractInt64ILi1EExDv2_x:
        pushl   %esi
        pshufd  $3, %xmm0, %xmm1
        xorl    %esi, %esi
        pushl   %ebx
        pshufd  $2, %xmm0, %xmm0
        movl    %esi, %edx
        movd    %xmm1, %ecx
        movd    %xmm0, %eax
        popl    %ebx
        orb     $0, %ah
        orl     %ecx, %edx
        popl    %esi
        ret

Here is a more optimal implementation of the above functions:
_Z16SSE2ExtractInt64ILi0EExDv2_x:
        pshufd  $1, %xmm0, %xmm1
        movd    %xmm1, %edx
        movd    %xmm0, %eax
        ret
_Z16SSE2ExtractInt64ILi1EExDv2_x:
        pshufd  $3, %xmm0, %xmm1
        pshufd  $2, %xmm0, %xmm0
        movd    %xmm1, %edx
        movd    %xmm0, %eax
        ret

Here is the code that is generated when the above C++ code is compiled with clang 13.0.0 with the -O2 -std=c++17 -march=nocona -mtune=skylake -m32 options:
_Z16SSE2ExtractInt64ILi0EExDv2_x:       # @_Z16SSE2ExtractInt64ILi0EExDv2_x
        movd    %xmm0, %eax
        pshufd  $85, %xmm0, %xmm0               # xmm0 = xmm0[1,1,1,1]
        movd    %xmm0, %edx
        retl
_Z16SSE2ExtractInt64ILi1EExDv2_x:       # @_Z16SSE2ExtractInt64ILi1EExDv2_x
        pshufd  $238, %xmm0, %xmm1              # xmm1 = xmm0[2,3,2,3]
        movd    %xmm1, %eax
        pshufd  $255, %xmm0, %xmm0              # xmm0 = xmm0[3,3,3,3]
        movd    %xmm0, %edx
        retl


---


### compiler : `gcc`
### title : `Modulo equality optimization`
### open_at : `2021-12-08T04:58:49Z`
### last_modified_date : `2021-12-11T02:38:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103614
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.2.0`
### severity : `normal`
### contents :
Code: https://gcc.godbolt.org/z/99s5fc695
Related: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82853

(signed)x%145==0, (unsigned)x%145==0, (unsigned)x%145==1 are all optimized into form x*c1+c2<c3, but (signed)x%145==1 isn't. (Intended to compile as (unsigned)(x-1)*266584177<14810233 )


---


### compiler : `gcc`
### title : `Missed popcount recognition`
### open_at : `2021-12-09T15:29:59Z`
### last_modified_date : `2022-10-10T18:10:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103633
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
For:

int f1 (unsigned long b) {
    int c = 0;
    while (b) {
        b &= b - 1;
        c++;
    }
    return c;
}

int f2(unsigned long x) {
    int c = 0;
    for (; x; x >>= 1) {
        if (x & 1)
            c++;
    }
    return c;
}

GCC recognizes f1 as popcount but not the more naive f2.


---


### compiler : `gcc`
### title : `Gimplifier does not remove empty struct stores in a few cases`
### open_at : `2021-12-10T10:43:22Z`
### last_modified_date : `2022-08-08T15:15:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103645
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization, patch`
### component : `middle-end`
### version : `12.0`
### severity : `minor`
### contents :
Take:
typedef struct { } spinlock_t;
void init_waitqueue_head(spinlock_t *q)
{
 q = (spinlock_t) { };
}
---- CUT -----

Most likely this is due to compound_literal_expr handling.

  *q = <<< Unknown tree: compound_literal_expr
    struct spinlock_t D.1982 = {}; >>>;


---


### compiler : `gcc`
### title : `constant array comparison not always folded`
### open_at : `2021-12-10T11:01:22Z`
### last_modified_date : `2022-01-04T09:24:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103647
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
On trunk and gcc 11, with -O2 the following functions are not optimized to 
xor eax, eax

------------
bool f() {
    char a[] = {'a', 'c'};
    char b[] = {'a', 'b'};
    return __builtin_memcmp(a, b, 2);
}

bool g() {
    char a[] = {'a', 'b'};
    char b[] = {'a', 'b', 'c'};
    return __builtin_memcmp(a, b, 2);
}
------------

.LC1:
        .string "ab"
.LC0:
        .string "ac"
f():
        movzx   eax, WORD PTR .LC1[rip]
        cmp     WORD PTR .LC0[rip], ax
        setne   al
        ret
g():
        cmp     WORD PTR .LC1[rip], 25185
        setne   al
        ret


---


### compiler : `gcc`
### title : `Missed optimization on arm64 when returning an empty struct.`
### open_at : `2021-12-10T12:12:49Z`
### last_modified_date : `2022-10-27T03:11:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103648
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.2.1`
### severity : `enhancement`
### contents :
In the following C code, gcc generates different assembly for `f` and `g`

```c
struct Void {};

struct Void f(int x) {
    return (struct Void) {};
}

void g(int x) {}
```

I believe it can be optimized somehow and clang does optimize them to be identical.

gcc trunk vs clang trunk on ARM 64-bit https://godbolt.org/z/W8ocj6vsK


Jonathan Wakely finds something that may be related.

https://reviews.llvm.org/D89490 which references
https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms#Pass-Arguments-to-Functions-Correctly


---


### compiler : `gcc`
### title : `Sub-optimal code with relational operators`
### open_at : `2021-12-11T13:14:51Z`
### last_modified_date : `2023-08-24T06:51:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103660
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
I recently looked at some of gcc's "if-conversions" and other optimisations of expressions involving relational operators - something people might use when trying to write branchless code.  I know that it is often best to write in the clearest way, including branches, and let the compiler handle the optimisation.  But people do try to do this kind of thing by hand.

I tested 6 examples of ways to write a simple "min" function:


int min1(int a, int b) {
    if (a < b) return a; else return b;
}

int min2(int a, int b) {
    return (a < b) ? a : b;
}

int min3(int a, int b) {
    return (a < b) * a | (a >= b) * b;
}

int min4(int a, int b) {
    return (a < b) * a + (a >= b) * b;
}

int min5(int a, int b) {
    const int c = a < b;
    return c * a + (1 - c) * b;
}

int min6(int a, int b) {
    const bool c = a < b;
    return c * a + !c * b;
}


gcc happily optimises the first two versions.  For the next two, it uses conditional moves for each half of the expression, then combines them with "or" or "add".  For version 5, it generates two multiply instructions, and version 6 is even worse in trunk (gcc 12).  This last one is a regression - gcc 11 generates the same code for version 6 as for version 4 (not optimal, but not as bad).

For comparison, clang 5+ generates optimal code for all versions.  I have tried a number of different targets (godbolt is wonderful for this stuff), with similar results.


---


### compiler : `gcc`
### title : `insert_trap in gimple-isolate-paths interferes badly with modref, pure-const and other optimizations`
### open_at : `2021-12-11T22:03:36Z`
### last_modified_date : `2023-09-21T18:38:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103665
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
When we detect load/store of NULL memory we produce:
  _9 ={v} MEM[(union tree_node *)0B].base.code;                                 
  __builtin_trap ();                                                            

The volatile memory store is intended to keep program segfaulting as it did before. However this representation is bad because it is now considered a significant side-effects.  This prevents such function from being discovered as pure/const and prevents other useful optimizations too


---


### compiler : `gcc`
### title : `Poor codegen for C++ casts`
### open_at : `2021-12-12T20:10:51Z`
### last_modified_date : `2021-12-14T09:42:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103674
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Compiling:

struct a {int a;};
struct b {int b;};
struct c : a, b {};
int
test(struct c *c)
{
        struct b *b=c;
        return b->b;
}

with -O2 leads to:

        .text
.LHOTB0:
        .p2align 4
        .globl  _Z4testP1c
        .type   _Z4testP1c, @function
_Z4testP1c:
.LFB0:
        .cfi_startproc
        testq   %rdi, %rdi
        je      .L2
        movl    4(%rdi), %eax
        ret
        .cfi_endproc
        .section        .text.unlikely
        .cfi_startproc
        .type   _Z4testP1c.cold, @function
_Z4testP1c.cold:
.LFSB0:
.L2:
        movl    0, %eax
        ud2
        .cfi_endproc
.LFE0:


while clang does:
_Z4testP1c:                             # @_Z4testP1c
        .cfi_startproc
# %bb.0:
        leaq    4(%rdi), %rax
        testq   %rdi, %rdi
        cmoveq  %rdi, %rax
        movl    (%rax), %eax
        retq

And so does gcc 4.8.5:

test(c*):
        leaq    4(%rdi), %rdx
        xorl    %eax, %eax
        testq   %rdi, %rdi
        cmovne  %rdx, %rax
        movl    (%rax), %eax
        ret

So this is technically a regression.


---


### compiler : `gcc`
### title : `gather is a loss for floats and win for doubles at zen3`
### open_at : `2021-12-12T20:20:04Z`
### last_modified_date : `2022-01-04T09:49:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103675
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Currently we disable gather on zen1/2 and enable it for zen3.  This was based on testing I did before Richard's code for open coding it.  Testing the only kernels using gather in zen3 with double I get:

jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=use_gather
jh@ryzen3:~/tsvc/src> ./a.out
Loop    Time(sec)       Checksum
 s352        7.529      1.644903
s4112        1.317      1127072.247160
s4114        1.377      32000.000684
s4115        1.250      1.038812
s4116        0.873      0.753265
s4117        1.283      32002.208699
s4121        0.583      196490.281734
jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=^use_gather
jh@ryzen3:~/tsvc/src> ./a.out
Loop    Time(sec)       Checksum
 s352        7.618      1.644903
s4112        1.223      1127072.247160
s4114        1.523      32000.000684
s4115        2.019      1.038812
s4116        1.531      0.753265
s4117        1.388      32002.208699
s4121        0.549      196490.281734

So it using gather is a win. For floats I get:

jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=use_gather ; ./a.out
Loop    Time(sec)       Checksum
 s352        7.419      1.644808
s4112        1.183      1127128.875000
s4114        1.198      32000.000000
s4115        1.056      1.038788
s4116        0.817      0.753265
s4117        1.179      32002.205078
s4121        0.264      196500.265625
jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=^use_gather ; ./a.out
Loop    Time(sec)       Checksum
 s352        7.435      1.644808
s4112        0.742      1127128.875000
s4114        1.078      32000.000000
s4115        0.673      1.038788
s4116        0.532      0.753264
s4117        1.044      32002.205078
s4121        0.266      196500.265625

so using gather is loss. For long I get:
jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=use_gather ; ./a.out
Loop    Time(sec)       Checksum
 s352        4.327      1.000000
s4112        1.576      132000.000000
s4114        1.603      32000.000000
s4115        1.453      0.000000
s4116        1.131      0.000000
s4117        1.432      32001.000000
s4121        0.629      132000.000000
jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=^use_gather ; ./a.out
Loop    Time(sec)       Checksum
 s352        4.532      1.000000
s4112        1.630      132000.000000
s4114        1.808      32000.000000
s4115        1.431      0.000000
s4116        1.130      0.000000
s4117        1.646      32001.000000
s4121        0.638      132000.000000
so gather is a win
For int I get:

jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=use_gather ; ./a.out
Loop    Time(sec)       Checksum
 s352        1.345      1.000000
s4112        1.057      132000.000000
s4114        1.095      32000.000000
s4115        1.055      0.000000
s4116        0.817      0.000000
s4117        1.022      32001.000000
s4121        0.266      132000.000000
jh@ryzen3:~/tsvc/src> ~/trunk-install/bin/gcc tsvc.c  -Ofast -march=native  common.c dummy.c -lm -mtune-ctrl=^use_gather ; ./a.out
Loop    Time(sec)       Checksum
 s352        1.345      1.000000
s4112        0.730      132000.000000
s4114        1.089      32000.000000
s4115        0.679      0.000000
s4116        0.528      0.000000
s4117        1.013      32001.000000
s4121        0.262      132000.000000

so gather is a loss. So i suppose we want to disable gathers for 32bit and smaller datatypes?


---


### compiler : `gcc`
### title : `Jump threading and switch corrupts profile`
### open_at : `2021-12-13T10:51:45Z`
### last_modified_date : `2023-07-01T07:11:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103680
### status : `NEW`
### tags : `internal-improvement, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Hi,
with -fprofile-report is it now again possible to produce quite useful stats on what passes misupdates profile.
I am attaching preport for FDO+LTO build of clang binary.  The column dynamic mismatch is most relevant and show
number of executions of BB where profile is mismatched.

Clearly threadfull, vrp, ch, dom and switchlower seems main offenders.
cddce is also very high, but I think it is just making previous mismatches worse - will look into that

It seems that the ranger threading revamp dropped profile updating code.
Andy, Jeff, I wonder what are your thoughts/plans on updating the profile here?

Profile consistency report:

Pass dump id and name            |static mismatch            |dynamic mismatch                                     |overall                                       |
                                 |in count     |out prob     |in count                  |out prob                  |size               |time                      |
 79i cp                          |   1245 +1245|     23   +23|    793393361   +793393361|            0             |  6865717          | 38369447330388             |
 83i inline                      |   2985 +1740|    204  +181|  10286436405  +9493043044|        15236       +15236| 10407224    +51.6%| 30175876351621       -21.4%|
 92t fixup_cfg                   |   2985      |    204      |  10286436405             |        15236             | 10407224          | 30175876351621             |
 97t ompdevlow                   |   2985      |    204      |  10286436405             |        15236             | 10407224          | 30175876351621             |
 99t adjust_alignment            |   2985      |    204      |  10286436405             |        15236             | 10407224          | 30175876351621             |
100t ccp                         |   3690  +705|    205    +1|  10489595202   +203158797|      1581960     +1566724| 10379522     -0.3%| 29996985353603        -0.6%|
101t objsz                       |   3690      |    205      |  10489595202             |      1581960             | 10379522          | 29996985353603             |
103t cunrolli                    |   3828  +138|    205      |  10056896600   -432698602|      1581960             | 10378081     -0.0%| 29950281172451        -0.2%|
104t backprop                    |   3828      |    205      |  10056896600             |      1581960             | 10378081          | 29950281172451             |
105t phiprop                     |   3828      |    205      |  10056896600             |      1581960             | 10387153     +0.1%| 29949755030810        -0.0%|
106t forwprop                    |   3843   +15|    205      |  10159477423   +102580823|      1581960             | 10378573     -0.1%| 29895061666317        -0.2%|
107t alias                       |   3843      |    205      |  10159477423             |      1581960             | 10378573          | 29895061666317             |
108t retslot                     |   3843      |    205      |  10159477423             |      1581960             | 10378573          | 29895061666317             |
109t fre                         |   5399 +1556|    204    -1|  19869551210  +9710073787|        15236     -1566724| 10134542     -2.4%| 28699013248678        -4.0%|
110t mergephi                    |   5278  -121|    204      |  20025371186   +155819976|        15236             | 10134542          | 28699013248678             |
111t threadfull                  |  72831 +67553|   1224 +1020| 202402568370 +182377197184|    972775135   +972759899| 10206940     +0.7%| 28026521150697        -2.3%|
112t vrp                         |  70137 -2694|   1276   +52| 216458254352 +14055685982|   1014740538    +41965403| 10113204     -0.9%| 27883751110042        -0.5%|
113t dse                         |  70069   -68|   1275    -1| 216259198094   -199056258|   1014740522          -16|  9976103     -1.4%| 27259758460246        -2.2%|
114t dce                         |  68646 -1423|   1251   -24| 213766589405  -2492608689|   1010738219     -4002303|  9925029     -0.5%| 26929885571404        -1.2%|
115t stdarg                      |  68646      |   1251      | 213766589405             |   1010738219             |  9925029          | 26929885571404             |
116t cdce                        |  68646      |   1251      | 213766589405             |   1010738219             |  9925029          | 26929885571404             |
117t cselim                      |  68623   -23|   1251      | 213766434213      -155192|   1010738219             |  9922625     -0.0%| 26933160470828        +0.0%|
118t copyprop                    |  68236  -387|   1240   -11| 212518014812  -1248419401|   1003793990     -6944229|  9921476     -0.0%| 26931328243255        -0.0%|
119t ifcombine                   |  68132  -104|   1185   -55| 215021849380  +2503834568|   1175015399   +171221409|  9923593     +0.0%| 27111703655114        +0.7%|
120t mergephi                    |  60635 -7497|   1183    -2| 213958226983  -1063622397|   1170269374     -4746025|  9923593          | 27111703655114             |
121t phiopt                      |  60064  -571|   1168   -15| 213278698412   -679528571|   1169780184      -489190|  9887693     -0.4%| 27065894430626        -0.2%|
122t tailr                       |  60077   +13|   1168      | 213591827870   +313129458|   1169780184             |  9885590     -0.0%| 27029704150703        -0.1%|
123t ch                          |  61604 +1527|   1166    -2| 231773845670 +18182017800|   1169779155        -1029|  9923814     +0.4%| 27005048401494        -0.1%|
124t cplxlower                   |  61604      |   1166      | 231773845670             |   1169779155             |  9923814          | 27005048401494             |
125t sra                         |  61604      |   1166      | 231773845670             |   1169779155             |  9836640     -0.9%| 26380634193283        -2.3%|
126t thread                      |  68862 +7258|   1419  +253| 282837813276 +51063967606|   1884527067   +714747912|  9867464     +0.3%| 26288555069118        -0.3%|
127t dom                         |  73213 +4351|   1439   +20| 289296055849  +6458242573|   1881062486     -3464580|  9851389     -0.2%| 25915894360837        -1.4%|
128t copyprop                    |  72922  -291|   1437    -2| 288791521291   -504534558|   1880452325      -610161|  9848087     -0.0%| 25909920082474        -0.0%|
129t isolate-paths               |  80793 +7871|   1437      | 289175243159   +383721868|   1887100849     +6648524|  9866571     +0.2%| 25908647817646        -0.0%|
130t reassoc                     |  81984 +1191|   1434    -3| 295644564738  +6469321579|   1886368237      -732612|  9879067     +0.1%| 25971893839783        +0.2%|
131t dce                         |  81377  -607|   1430    -4| 294937412703   -707152035|   1878493133     -7875104|  9801097     -0.8%| 25561486413957        -1.6%|
132t forwprop                    |  81666  +289|   1430      | 297646371403  +2708958700|   1878493133             |  9794576     -0.1%| 25528787255201        -0.1%|
133t phiopt                      |  81584   -82|   1430      | 297639291119     -7080284|   1878493133             |  9792840     -0.0%| 25528618873011        -0.0%|
134t ccp                         |  81588    +4|   1430      | 297550679459    -88611660|   1893125633    +14632500|  9791108     -0.0%| 25519558936471        -0.0%|
135t sincos                      |  81588      |   1430      | 297550679459             |   1893125633             |  9791108          | 25519558936471             |
136t bswap                       |  81588      |   1430      | 297550679459             |   1893125633             |  9791127     +0.0%| 25519558936471             |
137t laddress                    |  81588      |   1430      | 297550679459             |   1893125633             |  9793023     +0.0%| 25526884074520        +0.0%|
138t lim                         |  81648   +60|   1430      | 297859756610   +309077151|   1893125633             |  9794191     +0.0%| 25495948555139        -0.1%|
139t walloca                     |  81648      |   1430      | 297859756610             |   1893125633             |  9794191          | 25495948555139             |
140t pre                         |  80261 -1387|   1419   -11| 299154507133  +1294750523|   1885986610     -7139023|  9670301     -1.3%| 24860057746822        -2.5%|
141t sink                        |  80062  -199|   1416    -3| 299366883910   +212376777|   1855713098    -30273512|  9628335     -0.4%| 24691926355914        -0.7%|
145t dse                         |  80062      |   1416      | 299366883910             |   1855713098             |  9621313     -0.1%| 24673080303541        -0.1%|
146t dce                         |  79991   -71|   1416      | 299440495723    +73611813|   1855713098             |  9620600     -0.0%| 24670512443377        -0.0%|
147t fix_loops                   |  79991      |   1416      | 299440495723             |   1855713098             |  9620600          | 24670512443377             |
148t loop                        |  79991      |   1416      | 299440495723             |   1855713098             |  9620600          | 24670512443377             |
149t loopinit                    |  79991      |   1416      | 299440495723             |   1855713098             |  9620596     -0.0%| 24670508570371        -0.0%|
150t unswitch                    |  80230  +239|   1416      | 301091048942  +1650553219|   1855713098             |  9621927     +0.0%| 24664900640767        -0.0%|
151t sccp                        |  80229    -1|   1416      | 301090909602      -139340|   1855713098             |  9627674     +0.1%| 24665669129556        +0.0%|
152t lsplit                      |  80275   +46|   1416      | 301901055378   +810145776|   1855713098             |  9628013     +0.0%| 24669151374922        +0.0%|
153t lversion                    |  80275      |   1416      | 301901055378             |   1855713098             |  9628013          | 24669151374922             |
154t unrolljam                   |  80275      |   1416      | 301901055378             |   1855713098             |  9628013          | 24669151374922             |
155t cddce                       |  96307 +16032|   1405   -11| 400398657031 +98497601653|   1723477545   -132235553|  9615375     -0.1%| 24551469098820        -0.5%|
156t ivcanon                     |  96308    +1|   1405      | 400399330048      +673017|   1723477545             |  9617912     +0.0%| 24609543607312        +0.2%|
157t ldist                       |  96261   -47|   1530  +125| 396226006072  -4173323976|   2013657886   +290180341|  9617858     -0.0%| 24560695230994        -0.2%|
158t linterchange                |  96261      |   1530      | 396226006072             |   2013657886             |  9617858          | 24560695230994             |
159t copyprop                    |  96263    +2|   1530      | 396225913851       -92221|   2013657886             |  9617599     -0.0%| 24560616860061        -0.0%|
166t ompexpssa                   |  96263      |   1530      | 396225913851             |   2013657886             |  9617599          | 24560616860061             |
167t ch_vect                     |  97304 +1041|   1530      | 396265259912    +39346061|   2013657886             |  9617920     +0.0%| 24560139310683        -0.0%|
168t ifcvt                       |  97464  +160|   1738  +208| 399080908207  +2815648295|   3365644947  +1351987061|  9621367     +0.0%| 24646895835295        +0.4%|
169t vect                        |  97302  -162|   1530  -208| 395362097676  -3718810531|   2013657886  -1351987061|  9620671     -0.0%| 24574578374634        -0.3%|
170t dce                         |  97284   -18|   1530      | 395133116739   -228980937|   2013657886             |  9619530     -0.0%| 24561056957484        -0.1%|
171t pcom                        |  97284      |   1530      | 395133116739             |   2013657886             |  9619527     -0.0%| 24560026676163        -0.0%|
172t cunroll                     | 109977 +12693|   1636  +106| 478680255671 +83547138932|   1999297268    -14360618|  9849220     +2.4%| 24167289537557        -1.6%|
173t fre                         | 109849  -128|   1636      | 478355091001   -325164670|   1999297268             |  9823426     -0.3%| 24002006012531        -0.7%|
174t dse                         | 109849      |   1636      | 478355091001             |   1999297268             |  9822112     -0.0%| 23986782125423        -0.1%|
175t slp                         | 109849      |   1636      | 478355091001             |   1999297268             |  9748052     -0.8%| 23841228691052        -0.6%|
177t ivopts                      | 109849      |   1636      | 478355091001             |   1999297268             |  9754436     +0.1%| 23818230835963        -0.1%|
178t lim                         | 109849      |   1636      | 478355091001             |   1999297268             |  9754536     +0.0%| 23818105695337        -0.0%|
179t loopdone                    | 109726  -123|   1636      | 478267981458    -87109543|   1999297268             |  9754508     -0.0%| 23818061294635        -0.0%|
180t no_loop                     | 109726      |   1636      | 478267981458             |   1999297268             |  9754508          | 23818061294635             |
181t slp                         | 109726      |   1636      | 478267981458             |   1999297268             |  9718223     -0.4%| 23804224391421        -0.1%|
183t veclower2                   | 109575  -151|   1635    -1| 477128368553  -1139612905|   1999297226          -42|  9717499     -0.0%| 23803951872403        -0.0%|
184t switchlower                 | 216380 +106805|   1431  -204| 1380672969073 +903544600520|   1921470715    -77826511|  9714676     -0.0%| 25128620292826        +5.6%|
186t reassoc                     | 215688  -692|   1430    -1| 1380025901003   -647068070|   1921462330        -8385|  9725168     +0.1%| 25170465801635        +0.2%|
187t slsr                        | 215688      |   1430      | 1380025901003             |   1921462330             |  9725790     +0.0%| 25171051075791        +0.0%|
188t split-paths                 | 215690    +2|   1430      | 1380032502398     +6601395|   1921462330             |  9727714     +0.0%| 25171051342158        +0.0%|
189t tracer                      | 217826 +2136|   1458   +28| 1272463552867 -107568949531|   1909758769    -11703561|  9892396     +1.7%| 25179157448987        +0.0%|
190t fre                         | 218023  +197|   1456    -2| 1291101339405 +18637786538|   1905694277     -4064492|  9860835     -0.3%| 25066769644895        -0.4%|
191t thread                      | 224304 +6281|   2113  +657| 1260887440991 -30213898414|   3227986771  +1322292493|  9890836     +0.3%| 24869097162860        -0.8%|
192t dom                         | 231623 +7319|   2120    +7| 1265636026287  +4748585296|   3398605099   +170618329|  9891998     +0.0%| 24698836238947        -0.7%|
193t strlen                      | 231623      |   2120      | 1265636026287             |   3398605099             |  9883223     -0.1%| 24694963594820        -0.0%|
194t threadfull                  | 233644 +2021|   2218   +98| 1273228200834  +7592174547|   3495419473    +96814373|  9906384     +0.2%| 24638117243328        -0.2%|
195t vrp                         | 232054 -1590|   2294   +76| 1273076379738   -151821096|   4379683658   +884264185|  9878349     -0.3%| 24562986138635        -0.3%|
196t ccp                         | 231564  -490|   2289    -5| 1272303409980   -772969758|   4385826012     +6142354|  9877293     -0.0%| 24560983756866        -0.0%|
197t wrestrict                   | 231564      |   2289      | 1272303409980             |   4385826012             |  9877293          | 24560983756866             |
198t dse                         | 230745  -819|   2288    -1| 1272023199564   -280210416|   4385816013        -9999|  9810886     -0.7%| 24365376510779        -0.8%|
199t cddce                       | 241010 +10265|   2282    -6| 1369149644581 +97126445017|   4385012766      -803247|  9806927     -0.0%| 24352303495462        -0.1%|
200t forwprop                    | 241128  +118|   2277    -5| 1369582809624   +433165043|   4380754912     -4257854|  9788180     -0.2%| 24150309614008        -0.8%|
201t phiopt                      | 240767  -361|   2276    -1| 1369413694596   -169115028|   4371615103     -9139809|  9787199     -0.0%| 24147022235622        -0.0%|
202t fab                         | 238774 -1993|   2273    -3| 1356127299769 -13286394827|   4359973464    -11641639|  9776342     -0.1%| 24120633180272        -0.1%|
203t widening_mul                | 238774      |   2273      | 1356127299769             |   4359973464             |  9774585     -0.0%| 24117914735934        -0.0%|
204t sink                        | 237049 -1725|   2271    -2| 1352510559484  -3616740285|   4359971169        -2295|  9763920     -0.1%| 24054869732314        -0.3%|
205t store-merging               | 237049      |   2271      | 1352510559484             |   4359971169             |  9758244     -0.1%| 24015252481346        -0.2%|
206t tailc                       | 237045    -4|   2271      | 1352494254576    -16304908|   4359971169             |  9758192     -0.0%| 24015192437586        -0.0%|
207t dce                         | 235783 -1262|   2270    -1| 1350206240191  -2288014385|   4359971169             |  9751280     -0.1%| 23996865950908        -0.1%|
208t crited                      | 235783      |   2270      | 1350206240191             |   4359971169             |  9751280          | 23996865950908             |
209t uninit                      | 235783      |   2270      | 1350206240191             |   4359971169             |  9751280          | 23996865950908             |
210t local-pure-const            | 235783      |   2270      | 1350349863074   +143622883|   4359971169             |  9751280          | 23997783661558        +0.0%|
211t modref                      | 235783      |   2270      | 1350349863074             |   4359971169             |  9751280          | 23997783661558             |
212t uncprop                     | 235783      |   2270      | 1350349863074             |   4359971169             |  9751280          | 23997783661558             |
246t nrv                         | 235783      |   2270      | 1350349863074             |   4359971169             |  9751272     -0.0%| 23997783661558             |
247t isel                        | 235783      |   2270      | 1350349863074             |   4359971169             |  9751262     -0.0%| 23997641911017        -0.0%|
250t optimized                   | 230075 -5708|   2265    -5| 1350486491782   +136628708|   4380125028    +20153859|  9751194     -0.0%| 24087942045836        +0.4%|
251t waccess                     | 230075      |   2265      | 1350486491782             |   4380125028             |  9751194          | 24087942045836             |
252r expand                      | 219555 -10520|   1914  -351| 1300300159576 -50186332206|   3055506535  -1324618492|-------------------|--------------------------|
253r vregs                       | 219555      |   1914      | 1300300159576             |   3055506535             | 65793993     +0.0%| 158587648130835             |
254r into_cfglayout              | 215752 -3803|   1822   -92| 1300334045064    +33885488|   2856914371   -198592164| 65197156     -0.9%| 157473684195974        -0.7%|
255r jump                        | 215998  +246|   1826    +4| 1300487250854   +153205790|   2856927565       +13194| 65074406     -0.2%| 157201185251199        -0.2%|
256r subreg1                     | 215998      |   1826      | 1300487250854             |   2856927565             | 64661258     -0.6%| 156980226244131        -0.1%|
257r dfinit                      | 215998      |   1826      | 1300487250854             |   2856927565             | 64661258          | 156980226244131             |
258r cse1                        | 215922   -76|   1823    -3| 1300444468450    -42782404|   2856913789       -13776| 63667797     -1.5%| 156707311344200        -0.2%|
259r fwprop1                     | 215922      |   1823      | 1300444468450             |   2856913789             | 59414712     -6.7%| 151499311612714        -3.3%|
260r cprop                       | 217389 +1467|   1818    -5| 1302497316424  +2052847974|   2840122245    -16791544| 59226283     -0.3%| 150812689195162        -0.5%|
261r rtl pre                     | 217389      |   1818      | 1302497316424             |   2840122245             | 59622715     +0.7%| 151863962265772        +0.7%|
262r hoist                       | 217389      |   1818      | 1302497316424             |   2840122245             | 59755680     +0.2%| 151863967787842        +0.0%|
263r cprop                       | 217370   -19|   1817    -1| 1301012893039  -1484423385|   2839953333      -168912| 58928550     -1.4%| 150419774693093        -1.0%|
265r cse_local                   | 217370      |   1817      | 1301012893039             |   2839953333             | 58809547     -0.2%| 149902126430912        -0.3%|
266r ce1                         | 216528  -842|   1693  -124| 1297874035422  -3138857617|   2805149089    -34804244| 58601053     -0.4%| 149864991761443        -0.0%|
267r reginfo                     | 216528      |   1693      | 1297874035422             |   2805149089             | 58601053          | 149864991761443             |
268r loop2                       | 216528      |   1693      | 1297874035422             |   2805149089             | 58601053          | 149864991761443             |
269r loop2_init                  | 216528      |   1693      | 1297874035422             |   2805149089             | 58601053          | 149864991761443             |
270r loop2_invariant             | 216528      |   1693      | 1297874035422             |   2805149089             | 58809979     +0.4%| 150765280729437        +0.6%|
271r loop2_unroll                | 218657 +2129|   1693      | 1311747865069 +13873829647|   2805149089             | 59303324     +0.8%| 150013522697100        -0.5%|
273r loop2_done                  | 218275  -382|   1693      | 1317456278918  +5708413849|   2805149089             | 59303220     -0.0%| 150013718008790        +0.0%|
276r cprop                       | 219309 +1034|   1693      | 1334139229371 +16682950453|   2805149089             | 59015017     -0.5%| 148960435540453        -0.7%|
277r stv                         | 219309      |   1693      | 1334139229371             |   2805149089             | 59016692     +0.0%| 148964638599813        +0.0%|
278r cse2                        | 219303    -6|   1693      | 1334078649228    -60580143|   2805149089             | 58832341     -0.3%| 148449045444312        -0.3%|
279r dse1                        | 219303      |   1693      | 1334078649228             |   2805149089             | 58860273     +0.0%| 148453707235147        +0.0%|
280r fwprop2                     | 219272   -31|   1693      | 1333290149948   -788499280|   2805149089             | 58835261     -0.0%| 148462300686089        +0.0%|
282r init-regs                   | 219272      |   1693      | 1333290149948             |   2805149089             | 58926009     +0.2%| 148510272881097        +0.0%|
283r ud_dce                      | 219272      |   1693      | 1333290149948             |   2805149089             | 58889468     -0.1%| 148335390497275        -0.1%|
284r combine                     | 219352   +80|   1693      | 1333648234805   +358084857|   2805149089             | 58543412     -0.6%| 144713034986525        -2.4%|
286r stv                         | 219352      |   1693      | 1333648234805             |   2805149089             | 58543412          | 144713034986525             |
287r ce2                         | 219261   -91|   1692    -1| 1333465104885   -183129920|   2804833682      -315407| 58542853     -0.0%| 144717376160885        +0.0%|
288r jump_after_combine          | 220292 +1031|   1711   +19| 1334075792643   +610687758|   2813150500     +8316818| 58542297     -0.0%| 144708534014247        -0.0%|
289r bbpart                      | 220256   -36|   1706    -5| 1333936023873   -139768770|   2813142257        -8243| 58642109     +0.2%| 144695771569291        -0.0%|
290r outof_cfglayout             | 220256      |   1706      | 1333936023873             |   2813142257             | 59149195     +0.9%| 145394559874685        +0.5%|
291r split1                      | 220256      |   1706      | 1333936023873             |   2813142257             | 59197386     +0.1%| 145473058739988        +0.1%|
292r subreg3                     | 220256      |   1706      | 1333936023873             |   2813142257             | 59251600     +0.1%| 145488031064603        +0.0%|
294r mode_sw                     | 220256      |   1706      | 1333936023873             |   2813142257             | 59251600          | 145488031064603             |
295r asmcons                     | 220256      |   1706      | 1333936023873             |   2813142257             | 59251600          | 145488031064603             |
300r ira                         | 220256      |   1706      | 1333936023873             |   2813142257             | 61492017     +3.8%| 148246220021068        +1.9%|
301r reload                      | 220045  -211|   1705    -1| 1334581013551   +644989678|   2812625999      -516259| 57379814     -6.7%| 146213679526534        -1.4%|
303r postreload                  | 220045      |   1705      | 1334581013551             |   2812625999             | 57248415     -0.2%| 146114585033173        -0.1%|
304r gcse2                       | 220045      |   1705      | 1334581013551             |   2812625999             | 57248443     +0.0%| 146087407924926        -0.0%|
305r split2                      | 220045      |   1705      | 1334581013551             |   2812625999             | 57519980     +0.5%| 146648496543144        +0.4%|
306r ree                         | 220045      |   1705      | 1334581013551             |   2812625999             | 57550983     +0.1%| 146968013482552        +0.2%|
307r cmpelim                     | 220045      |   1705      | 1334581013551             |   2812625999             | 57507638     -0.1%| 146861143961269        -0.1%|
308r pro_and_epilogue            | 225303 +5258|   1709    +4| 1341177127695  +6596114144|   2812611798       -14201| 62704987     +9.0%| 162807859528298       +10.9%|
309r dse2                        | 225303      |   1709      | 1341177127695             |   2812611798             | 62701751     -0.0%| 162807307011532        -0.0%|
310r csa                         | 225303      |   1709      | 1341177127695             |   2812611798             | 62685811     -0.0%| 162802974126976        -0.0%|
311r jump2                       | 222011 -3292|   1700    -9| 1394955038853 +53777911158|   2811994770      -617028| 60591817     -3.3%| 162863551117381        +0.0%|
312r compgotos                   | 222011      |   1700      | 1394955038853             |   2811994770             | 60591817          | 162863551117381             |
314r peephole2                   | 222011      |   1700      | 1394955038853             |   2811994770             | 61026320     +0.7%| 163016193759849        +0.1%|
315r ce3                         | 221879  -132|   1700      | 1394975083423    +20044570|   2811994770             | 61000932     -0.0%| 163078660082367        +0.0%|
317r cprop_hardreg               | 221879      |   1700      | 1394975083423             |   2811994770             | 60690862     -0.5%| 162902791239877        -0.1%|
318r rtl_dce                     | 221879      |   1700      | 1394975083423             |   2811994770             | 60674443     -0.0%| 162847446231467        -0.0%|
319r bbro                        | 217835 -4044|   1700      | 1358424822396 -36550261027|   2811994765           -5| 60592640     -0.1%| 162149759759118        -0.4%|
320r split3                      | 217835      |   1700      | 1358424822396             |   2811994765             | 60593602     +0.0%| 162150528280896        +0.0%|
321r sched2                      | 217835      |   1700      | 1358424822396             |   2811994765             | 60598064     +0.0%| 162181461863043        +0.0%|
323r stack                       | 217835      |   1700      | 1358424822396             |   2811994765             | 60598064          | 162181461863043             |
324r zero_call_used_regs         | 217835      |   1700      | 1358424822396             |   2811994765             | 60598064          | 162181461863043             |
325r alignments                  | 217835      |   1700      | 1358424822396             |   2811994765             | 60598064          | 162181461863043             |


---


### compiler : `gcc`
### title : `Redundant !__builtin_isnan not removed with -fno-signaling-nans`
### open_at : `2021-12-13T12:51:36Z`
### last_modified_date : `2021-12-14T00:05:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103683
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `enhancement`
### contents :
If x is a NaN, then x > 1 will fail, so the __builtin_isnan function
could be removed in comparisons like (!__builtin_isnan (x)) && x > 1
lwhen -fno-signaling-nans is in effect.

Test case:

#include <math.h>
#include <stdio.h>

_Bool foo (double x)
{
  return (!__builtin_isnan (x)) && x > 1;
}

_Bool bar (double x)
{
  return x > 1;
}

void chk(double x)
{
  printf ("%f %x %x\n", x, foo(x), bar(x));
}
int main()
{
  chk(-1.0);
  chk(1.0);
  chk(2.9);
  chk(NAN);
  return 0;
}

With "gcc -fno-signaling-nans -S -O3 nan.c", one gets for foo

foo:
        .seh_endprologue
        ucomisd %xmm0, %xmm0
        jp      .L3
        comisd  .LC0(%rip), %xmm0
        seta    %al
        ret

and for bar

bar:
        .seh_endprologue
        comisd  .LC0(%rip), %xmm0
        seta    %al
        ret
        .seh_endproc


---


### compiler : `gcc`
### title : `Missed optimization: common bit prefix/suffixes of multiple values`
### open_at : `2021-12-14T15:33:00Z`
### last_modified_date : `2021-12-14T16:20:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103710
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.1`
### severity : `enhancement`
### contents :
Environment:
» ./xgcc -v ...
Reading specs from /home/jengelh/repos/gcc/host-x86_64-pc-linux-gnu/gcc/specs
COLLECT_GCC=./xgcc
Target: x86_64-pc-linux-gnu
Configured with: ./configure --enable-languages=c,c++
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.0.0 20211214 (experimental) (GCC) 
COLLECT_GCC_OPTIONS='-v' '-B' '/home/jengelh/repos/gcc/host-x86_64-pc-linux-gnu/gcc' '-D' 'noexcept=' '-c' '-O2' '-mtune=generic' '-march=x86-64'
GNU C17 (GCC) version 12.0.0 20211214 (experimental) (x86_64-pc-linux-gnu)
        compiled by GNU C version 11.2.1 20211124 [revision 7510c23c1ec53aa4a62705f0384079661342ff7b], GMP version 6.2.1, MPFR version 4.1.0-p7, MPC version 1.2.1, isl version none
GGC heuristics: --param ggc-min-expand=30 --param ggc-min-heapsize=4096


Input: attachment #0
Observed output:

$ ./xgcc -B. -x c++ -c -O2 t.cpp && readelf -a t.o | grep fct
     3: 0000000000000000    54 FUNC    GLOBAL DEFAULT    1 _Z4fct0j
     4: 0000000000000040    54 FUNC    GLOBAL DEFAULT    1 _Z4fct4j
     5: 0000000000000080    54 FUNC    GLOBAL DEFAULT    1 _Z4fct8j
     6: 00000000000000c0    54 FUNC    GLOBAL DEFAULT    1 _Z5fct12j
     7: 0000000000000100    54 FUNC    GLOBAL DEFAULT    1 _Z5fct16j
     8: 0000000000000140    54 FUNC    GLOBAL DEFAULT    1 _Z5fct20j
     9: 0000000000000180    38 FUNC    GLOBAL DEFAULT    1 _Z5fct24j
    10: 00000000000001b0    32 FUNC    GLOBAL DEFAULT    1 _Z5fct25j
    11: 00000000000001d0    35 FUNC    GLOBAL DEFAULT    1 _Z6fctmaxj

Desired output:

Provided I made no mistake in the equivalency of all fct functions, the fct functions should, preferably, all compile to the same machine code.


---


### compiler : `gcc`
### title : `zero extend not sunk out of the loop after IV-OPTS in the sink pass`
### open_at : `2021-12-15T02:54:40Z`
### last_modified_date : `2022-01-04T11:24:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103723
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
https://godbolt.org/z/b7x1s5Yrn

For this case, the instruction  `mov     ecx, eax` in bb `.L3` could remove from loop body and could sink to '.L8' and '.L1'.


After ivopt pass the tree dump is:

uint32_t test (uint32_t buf_avail, 
const uint8_t * buf, const uint8_t * buf_back)
{
  sizetype ivtmp.7;
  uint32_t len_test;
  unsigned char _3;
  sizetype _4;
  unsigned char _6;
  sizetype _7;
  unsigned int _15;
  unsigned int _16;
  unsigned int _17;

  <bb 2> [local count: 114863530]:
  if (buf_avail_8(D) > 2)
    goto <bb 6>; [94.50%]
  else
    goto <bb 8>; [5.50%]

  <bb 8> [local count: 6317494]:
  goto <bb 5>; [100.00%]

  <bb 6> [local count: 108546036]: 
 _15 = buf_avail_8(D) + 4294967293;
  _7 = (sizetype) _15;
  _4 = _7 + 3;
  goto <bb 4>; [100.00%]

  <bb 3> [local count: 958878296]:
  _16 = (unsigned int) ivtmp.7_19;
  len_test_12 = _16 + 1;
  ivtmp.7_18 = ivtmp.7_19 + 1;
  if (ivtmp.7_18 != _4)
    goto <bb 7>; [94.50%]
  else
    goto <bb 9>; [5.50%]

  <bb 9> [local count: 52738306]:
  # len_test_21 = PHI <len_test_12(3)>
  goto <bb 5>; [100.00%]

  <bb 7> [local count: 906139990]:

  <bb 4> [local count: 1014686026]:
  # ivtmp.7_19 = PHI <ivtmp.7_18(7), 2(6)>
  _17 = (unsigned int) ivtmp.7_19;
  len_test_13 = _17;
  _3 = MEM[(const uint8_t *)buf_9(D) + ivtmp.7_19 * 1];
  _6 = MEM[(const uint8_t *)buf_back_11(D) + ivtmp.7_19 * 1];
  if (_3 == _6)
    goto <bb 3>; [94.50%]
  else
    goto <bb 10>; [5.50%]

  <bb 10> [local count: 55807731]:
  # len_test_20 = PHI <len_test_13(4)>

  <bb 5> [local count: 114863531]:
  # len_test_14 = PHI <len_test_21(9), len_test_20(10), 2(8)>
  return len_test_14;

}

len_test_13 is unused in loop body and could sink to bb 5.


---


### compiler : `gcc`
### title : `390: inefficient 64-bit constant generation`
### open_at : `2021-12-15T09:52:54Z`
### last_modified_date : `2021-12-15T10:44:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103731
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `enhancement`
### contents :
unsigned long long M8()
{
   return 0x8888888888888888;
}

Generates:

.LC0:
        .quad   0x8888888888888888
.text
        .align  8
.globl _Z2M8v
        .type   _Z2M8v, @function
_Z2M8v:
.LFB0:
        .cfi_startproc
        lgrl    %r2,.LC0
        br      %r14
        .cfi_endproc

Expected 2 instructions:
load immediate + insert immedate(IIHF) instead of LOAD


---


### compiler : `gcc`
### title : `Defaulted operator== for arrays of integral types could be done using memcmp instead of loop`
### open_at : `2021-12-15T11:49:17Z`
### last_modified_date : `2023-07-13T18:15:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103733
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `enhancement`
### contents :
== Input ==
struct S {
        char b[16];
#if __cplusplus >= 202000L
        bool operator==(const S &) const = default;
#else
        inline bool operator==(const S &o) const { return __builtin_memcpy(const_cast<char *>(b), o.b, sizeof(b)); }
#endif
};
bool fff(const S &a, const S &b) { return a == b; }


== Observed ==
gcc version 12.0.0 20211214 973f6aedeb6489a9fdc7aeceaed0514348ee1e1a x86_64

$ ./xgcc -B. -c x.cpp -O2 -std=c++20 && objdump -d x.o
0000000000000000 <_Z3fffRK1SS1_>:
   0:   31 c0                   xor    %eax,%eax
   2:   66 0f 1f 44 00 00       nopw   0x0(%rax,%rax,1)
   8:   0f b6 14 06             movzbl (%rsi,%rax,1),%edx
   c:   38 14 07                cmp    %dl,(%rdi,%rax,1)
   f:   75 17                   jne    28 <_Z3fffRK1SS1_+0x28>
  11:   48 83 c0 01             add    $0x1,%rax
  15:   48 83 f8 10             cmp    $0x10,%rax
  19:   75 ed                   jne    8 <_Z3fffRK1SS1_+0x8>
  1b:   b8 01 00 00 00          mov    $0x1,%eax
  20:   c3                      ret    
  21:   0f 1f 80 00 00 00 00    nopl   0x0(%rax)
  28:   31 c0                   xor    %eax,%eax
  2a:   c3                      ret  

== Expected ==
Expected the same result as when -std=c++17/memcmp is used.

0000000000000000 <_Z3fffRK1SS1_>:
   0:   f3 0f 6f 06             movdqu (%rsi),%xmm0
   4:   b8 01 00 00 00          mov    $0x1,%eax
   9:   0f 11 07                movups %xmm0,(%rdi)
   c:   c3                      ret


---


### compiler : `gcc`
### title : `PPC: Inefficient equality compare for large 64-bit constants having only 16-bit relevant bits in high part`
### open_at : `2021-12-16T06:46:20Z`
### last_modified_date : `2023-01-09T05:29:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103743
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.1`
### severity : `enhancement`
### contents :
int overflow();
int negOverflow(long long in)
{
   if (in == 0x8000000000000000LL)
   {
      return overflow();
   }
   return 0;
}

Generates:
negOverflow(long long):
        .quad   .L.negOverflow(long long),.TOC.@tocbase,0
.L.negOverflow(long long):
        li 9,-1
        rldicr 9,9,0,0
        cmpd 0,3,9
        beq 0,.L10
        li 3,0
        blr
.L10:
        mflr 0
        std 0,16(1)
        stdu 1,-112(1)
        bl overflow()
        nop
        addi 1,1,112
        ld 0,16(1)
        mtlr 0
        blr
        .long 0
        .byte 0,9,0,1,128,0,0,0

Instead of:
        li 9,-1
        rldicr 9,9,0,0
        cmpd 0,3,9

Expected output:
        rotldi 3,3,1
        cmpdi 0,3,1

This should be only applied if constant fits into 16-bit and if those 16-bit are in the first 32-bit.


---


### compiler : `gcc`
### title : `[i386] GCC schedules KMOV instructions that destroys performance in loop`
### open_at : `2021-12-16T16:25:31Z`
### last_modified_date : `2022-01-07T15:09:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103750
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Testcase:

const char16_t *qustrchr(char16_t *n, char16_t *e, char16_t c) noexcept
{
    __m256i mch256 = _mm256_set1_epi16(c);
    for ( ; n < e; n += 32) {
        __m256i data1 = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(n));
        __m256i data2 = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(n) + 1);
        __mmask16 mask1 = _mm256_cmpeq_epu16_mask(data1, mch256);
        __mmask16 mask2 = _mm256_cmpeq_epu16_mask(data2, mch256);
        if (_kortestz_mask16_u8(mask1, mask2))
            continue;

        unsigned idx = _tzcnt_u32(mask1);
        if (mask1 == 0) {
            idx = __tzcnt_u16(mask2);
            n += 16;
        }
        return n + idx;
    }
    return e;
}

The assembly for this produces:

        vmovdqu16       (%rdi), %ymm1
        vmovdqu16       32(%rdi), %ymm2
        vpcmpuw $0, %ymm0, %ymm1, %k0
        vpcmpuw $0, %ymm0, %ymm2, %k1
        kmovw   %k0, %edx
        kmovw   %k1, %eax
        kortestw        %k1, %k0
        je      .L10

Those two KMOVW instructions aren't required for the check that follows. They're also dispatched on port 0, same as the KORTESTW, meaning the KORTEST can't be dispatched until those two have executed, thus introducing a 2-cycle delay in this loop.

Clang generates:

.LBB0_2:                                # =>This Inner Loop Header: Depth=1
        vpcmpeqw        (%rdi), %ymm0, %k0
        vpcmpeqw        32(%rdi), %ymm0, %k1
        kortestw        %k0, %k1
        jne     .LBB0_3

ICC inserts one KMOVW, but not the other.

Godbolt build link: https://gcc.godbolt.org/z/cc3heo48M

LLVM-MCA analysis: https://analysis.godbolt.org/z/dGvY1Wj78
It shows the Clang loop runs on average 2.0 cycles per loop, whereas the GCC code is 3 cycles/loop.

LLVM-MCA says the ICC loop with one of the two KMOV also runs at 2.0 cycles per loop, because it can run in parallel with the second load, given that the loads are ports 2 and 3.


---


### compiler : `gcc`
### title : `Unoptimal avx2 V16HF vector insert to element 0`
### open_at : `2021-12-16T19:55:21Z`
### last_modified_date : `2022-01-07T03:21:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103753
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
(Cloned from PR103571#18)

Following testcase:

--cut here--
typedef _Float16 __v16hf __attribute__ ((__vector_size__ (32)));

__v16hf foo (_Float16 x)
{
  return (__v16hf) { x, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f };
}
--cut here--

compiles with -O2 -mavx2 to:

        vpxor   %xmm1, %xmm1, %xmm1
        vpbroadcastw    %xmm0, %ymm0
        vpblendw        $1, %ymm0, %ymm1, %ymm0
        vpblendd        $15, %ymm0, %ymm1, %ymm1
        vmovdqa %ymm1, %ymm0
        ret

while similar version with 16bit integer:

--cut here--
typedef short __v16hi __attribute__ ((__vector_size__ (32)));

__v16hi bar (short x)
{
  return (__v16hi) { x, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
}
--cut here--

compiles to much shorter:

        vpxor   %xmm1, %xmm1, %xmm1
        vpinsrw $0, %edi, %xmm1, %xmm0
        vinserti128     $0x1, %xmm1, %ymm0, %ymm0
        ret

Please also note that with -O2 -mavx, the _Float16 version compiles to optimal:

        vpxor   %xmm1, %xmm1, %xmm1
        vpblendw        $1, %xmm0, %xmm1, %xmm0
        vinsertf128     $0x1, %xmm1, %ymm0, %ymm0
        ret


---


### compiler : `gcc`
### title : `[12 Regression] glibc master branch is miscompiled by r12-897`
### open_at : `2021-12-18T14:01:32Z`
### last_modified_date : `2022-02-04T01:34:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103762
### status : `RESOLVED`
### tags : `missed-optimization, patch, ra`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
On x86-64, r12-897

commit de56f95afaaa22c67cbeec780921d63e8b34514e
Author: Xionghu Luo <luoxhu@linux.ibm.com>
Date:   Tue May 18 21:34:18 2021 -0500

    Run pass_sink_code once more before store_merging
    
    Gimple sink code pass runs quite early, there may be some new
    oppertunities exposed by later gimple optmization passes, this patch
    runs the sink code pass once more before store_merging.  For detailed
    discussion, please refer to:
    https://gcc.gnu.org/pipermail/gcc-patches/2020-December/562352.html
    
    Tested the SPEC2017 performance on P8LE, 544.nab_r is improved
    by 2.43%, but no big changes to other cases, GEOMEAN is improved quite
    small with 0.25%.
    
    gcc/ChangeLog:
    
    2021-05-18  Xionghu Luo  <luoxhu@linux.ibm.com>
    
            * passes.def: Add sink_code pass before store_merging.
            * tree-ssa-sink.c (pass_sink_code:clone): New.
    
    gcc/testsuite/ChangeLog:
    
    2021-05-18  Xionghu Luo  <luoxhu@linux.ibm.com>
    
            * gcc.dg/tree-ssa/ssa-sink-1.c: Adjust.
            * gcc.dg/tree-ssa/ssa-sink-2.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-3.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-4.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-5.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-6.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-7.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-8.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-9.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-10.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-13.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-14.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-16.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-17.c: Ditto.
            * gcc.dg/tree-ssa/ssa-sink-18.c: New.

miscompiled glibc master branch:

FAIL: elf/tst-env-setuid

[hjl@gnu-skx-1 build-x86_64-linux]$ env GCONV_PATH=/export/build/gnu/tools-build/glibc-cet/build-x86_64-linux/iconvdata LOCPATH=/export/build/gnu/tools-build/glibc-cet/build-x86_64-linux/localedata LC_ALL=C MALLOC_CHECK_=2 MALLOC_MMAP_THRESHOLD_=4096 LD_HWCAP_MASK=0x1 /export/build/gnu/tools-build/glibc-test/build-x86_64-linux/elf/tst-env-setuid
Segmentation fault (core dumped)
[hjl@gnu-skx-1 build-x86_64-linux]$ 

(gdb) set env MALLOC_CHECK_=2
(gdb) r --direct
Starting program: /export/build/gnu/tools-build/glibc-test/build-x86_64-linux/elf/tst-env-setuid --direct

Program received signal SIGSEGV, Segmentation fault.
0x00007ffff7f67053 in do_tunable_update_val (cur=cur@entry=0xffffefefe7f0, valp=valp@entry=0x7fffffffdd38, minp=minp@entry=0x0, maxp=maxp@entry=0x0) at dl-tunables.c:102
102	  if (cur->type.type_code == TUNABLE_TYPE_STRING)
(gdb) bt
#0  0x00007ffff7f67053 in do_tunable_update_val (cur=cur@entry=0xffffefefe7f0, 
    valp=valp@entry=0x7fffffffdd38, minp=minp@entry=0x0, maxp=maxp@entry=0x0)
    at dl-tunables.c:102
#1  0x00007ffff7f6733c in tunable_initialize (strval=0x7fffffffefa7 "2", 
    cur=<optimized out>) at dl-tunables.c:151
#2  __tunables_init (envp=0x7fffffffe058) at dl-tunables.c:349
#3  0x00007ffff7f15f67 in __libc_start_main_impl (main=0x7ffff7f128f0 <main>, 
    argc=2, argv=0x7fffffffdea8, init=<optimized out>, fini=<optimized out>, 
    rtld_fini=0x0, stack_end=0x7fffffffde98) at ../csu/libc-start.c:291
#4  0x00007ffff7f129c5 in _start () at ../sysdeps/x86_64/start.S:115
(gdb)


---


### compiler : `gcc`
### title : `Missed arithmetic simplification for multiplication + division`
### open_at : `2021-12-18T23:30:11Z`
### last_modified_date : `2022-01-04T12:44:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103765
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
The following code is optimized strangely:

int foo(int x) {
    return (x * 72) / 3;
}
int bar(int x) {
    x *= 72;
    x /= 3;
    return x;
}

foo(int):
        lea     eax, [rdi+rdi*2]
        sal     eax, 3
        ret
bar(int):
        lea     edx, [rdi+rdi*8]
        sal     edx, 3
        movsx   rax, edx
        imul    rax, rax, 1431655766
        sar     edx, 31
        shr     rax, 32
        sub     eax, edx
        ret

https://godbolt.org/z/13PxxsM8G

For some foo is simplified to return x * 24; but bar is not. It appears the transformation is done before transformation to gimple.

Another case:

int foo(int x) {
    return (x * 64) / 8;
}
int bar(int x) {
    x *= 64;
    x /= 8;
    return x;
}

foo(int):
        lea     eax, [0+rdi*8]
        ret
bar(int):
        sal     edi, 6
        lea     eax, [rdi+7]
        cmovns  eax, edi
        sar     eax, 3
        ret

https://godbolt.org/z/vfG1TP7oG

Even with powers of 2 (and thus shift operations down the line), it is not transformed.

This would be a good enhancement, and I'd also be interested to learn why this strange behavior is occurring.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Missed vectorization under -mavx512f -mavx512vl after r12-5489`
### open_at : `2021-12-20T08:59:14Z`
### last_modified_date : `2023-05-08T07:39:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103771
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
cat vect.c

typedef unsigned char uint8_t;                                                               
                                                                                             
static uint8_t x264_clip_uint8( int x )                                                      
{                                                                                            
    return x&(~255) ? (-x)>>31 : x;                                                          
}                                                                                            
                                                                                             
void mc_weight( uint8_t * __restrict dst, uint8_t * __restrict src, int i_width, int i_scale)
{                                                                                            
  for( int x = 0; x < i_width; x++ )                                                         
    dst[x] = x264_clip_uint8(src[x] * i_scale);                                              
}                                                                                            

It can not be vectorized with -mavx512f -mavx512vl, but can be vectorized with -mavx2, See https://godbolt.org/z/M1jx161f6

The commit https://gcc.gnu.org/cgi-bin/gcc-gitref.cgi?r=r12-5489 converts  (x & (~255)) == 0 to x <= 255, which may trigger some missing pattern with -mavx512vl. 

Also an 1.5% regression was found on -march=cascadelake due to missing 128bit epilogue for this loop.


---


### compiler : `gcc`
### title : `[i386] GCC should swap the arguments to certain functions to generate a single instruction`
### open_at : `2021-12-20T15:59:03Z`
### last_modified_date : `2022-01-07T03:20:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103774
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
I don't know how widespread this is. Seen in the code generated at https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103750.

This code:
        __m256i data1 = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(n));
        __m256i data2 = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(n) + 1);
        __mmask16 mask1 = _mm256_cmpeq_epu16_mask(data1, mch256);
        __mmask16 mask2 = _mm256_cmpeq_epu16_mask(data2, mch256);
Generates:
        vmovdqu16       (%rdi), %ymm1
        vmovdqu16       32(%rdi), %ymm2
        vpcmpuw $0, %ymm0, %ymm1, %k0
        vpcmpuw $0, %ymm0, %ymm2, %k1

While if you invert the two operands in the cmpeq intrinsics, as in:
        __m256i data1 = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(n));
        __m256i data2 = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(n) + 1);
        __mmask16 mask1 = _mm256_cmpeq_epu16_mask(mch256, data1);
        __mmask16 mask2 = _mm256_cmpeq_epu16_mask(mch256, data2);
You get:
        vpcmpuw $0, (%rdi), %ymm0, %k0
        vpcmpuw $0, 32(%rdi), %ymm0, %k1


Godbolt link with full copileable source code: https://gcc.godbolt.org/z/rKo666MM7

Clang, ICC (Clang-based) do this. MSVC behaves like GCC.


---


### compiler : `gcc`
### title : `generic/cortex-a53 cost model for SLP for aarch64 is good`
### open_at : `2021-12-20T20:54:35Z`
### last_modified_date : `2022-01-05T12:39:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103781
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.2.1`
### severity : `normal`
### contents :
As of GCC 11, the AArch64 backend is very greedy in trying to vectorize mulv2di3. However, there is no mulv2di3 routine so it extracts from the vector.

The bad codegen should be obvious. 

#include <stdint.h>

void fma_u64(uint64_t *restrict acc, const uint64_t *restrict x, const uint64_t *restrict y)
{
    for (int i = 0; i < 16384; i++){
        acc[0] += *x++ * *y++;
        acc[1] += *x++ * *y++;
    }
}

gcc-11 -O3

fma_u64:
.LFB0:
        .cfi_startproc
        ldr     q1, [x0]
        add     x6, x1, 262144
        .p2align 3,,7
.L2:
        ldr     x4, [x1], 16
        ldr     x5, [x2], 16
        ldr     x3, [x1, -8]
        mul     x4, x4, x5
        ldr     x5, [x2, -8]
        fmov    d0, x4
        ins     v0.d[1], x5
        mul     x3, x3, x5
        ins     v0.d[1], x3
        add     v1.2d, v1.2d, v0.2d
        cmp     x1, x6
        bne     .L2
        str     q1, [x0]
        ret
        .cfi_endproc

GCC 10.2.1 emits better code.

fma_u64:
.LFB0:
        .cfi_startproc
        ldp     x4, x3, [x0]
        add     x9, x1, 262144
        .p2align 3,,7
.L2:
        ldr     x8, [x1], 16
        ldr     x7, [x2], 16
        ldr     x6, [x1, -8]
        ldr     x5, [x2, -8]
        madd    x4, x8, x7, x4
        madd    x3, x6, x5, x3
        cmp     x9, x1
        bne     .L2
        stp     x4, x3, [x0]
        ret
        .cfi_endproc

However, the ideal code would be a 2 iteration unroll.

Side note: why not ldp in the loop?


---


### compiler : `gcc`
### title : `suboptimal code for returning bool value on target ppc`
### open_at : `2021-12-21T01:42:31Z`
### last_modified_date : `2023-08-31T08:03:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103784
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
//test.c

#include <stdbool.h>

bool foo (int a, int b)
{
  if (a > 2)
    return false;
  if (b < 10)
    return true;
  return true;
}

//assembly with trunk
        ld 9,0(3)
        cmpdi 0,9,0
        add 10,9,4
        beq 0,.L5
        ldarx 8,0,3
        cmpd 0,8,9
        bne 0,.L4
        stdcx. 10,0,3
        bne 0,.L4
        li 3,1
        rldicl 3,3,0,63
        blr
        .p2align 4,,15
.L5:
        li 3,0
        rldicl 3,3,0,63
        blr

//assembly with at13.0
        subfic 3,3,2
        srdi 3,3,63
        xori 3,3,0x1
        blr

The second branch and two zero extend are unnecessary. If it returns a integer, the code seems good.

//test1.c
int foo (int a, int b)
{
  if (a > 2)
    return 0;
  if (b < 10)
    return 1;
  return 1;
}

//assembly with trunk
        li 9,1
        cmpwi 0,3,2
        isel 3,0,9,1
        blr


---


### compiler : `gcc`
### title : `Clang vectorized LightPixel while GCC does not`
### open_at : `2021-12-21T23:29:00Z`
### last_modified_date : `2022-01-07T06:39:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103797
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Clang vectorises divss in LightPixel while GCC does not (at -O3).  This seems to account for 17% difference in resteflood_svg benchmark of Firefox.

       │    0000000001864660 <mozilla::gfx::(anonymous namespace)::SpecularLightingSoftware::LightPixel(mozilla::gfx::Point3DTyped<mozilla::gfx::UnknownUnits, float> const&, mozilla::gfx::Point3DTyped<mozilla::gfx::UnknownUnits, float> const&, unsigned int)>:
       │    mozilla::gfx::(anonymous namespace)::SpecularLightingSoftware::LightPixel(mozilla::gfx::Point3DTyped<mozilla::gfx::UnknownUnits, float> const&, mozilla::gfx::Point3DTyped<mozilla::gfx::UnknownUnits, float> const&, unsigned int):
  0.05 │      push      %rbp
  0.07 │      mov       %rsp,%rbp
  0.71 │      xorps     %xmm6,%xmm6
  0.32 │      addss     %xmm6,%xmm4
       │      unpcklps  %xmm3,%xmm5
  0.78 │      movss     anon.5bcbce9b5eeaaf1a18a99b9a5b62e1ce.3.llvm.5306652999446557335+0x6d8,%xmm8
  0.01 │      addps     %xmm8,%xmm5
  1.47 │      movaps    %xmm4,%xmm9
       │      mulss     %xmm4,%xmm9
       │      movaps    %xmm5,%xmm7
  0.01 │      mulps     %xmm5,%xmm7
  3.35 │      movaps    %xmm7,%xmm3
       │      shufps    $0x55,%xmm7,%xmm3
  0.99 │      addss     %xmm9,%xmm3
  1.59 │      addss     %xmm7,%xmm3
  2.01 │      sqrtss    %xmm3,%xmm3
 11.43 │      divss     %xmm3,%xmm4
  6.76 │      shufps    $0x0,%xmm3,%xmm3
  0.01 │      divps     %xmm3,%xmm5
  2.58 │      mulss     %xmm1,%xmm4
  0.04 │      unpcklps  %xmm0,%xmm2
       │      mulps     %xmm5,%xmm2
  2.67 │      movaps    %xmm2,%xmm0
  0.04 │      shufps    $0x55,%xmm2,%xmm0
  2.11 │      addss     %xmm4,%xmm0
  1.87 │      addss     %xmm2,%xmm0
  2.82 │      cmpless   %xmm0,%xmm6
  2.20 │      andps     %xmm8,%xmm6
  1.05 │      mulss     %xmm0,%xmm6
  4.04 │      mulss     .str.6.llvm.231702015065810902+0x77,%xmm6
  3.14 │      cvttss2si %xmm6,%eax
  4.45 │      mov       0x8(%rdi),%ecx
  0.00 │      mov       0xc(%rdi),%edx
       │      movzwl    %ax,%eax
  1.10 │      test      %edx,%edx
       │    ↓ jle       92
       │88:   imul      %eax,%eax
  9.06 │      shr       $0xf,%eax
  3.12 │      dec       %edx
       │    ↑ jne       88
       │92:   shr       $0x8,%eax
  1.95 │      movzwl    0x10(%rdi,%rax,2),%eax
  6.48 │      imul      %eax,%ecx
  0.99 │      shr       $0x8,%ecx
  1.06 │      mov       %esi,%eax
  0.01 │      shr       $0x8,%eax
       │      mov       %esi,%edx
       │      shr       $0x10,%edx
  0.01 │      mov       $0xff,%edi
       │      and       %edi,%esi
  0.01 │      imul      %ecx,%esi
  3.32 │      shr       $0xf,%esi
  1.81 │      cmp       %edi,%esi ▒
  0.04 │      cmovae    %edi,%esi ▒
  1.99 │      and       %edi,%eax ▒
  0.01 │      imul      %ecx,%eax ▒
       │      shr       $0xf,%eax ▒
  0.01 │      cmp       %edi,%eax ▒
  0.28 │      cmovae    %edi,%eax ▒
  0.96 │      and       %edi,%edx ▒
       │      imul      %ecx,%edx ▒
       │      shr       $0xf,%edx ▒
  0.92 │      cmp       %edi,%edx ▒
  0.85 │      cmovae    %edi,%edx ▒
  1.00 │      cmp       %eax,%edx ▒
  1.20 │      mov       %eax,%ecx ▒
       │      cmova     %edx,%ecx ▒
  2.17 │      cmp       %esi,%ecx ▒
  1.15 │      cmovbe    %esi,%ecx ▒
  1.79 │      shl       $0x18,%ecx▒
  1.17 │      shl       $0x10,%edx▒
       │      shl       $0x8,%eax ▒
  0.03 │      or        %edx,%eax ▒
  0.01 │      or        %esi,%eax ▒
  0.14 │      or        %ecx,%eax ▒
  0.72 │      pop       %rbp      ▒
  0.04 │    ← ret                                                                                                                                                                                                                                                     ▒


---


### compiler : `gcc`
### title : `memchr with a (small) constant string should be expanded inline.`
### open_at : `2021-12-22T07:03:17Z`
### last_modified_date : `2022-08-28T07:42:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103798
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Given the following code:

#include <string_view>

size_t findFirstE_slow(std::string_view sv) {
  return sv.find_first_of("eE");
}

size_t findFirstE_fast(std::string_view sv) {
  auto it{sv.begin()};
  for (; it != sv.end() && *it != 'e' && *it != 'E'; ++it)
    ;
  return it == sv.end() ? std::string_view::npos : size_t(it - sv.begin());
}


x86-64 gcc (trunk) -std=c++20 -O3 generates the following assembly:

.LC0:
        .string "eE"
findFirstE_slow(std::basic_string_view<char, std::char_traits<char> >):
        push    r12
        push    rbp
        push    rbx
        test    rdi, rdi
        je      .L4
        mov     rbx, rdi
        mov     rbp, rsi
        xor     r12d, r12d
        jmp     .L3
.L8:
        add     r12, 1
        cmp     rbx, r12
        je      .L4
.L3:
        movsx   esi, BYTE PTR [rbp+0+r12]
        mov     edx, 2
        mov     edi, OFFSET FLAT:.LC0
        call    memchr
        test    rax, rax
        je      .L8
        mov     rax, r12
        pop     rbx
        pop     rbp
        pop     r12
        ret
.L4:
        mov     r12, -1
        pop     rbx
        pop     rbp
        mov     rax, r12
        pop     r12
        ret

findFirstE_fast(std::basic_string_view<char, std::char_traits<char> >):
        add     rdi, rsi
        cmp     rdi, rsi
        je      .L13
        mov     rax, rsi
        jmp     .L12
.L15:
        add     rax, 1
        cmp     rdi, rax
        je      .L13
.L12:
        movzx   edx, BYTE PTR [rax]
        and     edx, -33
        cmp     dl, 69
        jne     .L15
        sub     rax, rsi
        ret
.L13:
        mov     rax, -1
        ret


Even though both findFirstE_slow() and findFirstE_fast() are logically equivalent, findFirstE_slow() calls memchr() for every character in the input string.

findFirstE_fast() does what we would reasonably expect, comparing every character in the input string with 'e' and 'E'.

In practice, when the set of characters to be found is small, findFirstE_slow() runs noticeably slower than findFirstE_fast(). This seems to be a missed optimization in char_traits<char>::find(), which affects several find-related methods in string_view.

Relevant StackOverflow question with quick-bench output, Compiler Explorer output, and more discussion: https://stackoverflow.com/questions/70433152/missed-optimization-with-string-viewfind-first-of


---


### compiler : `gcc`
### title : `switch expansion could be smarter`
### open_at : `2021-12-22T07:44:03Z`
### last_modified_date : `2021-12-22T08:56:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103799
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
auto f(char c)
{
  int t1;
  switch (c)
    {
       case '1':
         t1 = 1;
         break;
       case '2':
         t1 = 2;
         break;
       case '3':
         t1 = 3;
         break;
       case '\0':
         t1 = 4;
         break;
       default:
         t1 = -1;
    }
   return t1;
}


auto f1(char c)
{
  int t1;
  switch (c)
    {
       case '1':
         t1 = 1;
         break;
       case '2':
         t1 = 2;
         break;
       case '3':
         t1 = 3;
         break;
    default:
      if (c == '\0') t1 = 4; else t1 = -1;
    }
   return t1;
}

f1 produces better code than f.


---


### compiler : `gcc`
### title : `[12 regression] recip-3.c fails after  r12-6087 on Power m32`
### open_at : `2021-12-22T08:56:44Z`
### last_modified_date : `2022-01-11T08:37:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103802
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Invoking the compiler as /home/luoxhu/workspace/gcc-master_build/gcc/xgcc -B/home/luoxhu/workspace/gcc-master_build/gcc/ /home/luoxhu/workspace/gcc-master/gcc/testsuite/gcc.dg/tree-ssa/recip-3.c  -fdiagnostics-plain-output   -O1 -fno-trapping-math -funsafe-math-optimizations -fdump-tree-recip -S  -m32  -o recip-3.s
Executing on host: /home/luoxhu/workspace/gcc-master_build/gcc/xgcc -B/home/luoxhu/workspace/gcc-master_build/gcc/ /home/luoxhu/workspace/gcc-master/gcc/testsuite/gcc.dg/tree-ssa/recip-3.c  -fdiagnostics-plain-output   -O1 -fno-trapping-math -funsafe-math-optimizations -fdump-tree-recip -S  -m32  -o recip-3.s    (timeout = 300)
gcc.dg/tree-ssa/recip-3.c: pattern found 3 times
FAIL: gcc.dg/tree-ssa/recip-3.c scan-tree-dump-times recip " / " 5


Reson is m32 fail to cunroll due to
 recip-3.m32.c.172t.cunroll:   Not unrolling loop 1: size would grow.


---


### compiler : `gcc`
### title : `i++; i = i % constant -> i >= constant-1 ? 0 : i+1`
### open_at : `2021-12-22T09:27:05Z`
### last_modified_date : `2022-01-15T15:16:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103803
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int next_trivial(int i) {
    if (i < 0 || i >= 10) __builtin_unreachable ();
    ++i;
    i = i % 10;
    return i;
}
int next_trivial1(int i) {
    if (i < 0 || i >= 10) __builtin_unreachable ();
    ++i;
    if (i >= 10) i = 0;
    return i;
}

These two should produce the same assembly code, the next_trivial1 is better than next_trivial


---


### compiler : `gcc`
### title : `IVCann/IVOPTs changes induction variable so it is an addition but the need for shift is there and the result could have used for the (loop) exit compare`
### open_at : `2021-12-23T13:16:53Z`
### last_modified_date : `2023-06-18T21:10:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103815
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
This code, which calculates an integer square root ..:

    #include <inttypes.h>
    uint16_t int_sqrt32(uint32_t x)
    {
        uint16_t res=0;
        uint16_t add= 0x8000;   
        do {
            uint16_t temp=res | add;
            uint32_t g2=temp*temp;      
            if (x>=g2)
                res=temp;           
            add>>=1;
        } while(add);
        return res;
    }

... should be compileable 1:1, since the right shift sets the condition flags appropriately.

Unfortunately, GCC's optimizer notices that this is a 16-step loop, "helpfully" invents a loop counter, and pessimizes the code to this sub-optimal result (ARM Thumb output; x86 has essentially the same problem):

   0:	b500      	push	{lr}
   2:	2110      	movs	r1, #16
   4:	4686      	mov	lr, r0
   6:	f44f 4200 	mov.w	r2, #32768	; 0x8000
   a:	2000      	movs	r0, #0
   c:	ea40 0302 	orr.w	r3, r0, r2
  10:	0852      	lsrs	r2, r2, #1
  12:	b29b      	uxth	r3, r3
  14:	fb03 fc03 	mul.w	ip, r3, r3
  18:	45f4      	cmp	ip, lr
  1a:	bf98      	it	ls
  1c:	4618      	movls	r0, r3
  1e:	3901      	subs	r1, #1
  20:	d1f4      	bne.n	c <int_sqrt32+0xc>
  22:	f85d fb04 	ldr.w	pc, [sp], #4


---


### compiler : `gcc`
### title : `function which takes an argument via (hidden) reference should assume the argument does not escape or is only read from`
### open_at : `2021-12-25T12:13:39Z`
### last_modified_date : `2022-03-18T08:07:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103827
### status : `UNCONFIRMED`
### tags : `alias, missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
#include <string>

extern void foo (const std::string );

void
bar ()
{
  foo ("abc");
  foo (std::string("abc"));
}

---- CUT ----
Right now we get:
  MEM[(struct _Alloc_hider *)&D.33220]._M_p = &D.33220.D.26223._M_local_buf;
  __builtin_memcpy (&D.33220.D.26223._M_local_buf, "abc", 3);
  D.33220._M_string_length = 3;
  MEM[(char_type &)&D.33220 + 19] = 0;
  foo (&D.33220);

  <bb 6> [local count: 1073741824]:
  _5 = D.33220._M_dataplus._M_p;
  if (&D.33220.D.26223._M_local_buf != _5)

But the address of what was passed to foo cannot escape and foo cannot change the content of D.33220. So the read of D.33220._M_dataplus._M_p should always return &D.33220.D.26223._M_local_buf.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] missing shrink wrapping for simple/obvious code`
### open_at : `2021-12-25T22:16:17Z`
### last_modified_date : `2023-07-07T10:41:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103829
### status : `UNCONFIRMED`
### tags : `missed-optimization, needs-bisection`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Take:

extern int _IO_getc (void *) ;
extern int *__errno_location (void) __attribute__ ((__const__));
void readError ( void );
typedef
   struct {
      void* handle;
      int buffer;
      int buffLive;
      char mode;
   }
   BitStream;

int bsGetBit ( BitStream* bs )
{  
   if (bs->buffLive > 0) {
      bs->buffLive --;
      return ( ((bs->buffer) >> (bs->buffLive)) & 0x1 );
   } else {
      int retVal = _IO_getc (bs->handle);
      if ( retVal == (-1) ) {
         if ((*__errno_location ()) != 0) readError();
         return 2;
      }
      bs->buffLive = 7;
      bs->buffer = retVal;
      return ( ((bs->buffer) >> 7) & 0x1 );
   }
}
----- CUT ----
GCC 4.9.0-8.5.0 was able to shrink wrap the above function.
But starting in GCC 9, GCC does not; there is an extra mov using a callee saved register.
This happens on both aarch64 and x86_64 So I suspect it was a generic change which caused it.


---


### compiler : `gcc`
### title : `bogus sprintf warnings due to missing strlen propagation`
### open_at : `2021-12-26T16:13:23Z`
### last_modified_date : `2022-01-06T23:24:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103835
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.2.0`
### severity : `normal`
### contents :
Please address these warnings because they create more noise than they help!

$ cat test.c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


const char* fun(char* buf, const char* pfx, int a, int b)
{
    sprintf(buf, "%sa = %d\n"
                 "%sb = %d\n",
                 pfx, a, pfx, b);
    return buf;
}


int main(int argc, char* argv[])
{
    char buf[500];
    const char* str;
    strcpy(buf, "\t");
    str = fun(buf + strlen(buf) + 1, buf, atoi(argv[1]), atoi(argv[2]));
    printf("%s\n", str);
    return 0;
}

$ gcc --version
gcc (GCC) 11.2.0
Copyright (C) 2021 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

$ gcc -Wall -O6 test.c
test.c: In function ‘main’:
test.c:8:21: warning: ‘a = ’ directive writing 4 bytes into a region of size between 0 and 499 [-Wformat-overflow=]
    8 |     sprintf(buf, "%sa = %d\n"
      |                     ^~~~
test.c:8:5: note: ‘sprintf’ output between 13 and 1031 bytes into a destination of size 499
    8 |     sprintf(buf, "%sa = %d\n"
      |     ^~~~~~~~~~~~~~~~~~~~~~~~~
    9 |                  "%sb = %d\n",
      |                  ~~~~~~~~~~~~~
   10 |                  pfx, a, pfx, b);
      |                  ~~~~~~~~~~~~~~~
test.c:8:5: warning: ‘sprintf’ arguments 3, 5 may overlap destination object ‘buf’ [-Wrestrict]
test.c:17:10: note: destination object referenced by ‘restrict’-qualified argument 1 was declared here
   17 |     char buf[500];
      |          ^~~

It's clear that the destination buffer will NOT overlap with anything related to "pfx" in the fun() function.  Is also clear that output will NOT contain that many characters that the warning claims (up to 1031).  If GCC can't estimate the length for sure, it's better NOT to emit any warnings, rather than printing this annoying noise.

Please be mindful of your users -- and their time to re-analyze the code that suddenly is now flagged with these senseless warnings, only to realize that it's all red herring.

Thank you


---


### compiler : `gcc`
### title : `missed optimization in AVX code`
### open_at : `2021-12-28T10:10:02Z`
### last_modified_date : `2022-01-04T14:12:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103850
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Created attachment 52076
test case

(I'm reporting this under "C" because I don't know which optimizer is responsible for this, but I observe the same beaviour in C++ programs as well.)

This test case was distilled from a hot loop in a library computing spherical harmonic transforms. Apparently it can be compiled in a way that gives close to theoretical peak performance at least on my hardware (Zen 2), but this only happens if the statements in the inner loop are arranged in a specific way. Trivial rearrangements result in a performance which is about 30% lower.

I would have expected that gcc would be able to spot this kind of rearrangement and do it by itself, but this doesn't seem the case at the moment. If that could be fixed, that would obviously be great, but if not, I'd be grateful for any tips how the most "efficient" arrangements can be found for such critical loops without resorting to trial and error.

The loops in question start at lines 27 and 78 in the attached test case.
On my machine the code reports

slow kernel version: 45.317578 GFlops/s
fast kernel version: 67.083952 GFlops/s

when compiled with "-O3 -march=znver2 -ffast-math -W -Wall"

Clang and Intel icx show the same discrepancy, so it seems that the required re-ordering is indeed hard to do.


---


### compiler : `gcc`
### title : `Missed optimization: 64bit division used instead of 32bit division`
### open_at : `2021-12-29T03:00:27Z`
### last_modified_date : `2022-03-01T07:27:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103855
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Compiler Explorer link: https://gcc.godbolt.org/z/KvW8sMsqz

I compiled the following code with `x86-64 gcc (trunk) -std=c++20 -O3 -Wall -Wextra -Werror`:

```
unsigned int optimized(unsigned int a, unsigned int b) {
    return (unsigned long long)a / b;
}

unsigned int unoptimized(unsigned int a, unsigned int b) {
    unsigned long long all = a;
    return all / b;
}
```

This is the assembly output:

```
optimized(unsigned int, unsigned int):
        mov     eax, edi
        xor     edx, edx
        div     esi
        ret
unoptimized(unsigned int, unsigned int):
        mov     eax, edi
        mov     esi, esi
        xor     edx, edx
        div     rsi
        ret
```

GCC uses a 64-bit divide for `unoptimized()`, when a 32-bit divide would be equivalent and faster. GCC uses a 32-bit divide for `optimized()`, which is fine. Note that LLVM does a 32-bit division in both cases.

I would like to tackle this optimization, but am not sure how to go about doing it. Could someone tell me/point me to resources that tell me what part of the compiler and codebase I should be looking to optimize? Thanks!


---


### compiler : `gcc`
### title : `implement ternary without jump (and comparison)`
### open_at : `2021-12-29T12:16:02Z`
### last_modified_date : `2021-12-29T20:26:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103857
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
This test case is derived from actual code and I expect it to be not uncommon in general.  Maybe not exactly in this form but perhaps the matcher can catch a few more cases.

Take this code:

extern void g(int);
void f1(int* a, int b, int c)
{
  for (unsigned i = 0; i < 100; ++i)
    g(a[i] == b ? c : b);
}
void f2(int* a)
{
  for (unsigned i = 0; i < 100; ++i)
    g(a[i] == 42 ? 10 : 42);
}


The function 'g' is called in each loop with one of two values which is the opposite from the one that is match in the condition of the ternary operation.  This allows the respective loops to be rewritten as

  for (unsigned i = 0; i < 100; ++i)
    g(a[i] ^ c ^ b);

and

  for (unsigned i = 0; i < 100; ++i)
    g(a[i] ^ 10 ^ 42);

In the former case the c ^ b operation can be hoisted.

This should be faster and smaller in pretty much all situations and on all platforms.


---


### compiler : `gcc`
### title : `[i386] vectorize v2qi vectors`
### open_at : `2021-12-29T18:09:56Z`
### last_modified_date : `2022-08-18T07:48:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103861
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Following testcase:

typedef char __v2qi __attribute__ ((__vector_size__ (2)));

__v2qi plus  (__v2qi a, __v2qi b) { return a + b; };

should be vectorized.


---


### compiler : `gcc`
### title : `x86: Missing optimizations with _mm_undefined_si128 and PMOVSX*`
### open_at : `2022-01-03T11:19:54Z`
### last_modified_date : `2023-06-10T16:16:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103897
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Hello, I was trying to use VPMOVSXWD and other PMOVSX* intrinsics and also emulate them for SSE2 targets. I noticed two (at least) distinct problems:
1) (V)PMOVSX** can use a memory operand, but emits a separate load instruction.
2) _mm_undefined_si128() always generates an additional zeroing instruction. Using it in combination with unpack and arithmetic shift instructions looks like an optimal way to emulate PMOVSX for SSE2 target.

Godbolt example includes clang output for comparison.

https://godbolt.org/z/KE8q9v6qG

#include <emmintrin.h>
#include <immintrin.h>

__attribute__((__target__("avx"))) void test0(__m128i* dst, __m128i* src)
{
    // Emit VPMOVSXWD: can combine load from memory, but emits 2 instructions
    // Looks like gcc 8.5 was doing better
    *dst = _mm_cvtepi16_epi32(*src);
}

void test1(__m128i* dst, __m128i* src)
{
    // Emulate VPMOVSXWD: sets zero specifically for _mm_undefined_si128
    *dst = _mm_srai_epi32(_mm_unpacklo_epi16(_mm_undefined_si128(), *src), 16);
}

void test2(__m128i* dst, __m128i* src)
{
    // Sets zero register but absolutely can reuse PSLLW result    
    *dst = _mm_srai_epi32(_mm_unpacklo_epi16(_mm_undefined_si128(), _mm_slli_epi16(*src, 1)), 16);
}

void test3(__m128i* dst, __m128i* src)
{
    // Similar to test1, but emulate "high" VPMOVSXWD
    *dst = _mm_srai_epi32(_mm_unpackhi_epi16(_mm_undefined_si128(), *src), 16);
}

__attribute__((__target__("avx"))) void test4(__m128i* dst, __m128i* src)
{
    // Bonus (not sure what is the idiomatic way to MOVSX high part)
    *dst = _mm_srai_epi32(_mm_unpackhi_epi16(_mm_undefined_si128(), *src), 16);
}

__attribute__((__target__("avx"))) void test5(__m128i* dst, __m128i* src)
{
    // Emits two zeroing instructions
    *dst = _mm_srai_epi32(_mm_unpackhi_epi16(_mm_undefined_si128(), _mm_packs_epi16(_mm_undefined_si128(), *src)), 16);
}


---


### compiler : `gcc`
### title : `[12 Regression] make profiledbootstrap fails due to uninitialized warning in expr.c`
### open_at : `2022-01-04T05:50:00Z`
### last_modified_date : `2022-01-18T13:56:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103899
### status : `RESOLVED`
### tags : `build, diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
I am filing this and will attach the preprocessed source in a few minutes:
/home/apinski/src/upstream-gcc/gcc/objdir1/./prev-gcc/xg++ -B/home/apinski/src/upstream-gcc/gcc/objdir1/./prev-gcc/ -B/usr/local/x86_64-pc-linux-gnu/bin/ -nostdinc++ -B/home/apinski/src/upstream-gcc/gcc/objdir1/prev-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs -B/home/apinski/src/upstream-gcc/gcc/objdir1/prev-x86_64-pc-linux-gnu/libstdc++-v3/libsupc++/.libs  -I/home/apinski/src/upstream-gcc/gcc/objdir1/prev-x86_64-pc-linux-gnu/libstdc++-v3/include/x86_64-pc-linux-gnu  -I/home/apinski/src/upstream-gcc/gcc/objdir1/prev-x86_64-pc-linux-gnu/libstdc++-v3/include  -I/home/apinski/src/upstream-gcc/gcc/libstdc++-v3/libsupc++ -L/home/apinski/src/upstream-gcc/gcc/objdir1/prev-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs -L/home/apinski/src/upstream-gcc/gcc/objdir1/prev-x86_64-pc-linux-gnu/libstdc++-v3/libsupc++/.libs  -fno-PIE -c   -g -O2 -fno-checking -gtoggle -fprofile-generate -DIN_GCC     -fno-exceptions -fno-rtti -fasynchronous-unwind-tables -W -Wall -Wno-narrowing -Wwrite-strings -Wcast-qual -Wno-error=format-diag -Wmissing-format-attribute -Woverloaded-virtual -pedantic -Wno-long-long -Wno-variadic-macros -Wno-overlength-strings -Werror -fno-common  -DHAVE_CONFIG_H -I. -I. -I../../gcc -I../../gcc/. -I../../gcc/../include -I../../gcc/../libcpp/include -I../../gcc/../libcody -I/home/apinski/src/upstream-gcc/gcc/objdir1/./gmp -I/home/apinski/src/upstream-gcc/gcc/gmp -I/home/apinski/src/upstream-gcc/gcc/objdir1/./mpfr/src -I/home/apinski/src/upstream-gcc/gcc/mpfr/src -I/home/apinski/src/upstream-gcc/gcc/mpc/src  -I../../gcc/../libdecnumber -I../../gcc/../libdecnumber/bid -I../libdecnumber -I../../gcc/../libbacktrace -I/home/apinski/src/upstream-gcc/gcc/objdir1/./isl/include -I/home/apinski/src/upstream-gcc/gcc/isl/include  -o expr.o -MT expr.o -MMD -MP -MF ./.deps/expr.TPo ../../gcc/expr.c
In file included from ../../gcc/expr.c:26:
../../gcc/tree.h: In function ‘rtx_def* expand_expr_real_1(tree, rtx, machine_mode, expand_modifier, rtx_def**, bool)’:
../../gcc/tree.h:244:56: error: ‘context’ may be used uninitialized in this function [-Werror=maybe-uninitialized]
  244 | #define TREE_CODE(NODE) ((enum tree_code) (NODE)->base.code)
      |                                                        ^~~~
../../gcc/expr.c:10343:8: note: ‘context’ was declared here
10343 |   tree context;
      |        ^~~~~~~
cc1plus: all warnings being treated as errors
Makefile:1143: recipe for target 'expr.o' failed
make[3]: *** [expr.o] Error 1

I just did:
mkdir objdir1
cd objdir1
../configure
make profiledbootstrap

I tried to get a reduced testcase but I think there is a missing jump threading so it just happen to work for the reduced testcase I tried.


---


### compiler : `gcc`
### title : `Loops handling r,g,b values are not vectorized to use power of 2 vectors even if they can`
### open_at : `2022-01-04T15:17:52Z`
### last_modified_date : `2022-01-05T09:49:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103903
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
This is another textcase comming from Firefox's LightPixel. I am not sure if this is duplicate, but I think it is quite common in programs dealing with RGB values.

To match the vectorized code we would need to move from SLP vectorizing the 3 parallel computations to vectorising the loop.

struct a {float r,g,b;};
struct a src[100000], dest[100000];

void
test ()
{
  int i;
  for (i=0;i<100000;i++)
  {
          dest[i].r/=src[i].g;
          dest[i].g/=src[i].g;
          dest[i].b/=src[i].b;
  }
}

is vectorized to do 3 operaitons at a time, while equivalent:

float src[300000], dest[300000];

void
test ()
{
  int i;
  for (i=0;i<300000;i++)
  {
          dest[i]/=src[i];
  }
}

runs faster.


---


### compiler : `gcc`
### title : `[meta-bug] -Os vs loop header copy`
### open_at : `2022-01-05T10:24:26Z`
### last_modified_date : `2022-01-05T11:02:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103916
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
There are a bunch of bugs related to -Os and the loop header copy pass not happening so record them all in one place.


---


### compiler : `gcc`
### title : `x86: strange unoptimized code generated (multiple negations of _mm_testz_si128 result)`
### open_at : `2022-01-06T19:11:03Z`
### last_modified_date : `2022-01-10T10:39:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103932
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
GCC generates seemingly unoptimized sequence of instructions in certain cases (can't tell exactly what triggers it, example code is below):

        xor     eax, eax
        vptest  xmm0, xmm0
        sete    al
        test    eax, eax
        sete    al
        movzx   eax, al

This should be something like this:
xor eax, eax
vptest xmm0, xmm0
setne al


https://godbolt.org/z/sTaG65Ksc
Code (-O3 -std=c++20 -march=skylake):

#include <emmintrin.h>
#include <immintrin.h>
#include <bit>
#include <cstdint>

template <typename T>
concept Vector128 = (sizeof(T) == 16);

using u64 = std::uint64_t;
using u32 = std::uint32_t;

union alignas(16) v128
{
	u64 _u64[2];

	v128() = default;

	constexpr v128(const v128&) noexcept = default;

	template <Vector128 T>
	constexpr v128(const T& rhs) noexcept
		: v128(std::bit_cast<v128>(rhs))
	{
	}

	constexpr v128& operator=(const v128&) noexcept = default;

	template <Vector128 T>
	constexpr operator T() const noexcept
	{
		return std::bit_cast<T>(*this);
	}
};

// Test if vector is zero
inline bool gv_testz(const v128& arg)
{
#if defined(__SSE4_1__)
	return _mm_testz_si128(arg, arg);
#else
	return !(arg._u64[0] | arg._u64[1]);
#endif
}

struct alignas(16) context_t
{
	v128 vec[32];
	v128 sat;
};

void test1(context_t& ctx, u32 n)
{
	const u64 bit = !gv_testz(ctx.sat);
	v128 r;
	r._u64[0] = 0;
	r._u64[1] = bit;
	ctx.vec[n] = r;
}

void test2(context_t& ctx, u32 n)
{
	ctx.vec[n]._u64[1] = !gv_testz(ctx.sat);
}


---


### compiler : `gcc`
### title : `uavgv2qi3_ceil is not used (SLP costing and patterns vs live stmts)`
### open_at : `2022-01-07T15:31:03Z`
### last_modified_date : `2022-04-19T14:44:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103941
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Following testcase:

unsigned char ur[16], ua[16], ub[16];

void avgu_v2qi (void)
{
  int i;

  for (i = 0; i < 2; i++)
    ur[i] = (ua[i] + ub[i] + 1) >> 1;
}

does not vectorize on x86_64-linux-gnu with -O2 -ftree-vectorize.


---


### compiler : `gcc`
### title : `Vectorizer does not use vec_cmpMN without vcondMN pattern`
### open_at : `2022-01-08T10:00:00Z`
### last_modified_date : `2022-01-10T19:49:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103948
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
I was trying to add v2qi vec_cmpv2qiv2qi pattern to x86:

(define_expand "vec_cmpv2qiv2qi"
  [(set (match_operand:V2QI 0 "register_operand")
	(match_operator:V2QI 1 ""
	  [(match_operand:V2QI 2 "register_operand")
	   (match_operand:V2QI 3 "register_operand")]))]
  "TARGET_SSE2"
{
  bool ok = ix86_expand_int_vec_cmp (operands);
  gcc_assert (ok);
  DONE;
})

but the vectorizer does not consider the above pattern *unless* vcondv2qiv2qi is also present:

(define_expand "vcondv2qiv2qi"
  [(set (match_operand:V2QI 0 "register_operand")
	(if_then_else:V2QI
	  (match_operator 3 ""
	    [(match_operand:V2QI 4 "register_operand")
	     (match_operand:V2QI 5 "register_operand")])
	  (match_operand:V2QI 1)
	  (match_operand:V2QI 2)))]
  "TARGET_SSE4_1")

As shown above, the pattern does not need to expand to anything, just needs to be present.

So the following testcase:

--cut here--
typedef signed char vec __attribute__((vector_size(2)));

vec lt (vec a, vec b) { return a < b; }
--cut here--

vectorizes with -msse4 and fails to vectorize with -msse2.

Looking a bit into tree-vect-generic.c, in expand_vector_comparison we do:

/* Try to expand vector comparison expression OP0 CODE OP1 by
   querying optab if the following expression:
	VEC_COND_EXPR< OP0 CODE OP1, {-1,...}, {0,...}>
   can be expanded.  */

but apparenlty only via vcondMN optab.

According to the documentation, vec_cmpMN does exactly the above:

'vec_cmpMN'
     Output a vector comparison.  Operand 0 of mode N is the destination
     for predicate in operand 1 which is a signed vector comparison with
     operands of mode M in operands 2 and 3.  Predicate is computed by
     element-wise evaluation of the vector comparison with a truth value
     of all-ones and a false value of all-zeros.

so, support should query vec_cmpMN optab (and vec_vmpeqMN) in addition to vcondMN optab.

I'll attach the complete patch to illustrate the issue on x86_64.


---


### compiler : `gcc`
### title : `float64x1_t not using fmov to generate some float values`
### open_at : `2022-01-10T07:30:18Z`
### last_modified_date : `2022-01-10T07:30:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103959
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
#include <arm_neon.h>
Take:
double ff(void)
{
  return 2.0;
}
float64x1_t g(void)
{
  return (float64x1_t){2.0};
}
---- CUT ----
These should produce the same code but currently does not.
ff:
        fmov    d0, 2.0e+0
        ret
g:
        mov     x0, 4611686018427387904
        fmov    d0, x0
        ret

I Noticed this while working on PR 64821 where we now fold sqrt but get g.


---


### compiler : `gcc`
### title : `std::atomic relaxed load, inc, store sub-optimal codegen`
### open_at : `2022-01-10T15:51:11Z`
### last_modified_date : `2022-01-10T20:17:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103966
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `normal`
### contents :
Both functions below, should compile to the same assembly on x86:

#include <atomic>
#include <cstdint>

uint64_t x;
void inc_a() {
    x++;
}

std::atomic<uint64_t> y;

void inc_b_non_atomic() {
    y.store(y.load(std::memory_order_relaxed) + 1, std::memory_order_relaxed);
}


and it does so in clang.

It does not in gcc 12 (and earlier).

https://godbolt.org/z/GcM67xz8T



This pattern is very popular in approximate statistical counters / metrics, where the flow of information is unidirectional (i.e. from one thread that does updates, to another thread that only reads the counters), and its performance is critical in many codebases.


---


### compiler : `gcc`
### title : `x86-64: bitfields make inefficient indexing for array with 16 byte+ objects`
### open_at : `2022-01-10T16:44:21Z`
### last_modified_date : `2023-07-19T04:10:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103967
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
Hello, this problem is seemingly not specific to GCC and is probably well known. Loading or storing 16-byte (or larger) vector from array using a bitfield as an index generates code that can be noticeably smaller in theory.

        shr     esi, 12 ; shift bitfield
        and     esi, 31 ; mask bitfield
        sal     rsi, 4 ; unnecessary, also could drop REX prefix for size
        pxor    xmm0, XMMWORD PTR [rsi+1024+rdi] ; index + offset addressing

1) Second shift can be fused with bitfield load
2) Bitfield load can then be adjusted for shifted indexing (rsi*8)
3) Optionally, array offset can be precomputed if it's used twice or more, which can result in smaller and potencially faster code.

shr esi, 12 - 1 ; adjusted shift
and esi, 31 << 1 ; adjusted mask which fits in 8-bit immediate
pxor xmm0, [rdi + rsi * 8] ; precomputed array offset

https://godbolt.org/z/7aa7oaMhn

#include <emmintrin.h>
struct bitfields
{
    unsigned dummy : 7;
    unsigned a : 5;
    unsigned b : 5;
    unsigned c : 5;
};
struct context
{
    unsigned dummy[256];
    __m128i data[32];
};

void xor_data(context& ctx, bitfields op)
{
    ctx.data[op.c] = _mm_xor_si128(ctx.data[op.a], ctx.data[op.b]);
}


---


### compiler : `gcc`
### title : `x86: 4-way comparison of floats/doubles with spaceship operator possibly suboptimal`
### open_at : `2022-01-11T03:54:37Z`
### last_modified_date : `2022-01-18T03:17:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103973
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Hello, I may be missing something here but the generated code seems strange and suboptimal. It looks like all 4 possible paths can use flags from a single UCOMISD instruction, not calling it 3 times in worst case.

cmp4way(double, double):
        ucomisd xmm0, xmm1
        jp      .L8
        mov     eax, 0
        jne     .L8
.L2:
        ret
.L8:
        comisd  xmm1, xmm0
        mov     eax, -1
        ja      .L2
        ucomisd xmm0, xmm1
        setbe   al
        add     eax, 1
        ret

https://godbolt.org/z/j1j7G1MYP

#include <compare>

auto cmp4way(double a, double b)
{
    return a <=> b;
}


---


### compiler : `gcc`
### title : `[12 Regression] 541.leela_r slower by 4.5-6% with PGO+LTO -Ofast -march=native in the first week of January 2022`
### open_at : `2022-01-12T13:15:35Z`
### last_modified_date : `2022-01-19T09:56:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103990
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
LNT reports that 541.leela_r from SPEC 2017 intrate suite regressed
when compiled with both PGO and LTO with -Ofast -march=native on all
machines in the first week of January:

zen3: https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=477.397.0
zen2: https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=286.397.0
zen1: https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=17.397.0
kaby: https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=16.397.0

On my zen2 desktop I have bisected the regression, or at least most of
it, to  r12-6208-gebc853deb7cc04:

  ebc853deb7cc0487de9ef6e891a007ba853d1933 is the first bad commit
  commit ebc853deb7cc0487de9ef6e891a007ba853d1933
  Author: Richard Biener <rguenther@suse.de>
  Date:   Tue Jan 4 11:59:35 2022 +0100

    tree-optimization/103690 - not up-to-date SSA and PRE DCE

    This avoids running simple_dce_from_worklist on partially not up-to-date
    SSA form (in unreachable code regions) by scheduling CFG cleanup
    manually as is done anyway when tail-merging runs.

    2022-01-04  Richard Biener  <rguenther@suse.de>
            
            PR tree-optimization/103690
            * tree-pass.h (tail_merge_optimize): Adjust.
            * tree-ssa-tail-merge.c (tail_merge_optimize): Pass in whether
            to re-split critical edges, move CFG cleanup ...
            * tree-ssa-pre.c (pass_pre::execute): ... here, before
            simple_dce_from_worklist and delay freeing inserted_exprs from
            ...
            (fini_pre): .. here.


---


### compiler : `gcc`
### title : `[12 Regression] Recent vectorizer testsuite regressions on x86 since r12-6420 and r12-6523`
### open_at : `2022-01-12T23:05:58Z`
### last_modified_date : `2022-01-25T11:06:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103998
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
I'm seeing on x86_64-linux:
make check-gcc RUNTESTFLAGS="--target_board=unix\{-m32,-m64\} vect.exp=vect-tail-nomask-1.c i386.exp='pr88531* vect-reduc-1.c'"
rm -rf testsuite/gcc-parallel
make[1]: Entering directory '/usr/src/gcc/obj/gcc'
(rootme=`${PWDCMD-pwd}`; export rootme; \
srcdir=`cd ../../gcc; ${PWDCMD-pwd}` ; export srcdir ; \
if [ -n "" ] \
   && [ -n "$GCC_RUNTEST_PARALLELIZE_DIR" ] \
   && [ -f testsuite/gcc-parallel/finished ]; then \
  rm -rf testsuite/gcc; \
else \
  cd testsuite/gcc; \
  rm -f tmp-site.exp; \
  sed '/set tmpdir/ s|testsuite$|testsuite/gcc|' \
	< ../../site.exp > tmp-site.exp; \
  /bin/sh ${srcdir}/../move-if-change tmp-site.exp site.exp; \
  EXPECT=`if [ -f ${rootme}/../expect/expect ] ; then echo ${rootme}/../expect/expect ; else echo expect ; fi` ; export EXPECT ; \
  if [ -f ${rootme}/../expect/expect ] ; then  \
    TCL_LIBRARY=`cd .. ; cd ${srcdir}/../tcl/library ; ${PWDCMD-pwd}` ; \
    export TCL_LIBRARY ; \
  fi ; \
  `if [ -f ${srcdir}/../dejagnu/runtest ] ; then echo ${srcdir}/../dejagnu/runtest ; else echo runtest; fi` --tool gcc --target_board=unix\{-m32,-m64\} vect.exp=vect-tail-nomask-1.c i386.exp='pr88531* vect-reduc-1.c'; \
  if [ -n "$GCC_RUNTEST_PARALLELIZE_DIR" ] ; then \
    touch ${rootme}/testsuite/gcc-parallel/finished; \
  fi ; \
fi )
WARNING: Couldn't find the global config file.
Test run by jakub on Wed Jan 12 23:52:59 2022
Native configuration is x86_64-pc-linux-gnu

		=== gcc tests ===

Schedule of variations:
    unix/-m32
    unix/-m64

Running target unix/-m32
Using /usr/share/dejagnu/baseboards/unix.exp as board description file for target.
Using /usr/share/dejagnu/config/unix.exp as generic interface file for target.
Using /usr/src/gcc/gcc/testsuite/config/default.exp as tool-and-target-specific interface file.
Running /usr/src/gcc/gcc/testsuite/gcc.dg/vect/vect.exp ...
FAIL: gcc.dg/vect/vect-tail-nomask-1.c scan-tree-dump-times vect "LOOP EPILOGUE VECTORIZED \\(MODE=V16QI\\)" 2
FAIL: gcc.dg/vect/vect-tail-nomask-1.c -flto -ffat-lto-objects  scan-tree-dump-times vect "LOOP EPILOGUE VECTORIZED \\(MODE=V16QI\\)" 2
Running /usr/src/gcc/gcc/testsuite/gcc.target/i386/i386.exp ...
FAIL: gcc.target/i386/pr88531-1b.c scan-assembler-times vgatherdpd 4
FAIL: gcc.target/i386/pr88531-1b.c scan-assembler-times vmulpd 4
FAIL: gcc.target/i386/pr88531-1c.c scan-assembler-times vgatherdpd 4
FAIL: gcc.target/i386/pr88531-1c.c scan-assembler-times vmulpd 4
FAIL: gcc.target/i386/pr88531-2b.c scan-assembler-times vmulps 2
FAIL: gcc.target/i386/pr88531-2c.c scan-assembler-times vmulps 2
FAIL: gcc.target/i386/vect-reduc-1.c scan-tree-dump vect "LOOP EPILOGUE VECTORIZED"
FAIL: gcc.target/i386/vect-reduc-1.c scan-assembler-times padd 5

		=== gcc Summary for unix/-m32 ===

# of expected passes		16
# of unexpected failures	10
Running target unix/-m64
Using /usr/share/dejagnu/baseboards/unix.exp as board description file for target.
Using /usr/share/dejagnu/config/unix.exp as generic interface file for target.
Using /usr/src/gcc/gcc/testsuite/config/default.exp as tool-and-target-specific interface file.
Running /usr/src/gcc/gcc/testsuite/gcc.dg/vect/vect.exp ...
FAIL: gcc.dg/vect/vect-tail-nomask-1.c scan-tree-dump-times vect "LOOP EPILOGUE VECTORIZED \\(MODE=V16QI\\)" 2
FAIL: gcc.dg/vect/vect-tail-nomask-1.c -flto -ffat-lto-objects  scan-tree-dump-times vect "LOOP EPILOGUE VECTORIZED \\(MODE=V16QI\\)" 2
Running /usr/src/gcc/gcc/testsuite/gcc.target/i386/i386.exp ...
FAIL: gcc.target/i386/pr88531-1b.c scan-assembler-times vgatherqpd 4
FAIL: gcc.target/i386/pr88531-1b.c scan-assembler-times vmulpd 4
FAIL: gcc.target/i386/pr88531-1c.c scan-assembler-times vgatherqpd 4
FAIL: gcc.target/i386/pr88531-1c.c scan-assembler-times vmulpd 4
FAIL: gcc.target/i386/pr88531-2b.c scan-assembler-times vmulps 2
FAIL: gcc.target/i386/pr88531-2c.c scan-assembler-times vmulps 2
FAIL: gcc.target/i386/vect-reduc-1.c scan-tree-dump vect "LOOP EPILOGUE VECTORIZED"
FAIL: gcc.target/i386/vect-reduc-1.c scan-assembler-times padd 5

		=== gcc Summary for unix/-m64 ===

# of expected passes		16
# of unexpected failures	10

		=== gcc Summary ===

# of expected passes		32
# of unexpected failures	20
/usr/src/gcc/obj/gcc/xgcc  version 12.0.0 20220112 (experimental) (GCC) 

make[1]: Leaving directory '/usr/src/gcc/obj/gcc'

None of this was present when I bootstrapped/regtested 24 hours ago with r12-6420 reverted.
If I revert r12-6523 and r12-6420 on current trunk, then all these pass again.


---


### compiler : `gcc`
### title : `Vectorizer failed to reduce sum with conversion.`
### open_at : `2022-01-13T01:25:17Z`
### last_modified_date : `2022-01-13T08:35:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103999
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
Vectorizer failed for sum_reduce_cd, but ok for sum_reduce/sum_reduce_cint

float sum_reduce_cd (int n, float* array)
{
    float ans = 0.0f, x;
    for (int i = 0; i != n; i++)
      {
          x = array[i];
          ans += x + 1.0;
      }
    return ans;
}

float sum_reduce (int n, float* array)
{
    float ans = 0.0f, x;
    for (int i = 0; i != n; i++)
      {
          x = array[i];
          ans += x + 1.0f;
      }
    return ans;
}

float sum_reduce_cint (int n, float* array)
{
    int ans = 0;
    float x;
    for (int i = 0; i != n; i++)
      {
          x = array[i];
          ans += (int)(x + 1.0);
      }
    return ans;
}

https://godbolt.org/z/q3McP6d15


---


### compiler : `gcc`
### title : `[12 regression] short loop no longer vectorized with Neon after r12-3362`
### open_at : `2022-01-13T15:52:23Z`
### last_modified_date : `2022-04-19T14:43:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104010
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
This short loop:
void test_vcmpeq_s32x2 (int32_t * __restrict__ dest, int32_t *a, int32_t *b)
{
  int i;
  for (i=0; i<4; i++) {
    dest[i] = a[i] == b[i];
  }
}

used to be vectorized as:
test_vcmpeq_s32x2:
        vld1.32 {d16}, [r1]
        vmov.i32        d17, #0x1  @ v2si
        vld1.32 {d19}, [r2]
        vmov.i32        d18, #0  @ v2si
        vceq.i32        d16, d16, d19
        vbsl    d16, d17, d18
        vst1.32 {d16}, [r0]
        bx      lr

After r12-6513, we get:
test_vcmpeq_s32x2:
        ldr     ip, [r1]
        ldr     r3, [r1, #4]
        str     lr, [sp, #-4]!
        ldr     lr, [r2]
        ldr     r2, [r2, #4]
        sub     ip, ip, lr
        clz     ip, ip
        sub     r3, r3, r2
        lsr     ip, ip, #5
        clz     r3, r3
        lsr     r3, r3, #5
        str     ip, [r0]
        str     r3, [r0, #4]
        ldr     pc, [sp], #4

when compiling for arm-none-linux-gnueabihf with -mcpu=cortex-a9 -mfpu=neon


---


### compiler : `gcc`
### title : `Comparison against 0 propagates into other statement causing no-CSE from happening`
### open_at : `2022-01-13T23:23:36Z`
### last_modified_date : `2022-01-14T08:08:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104018
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
Take:
int f(unsigned int a, unsigned int b) {
    if (a > 0) {
        return a == b;
    } else {
        return a == b;
    }
}
int f2(unsigned int a, unsigned int b) {
    if (a > 1) {
        return a == b;
    } else {
        return a == b;
    }
}

They should produce the same results but don't currently as in the first case the >0 is turned into != 0 and then the 0 is propagated into the comparison and then the comparison in the other BBSs are not able to be CSE'ed with the other one.


---


### compiler : `gcc`
### title : `[10/11 Regression] AArch64 Redundant instruction moving general to vector register`
### open_at : `2022-01-15T03:27:25Z`
### last_modified_date : `2023-03-10T20:45:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104039
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Compiling the following code on AArch64 with -O2 or -O3:

typedef unsigned long u64x2 __attribute__((vector_size(16)));

u64x2 combine(unsigned long a, unsigned long b) {
	u64x2 v = {a,b};
    return v;
}

yields the following assembly:

combine:
        fmov    d0, x0
        ins     v0.d[1], x1
        ins     v0.d[1], x1
        ret

where the second ins is entirely redundant with the first and serves no apparent purpose.  (Unless it is something extremely clever...)

This seems to be a regression from 8.x to 9.x; Godbolt's 8.5 looks correct with just one ins, but 9.3 has the two.

Originally noticed by Peter Cordes on StackOverflow: https://stackoverflow.com/questions/70717360/how-to-load-vector-registers-from-integer-registers-in-arm64-m1/70718572#comment125016906_70717360


---


### compiler : `gcc`
### title : `[12/13/14 Regression] vec_select to subreg lowering causes superfluous moves`
### open_at : `2022-01-16T12:19:08Z`
### last_modified_date : `2023-07-27T09:22:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104049
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
Consider:

int test (uint8_t *p, uint32_t t[1][1], int n) {

  int sum = 0;
  uint32_t a0;
  for (int i = 0; i < 4; i++, p++)
    t[i][0] = p[0];

  for (int i = 0; i < 4; i++) {
    {
      int t0 = t[0][i] + t[0][i];
      a0 = t0;
    };
    sum += a0;
  }
  return (((uint16_t)sum) + ((uint32_t)sum >> 16)) >> 1;
}

Which after the reduction gets SLP'd used to generate at -O3

        addv    s0, v0.4s
        fmov    w0, s0
        lsr     w1, w0, 16
        add     w0, w1, w0, uxth
        lsr     w0, w0, 1

which was pretty good. However in GCC 12 we now generate worse code:

        addv    s0, v0.4s
        fmov    w0, s0
        fmov    w1, s0
        and     w0, w0, 65535
        add     w0, w0, w1, lsr 16
        lsr     w0, w0, 1

Notice the double transfer of the same value.

This is because at the RTL level the original mov becomes a vec_select

(insn 19 18 20 2 (set (reg:SI 102 [ _43 ])
        (vec_select:SI (reg:V4SI 117)
            (parallel [
                    (const_int 0 [0])
                ]))) -1
     (nil))

which previously stayed as a vec_select and the RA would use this pattern for the w -> r move.

Now however this vec_select gets transformed into a subreg 0, which causes combine to push the subreg into each instruction using reg 102.

(insn 21 18 22 2 (set (reg:SI 120)
        (and:SI (subreg:SI (reg:V4SI 117) 0)
            (const_int 65535 [0xffff]))) "/app/example.c":30:27 492 {andsi3}
     (nil))
(insn 22 21 28 2 (set (reg:SI 121)
        (plus:SI (lshiftrt:SI (subreg:SI (reg:V4SI 117) 0)
                (const_int 16 [0x10]))
            (reg:SI 120))) "/app/example.c":30:27 211 {*add_lsr_si}
     (expr_list:REG_DEAD (reg:SI 120)
        (expr_list:REG_DEAD (reg:V4SI 117)
            (nil))))

and because these operations don't exist on the w side, reload is forced to materialized many duplicate moves from w -> r.  So every operation that gets the subreg pushed into it for which we don't have an operation for on the w side gets an extra move.

Aside from that, we seem to lose that the & can be folded into the subreg by simply truncating the subreg from SI to HI and zero extending that out.

A different reproducer is

#include <arm_neon.h>

typedef int v4si __attribute__ ((vector_size (16)));

int bar (v4si x)
{
  unsigned int sum = vaddvq_s32 (x);
  return (((uint16_t)(sum & 0xffff)) + ((uint32_t)sum >> 16));
}

Note that using -frename-registers does get us to an optimal sequence here which is better than GCC 11.


---


### compiler : `gcc`
### title : `[12 Regression] 6-7% x264_r regression with -march=native -Ofast -funroll-loops -flto on x86 since r12-6420-gd3ff7420e941931d32ce2e332e7968fe67ba20af`
### open_at : `2022-01-17T02:04:14Z`
### last_modified_date : `2022-01-20T08:51:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104058
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `normal`
### contents :
We observed regression on 525.x264_r with commit d3ff7420e941931d32ce2e332e7968fe67ba20af

On IceLake(8358):
-7.27%

On Zen3(7763):
-6.67%

On Zen3(5800x):
-6.45%

The regression on Zen 3 can also be found in https://lnt.opensuse.org/db_default/v4/SPEC/graph?highlight_run=22984&plot.0=475.377.0


---


### compiler : `gcc`
### title : `[12 Regression] cprop_hardreg propgates hard registers for mov instructions between different REG_CLASS without considering cost`
### open_at : `2022-01-17T03:36:51Z`
### last_modified_date : `2022-02-15T06:37:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104059
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.0`
### severity : `normal`
### contents :
int test (uint8_t *p, uint32_t t[1][1], int n) {

  int sum = 0;
  uint32_t a0;
  for (int i = 0; i < 4; i++, p++)
    t[i][0] = p[0];

  for (int i = 0; i < 4; i++) {
    {
      int t0 = t[0][i] + t[0][i];
      a0 = t0;
    };
    sum += a0;
  }
  return (((uint16_t)sum) + ((uint32_t)sum >> 16)) >> 1;
}

testcase is from PR104049, for x86 with -O3 -mavx2 ,before cprop_hardregs it's

----before cprop_hardreg------
(insn 100 79 81 2 (set (reg:SI 1 dx [orig:90 stmp__9.14 ] [90])
        (reg:SI 20 xmm0 [114])) 81 {*movsi_internal}
     (expr_list:REG_DEAD (reg:SI 20 xmm0 [114])
        (nil)))
(debug_insn 81 100 96 2 (debug_marker) "/app/example.cpp":16:3 -1
     (nil))
(insn 96 81 82 2 (set (reg:SI 0 ax [116])
        (reg:SI 1 dx [orig:90 stmp__9.14 ] [90])) "/app/example.cpp":16:44 81 {*movsi_internal}
     (nil))
---end------------

------after cprop_hardreg--------
(insn 100 79 81 2 (set (reg:SI 1 dx [orig:90 stmp__9.14 ] [90])
        (reg:SI 20 xmm0 [114])) 81 {*movsi_internal}
     (nil))
(debug_insn 81 100 96 2 (debug_marker) "/app/example.cpp":16:3 -1
     (nil))
(insn 96 81 82 2 (set (reg:SI 0 ax [116])
        (reg:SI 20 xmm0 [orig:90 stmp__9.14 ] [90])) "/app/example.cpp":16:44 81 {*movsi_internal}
     (expr_list:REG_DEAD (reg:SI 20 xmm0 [orig:90 stmp__9.14 ] [90])
        (nil)))
------end--------------

it's
        vmovd   edx, xmm0
        movl   eax, edx
 
vs
        vmovd   edx, xmm0
        vmovd   eax, xmm0

vmovd is expensive for many x86 targets.


---


### compiler : `gcc`
### title : `Unused nothrow new not optimized`
### open_at : `2022-01-18T21:13:05Z`
### last_modified_date : `2022-01-19T10:28:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104105
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `enhancement`
### contents :
g++ can remove new calls when the result is unused, but fails to di the same with nothrow new calls. f() could be compiled to an empty function with optimisations on.

------------
void f() {
    new (std::nothrow) int(5);
    new (std::nothrow) int[5];
}
------------


---


### compiler : `gcc`
### title : `Fail to remove stores to VLA inside loops`
### open_at : `2022-01-18T22:25:22Z`
### last_modified_date : `2022-01-19T10:55:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=104106
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.0`
### severity : `enhancement`
### contents :
In the following snippet none of the loops are removed when compiled with -O2 or -O3.

In f and g the optimizers shoulds detect that tmp_a is only written and never read.

In h, only one index of tmp_a is read, so it should be the only one computed.

Ideally, if not too complex for gcc, the first two loops should be removed and the computations, if any, done on the last loop.

-------------
int f(char *a, unsigned n) {
    char tmp_a[n];

    for (unsigned i = 1; i != n; i++) tmp_a[i] = a[i];
    return a[0];
}

int g(char *a, int n) {
    char tmp_a[n];

    for (int i = 1; i < n; i++) tmp_a[i] = a[i] - a[i - 1];
    return a[0];
}

int h(char *a, int n) {
    char tmp_a[n];

    for (int i = 0; i < n; i++) tmp_a[i] = a[i];
    return tmp_a[1];
}

int i(char *a, char *b, int n) {
    char tmp_a[n];
    char tmp_b[n];

    for (int i = 1; i < n; i++) tmp_a[i] = a[i] - a[i - 1];
    for (int i = 1; i < n; i++) tmp_b[i] = b[i] - b[i - 1];
    
    int result = 0;
    for (int i = 1; i < n; i++) result += tmp_a[i] + tmp_b[i];

    return result;
}
---------------------


---
