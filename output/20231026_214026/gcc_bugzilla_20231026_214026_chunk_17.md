### Total Bugs Detected: 4649
### Current Chunk: 17 of 30
### Bugs in this Chunk: 160 (From bug 2561 to 2720)
---


### compiler : `gcc`
### title : `[meta-bug] store-merging and/or bswap load/store-merging missed optimizations`
### open_at : `2020-03-09T09:43:39Z`
### last_modified_date : `2023-07-13T22:43:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94094
### status : `NEW`
### tags : `meta-bug, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Bug tracking missed cases, both passes could/should be merged.


---


### compiler : `gcc`
### title : `[10 regression] r10-1734, SVN r273240, causes gcc.target/powerpc/pr87507.c to fail`
### open_at : `2020-03-10T14:53:36Z`
### last_modified_date : `2020-04-01T19:27:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94123
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Executing on host: /home3/seurer/gcc/git/build/gcc-test2/gcc/xgcc -B/home3/seurer/gcc/git/build/gcc-test2/gcc/ /home/seurer/gcc/git/gcc-test2/gcc/testsuite/gcc.target/powerpc/pr87507.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -fdiagnostics-urls=never  -O2 -mdejagnu-cpu=power8 -ffat-lto-objects -fno-ident -S -o pr87507.s    (timeout = 300)
spawn -ignore SIGHUP /home3/seurer/gcc/git/build/gcc-test2/gcc/xgcc -B/home3/seurer/gcc/git/build/gcc-test2/gcc/ /home/seurer/gcc/git/gcc-test2/gcc/testsuite/gcc.target/powerpc/pr87507.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -O2 -mdejagnu-cpu=power8 -ffat-lto-objects -fno-ident -S -o pr87507.s
PASS: gcc.target/powerpc/pr87507.c (test for excess errors)
gcc.target/powerpc/pr87507.c: \\mstd\\M found 6 times
FAIL: gcc.target/powerpc/pr87507.c scan-assembler-times \\mstd\\M 4
FAIL: gcc.target/powerpc/pr87507.c scan-assembler-not \\mld\\M
Executing on host: /home3/seurer/gcc/git/build/gcc-test2/gcc/xgcc -B/home3/seurer/gcc/git/build/gcc-test2/gcc/ vmx_hw_available12365.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -fdiagnostics-urls=never  -mno-vsx  -lm  -o vmx_hw_available12365.exe    (timeout = 300)
spawn -ignore SIGHUP /home3/seurer/gcc/git/build/gcc-test2/gcc/xgcc -B/home3/seurer/gcc/git/build/gcc-test2/gcc/ vmx_hw_available12365.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -mno-vsx -lm -o vmx_hw_available12365.exe
Setting LD_LIBRARY_PATH to :/home3/seurer/gcc/git/build/gcc-test2/gcc::/home3/seurer/gcc/git/build/gcc-test2/gcc:/home/seurer/gcc/git/build/gcc-test2/./gmp/.libs:/home/seurer/gcc/git/build/gcc-test2/./prev-gmp/.libs:/home/seurer/gcc/git/build/gcc-test2/./mpfr/src/.libs:/home/seurer/gcc/git/build/gcc-test2/./prev-mpfr/src/.libs:/home/seurer/gcc/git/build/gcc-test2/./mpc/src/.libs:/home/seurer/gcc/git/build/gcc-test2/./prev-mpc/src/.libs:/home/seurer/gcc/git/build/gcc-test2/./isl/.libs:/home/seurer/gcc/git/build/gcc-test2/./prev-isl/.libs
Execution timeout is: 300
spawn [open ...]
testcase /home/seurer/gcc/git/gcc-test2/gcc/testsuite/gcc.target/powerpc/powerpc.exp completed in 1 seconds

		=== gcc Summary ===

# of expected passes		1
# of unexpected failures	2


The test now generates a bunch more loads and stores.  Looks like it is spilling some registers.

Old code (r10-7092):

.LFB0:
	.cfi_startproc
	cmpdi 0,3,0
	beqlr 0
	mr 10,6
	mr 11,5
	addi 3,4,16
	mr 8,10
	mr 9,11
	std 11,0(4)
	std 10,8(4)
	std 9,0(3)
	std 8,8(3)
	blr


New code (r10-7093):

.LFB0:
	.cfi_startproc
	cmpdi 0,3,0
	beqlr 0
	std 30,-16(1)
	std 31,-8(1)
	.cfi_offset 30, -16
	.cfi_offset 31, -8
	mr 30,6
	mr 31,5
	addi 9,4,16
	mr 10,30
	mr 11,31
	std 31,0(4)
	std 30,8(4)
	std 11,0(9)
	std 10,8(9)
	ld 30,-16(1)
	ld 31,-8(1)
	.cfi_restore 31
	.cfi_restore 30
	blr


---


### compiler : `gcc`
### title : `GCC loses track of SHIFT optimization`
### open_at : `2020-03-11T03:07:50Z`
### last_modified_date : `2020-03-11T03:34:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94133
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
https://godbolt.org/z/pkXsfc

void foo1(unsigned sh, const uint64_t src[3], uint64_t dst[])
{
  uint64_t w0 = src[0];
  uint64_t w1 = src[1];
  dst[0] = (uint64_t)(((((unsigned __int128)w1 << 64) | w0) << sh % 64) >> 64);
}

foo1:
  mov rax, QWORD PTR [rsi+8]
  mov r8, rdx
  mov rdx, QWORD PTR [rsi]
  mov ecx, edi
  shld rax, rdx, cl
  mov QWORD PTR [r8], rax
  ret

void foo2(unsigned sh, const uint64_t src[3], uint64_t dst[])
{
  uint64_t w0 = src[0];
  uint64_t w1 = src[1];
  uint64_t w2 = src[2];
  dst[1] = (uint64_t)(((((unsigned __int128)w2 << 64) | w1) << sh % 64) >> 64);
  dst[0] = (uint64_t)(((((unsigned __int128)w1 << 64) | w0) << sh % 64) >> 64);
}

foo2:
  mov r9, QWORD PTR [rsi+8]
  mov r8, rdx
  mov ecx, edi
  mov rdx, QWORD PTR [rsi+16]
  and ecx, 63
  mov r10, QWORD PTR [rsi]
  mov rax, r9
  shld rdx, r9, cl
  sal rax, cl
  test cl, 64
  cmovne rdx, rax
  mov rax, r10
  sal rax, cl
  mov QWORD PTR [r8+8], rdx
  mov rdx, r9
  shld rdx, r10, cl
  test cl, 64
  cmovne rdx, rax
  mov QWORD PTR [r8], rdx
  ret


---


### compiler : `gcc`
### title : `PPC: subfic instead of neg used for rotate right`
### open_at : `2020-03-11T07:06:50Z`
### last_modified_date : `2022-03-08T16:20:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94135
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
Input:

unsigned int rotr32(unsigned int v, unsigned int r)
{
   return (v>>r)|(v<<(32-r));
}

unsigned long long rotr64(unsigned long long v, unsigned long long r)
{
   return (v>>r)|(v<<(64-r));
}

Command line:
gcc -O2 -save-temps rotr.C

Output:
_Z6rotr32jj:
.LFB0:
        .cfi_startproc
        subfic 4,4,32
        rotlw 3,3,4
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0
        .cfi_endproc

_Z6rotr64yy:
.LFB1:
        .cfi_startproc
        subfic 4,4,64
        rotld 3,3,4
        blr
        .long 0
        .byte 0,9,0,0,0,0,0,0
        .cfi_endproc

subfic is a 2 cycle instruction, but can be replaced by 1 cycle instruction neg.
rotr32(v,r) = rotl32(v,32-r) = rotl32(v,(32-r)%32) = rotl32(v,(-r)%32))= rotl32(v,-r) as long as you have a modulo rotate like rotlw/rlwnm.

Same for 64-bit.


---


### compiler : `gcc`
### title : `Longcalls mis-optimize loading the function address`
### open_at : `2020-03-11T16:00:40Z`
### last_modified_date : `2020-05-01T01:19:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94145
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
I'm working on a feature where we convert some/all built-in function calls to use the longcall sequence.  I discovered that the compiler is mis-optimizing loading up the function address.  This showed up in the Spec 2017 wrf_r benchmark where I replaced some 60,000 direct calls to longcalls.

In particular, the PowerPC backend is not marking the load of the function address as being volatile.  This allows the compiler to move the load out of a loop.

However with the current ELF semantics, you don't want to do this because the function address changes.  The first call to the function, the address is the PLT stub, but in subsequent calls it is the address of the function itself after the shared library is loaded.

In addition, because UNSPECs are used, the compiler is likely to store the function address in the stack and reload it.  Given that the UNSPEC is just a load, it would be better not to optimize this to doing the extra load/store.

In fixing the linker bug that this feature uncovered, Alan Modra has a simple patch to fix it.


---


### compiler : `gcc`
### title : `[10 Regression] Merging functions with same bodies stopped working`
### open_at : `2020-03-11T16:11:24Z`
### last_modified_date : `2020-03-12T10:38:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94146
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Consider the example:

extern int x , y;

int ternary(int i) { return i > 0 ? x : y; }
int ternary2(int i) { return i > 0 ? x : y; }


GCC9 was merging the functions with -O2:

ternary2(int):
        jmp     ternary(int)


With GCC10 merging at -O2 is missing and function bodies are duplicated even for very big functions: https://godbolt.org/z/2kH8VR


---


### compiler : `gcc`
### title : `[RISCV] Superfluous stackpointer manipulation`
### open_at : `2020-03-14T09:04:14Z`
### last_modified_date : `2021-08-16T01:15:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94173
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 48033
demonstrate how bar function yields superfluous stackpointer instructions

GCC emits stackpointer decrement/increment instructions in some functions where they are superfluous.

Example:

struct Pair {
    char *s;
    char *t;
};
typedef struct Pair Pair;

Pair bar(char *s, char *t)
{
    return (Pair){s, t};
}

Expected assembly:

0000000000000002 <bar>:
   2:	8082                	ret


Actual assembly:

0000000000000002 <bar>:
   2:	1141                	addi	sp,sp,-16
   4:	0141                	addi	sp,sp,16
   6:	8082                	ret


Notes:

In an example where the function just returns one value the assembly code actually just contain the expected assenbly code:


char *foo(char *s, char *t)
{
    return s;
}

Note that per the RISC-V calling-conventions up to 2 integer like values are returned via the a0/a1 registers, i.e. the same registers where also the first 2 function arguments are placed.


FWIW, Clang 9.0.1 generates for both examples the expected code without superfluous stackpointer adjustments.


Example code is attched.

I tested with:

riscv64-linux-gnu-gcc -O3 -c -o pair.o pair.c


---


### compiler : `gcc`
### title : `Missed ccmp optimizations`
### open_at : `2020-03-14T09:48:25Z`
### last_modified_date : `2023-06-02T14:24:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94174
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
(1) Case 1:

void doit(void);
void test(unsigned long a, unsigned long l)
{
  if (!__builtin_add_overflow(a, 8 - 1, &a) && a <= l)
    doit();
}

currently generates as

        adds    x0, x0, #7
        cset    x2, cs
        cmp     x0, x1
        eor     w2, w2, 1
        cset    w0, ls
        tst     w0, w2
        bne     .L22

but could be

        adds    x2, x0, #7
        ccmp    x2, x1, #0, cc
        b.ls    .L22


---


### compiler : `gcc`
### title : `[10 Regression] Passing constexpr empty class variable to function since r10-599`
### open_at : `2020-03-14T12:49:07Z`
### last_modified_date : `2020-03-20T04:04:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94175
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.0`
### severity : `normal`
### contents :
Since r10-599-gc652ff83124334837dc16626f9e1040e4fe41fc9 following testcase with -O2 -m32 (or -O0 -m32 or any arch other than x86_64 64-bit):
struct Foo
{
  struct NoStartBar {};
  static constexpr NoStartBar noStartBar = NoStartBar();
  [[gnu::noinline, gnu::noclone]] Foo(int x, NoStartBar) : f (x) { f++; }
  Foo(double x) : Foo(0, noStartBar) {}
  int f;
};
Foo a = 6.0;
needs the Foo::noStartBar definition, while before it didn't and just passed NoStartBar {}.
Is passing the static data member an ODR use of it or not?
In any case, looks like a missed optimization, there shouldn't be reason to copy the empty class from a variable rather than just passing uninitialized bytes.


---


### compiler : `gcc`
### title : `Global variable inline constructor elision.`
### open_at : `2020-03-15T19:36:21Z`
### last_modified_date : `2022-10-17T14:11:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94184
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `9.2.0`
### severity : `enhancement`
### contents :
The following code:

    struct A { 
        int a_; 
        A(int a) : a_(a) {}
    };
    
    struct B { 
        int a_; 
        constexpr B(int a) : a_(a) {}
    };
    
    A a{1};
    B b{1};

When compiled with `gcc-9.2 -O3 -march=skylake -mtune=skylake -std=gnu++17` generates a call to `A` constructor for variable `a`:

    _GLOBAL__sub_I_a:
        mov    DWORD PTR [rip+0x200bd6],0x1        # 601030 <__TMC_END__>
        ret    

And no call for `B` constructor for variable `b`, as expected of `constexpr`.

`clang-9.0 -O3 -march=skylake -mtune=skylake -std=gnu++17`, however, elides the constructor calls for both `a` and `b` global variables, regardless of `constexpr`.

Why can't `gcc` elide the call to `A` constructor for `a`, since `A` constuctor is an inline function with a clear side-effect?

https://gcc.godbolt.org/z/5y3cD5


---


### compiler : `gcc`
### title : `powerpc: Provide fegetround/feraiseexcept/feclearexcept builtins`
### open_at : `2020-03-16T15:19:01Z`
### last_modified_date : `2022-01-27T12:46:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94193
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
For hard-float powerpc, GCC should support inline code generation for the  fegetround, feraiseexcept, and feclearexcept. These optimization has been provided by glibc through fenvinline and glibc idea is to remove both the headers and the header optimizations [1].

[1] https://sourceware.org/pipermail/libc-alpha/2020-March/111752.html


---


### compiler : `gcc`
### title : `x86: Provide feraiseexcept builtins`
### open_at : `2020-03-16T15:21:07Z`
### last_modified_date : `2020-06-26T19:34:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94194
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
GCC should support inline code generation for feraiseexcept. These optimization has been provided by glibc through fenv.h and glibc idea is to remove both the headers and the optimization [1].

[1] https://sourceware.org/pipermail/libc-alpha/2020-March/111753.html


---


### compiler : `gcc`
### title : `m68k: Provide builtins for mathemactical functions`
### open_at : `2020-03-17T16:34:16Z`
### last_modified_date : `2020-03-18T08:06:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94204
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
For hard-float, GCC should support inline code generation for the trunc, floor, ceil, isinf, finite, scalbn, isnan, scalbln, nearbyint, lrint, and sincos.

These optimization has been provided by glibc through mathinline.h headers and glibc idea is to remove both the header and the optimizations [1].

[1] https://sourceware.org/pipermail/libc-alpha/2018-March/092351.html


---


### compiler : `gcc`
### title : `missed ccp folding for (addr + 8 * n) - (addr + 8 * (n - 1))`
### open_at : `2020-03-20T11:14:05Z`
### last_modified_date : `2023-10-24T03:57:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94234
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
CCP could not fold the following expression to a constant "8":

size_t foo (char *a, size_t n)
{
  char *b1 = a + 8 * n;
  char *b2 = a + 8 * (n - 1);

  return b1 - b2;
}

But if we change b1 and b2 to integer type, folding happens.

size_t foo (char *a, size_t n)
{
  size_t b1 = (size_t)(a + 8 * n);
  size_t b2 = (size_t)(a + 8 * (n - 1));

  return b1 - b2;
}


---


### compiler : `gcc`
### title : `Missed C++ front-end devirtualizations from Clang testsuite`
### open_at : `2020-03-20T21:12:55Z`
### last_modified_date : `2020-03-23T07:33:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94243
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.0`
### severity : `normal`
### contents :
While working on PR93347 I noticed that we do not devirtualize the following testcases that clang's testsuite tests to be devirtualized:

namespace Test2a {
  struct A {
    virtual ~A() final {}
    virtual int f();
  };

  // CHECK-LABEL: define i32 @_ZN6Test2a1fEPNS_1AE
  int f(A *a) {
    // CHECK: call i32 @_ZN6Test2a1A1fEv
    return a->f();
  }
}

Here I guess the final destructor makes the whole class final?

namespace Test4 {
  struct A {
    virtual void f();
    virtual int operator-();
  };

  struct B final : A {
    virtual void f();
    virtual int operator-();
  };

  // CHECK-LABEL: define void @_ZN5Test41fEPNS_1BE
  void f(B* d) {
    // CHECK: call void @_ZN5Test41B1fEv
    static_cast<A*>(d)->f();
    // CHECK: call i32 @_ZN5Test41BngEv
    -static_cast<A&>(*d);
  }
}

Her I am not sure, I think parameter d may point to instance of struct A,
so is it Clang's bug to devirtualize?

namespace Test5 {
  struct A {
    virtual void f();
    virtual int operator-();
  };

  struct B : A {
    virtual void f();
    virtual int operator-();
  };

  struct C final : B {
  };

  // CHECK-LABEL: define void @_ZN5Test51fEPNS_1CE
  void f(C* d) {
    // FIXME: It should be possible to devirtualize this case, but that is
    // not implemented yet.
    // CHECK: getelementptr
    // CHECK-NEXT: %[[FUNC:.*]] = load
    // CHECK-NEXT: call void %[[FUNC]]
    static_cast<A*>(d)->f();
  }
  // CHECK-LABEL: define void @_ZN5Test53fopEPNS_1CE
  void fop(C* d) {
    // FIXME: It should be possible to devirtualize this case, but that is
    // not implemented yet.
    // CHECK: getelementptr
    // CHECK-NEXT: %[[FUNC:.*]] = load
    // CHECK-NEXT: call i32 %[[FUNC]]
    -static_cast<A&>(*d);
  }
}

this seems similar to me.
namespace Test7 {
  struct foo {
    virtual void g() {}
  };

  struct bar {
    virtual int f() { return 0; }
  };

  struct zed final : public foo, public bar {
    int z;
    virtual int f() {return z;}
  };

  // CHECK-LABEL: define i32 @_ZN5Test71fEPNS_3zedE
  int f(zed *z) {
    // CHECK: alloca
    // CHECK-NEXT: store
    // CHECK-NEXT: load
    // CHECK-NEXT: call i32 @_ZN5Test73zed1fEv
    // CHECK-NEXT: ret
    return static_cast<bar*>(z)->f();
  }
}

namespace Test8 {
  struct A { virtual ~A() {} };
  struct B {
    int b;
    virtual int foo() { return b; }
  };
  struct C final : A, B {  };
  // CHECK-LABEL: define i32 @_ZN5Test84testEPNS_1CE
  int test(C *c) {
    // CHECK: %[[THIS:.*]] = phi
    // CHECK-NEXT: call i32 @_ZN5Test81B3fooEv(%"struct.Test8::B"* %[[THIS]])
    return static_cast<B*>(c)->foo();
  }
}

namespace Test9 {
  struct A {
    int a;
  };
  struct B {
    int b;
  };
  struct C : public B, public A {
  };
  struct RA {
    virtual A *f() {
      return 0;
    }
    virtual A *operator-() {
      return 0;
    }
  };
  struct RC final : public RA {
    virtual C *f() {
      C *x = new C();
      x->a = 1;
      x->b = 2;
      return x;
    }
    virtual C *operator-() {
      C *x = new C();
      x->a = 1;
      x->b = 2;
      return x;
    }
  };
  // CHECK: define {{.*}} @_ZN5Test91fEPNS_2RCE
  A *f(RC *x) {
    // FIXME: It should be possible to devirtualize this case, but that is
    // not implemented yet.
    // CHECK: load
    // CHECK: bitcast
    // CHECK: [[F_PTR_RA:%.+]] = bitcast
    // CHECK: [[VTABLE:%.+]] = load {{.+}} [[F_PTR_RA]]
    // CHECK: [[VFN:%.+]] = getelementptr inbounds {{.+}} [[VTABLE]], i{{[0-9]+}} 0
    // CHECK-NEXT: %[[FUNC:.*]] = load {{.+}} [[VFN]]
    return static_cast<RA*>(x)->f();
  }
  // CHECK: define {{.*}} @_ZN5Test93fopEPNS_2RCE
  A *fop(RC *x) {
    // FIXME: It should be possible to devirtualize this case, but that is
    // not implemented yet.
    // CHECK: load
    // CHECK: bitcast
    // CHECK: [[F_PTR_RA:%.+]] = bitcast
    // CHECK: [[VTABLE:%.+]] = load {{.+}} [[F_PTR_RA]]
    // CHECK: [[VFN:%.+]] = getelementptr inbounds {{.+}} [[VTABLE]], i{{[0-9]+}} 1
    // CHECK-NEXT: %[[FUNC:.*]] = load {{.+}} [[VFN]]
    // CHECK-NEXT: = call {{.*}} %[[FUNC]]
    return -static_cast<RA&>(*x);
  }
}

namespace Test10 {
  struct A {
    virtual int f();
  };

  struct B : A {
    int f() final;
  };

  // CHECK-LABEL: define i32 @_ZN6Test101fEPNS_1BE
  int f(B *b) {
    // CHECK: call i32 @_ZN6Test101B1fEv
    return static_cast<A *>(b)->f();
  }
}


---


### compiler : `gcc`
### title : `std::filebuf is extremely (at least 10x) slow on windows compared to Linux. Even much slower MSVC STL with terrible ABI.`
### open_at : `2020-03-23T09:08:22Z`
### last_modified_date : `2020-10-28T20:29:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94268
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.0`
### severity : `normal`
### contents :
Even the hacks work the same result.

https://bitbucket.org/ejsvifq_mabmip/fast_io/src/reserver_test/benchmarks/0000.10m_size_t/unit/filebuf_io_observer.cc

D:\hg\w4\f8\fast_io\benchmarks\0000.10m_size_t\unit>g++ -o filebuf_io_observer filebuf_io_observer.cc -Ofast -std=c++2a -s

D:\hg\w4\f8\fast_io\benchmarks\0000.10m_size_t\unit>filebuf_io_observer
output: 0.5130060000000001s
input:  0.256011s

running the same program on Linux WSL2

cqwrteur@Home-Server:~/myhome/fast_io/benchmarks/0000.10m_size_t/unit$ g++ -o filebuf_io_observer filebuf_io_observer.cc
 -Ofast -std=c++2a -s
cqwrteur@Home-Server:~/myhome/fast_io/benchmarks/0000.10m_size_t/unit$ ./filebuf_io_observer
output: 0.058395978s
input:  0.06603426700000001s

It is not possible to be an optimization problem since my code has no difference on windows and linux. I think the only reason is FILE*'s issue.
I guess the problem is just that code relies on unknown libc does not correctly on windows. One explanation might be the underlining FILE* buffer size's problem. I think you guys need to try larger buffer size.

Same program builds with MSVC STL (Their ABIs are terrible)
MSVC STL with hacking:
D:\hg\w4\f8\fast_io\benchmarks\0000.10m_size_t\unit>filebuf_io_observer
output: 0.119666s
input:  0.2497127s

Other comparison benchmarks

GCC 10.0.1 with msvcrt hacking.
https://bitbucket.org/ejsvifq_mabmip/fast_io/src/reserver_test/include/fast_io_legacy_impl/c/msvcrt.h

D:\hg\w4\f8\fast_io\benchmarks\0000.10m_size_t\unit>c_file_unlocked
output: 0.09216600000000001s
input:  0.12230700000000001s

GCC


---


### compiler : `gcc`
### title : `fold phi whose incoming args are defined from binary operations`
### open_at : `2020-03-23T11:42:07Z`
### last_modified_date : `2023-09-16T02:23:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94274
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
For if/else structure, 
Example 1:

int test(int cond, int a, int b, int c)
{
  int result = 0;

  if (cond)
    result = a + b;
  else
    result = a + c;
  return result;
}

The expressions is binary operation and have a common subexpression "a", and the opcode is the same.

E.g. on aarch64, gcc will do the binary operation first, and then do csel:

cmp     w0, 0
add     w0, w1, w2
add     w1, w1, w3
csel    w0, w1, w0, eq

In fact, it can be optimized to do csel first and then do binary operations:

cmp     w0, 0
csel    w2, w2, w3, ne
add     w0, w2, w1

This can eliminate one instruction. This scenario is very common, and the switch/case structure is the same.

Example 2:

int test(int cond, int a, int b, int c, int d)
{
  int result = 0;

  switch (cond) {
    case 1:
      result = a + b;
      break;
    case 8:
      result = a + c;
      break;
    default:
      result = a + d;
      break;
  }
  return result;
}

gcc will do the binary operation first, and then do csel :

        mov     w5, w0
        add     w0, w1, w2
        cmp     w5, 1
        beq     .L1
        add     w4, w1, w4
        cmp     w5, 8
        add     w1, w1, w3
        csel    w0, w1, w4, eq
.L1:
        ret

Which can further optimized into :

        cmp     w0, 1
        beq     .L3
        cmp     w0, 8
        csel    w4, w4, w3, ne
        add     w0, w1, w4
        ret
.L3:
        mov     w4, w2
        add     w0, w1, w4
        ret

My proposal: fold the merging phi node in tree_ssa_phiopt_worker (ssa-phiopt) :

For example 1:

replaces

bb0:
  if (cond) goto bb1; else goto bb2;
bb1:
  x1 = a + b;
  goto <bb3>
bb2:
  x2 = a + c;
bb3:
  x = PHI <x1 (bb1), x2 (bb2), ...>;

with

bb0:
  if (cond) goto bb1; else goto bb2;
bb1:
bb2:
bb3:
  x3 = PHI <b (bb1), c (bb2), ...>;
  x = a + x3;


For example 2:

replaces

bb0:
  if (cond == 1) goto bb2; else goto bb1;
bb1:
  if (cond == 8) goto bb3; else goto bb4;
bb2:
  x2 = a + b;
  goto <bb5>
bb3:
  x3 = a + c;
  goto <bb5>
bb4:
  x4 = a + d;
bb5:
  x5 = PHI <x2 (bb2), x3 (bb3), x4 (bb4), ...>;

with

bb0:
  if (cond == 1) goto bb2; else goto bb1;
bb1:
  if (cond == 8) goto bb3; else goto bb4;
bb2:
bb3:
bb4:
bb5:
  x5 = PHI <b (bb2), c (bb3), c (bb4), ...>;
  x = a + x5;

I have an initial implementation that is under testing. In part, it based on the LLVM InstCombinePass(InstCombinePHI.cpp).

Any suggestions?


---


### compiler : `gcc`
### title : `[missed optimization] Useless statements populating local string not removed`
### open_at : `2020-03-23T21:07:32Z`
### last_modified_date : `2020-03-24T08:43:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94293
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
(Relevant Godbolt: https://godbolt.org/z/GygbjZ)

This is the first of two apparent bugs manifesting when compiling the following program:

#include <string>
  
int bar() {
    struct poor_mans_pair {
        int first;
        std::string second;
    };
    poor_mans_pair p { 
        123, "Hey... no small-string optimization for me please!" };
    return p.first;
}

For x86_64, this would ideally compile into:

bar():
        mov     eax, 123
        ret

but when compiling this  with GCC 10.0.1 20200322 (or GCC 9.x etc.), we get assembly which calls operator new[](), populates the string, calls operator delete[](), then returns 123:

bar():
        sub     rsp, 8
        mov     edi, 51
        call    operator new(unsigned long)
        movdqa  xmm0, XMMWORD PTR .LC0[rip]
        mov     esi, 51
        mov     rdi, rax
        movups  XMMWORD PTR [rax], xmm0
        movdqa  xmm0, XMMWORD PTR .LC1[rip]
        movups  XMMWORD PTR [rax+16], xmm0
        movdqa  xmm0, XMMWORD PTR .LC2[rip]
        movups  XMMWORD PTR [rax+32], xmm0
        mov     eax, 8549
        mov     WORD PTR [rdi+48], ax
        mov     BYTE PTR [rdi+50], 0
        call    operator delete(void*, unsigned long)
        mov     eax, 123
        add     rsp, 8
        ret
.LC0:
        .quad   7935393319309894984
        .quad   3273110194895396975
.LC1:
        .quad   8007513861377913971
        .quad   8386118574366356592
.LC2:
        .quad   2338053640980164457
        .quad   8314037903514690925

 This bug report is about the population of the string, i.e. let's ignore the question of whether any memory should be allocated at all.

g++ should be aware that the string has no visibility outside `bar()` (except through access using raw arbitrary memory addresses from another while `bar()` is executing). Also, IANALL, even if the allocation can be considered observable behavior which needs to be maintained - values at that memory location, which may transiently be present, do not constitute such behavior. Why even set those values, therefore, when they are not used? At least these string constants and population statements should be optimized away, into something like (hand-written assembly):

bar():
        sub     rsp, 8
        mov     edi, 51
        call    operator new(unsigned long)
        mov     rdi, rax
        mov     esi, 51
        call    operator delete(void*, unsigned long)
        mov     eax, 123
        add     rsp, 8
        ret


---


### compiler : `gcc`
### title : `[missed optimization] new+delete of unused local string not removed`
### open_at : `2020-03-23T21:21:30Z`
### last_modified_date : `2023-07-26T16:42:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94294
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
(Relevant Godbolt: https://godbolt.org/z/GygbjZ)

This is the second of two apparent bugs, following bug 94293. They both manifest when compiling the following program:

#include <string>
  
int bar() {
    struct poor_mans_pair {
        int first;
        std::string second;
    };
    poor_mans_pair p { 
        123, "Hey... no small-string optimization for me please!" };
    return p.first;
}

For x86_64, this would ideally compile into:

bar():
        mov     eax, 123
        ret

but when compiling this  with GCC 10.0.1 20200322 (or GCC 9.x etc.), we get assembly which calls operator new[](), populates the string, calls operator delete[](), then returns 123:

bar():
        sub     rsp, 8
        mov     edi, 51
        call    operator new(unsigned long)
        movdqa  xmm0, XMMWORD PTR .LC0[rip]
        mov     esi, 51
        mov     rdi, rax
        movups  XMMWORD PTR [rax], xmm0
        movdqa  xmm0, XMMWORD PTR .LC1[rip]
        movups  XMMWORD PTR [rax+16], xmm0
        movdqa  xmm0, XMMWORD PTR .LC2[rip]
        movups  XMMWORD PTR [rax+32], xmm0
        mov     eax, 8549
        mov     WORD PTR [rdi+48], ax
        mov     BYTE PTR [rdi+50], 0
        call    operator delete(void*, unsigned long)
        mov     eax, 123
        add     rsp, 8
        ret
.LC0:
        .quad   7935393319309894984
        .quad   3273110194895396975
.LC1:
        .quad   8007513861377913971
        .quad   8386118574366356592
.LC2:
        .quad   2338053640980164457
        .quad   8314037903514690925

This bug report is about how the allocation and de-allocation are not elided/optimized-away, even though the std::string variable is local and unused.

AFAICT, g++ is not required to do this. And, in fact, clang++ doesn't do this with its libc++. cppreference says that, starting in C++14,

> New-expressions are allowed to elide or combine allocations made 
> through replaceable allocation functions. In case of elision, the
> storage may be provided by the compiler without making the call to 
> an allocation function (this also permits optimizing out unused
> new-expression)

and this is, indeed, the case of an unused new-expression. Well, eventually-unused. 

Note: I suppose it's theoretically possible that this bug only manifests because   bug 94293 prevents the allocated space from being recognized as unused; but I can't tell whether that's the case.


---


### compiler : `gcc`
### title : `use __builtin_operator_new and __builtin_operator_delete when available`
### open_at : `2020-03-23T23:16:46Z`
### last_modified_date : `2023-06-09T14:37:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94295
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `libstdc++`
### version : `unknown`
### severity : `enhancement`
### contents :
See https://bugs.llvm.org/show_bug.cgi?id=45287 for some background.

The C++ language rules do not permit optimization (eg, deletion) of direct calls to 'operator new' and 'operator delete'. libstdc++ uses such calls to implement std::allocator:

https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/ext/new_allocator.h#L112

As a consequence, allocations performed by libstdc++'s containers are not optimizable.

Clang provides a pair of builtin functions to work around this issue: https://clang.llvm.org/docs/LanguageExtensions.html#builtin-operator-new-and-builtin-operator-delete

__builtin_operator_new(args) is equivalent to ::operator new(args) except that it permits optimizations.
__builtin_operator_delete(args) is equivalent to ::operator delete(args) except that it permits optimizations.

You can detect support for these builtins with

#ifdef __has_builtin
#if __has_builtin(__builtin_operator_new) >= 201802L
// ...
#endif
#endif

(Note that __has_builtin(...) returned 1 for an older version of the builtins that didn't support placement forms, and so couldn't be used for aligned allocation and sized delete. It's probably not worth your time dealing with those.)

This bug requests that libstdc++ uses these builtins when available. (Separately, it'd be great if GCC considered supporting them too.)


---


### compiler : `gcc`
### title : `x86 duplicates loads`
### open_at : `2020-03-24T10:17:34Z`
### last_modified_date : `2020-03-30T14:42:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94298
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
For the following testcase at -O3 -fgimple (gimple testcase because the
vectorizer generated code depends on not committed patches) we somehow
duplicate the load from y:

typedef double v2df __attribute__((vector_size(16)));
typedef long v2di __attribute__((vector_size(16)));
double x[1024], y[1024];
void __GIMPLE (ssa,guessed_local(10737416))
foo ()
{
  v2df * vectp_x7;
  v2df vect__56;
  v2df vect__45;
  v2df * vectp_y3;
  v2df _12;
  v2df _13;
  unsigned int _19;
  unsigned int _24;

  __BB(2,guessed_local(10737416)):
  goto __BB3(precise(134217728));

  __BB(3,loop_header(1),guessed_local(1063004409)):
  vectp_y3_21 = __PHI (__BB2: &y, __BB3: vectp_y3_17);
  vectp_x7_6 = __PHI (__BB2: &x, __BB3: vectp_x7_20);
  _19 = __PHI (__BB2: 0u, __BB3: _24);
  vect__45_14 = __MEM <v2df> ((double *)vectp_y3_21);
  _13 = __VEC_PERM (vect__45_14, vect__45_14, _Literal (v2di) { 1l, 1l });
  _12 = __VEC_PERM (vect__45_14, vect__45_14, _Literal (v2di) { 0l, 0l });
  vect__56_7 = _12 + _13;
  __MEM <v2df> ((double *)vectp_x7_6) = vect__56_7;
  vectp_y3_17 = vectp_y3_21 + 16ul;
  vectp_x7_20 = vectp_x7_6 + 16ul;
  _24 = _19 + 1u;
  if (_24 != 512u)
    goto __BB3(adjusted(132875551));
  else
    goto __BB4(adjusted(1342177));

  __BB(4,guessed_local(10737416)):
  return;

}


results in

foo:
.LFB0:
        .cfi_startproc
        xorl    %eax, %eax
        .p2align 4,,10
        .p2align 3
.L2:
        movapd  y(%rax), %xmm1
        movapd  y(%rax), %xmm0
        addq    $16, %rax
        unpcklpd        %xmm1, %xmm1
        unpckhpd        %xmm0, %xmm0
        addpd   %xmm1, %xmm0
        movaps  %xmm0, x-16(%rax)
        cmpq    $8192, %rax
        jne     .L2
        ret

The duplication happens in IRA/LRA but I suspect either x86 costing or
operand constraints makes them think this is cheaper.  LRA is fed with

(insn 8 6 11 3 (set (reg/v:V2DF 85 [ vect__45 ])
        (mem:V2DF (plus:DI (reg:DI 86 [ ivtmp.6 ])
                (symbol_ref:DI ("y") [flags 0x2]  <var_decl 0x7f2a94b8dbd0 y>)) [1 MEM[symbol: y, index: ivtmp.6_7, offset: 0B]+0 S16 A128])) 1338 {movv2df_internal}
     (expr_list:REG_EQUIV (mem:V2DF (plus:DI (reg:DI 86 [ ivtmp.6 ])
                (symbol_ref:DI ("y") [flags 0x2]  <var_decl 0x7f2a94b8dbd0 y>)) [1 MEM[symbol: y, index: ivtmp.6_7, offset: 0B]+0 S16 A128])
        (nil)))
(insn 11 8 12 3 (set (reg:V2DF 89)
        (vec_select:V2DF (vec_concat:V4DF (reg/v:V2DF 85 [ vect__45 ])
                (reg/v:V2DF 85 [ vect__45 ]))
            (parallel [
                    (const_int 0 [0])
                    (const_int 2 [0x2])
                ]))) "t2.c":27:3 2995 {*vec_interleave_lowv2df}
     (nil))
(insn 12 11 13 3 (set (reg:V2DF 90)
        (vec_select:V2DF (vec_concat:V4DF (reg/v:V2DF 85 [ vect__45 ])
                (reg/v:V2DF 85 [ vect__45 ]))
            (parallel [
                    (const_int 1 [0x1])
                    (const_int 3 [0x3])
                ]))) "t2.c":27:3 2989 {*vec_interleave_highv2df}
     (expr_list:REG_DEAD (reg/v:V2DF 85 [ vect__45 ])
        (nil)))
(insn 13 12 14 3 (set (reg:V2DF 91 [ vect__56 ])
        (plus:V2DF (reg:V2DF 89)
            (reg:V2DF 90))) "t2.c":27:3 1519 {*addv2df3}
     (expr_list:REG_DEAD (reg:V2DF 90)
        (expr_list:REG_DEAD (reg:V2DF 89)
            (expr_list:REG_EQUIV (mem:V2DF (plus:DI (reg:DI 86 [ ivtmp.6 ])
                        (symbol_ref:DI ("x") [flags 0x2]  <var_decl 0x7f2a94b8db40 x>)) [1 MEM[symbol: x, index: ivtmp.6_7, offset: 0B]+0 S16 A128])
                (nil)))))
(insn 14 13 15 3 (set (mem:V2DF (plus:DI (reg:DI 86 [ ivtmp.6 ])
                (symbol_ref:DI ("x") [flags 0x2]  <var_decl 0x7f2a94b8db40 x>)) [1 MEM[symbol: x, index: ivtmp.6_7, offset: 0B]+0 S16 A128])
        (reg:V2DF 91 [ vect__56 ])) "t2.c":27:3 1338 {movv2df_internal}
     (expr_list:REG_DEAD (reg:V2DF 91 [ vect__56 ])
        (nil)))

and the LRA:

         Choosing alt 3 in insn 11:  (0) x  (1) 0  (2) m {*vec_interleave_lowv2df}
      Creating newreg=92 from oldreg=89, assigning class SSE_REGS to r92
   11: r92:V2DF=vec_select(vec_concat(r92:V2DF,[r86:DI+`y']),parallel)
    Inserting insn reload before:
   26: r92:V2DF=[r86:DI+`y']
    Inserting insn reload after:
   27: r89:V2DF=r92:V2DF

and postreload CSE cannot do anything because the shuffle clobbers the
reg we loaded into (only sched2 moves things in a way that CSE would
be possible again but after sched2 there's no CSE anymore).


---


### compiler : `gcc`
### title : `Missed vector-vector CTOR / permute simplification`
### open_at : `2020-03-24T14:43:09Z`
### last_modified_date : `2021-12-12T13:26:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94301
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
When the vectorizer creates sth stupid like

  _11 = __MEM <vector(1) double> ((double *)vectp_y.3_4);
  vect__2.5_15 = _Literal (vector(2) double) {_11, _Literal (vector(1) double) { 0.0 }};
  vectp_y.3_16 = vectp_y.3_4 + 16ul;
  _17 = __MEM <vector(1) double> ((double *)vectp_y.3_16);
  vect__2.6_18 = _Literal (vector(2) double) {_17, _Literal (vector(1) double) { 0.0 }};
  vect__2.7_19 = __VEC_PERM (vect__2.5_15, vect__2.6_18, _Literal (vector(2) ssizetype) { 0l, 2l });
  _20 = __VEC_PERM (vect__2.7_19, vect__2.7_19, _Literal (vector(2) ssizetype) { 0l, 0l });
  _21 = __VEC_PERM (vect__2.7_19, vect__2.7_19, _Literal (vector(2) ssizetype) { 1l, 1l });

we fail to combine those instructions to

  _20 = { _11, _11 };
  _21 = { _17, _17 };

and instead end up with

  _11 = MEM[symbol: y, index: ivtmp.13_10, offset: _Literal (double *) 0];
  vect__2.5_15 = _Literal (vector(2) double) {_11, _Literal (vector(1) double) { 0.0 }};
  _17 = MEM[symbol: y, index: ivtmp.13_10, offset: _Literal (double *) 16];
  vect__2.7_19 = __BIT_INSERT (vect__2.5_15, _17, 64u);
  _20 = __VEC_PERM (vect__2.7_19, vect__2.7_19, _Literal (vector(2) ssizetype) { 0l, 0l });
  _21 = __VEC_PERM (vect__2.7_19, vect__2.7_19, _Literal (vector(2) ssizetype) { 1l, 1l });

where RTL expansion even ICEs on when trying to expand the __BIT_INSERT,
probably because of the V1DFmode insert which eventually ends up as
BLKmode to store_bit_field:

#4  0x0000000000d27c83 in store_bit_field (str_rtx=0x7ffff6dab000, bitsize=..., bitnum=..., 
    bitregion_start=..., bitregion_end=..., fieldmode=E_BLKmode, value=0x7ffff6da3888, 
    reverse=false) at ../../src/trunk/gcc/expmed.c:1174


That said, the vector-vector CTORs are probably unhandled in forwprop
simplifications.


---


### compiler : `gcc`
### title : `[10 Regression] invalid AVX512VL vpternlogd instruction emitted for -march=knl`
### open_at : `2020-03-26T15:24:35Z`
### last_modified_date : `2020-03-30T16:06:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94343
### status : `RESOLVED`
### tags : `missed-optimization, wrong-code`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Test case (`-O1 -march=knl`, cf. https://godbolt.org/z/qQc3Sf):

using W [[gnu::vector_size(16)]] = long long;
using V [[gnu::vector_size(16)]] = int;

auto f(V a) {
    return __builtin_ia32_pandn128(reinterpret_cast<W>(~V() ^ a), ~W());
}


This emits a XMM variant of `vpternlogd` which requires AVX512VL. But it was supposed to compile for KNL.

Besides the bug, there's also a missed optimization here: `~V() ^ a` flips all bits and pandn flips all bits again. Thus it should compile to a single `ret` instruction. Note that the variation:

auto f(V a) {
    return ~reinterpret_cast<W>(~V() ^ a) & ~W();
}

compiles to

  vpternlogd xmm0, xmm0, xmm0, 0x55
  vpternlogq xmm0, xmm0, xmm0, 0x55

for KNL.


---


### compiler : `gcc`
### title : `[9 Regression] Rotate pattern not recognized anymore`
### open_at : `2020-03-26T16:14:14Z`
### last_modified_date : `2020-04-03T06:44:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94344
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.3.1`
### severity : `normal`
### contents :
Consider the following MWE:

int64_t f (int64_t x)                                                                                                                                                         
{                                                                                                                                                                             
  int64_t y = x & -49;                                                                                                                                                      
  return (y << 5) | (int64_t)((uint64_t)y >> 59);                                                                                                                           
}

Prior to patch c4c5ad1d6d1 combine successfully recognized the rotate pattern and suggested amongst other the following insn (using `gcc -O3 -S -fdump-rtl-combine-details test.c`):

(set (reg:DI 67)                                                                                                                                                              
    (rotate:DI (reg/v:DI 64 [ y ])                                                                                                                                            
        (const_int 5 [0x5])))

However, with patch c4c5ad1d6d1 applied, this is not the case anymore. Thus, only combinations of shifts+and+ior are emitted.

This is reproducible for s390x on HEAD (16948c54b75) by simply reverting the patch. Any idea how we could tweak combine back to the old behaviour where it detected a rotate successfully?


---


### compiler : `gcc`
### title : `Missed optimisation: useless multiplication generated for pointer comparison`
### open_at : `2020-03-27T13:27:04Z`
### last_modified_date : `2022-12-12T20:09:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94356
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `enhancement`
### contents :
The closest existing ticket I found for this one is https://gcc.gnu.org/bugzilla/show_bug.cgi?id=48316 but this seems different enough, although it might be linked.

Consider the function:

typedef int t[100000];

int f(t *p, long long o) {
    return p < p+o;
}

GCC 9.3 with -O2, targeting x86-64, correctly simplifies by p, but still generates a 64x64->64 multiplication in the computation of the offset:

f:
        imulq   $400000, %rsi, %rsi
        xorl    %eax, %eax
        testq   %rsi, %rsi
        setg    %al
        ret

(Compiler Explorer link: https://gcc.godbolt.org/z/_pT8E- )

Clang 10 avoids generating the multiplication on this example:

f:                                      # @f
        xorl    %eax, %eax
        testq   %rsi, %rsi
        setg    %al
        retq

A variant of this example is this other function g with two offsets:

int g(t *p, long long o1, long long o2) {
    return p+o1 < p+o2;
}

Clang generates the same code as for “o1<o2”, 
whereas GCC generates two multiplications:

g:
        imulq   $400000, %rsi, %rsi
        xorl    %eax, %eax
        imulq   $400000, %rdx, %rdx
        cmpq    %rdx, %rsi
        setl    %al
        ret

Compiler Explorer link: https://gcc.godbolt.org/z/sDJyHP


---


### compiler : `gcc`
### title : `505.mcf_r is 8% faster when compiled with -mprefer-vector-width=128`
### open_at : `2020-03-27T18:06:59Z`
### last_modified_date : `2020-04-02T14:37:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94364
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
SPEC 2017 INTrate benchmark 505.mcf_r, when compiled with options
-Ofast -march=native -mtune=native, is 8% slower than when we also use
option -mprefer-vector-width=128.  I have observed it on both AMD Zen2
and Intel Cascade Lake Server CPUs (using master revision 26b3e568a60).

Better vector width selection would therefore bring about noticeable
speed-up.


Symbol profiles (collected on AMD Rome):

-Ofast -march=native -mtune=native:

  Overhead       Samples  Shared Object    Symbol                          
  ........  ............  ...............  ................................
 
    28.64%        462302  mcf_r_peak.mine  spec_qsort
    21.58%        348703  mcf_r_peak.mine  cost_compare
    15.81%        255029  mcf_r_peak.mine  primal_bea_mpp
    15.58%        251176  mcf_r_peak.mine  replace_weaker_arc
     7.37%        118646  mcf_r_peak.mine  arc_compare
     6.53%        105337  mcf_r_peak.mine  price_out_impl
     1.38%         22276  mcf_r_peak.mine  update_tree

-Ofast -march=native -mtune=native -mprefer-vector-width=128:

  Overhead       Samples  Shared Object    Symbol                          
  ........  ............  ...............  ................................

    23.57%        354536  mcf_r_peak.mine  spec_qsort
    23.51%        353767  mcf_r_peak.mine  cost_compare
    16.98%        255104  mcf_r_peak.mine  primal_bea_mpp
    16.65%        249891  mcf_r_peak.mine  replace_weaker_arc
     7.29%        109267  mcf_r_peak.mine  arc_compare
     7.09%        106380  mcf_r_peak.mine  price_out_impl
     1.53%         22968  mcf_r_peak.mine  update_tree


---


### compiler : `gcc`
### title : `548.exchange2_r run time is 16-35% worse than GCC 9 at -O2 and generic march/mtune`
### open_at : `2020-03-27T21:49:19Z`
### last_modified_date : `2023-01-18T16:57:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94373
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
When compiled with just -O2, SPEC 2017 INTrate benchmark
548.exchange2_r runs slower than when compiled with GCC 9.2. It is:

-  8% slower on AMD Zen2-based server CPU (rev. 26b3e568a60)
- 12% slower on Intel Cascade Lake server CPU (rev. abe13e1847f)
-  7% slower on AMD Zen1-based server CPU (rev. 26b3e568a60)

During GCC 10 development cycle the benchmark was relatively noisy and
the run time was increasing in many small steps, but between October 7
and November 15 we were doing 3% better than GCC 9 (on Zen2).
Specifically the following commit brought about the improvement:

  commit 806bdf4e40d31cf55744c876eb9f17654de36b99
  Author: Richard Biener <rguenther@suse.de>
  Date:   Mon Oct 7 07:53:45 2019 +0000

    re PR tree-optimization/91975 (worse code for small array copy using pointer arithmetic than array indexing)
    
    2019-10-07  Richard Biener  <rguenther@suse.de>
    
            PR tree-optimization/91975
            * tree-ssa-loop-ivcanon.c (constant_after_peeling): Consistently
            handle invariants.
    
    From-SVN: r276645

But it was undone by its revert:

  commit f0af4848ac40d2342743c9b16416310d61db85b5
  Author: Richard Biener <rguenther@suse.de>
  Date:   Fri Nov 15 09:09:16 2019 +0000

    re PR tree-optimization/92039 (Spurious -Warray-bounds warnings building 32-bit glibc)

    2019-11-15  Richard Biener  <rguenther@suse.de>
            
            PR tree-optimization/92039
            PR tree-optimization/91975
            * tree-ssa-loop-ivcanon.c (constant_after_peeling): Revert
            previous change, treat invariants consistently as non-constant.
            (tree_estimate_loop_size): Ternary ops with just the first op
            constant are not optimized away.
            
            * gcc.dg/tree-ssa/cunroll-2.c: Revert to state previous to
            unroller adjustment.
            * g++.dg/tree-ssa/ivopts-3.C: Likewise.

    From-SVN: r278281

On the Intel machine, reverting the revert fixes the regression too.


---


### compiler : `gcc`
### title : `Powerpc suboptimal 64-bit constant comparison`
### open_at : `2020-03-29T23:02:57Z`
### last_modified_date : `2023-10-08T03:08:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94393
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
--- test case ----
void test1(unsigned long a)
{ 
        if (a > 0xc000000000000000ULL)
                printf("yes\n");
}
void test2(unsigned long a)
{
        if (a >= 0xc000000000000000ULL)
                printf("yes\n");
}
------------------

The first (important part) compiles to

        li 9,-1
        rldicr 9,9,0,1
        cmpld 0,3,9
        blelr 0

The second to

        lis 9,0xbfff
        ori 9,9,0xffff
        sldi 9,9,32
        oris 9,9,0xffff
        ori 9,9,0xffff
        cmpld 0,3,9
        blelr 0

The second could use the same 2-insn constant as the first, but with bltlr.


---


### compiler : `gcc`
### title : `Powerpc suboptimal 64-bit constant generation near large values with few bits set`
### open_at : `2020-03-30T05:34:18Z`
### last_modified_date : `2023-10-08T03:12:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94395
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
0xc000000000000000UL is generated with

        li 9,-1
        rldicr 9,9,0,1

0xbfffffffffffffffUL (0xc000000000000000UL - 1) is

        lis 9,0xbfff
        ori 9,9,0xffff
        sldi 9,9,32
        oris 9,9,0xffff
        ori 9,9,0xffff

Could be

        li 9,-1
        rldicr 9,9,0,1
        subi 9,9,1


---


### compiler : `gcc`
### title : `Missed optimization bswap`
### open_at : `2020-03-30T15:40:51Z`
### last_modified_date : `2021-07-19T03:41:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94403
### status : `REOPENED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
full code:
https://godbolt.org/z/zjNqYV

template <typename T>
auto reverse(T num) {

    // misses optimization when num is int32_t OK for int64_t
    auto* bytes = reinterpret_cast<char*>(&num);

    // misses optimization for both 32 and 64 bit ints
    //auto* bytes = reinterpret_cast<std::byte*>(&num);
    
    constexpr auto size = sizeof(num);
    for (int i = 0; i < size / 2; i++) {
        std::swap(bytes[i], bytes[size-i-1]);
    }

    return num;
}


---


### compiler : `gcc`
### title : `only `--` gives constexpr`
### open_at : `2020-03-30T21:58:56Z`
### last_modified_date : `2021-12-14T08:54:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94414
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
x86-64 gcc (trunk)
-std=c++20 -O3

`++` - is not constexpr
`--` - is constexpr

==========================================
#include <ranges>
#include <array>

namespace rv = std::views;

int main()
{
    constexpr std::array arr{5, 6, 7, 8};
    auto contfilt = arr | rv::filter([](auto x){return x>5;})
    | rv::transform([](auto x){return x*x;});
    return *(++contfilt.begin());
}

===========================================

main:
        movdqa  xmm0, XMMWORD PTR .LC0[rip]
        lea     rax, [rsp-16]
        movaps  XMMWORD PTR [rsp-24], xmm0
        jmp     .L3
.L6:
        add     rax, 4
        lea     rdx, [rsp-8]
        cmp     rax, rdx
        je      .L2
.L3:
        cmp     DWORD PTR [rax], 5
        jle     .L6
.L2:
        mov     eax, DWORD PTR [rax]
        imul    eax, eax
        ret
.LC0:
        .long   5
        .long   6
        .long   7
        .long   8
=========================================

Okay, but ...

========================================

#include <ranges>
#include <array>

namespace rv = std::views;

int main()
{
    constexpr std::array arr{5, 6, 7, 8};
    auto contfilt = arr | rv::filter([](auto x){return x>5;})
    | rv::transform([](auto x){return x*x;});
    return *(----contfilt.end());
}

========================================

main:
        mov     eax, 49
        ret

========================================

Okay, but.....

========================================

#include <ranges>
#include <array>

namespace rv = std::views;

int main()
{
    constexpr std::array arr{5, 6, 7, 8};
    auto contfilt = arr | rv::filter([](auto x){return x>5;})
    | rv::transform([](auto x){return x*x;});
    return *(++(------contfilt.end()));
}

========================================

main:
        mov     eax, 49
        ret


---


### compiler : `gcc`
### title : `passing a restricted pointer to a function can be assumed not to modify an accessed object`
### open_at : `2020-03-30T23:21:11Z`
### last_modified_date : `2020-03-31T07:49:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94416
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
In the test case below, the subtraction can safely be folded to zero because a is a restricted pointer (as is effectively also b), *a is read, and if (a == b) were true, the call g(b) couldn't also modify *a either via *(int*)b or by any other means; as a result, a == b must either be false or g(b) cannot modify *a.  Clang folds the subtraction but GCC does not.

$ cat c.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout c.c
void g (void *);

int f (int * restrict a, void * /* restrict */ b)
{
  int t = *a;

  g (b);

  return *a - t;   // can be folded to zero
}

;; Function f (f, funcdef_no=0, decl_uid=1933, cgraph_uid=1, symbol_order=0)

f (int * restrict a, void * restrict b)
{
  int t;
  int _1;
  int _7;

  <bb 2> [local count: 1073741824]:
  t_4 = *a_3(D);
  g (b_5(D));
  _1 = *a_3(D);
  _7 = _1 - t_4;
  return _7;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] Redundant loads/stores emitted at -O3`
### open_at : `2020-04-01T12:57:22Z`
### last_modified_date : `2023-08-04T17:21:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94442
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Test case:

#include <arm_neon.h>

struct __m256i
{
  int8x16_t vect_s8[2];
};

__attribute__((inline)) __m256i _mm256_adds_epi8(__m256i a, __m256i b)
{
    __m256i res_m256i;
    res_m256i.vect_s8[0] = vqaddq_s8(a.vect_s8[0], b.vect_s8[0]);
    res_m256i.vect_s8[1] = vqaddq_s8(a.vect_s8[1], b.vect_s8[1]);
    return res_m256i;
}

void PerfTest1(__m256i *output, unsigned caseCount)
{
    unsigned loopCount = caseCount;
    __m256i& a = output[0];
    __m256i& b = output[1];
    __m256i& c = output[2];
    for (unsigned i = 0; i < loopCount; i++) {
        a = _mm256_adds_epi8(b, c);
        b = _mm256_adds_epi8(a, c);
        c = _mm256_adds_epi8(c, b);
        a = _mm256_adds_epi8(b, c);
        b = _mm256_adds_epi8(a, c);
        c = _mm256_adds_epi8(c, b);
        a = _mm256_adds_epi8(b, c);
        b = _mm256_adds_epi8(a, c);
        c = _mm256_adds_epi8(c, b);
        b = _mm256_adds_epi8(a, c);
    }
}

Command line (GCC version 10.0): aarch64-linux-gnu-g++ -S -O3 a.c

.L6:
        ldp     q3, q2, [x2]
        add     w4, w4, 1
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        stp     q1, q0, [x0]
        ldp     q3, q2, [x2]
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        stp     q1, q0, [x0, 32]
        ldp     q3, q2, [x2]
        sqadd   v3.16b, v3.16b, v1.16b
        sqadd   v2.16b, v2.16b, v0.16b
        stp     q3, q2, [x0, 64]
        ldp     q1, q0, [x3]
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        stp     q1, q0, [x0]
        ldp     q3, q2, [x2]
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        stp     q1, q0, [x0, 32]
        ldp     q3, q2, [x2]
        sqadd   v3.16b, v3.16b, v1.16b
        sqadd   v2.16b, v2.16b, v0.16b
        stp     q3, q2, [x0, 64]
        ldp     q1, q0, [x3]
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        stp     q1, q0, [x0]
        ldp     q2, q3, [x2]
        sqadd   v4.16b, v1.16b, v2.16b
        sqadd   v5.16b, v0.16b, v3.16b
        stp     q4, q5, [x0, 32]
        ldp     q2, q3, [x2]
        sqadd   v3.16b, v3.16b, v5.16b
        sqadd   v2.16b, v2.16b, v4.16b
        sqadd   v0.16b, v0.16b, v3.16b
        sqadd   v1.16b, v1.16b, v2.16b
        stp     q2, q3, [x0, 64]
        stp     q1, q0, [x0, 32]
        cmp     w1, w4
        bne     .L6

And command line (GCC version 10.0): aarch64-linux-gnu-g++ -S -O1 a.c
Or (GCC version 9.2.0): aarch64-linux-gnu-g++ -S -O3 a.c

.L4:
        ldr     q0, [x0, 48]
        ldr     q2, [x0, 80]
        ldr     q1, [x0, 32]
        ldr     q3, [x0, 64]
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        sqadd   v3.16b, v3.16b, v1.16b
        sqadd   v2.16b, v2.16b, v0.16b
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        sqadd   v3.16b, v3.16b, v1.16b
        sqadd   v2.16b, v2.16b, v0.16b
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        str     q1, [x0]
        str     q0, [x0, 16]
        sqadd   v5.16b, v1.16b, v3.16b
        sqadd   v4.16b, v0.16b, v2.16b
        sqadd   v3.16b, v3.16b, v5.16b
        sqadd   v2.16b, v2.16b, v4.16b
        str     q3, [x0, 64]
        str     q2, [x0, 80]
        sqadd   v1.16b, v1.16b, v3.16b
        sqadd   v0.16b, v0.16b, v2.16b
        str     q1, [x0, 32]
        str     q0, [x0, 48]
        add     w3, w3, 1
        cmp     w1, w3
        bne     .L4

This issue triggers after commit
https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=3b47da42de621c6c3bf7d2f9245df989aa7eb5a1

This commit changes the gimple from
  a = MEM[(const struct __m256i &)output_5(D) + 32];
  a$vect_s8$0_4 = MEM <int8x16_t> [(const struct __m256i &)output_5(D) + 32];
  a$vect_s8$1_6 = MEM <int8x16_t> [(const struct __m256i &)output_5(D) + 48];
  b = MEM[(const struct __m256i &)output_5(D) + 64];
  b$vect_s8$0_9 = MEM <int8x16_t> [(const struct __m256i &)output_5(D) + 64];
  b$vect_s8$1_11 = MEM <int8x16_t> [(const struct __m256i &)output_5(D) + 80];
  _76 = a$vect_s8$0_4;
  _77 = b$vect_s8$0_9;
To
  a = MEM[(const struct __m256i &)output_5(D) + 32];
  a$vect_s8$0_4 = MEM[(const struct __m256i &)output_5(D) + 32].vect_s8[0];  <========
  a$vect_s8$1_6 = MEM[(const struct __m256i &)output_5(D) + 32].vect_s8[1];  <========
  b = MEM[(const struct __m256i &)output_5(D) + 64];
  b$vect_s8$0_9 = MEM[(const struct __m256i &)output_5(D) + 64].vect_s8[0];  <========
  b$vect_s8$1_11 = MEM[(const struct __m256i &)output_5(D) + 64].vect_s8[1];  <========
  _76 = a$vect_s8$0_4;
  _77 = b$vect_s8$0_9;

When expand to RTL, the latter form will emit two insns.
(insn 23 22 24 6 (set (reg/f:DI 140)
        (plus:DI (reg/v/f:DI 133 [ output ])
            (const_int 64 [0x40]))) -1
     (nil))
(insn 24 23 25 6 (set (reg:V16QI 94 [ b$vect_s8$1 ])
        (mem:V16QI (plus:DI (reg/f:DI 140)
                (const_int 16 [0x10])) [0 MEM[(const struct __m256i &)output_5(D) + 64]+16 S16 A128])) -1
     (nil))

And later in rtl pre pass, insn 23 will be extracted outside the loop as a common subexpression.
This will cause in dse pass it cannot determine whether the following two insns reference the same location.
(insn 33 32 36 5 (set (mem:V16QI (plus:DI (reg/v/f:DI 133 [ output ])
                (const_int 16 [0x10])) [1 MEM <int8x16_t> [(struct __m256i *)output_5(D) + 16B]+0 S16 A128])
        (reg:V16QI 114 [ _35 ])) "a.c":23:34 1203 {*aarch64_simd_movv16qi}
     (nil))
(insn 36 33 41 5 (set (reg:V16QI 116 [ b$vect_s8$1 ])
        (mem:V16QI (plus:DI (reg/f:DI 194)
                (const_int 16 [0x10])) [0 MEM[(const struct __m256i &)output_5(D) + 64]+16 S16 A128])) 1203 {*aarch64_simd_movv16qi}
     (nil))

Because insn
(insn 140 5 130 4 (set (reg/f:DI 194)
        (plus:DI (reg/v/f:DI 133 [ output ])
            (const_int 64 [0x40]))) 121 {*adddi3_aarch64}
     (nil))

has just be extracted to another bb in rtl pre pass and dse pass is unable to get this information.
Thus dse pass cannot eliminate these extra STRs.

I would like to solve this problem by propagating insn 23 to its use in fwprop pass.
However, there exists some restrictions here. I try to modify like this:
diff --git a/gcc/fwprop.c b/gcc/fwprop.c
index 705d2885aae..0edbbc65047 100644
--- a/gcc/fwprop.c
+++ b/gcc/fwprop.c
@@ -416,7 +416,7 @@ should_replace_address (rtx old_rtx, rtx new_rtx, machine_mode mode,
     gain = (set_src_cost (new_rtx, VOIDmode, speed)
            - set_src_cost (old_rtx, VOIDmode, speed));

-  return (gain > 0);
+  return (gain >= 0);
 }


@@ -1573,10 +1573,14 @@ fwprop (bool fwprop_addr_p)
       df_ref use = DF_USES_GET (i);
       if (use)
        {
+         df_ref def = get_def_for_use (use);
          if (DF_REF_TYPE (use) == DF_REF_REG_USE
              || DF_REF_BB (use)->loop_father == NULL
              /* The outer most loop is not really a loop.  */
-             || loop_outer (DF_REF_BB (use)->loop_father) == NULL)
+             || loop_outer (DF_REF_BB (use)->loop_father) == NULL
+             || (def && (DF_REF_BB (def)->loop_father == DF_REF_BB (use)->loop_father
+                         || flow_loop_nested_p (DF_REF_BB(use)->loop_father,
+                                                DF_REF_BB(def)->loop_father))))
            forward_propagate_into (use, fwprop_addr_p);

          else if (fwprop_addr_p)

some discussion mails here
https://gcc.gnu.org/pipermail/gcc/2020-March/231980.html


---


### compiler : `gcc`
### title : `Branchless clamp in the general case gets a branch in a particular case ?`
### open_at : `2020-04-06T09:35:30Z`
### last_modified_date : `2023-07-20T08:38:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94497
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
(Triage note: I think this is probably a compiler middle-end or back-end issue, but I am not knowledgeable enough about the structure of the GCC codebase to pick the right component.)

---

I am trying to make a floating-point computation autovectorization-friendly, without mandating the use of -ffast-math for optimal performance as that is a numerical stability and compiler portability hazard. This turned out to be an interesting exercise in IEEE-754 pedantry, of course, but I can live with that.

However, while trying to optimize a "clamp" computation, I ended up at a point where the behavior of the GCC optimizer just does not make sense to me and I could use the opinion of an expert.

Consider the following functions:

```
double fast_min(double x, double y) {
    return (x < y) ? x : y;
}

double fast_max(double x, double y) {
    return (x > y) ? x : y;
}
```

The definitions of fast_min and fast_max are carefully crafted to match the semantics of x86's min and max instruction family, and indeed if I compile this code with -O1 or above I get minsd/maxsd or vminsd/vmaxsd instructions depending on which vector instruction sets are enabled.

This is exactly what I wanted, so far I'm happy. And if I now try to use these min and max functions to write a clamp function...

```
double fast_clamp(double x, double min, double max) {
    return fast_max(fast_min(x, max), min);
}
```

...again, at -O1 optimization level and above, I get a minsd/maxsd pair, short and sweet:

```
fast_clamp(double, double, double):
        minsd   xmm0, xmm2
        maxsd   xmm0, xmm1
        ret
```

Where this perfect picture becomes tainted, however, is as soon as I try to _use_ this function with certain min/max arguments.

```
double use_fast_clamp(double x) {
    return fast_clamp(x, 0.0, 1.0);
}
```

All of a sudden, the assembly becomes branchy and terrible-looking, even in -O3 mode!

```
use_fast_clamp(double):
        movapd  xmm1, xmm0
        movsd   xmm0, QWORD PTR .LC0[rip]
        comisd  xmm0, xmm1
        jbe     .L13
        maxsd   xmm1, QWORD PTR .LC1[rip]
        movapd  xmm0, xmm1
.L13:
        ret
.LC0:
        .long   0
        .long   1072693248
.LC1:
        .long   0
        .long   0
```

I can make the generated code go back to a minsd/maxsd pair if I enable -ffast-math (more precisely -ffinite-math-only -funsafe-math-optimizations), but to the best of my knowledge, I shouldn't need fast-math flags here.

Further, even if I did forget about an IEEE-754 oddity that requires fast-math flags, it would still mean that the above compilation of the general fast_clamp function is incorrect: if this compilation output should work for any pair of "min" and "max" double-precision arguments, then it trivially should work when the min is 0.0 and max is 1.0. So one way or another, I think the GCC optimizer is doing something strange here.

---

This is the most minimal example of this behavior that I managed to come up with. Using only the fast_min or fast_math functions in isolation will behave as expected and codegen into a single minsd or maxsd:

```
double use_fast_min(double x) {
    return fast_min(x, 1.0);
}

double use_fast_max(double x) {
    return fast_max(x, 0.0);
}
```

I observed similar behavior on any GCC build I could get my hands on, all the way from the most recent GCC trunk build currently available on godbolt (10.0.1 20200405) to the most ancient build provided by godbolt (4.1.2).

Both my local system and godbolt run are Linux-based.

My local GCC build was configured with  ../configure --prefix=/usr --infodir=/usr/share/info --mandir=/usr/share/man --libdir=/usr/lib64 --libexecdir=/usr/lib64 --enable-languages=c,c++,objc,fortran,obj-c++,ada,go,d --enable-offload-targets=hsa,nvptx-none=/usr/nvptx-none, --without-cuda-driver --disable-werror --with-gxx-include-dir=/usr/include/c++/9 --enable-ssp --disable-libssp --disable-libvtv --disable-cet --disable-libcc1 --enable-plugin --with-bugurl=https://bugs.opensuse.org/ --with-pkgversion='SUSE Linux' --with-slibdir=/lib64 --with-system-zlib --enable-libstdcxx-allocator=new --disable-libstdcxx-pch --enable-libphobos --enable-version-specific-runtime-libs --with-gcc-major-version-only --enable-linker-build-id --enable-linux-futex --enable-gnu-indirect-function --program-suffix=-9 --without-system-libunwind --enable-multilib --with-arch-32=x86-64 --with-tune=generic --with-build-config=bootstrap-lto-lean --enable-link-mutex --build=x86_64-suse-linux --host=x86_64-suse-linux

As for godbolt builds, it is easy to go to godbolt.org and add a -v to the compiler options of the build you're interested in, so I will invite you to do that instead of cluttering this already long bug report further.

---

FWIW, clang 10 behaves the way I would expect without fast-math flags (and also generates the zero in place with a xorpd instead of loading it from memory, which is kind of cool), but I'm well aware of the danger of comparing the floating-point behavior of various compiler optimizers. So I wouldn't read too much into that:

```
.LCPI5_0:
        .quad   4607182418800017408     # double 1
use_fast_clamp(double):                    # @use_fast_clamp(double)
        minsd   xmm0, qword ptr [rip + .LCPI5_0]
        xorpd   xmm1, xmm1
        maxsd   xmm0, xmm1
        ret
```

If you like to experiment on godbolt too, here's my setup: https://godbolt.org/z/eD-guY .


---


### compiler : `gcc`
### title : `On powerpc, -ffunction-sections -fdata-sections is not as effective as expected for PIE and PIC`
### open_at : `2020-04-06T18:08:15Z`
### last_modified_date : `2020-04-13T06:53:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94504
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `enhancement`
### contents :
I try to compile the following test program using

gcc -ffunction-sections -fdata-sections -pie -Wl,--gc-sections input.c

The linking step fails, because g is not defined. On most architectures except powerpc (32 bit), the garbage collection is able to discard both fptr and f, and so the reference to g vanishes. This is not the case on powerpc, where f is referencing fptr through a GOT entry in the .got2 section.

This issue (I don't dare to call it "bug" yet) is the cause of of https://bugs.debian.org/955845. The librsvg build process is quirky and building the tests only works if garbage collection is able to collect a hughe amount of dead code. Garbage collection is able to do that on all architectures Debian tried it on except for powerpc (and possibly ppc64, see https://bugs.debian.org/895723). The attached example program does compile fine on ppc64, though.

/* dead code */
extern void g(int x, ...);
extern void (*fptr)();
void f()
{
        /* using fptr creates a GOT entry for fptr */
        g(0, fptr);
}
/* fptr is reference from the GOT. Let's reference f from fptr */
void (*fptr)() = f;

/* Non-dead code */
int x = 5;
int main(void)
{
        /* using x goes through the GOT. This prevents the GC to kill it */
        return x;
}


---


### compiler : `gcc`
### title : `[10 Regression] gnutls test ./psk-file fails since r10-7515-g2c0fa3ecf70d199af18785702e9e0548fd3ab793`
### open_at : `2020-04-07T11:52:04Z`
### last_modified_date : `2023-07-07T08:48:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94516
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
The following release:
https://www.gnupg.org/ftp/gcrypt/gnutls/v3.6/gnutls-3.6.12.tar.xz

fails with:
$ export CFLAGS="-O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -Werror=return-type -g -fPIE"
$ ./configure
$ make
$ cd tests
$ make check
$ valgrind ./psk-file 
==22044== Memcheck, a memory error detector
==22044== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==22044== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info
==22044== Command: ./psk-file
==22044== 
ntest NORMAL:-KX-ALL:+PSK (user:non-hex)
==22045== 
==22045== HEAP SUMMARY:
==22045==     in use at exit: 337 bytes in 7 blocks
==22045==   total heap usage: 1,371 allocs, 1,364 frees, 203,699 bytes allocated
==22045== 
==22045== LEAK SUMMARY:
==22045==    definitely lost: 0 bytes in 0 blocks
==22045==    indirectly lost: 0 bytes in 0 blocks
==22045==      possibly lost: 0 bytes in 0 blocks
==22045==    still reachable: 337 bytes in 7 blocks
==22045==         suppressed: 0 bytes in 0 blocks
==22045== Rerun with --leak-check=full to see details of leaked memory
==22045== 
==22045== For lists of detected and suppressed errors, rerun with: -s
==22045== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
test NORMAL:-VERS-ALL:+VERS-TLS1.3:-CIPHER-ALL:+AES-128-GCM:+DHE-PSK:+PSK:-GROUP-DH-ALL (user:jas)
==22046== 
==22046== Process terminating with default action of signal 11 (SIGSEGV): dumping core
==22046==  General Protection Fault
==22046==    at 0x48D291A: UnknownInlinedFun (pk.c:1925)
==22046==    by 0x48D291A: wrap_nettle_pk_generate_keys.lto_priv.0 (pk.c:2509)
==22046==    by 0x48F53FB: client_gen_key_share (key_share.c:95)
==22046==    by 0x48F59CC: key_share_send_params.lto_priv.0 (key_share.c:725)
==22046==    by 0x4998F6B: hello_ext_send (hello_ext.c:368)
==22046==    by 0x49660D5: UnknownInlinedFun (extv.c:218)
==22046==    by 0x49660D5: _gnutls_extv_append (extv.c:200)
==22046==    by 0x4999301: _gnutls_gen_hello_extensions (hello_ext.c:436)
==22046==    by 0x49B2731: UnknownInlinedFun (handshake.c:2265)
==22046==    by 0x49B2731: UnknownInlinedFun (handshake.c:2895)
==22046==    by 0x49B2731: gnutls_handshake (handshake.c:2727)
==22046==    by 0x402C1F: client (psk-file.c:102)
==22046==    by 0x4030B3: run_test3 (psk-file.c:349)
==22046==    by 0x4031FE: run_test2 (psk-file.c:358)
==22046==    by 0x4031FE: doit (psk-file.c:407)


---


### compiler : `gcc`
### title : `missed optimization with MIN and AND with type promotion`
### open_at : `2020-04-09T19:13:01Z`
### last_modified_date : `2023-07-21T23:19:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94543
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
unsigned f(unsigned short x) { return (x > 0xff ? 0xff : x) & 0xff; }

	cmpw	$255, %di
	movl	$255, %eax
	cmova	%eax, %edi
	movzwl	%di, %eax
	ret

The final AND is of course redundant.  The optimizer removes it
for wider types, but fails to do so when promoting from short.


---


### compiler : `gcc`
### title : `conversion between std::strong_ordering and int`
### open_at : `2020-04-11T22:04:21Z`
### last_modified_date : `2023-09-21T13:55:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94566
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
#include <compare>

int conv1(std::strong_ordering s){
  if(s==std::strong_ordering::less) return -1;
  if(s==std::strong_ordering::equal) return 0;
  if(s==std::strong_ordering::greater) return 1;
  __builtin_unreachable();
}
std::strong_ordering conv2(int i){
  switch(i){
    case -1: return std::strong_ordering::less;
    case 0: return std::strong_ordering::equal;
    case 1: return std::strong_ordering::greater;
    default: __builtin_unreachable();
  }
}

Compiling with -std=gnu++2a -O3. I would like the compiler to notice that those are just NOP (at most a sign-extension). Clang manages it for conv2. Gcc generates:

	movl	$-1, %eax
	cmpb	$-1, %dil
	je	.L1
	xorl	%eax, %eax
	testb	%dil, %dil
	setne	%al
.L1:
	ret

and

	xorl	%eax, %eax
	testl	%edi, %edi
	je	.L10
	cmpl	$1, %edi
	sete	%al
	leal	-1(%rax,%rax), %eax
.L10:
	ret


(apparently the C++ committee thinks it is a good idea to provide a type that is essentially an int that can only be -1, 0 or 1, but not provide any direct way to convert to/from int)


---


### compiler : `gcc`
### title : `Optimizer produces suboptimal code related to -fstore-merging`
### open_at : `2020-04-13T04:59:16Z`
### last_modified_date : `2020-04-14T14:15:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94573
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
For the following code, we can known init the array C000016DD is always  consecutive, so we can use the more bigger mode size.
test base on the x86-64 gcc 9.2 on https://gcc.godbolt.org/, now it is still handled DWORD by DWORD, and we except optimize it with QWORD or more bigger size.

extern signed int C000016DD[43][12]; 

void C00001F93(int index)
{
    C000016DD[index][0] = 0;
    C000016DD[index][1] = 0;
    C000016DD[index][2] = 0;
    C000016DD[index][3] = 0;
    C000016DD[index][4] = 0;
    C000016DD[index][5] = 0;
    C000016DD[index][6] = 0;
    C000016DD[index][7] = 0;

    return;
}

============= related assemble =============
C00001F93(int):
        movsx   rdi, edi
        lea     rax, [rdi+rdi*2]
        sal     rax, 4
        mov     DWORD PTR C000016DD[rax], 0
        mov     DWORD PTR C000016DD[rax+4], 0
        mov     DWORD PTR C000016DD[rax+8], 0
        mov     DWORD PTR C000016DD[rax+12], 0
        mov     DWORD PTR C000016DD[rax+16], 0
        mov     DWORD PTR C000016DD[rax+20], 0
        mov     DWORD PTR C000016DD[rax+24], 0
        mov     DWORD PTR C000016DD[rax+28], 0
        ret


---


### compiler : `gcc`
### title : `Optimize (i<=>0)>0 to i>0`
### open_at : `2020-04-14T06:57:31Z`
### last_modified_date : `2023-09-21T14:01:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94589
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
g++-10 -std=gu++2a -O3

#include <compare>
bool k(int i){
  auto c=i<=>0;
  return c>0;
}

  <bb 2> [local count: 1073741824]:
  if (i_1(D) != 0)
    goto <bb 3>; [50.00%]
  else
    goto <bb 4>; [50.00%]

  <bb 3> [local count: 536870913]:
  _2 = i_1(D) >= 0;

  <bb 4> [local count: 1073741824]:
  # prephitmp_6 = PHI <_2(3), 0(2)>
  return prephitmp_6;

For most comparisons @ we do optimize (i<=>0)@0 to just i@0, but not for > and <=. Spaceship operator<=> is very painful to use, but I expect we will end up seeing a lot of it with C++20, and comparing its result with 0 is almost the only way to use its output, so it seems important to optimize this common case.

(there is probably a very old dup, but I couldn't find it)


---


### compiler : `gcc`
### title : `Simple if condition not optimized`
### open_at : `2020-04-16T11:28:24Z`
### last_modified_date : `2023-06-26T09:21:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94617
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Given the following C++ snippet

  const char* vanilla_bandpass(int a, int b, int x, const char* low, const char* high)
  {
      const bool within_interval { (a <= x) && (x < b) };
      return (within_interval ? high : low);
  }

GCC trunk yields with -O3 -march=znver2 the following assembly

  vanilla_bandpass(int, int, int, char const*, char const*):
          mov     rax, r8
          cmp     edi, edx
          jg      .L4
          cmp     edx, esi
          jge     .L4
          ret
  .L4:
          mov     rax, rcx
          ret

which is terrible. On the other hand, Clang emits

  vanilla_bandpass(int, int, int, char const*, char const*):
          cmp     edx, esi
          cmovge  r8, rcx
          cmp     edi, edx
          cmovg   r8, rcx
          mov     rax, r8
          ret

which is a lot better. There exists an unbranched version for which I'm not 100% certain whether it's free of UB:

  #include <cstdint>

  const char* funky_bandpass(int a, int b, int x, const char* low, const char* high)
  {
      const bool within_interval { (a <= x) && (x < b) };
      const auto low_ptr = reinterpret_cast<uintptr_t>(low) * (!within_interval);
      const auto high_ptr = reinterpret_cast<uintptr_t>(high) * within_interval;
  
      const auto ptr_sum = low_ptr + high_ptr;
      const auto* result = reinterpret_cast<const char*>(ptr_sum);
      return result;
  }

which yields

  funky_bandpass(int, int, int, char const*, char const*):
          cmp     edi, edx
          setle   al
          cmp     edx, esi
          setl    dl
          and     eax, edx
          mov     edx, eax
          xor     edx, 1
          movzx   edx, dl
          movzx   eax, al
          imul    rcx, rdx
          imul    rax, r8
          add     rax, rcx
          ret

which is jump-free and in practice executes at the same observable rate as Clang's assembly, but still looks needlessly complex. Clang manages to compile this code to the same assembly as vanilla_bandpass.

Any chance of getting the optimizer ironed out for this?


---


### compiler : `gcc`
### title : `sign extension of nonnegative value from 32 to 64 bits`
### open_at : `2020-04-18T00:50:08Z`
### last_modified_date : `2020-04-20T06:57:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94643
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Test case:

#include <stdint.h>
uint16_t a[];
uint64_t f(int i)
{
	return a[i]*16;
}

Produces:

        movslq  %edi, %rdi
        movzwl  a(%rdi,%rdi), %eax
        sall    $4, %eax
        cltq
        ret

The value is necessarily in the range [0,1M) (in particular, nonnegative) and operation on eax has already cleared the upper bits of rax, so cltq is completely gratuitous. I've observed the same in nontrivial examples where movslq gets used.


---


### compiler : `gcc`
### title : `16-byte aligned atomic_compare_exchange doesn not generate cmpxcg16b on x86_64`
### open_at : `2020-04-18T18:17:38Z`
### last_modified_date : `2023-02-17T07:53:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94649
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
The code

#include <atomic>

struct alignas(16) a {
    long x;
    long y;
};

bool cmpxchg(std::atomic<a>& data, a expected, a newval) {
    return std::atomic_compare_exchange_weak(&data, &expected, newval);
}

compiles with -O3 -mcx16 generates a "lock cmpxchg16b" instruction with clang, but not with gcc, where it generates a library call to __atomic_compare_exchange_16. This is a missed optimization.


---


### compiler : `gcc`
### title : `Missed x86-64 peephole optimization: x >= large power of two`
### open_at : `2020-04-18T18:28:31Z`
### last_modified_date : `2020-05-04T11:54:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94650
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
Consider the three functions check, test0 and test1:

(Compiler Explorer link: https://gcc.godbolt.org/z/Sh4GpR )

#include <string.h>

#define LARGE_POWER_OF_TWO (1UL << 40)

int check(unsigned long m)
{
    return m >= LARGE_POWER_OF_TWO;
}

void g(int);

void test0(unsigned long m)
{
    if (m >= LARGE_POWER_OF_TWO) g(0);
}

void test1(unsigned long m)
{
    if (m >= LARGE_POWER_OF_TWO) g(m);
}

At least in the case of check and test0, the optimal way to compare m to 1<<40 is to shift m by 40 and compare the result to 0. This is the code generated for these functions by Clang 10:

check:                                  # @check
        xorl    %eax, %eax
        shrq    $40, %rdi
        setne   %al
        retq
test0:                                  # @test0
        shrq    $40, %rdi
        je      .LBB1_1
        xorl    %edi, %edi
        jmp     g                       # TAILCALL
.LBB1_1:
        retq

In contrast, GCC 9.3 uses a 64-bit constant that needs to be loaded in a register with movabsq:

check:
        movabsq $1099511627775, %rax
        cmpq    %rax, %rdi
        seta    %al
        movzbl  %al, %eax
        ret
test0:
        movabsq $1099511627775, %rax
        cmpq    %rax, %rdi
        ja      .L5
        ret
.L5:
        xorl    %edi, %edi
        jmp     g


In the case of the function test1 the comparison is between these two version, because the shift is destructive:

Clang10:
test1:                                  # @test1
        movq    %rdi, %rax
        shrq    $40, %rax
        je      .LBB2_1
        jmp     g                       # TAILCALL
.LBB2_1:
        retq

GCC9.3:
test1:
        movabsq $1099511627775, %rax
        cmpq    %rax, %rdi
        ja      .L8
        ret
.L8:
        jmp     g

It is less obvious which approach is better in the case of the function test1, but generally speaking the shift approach should still be faster. The register-register move can be free on Skylake (in the sense of not needing any execution port), whereas movabsq requires an execution port and also it's a 10-byte instruction!


---


### compiler : `gcc`
### title : `Missed peephole optimization: m >= POWER_OF_TWO || n >= POWER_OF_TWO`
### open_at : `2020-04-18T18:47:18Z`
### last_modified_date : `2021-08-01T18:10:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94651
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `enhancement`
### contents :
Consider the functions:

(Compiler Explorer link: https://gcc.godbolt.org/z/Uzd6nd )

#define POWER_OF_TWO (1UL << 20)

int check(unsigned long m, unsigned long n)
{
    return m >= POWER_OF_TWO || n >= POWER_OF_TWO;
}

void g(unsigned long, unsigned long);

void test1(unsigned long m, unsigned long n)
{
    if (m >= POWER_OF_TWO || n >= POWER_OF_TWO) g(m, 0);
}

void test2(unsigned long m, unsigned long n)
{
    if (m >= POWER_OF_TWO || n >= POWER_OF_TWO) g(m, n);
}

At least for the test1 and test2 functions, it seems that code that implements (m|n) >= POWER_OF_TWO will be faster on average for more input distributions than code with two comparisons on pretty much every modern architecture. This is what Clang 10 generates:

check:                                  # @check
        orq     %rsi, %rdi
        xorl    %eax, %eax
        cmpq    $1048575, %rdi          # imm = 0xFFFFF
        seta    %al
        retq
test1:                                  # @test1
        orq     %rdi, %rsi
        cmpq    $1048576, %rsi          # imm = 0x100000
        jb      .LBB1_1
        xorl    %esi, %esi
        jmp     g                       # TAILCALL
.LBB1_1:
        retq
test2:                                  # @test2
        movq    %rsi, %rax
        orq     %rdi, %rax
        cmpq    $1048576, %rax          # imm = 0x100000
        jb      .LBB2_1
        jmp     g                       # TAILCALL
.LBB2_1:
        retq


GCC 9.3 does one comparison after the other. This leads to extra instructions being necessary afterwards for the function check on x86, although it saves one register-register move in the function test2:

check:
        cmpq    $1048575, %rdi
        seta    %al
        cmpq    $1048575, %rsi
        seta    %dl
        orl     %edx, %eax
        movzbl  %al, %eax
        ret
test1:
        cmpq    $1048575, %rdi
        ja      .L6
        cmpq    $1048575, %rsi
        ja      .L6
        ret
.L6:
        xorl    %esi, %esi
        jmp     g
test2:
        cmpq    $1048575, %rdi
        ja      .L10
        cmpq    $1048575, %rsi
        ja      .L10
        ret
.L10:
        jmp     g


---


### compiler : `gcc`
### title : `[missed optimization] _mm512_dpbusds_epi32 generates excess vmovdqa64`
### open_at : `2020-04-19T19:57:04Z`
### last_modified_date : `2023-10-17T00:50:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94663
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
The _mm512_dpbusds_epi32 intrinsic generates extra vmovdqa64 instructions when used inside a loop.  The underlying instruction, vpdpbusds, adds to an accumulator, so it is commonly used in loops.  The compiler appears to be unnecessarily using two registers for the accumulator by copying it.  

Example:

#include "immintrin.h"
__m512i Slow(const __m512i *a, const __m512i b0, const __m512i b1, std::size_t count) {
  __m512i c0 = _mm512_setzero_epi32();
  __m512i c1 = _mm512_setzero_epi32();
  for (std::size_t i = 0; i < count; ++i) {
    c0 = _mm512_dpbusds_epi32(c0, a[i], b0);
    c1 = _mm512_dpbusds_epi32(c1, a[i], b1);
  }
  // Do not optimize away
  return _mm512_sub_epi32(c0, c1);
}

When compiled with g++ -O3 -mavx512vnni example.cc -S, the main loop is:

.L3:
        vmovdqa64       (%rdi), %zmm6
        vmovdqa64       %zmm3, %zmm0
        vmovdqa64       %zmm4, %zmm2
        addq    $64, %rdi
        vpdpbusds       %zmm5, %zmm6, %zmm0
        vpdpbusds       %zmm1, %zmm6, %zmm2
        vmovdqa64       %zmm0, %zmm3
        vmovdqa64       %zmm2, %zmm4
        cmpq    %rdi, %rax
        jne     .L3

It's copying accumulator zmm3 to zmm0, accumulating in zmm0, then copying back to zmm3.  It should have just used one register.  The same happens for zmm4 and zmm2.  

Workaround: use inline assembly.  

__m512i Fast(const __m512i *a, const __m512i b0, const __m512i b1, std::size_t count) {
  __m512i c0 = _mm512_setzero_epi32();
  __m512i c1 = _mm512_setzero_epi32();
  for (std::size_t i = 0; i < count; ++i) {
    asm ("vpdpbusds %1, %2, %0" : "+x"(c0) : "mx"(a[i]), "x"(b0));
    asm ("vpdpbusds %1, %2, %0" : "+x"(c1) : "mx"(a[i]), "x"(b1));
  }
  // Do not optimize away
  return _mm512_sub_epi32(c0, c1);
}

Here, the generated code is better, with no extra moves.  

.L10:
#APP
# 19 "example.cc" 1
        vpdpbusds (%rdi), %zmm3, %zmm0
# 0 "" 2
# 20 "example.cc" 1
        vpdpbusds (%rdi), %zmm1, %zmm2
# 0 "" 2
#NO_APP
        addq    $64, %rdi
        cmpq    %rax, %rdi
        jne     .L10

Reproduced on the following versions of g++:

g++ -v 
Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-pc-linux-gnu/9.2.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: /var/tmp/portage/sys-devel/gcc-9.2.0-r2/work/gcc-9.2.0/configure --host=x86_64-pc-linux-gnu --build=x86_64-pc-linux-gnu --prefix=/usr --bindir=/usr/x86_64-pc-linux-gnu/gcc-bin/9.2.0 --includedir=/usr/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include --datadir=/usr/share/gcc-data/x86_64-pc-linux-gnu/9.2.0 --mandir=/usr/share/gcc-data/x86_64-pc-linux-gnu/9.2.0/man --infodir=/usr/share/gcc-data/x86_64-pc-linux-gnu/9.2.0/info --with-gxx-include-dir=/usr/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/g++-v9 --with-python-dir=/share/gcc-data/x86_64-pc-linux-gnu/9.2.0/python --enable-languages=c,c++,fortran --enable-obsolete --enable-secureplt --disable-werror --with-system-zlib --enable-nls --without-included-gettext --enable-checking=release --with-bugurl=https://bugs.gentoo.org/ --with-pkgversion='Gentoo 9.2.0-r2 p3' --disable-esp --enable-libstdcxx-time --enable-shared --enable-threads=posix --enable-__cxa_atexit --enable-clocale=gnu --enable-multilib --with-multilib-list=m32,m64 --disable-altivec --disable-fixed-point --enable-targets=all --enable-libgomp --disable-libmudflap --disable-libssp --disable-systemtap --enable-vtable-verify --enable-lto --without-isl --enable-default-pie --enable-default-ssp
Thread model: posix
gcc version 9.2.0 (Gentoo 9.2.0-r2 p3) 

g++ -v
Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/8/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 8.4.0-1ubuntu1~18.04' --with-bugurl=file:///usr/share/doc/gcc-8/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-8 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 8.4.0 (Ubuntu 8.4.0-1ubuntu1~18.04) 

Full source code:
#include <immintrin.h>
#include <cstddef>

__m512i Slow(const __m512i *a, const __m512i b0, const __m512i b1, std::size_t count) {
  __m512i c0 = _mm512_setzero_epi32();
  __m512i c1 = _mm512_setzero_epi32();
  for (std::size_t i = 0; i < count; ++i) {
    c0 = _mm512_dpbusds_epi32(c0, a[i], b0);
    c1 = _mm512_dpbusds_epi32(c1, a[i], b1);
  }
  // Do not optimize away
  return _mm512_sub_epi32(c0, c1);
}

__m512i Fast(const __m512i *a, const __m512i b0, const __m512i b1, std::size_t count) {
  __m512i c0 = _mm512_setzero_epi32();
  __m512i c1 = _mm512_setzero_epi32();
  for (std::size_t i = 0; i < count; ++i) {
    asm ("vpdpbusds %1, %2, %0" : "+x"(c0) : "mx"(a[i]), "x"(b0));
    asm ("vpdpbusds %1, %2, %0" : "+x"(c1) : "mx"(a[i]), "x"(b1));
  }
  // Do not optimize away
  return _mm512_sub_epi32(c0, c1);
}

Command line: g++ -O3 -mavx512vnni -S example.cc
(It also happens with g++ -O3 -march=native -S example.cc on a Cascade Lake CPU with g++ 8.4.0).  
Output: none


---


### compiler : `gcc`
### title : `missed minmax optimization opportunity for if/else structure.`
### open_at : `2020-04-20T03:52:13Z`
### last_modified_date : `2020-04-23T01:08:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94665
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Minmax optimization for fortran，
for example:

SUBROUTINE mydepart(vara,varb,varc,res)
      REAL, INTENT(IN) :: vara,varb,varc
      REAL, INTENT(out) :: res

      res = vara
      if (res .lt. varb)  res = varb
      if (res .gt. varc)  res = varc
end SUBROUTINE

on aarch64, compile with -O2 -S -funsafe-math-optimizations, the asm:

ldr     s2, [x0]
ldr     s0, [x1]
ldr     s1, [x2]
fcmpe   s2, s0
fcsel   s0, s0, s2, mi
fminnm  s1, s1, s0
str     s1, [x3]
ret

The second if statement is optimized to fminnm, but the first can not.

In fact, it can be optimized to:

ldr     s2, [x0]
ldr     s1, [x1]
ldr     s0, [x2]
fmaxnm  s1, s2, s1
fminnm  s0, s0, s1
str     s0, [x3]

My proposal: I tracked the generation of fminnm is done in simplify_if_then_else. The reason why the first statement optimization is not done is that the conditions are not met:
rtx_equal_p (XEXP (cond, 0), true_rtx) && rtx_equal_p (XEXP (cond, 1), false_rtx).

The RTX:

(if_then_else:SF (lt (reg:SF 92 [ _1 ])
        (reg:SF 93 [ _2 ]))
    (reg:SF 93 [ _2 ])
    (reg:SF 92 [ _1 ]))

We can swap the true_rtx/false_rtx, and take the maximum.

the patch:

diff --git a/gcc/combine.c b/gcc/combine.c
--- a/gcc/combine.c
+++ b/gcc/combine.c
@@ -6641,25 +6641,43 @@ simplify_if_then_else (rtx x)
 
   if ((! FLOAT_MODE_P (mode) || flag_unsafe_math_optimizations)
       && comparison_p
-      && rtx_equal_p (XEXP (cond, 0), true_rtx)
-      && rtx_equal_p (XEXP (cond, 1), false_rtx)
       && ! side_effects_p (cond))
-    switch (true_code)
-      {
-      case GE:
-      case GT:
-	return simplify_gen_binary (SMAX, mode, true_rtx, false_rtx);
-      case LE:
-      case LT:
-	return simplify_gen_binary (SMIN, mode, true_rtx, false_rtx);
-      case GEU:
-      case GTU:
-	return simplify_gen_binary (UMAX, mode, true_rtx, false_rtx);
-      case LEU:
-      case LTU:
-	return simplify_gen_binary (UMIN, mode, true_rtx, false_rtx);
-      default:
-	break;
+    {
+      int swapped = 0;
+      if (rtx_equal_p (XEXP (cond, 0), false_rtx)
+	  && rtx_equal_p (XEXP (cond, 1), true_rtx))
+	{
+	  std::swap (true_rtx, false_rtx);
+	  swapped = 1;
+	}
+
+      if (rtx_equal_p (XEXP (cond, 0), true_rtx)
+	  && rtx_equal_p (XEXP (cond, 1), false_rtx))
+	switch (true_code)
+	  {
+	  case GE:
+	  case GT:
+	    return simplify_gen_binary (swapped ? SMIN : SMAX,
+					mode, true_rtx, false_rtx);
+	  case LE:
+	  case LT:
+	    return simplify_gen_binary (swapped ? SMAX : SMIN,
+					mode, true_rtx, false_rtx);
+	  case GEU:
+	  case GTU:
+	    return simplify_gen_binary (swapped ? UMIN : UMAX,
+					mode, true_rtx, false_rtx);
+	  case LEU:
+	  case LTU:
+	    return simplify_gen_binary (swapped ? UMAX : UMIN,
+					mode, true_rtx, false_rtx);
+	  default:
+	    break;
+	  }
+
+      /* Restore if not MIN or MAX.  */
+      if (swapped)
+	std::swap (true_rtx, false_rtx);
       }
 
   /* If we have (if_then_else COND (OP Z C1) Z) and OP is an identity when its

Any suggestions?


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] -Warray-bounds false positive with -O2 since r9-1948`
### open_at : `2020-04-20T19:42:11Z`
### last_modified_date : `2023-07-07T10:37:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94675
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `normal`
### contents :
Created attachment 48319
test case

Tested with gcc 9.1 9.2 and 9.3 on godbolt.

Compiling with "-O2 -Warray-bounds" gives the following false positive :

---
<source>: In function 'f':
<source>:38:11: warning: array subscript 7 is outside array bounds of 'byte[1]' {aka 'unsigned char[1]'} [-Warray-bounds]
   38 |     ps->s += len;
      |           ^~
<source>:46:6: note: while referencing 'c'

   46 | byte c;
      |      ^
Compiler returned: 0
---

Error also with -O3 but not with -O.

Note that the error goes after removing the assert.


---


### compiler : `gcc`
### title : `Missed optimization with __builtin_shuffle and zero vector`
### open_at : `2020-04-21T08:35:52Z`
### last_modified_date : `2022-03-17T15:12:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94680
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
typedef float V __attribute__((vector_size(16)));
typedef int VI __attribute__((vector_size(16)));

V
foo (V x)
{
  return __builtin_shuffle (x, (V) { 0, 0, 0, 0 }, (VI) {0, 1, 4, 5});
}

can be done using movq or vmovq in a single insn, rather than zeroing a register and doing movlhps.


---


### compiler : `gcc`
### title : `OpenACC 'async' clause optimizations`
### open_at : `2020-04-21T09:36:27Z`
### last_modified_date : `2020-04-21T10:36:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94684
### status : `UNCONFIRMED`
### tags : `missed-optimization, openacc`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
In 'gcc/omp-expand.c:expand_omp_target', we have an optimization that for an OpenACC 'async' clause, we try to "pack the async arg in to the tag's operand" ('i_async' handling).

(... which doesn't seem to have any dedicated testsuite coverage.)

That currently only works for 'INTEGER_CST' literals, but not for 'const int async = 1', for example.  Could it, or is 'expand_omp_target' too early for such things?

Due to only accepting positive values, this doesn't work for the very common case of 'async' clause without argument, that is: 'async(acc_async_noval)', that is: 'async(-1)'.

And, couldn't we also optimize the (unusual) case of 'async(acc_async_sync)', that is: 'async(-2)' by removing the 'async' clause altogether?


---


### compiler : `gcc`
### title : `PPC vector fails to optimize shift (used bits)`
### open_at : `2020-04-21T10:32:28Z`
### last_modified_date : `2020-06-16T11:40:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94687
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
https://godbolt.org/z/ZyTG9b


#include <altivec.h>

typedef vector unsigned __int128 block;
typedef vector unsigned long long vector2_u64;

block swap_with_shift(block num) {
    return num << 64 | num >> 64;
}

block swap_without_shift(block num) {
    vector unsigned long long ret;
    ret[0] = ((vector2_u64)num)[1];
    ret[1] = ((vector2_u64)num)[0];
    return (block)ret;
}

typedef unsigned __int128 u128;

u128 swap_scalar(u128 in) {
    return in << 64 | in >> 64;
}

swap_with_shift:
        xxpermdi 34,34,34,2
        addi 9,1,-16
        stxvd2x 34,0,9
        ld 8,-8(1)
        ld 9,-16(1)
        mtvsrd 1,8
        mtvsrd 0,9
        xxpermdi 34,0,1,0
        blr
        .long 0
        .byte 0,0,0,0,0,0,0,0
swap_without_shift:
        xxpermdi 34,34,34,2
        blr
        .long 0
        .byte 0,0,0,0,0,0,0,0
swap_scalar:
        mr 9,3
        mr 3,4
        mr 4,9
        blr
        .long 0
        .byte 0,0,0,0,0,0,0,0


---


### compiler : `gcc`
### title : `Small-sized  memcpy leading to unnecessary register spillage unless done through a dummy union`
### open_at : `2020-04-21T21:07:23Z`
### last_modified_date : `2020-05-14T09:55:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94703
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
The problem, demonstrated in code examples below, can be suppressed by memcpying into a union (possibly just a one-member union), but that seems like a silly workaround that shouldn't be required.

Examples:

#include <stdint.h>
#include <string.h>

uint64_t get4_1(void const *X)
{
	//spills
	uint64_t r = 0; memcpy(&r,X,4); return r;
}

uint64_t get4_nospill(void const *X)
{
	//doesn't spill
	union { uint64_t u64; } u = {0};
	memcpy(&u.u64,X,sizeof(uint32_t));
	return u.u64;
}

uint64_t get2_1(void const *X)
{
	//spills
	uint64_t r = 0; memcpy(&r,X,2); return r;
}


uint64_t get2_nospill(void const *X)
{
	//doesn't spill
	union { uint64_t u64; } u = {0};
	memcpy(&u.u64,X,sizeof(uint16_t));
	return u.u64;
}

	void backend(void const*Src, size_t Sz);
	static inline void valInPtrInl(void *Src, size_t Sz)
	{
		if(Sz<=sizeof(void const*)){
			#if 1 //spills
				void const*inlSrc; 
				memcpy(&inlSrc,Src,Sz);
				backend(inlSrc,Sz); return;
			#else
				//doesn't spill
				union{ void const*inlSrc; } u;
				memcpy(&u.inlSrc,Src,Sz);
				backend(u.inlSrc,Sz); return;
			#endif
		}

		backend(Src,Sz);
		return;

	}
void valInPtr(int X) { valInPtrInl(&X,sizeof(X)); }

GCC 9.3 output on x86_64:

get4_1:
        mov     QWORD PTR [rsp-8], 0
        mov     eax, DWORD PTR [rdi]
        mov     DWORD PTR [rsp-8], eax
        mov     rax, QWORD PTR [rsp-8]
        ret
get4_nospill:
        mov     eax, DWORD PTR [rdi]
        ret
get2_1:
        mov     QWORD PTR [rsp-8], 0
        movzx   eax, WORD PTR [rdi]
        mov     WORD PTR [rsp-8], ax
        mov     rax, QWORD PTR [rsp-8]
        ret
get2_nospill:
        xor     eax, eax
        mov     ax, WORD PTR [rdi]
        ret
valInPtr:
        mov     DWORD PTR [rsp-16], edi
        mov     rdi, QWORD PTR [rsp-16]
        mov     esi, 4
        jmp     backend

Clang 3.1 output on x86_64:

get4_1:                                 # @get4_1
        mov     EAX, DWORD PTR [RDI]
        ret

get4_nospill:                           # @get4_nospill
        mov     EAX, DWORD PTR [RDI]
        ret

get2_1:                                 # @get2_1
        movzx   EAX, WORD PTR [RDI]
        ret

get2_nospill:                           # @get2_nospill
        movzx   EAX, WORD PTR [RDI]
        ret

valInPtr:                               # @valInPtr
        mov     EDI, EDI
        mov     ESI, 4
        jmp     backend                 # TAILCALL

 
https://gcc.godbolt.org/z/rwq2UY


---


### compiler : `gcc`
### title : `Squared multiplies are unnecessarily signextended`
### open_at : `2020-04-22T15:23:07Z`
### last_modified_date : `2020-04-23T11:09:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94715
### status : `REOPENED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
The following example generates incorrect code with -O2:

unsigned long long f (int x)
{
  unsigned int t = x * x;
  return t;
}

On AArch64 I get:

  mul w0, w0, w0
  sxtw x0, w0
  ret

It's correct if you do x * y or x * 100.


---


### compiler : `gcc`
### title : `Failure to optimize opposite signs check`
### open_at : `2020-04-22T16:51:17Z`
### last_modified_date : `2023-09-21T12:29:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94718
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
bool f(int x, int y)
{
    return (x < 0) != (y < 0); 
}

`(x < 0) != (y < 0)` can be optimized to `(x ^ y) < 0`. 
This transformation is done by clang, but not by GCC, as seen here : https://godbolt.org/z/AnYPBF.


---


### compiler : `gcc`
### title : `[haifa-sched][restore_pattern] recalculate INSN_TICK for the dependence type of REG_DEP_CONTROL`
### open_at : `2020-04-23T08:21:14Z`
### last_modified_date : `2020-04-23T11:03:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94728
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.3.0`
### severity : `normal`
### contents :
If we add the flag DO_PREDICATION in scheduling ebb, the compiler will try to predicate the insn when the producer insn has been issued, and put the consumer insn into suitable queue.  For example as shown in schedule verbose dump:

;;      | insn | prio |                                                               
......................                        
;;      |  387 |   27 | t1=a5==0                       AGS0|AGS1                                           
;;      |  388 |   27 | pc={(t1!=0)?L184:pc}           PCU                            
;;      |  459 |   26 | ev10=[a0+0xbc0]                AGL0|AGL1   
......................

                   
;;        1--> +  387 t1=a5==0                                :AGS0|AGS1 

deferring rescan insn with uid = 459.
;;              dependencies resolved: insn +  459 predicated 
;;              Ready-->Q: insn +  459: queued for 2 cycles (change queue index)
;;              tick updated: insn +  459 into queue with cost=2 

insn 387 is a test insn, insn 388 is a jump insn, insn 459 is a load insn.
After predicating, insn 459 convert into this form: [!t1] ev10 = [a0+0xbc0]
and put insn 459 into queue[3]. INSN_TICK (459) = 3;

After issuing jump insn 388, the compiler will try to resotre pattern in insn 459 as shown in the following dump files.

;;              Ready list after ready_sort:      +  388:94:prio=27              
;;      Ready list (t =   1):    +  388:94:prio=27                               
[1;1]:388                                                                        
;;        1--> +  388 pc={(t1!=0)?L184:pc}                    :PCU     
restoring pattern for insn 459                                                   
deferring rescan insn with uid = 459.            

However, the INSN_TICK of insn 459 doesn't calculate again.
Actually, after restoring pattern, the insn can issue more earlier.
If we recalculate the INSN_TICK of insn 459, we will get INSN_TICK (459) = 2, then the load insn 459 can issue at clock t = 2 instead of clock t = 3.

So, can we add the following code to recalculate the INSN_TICK in function restore pattern?

restore_pattern (dep_t dep, bool immediately)
{
  rtx_insn *next = DEP_CON (dep);
  int tick = INSN_TICK (next);

.........................

  if (DEP_TYPE (dep) == REG_DEP_CONTROL)
    {
      if (sched_verbose >= 5)
	fprintf (sched_dump, "restoring pattern for insn %d\n",
		 INSN_UID (next));
      haifa_change_pattern (next, ORIG_PAT (next));
 +    update_insn_after_change (next);
 +    if ((TODO_SPEC (next) & (HARD_DEP | DEP_POSTPONED)) == 0)
 +      {
 +        fix_tick_ready (next);
 +        tick = INSN_TICK (next);
 +      }
    }
...........................

I found the similiar code in function apply_replacement (dep_t dep, bool immediately).


---


### compiler : `gcc`
### title : `aarch64: many unnecessary bti j emitted`
### open_at : `2020-04-24T16:45:32Z`
### last_modified_date : `2020-05-14T15:54:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94748
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
__attribute__((target("branch-protection=bti")))
int foo(void)
{
label:
  return 0;
} 


compiles to

foo:
        hint    34 // bti c
        hint    36 // bti j
        mov     w0, 0
        ret

the bti j is not necessary and bti j should be rarely emitted
otherwise the security architecture is weakened.


---


### compiler : `gcc`
### title : `GCC does not optimise unsigned multiplication known not to overflow`
### open_at : `2020-04-25T08:55:08Z`
### last_modified_date : `2021-12-15T23:56:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94757
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
This code:

    #include <limits.h>

    unsigned f(unsigned x) {
        if (x > UINT_MAX / 3)
            __builtin_unreachable();
        return (x * 3) / 3;
    }

should be possible to optimise into the identity function, but (on x86_64) gcc -O3 generates a multiplication (via LEA) then a division (via multiplication). GCC knows that the multiplication cannot overflow, because replacing the returned expression with __builtin_mul_overflow_p(x, 3, x) is makes it optimise to returning constant 0.


---


### compiler : `gcc`
### title : `Bad optimization of simple switch`
### open_at : `2020-04-27T00:40:24Z`
### last_modified_date : `2023-09-21T12:29:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94779
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f1(unsigned x)
{
    switch (x)
    {
        case 0:
            return 1;
        case 1:
            return 2;
    }
}

gcc fails to optimize this to `return x + 1`, instead opting for some rather weird code generation (involving `sbb` on x86). However, adding this :

     if (x >= 2)
        __builtin_unreachable();

at the beginning of the function makes it be optimized properly. Maybe this is a sign of the `x >= 2` condition being always false (due to it leading to UB) being found too late ?


---


### compiler : `gcc`
### title : `Simple multiplication-related arithmetic not optimized to direct multiplication`
### open_at : `2020-04-27T02:08:53Z`
### last_modified_date : `2023-09-21T12:28:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94782
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f(int a, int b)
{
    return (int)((a - 1U) * b) + b;
}

Can be optimized to `a * b`. LLVM does this transformation, GCC does not.

Comparison here : https://godbolt.org/z/LFJwFJ


---


### compiler : `gcc`
### title : `Abs-equivalent pattern is not recognized as abs`
### open_at : `2020-04-27T04:17:24Z`
### last_modified_date : `2023-09-21T12:27:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94783
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
unsigned r(int v)
{
    const int mask = v >> (sizeof(int) * CHAR_BIT - 1);
    return (v + mask) ^ mask;
}

This can be optimized to `return abs(v)`. This transformation is done by LLVM, not by GCC.

Comparison here : https://godbolt.org/z/4rduiJ


---


### compiler : `gcc`
### title : `Failure to detect abs pattern using multiplication`
### open_at : `2020-04-27T07:57:35Z`
### last_modified_date : `2023-09-21T12:27:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94785
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
unsigned r(int v)
{
    return (1 | -(v < 0)) * v;
}

`r` is equivalent to `abs(v)`. GCC does not make the transformation to an `abs`.

Example of the optimization not being done : https://godbolt.org/z/Rw-hBt


---


### compiler : `gcc`
### title : `Missed min/max pattern using xor+and+less`
### open_at : `2020-04-27T08:11:06Z`
### last_modified_date : `2023-09-21T12:26:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94786
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int r1(int x, int y)
{
    return y ^ ((x ^ y) & -(x < y));
}

int r2(int x, int y)
{
    return x ^ ((x ^ y) & -(x < y));
}

`r1` can be optimized to `min` and `r2` to `max`. This transformation is done by LLVM, but not by GCC.

Comparison here: https://godbolt.org/z/hNhkqM


---


### compiler : `gcc`
### title : `Failure to take advantage of shift operand semantics to turn subtraction into negate`
### open_at : `2020-04-27T08:56:58Z`
### last_modified_date : `2023-09-21T12:12:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94789
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
int r(int x, unsigned b)
{
    int const m = CHAR_BIT * sizeof(x) - b;
    return (x << m);
}

`CHAR_BIT * sizeof(x) - b;` can be optimized to `-b`. LLVM does this transformation, not GCC.

Comparison here : https://godbolt.org/z/5byJ2E


---


### compiler : `gcc`
### title : `Failure to use andn in specific pattern in which it is available`
### open_at : `2020-04-27T09:30:45Z`
### last_modified_date : `2023-09-21T11:44:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94790
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
unsigned r1(unsigned a, unsigned b, unsigned mask)
{
    return a ^ ((a ^ b) & mask);
}

unsigned r2(unsigned a, unsigned b, unsigned mask)
{
    return (~mask & a) | (b & mask);
}

`r1` and `r2` are equivalent. `r2` is translated into `r1` by GCC. LLVM instead transforms `r1` into `r2` when an "and not" instruction is available. I haven't benchmarked the resulting code a lot, but basic measurements and llvm-mca indicates that code using andn is faster than the code using the `r1` pattern (and the code using andn takes 1 less instruction on x86)

Comparison of generated code with `-mbmi` : https://godbolt.org/z/2PUhBX


---


### compiler : `gcc`
### title : `Missed SLP optimization in pr65930-2.c variation`
### open_at : `2020-04-27T09:49:26Z`
### last_modified_date : `2020-04-27T10:16:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94792
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
gcc commit cf3a909cf455. Consider the following variation of pr65930-2.c:

$ cat pr65930-2b.c
#include "tree-vect.h"

int __attribute__((noipa))
bar (unsigned int *x, int n)
{
  unsigned int sum = 4;
  x = __builtin_assume_aligned (x, __BIGGEST_ALIGNMENT__);
  for (int i = 0; i < n; ++i)
    sum += x[i*4+0]+ x[i*4 + 1] + x[i*4 + 2] + x[i*4 + 3];
  return sum;
}

int
main ()
{
  static int a[16] __attribute__((aligned(__BIGGEST_ALIGNMENT__)))
    = { 1, 3, 5, 8, 9, 10, 17, 18, 23, 29, 30, 55, 42, 2, 3, 1 };
  check_vect ();
  if (bar (a, 4) != 260)
    abort ();
  return 0;
}

This differs from pr65930-2.c only in that sum type is unsigned int, which should be on cast less. And yet:

$ gcc pr65930-2b.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -msse2 -ftree-vectorize -fno-tree-loop-distribute-patterns -fno-vect-cost-model -fno-common -O2 -fdump-tree-vect-details -lm -o ./pr65930-2.exe ; grep SLP pr65930-2b.c.161t.vect | wc -l
0

whereas for the original version:

$ gcc pr65930-2.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -msse2 -ftree-vectorize -fno-tree-loop-distribute-patterns -fno-vect-cost-model -fno-common -O2 -fdump-tree-vect-details -lm -o ./pr65930-2.exe ; grep SLP pr65930-2.c.161t.vect | wc -l
33

The resulting assembly is also noticeably larger and uses regular adds for at least part of the data.


---


### compiler : `gcc`
### title : `Failure to optimize clz idiom`
### open_at : `2020-04-27T10:00:20Z`
### last_modified_date : `2023-09-21T09:23:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94793
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
unsigned r1(unsigned v)
{
    unsigned r = 0;

    while (v >>= 1)
        r++;

    return r;
}

This can optimized to `32 - __builtin_clz(v >> 1);`. LLVM does this transformation, but GCC does not.

Comparison here : https://godbolt.org/z/u73iap


---


### compiler : `gcc`
### title : `Failure to use fast sbb method on x86 for spreading any set bit to all bits`
### open_at : `2020-04-27T11:27:38Z`
### last_modified_date : `2023-09-21T11:16:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94795
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
int isNonzero(int x)
{
    if (x == 0)
        return 0x00000000;
    else
        return 0xFFFFFFFF;
}

On x86, this can be simplified to a `neg`+`sbb`. LLVM does this transformation, but GCC doesn't

Comparison here : https://godbolt.org/z/QFz9to


---


### compiler : `gcc`
### title : `Failure to reuse flags from substraction`
### open_at : `2020-04-27T12:32:50Z`
### last_modified_date : `2023-09-21T11:15:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94796
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f(int a, int b)
{
    return ((a == b) & (a - b));
}

The `a == b` is able to use condition flags resulting from `a - b`, and thus avoid an extra compare. LLVM does this transformation, but GCC does not.

https://godbolt.org/z/SHvTW8


---


### compiler : `gcc`
### title : `Failure to optimize subtraction and 0 literal properly`
### open_at : `2020-04-27T13:47:22Z`
### last_modified_date : `2023-09-21T11:14:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94798
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
int f(int a, int b)
{
    return (b >= a) ? (b - a) : 0;
}

Generates some *really* bad code with GCC right now, it seems to forget such basic things as how to set a register to 0 on x86. LLVM generates good code.

Comparison here : https://godbolt.org/z/LZ8dBy

Also, this seems to have regressed back in GCC 4.9, GCC 4.8 at least generates a `xor reg, reg` instead of a `mov reg, 0`


---


### compiler : `gcc`
### title : `Failure to optimize yet another popcount idiom`
### open_at : `2020-04-27T14:42:31Z`
### last_modified_date : `2020-05-05T09:38:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94800
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int populationCount(uint32_t x)
{
    x = x - ((x >> 1) & 0x55555555);
    x = (x & 0x33333333) + ((x >> 2) & 0x33333333);
    x = (x + (x >> 4)) & 0x0F0F0F0F;
    x += (x << 8);
    return ((x + (x << 16)) >> 24);
}

This can be optimized to `__builtin_popcount(x)` (when `sizeof(int) == sizeof(uint32_t)`). This transformation is done by LLVM, but not by GCC

Comparison here : https://godbolt.org/z/iz9qJf


---


### compiler : `gcc`
### title : `Failure to optimize narrowed __builtin_clz`
### open_at : `2020-04-27T14:59:36Z`
### last_modified_date : `2022-01-28T16:10:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94801
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f(int a)
{
    return __builtin_clz(a) >> 5;
}

Can be optimized to `return 0;`. This transformation is done by LLVM, but not by GCC.

Comparison here : https://godbolt.org/z/jhqQ2u


---


### compiler : `gcc`
### title : `Failure to recognize identities with __builtin_clz`
### open_at : `2020-04-27T15:10:26Z`
### last_modified_date : `2021-08-14T22:29:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94802
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
bool f(int a, int b)
{
    return __builtin_clz(a - b);
}

This is equivalent to `return a >= b`. This transformation is done by LLVM, but not by GCC.

Comparison here : https://godbolt.org/z/jvahPh


---


### compiler : `gcc`
### title : `Failure to elide useless movs in 128-bit addition with __int128_t arguments`
### open_at : `2020-04-27T16:42:44Z`
### last_modified_date : `2023-01-19T23:25:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94804
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
using i128 = __int128;

i128 add128(i128 a, i128 b)
{
    return a + b;
}

This is how LLVM handles this code : 

add128(__int128, __int128):
  mov rax, rdi
  add rax, rdx
  adc rsi, rcx
  mov rdx, rsi
  ret

GCC seems to have an insistence on moving `b` from its original registers before actually doing the addition : 

add128(__int128, __int128):
  mov r9, rdi ; useless
  mov rax, rdx
  mov r8, rsi ; useless
  mov rdx, rcx
  add rax, r9 ; could just have used rdi
  adc rdx, r8 ; could just have used rsi
  ret

This seems to be specific to x86_64 : It does not occur on aarch64 or ppc64le


---


### compiler : `gcc`
### title : `Failure to optimize unary minus for 128-bit operand`
### open_at : `2020-04-27T18:12:40Z`
### last_modified_date : `2022-02-21T05:57:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94806
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
__int128 f(__int128 x)
{
    return -x;
}

It would appear like unary minus is badly optimized by GCC.

This is what LLVM outputs for this : 

f(__int128): # @f(__int128)
  mov rax, rdi
  xor edx, edx
  neg rax
  sbb rdx, rsi
  ret

And this is what GCC outputs : 

f(__int128):
  mov rax, rdi
  mov rdx, rsi
  neg rax
  adc rdx, 0
  neg rdx
  ret

GCC's output is obviously worse (unless an `adc`+`neg` pair can somehow be better than an `sbb`)

(See also https://godbolt.org/z/9AUd79)


---


### compiler : `gcc`
### title : `Abusive -Wrestrict warning with sprintf`
### open_at : `2020-04-28T11:56:29Z`
### last_modified_date : `2020-04-28T16:58:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94815
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Testcase:

$ cat foo.c && m68k-elf-gcc -c foo.c -Wall -O
char *strcpy(char *, const char *);
int sprintf(char *, const char *, ...);

char* myalloc(int n);

void f(void)
{
    char* buf = myalloc(20);
    char* str1 = buf;
    char* str2 = buf + 10;

    strcpy(str2, "123");
    sprintf(str1, "ABC%s", str2);
}
foo.c: In function 'f':
foo.c:13:5: warning: 'sprintf' argument 3 may overlap destination object 'buf' [-Wrestrict]
   13 |     sprintf(str1, "ABC%s", str2);
      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~

This warning is a unexpected because:

1) strcpy() and sprintf() are declared without __restrict, but __restrict rules are still actually used.

2) In this simple example, it is obvious that the buffer will not overflow.

This is annoying, because it prevents creating several logical buffers from a single allocation.


---


### compiler : `gcc`
### title : `Failure to optimize with __builtin_bswap32 as well as with a function recognized as such`
### open_at : `2020-04-28T15:14:51Z`
### last_modified_date : `2023-05-13T04:52:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94824
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
uint32_t swap32(uint32_t x)
{
    return ((x << 24) | ((x << 8) & 0x00FF0000) | ((x >> 8) & 0x0000FF00) | (x >> 24));
}

uint64_t swap64v1(uint64_t x)
{
    uint64_t a = __builtin_bswap32(x);
    x >>= 32;
    a <<= 32;
    return __builtin_bswap32(x) | a;
}

uint64_t swap64v2(uint64_t x)
{
    uint64_t a = swap32(x);
    x >>= 32;
    a <<= 32;
    return swap32(x) | a;
}

swap64v1 and swap64v2 are identical, since bswap32 is equivalent to __builtin_bswap32. However, only swap64v2 is optimized to __builtin_bswap64.

swap64v1 is compiled to this by gcc -O3 :

swap64v1(unsigned long):
  mov rdx, rdi
  mov eax, edi
  shr rdx, 32
  bswap eax
  sal rax, 32
  bswap edx
  mov edi, edx
  or rax, rdi
  ret

And swap64v2 gives this :

swap64v2(unsigned long):
  mov rax, rdi
  bswap rax
  ret


---


### compiler : `gcc`
### title : `Loop fusion is not implemented outside of ISL`
### open_at : `2020-04-28T17:49:57Z`
### last_modified_date : `2020-04-29T07:02:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94828
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
void f(int *__restrict a, int *__restrict b, size_t sz)
{
    for (int i = 0; i < sz; ++i)
        a[i] += b[i];

    for (int i = 0; i < sz; ++i)
        a[i] += b[i];
}

These two loops could be merged into a single one doing two additions per iteration. ICC does this transformation, GCC does not.

Also, I'd like to note that from the GCC output, only the first loop is vectorized, and not the second one.

See https://godbolt.org/z/DfGpSi


---


### compiler : `gcc`
### title : `Failure to optimize loop bswap pattern`
### open_at : `2020-04-28T22:35:21Z`
### last_modified_date : `2021-09-04T21:33:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94834
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
uint32_t load(const uint8_t* data)
{
    uint32_t val = 0;
    for (int i = 0; i < sizeof(val) * CHAR_BIT; i += CHAR_BIT)
    {
        val |= *data++ << i;
    }
    return val;
}

This can be optimized to a single 32-bit load. LLVM does this transformation, gcc just unrolls the loop and misses the transformation.

LLVM gives :

load(unsigned char const*): # @load(unsigned char const*)
  mov eax, dword ptr [rdi]
  ret

GCC gives :

load(unsigned char const*):
  movzx edx, BYTE PTR [rdi+1]
  movzx eax, BYTE PTR [rdi]
  sal edx, 8
  or edx, eax
  movzx eax, BYTE PTR [rdi+2]
  sal eax, 16
  or edx, eax
  movzx eax, BYTE PTR [rdi+3]
  sal eax, 24
  or eax, edx
  ret

See also https://godbolt.org/z/kmYTLZ


---


### compiler : `gcc`
### title : `Failure to optimize condition based on known value of static variable`
### open_at : `2020-04-28T23:56:36Z`
### last_modified_date : `2021-12-20T20:25:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94836
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
int f(int x)
{
    static int s;

    if (s)
        s = x;

    return s;
}

This can be optimized to `return 0`. This transformation is done by LLVM, but not by GCC

LLVM -O3 outputs :

f(int): # @f(int)
  xor eax, eax
  ret

GCC -O3 outputs :

f(int):
  mov eax, DWORD PTR f(int)::s[rip]
  test eax, eax
  je .L1
  mov DWORD PTR f(int)::s[rip], edi
  mov eax, edi
.L1:
  ret

(see also https://godbolt.org/z/FzpjCb)


---


### compiler : `gcc`
### title : `Failure to optimize out useless zero-ing after register was already zero-ed`
### open_at : `2020-04-29T01:32:48Z`
### last_modified_date : `2020-04-29T09:00:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94838
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
int f(bool b, int *p)
{
    return b && *p;
}

GCC generates this with -O3:

f(bool, int*):
  xor eax, eax
  test dil, dil
  je .L1
  mov edx, DWORD PTR [rsi]
  xor eax, eax ; This can be removed, since eax is already 0 here
  test edx, edx
  setne al
.L1:
  ret


---


### compiler : `gcc`
### title : `Failure to optimize jnc+inc into adc`
### open_at : `2020-04-29T12:05:38Z`
### last_modified_date : `2021-09-05T00:13:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94846
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
unsigned f(unsigned *p, unsigned x)
{
    unsigned u = *p;
    *p += x;
    if (u > *p)
        ++*p;

    return *p;
}

This is what LLVM outputs with -O3 : 

f(unsigned int*, unsigned int): # @f(unsigned int*, unsigned int)
  mov eax, esi
  add eax, dword ptr [rdi]
  adc eax, 0
  mov dword ptr [rdi], eax
  ret

This is what GCC outputs :

f(unsigned int*, unsigned int):
  mov eax, esi
  add eax, DWORD PTR [rdi]
  jnc .L6
  add eax, 1
.L6:
  mov DWORD PTR [rdi], eax
  ret

GCC should most likely be optimizing this to the same thing as LLVM.


---


### compiler : `gcc`
### title : `Failure to optimize operation corresponding to shrd to shrd`
### open_at : `2020-04-29T14:33:28Z`
### last_modified_date : `2020-04-29T21:24:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94850
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
struct testStruct {
        uint64_t a;
        uint64_t b;
};

uint64_t f(testStruct t, int x)
{
        return ((t.a << (64 - x)) | (t.b >> (x)));
}

LLVM produces this : 

f(testStruct, int): # @f(testStruct, int)
  mov ecx, edx
  mov rax, rsi
  shrd rax, rdi, cl
  ret

GCC produces this :

f(testStruct, int):
  mov ecx, 64
  mov rax, rsi
  sub ecx, edx
  sal rdi, cl
  mov ecx, edx
  shr rax, cl
  or rax, rdi
  ret

A similar optimization can be done for shld for this code : 

uint64_t f(uint64_t a, uint64_t b, int x)
{
        return ((a << (x)) | (b >> (64 - x)));
}


---


### compiler : `gcc`
### title : `Failure to optimize load+add+store into add on memory when getting carry flag afterwards on x86`
### open_at : `2020-04-29T17:45:57Z`
### last_modified_date : `2020-05-08T08:06:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94857
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
bool f(unsigned *p, unsigned x)
{
    unsigned u = *p;
    *p += x;
    return u > *p;
}

With -O3, LLVM outputs :

f(unsigned int*, unsigned int): # @f(unsigned int*, unsigned int)
  add dword ptr [rdi], esi
  setb al
  ret

GCC outputs :

f(unsigned int*, unsigned int):
  add esi, DWORD PTR [rdi]
  mov DWORD PTR [rdi], esi
  setc al
  ret


---


### compiler : `gcc`
### title : `Failure to recognize bzhi pattern`
### open_at : `2020-04-29T20:09:43Z`
### last_modified_date : `2020-04-30T06:52:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94860
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
uint32_t bzhi32(uint32_t x, uint32_t y)
{
  return ((x << (32 - y)) >> (32 - y));
}

LLVM with -O3 -mbmi2 optimizes this to : 

bzhi32(unsigned int, unsigned int): # @bzhi32(unsigned int, unsigned int)
  bzhi eax, edi, esi
  ret

GCC outputs this :

bzhi32(unsigned int, unsigned int):
  mov eax, 32
  sub eax, esi
  shlx edi, edi, eax
  shrx eax, edi, eax
  ret

It should be optimized down to bzhi.

This optimization can be applied to :
- x86-64 (with bzhi)
- i686 (with bzhi)
- AMDGCN (with v_bfe_u32)


---


### compiler : `gcc`
### title : `Failure to use blendps over mov when possible`
### open_at : `2020-04-29T21:54:09Z`
### last_modified_date : `2021-04-26T01:18:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94863
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
typedef double v2df __attribute__((vector_size(16)));

v2df move_sd(v2df a, v2df b)
{
    v2df result = a;
    result[0] = b[0];
    return result;
}

LLVM -O3 compiles this as such :

move_sd(double __vector(2), double __vector(2)): # @move_sd(double __vector(2), double __vector(2))
  blendps xmm0, xmm1, 3 # xmm0 = xmm1[0,1],xmm0[2,3]
  ret

GCC gives this : 

move_sd(double __vector(2), double __vector(2)):
  movsd xmm0, xmm1
  ret

Using `blendps` here should be a worthy tradeoff. Here is a table of throughputs for various CPU architectures formatted as "arch-name: blendps-throughput, movsd-throughput" :

Wolfdale: 1, 0.33
Nehalem: 1, 1
Westmere: 1, 1
Sandy Bridge: 0.5, 1
Ivy Bridge: 0.5, 1
Haswell: 0.33, 1
Broadwell: 0.33, 1
Skylake: 0.33, 1
Skylake-X: 0.33, 1
Kaby Lake: 0.33, 1
Coffee Lake: 0.33, 1
Cannon Lake: 0.33, 0.33
Ice Lake: 0.33, 0.33
Zen+: 0.5, 0.25
Zen 2: 0.33, 0.25

Unless there is an important factor other than thoughput that could affect this, this should improve performance or keep it identical on every architecture except Zen+


---


### compiler : `gcc`
### title : `Failure to combine vunpckhpd+movsd into single vunpckhpd`
### open_at : `2020-04-29T21:59:54Z`
### last_modified_date : `2023-08-22T09:35:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94864
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
typedef double v2df __attribute__((vector_size(16)));

v2df move_sd(v2df a, v2df b)
{
    v2df result = a;
    result[0] = b[1];
    return result;
}

LLVM outputs this :

move_sd(double __vector(2), double __vector(2)): # @move_sd(double __vector(2), double __vector(2))
  vunpckhpd xmm0, xmm1, xmm0 # xmm0 = xmm1[1],xmm0[1]
  ret

GCC outputs this : 

move_sd(double __vector(2), double __vector(2)):
  vunpckhpd xmm1, xmm1, xmm1
  vmovsd xmm0, xmm0, xmm1
  ret


---


### compiler : `gcc`
### title : `Failure to combine unpckhpd+unpcklpd into blendps`
### open_at : `2020-04-29T22:02:34Z`
### last_modified_date : `2023-08-22T09:35:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94865
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
typedef double v2df __attribute__((vector_size(16)));

v2df move_sd(v2df a, v2df b)
{
    v2df result = a;
    result[1] = b[1];
    return result;
}

With `-O3 -msse4.1`, LLVM gives : 

move_sd(double __vector(2), double __vector(2)): # @move_sd(double __vector(2), double __vector(2))
  blendps xmm0, xmm1, 12 # xmm0 = xmm0[0,1],xmm1[2,3]
  ret

GCC gives : 

move_sd(double __vector(2), double __vector(2)):
  unpckhpd xmm1, xmm1
  unpcklpd xmm0, xmm1
  ret


---


### compiler : `gcc`
### title : `Failure to optimize pinsrq of 0 with index 1 into movq`
### open_at : `2020-04-30T00:21:37Z`
### last_modified_date : `2023-08-24T21:50:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94866
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
typedef int64_t v2di __attribute__((vector_size(16)));
typedef int32_t v2si __attribute__((vector_size(8)));

v2di _mm_move_epi64(v2di a)
{
    return v2di{a[0], 0LL};
}

LLVM with `-O3 -msse4.1` compiles this to this : 

_mm_move_epi64(long __vector(2)): # @_mm_move_epi64(long __vector(2))
  movq xmm0, xmm0 # xmm0 = xmm0[0],zero
  ret

GCC gives :

_mm_move_epi64(long __vector(2)):
  xor eax, eax
  pinsrq xmm0, rax, 1
  ret

GCC's output seems like it would naturally be much slower, so unless there is something seriously messed up with x86 chips that I've missed, LLVM's version should be faster


---


### compiler : `gcc`
### title : `Failure to use movhlps instead of seperated mov+unpckhpd`
### open_at : `2020-04-30T08:37:23Z`
### last_modified_date : `2023-08-24T21:49:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94870
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
typedef double v2df __attribute__((vector_size(16)));

v2df _mm_sqrt_sd(v2df a, v2df b)
{
    v2df c = __builtin_ia32_sqrtpd((v2df){b[0], b[1]});
    return (v2df){c[1], a[1]};
}

With -O3, LLVM outputs :

_mm_sqrt_sd(double __vector(2), double __vector(2)):
  sqrtpd xmm1, xmm1
  movhlps xmm0, xmm1 # xmm0 = xmm1[1],xmm0[1]
  ret

GCC outputs :

_mm_sqrt_sd(double __vector(2), double __vector(2)):
  movapd xmm2, xmm0
  sqrtpd xmm0, xmm1
  unpckhpd xmm0, xmm2
  ret

unpckhpd and movhlps seem to have equivalent performance, so using movhlps to elide the extra movapd seems like it would make sense


---


### compiler : `gcc`
### title : `Failure to convert cmpeqpd+pxor with -1 into cmpneqpd`
### open_at : `2020-04-30T09:20:16Z`
### last_modified_date : `2023-08-24T21:49:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94871
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
typedef double v2df __attribute__((vector_size(16)));
typedef int64_t v2di __attribute__((vector_size(16)));
typedef int8_t v16qi __attribute__((vector_size(16)));

inline v2di set1_epi8(int8_t a)
{
    return (v2di)(v16qi){a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a};
}

v2di cmpneq_pd(v2df a, v2df b)
{
    return ((v2di)__builtin_ia32_cmpeqpd(a, b) ^ set1_epi8(0xFF));
}

With -O3, LLVM outputs : 

cmpneq_pd(double __vector(2), double __vector(2)):
  cmpneqpd xmm0, xmm1
  ret

GCC outputs :

cmpneq_pd(double __vector(2), double __vector(2)):
  cmpeqpd xmm0, xmm1
  pcmpeqd xmm1, xmm1
  pxor xmm0, xmm1
  ret


---


### compiler : `gcc`
### title : `Failure to optimize shuffle from u32 array into u64 array properly`
### open_at : `2020-04-30T10:42:43Z`
### last_modified_date : `2023-08-24T21:49:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94872
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
union u64Elems
{
    uint64_t as_u64;
    int32_t as_i32[2];
};

uint64_t f(u64Elems m1, u64Elems m2)
{
    u64Elems res;
    res.as_i32[0] = m1.as_i32[1];
    res.as_i32[1] = m2.as_i32[1];

    return res.as_u64;
}

With -O3, LLVM outputs :

f(u64Elems, u64Elems): # @f(u64Elems, u64Elems)
  shr rdi, 32
  movabs rax, -4294967296
  and rax, rsi
  or rax, rdi
  ret

GCC outputs :

f(u64Elems, u64Elems):
  sar rdi, 32
  sar rsi, 32
  movd xmm0, edi
  movd xmm1, esi
  punpckldq xmm0, xmm1
  movq rax, xmm0
  ret

With -mno-sse, it's even worse :

f(u64Elems, u64Elems):
  sar rdi, 32
  sar rsi, 32
  mov QWORD PTR [rsp-8], 0
  mov DWORD PTR [rsp-8], edi
  mov rax, QWORD PTR [rsp-8]
  mov QWORD PTR [rsp-16], rax
  mov DWORD PTR [rsp-12], esi
  mov rax, QWORD PTR [rsp-16]
  ret

Looking at the final tree representation :

f (union u64Elems m1, union u64Elems m2)
{
  int _1;
  int _2;
  long unsigned int _4;
  vector(2) int _8;

  <bb 2> [local count: 1073741824]:
  # DEBUG BEGIN_STMT
  # DEBUG BEGIN_STMT
  _1 = m1.as_i32[1];
  # DEBUG BEGIN_STMT
  _2 = m2.as_i32[1];
  _8 = {_1, _2};
  # DEBUG BEGIN_STMT
  _4 = VIEW_CONVERT_EXPR<long unsigned int>(_8);
  return _4;

}

It looks to me like the conversion to vector(2) is the culprit here, it's not getting optimized properly


---


### compiler : `gcc`
### title : `Failure to simplify ~(x + 1) to -2 - x`
### open_at : `2020-04-30T12:02:00Z`
### last_modified_date : `2023-08-24T21:48:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94877
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f(int x)
{
    return ~(x + 1);
}

With -O3, LLVM outputs this : 

f(int): # @f(int)
  mov eax, -2
  sub eax, edi
  ret

GCC outputs this :

f(int):
  lea eax, [rdi+1]
  not eax
  ret

`~x - 1` is already simplified to `-2 - x`, so optimizing this looks like it would make sense too.


---


### compiler : `gcc`
### title : `Failure to optimize div with bls/or pattern`
### open_at : `2020-04-30T12:15:54Z`
### last_modified_date : `2023-08-24T21:47:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94878
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
unsigned y(unsigned x)
{
    unsigned s = x & -x;
    return s | (x % s);
}

With -O3, LLVM outputs : 

y(unsigned int):
  blsi ecx, edi
  lea eax, [rcx - 1]
  and eax, edi
  or eax, ecx
  ret

GCC outputs :

y(unsigned int):
  mov eax, edi
  xor edx, edx
  blsi ecx, edi
  div ecx
  mov eax, edx
  or eax, ecx
  ret

If a single change to this function is applied, changing it to this :

unsigned y(unsigned x)
{
    unsigned s = x & -x;
    return x | (x % s);
}

It's equivalent to `return x`, and LLVM recognises this : 

y(unsigned int):
  mov eax, edi
  ret

While GCC does not :

y(unsigned int):
  mov eax, edi
  xor edx, edx
  blsi ecx, edi
  div ecx
  mov eax, edx
  or eax, edi
  ret


---


### compiler : `gcc`
### title : `Failure to recognize andn pattern`
### open_at : `2020-04-30T12:32:54Z`
### last_modified_date : `2023-08-24T21:47:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94880
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return (x | y) - y;
}

This can be optimized to a single andn : 

f(int, int): # @f(int, int)
  andn eax, esi, edi
  ret

(LLVM output with -O3 -mbmi)

GCC currently outputs this : 

f(int, int):
  or edi, esi
  mov eax, edi
  sub eax, esi
  ret


---


### compiler : `gcc`
### title : `Failure to optimize and+or+sub into xor+not`
### open_at : `2020-04-30T12:35:14Z`
### last_modified_date : `2023-08-24T21:47:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94882
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return (x & y) - (x | y) - 1;
}

This can be optimized to `~(x ^ y)`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to recognize that result of or is always superior to operands`
### open_at : `2020-04-30T12:52:22Z`
### last_modified_date : `2023-08-24T21:47:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94884
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
bool decide() __attribute((const));

inline unsigned getXOrY(unsigned x, unsigned y)
{
    return decide() ? y : x;
}

bool f(unsigned x, unsigned y)
{
    return (x | y) >= getXOrY(x, y);
}

`f` can be optimized to `return true`. This transformation is done by LLVM, not by GCC.


---


### compiler : `gcc`
### title : `Negate function not getting optimised to bitwise not`
### open_at : `2020-04-30T14:32:08Z`
### last_modified_date : `2023-10-12T03:46:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94889
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.3.0`
### severity : `enhancement`
### contents :
int func(int x)
{
    for(int i=0;i<sizeof(int)*8;i++)
    {
        x ^= (int)1<<i;
    }
    return x;
}

Does not get optimised to just a negate instruction


---


### compiler : `gcc`
### title : `(x >> 31) + 1 not getting narrowed to compare`
### open_at : `2020-04-30T17:25:03Z`
### last_modified_date : `2023-08-24T21:46:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94892
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
inline int sign(int x)
{
    return (x >> 31) | ((unsigned)-x >> 31);
}

bool f(int x)
{
    return sign(x) > -1;
}

With -O3, LLVM produces this :

f(int):
  test edi, edi
  setns al
  ret

GCC produces this :

f(int):
  sar edi, 31
  lea eax, [rdi+1]
  ret

Changing `f` to `(x >> 31) + 1` results in it being optimized optimally


---


### compiler : `gcc`
### title : `Sign function not getting optimized to simple compare`
### open_at : `2020-04-30T17:31:16Z`
### last_modified_date : `2023-08-24T21:46:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94893
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
inline int sign(int x)
{
    return (x >> 31) | ((unsigned)-x >> 31);
}

bool f(int x)
{
    return sign(x) < 1;
}

With -O3, LLVM outputs :

f(int):
  test edi, edi
  setle al
  ret

GCC outputs :

f(int):
  mov eax, edi
  sar edi, 31
  neg eax
  shr eax, 31
  or eax, edi
  setle al
  ret


---


### compiler : `gcc`
### title : `Failure to optimize compare plus sub of same operands into compare`
### open_at : `2020-04-30T23:58:13Z`
### last_modified_date : `2023-08-24T21:46:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94898
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
bool f(int x, int y)
{
    if (x >= y)
        return x - y;
    return 0;
}

This can be optimized to `x > y`. This transformation is done by LLVM, but not by GCC


---


### compiler : `gcc`
### title : `Failure to optimize out add before compare with INT_MIN`
### open_at : `2020-05-01T00:17:44Z`
### last_modified_date : `2023-08-24T21:45:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94899
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return x + 0x80000000 < y + 0x80000000;
}

This can be optimized to `return x < y`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to optimally optimize certain shuffle patterns`
### open_at : `2020-05-01T19:02:46Z`
### last_modified_date : `2023-08-24T21:44:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94908
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
typedef float v4sf __attribute__((vector_size(16)));

v4sf g();

v4sf f(v4sf a, v4sf b)
{
    return (v4sf){g()[1], a[1], a[2], a[3]};
}

With -O3, LLVM outputs this :

f(float __vector(4), float __vector(4)): # @f(float __vector(4), float __vector(4))
  sub rsp, 24
  movaps xmmword ptr [rsp], xmm0 # 16-byte Spill
  call g()
  movaps xmm1, xmmword ptr [rsp] # 16-byte Reload
  shufps xmm0, xmm1, 17 # xmm0 = xmm0[1,0],xmm1[1,0]
  shufps xmm0, xmm1, 232 # xmm0 = xmm0[0,2],xmm1[2,3]
  add rsp, 24
  ret

GCC outputs this : 

f(float __vector(4), float __vector(4)):
  sub rsp, 24
  movaps XMMWORD PTR [rsp], xmm0
  call g()
  movaps xmm1, XMMWORD PTR [rsp]
  add rsp, 24
  shufps xmm0, xmm0, 85
  movaps xmm2, xmm1
  shufps xmm2, xmm1, 85
  movaps xmm3, xmm2
  movaps xmm2, xmm1
  unpckhps xmm2, xmm1
  unpcklps xmm0, xmm3
  shufps xmm1, xmm1, 255
  unpcklps xmm2, xmm1
  movlhps xmm0, xmm2
  ret

This also seems to occurs on powerpc64le, so I haven't marked it as target-specific.


---


### compiler : `gcc`
### title : `Failure to optimize comparisons of VLA sizes`
### open_at : `2020-05-01T21:04:06Z`
### last_modified_date : `2023-08-24T21:44:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94911
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
inline void assume(_Bool b)
{
    if (!b)
        __builtin_unreachable();
}

_Bool f(int n)
{
    assume(n >= 1);
    typedef int A[n];
    ++n;
    A a;
    int b[n];
    n -= 2;
    typedef int C[n];
    C c;

    return (sizeof(a) < sizeof(b)) && (sizeof(a) > sizeof(c));
}

This C code (this will have different results in C++ with GCC) should always `return true`. LLVM makes this transformation, but GCC does not.

Also, extra question, is the fact that this always returns `false` when compiled as C++ normal ? Clang has it return `true` if compiled as C++.


---


### compiler : `gcc`
### title : `Failure to optimize not+cmp into overflow check`
### open_at : `2020-05-02T02:15:33Z`
### last_modified_date : `2023-08-24T21:44:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94913
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
bool f(unsigned x, unsigned y)
{
    return ~x < y;
}

With -O3, LLVM outputs this :

f(unsigned int, unsigned int):
  add edi, esi
  setb al
  ret

GCC outputs this :

f(unsigned int, unsigned int):
  not edi
  cmp edi, esi
  setb al
  ret


---


### compiler : `gcc`
### title : `Failure to optimize check of high part of 64-bit result of 32 by 32 multiplication into overflow check`
### open_at : `2020-05-02T02:24:19Z`
### last_modified_date : `2023-08-24T21:44:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94914
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
bool f(uint32_t x, uint32_t y)
{
    return (((uint64_t)x * y) >> 32) != 0;
}

This can be optimized an overflow check on a 32-bit multiplication. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `MAX_EXPR weirdly optimized on x86 with -mtune=core2`
### open_at : `2020-05-02T08:20:48Z`
### last_modified_date : `2023-08-24T21:44:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94915
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return x > y ? x : y;
}

When compiling with -O3 -mtune=core2 -msse4.1, GCC outputs this :

f(int, int):
  movd xmm0, edi
  movd xmm1, esi
  pmaxsd xmm0, xmm1
  movd eax, xmm0
  ret

It would seem rather doubtful that this is the optimal solution for doing max over a simple compare+cmov


---


### compiler : `gcc`
### title : `Failure to optimize pattern into difference or zero selector`
### open_at : `2020-05-02T08:35:57Z`
### last_modified_date : `2023-08-24T21:44:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94916
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
int f(int x, int y)
{
    return (x - y) & -(x >= y);
}

This can be optimized to return x >= y ? x - y : 0. LLVM does this transformation, but GCC does not. (also, GCC fails to optimize the resulting pattern properly, but this is already reported in https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94898)


---


### compiler : `gcc`
### title : `Failure to recognize max pattern`
### open_at : `2020-05-02T09:51:38Z`
### last_modified_date : `2023-08-24T21:44:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94919
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x, int y)
{
    return ((x ^ y) & -(x >= y)) ^ y;
}

This can be optimized to `x >= y ? x : y`. LLVM makes this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to optimize abs pattern from arithmetic with selected operands based on comparisons with 0`
### open_at : `2020-05-02T10:53:29Z`
### last_modified_date : `2023-09-17T16:44:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94920
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
unsigned f(int x)
{
    return (x >= 0 ? x : 0) + (x <= 0 ? -x : 0);
}

This can be optimized to `return abs(x)`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to optimize nots with sub into single add`
### open_at : `2020-05-02T12:39:08Z`
### last_modified_date : `2023-08-24T21:43:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94921
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return ~(~x - y);
}

This can be optimized to `x - y`. This transformation is done by LLVM, but not by GCC


---


### compiler : `gcc`
### title : `Failure to optimize out subvsi in expansion of __builtin_memcmp with 1 as the operand with -ftrapv`
### open_at : `2020-05-03T00:22:34Z`
### last_modified_date : `2023-08-24T21:43:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94930
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int memcmp1(const void *s, const void *c)
{
    return __builtin_memcmp(s, c, 1);
}

With -O3 -ftrapv, LLVM outputs this :

memcmp1(void const*, void const*): # @memcmp1(void const*, void const*)
  movzx eax, byte ptr [rdi]
  movzx ecx, byte ptr [rsi]
  sub eax, ecx
  ret

GCC outputs this :

memcmp1(void const*, void const*):
  sub rsp, 8
  movzx edi, BYTE PTR [rdi]
  movzx esi, BYTE PTR [rsi]
  call __subvsi3
  add rsp, 8
  ret


---


### compiler : `gcc`
### title : `Failure to inline addv`
### open_at : `2020-05-03T11:08:32Z`
### last_modified_date : `2023-08-24T21:43:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94934
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return x + y;
}

With -O3 -ftrapv, LLVM outputs this :

f(int, int): # @f(int, int)
  mov eax, edi
  add eax, esi
  jo .LBB0_1
  ret
.LBB0_1:
  ud2

GCC outputs this :

f(int, int):
  sub rsp, 8
  call __addvsi3
  add rsp, 8
  ret

`-ftrapv` may not be specifically intended for speed, but it seems to make sense to inline it on targets that have direct support for the instructions it uses.


---


### compiler : `gcc`
### title : `Missed optimization: Carry chain not recognized in manually unrolled loop`
### open_at : `2020-05-04T16:44:37Z`
### last_modified_date : `2021-08-20T01:22:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94945
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.1.1`
### severity : `normal`
### contents :
Context: Big integer addition using ADC (_addcarry_u64).

See Godbolt link: https://godbolt.org/z/rMxe6W

Example:
Suppose the case of big integer addition:

// pa, pb: pointer to big integer A, B
// n: size of big integer A, B
// pr: pointer to result

void add(const uint64_t * __restrict__ pa, const uint64_t * __restrict__ pb, uint64_t * __restrict__ pr, unsigned n) {
    unsigned char carry = 0;
    unsigned i;
    for(i = 0; i<n; i += 4) {
        carry = _addcarry_u64(carry, pa[i+0], pb[i+0], &pr[i+0]);
        carry = _addcarry_u64(carry, pa[i+1], pb[i+1], &pr[i+1]);
        carry = _addcarry_u64(carry, pa[i+2], pb[i+2], &pr[i+2]);
        carry = _addcarry_u64(carry, pa[i+3], pb[i+3], &pr[i+3]);
    }
}


Without loop unrolling GCC saves the Carry Flag at the end of the loop and again sets the saved carry flag in the next iteration. GCC doesn't recognize the propagation of Carry Flag across loop iterations even when manually unrolling the loop (while Clang does). GCC saves the carry and triggers it again in this fashion (2 iterations shown):

        mov     ecx, eax  # i, i
        add     r9b, -1   # carry,
        mov     rdx, QWORD PTR [rdi+rcx*8]        # tmp132, *_6
        adc     rdx, QWORD PTR [rsi+rcx*8]        # tmp132, *_4
        mov     QWORD PTR [r8+rcx*8], rdx #* pr, tmp132
        setc    r9b     #, _48   <--------SAVING CARRY

        lea     ecx, [rax+1]      # tmp134,
        mov     rdx, QWORD PTR [rdi+rcx*8]        # tmp140, *_15
        add     r9b, -1   # _48, <--------SETTING CARRY
        adc     rdx, QWORD PTR [rsi+rcx*8]        # tmp140, *_13
        mov     QWORD PTR [r8+rcx*8], rdx #* pr, tmp140
        setc    r9b     #, _47


Another optimization:
Trigger loop unrolling (without the need to manually unrolling) and propagate the carry without the need to save/set it in between.

Side Note:
Is this the fastest and optimal way to add two big integers? Considering ASM to be the last resort?


---


### compiler : `gcc`
### title : `Unable to remove impossible ffs() test for zero`
### open_at : `2020-05-05T15:38:02Z`
### last_modified_date : `2021-08-03T02:09:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94956
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Hi,

Given GCC 10 x86-64, and this code:

#include <stdint.h>
#include <string.h>

int foo(uint32_t x) {
        if (x == 0) __builtin_unreachable();
        return ffs(x) - 1;
}

I get this assembler:

atum17:~> gcc-10 -O2 -c test.c 
atum17:~> objdump --disassemble test.o
                    
The cmovne test is rather expensive for me due to high instruction latency,and I can never have zero in my situation. (It costs ~10% in a much larger graph algorithm.) I'm unable to get GCC to understand that it doesn't need it, save for using an explicit asm statement.

By contrast, Clang 10 gets this right:

atum17:~> clang-10 -O2 -c test.c        
atum17:~> objdump --disassemble test.o
                    
test.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 <foo>:
   0:	0f bc c7             	bsf    %edi,%eax
   3:	c3                   	retq   

Is it possible to get access to the raw instruction by some clever means? :-)


---


### compiler : `gcc`
### title : `extern template prevents inlining of standard library objects`
### open_at : `2020-05-05T20:09:58Z`
### last_modified_date : `2022-02-18T00:53:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94960
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `9.1.0`
### severity : `normal`
### contents :
Consider this example
void foo()
{
  std::string(1, 0);
}
(https://godbolt.org/z/AlkBBJ)
This function creates a string using the `basic_string(size_t, CharT)` constructor and then discards it. This particular constructor uses _M_construct internally, which is declared as an out of line member function. Because of this, and because the function isn't marked as `inline`, when the compiler reaches the `extern template class basic_string<char>;`, it foregoes trying to find the definition for _M_construct, instead generating a call to it, causing foo() to fully instantiate a string object and then delete it, since the compiler can't find _M_construct within its own translation unit.

This problem applies to every member function of any class which has an extern template, is defined out of line and is not marked as `inline`.


---


### compiler : `gcc`
### title : `Suboptimal AVX2 code for _mm256_zextsi128_si256(_mm_set1_epi8(-1))`
### open_at : `2020-05-05T21:41:32Z`
### last_modified_date : `2022-09-26T05:30:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94962
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Background: https://stackoverflow.com/q/61601902/

GCC emits an unnecessary "vmovdqa xmm0,xmm0" for the following code:

     __m256i mask()
    {
        return _mm256_zextsi128_si256(_mm_set1_epi8(-1));
    }

Live example on godbolt: https://gcc.godbolt.org/z/PbsQDR

I have found no way to avoid this except by resorting to inline asm.


---


### compiler : `gcc`
### title : `[11 Regression] FAIL: gcc.target/i386/pr64110.c scan-assembler vmovd[\\t ]`
### open_at : `2020-05-07T18:29:21Z`
### last_modified_date : `2020-06-16T11:32:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94988
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
commit 283cb9ea6293e813e48a1b769e1e0779918ea20a (r11-161)

Author: Richard Biener <rguenther@suse.de>
Date:   Mon Apr 27 14:45:54 2020 +0200

    tree-optimization/57359 - rewrite SM code
    
    This rewrites store-motion to process candidates where we can
    ensure order preserving separately and with no need to disambiguate
    against all stores.  Those candidates we cannot handle this way
    are validated to be independent on all stores (w/o TBAA) and then
    processed as "unordered" (all conditionally executed stores are so
    as well).
    
    This will necessary cause
      FAIL: gcc.dg/graphite/pr80906.c scan-tree-dump graphite "isl AST to Gimple succeeded"
    because the SM previously performed is not valid for exactly the PR57359
    reason, we still perform SM of qc for the innermost loop but that's not enough.
    
    There is still room for improvements because we still check some constraints
    for the order preserving cases that are only necessary in the current
    strict way for the unordered ones.  Leaving that for the furture.

caused:

FAIL: gcc.target/i386/pr64110.c scan-assembler vmovd[\\t ]


---


### compiler : `gcc`
### title : `std::terminate() and abort() do not have __builtin_unreachable() semantics`
### open_at : `2020-05-08T09:07:14Z`
### last_modified_date : `2020-05-08T13:20:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95001
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
Consider the codegen from https://godbolt.org/z/xhmBrL:

```
#include <cstddef>
#include <cstdint>
#include <cstdlib>
#include <exception>

void sum(uint32_t *__restrict a, const uint32_t *__restrict b, const uint32_t *__restrict c, size_t count)
{
    auto invoke_terminate = []{
#ifdef USE_UNREACHABLE
        __builtin_unreachable();
#else
        std::terminate();
#endif
    };
    if((((uintptr_t)a) & 15)!=0) invoke_terminate();
    if((((uintptr_t)b) & 15)!=0) invoke_terminate();
    if((((uintptr_t)c) & 15)!=0) invoke_terminate();
    if((count & 15) != 0) invoke_terminate();
    while(count != 0)
    {
        *a++ = *b++ + *c++;
        count--;
    }
}
```

It would seem that functions marked with both [[noreturn]] and noexcept do not have the same improvements on codegen as __builtin_unreachable() has. This is despite that [[noreturn]] functions returning is explicitly required to be UB in the standard, and if they are noexcept then they cannot throw an exception either.

Can noexcept functions marked [[noreturn]] please gain the same effects on codegen as __builtin_unreachable() has please?


---


### compiler : `gcc`
### title : `gcc fails to merge two identical returns`
### open_at : `2020-05-08T18:40:05Z`
### last_modified_date : `2023-08-08T06:23:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95014
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.1.1`
### severity : `enhancement`
### contents :
Given

#include <new>
union any {
    any(any&& x) noexcept {
        if (x.st < 4) {
            st = x.st;
            x.st = 0;
        } else {
            ex = x.ex;
            x.ex = nullptr;
        }
    }
    long st;
    void* ex;
};
void f(any* a, any* b) {
    new (a) any(std::move(*b));
}

gcc produces

        movq    (%rsi), %rax
        movq    %rax, (%rdi)
        movq    $0, (%rsi)
        cmpq    $3, %rax
        jg      .L2
        ret
.L2:
        ret


clang produces

        movq    (%rsi), %rax
        cmpq    $3, %rax
        movq    %rax, (%rdi)
        movq    $0, (%rsi)
        retq

So clang has a redundant cmpq and gcc has the cmpq and two identical branch destinations.


---


### compiler : `gcc`
### title : `[10/11 Regression] Excessive unrolling for Fortran library array handling`
### open_at : `2020-05-09T07:58:29Z`
### last_modified_date : `2020-06-19T14:39:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95018
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 48488
Generated assembly

The code generated for in_pack_i4.c from libgfortran on POWER9 is huge
and, presumably, slow; I assume that other code in the library is
similarly affected.

The problem manifests itself in the unrolling of the loops which
do all the work:

  while (src)
    {
      /* Copy the data.  */
      *(dest++) = *src;
      /* Advance to the next element.  */
      src += stride0;
      count[0]++;
      /* Advance to the next source element.  */
      index_type n = 0;
      while (count[n] == extent[n])
        {
          /* When we get to the end of a dimension, reset it and increment
             the next dimension.  */
          count[n] = 0;
          /* We could precalculate these products, but this is a less
             frequently used path so probably not worth it.  */
          src -= stride[n] * extent[n];
          n++;
          if (n == dim)
            {
              src = NULL;
              break;
            }
          else
            {
              count[n]++;
              src += stride[n];
            }
        }
    }
  return destptr;
}

One problem here is the while (count[n] == extent[n]) loop.
This is an odometer algorithm to look for the next element to
go to. Most of the times, the while is false, so it is definitely
not good.

x86_64 does not appear to be affected.


---


### compiler : `gcc`
### title : `Optimizer produces suboptimal code related to -ftree-ivopts`
### open_at : `2020-05-09T08:16:36Z`
### last_modified_date : `2020-05-13T07:09:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95019
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
For the following code, we can known the variable C000005A1 is only used for the offset of array Dest and Src, and the unit size of the array is 8 bytes, so an iv variable with step 8 will be good for targets, whose load/store insns don't folded the lshift operand.

typedef unsigned long long UINT64;

void C00000ADA(UINT64 len, long long *__restrict Src, long long *__restrict Dest)
{
    UINT64 C00000ADD, index, C00000068, offset, C00000ADF;
    UINT64 C000005A1 = 0;
    
    for (index = 0; index < len; index++) {
        
        Dest[C000005A1] =  Src[C000005A1] * Src[C000005A1];
        C000005A1 += len - index;
    }
}

test base on the MIPS64 gcc 5.4 on https://gcc.godbolt.org, as the MIPS64 target doesn't have load/store folded the lshift operand such as 'ldr     x3, [x1, x4, lsl 3]' in ARM64 targets , so use ivtmp with step 8 can eliminate the dsll insn, which is in the kernel loop.

@@ -2,16 +2,17 @@ C00000ADA(unsigned long long, long long*, long long*):
         beq     $4,$0,.L10         #, len,,
         move    $7,$0    # C000005A1,
 
+        dsll    $8,$4,3  # tmp, len << 3  
+
 .L4:
-        dsll    $2,$7,3  # D.2019, C000005A1,
-        daddu   $3,$5,$2       # tmp204, Src, D.2019
+        daddu   $3,$5,$7       # tmp204, Src, D.2019
         ld      $3,0($3)     # D.2021, *_10
-        daddu   $2,$6,$2       # tmp205, Dest, D.2019
+        daddu   $2,$6,$7       # tmp205, Dest, D.2019
         dmult   $3,$3  # D.2021, D.2021
         daddu   $7,$7,$4       # C000005A1, C000005A1, ivtmp.6
-        daddiu  $4,$4,-1     # ivtmp.6, ivtmp.6,
+        daddiu  $4,$4,-8     # ivtmp.6, ivtmp.6,
         mflo    $3       # D.2021
-        bne     $4,$0,.L4  #, ivtmp.6,,
+        bne     $8,$0,.L4  #, ivtmp.6,,
         sd      $3,0($2)     # D.2021, *_8
 
 .L10:


---


### compiler : `gcc`
### title : `Failure to convert xor pattern (made out of or+and) to xor`
### open_at : `2020-05-10T09:48:44Z`
### last_modified_date : `2023-10-25T13:14:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95034
### status : `NEW`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool combine(bool a, bool b)
{
    return (a || b) && !(a && b);
}

This can be converted to `a ^ b`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Vectorize V2SFmode operations`
### open_at : `2020-05-11T07:12:21Z`
### last_modified_date : `2021-12-27T09:12:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95046
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
The compiler should vectorize V2SF operations using XMM registers.

The same principles as applied to integer MMX operations (mmx-with-sse) should also apply to V2SF mode operations, but to avoid unwanted secondary effects (e.g. exceptions) extra care should be taken to load values to registers with parts outside V2SFmode cleared.

Following testcase:

--cut here--
float r[2], a[2], b[2];

void foo (void)
{
  for (int i = 0; i < 2; i++)
    r[i] = a[i] + b[i];
}
--cut here--

should vectorize to:

        movq    a(%rip), %xmm0
        movq    b(%rip), %xmm1
        addps   %xmm1, %xmm0
        movlps  %xmm0, r(%rip)
 
Please note movq insn that assures clearing of top 64bits of 128bit xmm register.


---


### compiler : `gcc`
### title : `[10 Regression] Excess padding of partially initialized strings/char arrays`
### open_at : `2020-05-11T10:50:53Z`
### last_modified_date : `2023-07-07T08:52:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95052
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 48506
generated assembly (GCC 11.0 trunk, -Os -g0)

When compiling the following code with -Os:

  extern void func(char *buf, unsigned size);
  int main(int argc, char *argv[])
  {
    char str[1*1024*1024] = "fooiuhluhpiuhliuhliyfyukyfklyugkiuhpoipoipoipoipoipoipoipoipoipoipoipoipoimipoipiuhoulouihnliuhl";
    char arr[1*1024*1024] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 6, 2, 3, 4, 5, 6, 7, 8, 9, 0, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0};
    func(str, sizeof(str));
    func(arr, sizeof(arr));
 }

GCC generates initializers for both local variables (str and arr) in the .rodata section and at run-time initializes the explicit part of the variable with the provided contents, and zero-inits the remainder.

Unfortunately the initializer stored in the .rodata section is padded up to the target array size:

.LC0:
        .string "fooiuhluhpiuhliuhliyfyukyfklyugkiuhpoipoipoipoipoipoipoipoipoipoipoipoipoimipoipiuhoulouihnliuhl"
        .zero   1048479
.LC1:
        .string "\001\026\003\004\005?\007\b'"
        .string "\001\002\003\004\005>\033\b1"
        .string "\001\006\002\003\004\005\006\007\b\t"
        .string "\003\001\002\003\004\005\006\007\b\t"
        .string "\001\002\003\004\005\006\007\b\t"
        .string "\001\002\003\004\005\006\007\b\t"
        .string "\002\002\003\004\005>\033\b1"
        .string "\001\006\002\003\004\005\006\007\b\t"
        .string "\003\001\002\003\004\005\006\007\b\t"
        .string "\001\002\003\004\005\006\007\b\t"
        .string "\001\002\003\004\005"
        .zero   1048466

This causes the resulting binary to become unnecessarily large, even though the zero padding is completely redundant (the run-time initializer code does not copy these bytes to the target variable, but zero-initializes them.

I suspect that this is caused by GCC not being able to distinguish between:
 - initialization of a global (or static local) variable,
 - initialization of a local variable

In the former case the contents of the variable live in the read/write data section and are initialized by the compiler. In such case padding is necessary as any further changes to the variable will be done in-place.

In the latter case the contents of the variable live on the stack and are initialized from a read-only copy in the read-only data section. In such case only the non-zero explicitly initialized part needs to be stored - any padding can be skipped as it will not be used.

This mis-optimization occurs depending on compiler flags, architecture and size of the array as well as the initialized part, as GCC may choose (and usually does) to initialize the variable by using store assembly instructions with immediate values, as this method is usually faster at the cost of increased code size.


---


### compiler : `gcc`
### title : `[11 Regression] slp-perm-9.c fails on aarch64 after gbc484e250990393e887f7239157cc85ce6fadcce`
### open_at : `2020-05-11T12:48:00Z`
### last_modified_date : `2020-12-08T12:10:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95056
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Hi,

I've noticed that
FAIL: gcc.dg/vect/slp-perm-9.c -flto -ffat-lto-objects  scan-tree-dump-times vect "permutation requires at least three vectors" 1
FAIL: gcc.dg/vect/slp-perm-9.c scan-tree-dump-times vect "permutation requires at least three vectors" 1

on aarch64

since commit gbc484e250990393e887f7239157cc85ce6fadcce


---


### compiler : `gcc`
### title : `vfnmsub132ps is not generated with -ffast-math`
### open_at : `2020-05-11T15:07:42Z`
### last_modified_date : `2021-08-10T23:13:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95060
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
float r[8], a[8], b[8], c[8];

void
test_fnms (void)
{
  for (int i = 0; i < 8; i++)
    r[i] = -(a[i] * b[i]) - c[i];
}
--cut here--

compiles on x86_64 with "-O3 -mfma" to

        vmovaps b(%rip), %ymm0
        vmovaps c(%rip), %ymm1
        vfnmsub132ps    a(%rip), %ymm1, %ymm0
        vmovaps %ymm0, r(%rip)
        vzeroupper
        ret

However, when -ffast-math is added, negation gets moved out of the insn:

        vmovaps b(%rip), %ymm0
        vmovaps c(%rip), %ymm1
        vfmadd132ps     a(%rip), %ymm1, %ymm0
->      vxorps  .LC0(%rip), %ymm0, %ymm0
        vmovaps %ymm0, r(%rip)
        vzeroupper
        ret


---


### compiler : `gcc`
### title : `Failure to tail-call on function call of different return type`
### open_at : `2020-05-12T07:47:48Z`
### last_modified_date : `2023-08-24T21:43:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95076
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
long long f();

int g()
{
    return f();
}

With -O3, LLVM outputs :

g(): # @g()
  jmp f() # TAILCALL

GCC outputs : 

g():
  sub rsp, 8
  call f()
  add rsp, 8
  ret


---


### compiler : `gcc`
### title : `Missing fwprop for SIB address`
### open_at : `2020-05-12T09:18:49Z`
### last_modified_date : `2021-05-20T09:34:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95078
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c

int foo (int* p1, int* p2, int scale)
{
    int ret = *(p1 + scale * 4 + 11);
    *p2 = 3;
    int ret2 = *(p1 + scale * 4 + 11);
    return ret + ret2;
}

gcc11 -O2 test.c -S 

foo(int*, int*, int):
        sall    $2, %edx
        movslq  %edx, %rdx
        leaq    44(%rdi,%rdx,4), %rdx  --- redundant could be fwprop
        movl    (%rdx), %eax
        movl    $3, (%rsi)
        addl    (%rdx), %eax
        ret

fwprop failed to propagate this because it think cost of address 44(%rdi,%rdx,4) is more expensive than (%rdx), that's correct locally, but under global view, if it could be propagated into both movl, leaq would be eliminated, which benifits performance.

The ideal place to handle this issue is TER opt in pass_expand, but currently TER only handle simple situation ---- single use and block level
 
 48   A pass is made through the function, one block at a time.  No cross block                                                                                                                                                                                                           
 49   information is tracked.                                                                                                                                                                                                                                                             
 50                                                                                                                                                                                                                                                                                       
 51   Variables which only have one use, and whose defining stmt is considered                                                                                                                                                                                                            
 52   a replaceable expression (see ssa_is_replaceable_p) are tracked to see whether                                                                                                                                                                                                      
 53   they can be replaced at their use location.       

Should TER be extended?

Another testcase has this issue in more complex cfg

Refer to
https://godbolt.org/z/ofjH9R


---


### compiler : `gcc`
### title : `unorderd_map::insert_or_assign and try_emplace should only hash and mod once unless there is a rehash.`
### open_at : `2020-05-12T11:00:56Z`
### last_modified_date : `2021-03-02T16:49:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95079
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.1.0`
### severity : `normal`
### contents :
Currently insert_or_assign() and try_emplace() call find(key) and fall back to emplace(...) if that fails to find the key. The computed hash (and more importantly in general) the modded bucket index computed by find() is thrown away and recomputed by emplace(). Instead they should compute the hash once, and unless there is a rehash, also only do the modulus once. This optimization is already performed for operator[].

https://godbolt.org/z/cw82RC shows that the hasher is invoked once for operator[] and twice for insert_or_assign(). http://quick-bench.com/ge8Suq7PcdRKm6IBQbjvwuXhW6Y shows that there is a significant performance difference (20% in this test).

(I know std::unordered_map is always going to be less than fast on 64-bit platforms, but it doesn't need to be slower than it needs to be...)


---


### compiler : `gcc`
### title : `x86 fp_movcc expansion depends on real_cst sharing`
### open_at : `2020-05-12T13:41:08Z`
### last_modified_date : `2023-07-20T08:42:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95083
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
I see gcc.target/i386/avxfp-1.c FAILing, which is

double x;
void
t()
{
  x=x>5?x:5;
}

double x;
void
q()
{
  x=x<5?x:5;
}

and q() recognized as FP min by ix86_expand_fp_movcc because the doesn't
pass prepare_cmp_insn () and later ifcvt matches up the originally
distinct pseudos for the two mentions of '5'.  For t() prepare_cmp_insn ()
succeeeds and ix86_expand_fp_movcc expands this to a UNSPEC_BLEND
(because the two mentions of '5' get a different pseudo so this doesn't
look like a max).  The first prepare_cmp_insn fails because it is fed

(lt (reg:DF 82 [ x.3_1 ])
    (const_double:DF 5.0e+0 [0x0.ap+3]))

and appearantly we cannot do a lt compare(?) (but later during ifcvt we can).

Note the above is when expanding from a COND_EXPR, thus

t ()
{
  double x.1_1;
  double iftmp.0_3;

;;   basic block 2, loop depth 0
;;    pred:       ENTRY
  x.1_1 = x;
  iftmp.0_3 = x.1_1 > 5.0e+0 ? x.1_1 : 5.0e+0;
  x = iftmp.0_3;
  return;

and

q ()
{
  double x.3_1;
  double iftmp.2_3;

;;   basic block 2, loop depth 0
;;    pred:       ENTRY
  x.3_1 = x;
  iftmp.2_3 = x.3_1 < 5.0e+0 ? x.3_1 : 5.0e+0;
  x = iftmp.2_3;
  return;

similar FAILs occur for

FAIL: gcc.target/i386/avxfp-1.c scan-assembler vmaxsd
FAIL: gcc.target/i386/avxfp-2.c scan-assembler vminsd
FAIL: gcc.target/i386/ssefp-1.c scan-assembler maxsd
FAIL: gcc.target/i386/ssefp-2.c scan-assembler minsd

So what's missing is simplification of 

Trying 8 -> 9:
    8: r87:DF=r85:DF<r82:DF
    9: r84:DF=unspec[r85:DF,r82:DF,r87:DF] 105
      REG_DEAD r87:DF
      REG_DEAD r85:DF
      REG_DEAD r82:DF
Failed to match this instruction:
(set (reg:DF 84)
    (unspec:DF [
            (reg:DF 85)
            (reg:DF 82 [ x.1_1 ])
            (lt:DF (reg:DF 85)
                (reg:DF 82 [ x.1_1 ]))
        ] UNSPEC_BLENDV))

to UNSPEC_MIN/MAX I guess?


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] code sinking prevents if-conversion`
### open_at : `2020-05-12T13:46:09Z`
### last_modified_date : `2023-08-04T17:26:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95084
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
There's a pass ordering issue between the sink pass and tree-if-conv, if conversion for vectorization.  When sink sinks a possibly trapping operation
to a place that is only conditionally executed if-conversion fails which
results in failed vectorization.  This can be seen with
https://gcc.gnu.org/pipermail/gcc-patches/2020-May/545588.html applied
for gcc.dg/vect/pr56541.c (and it's ifcvt counterpart gcc.dg/tree-ssa/ifc-pr56541.c).  But I've also seen this in other context.

Here

  iftmp.2_17 = rR_19 < rL_20 ? rR_19 : rL_20;
  iftmp.3_3 = rR_19 < rL_20 ? rL_20 : rR_19;
  if (iftmp.3_3 > 0.0)
    goto <bb 5>; [INV]
  else
    goto <bb 4>; [INV]

  <bb 4> :

  <bb 5> :
  # iftmp.4_14 = PHI <iftmp.2_17(3), 1.5e+2(4)>
  if (iftmp.4_14 > 0.0)

becomes

  iftmp.3_3 = rR_17 < rL_18 ? rL_18 : rR_17;
  if (iftmp.3_3 > 0.0)
    goto <bb 4>; [59.00%]
  else
    goto <bb 9>; [41.00%]

  <bb 9> [local count: 435831803]:
  goto <bb 6>; [100.00%]

  <bb 4> [local count: 627172605]:
  iftmp.2_15 = rR_17 < rL_18 ? rR_17 : rL_18;
  if (iftmp.2_15 > 0.0)

and the now conditionally executed FP comparison can trap.


---


### compiler : `gcc`
### title : `Missed optimization with bitfield value ranges`
### open_at : `2020-05-13T03:00:52Z`
### last_modified_date : `2023-07-19T04:08:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95097
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
#include <stdint.h>
struct foo {
	uint32_t x:20;
};
int bar(struct foo f)
{
	if (f.x) {
		uint32_t y = (uint32_t)f.x*4096;
		if (y<200) return 1;
		else return 2;
	}
	return 3;
}

Here, truth of the condition f.x implies y>=4096, but GCC does not DCE the y<200 test and return 1 codepath.

I actually had this come up in real world code, where I was considering use of an inline function with nontrivial low size cases when a "page count" bitfield is zero, where I expected these nontrivial cases to be optimized out based on already having tested that the page count being nonzero, but GCC was unable to do it. LLVM/clang does it.


---


### compiler : `gcc`
### title : `missed if-conversion`
### open_at : `2020-05-13T08:58:43Z`
### last_modified_date : `2021-07-20T07:27:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95102
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
If you rewrite gcc.target/i386/pr54855-9.c to a form GIMPLE looks like after
some PRE you end up with

typedef float vec __attribute__((vector_size(16)));

vec
foo (vec x, float a)
{
  if (!(x[0] < a))
    x[0] = a;
  return x;
}

which is no longer recognized as the same and emits

foo:
.LFB0:
        .cfi_startproc
        comiss  %xmm0, %xmm1
        ja      .L2
        movss   %xmm1, %xmm0
.L2:
        ret

instead of

foo:
.LFB1:  
        .cfi_startproc
        minss   %xmm1, %xmm0
        ret

this is because RTL if-conversion does not recognize

    7: r86:SF=vec_select(r84:V4SF,parallel)
    8: flags:CCFP=cmp(r85:SF,r86:SF)
      REG_DEAD r86:SF
    9: pc={(flags:CCFP>0)?L14:pc}
      REG_DEAD flags:CCFP
      REG_BR_PROB 536870916

   10: NOTE_INSN_BASIC_BLOCK 3
   12: r84:V4SF=vec_merge(vec_duplicate(r85:SF),r84:V4SF,0x1)
      REG_DEAD r85:SF

   14: L14:
   15: NOTE_INSN_BASIC_BLOCK 4
   20: xmm0:V4SF=r84:V4SF

the form it does recognize is

    8: r82:SF=vec_select(r84:V4SF,parallel)
    9: flags:CCFP=cmp(r85:SF,r82:SF)
   10: pc={(flags:CCFP>0)?L28:pc}
      REG_DEAD flags:CCFP
      REG_BR_PROB 536870916

   28: L28:
   14: NOTE_INSN_BASIC_BLOCK 3
    5: r85:SF=r82:SF
      REG_DEAD r82:SF

   15: L15:
   16: NOTE_INSN_BASIC_BLOCK 4
   18: r87:V4SF=vec_merge(vec_duplicate(r85:SF),r84:V4SF,0x1)


---


### compiler : `gcc`
### title : `Unoptimal code for vectorized conversions`
### open_at : `2020-05-14T10:04:29Z`
### last_modified_date : `2021-08-03T03:35:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95125
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Following testcase

--cut here--
float f[4];
double d[4];
int i[4];

void
float_truncate (void)
{
  for (int n = 0; n < 4; n++)
    f[n] = d[n];
}

void
float_extend (void)
{
  for (int n = 0; n < 4; n++)
    d[n] = f[n];
}

void
float_float (void)
{
  for (int n = 0; n < 4; n++)
    f[n] = i[n];
}

void
fix_float (void)
{
  for (int n = 0; n < 4; n++)
    i[n] = f[n];
}

void
float_double (void)
{
  for (int n = 0; n < 4; n++)
    d[n] = i[n];
}

void
fix_double (void)
{
  for (int n = 0; n < 4; n++)
    i[n] = d[n];
}
--cut here--

when compiled with "-O3 -mavx" should result in a single conversion instruction.

float_truncate:
        vxorps  %xmm0, %xmm0, %xmm0
        vcvtsd2ss       d+8(%rip), %xmm0, %xmm2
        vmovaps %xmm2, %xmm3
        vcvtsd2ss       d(%rip), %xmm0, %xmm1
        vcvtsd2ss       d+16(%rip), %xmm0, %xmm2
        vcvtsd2ss       d+24(%rip), %xmm0, %xmm0
        vunpcklps       %xmm0, %xmm2, %xmm2
        vunpcklps       %xmm3, %xmm1, %xmm0
        vmovlhps        %xmm2, %xmm0, %xmm0
        vmovaps %xmm0, f(%rip)
        ret

float_extend:
        vcvtps2pd       f(%rip), %xmm0
        vmovapd %xmm0, d(%rip)
        vxorps  %xmm0, %xmm0, %xmm0
        vmovlps f+8(%rip), %xmm0, %xmm0
        vcvtps2pd       %xmm0, %xmm0
        vmovapd %xmm0, d+16(%rip)
        ret

float_float:
        vcvtdq2ps       i(%rip), %xmm0
        vmovaps %xmm0, f(%rip)
        ret

fix_float:
        vcvttps2dq      f(%rip), %xmm0
        vmovdqa %xmm0, i(%rip)
        ret

float_double:
        vcvtdq2pd       i(%rip), %xmm0
        vmovapd %xmm0, d(%rip)
        vpshufd $238, i(%rip), %xmm0
        vcvtdq2pd       %xmm0, %xmm0
        vmovapd %xmm0, d+16(%rip)
        ret

fix_double:
        pushq   %rbp
        vmovapd d(%rip), %xmm1
        vinsertf128     $0x1, d+16(%rip), %ymm1, %ymm0
        movq    %rsp, %rbp
        vcvttpd2dqy     %ymm0, %xmm0
        vmovdqa %xmm0, i(%rip)
        vzeroupper
        popq    %rbp
        ret

Clang manages to emit optimal code.


---


### compiler : `gcc`
### title : `[10/11/12/13 Regression] Missed opportunity to turn static variables into immediates`
### open_at : `2020-05-14T10:16:30Z`
### last_modified_date : `2022-06-10T13:55:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95126
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.2.1`
### severity : `enhancement`
### contents :
Example:

For:

struct small{ short a,b; signed char c; };

void call_func(void)
{
	extern int func(struct small X);
	static struct small const s = { 1,2,0 };
        func(s);
}

clang renders (x86_64):

0000000000000000 <call_func>:
   0:	bf 01 00 02 00       	mov    edi,0x20001
   5:	e9 00 00 00 00       	jmp    a <call_func+0xa>	6: R_X86_64_PLT32	func-0x4

whereas gcc renders:

0000000000000000 <call_func>:
   0:	0f b7 3d 00 00 00 00 	movzx  edi,WORD PTR [rip+0x0]        # 7 <call_func+0x7>	3: R_X86_64_PC32	.rodata-0x2
   7:	0f b7 05 00 00 00 00 	movzx  eax,WORD PTR [rip+0x0]        # e <call_func+0xe>	a: R_X86_64_PC32	.rodata-0x4
   e:	48 c1 e7 10          	shl    rdi,0x10
  12:	48 09 f8             	or     rax,rdi
  15:	0f b7 3d 00 00 00 00 	movzx  edi,WORD PTR [rip+0x0]        # 1c <call_func+0x1c>	18: R_X86_64_PC32	.rodata
  1c:	48 c1 e7 20          	shl    rdi,0x20
  20:	48 09 c7             	or     rdi,rax
  23:	e9 00 00 00 00       	jmp    28 <call_func+0x28>	24: R_X86_64_PLT32	func-0x4


https://gcc.godbolt.org/z/Qxq6Rh


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] bogus -Wstringop-overflow for a loop unrolled past the end of an array`
### open_at : `2020-05-14T19:19:14Z`
### last_modified_date : `2023-07-07T10:37:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95140
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
As reported in https://bugzilla.redhat.com/show_bug.cgi?id=1835906, compiling loops that copy a variable number of elements to a trailing character array member results in many spurious instances of -Wstringop-overflow (below).

The reporter expects "No warnings about an overflow, and little or no code to handle c > 8, as that would be undefined behaviour" and adds "It would be nice if GCC still warned if a function like f was called with a value of c that was a compile time constant > 8 however."

$ cat rhbz1835906.c && gcc -O3 -S -Wall -fdump-tree-strlen=/dev/stdout rhbz1835906.c
struct A
{
  char v[8];
};

void f (struct A *p, char * s, int c)
{
  for (int i = 0; i < c; ++i)
    p->v[i] = s[i];
}

;; Function f (f, funcdef_no=0, decl_uid=1934, cgraph_uid=1, symbol_order=0)

Created preheader block for loop 2
Created preheader block for loop 1
;; 4 loops found
;;
;; Loop 0
;;  header 0, latch 1
;;  depth 0, outer -1
;;  nodes: 0 1 2 3 4 28 5 30 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 27 23 29 24 25 26
;;
;; Loop 2
;;  header 23, latch 29
;;  depth 1, outer 0
;;  nodes: 23 29
;;
;; Loop 1
;;  header 5, latch 30
;;  depth 1, outer 0
;;  nodes: 5 30
;; 2 succs { 3 26 }
;; 3 succs { 14 4 }
;; 4 succs { 28 6 }
;; 28 succs { 5 }
;; 5 succs { 30 6 }
;; 30 succs { 5 }
;; 6 succs { 7 24 }
;; 7 succs { 8 24 }
;; 8 succs { 9 24 }
;; 9 succs { 10 24 }
;; 10 succs { 11 24 }
;; 11 succs { 12 24 }
;; 12 succs { 13 24 }
;; 13 succs { 24 }
;; 14 succs { 15 25 }
;; 15 succs { 16 25 }
;; 16 succs { 17 25 }
;; 17 succs { 18 25 }
;; 18 succs { 19 25 }
;; 19 succs { 20 25 }
;; 20 succs { 21 25 }
;; 21 succs { 22 25 }
;; 22 succs { 27 25 }
;; 27 succs { 23 }
;; 23 succs { 29 25 }
;; 29 succs { 23 }
;; 24 succs { 25 }
;; 25 succs { 26 }
;; 26 succs { 1 }
rhbz1835906.c: In function ‘f’:
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset [8, 2147483640] to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset [9, 2147483641] to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset [10, 2147483642] to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset [11, 2147483643] to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset [12, 2147483644] to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset [13, 2147483645] to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset [14, 2147483646] to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
rhbz1835906.c:9:13: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]
    9 |     p->v[i] = s[i];
      |     ~~~~~~~~^~~~~~
rhbz1835906.c:3:8: note: at offset 8 to object ‘v’ with size 8 declared here
    3 |   char v[8];
      |        ^
f (struct A * p, char * s, int c)
{
  unsigned long ivtmp.24;
  sizetype ivtmp.15;
  vector(8) char * vectp_p.12;
  vector(8) char * vectp_p.11;
  vector(8) char vect__3.10;
  vector(8) char * vectp_s.9;
  vector(8) char * vectp_s.8;
  int tmp.7;
  unsigned int niters_vector_mult_vf.6;
  unsigned int bnd.5;
  unsigned int niters.4;
  int i;
  unsigned int _4;
  unsigned int _5;
  char _11;
  ssizetype _12;
  char * _13;
  _Bool _17;
  sizetype _19;
  _Bool _20;
  _Bool _21;
  char _26;
  sizetype _33;
  char * _34;
  char _35;
  char _54;
  char * _60;
  char _61;
  char * _67;
  char _68;
  char * _74;
  char _75;
  char * _81;
  char _82;
  char * _88;
  char _89;
  char * _95;
  char _96;
  char * _102;
  char _103;
  sizetype _108;
  char * _109;
  char _110;
  sizetype _115;
  char * _116;
  char _117;
  sizetype _122;
  char * _123;
  char _124;
  sizetype _129;
  char * _130;
  char _131;
  sizetype _136;
  char * _137;
  char _138;
  sizetype _143;
  char * _144;
  char _145;
  unsigned int _159;
  unsigned int _160;

  <bb 2> [local count: 118111600]:
  if (c_7(D) > 0)
    goto <bb 3>; [89.00%]
  else
    goto <bb 26>; [11.00%]

  <bb 3> [local count: 105119324]:
  _5 = (unsigned int) c_7(D);
  _4 = _5 + 4294967295;
  _17 = _4 > 6;
  _13 = s_8(D) + 1;
  _12 = p_9(D) - _13;
  _19 = (sizetype) _12;
  _20 = _19 > 6;
  _21 = _20 & _17;
  if (_21 != 0)
    goto <bb 4>; [80.00%]
  else
    goto <bb 14>; [20.00%]

  <bb 4> [local count: 84095460]:
  bnd.5_39 = _5 >> 3;
  vect__3.10_152 = MEM <vector(8) char> [(char *)s_8(D)];
  MEM <vector(8) char> [(char *)p_9(D)] = vect__3.10_152;
  vectp_s.8_154 = s_8(D) + 8;
  vectp_p.11_155 = p_9(D) + 8;
  if (bnd.5_39 > 1)
    goto <bb 28>; [83.33%]
  else
    goto <bb 6>; [16.67%]

  <bb 28> [local count: 70079550]:

  <bb 5> [local count: 70079549]:
  # ivtmp.24_148 = PHI <0(28), ivtmp.24_158(30)>
  vect__3.10_45 = MEM[base: s_8(D), index: ivtmp.24_148, step: 8, offset: 8B];
  MEM[base: p_9(D), index: ivtmp.24_148, step: 8, offset: 8B] = vect__3.10_45;
  ivtmp.24_158 = ivtmp.24_148 + 1;
  _159 = (unsigned int) ivtmp.24_158;
  _160 = _159 + 1;
  if (_160 < bnd.5_39)
    goto <bb 30>; [83.33%]
  else
    goto <bb 6>; [16.67%]

  <bb 30> [local count: 58399624]:
  goto <bb 5>; [100.00%]

  <bb 6> [local count: 84095460]:
  niters_vector_mult_vf.6_40 = bnd.5_39 << 3;
  tmp.7_41 = (int) niters_vector_mult_vf.6_40;
  if (_5 == niters_vector_mult_vf.6_40)
    goto <bb 24>; [12.50%]
  else
    goto <bb 7>; [87.50%]

  <bb 7> [local count: 73583527]:
  _108 = (sizetype) tmp.7_41;
  _109 = s_8(D) + _108;
  _110 = *_109;
  p_9(D)->v[tmp.7_41] = _110;
  i_112 = tmp.7_41 + 1;
  if (c_7(D) > i_112)
    goto <bb 8>; [89.00%]
  else
    goto <bb 24>; [11.00%]

  <bb 8> [local count: 65489342]:
  _115 = (sizetype) i_112;
  _116 = s_8(D) + _115;
  _117 = *_116;
  p_9(D)->v[i_112] = _117;
  i_119 = i_112 + 1;
  if (c_7(D) > i_119)
    goto <bb 9>; [89.00%]
  else
    goto <bb 24>; [11.00%]

  <bb 9> [local count: 58285513]:
  _122 = (sizetype) i_119;
  _123 = s_8(D) + _122;
  _124 = *_123;
  p_9(D)->v[i_119] = _124;
  i_126 = i_119 + 1;
  if (c_7(D) > i_126)
    goto <bb 10>; [89.00%]
  else
    goto <bb 24>; [11.00%]

  <bb 10> [local count: 51874105]:
  _129 = (sizetype) i_126;
  _130 = s_8(D) + _129;
  _131 = *_130;
  p_9(D)->v[i_126] = _131;
  i_133 = i_126 + 1;
  if (c_7(D) > i_133)
    goto <bb 11>; [89.00%]
  else
    goto <bb 24>; [11.00%]

  <bb 11> [local count: 46167954]:
  _136 = (sizetype) i_133;
  _137 = s_8(D) + _136;
  _138 = *_137;
  p_9(D)->v[i_133] = _138;
  i_140 = i_133 + 1;
  if (c_7(D) > i_140)
    goto <bb 12>; [89.00%]
  else
    goto <bb 24>; [11.00%]

  <bb 12> [local count: 41089477]:
  _143 = (sizetype) i_140;
  _144 = s_8(D) + _143;
  _145 = *_144;
  p_9(D)->v[i_140] = _145;
  i_147 = i_140 + 1;
  if (c_7(D) > i_147)
    goto <bb 13>; [89.00%]
  else
    goto <bb 24>; [11.00%]

  <bb 13> [local count: 36569637]:
  _33 = (sizetype) i_147;
  _34 = s_8(D) + _33;
  _35 = *_34;
  p_9(D)->v[i_147] = _35;
  i_37 = i_147 + 1;
  goto <bb 24>; [100.00%]

  <bb 14> [local count: 21023864]:
  _11 = *s_8(D);
  p_9(D)->v[0] = _11;
  if (c_7(D) > 1)
    goto <bb 15>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 15> [local count: 18711240]:
  _54 = *_13;
  p_9(D)->v[1] = _54;
  if (c_7(D) > 2)
    goto <bb 16>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 16> [local count: 16653003]:
  _60 = s_8(D) + 2;
  _61 = *_60;
  p_9(D)->v[2] = _61;
  if (c_7(D) > 3)
    goto <bb 17>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 17> [local count: 14821173]:
  _67 = s_8(D) + 3;
  _68 = *_67;
  p_9(D)->v[3] = _68;
  if (c_7(D) > 4)
    goto <bb 18>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 18> [local count: 13190844]:
  _74 = s_8(D) + 4;
  _75 = *_74;
  p_9(D)->v[4] = _75;
  if (c_7(D) > 5)
    goto <bb 19>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 19> [local count: 11739850]:
  _81 = s_8(D) + 5;
  _82 = *_81;
  p_9(D)->v[5] = _82;
  if (c_7(D) > 6)
    goto <bb 20>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 20> [local count: 10448467]:
  _88 = s_8(D) + 6;
  _89 = *_88;
  p_9(D)->v[6] = _89;
  if (c_7(D) > 7)
    goto <bb 21>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 21> [local count: 9299136]:
  _95 = s_8(D) + 7;
  _96 = *_95;
  p_9(D)->v[7] = _96;
  if (c_7(D) > 8)
    goto <bb 22>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 22> [local count: 8276231]:
  _102 = s_8(D) + 8;
  _103 = *_102;
  p_9(D)->v[8] = _103;
  if (c_7(D) > 9)
    goto <bb 27>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 27> [local count: 7365846]:

  <bb 23> [local count: 7365845]:
  # ivtmp.15_151 = PHI <9(27), ivtmp.15_150(29)>
  _26 = MEM[base: s_8(D), index: ivtmp.15_151, offset: 0B];
  MEM[base: p_9(D), index: ivtmp.15_151, offset: 0B] = _26;
  ivtmp.15_150 = ivtmp.15_151 + 1;
  i_149 = (int) ivtmp.15_150;
  if (c_7(D) > i_149)
    goto <bb 29>; [89.00%]
  else
    goto <bb 25>; [11.00%]

  <bb 29> [local count: 6555602]:
  goto <bb 23>; [100.00%]

  <bb 24> [local count: 84095460]:

  <bb 25> [local count: 105119324]:

  <bb 26> [local count: 118111600]:
  return;

}


---


### compiler : `gcc`
### title : `Failure to optimize division followed by multiplication to modulo followed by subtraction`
### open_at : `2020-05-17T17:13:57Z`
### last_modified_date : `2023-08-24T21:43:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95176
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int a, int b)
{
    return a * (b / a);
}

This is equivalent to `return b - (b % a);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Failure to optimize specific kind of sign comparison check`
### open_at : `2020-05-18T12:05:22Z`
### last_modified_date : `2023-09-11T22:14:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95185
### status : `NEW`
### tags : `easyhack, missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return (x >= 0) == (y <= 0);
}

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94718 was resolved and most of the patterns are now optimized, but this specific one isn't. It's equivalent to `(x < 0) ^ (y <= 0)`, which can be done with a single compare instead of two. That transformation is made by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize bool check into consecutive literals`
### open_at : `2020-05-18T13:16:58Z`
### last_modified_date : `2023-08-24T21:42:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95187
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(bool b)
{
    if (b)
        return 1;
    return 2;
}

This can be optimized into `2 - (int)b`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Remove extra variable created for memory reference in loop vectorization.`
### open_at : `2020-05-19T02:55:53Z`
### last_modified_date : `2020-06-17T19:22:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95199
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
The function vect_create_data_ref_ptr created two equal variable for two equal memory references.

gcc version 11.0.0 20200515 (experimental) (GCC)
Target: aarch64-unknown-linux-gnu
Configured with: ../configure
Command: gcc -O2 -march=armv8.2-a+fp+sve -ftree-vectorize test.c -S

Testcase:
void
foo (double *a, double *b, double m, int inc_x, int inc_y) {
  int ix = 0, iy = 0;
  for (int i = 0; i < 1000; ++i)
    {
      a[ix] += m * b[iy];
      ix += inc_x;
      iy += inc_y;
    }
  return ;
}

Assembly code
.L5:
  ld1d         z3.d, p0/z, [x5, z2.d, lsl 3]
  ld1d         z1.d, p0/z, [x3, z4.d, lsl 3]
  fmad         z1.d, p1/m, z0.d, z3.d
  st1d         z1.d, p0, [x2, z2.d, lsl 3]
  incd         x1
  add          x5, x5, x6
  add          x3, x3, x4
  add          x2, x2, x6
  whilelo      p0.d, x1, x0
  b.any	       .L5

x2 is the same as x5.
vectorizable_load and vectorizable_store called vect_create_data_ref_ptr twice for a[ix].

Dump Log in test.c.161.vect
test.c:4:2: note:  create real_type-pointer variable to type: double  vectorizing a pointer ref: *a_16(D)
test.c:4:2: note:  created a_16(D)
test.c:4:2: note:  add new stmt: vect__4.5_94 = .MASK_GATHER_LOAD (vectp_a.3_91, _90, 8, { 0.0, ... }, loop_mask_93);
...
test.c:4:2: note:  create real_type-pointer variable to type: double  vectorizing a pointer ref: *a_16(D)
test.c:4:2: note:  created a_16(D)
test.c:4:2: note:  add new stmt: .MASK_SCATTER_STORE (vectp_a.11_117, _116, 8, vect__10.10_108, loop_mask_93);

I plan to add a hash_map to loop_vec_info for dr and the corresponding pointer created by vect_create_data_ref_ptr. If the dr->ref has been handled, return the corresponding pointer.

Optimized assembly code
.L3:
  ld1d    z2.d, p0/z, [x0, z1.d, lsl 3]
  ld1d    z0.d, p0/z, [x1, z4.d, lsl 3]
  fmad    z0.d, p1/m, z3.d, z2.d
  st1d    z0.d, p0, [x0, z1.d, lsl 3]
  incd    x2
  add     x0, x0, x5
  add     x1, x1, x4
  whilelo p0.d, w2, w3
  b.any   .L3
  ret


---


### compiler : `gcc`
### title : `Some x86 vector-extend patterns are not exercised.`
### open_at : `2020-05-19T09:52:26Z`
### last_modified_date : `2022-08-18T07:45:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95201
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Some of x86 vector extend patterns are not exercised by middle end. Currently, they are XFAILed in gcc.target/i386/pr92658-*.c:


pr92658-avx2.c:/* { dg-final { scan-assembler-times "pmovzxbq" 2 { xfail *-*-* } } } */
pr92658-sse4.c:/* { dg-final { scan-assembler-times "pmovzxbd" 2 { xfail *-*-* } } } */
pr92658-sse4.c:/* { dg-final { scan-assembler-times "pmovzxbq" 2 { xfail *-*-* } } } */
pr92658-sse4.c:/* { dg-final { scan-assembler-times "pmovzxwq" 2 { xfail *-*-* } } } */

These correspond to:

-O2 -ftree-vectorize -mavx2 is required:

--cut here--
typedef unsigned char v32qi __attribute__((vector_size (32)));
typedef unsigned short v16hi __attribute__((vector_size (32)));
typedef unsigned int v8si __attribute__((vector_size (32)));
typedef unsigned long long v4di __attribute__((vector_size (32)));

void
foo_u8_u64 (v4di * dst, v32qi * __restrict src)
{
  unsigned long long tem[4];
  tem[0] = (*src)[0];
  tem[1] = (*src)[1];
  tem[2] = (*src)[2];
  tem[3] = (*src)[3];
  dst[0] = *(v4di *) tem;
}

void
bar_u8_u64 (v4di * dst, v32qi src)
{
  unsigned long long tem[4];
  tem[0] = src[0];
  tem[1] = src[1];
  tem[2] = src[2];
  tem[3] = src[3];
  dst[0] = *(v4di *) tem;
}

/* { dg-final { scan-assembler-times "pmovzxbq" 2 { xfail *-*-* } } } */
--cut here--

-O2 -ftree-vectorize -msse4.1 is required:

--cut here--
void
foo_u8_u32 (v4si * dst, v16qi * __restrict src)
{
  unsigned int tem[4];
  tem[0] = (*src)[0];
  tem[1] = (*src)[1];
  tem[2] = (*src)[2];
  tem[3] = (*src)[3];
  dst[0] = *(v4si *) tem;
}

void
bar_u8_u32 (v4si * dst, v16qi src)
{
  unsigned int tem[4];
  tem[0] = src[0];
  tem[1] = src[1];
  tem[2] = src[2];
  tem[3] = src[3];
  dst[0] = *(v4si *) tem;
}

/* { dg-final { scan-assembler-times "pmovzxbd" 2 { xfail *-*-* } } } */

void
foo_u8_u64 (v2di * dst, v16qi * __restrict src)
{
  unsigned long long tem[2];
  tem[0] = (*src)[0];
  tem[1] = (*src)[1];
  dst[0] = *(v2di *) tem;
}

void
bar_u8_u64 (v2di * dst, v16qi src)
{
  unsigned long long tem[2];
  tem[0] = src[0];
  tem[1] = src[1];
  dst[0] = *(v2di *) tem;
}

/* { dg-final { scan-assembler-times "pmovzxbq" 2 { xfail *-*-* } } } */

void
foo_u16_u64 (v2di * dst, v8hi * __restrict src)
{
  unsigned long long tem[2];
  tem[0] = (*src)[0];
  tem[1] = (*src)[1];
  dst[0] = *(v2di *) tem;
}

void
bar_u16_u64 (v2di * dst, v8hi src)
{
  unsigned long long tem[2];
  tem[0] = src[0];
  tem[1] = src[1];
  dst[0] = *(v2di *) tem;
}

/* { dg-final { scan-assembler-times "pmovzxwq" 2 { xfail *-*-* } } } */

Please note that these testcases fail to vectorize also in their loop forms, e.g.:

--cut here--
void
foo_u8_u64 (v4di * dst, v32qi * __restrict src)
{
  unsigned long long tem[4];

  for (int i = 0; i < 4; i++)
    tem[i] = (*src)[i];

  dst[0] = *(v4di *) tem;
}

void
bar_u8_u64 (v4di * dst, v32qi src)
{
  unsigned long long tem[4];

  for (int i = 0; i < 4; i++)
    tem[i] = src[i];

  dst[0] = *(v4di *) tem;
}
--cut here--

Please see also PR 92658#c8 for some analysis.


---


### compiler : `gcc`
### title : `missed switch optimization as bit test`
### open_at : `2020-05-19T13:42:07Z`
### last_modified_date : `2021-12-07T08:18:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95208
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Created attachment 48565
The if case is optimized better

This comes from libcpp/lex.c's raw string lexer.  We're testing whether a character falls into a particular set of values.  The switch is emitted as the usual dispatch table.  The if case, after range checking, turns into
'if ((1ul << (c - BASE)) & MAGIC_VALUE)'  Which is somewhat better.  Why doesn't the switch form do that?

_Z3bazc:
.LFB1:
	.cfi_startproc
	leal	-97(%rdi), %eax
	cmpb	$29, %al
	jbe	.L7
	subl	$33, %edi
	cmpb	$62, %dil
	ja	.L6
	movabsq	$8646911282403868279, %rax
	btq	%rdi, %rax
	jc	.L7
.L6:
	ret
	.p2align 4,,10
	.p2align 3
.L7:
	jmp	_Z3barv
	.cfi_endproc


---


### compiler : `gcc`
### title : `Failure to optimize register allocation around atomic loads/stores`
### open_at : `2020-05-20T02:23:52Z`
### last_modified_date : `2023-08-24T21:42:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95228
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int x;
int y;

int f()
{
    int ret;
    __atomic_load(&y, &ret, 0);
    int val = 0;
    __atomic_store(&y, &val, 5);
    return ret;
}

With -O3, GCC outputs this :

f():
  mov r8d, DWORD PTR y[rip]
  xor eax, eax
  xchg eax, DWORD PTR y[rip]
  mov eax, r8d
  ret

LLVM outputs this :

f(): # @f()
  mov eax, dword ptr [rip + .Ly$local]
  xor ecx, ecx
  xchg dword ptr [rip + .Ly$local], ecx
  ret

eax can be replaced with ecx in the store to avoid having to store the load of y into a seperate register.


---


### compiler : `gcc`
### title : `Failure to optimize bit-scatter pattern to and 1`
### open_at : `2020-05-20T08:25:40Z`
### last_modified_date : `2023-08-24T21:42:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95230
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
uint32_t scatter(uint32_t val)
{
    uint32_t res = 0;
    uint32_t off = 0;

    for (uint32_t I = 0; I < 32; ++I)
        if (1 & (1 << I))
            res |= (val & (1 << off++)) << I;
    return res;
}

This can be optimized to `return val & 1;`. LLVM does this transformation, but GCC does not. There is a more generic optimization that looks like it can be done using something else than `1` as the left operand of the `&` in the `if`.


---


### compiler : `gcc`
### title : `Failure to properly optimize out register use in bit-twiddling code`
### open_at : `2020-05-20T09:32:58Z`
### last_modified_date : `2023-08-24T21:41:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95235
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int f(int a, int b)
{
    return ((a & 1) != 0) != (b != 0);
}

With -O3, GCC outputs this :

f(int, int):
  test esi, esi
  setne al
  xor edi, eax
  mov eax, edi
  and eax, 1
  ret

GCC should be able of outputting this instead :

f(int, int):
  test esi, esi
  setne al
  xor eax, edi
  and eax, 1
  ret

But it does not do so.


---


### compiler : `gcc`
### title : `std::sort copies custom comparator`
### open_at : `2020-05-20T19:12:14Z`
### last_modified_date : `2020-05-21T06:16:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95245
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `7.5.0`
### severity : `normal`
### contents :
std::sort copies a custom comparator numerous times.  If the comparator copy is expensive, it makes the sort very slow.  Of course, making the comparator cheap to copy is an easy fix, but it took me by surprise that the algorithm copied my comparator at all.  Other c++ library implementations don't have this limitation.  I found this on version 7.5, but I believe the same behavior is in GCC 10.1.  The following program exhibits the issue:

#include <vector>
#include <algorithm>
#include <numeric>
#include <iostream>

int main()
{
    struct comp
    {
        comp(int i)
        {
            v.resize(i);
            std::iota(v.begin(), v.end(), 0);
        }

        comp(const comp& c) : v(c.v)
        {
            std::cerr << "Copy ctor!\n";
        }

        comp(comp&& c) : v(std::move(c.v))
        {
            std::cerr << "Move ctor!\n";
        }

        bool operator()(size_t p1, size_t p2)
        {
            return p1 < p2;
        }

        // Vector is just here to cause an expensive copy and slow the sort,
        // but it could,
        // for example, be a cache that is used in the comparison.
        std::vector<int> v;
    };

    std::vector<size_t> s(5000000);
    std::iota(s.begin(), s.end(), 0);
    std::random_shuffle(s.begin(), s.end());
    std::sort(s.begin(), s.end(), comp(2000000));

    return 0;
}


---


### compiler : `gcc`
### title : `x86 code size expansion inserting field into a union`
### open_at : `2020-05-21T03:07:11Z`
### last_modified_date : `2020-05-22T06:19:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95251
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.1`
### severity : `normal`
### contents :
Testing code on Godbolt and I came across some pathological code amplification when SSE is enabled for field insertion into a structure containing a union. 

Here is the Godbolt link: https://godbolt.org/z/z_RpFt

Compiler flags: gcc -Os --save-temps -march=ivybridge -c x7b00.c

The function `x7b00`, inserts into the structure via char fields and it has a voluminous translation (30 instructions).  The functionally equivalent `xyb87` inserts into the structure via an 64-bit integer and it translates simply (5 instructions). `x`, `a7x` and `x7bcd` are for comparison.

Not adding  -march=ivybridge improves the code size but it is still nowhere near optimal. `xyb87` serves as a reference for near optimal translation. It seemed worthy of filing a bug due to the observed code amplification factor (6X).

Can the backend choose the non-SSE code generation if it is more efficient?


--- CODE SNIPPET BEGINS ---

typedef unsigned long long u64;
typedef char u8;

typedef struct mr
{
    union {
        u64 y;
        struct {
            u8 a,b,c,d;
        } i;
    } u;
    u64 x;
} mr;

u64 x(mr mr) { return mr.x; }
mr a7x(u64 x) { return (mr) { .u = { .i = { 7,0,0,0 } }, .x = x }; }
mr x7bcd(u64 x,u8 b,u8 c,u8 d) { return (mr) {.u={.i={7,b,c,d }}, .x=x }; }
mr xyb87(u64 x, u8 b) { return (mr) {.u={ .y =(u64)b << 8|7},.x=x }; }
mr x7b00(u64 x, u8 b) { return (mr) {.u={ .i ={7,b,0,0}}, .x=x }; }


--- EXPECTED OUTPUT ---

	.cfi_startproc
	endbr64
	movsbq	%sil, %rax
	movq	%rdi, %rdx
	salq	$8, %rax
	orq	$7, %rax
	ret
	.cfi_endproc


--- OBSERVED OUTPUT ---

	.cfi_startproc
	endbr64
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rdi, %r8
	xorl	%eax, %eax
	movl	$6, %ecx
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	andq	$-32, %rsp
	leaq	-32(%rsp), %rdi
	rep stosb
	movq	$0, -48(%rsp)
	movabsq	$281474976710655, %rax
	movq	$0, -40(%rsp)
	movq	-48(%rsp), %rdx
	andq	-32(%rsp), %rax
	movzwl	%dx, %edx
	salq	$16, %rax
	orq	%rax, %rdx
	movq	%rdx, -48(%rsp)
	movb	$7, -48(%rsp)
	vmovdqa	-48(%rsp), %xmm1
	vpinsrb	$1, %esi, %xmm1, %xmm0
	vmovaps	%xmm0, -48(%rsp)
	movq	-48(%rsp), %rax
	movq	%r8, -40(%rsp)
	movq	-40(%rsp), %rdx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc


---


### compiler : `gcc`
### title : `aarch64: gcc generate inefficient code with fixed sve vector length`
### open_at : `2020-05-21T07:33:10Z`
### last_modified_date : `2020-06-05T09:41:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95254
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Test case:

typedef short __attribute__((vector_size (8))) v4hi;

typedef union U4HI { v4hi v; short a[4]; } u4hi;

short b[4];

void pass_v4hi (v4hi v)
{
    int i;
    u4hi u;
    u.v = v;
    for (i = 0; i < 4; i++)
      b[i] = u.a[i];
};

$ gcc -O2 -ftree-slp-vectorize -S -march=armv8.2-a+sve foo.c
assembly code:
pass_v4hi:
.LFB0:
        .cfi_startproc
        adrp    x0, .LANCHOR0
        str     d0, [x0, #:lo12:.LANCHOR0]
        ret
        .cfi_endproc

$ gcc -O2 -ftree-slp-vectorize -S -march=armv8.2-a+sve -msve-vector-bits=256 foo.c
assembly code:
pass_v4hi:
.LFB0:
        .cfi_startproc
        sub     sp, sp, #16
        .cfi_def_cfa_offset 16
        ptrue   p0.b, vl32
        adrp    x0, .LANCHOR0
        add     x0, x0, :lo12:.LANCHOR0
        str     d0, [sp, 8]
        ld1h    z0.d, p0/z, [sp, #1, mul vl]
        st1h    z0.d, p0, [x0]
        add     sp, sp, 16
        .cfi_def_cfa_offset 0
        ret
        .cfi_endproc


The root cause here is that we choose a different mode in aarch64_vectorize_related_mode[1]: VNx2HImode instead of V4HImode.
Then in the final tree ssa forwprop pass, we need to do a VIEW_CONVERT from V4HImode to VNx2HImode.
One way to fix this is to catch and simplify the pattern in aarch64_expand_sve_mem_move, emitting a mov pattern of V4HImode instead.
I am assuming endianness does not make a difference here. Will propose a patch for comments.


[1] call trace:
(gdb) bt
#0  aarch64_vectorize_related_mode (vector_mode=E_VNx8HImode, element_mode=..., nunits=...) at ../../gcc-git/gcc/config/aarch64/aarch64.c:2377
#1  0x00000000012983b4 in related_vector_mode (vector_mode=E_VNx8HImode, element_mode=..., nunits=...) at ../../gcc-git/gcc/stor-layout.c:535
#2  0x0000000001652918 in get_related_vectype_for_scalar_type (prevailing_mode=E_VNx8HImode, scalar_type=0xffffb22da498, nunits=...)
    at ../../gcc-git/gcc/tree-vect-stmts.c:11463
#3  0x0000000001653304 in get_vectype_for_scalar_type (vinfo=0x2f0dc80, scalar_type=0xffffb22da498, group_size=4)
    at ../../gcc-git/gcc/tree-vect-stmts.c:11545
#4  0x00000000016533a0 in get_vectype_for_scalar_type (vinfo=0x2f0dc80, scalar_type=0xffffb22da498, node=0x2e5d460)
    at ../../gcc-git/gcc/tree-vect-stmts.c:11569
#5  0x00000000016987e8 in vect_get_constant_vectors (vinfo=0x2f0dc80, slp_node=0x2e53080, op_num=0, vec_oprnds=0xffffffffc738)
    at ../../gcc-git/gcc/tree-vect-slp.c:3562
#6  0x00000000016993f8 in vect_get_slp_defs (vinfo=0x2f0dc80, slp_node=0x2e53080, vec_oprnds=0xffffffffc7a8, n=1) at ../../gcc-git/gcc/tree-vect-slp.c:3786
#7  0x0000000001631c70 in vect_get_vec_defs (vinfo=0x2f0dc80, op0=0xffffb20e3120, op1=0x0, stmt_info=0x2feef60, vec_oprnds0=0xffffffffcdd0, vec_oprnds1=0x0,
    slp_node=0x2e53080) at ../../gcc-git/gcc/tree-vect-stmts.c:1726
#8  0x0000000001648bc8 in vectorizable_store (vinfo=0x2f0dc80, stmt_info=0x2feef60, gsi=0xffffffffdad0, vec_stmt=0xffffffffd5b0, slp_node=0x2e53080,
    cost_vec=0x0) at ../../gcc-git/gcc/tree-vect-stmts.c:8186
#9  0x0000000001651808 in vect_transform_stmt (vinfo=0x2f0dc80, stmt_info=0x2feef60, gsi=0xffffffffdad0, slp_node=0x2e53080, slp_node_instance=0x2fefe70)
    at ../../gcc-git/gcc/tree-vect-stmts.c:11184
#10 0x000000000169a4a0 in vect_schedule_slp_instance (vinfo=0x2f0dc80, node=0x2e53080, instance=0x2fefe70) at ../../gcc-git/gcc/tree-vect-slp.c:4134
#11 0x000000000169aaac in vect_schedule_slp (vinfo=0x2f0dc80) at ../../gcc-git/gcc/tree-vect-slp.c:4258
#12 0x00000000016972f0 in vect_slp_bb_region (region_begin=..., region_end=..., datarefs=..., n_stmts=10) at ../../gcc-git/gcc/tree-vect-slp.c:3227
#13 0x0000000001697c60 in vect_slp_bb (bb=0xffffb22ce340) at ../../gcc-git/gcc/tree-vect-slp.c:3350
#14 0x00000000016a56f0 in (anonymous namespace)::pass_slp_vectorize::execute (this=0x2e6aae0, fun=0xffffb2116000) at ../../gcc-git/gcc/tree-vectorizer.c:1320


---


### compiler : `gcc`
### title : `aarch64: suboptimal code generation for common neon intrinsic sequence involving shrn and mull`
### open_at : `2020-05-21T22:36:47Z`
### last_modified_date : `2021-02-10T12:36:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95265
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `enhancement`
### contents :
Compileable example:

#include <arm_neon.h>

int32x4_t func(int32x4_t a, int32x4_t b)
{
    return vshrn_high_n_s64(
        vshrn_n_s64(vmull_s32(vget_low_s32(a), vget_low_s32(b)), 12), 
        vmull_high_s32(a, b), 12);
}

with gcc -O3 the generated code contains two superfluent movs and and one unecessary dup.

output of gcc -v
Using built-in specs.
COLLECT_GCC=C:\msys64\opt\devkitpro\devkitA64\bin\aarch64-none-elf-gcc.exe
COLLECT_LTO_WRAPPER=c:/msys64/opt/devkitpro/devkita64/bin/../libexec/gcc/aarch64-none-elf/10.1.0/lto-wrapper.exe
Target: aarch64-none-elf
Configured with: ../../gcc-10.1.0/configure --enable-languages=c,c++,objc,lto --with-gnu-as --with-gnu-ld --with-gcc --with-march=armv8 --enable-cxx-flags=-ffunction-sections --disable-libstdcxx-verbose --enable-poison-system-directories --enable-interwork --enable-multilib --enable-threads --disable-win32-registry --disable-nls --disable-debug --disable-libmudflap --disable-libssp --disable-libgomp --disable-libstdcxx-pch --enable-libstdcxx-time --enable-libstdcxx-filesystem-ts --target=aarch64-none-elf --with-newlib=yes --with-headers=../../newlib-3.3.0/newlib/libc/include --prefix=/opt/devkitpro/x86_64-w64-mingw32/devkitA64 --enable-lto --with-system-zlib --with-bugurl=https://github.com/devkitPro/buildscripts/issues --with-pkgversion='devkitA64 release 15' --build=x86_64-unknown-linux-gnu --host=x86_64-w64-mingw32 --with-gmp=/opt/mingw64/mingw --with-mpfr=/opt/mingw64/mingw --with-mpc=/opt/mingw64/mingw
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.1.0 (devkitA64 release 15)


---


### compiler : `gcc`
### title : `[11 Regression] Many targets failing ssa-dom-cse-2.c after vectorizer changes`
### open_at : `2020-05-25T04:53:51Z`
### last_modified_date : `2020-05-25T14:02:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95309
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
Various ports have regressed the tree-ssa/ssa-dom-cse-2.c after this change:

commit a4b48fc47c3406b6f41be093c4615879b7006710
Author: Richard Biener <rguenther@suse.de>
Date:   Mon May 18 16:05:00 2020 +0200

    cost invariant nodes from vect_slp_analyze_node_operations SLP walk
    
    2020-05-19  Richard Biener  <rguenther@suse.de>
    
            * tree-vectorizer.h (_slp_tree::vectype): Add field.
            (SLP_TREE_VECTYPE): New.
            * tree-vect-slp.c (vect_create_new_slp_node): Initialize
            SLP_TREE_VECTYPE.
            (vect_create_new_slp_node): Likewise.
            (vect_prologue_cost_for_slp): Move here from tree-vect-stmts.c
            and simplify.
            (vect_slp_analyze_node_operations): Walk nodes children for
            invariant costing.
            (vect_get_constant_vectors): Use local scope op variable.
            * tree-vect-stmts.c (vect_prologue_cost_for_slp_op): Remove here.
            (vect_model_simple_cost): Adjust.
            (vect_model_store_cost): Likewise.
            (vectorizable_store): Likewise.

You should be able to see this on the iq2000-elf port (and many others).

I believe the test is supposed to verify that we're able to determine the return
value statically after unrolling the loop.  It's unclear if the stores get in the
way of cse-ing the assignment to _22 and _29 in the dump or if there's something
else going on:

  MEM[(int *)&a] = { 0 };
  MEM[(int *)&a + 4B] = { 1 };
  MEM[(int *)&a + 8B] = { 2 };
  MEM[(int *)&a + 12B] = { 3 };
  MEM[(int *)&a + 16B] = { 4 };
  MEM[(int *)&a + 20B] = { 5 };
  MEM[(int *)&a + 24B] = { 6 };
  MEM[(int *)&a + 28B] = { 7 };
  _22 = a[0];
  _29 = a[1];
  sum_30 = _22 + _29;
  _36 = a[2];
  sum_37 = sum_30 + _36;
  _43 = a[3];
  sum_44 = sum_37 + _43;
  _50 = a[4];
  sum_51 = sum_44 + _50;
  _57 = a[5];
  sum_58 = sum_51 + _57;
  _64 = a[6];
  sum_65 = sum_58 + _64;
  _1 = a[7];
  sum_16 = _1 + sum_65;
  a ={v} {CLOBBER};
  return sum_16;


---


### compiler : `gcc`
### title : `Poor vector_size decomposition when SVE is enabled`
### open_at : `2020-05-26T16:44:05Z`
### last_modified_date : `2022-02-11T18:18:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95341
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Compiling the following with -O2 -march=armv8-a:

  typedef unsigned int uint_vec __attribute__((vector_size(32)));
  uint_vec f1(uint_vec x, uint_vec y) { return x + y; }

generates nice Advanced SIMD code:

        ld1     {v4.16b - v5.16b}, [x0]
        ld1     {v2.16b - v3.16b}, [x1]
        add     v0.4s, v4.4s, v2.4s
        add     v1.4s, v5.4s, v3.4s
        st1     {v0.16b - v1.16b}, [x8]
        ret

But compiling with -march=armv8.2-a+sve generates extremely bad
scalar code, so bad that I'll spare people's eyes by not quoting
it here.

I haven't yet analysed this or checked whether it's a regression.


---


### compiler : `gcc`
### title : `Failure to optimize printfs with extraneous arguments`
### open_at : `2020-05-27T07:34:14Z`
### last_modified_date : `2023-08-24T21:40:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95359
### status : `WAITING`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
void f()
{
    printf("", 2);
}

This can be optimized to doing nothing (the extraneous arguments should be ignored and just evaluated by themselves, it's not UB). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Poor codegen when constructing a trivial Optional`
### open_at : `2020-05-28T13:56:16Z`
### last_modified_date : `2023-07-12T21:35:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95383
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Here is a complete example:

struct nullopt_t {} inline constexpr nullopt{};

template <typename T>
struct Optional {
    struct Empty { };
    union {
        Empty _;
        T value;
    };
    bool engaged;

    Optional(nullopt_t) : _(), engaged(false) { }
    Optional(T v) : value(v), engaged(true) { }
};

Optional<int> foo(bool b) {
    if (b) {
        return 42;
    }
    return nullopt;
}

Optional here is a valid implementation strategy for trivial types, like int. You can see some codegen examples here: https://godbolt.org/z/KwuWzB

gcc 10.1 -O2 emits:

foo(bool):
        test    dil, dil
        mov     eax, 0
        movabs  rdx, 4294967338
        cmovne  rax, rdx
        ret

gcc 10.1 -O3 is worse:

foo(bool):
        xor     eax, eax
        mov     ecx, 42
        test    dil, dil
        cmovne  rdx, rcx
        mov     ecx, 1
        cmovne  rax, rcx
        movabs  rcx, -1095216660481
        and     rdx, rcx
        sal     rax, 32
        or      rax, rdx
        ret

gcc trunk (as of today) -O2 is the same as this bad -O3 version.

clang 10, on the other hand, on -O2 or -O3, emits:

foo(bool):                                # @foo(bool)
        shl     rdi, 32
        lea     rax, [rdi + 42]
        ret

which is much better. 

Using std::optional instead of this Optional (https://godbolt.org/z/By-fYx) for comparison, clang emits the same code as above. gcc 10 -O3 emits a branch:

foo(bool):
        xor     eax, eax
        test    dil, dil
        je      .L2
        mov     DWORD PTR [rsp-8], 42
        mov     eax, 1
.L2:
        mov     BYTE PTR [rsp-4], al
        mov     rax, QWORD PTR [rsp-8]
        ret


---


### compiler : `gcc`
### title : `Poor codegen cause by using base class instead of member for Optional construction`
### open_at : `2020-05-28T14:07:47Z`
### last_modified_date : `2023-03-25T18:44:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95384
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Following up on #95383:

struct nullopt_t {} inline constexpr nullopt{};

template <typename T>
struct OptionalStorage {
    struct Empty { };
    union {
        Empty _;
        T value;
    };
    bool engaged;

    OptionalStorage(nullopt_t) : _(), engaged(false) { }
    OptionalStorage(T v) : value(v), engaged(true) { }
};

template <typename T>
struct OptionalWithBase : OptionalStorage<T> {
    using OptionalStorage<T>::OptionalStorage;
};

template <typename T>
struct OptionalWithMember {
    OptionalStorage<T> o;
    OptionalWithMember(nullopt_t) : o(nullopt) { }
    OptionalWithMember(T v) : o(v) { }
};

OptionalWithBase<int> foo_with_base(bool b) {
    if (b) {
        return 42;
    }
    return nullopt;
}

OptionalWithMember<int> foo_with_member(bool b) {
    if (b) {
        return 42;
    }
    return nullopt;
}

OptionalWithBase<T> and OptionalWithMember<T> should be the same thing. It's just that one inherits from its storage and the other has it as a member. But the codegen is very different (https://godbolt.org/z/j8m68Y), probably something to do with tail padding?

gcc 10.1 -O2:

foo_with_base(bool):
        test    dil, dil
        je      .L2
        mov     DWORD PTR [rsp-8], 42
        mov     BYTE PTR [rsp-4], 1
        mov     rax, QWORD PTR [rsp-8]
        ret
.L2:
        mov     BYTE PTR [rsp-4], 0
        mov     rax, QWORD PTR [rsp-8]
        ret
foo_with_member(bool):
        test    dil, dil
        mov     eax, 0
        movabs  rdx, 4294967338
        cmovne  rax, rdx
        ret

gcc 10.2 -O3 or gcc trunk -O2:

foo_with_base(bool):
        xor     eax, eax
        test    dil, dil
        je      .L2
        mov     DWORD PTR [rsp-8], 42
        mov     eax, 1
.L2:
        mov     BYTE PTR [rsp-4], al
        mov     rax, QWORD PTR [rsp-8]
        ret
foo_with_member(bool):
        xor     edx, edx
        mov     ecx, 42
        test    dil, dil
        cmovne  rax, rcx
        mov     ecx, 1
        cmovne  rdx, rcx
        movabs  rcx, -1095216660481
        and     rax, rcx
        sal     rdx, 32
        or      rax, rdx
        ret

clang 10, -O2 or -O3:

foo_with_base(bool):                     # @foo_with_base(bool)
        shl     rdi, 32
        lea     rax, [rdi + 42]
        ret
foo_with_member(bool):                   # @foo_with_member(bool)
        shl     rdi, 32
        lea     rax, [rdi + 42]
        ret


---


### compiler : `gcc`
### title : `Failure to optimize loop condition arithmetic for mismatched types`
### open_at : `2020-05-28T17:01:23Z`
### last_modified_date : `2023-08-24T21:40:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95393
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(unsigned s)
{
    int i;
    for (i = 0; i < s; ++i)
        ;

    return i;
}

This can be optimized to `return s;`. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `Failure to optimize compare to power of 2 and bitwise and to more direct bitwise and`
### open_at : `2020-05-29T07:37:23Z`
### last_modified_date : `2023-08-24T21:40:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95404
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f2(int x)
{
    if ((unsigned)x > 15)
        return false;

    return (x & 1) == 0;
}

This can be optimized to `return !(x & -15);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Unnecessary stores with std::optional`
### open_at : `2020-05-29T08:38:55Z`
### last_modified_date : `2023-07-12T21:35:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95405
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.1.0`
### severity : `enhancement`
### contents :
I posted this to the gcc-help mailing list a few days ago (https://gcc.gnu.org/pipermail/gcc-help/2020-May/138978.html).

GCC produces stores that don't seem to be required for std::optional.

Code:
--------
#include <optional>
std::optional<long> foo();
long bar()
{
    auto r = foo();
    if (r)
        return *r;
    else
        return 0L;
}
--------

What gcc 10.1 with -std=c++17 -O3 produces is:
bar():
        sub     rsp, 24
        call    foo()
        mov     QWORD PTR [rsp+8], rdx
        cmp     BYTE PTR [rsp+8], 0
        mov     QWORD PTR [rsp], rax
        mov     rax, QWORD PTR [rsp]
        jne     .L1
        xor     eax, eax
.L1:
        add     rsp, 24
        ret

(see: https://godbolt.org/z/uHE6QB)

I don't understand the stores (and loads) after the call to foo. They
don't seem necessary to me.


Marc Glisse pointed out (https://gcc.gnu.org/pipermail/gcc-help/2020-May/138982.html) that the first pair of store/load seems to be a tuning choice and can be removed with the correct tuning flags.


What I expected is:
        mov     QWORD PTR [rsp+8], rdx
        cmp     BYTE PTR [rsp+8], 0
should be a compare/test directly of dl.

And:
        mov     QWORD PTR [rsp], rax
        mov     rax, QWORD PTR [rsp]
is not present at all.

Can someone explain this behavior? Shouldn't the optimizer produce what I pointed out?


---


### compiler : `gcc`
### title : `Failure to optimize bitwise and with negated conditional using the same operand to conditional with decremented operand`
### open_at : `2020-05-29T09:40:02Z`
### last_modified_date : `2023-09-16T06:25:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95408
### status : `ASSIGNED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int x)
{
    return x & -((unsigned)x < some_constant);
}

This can be optimized to `return (unsigned)(x - 1) < (some_constant - 1);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to xor register before usage of 8-bit part in some bitshifting situations`
### open_at : `2020-05-29T10:32:32Z`
### last_modified_date : `2023-08-24T21:39:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95409
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f1(int x)
{
    return ((unsigned)x < 15) & ((x & 1) == 0);
}

bool f2(int x)
{
    return ~x & ((unsigned)x < 15);
}

These are equivalent (according to LLVM), but GCC outputs :

f1(int):
  cmp edi, 14
  setbe al
  andn eax, edi, eax
  ret
f2(int):
  xor eax, eax
  cmp edi, 14
  setbe al
  andn eax, edi, eax
  ret

I believe that the code generation for `f1` is suboptimal.


---


### compiler : `gcc`
### title : `Failure to optimize compare next to and properly`
### open_at : `2020-05-29T10:53:44Z`
### last_modified_date : `2023-08-24T21:39:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95410
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int x, int y)
{
    if ((unsigned)x >= y)
        return false;

    return (x & 1) == 0;
}

This can be optimized to `return ((unsigned)x < y) & !(x & 1);` (and further to `return ~x & ((unsigned)x < y);`). This transformation is done by LLVM, but not by GCC.


---
