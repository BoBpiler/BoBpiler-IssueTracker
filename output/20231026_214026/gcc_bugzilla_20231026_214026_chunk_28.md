### Total Bugs Detected: 4649
### Current Chunk: 28 of 30
### Bugs in this Chunk: 160 (From bug 4321 to 4480)
---


### compiler : `gcc`
### title : `[14 regression] Several powerpc64 vector test cases fail after r14-1242-gf574e2dfae7905`
### open_at : `2023-05-25T22:05:55Z`
### last_modified_date : `2023-07-17T02:24:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109971
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55158
diff of assembler output between r14-1241 and r14-1242

g:f574e2dfae79055f16d0c63cc12df24815d8ead6, r14-1242-gf574e2dfae7905

make  -k check-gcc RUNTESTFLAGS="powerpc.exp=gcc.target/powerpc/p9-vec-length-full-1.c"
FAIL: gcc.target/powerpc/p9-vec-length-full-1.c scan-assembler-times \\mlxvl\\M 20
FAIL: gcc.target/powerpc/p9-vec-length-full-1.c scan-assembler-times \\mstxvl\\M 10
# of expected passes		5
# of unexpected failures	2

There are a few others, too:

FAIL: gcc.target/powerpc/p9-vec-length-full-2.c scan-assembler-times \\\\mlxvl\\\\M 20
FAIL: gcc.target/powerpc/p9-vec-length-full-2.c scan-assembler-times \\\\mstxvl\\\\M 10
FAIL: gcc.target/powerpc/p9-vec-length-full-6.c scan-assembler-times \\\\mlxvl\\\\M 10
FAIL: gcc.target/powerpc/p9-vec-length-full-6.c scan-assembler-times \\\\mstxvl\\\\M 10
FAIL: gcc.target/powerpc/p9-vec-length-full-7.c scan-assembler-times \\\\mstxvl\\\\M 12

There are hundreds of assembler code differences caused by the commit.  Diff file attached.

commit f574e2dfae79055f16d0c63cc12df24815d8ead6 (HEAD, refs/bisect/bad)
Author: Ju-Zhe Zhong <juzhe.zhong@rivai.ai>
Date:   Thu May 25 22:42:35 2023 +0800

    VECT: Add decrement IV iteration loop control by variable amount support


---


### compiler : `gcc`
### title : `missing fold (~a | b) ^ a => ~(a & b)`
### open_at : `2023-05-26T11:48:54Z`
### last_modified_date : `2023-09-17T06:11:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109986
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
int foo(int a, int b)
{
    return (~a | b) ^ a;
}

This can be optimized to `return ~(a | b);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Addition/subtraction to the last bitfield of a struct can be optimized`
### open_at : `2023-05-26T15:48:36Z`
### last_modified_date : `2023-07-19T04:12:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109992
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
For an unsigned bit field:
```
struct foo
  {
    unsigned a :  3;
    unsigned b : 29;
  };

void
bad_add(struct foo* p, unsigned add)
  {
    p->b += add;
  }
```

GCC:
```
bad_add:
        mov     eax, DWORD PTR [rdi]
        mov     edx, eax
        and     eax, 7
        shr     edx, 3
        add     edx, esi
        sal     edx, 3
        or      eax, edx
        mov     DWORD PTR [rdi], eax
        ret
```

Clang:
```
bad_add:                                # @bad_add
        shl     esi, 3
        add     dword ptr [rdi], esi
        ret
```

It looks like GCC extracts the bitfield first, performs the addition, then inserts it back.

The result is almost the same for a signed bitfield, but not exacting the bitfield first is subject to overflows, so it may be a different story.


---


### compiler : `gcc`
### title : `[13/14 regression] Suboptimal code generation for branchless binary search`
### open_at : `2023-05-27T03:20:34Z`
### last_modified_date : `2023-07-27T09:26:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110001
### status : `UNCONFIRMED`
### tags : `missed-optimization, needs-bisection, ra`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
GCC 12.3 generated beautiful code for this, with all but the last of the unrolled loop iterations using only 3 instructions:

https://gcc.godbolt.org/z/eGbEj9YKd

Currently, GCC generates 4 instructions:

https://gcc.godbolt.org/z/Ebczq8jjx

This probably does not make a huge difference given the data hazard, but there is something awe-inspiring from seeing GCC generate only 3 instructions per unrolled loop iteration for binary search. It would be nice if future versions went back to generating three instructions.

This function was inspired by this D code:

https://godbolt.org/z/5En7xajzc

The bsearch1000() function is entirely branchless and has no more than 2 instructions for every cmov, excluding ret. I wrote a more general version in C that can handle variable array sizes, and to my pleasant surprise, GCC 12.3 generated a similar 3 instruction sequence for all but the last of the unrolled loop iterations. I was saddened when I saw the output from GCC 13.1 and trunk.

Anyway, all recent versions of GCC that I cared to check generate a branch for the last unrolled iteration on line 58. That branch is unpredictable, so GCC would generate better code here if it used predication to avoid the branch. I had been able to give GCC a hint to avoid a similar branch at the end using __builtin_expect_with_probability(), but that trick did not work for line 58.

Also, if anyone is interested in where that D code originated, here is the source:

https://muscar.eu/shar-binary-search-meta.html


---


### compiler : `gcc`
### title : `shrink wrapping could be improved`
### open_at : `2023-05-27T18:47:04Z`
### last_modified_date : `2023-07-10T14:34:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110008
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
https://gcc.godbolt.org/z/94Wf3Worq
```
int complex_one(int, int);

int
test(int a, int b, int c)
  {
    if(__builtin_expect(a, 0) == 0)
      return 0;

    int r = complex_one(a, b);
    r += complex_one(r, c);
    return r + a + b;
  }
```

GCC:
```
test:
        push    rdi
        push    rsi
        push    rbx
        sub     rsp, 32
        mov     ebx, ecx
        mov     esi, edx
        test    ecx, ecx
        jne     .L7
        mov     eax, ebx
        add     rsp, 32
        pop     rbx
        pop     rsi
        pop     rdi
        ret
.L7:
        mov     DWORD PTR 80[rsp], r8d
        call    complex_one
        mov     edx, DWORD PTR 80[rsp]
        mov     ecx, eax
        mov     edi, eax
        call    complex_one
        add     edi, eax
        add     ebx, edi
        add     ebx, esi
        mov     eax, ebx
        add     rsp, 32
        pop     rbx
        pop     rsi
        pop     rdi
        ret
```

Clang:
```
test:                                   # @test
        xor     eax, eax
        test    edi, edi
        jne     .LBB0_1
        ret
.LBB0_1:
        push    rbp
        push    r15
        push    r14
        push    rbx
        push    rax
        mov     r14d, edx
        mov     ebx, esi
        mov     ebp, edi
        call    complex_one@PLT
        mov     r15d, eax
        mov     edi, eax
        mov     esi, r14d
        call    complex_one@PLT
        add     ebx, ebp
        add     ebx, r15d
        add     ebx, eax
        mov     eax, ebx
        add     rsp, 8
        pop     rbx
        pop     r14
        pop     r15
        pop     rbp
        ret
```

There are two issues in this code: The first one is that GCC uses apparently more space for temporary variables than Clang. The other is that when `a` equals zero, Clang skips the normal function prologue which pushes a lot of registers onto the stack, but GCC performs the check after it, in which case both the prologue and epilogue get executed for nothing.


---


### compiler : `gcc`
### title : `Another missing ABS detection`
### open_at : `2023-05-28T00:18:37Z`
### last_modified_date : `2023-09-04T19:35:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110009
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
unsigned
f1 (int v)
{
  unsigned int d_6;
  int b_5;
  unsigned int v1_2;
  unsigned int _7;
  int _9;

  b_5 = v>>(sizeof(v)*8 - 1);
  _9 = b_5 | 1;
  d_6 = (unsigned int) _9;
  v1_2 = (unsigned int) v;
  _7 = v1_2 * d_6;
  return _7;
}
```
This currently does not get converted to `return ABSU<v>`.

I found this while implementing a different match pattern and found that the testcase pr103245-1.c was failing.

This blocks PR 109907 because the patch which is to fix a regression from the patch of bug 109907 comment #25 introduces this.


---


### compiler : `gcc`
### title : `(a>>N) != (b>>N) is not optimized to (a^b) >= (1<<N)`
### open_at : `2023-05-28T00:31:46Z`
### last_modified_date : `2023-09-21T13:56:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110010
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
#define N 5
unsigned f(unsigned a, unsigned b)
{
  return (a>>N) != (b>>N);
}
```
This could be optimized down to just `return (a^b) <= (1<<N)`.

This shows up in the testsuite as a pr94718-5.c failure when you add the following 2 match patterns:
(simplify
 (negate (convert (lt @0 integer_zerop)))
 (if (!TYPE_UNSIGNED (TREE_TYPE (@0)))
  (with { tree type_unsigned = unsigned_type_for (TREE_TYPE (@0)); }
   (if (types_match (type, TREE_TYPE (@0)) || types_match (type_unsigned, type))
    (convert (rshift @0
		     { build_int_cst (integer_type_node,
				      element_precision (type) - 1); }))))))
(simplify
 (convert (lt @0 integer_zerop))
 (if (!TYPE_UNSIGNED (TREE_TYPE (@0)))
  (with { tree type_unsigned = unsigned_type_for (TREE_TYPE (@0)); }
   (if (types_match (type, TREE_TYPE (@0)) || types_match (type_unsigned, type))
   (convert (rshift (convert:type_unsigned @0)
                    { build_int_cst (integer_type_node,
				     element_precision (type) - 1); }))))))


---


### compiler : `gcc`
### title : `openjpeg is slower when built with gcc13 compared to clang16`
### open_at : `2023-05-28T19:15:01Z`
### last_modified_date : `2023-05-30T07:53:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110015
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
I tried to reproduce openjpeg benchmarks from Phoronix
https://www.phoronix.com/review/gcc13-clang16-raptorlake/5

On zen3 hardware I get 42607ms for clang build and 45702ms for gcc build that is a 7% difference (Phoronix reports 10% on RaptorLake)

perf of clang build:
  88.64%  opj_t1_cblk_encode_processor
   6.68%  opj_dwt_encode_and_deinterleave_v
   1.30%  opj_dwt_encode_and_deinterleave_h_one_row

opj_t1_cblk_encode_processor is huge with no obvious hot spots.

perf of gcc build:

  70.36% opj_t1_cblk_encode_processor                                                                                                                                                                                
  16.12% opj_t1_enc_refpass.lto_priv.0                                                                                                                                                                               
   3.88% opj_dwt_encode_and_deinterleave_v                                                                                                                                                                           
   2.46% pj_dwt_fetch_cols_vertical_pass                                                                                                                                                                            
   2.35% opj_mqc_byteout                                                                                                                                                                                             

So we apparently inline less even at -O3


---


### compiler : `gcc`
### title : `Missing vectorizable_conversion(unsigned char -> double) for BB vectorizer`
### open_at : `2023-05-29T02:35:32Z`
### last_modified_date : `2023-06-26T07:49:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110018
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
When Looking at PR109812, I noticed there's missing vectorizable_conversion for BB vectorizer when target doesn't support direct optab for unsigned char to double. But actually it can be vectorized via unsigned char -> short/int/long long -> double when vectorizable_conversion is ok for any of the immediate type.

Currently, when modifier is NONE, vectorizable_conversion doesn't try any immediate type, it can be extended similar like WIDEN.

 5158    case NONE:
 5159      if (code != FIX_TRUNC_EXPR
 5160          && code != FLOAT_EXPR
 5161          && !CONVERT_EXPR_CODE_P (code))
 5162        return false;
 5163      if (supportable_convert_operation (code, vectype_out, vectype_in, &code1))
 5164        break;
 5165      /* FALLTHRU */

void
foo (double* __restrict a, unsigned char* b)
{
    a[0] = b[0];
    a[1] = b[1];
    a[2] = b[2];
    a[3] = b[3];
    a[4] = b[4];
    a[5] = b[5];
    a[6] = b[6];
    a[7] = b[7];
}

missed:   conversion not supported by target.


---


### compiler : `gcc`
### title : `10% performance drop on important benchmark after r247544.`
### open_at : `2023-05-29T14:20:36Z`
### last_modified_date : `2023-05-30T14:46:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110023
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.3.0`
### severity : `normal`
### contents :
Created attachment 55183
Open-source stream benchmark

The stream benchmark performance deteriorates. The loop peeling policy in the vect_enhance_data_refs_alignment function is modified to degrade the benchmark performance,which can be demonstrated on example from attachment. 

Alternatively, you can obtain it from https://github.com/jeffhammond/stream/archive/master.zip.

Compiling & Running:
gcc -fopenmp -O -DSTREAM_ARRAY_SIZE=100000000 stream.c  -o stream
./stream

Patches to modify the loop stripping policy of the vect_enhance_data_refs_alignment function are available from:
https://gcc.gnu.org/git/?p=gcc.git&a=commit;h=49ab46214e9288ee1268f87ddcd64dacfd21c31d

After you open OpenMP,

Before modification: (Add subitem)
ldr d0, [x5, x1, lsl #3]
fadd d0, d0, d1
str d0, [x4, x1, lsl #3]
mov w4, w2
sub w7, w7, w2
add x4, x4, x1
ldr x1, [x10, #888]
lsl x4, x4, #3
lsr w8, w7, #1
add x6, x4, x6
add x5, x4, x5
mov w2, #0x0 /
add x4, x4, x1
mov x1, #0x0 /
ldr q0, [x5, x1]
add w2, w2, #0x1
ldr q1, [x6, x1]
cmp w2, w8
fadd v0.2d, v0.2d, v1.2d
str q0, [x4, x1]
add x1, x1, #0x10
b.cc 4012d8 <main._omp_fn.4+0xd8>
and w1, w7, #0xfffffffe
add w0, w0, w1
cmp w7, w1
b.eq 401348 <main._omp_fn.4+0x148>
ldr x5, [x9, #880]
sxtw x1, w0
ldr x4, [x11, #896]
add w0, w0, #0x1
ldr d1, [x5, x1, lsl #3]
cmp w3, w0
ldr x2, [x10, #888]
ldr d0, [x4, x1, lsl #3]
fadd d0, d0, d1
str d0, [x2, x1, lsl #3]
b.le 401348 <main._omp_fn.4+0x148>
sxtw x0, w0
ldr d0, [x5, x0, lsl #3]
ldr d1, [x4, x0, lsl #3]
fadd d0, d0, d1
str d0, [x2, x0, lsl #3]
ldr x19, [sp, #16]
ldp x29, x30, [sp], #32

After the modification:
mov x29, sp
str x19, [sp, #16]
bl 4006e0 <omp_get_num_threads@plt>
mov w19, w0
bl 4006b0 <omp_get_thread_num@plt>
mov w2, #0x8000 /
movk w2, #0x61a, lsl #16
sdiv w1, w2, w19
msub w2, w1, w19, w2
cmp w0, w2
b.ge 401238 <main._omp_fn.4+0x38>
add w1, w1, #0x1
mov w2, #0x0 /
madd w0, w1, w0, w2
add w1, w1, w0
cmp w0, w1
b.ge 4012d8 <main._omp_fn.4+0xd8>
sub w2, w1, w0
adrp x8, 401000 <main._omp_fn.3+0x100>
adrp x9, 401000 <main._omp_fn.3+0x100>
adrp x7, 401000 <main._omp_fn.3+0x100>
cmp w2, #0x1
b.eq 4012b8 <main._omp_fn.4+0xb8>
ldr x1, [x7, #760]
sbfiz x4, x0, #3, #32
ldr x6, [x8, #744]
lsr w10, w2, #1
ldr x5, [x9, #752]
mov w3, #0x0 /
add x6, x4, x6
add x5, x4, x5
add x4, x4, x1
mov x1, #0x0 /
ldr q0, [x6, x1]
add w3, w3, #0x1
ldr q1, [x5, x1]
cmp w3, w10
fadd v0.2d, v0.2d, v1.2d
str q0, [x4, x1]
add x1, x1, #0x10
b.cc 401288 <main._omp_fn.4+0x88>
and w1, w2, #0xfffffffe
add w0, w0, w1
cmp w2, w1
b.eq 4012d8 <main._omp_fn.4+0xd8>
ldr x3, [x9, #752]
sxtw x0, w0
ldr x2, [x8, #744]
ldr x1, [x7, #760]
ldr d0, [x3, x0, lsl #3]
ldr d1, [x2, x0, lsl #3]
fadd d0, d0, d1
str d0, [x1, x0, lsl #3]
ldr x19, [sp, #16]
ldp x29, x30, [sp], #32
ret

After modifying the peeling policy, the vectorization of the for loop in the Add subitem does not attempt to peel the loop, but the performance eventually degrades.


---


### compiler : `gcc`
### title : `[Bug] 5% performance drop on important benchmark after r260951.`
### open_at : `2023-05-29T15:14:27Z`
### last_modified_date : `2023-05-30T14:48:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110026
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.3.0`
### severity : `normal`
### contents :
Created attachment 55184
Open-source stream benchmark

After the patch is submitted on AArch64, the performance of copying subitems in the stream benchmark decreases by 3%.

Alternatively, you can obtain it from https://github.com/jeffhammond/stream/archive/master.zip.

Compiling & Running:
gcc -fopenmp -O -DSTREAM_ARRAY_SIZE=100000000 stream.c  -o stream
./stream

Before modification: (copy subitem)
ldr x2, [x3, x0, lsl #3]
str x2, [x4, x0, lsl #3]
add x0, x0, #0x1
cmp x1, x0
b.ne 400a00 <main._omp_fn.4+0x54>
ldr x19, [sp, #16]
ldp x29, x30, [sp], #32
ret

After the modification:
ldr d0, [x2, x0, lsl #3]
str d0, [x3, x0, lsl #3]
add x0, x0, #0x1
cmp x1, x0
b.ne 400a00 <main._omp_fn.4+0x54>
ldr x19, [sp, #16]
ldp x29, x30, [sp], #32
ret

It can be seen that the vector register (X0) is used before the modification, and the common register (D0) is used after the modification.


---


### compiler : `gcc`
### title : `The first popped allcono doesn't take precedence over later popped in ira coloring`
### open_at : `2023-05-30T08:02:58Z`
### last_modified_date : `2023-08-31T05:02:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110034
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Followings are ira dumps from a test case. r134 has only one cp(shuffle) with r173. The r173 and r124 both have preferred hard register r3. r134 is first popped the register but it fails to get hard register r3 as the conflict cost is high. If r173 is aheaf of r134, the r134 can get hard register r3 as there is no hop between r3 and r134 after r173 is assigned r3. Seems the first popped allcono(r134) doesn't take precedence over later popped allcono(r124).

r173 has a preferred hard register and has no conflict allcono. So r173 can always be assigned r3. 

;; a29(r173,l0) conflicts:
;;     total conflict hard regs:
;;     conflict hard regs:

...

  cp11:a18(r134)<->a29(r173)@125:shuffle
  pref0:a12(r158)<-hr100@711
  pref1:a23(r144)<-hr100@920
  pref2:a25(r140)<-hr100@1842
  pref3:a29(r173)<-hr3@2000
  pref4:a0(r124)<-hr3@125

...

        Start updating from pref of hr3 for a29r173:
          a18r134 (hr3): update cost by -62, conflict cost by -62

...

      Pushing a1(r169,l0)(cost 0)
      Pushing a0(r124,l0)(cost 0)
      Pushing a22(r146,l0)(cost 0)
      Pushing a20(r125,l0)(cost 0)
      Pushing a29(r173,l0)(cost 0)
      Pushing a18(r134,l0)(cost 0)
      Popping a18(r134,l0)  -- (9=0,0) (10=0,0) (8=0,0) (7=0,0) (6=0,0) (5=0,0) (4=0,0) (3=-62,147) (11=0,0) (0=8000,8000) (31=7,7) (30=7,7) (29=7,7) (28=7,7) (27=7,7) (26=7,7) (25=7,7) (24=7,7) (23=7,7) (22=7,7) (21=7,7) (20=7,7) (19=7,7) (18=7,7) (17=7,7) (16=7,7) (15=7,7) (14=7,7) (12=0,0)
        Start restoring from a18r134:
          a29r173 (hr3): update cost by -62, conflict cost by -62
        Start updating from a18r134 by copies:
          a29r173 (hr9): update cost by -250, conflict cost by -250
        assign reg 9
      Popping a29(r173,l0)  -- (9=1750,1750) (10=2000,2000) (8=2000,2000) (7=2000,2000) (6=2000,2000) (5=2000,2000) (4=2000,2000) (3=-2062,-2062) (11=2000,2000) (0=2000,2000) (31=2007,2007) (30=2007,2007) (29=2007,2007) (28=2007,2007) (27=2007,2007) (26=2007,2007) (25=2007,2007) (24=2007,2007) (23=2007,2007) (22=2007,2007) (21=2007,2007) (20=2007,2007) (19=2007,2007) (18=2007,2007) (17=2007,2007) (16=2007,2007) (15=2007,2007) (14=2007,2007) (12=2000,2000)
        Start restoring from a29r173:
        Start updating from a29r173 by copies:
        assign reg 3

...

      Popping a0(r124,l0)  -- (8=0,0) (7=0,0) (6=0,0) (5=0,0) (4=0,0) (3=-250,-250) (11=0,0) (0=0,0) (31=7,7) (30=7,7) (29=7,7) (28=7,7) (27=7,7) (26=7,7) (25=7,7) (24=7,7) (23=7,7) (22=7,7) (21=7,7) (20=7,7) (19=7,7) (18=7,7) (17=7,7) (16=7,7) (15=7,7) (14=7,7) (12=0,0)
        Start restoring from a0r124:
        Start updating from a0r124 by copies:
          a1r169 (hr3): update cost by -44, conflict cost by -44
          a2r177 (hr3): update cost by -11, conflict cost by -11
        assign reg 3


---


### compiler : `gcc`
### title : `Missed optimization for dependent assignment statements`
### open_at : `2023-05-30T08:35:53Z`
### last_modified_date : `2023-06-06T11:54:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110035
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.1.0`
### severity : `enhancement`
### contents :
Created attachment 55212
Test case, compiled with -stdc++=20 -O2

The test case, when compiled, produces additional move instructions:

movdqu	(%rdi), %xmm2
movdqu	16(%rdi), %xmm1
movdqu	32(%rdi), %xmm0
movl	$48, %edi
movaps	%xmm2, 32(%rsp)
movaps	%xmm1, 16(%rsp)
movaps	%xmm0, (%rsp)
call	_Znwm@PLT
movdqa	32(%rsp), %xmm2
movdqa	16(%rsp), %xmm1
movdqa	(%rsp), %xmm0
movq	%rax, %rdi
movups	%xmm2, (%rax)
movups	%xmm1, 16(%rax)
movups	%xmm0, 32(%rax)

compared to more optimized result using clang++ 14.0.0 with same flags:

callq	_Znwm@PLT
movups	(%rbx), %xmm0
movups	16(%rbx), %xmm1
movups	32(%rbx), %xmm2
movups	%xmm0, (%rax)
movups	%xmm1, 16(%rax)
movups	%xmm2, 32(%rax)
movq	%rax, %rdi

Clang has MemCpyOptPass which detects and removes memory dependency of the second set of move instructions, which allows Dead Store Elimination pass to remove the first set of move instructions.

g++-12 -v
Using built-in specs.
COLLECT_GCC=g++-12
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/12/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none:amdgcn-amdhsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 12.1.0-2ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-12/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-12 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --enable-cet --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-12-sZcx2y/gcc-12-12.1.0/debian/tmp-nvptx/usr,amdgcn-amdhsa=/build/gcc-12-sZcx2y/gcc-12-12.1.0/debian/tmp-gcn/usr --enable-offload-defaulted --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.1.0 (Ubuntu 12.1.0-2ubuntu1~22.04)


---


### compiler : `gcc`
### title : `[14 Regression] FAIL: gcc.target/aarch64/rev16_2.c scan-assembler-times rev16\\tw[0-9]+ 2`
### open_at : `2023-05-30T12:49:21Z`
### last_modified_date : `2023-05-31T13:27:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110039
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
I think after g:d8545fb2c71683f407bfd96706103297d4d6e27b the test regresses on aarch64.
We now generate:
__rev16_32_alt:
        rev     w0, w0
        ror     w0, w0, 16
        ret

__rev16_32:
        rev     w0, w0
        ror     w0, w0, 16
        ret

whereas before it was:
__rev16_32_alt:
        rev16   w0, w0
        ret

__rev16_32:
        rev16   w0, w0
        ret

I think the GIMPLE at expand time is better and the RTL that it tries to match is simpler:
Failed to match this instruction:
(set (reg:SI 95)
    (rotate:SI (bswap:SI (reg:SI 96))
        (const_int 16 [0x10])))

So maybe it's simply a matter of adding that pattern to aarch64.md.

Anyway, filing this here to track the regression


---


### compiler : `gcc`
### title : `rs6000 port emits dead mfvsrd instruction for simple test case`
### open_at : `2023-05-30T15:19:39Z`
### last_modified_date : `2023-10-17T10:36:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110040
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
GCC Trunk generates a dead mfvsrd for the following test case.

[jeevitha@ltcden2-lp1 ~]$ cat bug.c 
#include <altivec.h>

void
foo (signed long *dst, vector signed __int128 src)
{
  *dst = (signed long) src[0];
}

[jeevitha@ltcden2-lp1 ~]$ gcc bug.c  -O2 -mcpu=power9 -S -o bug.s
[jeevitha@ltcden2-lp1 ~]$ cat bug.s
	.file	"bug.c"
	.machine power9
	.abiversion 2
	.section	".text"
	.align 2
	.p2align 4,,15
	.globl foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	mfvsrd 11,34   #dead instruction
	mfvsrld 10,34
	std 10,0(3)
	blr


---


### compiler : `gcc`
### title : `[14 Regression] missed cmov optimization after r14-1014-gc5df248509b489364c573e8`
### open_at : `2023-05-30T19:36:03Z`
### last_modified_date : `2023-06-02T19:31:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110042
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
unsigned
f1(int t, int t1)
{
  int tt = 0;
  if(t)
    tt = (t1&0x8)!=0;
  return tt;
}
```
On aarch64 we should produce:
```
        cmp     w0, 0
        ubfx    x0, x1, 3, 1
        csel    w0, w0, wzr, ne
        ret
```

But on the trunk we get:
```
        cbz     w0, .L3
        ubfx    x0, x1, 3, 1
        ret
        .p2align 2,,3
.L3:
        mov     w0, 0
        ret
```


The difference in the IR is:
old:
```
(insn 11 10 12 3 (set (reg:SI 97)
        (lshiftrt:SI (reg/v:SI 96 [ t1 ])
            (const_int 3 [0x3]))) "/app/example.cpp":7:18 782 {*aarch64_lshr_sisd_or_int_si3}
     (expr_list:REG_DEAD (reg/v:SI 96 [ t1 ])
        (nil)))
(insn 12 11 25 3 (set (reg:SI 94 [ <retval> ])
        (and:SI (reg:SI 97)
            (const_int 1 [0x1]))) "/app/example.cpp":7:18 533 {andsi3}
     (expr_list:REG_DEAD (reg:SI 97)
        (nil)))
```
new:
```
(insn 11 10 12 3 (set (subreg:DI (reg:SI 97) 0)
        (zero_extract:DI (subreg:DI (reg/v:SI 96 [ t1 ]) 0)
            (const_int 1 [0x1])
            (const_int 3 [0x3]))) "/app/example.cpp":7:18 832 {*extzvdi}
     (expr_list:REG_DEAD (reg/v:SI 96 [ t1 ])
        (nil)))
(insn 12 11 24 3 (set (reg:SI 94 [ <retval> ])
        (reg:SI 97)) "/app/example.cpp":8:10 64 {*movsi_aarch64}
     (expr_list:REG_DEAD (reg:SI 97)
        (nil)))
```
noce_try_cmove_arith handles the old one but not the new one for some reason:
```
if-conversion succeeded through noce_try_cmove_arith
``


---


### compiler : `gcc`
### title : `useless local variable not optimized away`
### open_at : `2023-05-31T08:06:43Z`
### last_modified_date : `2023-06-01T12:48:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110052
### status : `WAITING`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55221
reduced test, v1

A superfluous local variable is not optimized away.

Consider:
$ cat localvar.c ; echo EOF
#include <stddef.h>
// original flags: gcc-12 -std=gnu99 -malign-data=abi -finline-limit=0 -fno-builtin-strlen -fomit-frame-pointer -ffunction-sections -fdata-sections -fno-guess-branch-probability -funsigned-char -falign-functions=1 -falign-jumps=1 -falign-labels=1 -falign-loops=1 -fno-unwind-tables -fno-asynchronous-unwind-tables -fno-builtin-printf -Oz
extern void *realloc(void *ptr, size_t size);
static char * __attribute__((noipa))
myrealloc(void *ptr, int n, int *size)
{
  if (!ptr || n >= *size) {
    *size = n + (n >> 1) + 80;
    ptr = realloc (ptr, *size);
    if (ptr == NULL) /* mimic xrealloc */
      __builtin_abort();
  }
  return ptr;
}
typedef struct foo_s {
  int fd;
  char *buffer;
  int adv;
  int size;
  int pos;
  signed char boo;
} foo_t;

void bloat(foo_t *);
void bloat(foo_t *foo) {
  char *m = foo->buffer;
  /* This is the original code, the local variable is superfluous */
  int size = foo->size;
  if (!m)
    m = myrealloc(m, 256, &size);
  foo->size = size;
}

void ok(foo_t *);
void ok(foo_t *foo) {
  char *m = foo->buffer;
  if (!m)
    m = myrealloc(m, 256, &foo->size);
}
EOF


$ readelf -s localvar.o | grep "FUNC *GLOBAL"
    13: 0000000000000000    52 FUNC    GLOBAL DEFAULT    6 bloat
    14: 0000000000000000    24 FUNC    GLOBAL DEFAULT    8 ok
$ strip -R .comment localvar.o
$ objdump -D localvar.o

localvar.o:     file format elf64-x86-64


Disassembly of section .text.myrealloc:

0000000000000000 <.text.myrealloc>:
   0:	48 89 f8             	mov    %rdi,%rax
   3:	48 85 ff             	test   %rdi,%rdi
   6:	74 04                	je     0xc
   8:	39 32                	cmp    %esi,(%rdx)
   a:	7f 21                	jg     0x2d
   c:	51                   	push   %rcx
   d:	89 f1                	mov    %esi,%ecx
   f:	48 97                	xchg   %rax,%rdi
  11:	d1 f9                	sar    %ecx
  13:	8d 74 0e 50          	lea    0x50(%rsi,%rcx,1),%esi
  17:	89 32                	mov    %esi,(%rdx)
  19:	48 63 f6             	movslq %esi,%rsi
  1c:	e8 00 00 00 00       	callq  0x21
  21:	48 85 c0             	test   %rax,%rax
  24:	75 05                	jne    0x2b
  26:	e8 00 00 00 00       	callq  0x2b
  2b:	5a                   	pop    %rdx
  2c:	c3                   	retq   
  2d:	c3                   	retq   

Disassembly of section .text.bloat:

0000000000000000 <.text.bloat>:
   0:	53                   	push   %rbx
   1:	48 89 fb             	mov    %rdi,%rbx
   4:	48 83 ec 10          	sub    $0x10,%rsp
   8:	8b 47 14             	mov    0x14(%rdi),%eax
   b:	48 83 7f 08 00       	cmpq   $0x0,0x8(%rdi)
  10:	89 44 24 0c          	mov    %eax,0xc(%rsp)
  14:	75 11                	jne    0x27
  16:	48 8d 54 24 0c       	lea    0xc(%rsp),%rdx
  1b:	be 00 01 00 00       	mov    $0x100,%esi
  20:	31 ff                	xor    %edi,%edi
  22:	e8 00 00 00 00       	callq  0x27
  27:	8b 44 24 0c          	mov    0xc(%rsp),%eax
  2b:	89 43 14             	mov    %eax,0x14(%rbx)
  2e:	48 83 c4 10          	add    $0x10,%rsp
  32:	5b                   	pop    %rbx
  33:	c3                   	retq   

Disassembly of section .text.ok:

0000000000000000 <.text.ok>:
   0:	48 83 7f 08 00       	cmpq   $0x0,0x8(%rdi)
   5:	75 10                	jne    0x17
   7:	48 8d 57 14          	lea    0x14(%rdi),%rdx
   b:	be 00 01 00 00       	mov    $0x100,%esi
  10:	31 ff                	xor    %edi,%edi
  12:	e9 00 00 00 00       	jmpq   0x17
  17:	c3                   	retq   

object size with each myrealloc and either bloat or ok:
$ size localvar-*.o
   text	   data	    bss	    dec	    hex	filename
     70	      0	      0	     70	     46	localvar-ok.o
     98	      0	      0	     98	     62	localvar-bloat.o


---


### compiler : `gcc`
### title : `[14 Regression] Adding optimizer hints to std::vector causes a new -Wstringop-overread false positive`
### open_at : `2023-05-31T11:59:20Z`
### last_modified_date : `2023-06-01T15:11:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110060
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `libstdc++`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55223
Preprocessed source for FAIL 23_containers/vector/types/1.cc

As discussed on IRC recently, I want to add optimization hints to std::vector::size() and std::vector::capacity() so that gcc knows there will be no reallocation here:

  vec.assign(vec.size(), 0);

The call to assign will not reallocate because size() <= capacity() is always true. We can express that invariant in the code.

However, doing so causes a new testsuite FAIL:

FAIL: 23_containers/vector/types/1.cc (test for excess errors)

$ ~/gcc/latest/bin/g++ -O2 -Wall a-1.ii
In file included from /home/jwakely/gcc/14/include/c++/14.0.0/vector:62,
                 from /home/jwakely/src/gcc/gcc/libstdc++-v3/testsuite/23_containers/vector/types/1.cc:23:
In static member function ‘static _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = greedy_ops::X; _Up = greedy_ops::X; bool _IsMove = false]’,
    inlined from ‘_OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:506:30,
    inlined from ‘_OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:533:42,
    inlined from ‘_OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:540:31,
    inlined from ‘_OI std::copy(_II, _II, _OI) [with _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:633:7,
    inlined from ‘std::vector<_Tp, _Alloc>& std::vector<_Tp, _Alloc>::operator=(const std::vector<_Tp, _Alloc>&) [with _Tp = greedy_ops::X; _Alloc = std::allocator<greedy_ops::X>]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/vector.tcc:255:17,
    inlined from ‘int main()’ at /home/jwakely/src/gcc/gcc/libstdc++-v3/testsuite/23_containers/vector/types/1.cc:35:7:
/home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:437:30: warning: ‘void* __builtin_memcpy(void*, const void*, long unsigned int)’ forming offset 1 is out of the bounds [0, 1] [-Warray-bounds=]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In static member function ‘static _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = greedy_ops::X; _Up = greedy_ops::X; bool _IsMove = false]’,
    inlined from ‘_OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:506:30,
    inlined from ‘_OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:533:42,
    inlined from ‘_OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:540:31,
    inlined from ‘_OI std::copy(_II, _II, _OI) [with _II = greedy_ops::X*; _OI = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:633:7,
    inlined from ‘static _ForwardIterator std::__uninitialized_copy<true>::__uninit_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = greedy_ops::X*; _ForwardIterator = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_uninitialized.h:147:27,
    inlined from ‘_ForwardIterator std::uninitialized_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = greedy_ops::X*; _ForwardIterator = greedy_ops::X*]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_uninitialized.h:185:15,
    inlined from ‘_ForwardIterator std::__uninitialized_copy_a(_InputIterator, _InputIterator, _ForwardIterator, allocator<_Tp>&) [with _InputIterator = greedy_ops::X*; _ForwardIterator = greedy_ops::X*; _Tp = greedy_ops::X]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_uninitialized.h:373:37,
    inlined from ‘std::vector<_Tp, _Alloc>& std::vector<_Tp, _Alloc>::operator=(const std::vector<_Tp, _Alloc>&) [with _Tp = greedy_ops::X; _Alloc = std::allocator<greedy_ops::X>]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/vector.tcc:257:35,
    inlined from ‘int main()’ at /home/jwakely/src/gcc/gcc/libstdc++-v3/testsuite/23_containers/vector/types/1.cc:35:7:
/home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_algobase.h:437:30: warning: ‘void* __builtin_memcpy(void*, const void*, long unsigned int)’ reading between 2 and 9223372036854775807 bytes from a region of size 1 [-Wstringop-overread]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/jwakely/gcc/14/include/c++/14.0.0/x86_64-pc-linux-gnu/bits/c++allocator.h:33,
                 from /home/jwakely/gcc/14/include/c++/14.0.0/bits/allocator.h:46,
                 from /home/jwakely/gcc/14/include/c++/14.0.0/vector:63:
In member function ‘_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = greedy_ops::X]’,
    inlined from ‘static _Tp* std::allocator_traits<std::allocator<_Tp1> >::allocate(allocator_type&, size_type) [with _Tp = greedy_ops::X]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/alloc_traits.h:482:28,
    inlined from ‘std::_Vector_base<_Tp, _Alloc>::pointer std::_Vector_base<_Tp, _Alloc>::_M_allocate(std::size_t) [with _Tp = greedy_ops::X; _Alloc = std::allocator<greedy_ops::X>]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_vector.h:378:33,
    inlined from ‘std::_Vector_base<_Tp, _Alloc>::pointer std::_Vector_base<_Tp, _Alloc>::_M_allocate(std::size_t) [with _Tp = greedy_ops::X; _Alloc = std::allocator<greedy_ops::X>]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_vector.h:375:7,
    inlined from ‘void std::_Vector_base<_Tp, _Alloc>::_M_create_storage(std::size_t) [with _Tp = greedy_ops::X; _Alloc = std::allocator<greedy_ops::X>]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_vector.h:413:44,
    inlined from ‘std::_Vector_base<_Tp, _Alloc>::_Vector_base(std::size_t, const allocator_type&) [with _Tp = greedy_ops::X; _Alloc = std::allocator<greedy_ops::X>]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_vector.h:332:26,
    inlined from ‘std::vector<_Tp, _Alloc>::vector(size_type, const allocator_type&) [with _Tp = greedy_ops::X; _Alloc = std::allocator<greedy_ops::X>]’ at /home/jwakely/gcc/14/include/c++/14.0.0/bits/stl_vector.h:572:47,
    inlined from ‘int main()’ at /home/jwakely/src/gcc/gcc/libstdc++-v3/testsuite/23_containers/vector/types/1.cc:29:39:
/home/jwakely/gcc/14/include/c++/14.0.0/bits/new_allocator.h:147:48: note: source object of size 1 allocated by ‘operator new’
  147 |         return static_cast<_Tp*>(_GLIBCXX_OPERATOR_NEW(__n * sizeof(_Tp)));
      |                                  ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~



The preprocessed source of the test is attached (with the proposed new hints added to the std::vector code). There's no warning with -O1 or -O3, only -O2.


---


### compiler : `gcc`
### title : `missed vectorization in graphicsmagick`
### open_at : `2023-05-31T13:20:47Z`
### last_modified_date : `2023-07-31T11:28:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110062
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `normal`
### contents :
Phoronix claims 31% performance difference between gcc13 and clang on sharpen benchmark of graphicsmagick.  On zen3 I reproduce only 4%, but the benchmark has only single short internal loop:

214
  97.56%  gm               gm                          [.] ConvolveImage.◆
   0.88%  gm               libgomp.so.1.0.0            [.] 0x000000000002▒
   0.67%  gm               libc.so.6                   [.] __memmove_avx_▒
 
GCC version:
  2.38 │500:┌─→vmovss      (%r8,%rax,4),%xmm2                            ▒
  0.04 │    │  movzbl      0x2(%rdx,%rax,4),%ebp                         ▒
  0.09 │    │  vcvtsi2ss   %ebp,%xmm0,%xmm1                              ▒
  7.44 │    │  movzbl      0x1(%rdx,%rax,4),%ebp                         ▒
  0.16 │    │  vfmadd231ss %xmm1,%xmm2,%xmm7                             ▒
 30.23 │    │  vcvtsi2ss   %ebp,%xmm0,%xmm1                              ▒
  2.38 │    │  movzbl      (%rdx,%rax,4),%ebp                            ▒
  0.03 │    │  inc         %rax                                          ▒
  0.00 │    │  vfmadd231ss %xmm1,%xmm2,%xmm9                             ▒
 22.80 │    │  vcvtsi2ss   %ebp,%xmm0,%xmm1                              ▒
  1.03 │    │  vfmadd231ss %xmm1,%xmm2,%xmm10                            ▒
 30.49 │    ├──cmp         %rax,%rbx                                     ▒
  0.18 │    └──jne         500                                           ▒

Clangs:
  0.00 │1e70:┌─→movzbl       0x2(%rdx,%rsi,4),%r9d                       ▒
  0.05 │     │  vbroadcastss (%rcx,%rsi,4),%xmm3                         ▒
  0.56 │     │  movzwl       (%rdx,%rsi,4),%r11d                         ▒
  0.05 │     │  inc          %rsi                                        ▒
  0.00 │     │  vcvtsi2ss    %r9d,%xmm10,%xmm2                           ▒
  0.71 │     │  vfmadd231ss  %xmm2,%xmm3,%xmm0                           ▒
  1.17 │     │  vmovd        %r11d,%xmm2                                 ▒
  0.00 │     │  vpmovzxbd    %xmm2,%xmm2                                 ▒
  0.06 │     │  vcvtdq2ps    %xmm2,%xmm2                                 ▒
  0.89 │     │  vfmadd231ps  %xmm2,%xmm3,%xmm1                           ▒
  1.98 │     ├──cmp          %rsi,%r10                                   ▒
  0.00 │     └──jne          1e70                                        ▒
  0.00 │      ↑ jmp          1630                                        ▒

Probably same issue as in PR109812 but reproduces on zens and loop is even shorter.


---


### compiler : `gcc`
### title : `missing min detection`
### open_at : `2023-06-01T03:30:10Z`
### last_modified_date : `2023-09-10T06:58:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110068
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
#define min1(x,y) ((x) < (y) ? (x) : (y))
#define min2(x,y) ((x) <= (y) ? (x) : (y))
static inline unsigned
min3(unsigned x, unsigned y)
{
        return min1(x, y);
}
static inline unsigned
min4(unsigned x, unsigned y)
{
        return min3(x, y);
}
unsigned 
f1 (unsigned  x)
{
  return min1(x, 1U<<(sizeof(x)*8-1));
}
unsigned
f2 (unsigned  x)
{
  return min2(x, 1U<<(sizeof(x)*8-1));
}
unsigned
f3 (unsigned  x)
{
  return min3(x, 1U<<(sizeof(x)*8-1));
}
unsigned
f4 (unsigned  x)
{
  return min4(x, 1U<<(sizeof(x)*8-1));
}
```
f1,f2,f3, and f4 should all produce MIN_EXPRs but currently f1 does not.
I would have thought r11-1504-g2e0f4a18bc978  fixed this case but it does not.


---


### compiler : `gcc`
### title : `Code quality regression with for (int i: {1,2,4,6})`
### open_at : `2023-06-01T07:26:09Z`
### last_modified_date : `2023-08-18T17:52:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110070
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.3.1`
### severity : `normal`
### contents :
The fix for PR c++/70167 (in GCC 11.3) inadvertently introduced a code quality regression for simple range-for using initializer lists.  The motivating example is an idiom from the stockfish benchmark [update_continuation_histories in src/search.cpp]:

#include <initializer_list>
extern void ext(int);
void foo()
{
  for (int i: {1,2,4,6})
    ext(i);
}

which currently generates inefficient code by copying the array (to the stack) before use:
foo():
        pushq   %rbp
        pushq   %rbx
        subq    $24, %rsp
        movdqa  .LC0(%rip), %xmm0
        movq    %rsp, %rbx
        leaq    16(%rsp), %rbp
        movaps  %xmm0, (%rsp)
.L2:
        movl    (%rbx), %edi
        addq    $4, %rbx
        call    ext(int)
        cmpq    %rbp, %rbx
        jne     .L2
        addq    $24, %rsp
        popq    %rbx
        popq    %rbp
        ret
.LC0:
        .long   1
        .long   2
        .long   4
        .long   6

In GCC 11.2 and earlier, the initializing array is efficiently used without copying:

foo():
        pushq   %rbx
        movl    $C.0.0, %ebx
.L2:
        movl    (%rbx), %edi
        addq    $4, %rbx
        call    ext(int)
        cmpq    $C.0.0+16, %rbx
        jne     .L2
        popq    %rbx
        ret
C.0.0:
        .long   1
        .long   2
        .long   4
        .long   6

The underlying cause of the code difference stems from whether the initializer is marked "static" in the middle-end, as shown by the differences between:

  const int init[4] = {1,2,4,6};
  for (int i: init) ... // generates a copy

and 

  static const int init[4] = {1,2,4,6};
  for (int i: init) ... // doesn't generate a copy


Fortunately, there's already code in the depth of the C++ front=end for marking such initializer lists/constructors as static, so I initially tried fixing this myself, at first trying:

diff --git a/gcc/cp/semantics.cc b/gcc/cp/semantics.cc
index 05df628..a91693d 100644
--- a/gcc/cp/semantics.cc
+++ b/gcc/cp/semantics.cc
@@ -3314,7 +3314,6 @@ finish_compound_literal (tree type, tree compound_literal,
   /* FIXME all C99 compound literals should be variables rather than C++
      temporaries, unless they are used as an aggregate initializer.  */
   if ((!at_function_scope_p () || CP_TYPE_CONST_P (type))
-      && fcl_context == fcl_c99
       && TREE_CODE (type) == ARRAY_TYPE
       && !TYPE_HAS_NONTRIVIAL_DESTRUCTOR (type)
       && initializer_constant_valid_p (compound_literal, type))

and then trying:

diff --git a/gcc/cp/call.cc b/gcc/cp/call.cc
index 2736f55..53220da 100644
--- a/gcc/cp/call.cc
+++ b/gcc/cp/call.cc
@@ -8557,7 +8557,10 @@ convert_like_internal (conversion *convs, tree expr, tree
 fn, int argnum,
 	    elttype = cp_build_qualified_type
 	      (elttype, cp_type_quals (elttype) | TYPE_QUAL_CONST);
 	    array = build_array_of_n_type (elttype, len);
-	    array = finish_compound_literal (array, new_ctor, complain);
+	    /* Indicate that a non-lvalue static const array is acceptable
+ 	       by specifying fcl_c99.  */
+	    array = finish_compound_literal (array, new_ctor, complain,
+					     fcl_c99);
 	    /* Take the address explicitly rather than via decay_conversion
 	       to avoid the error about taking the address of a temporary.  */
 	    array = cp_build_addr_expr (array, complain);


Both of which fix/improve code generation for this case, but break the initializer tests in the g++.dg testsuite in interesting ways.  At this point I thought I'd give up and leave the fix to the experts.  The range_expr passed to cp_convert_range_for is:

 <constructor 0x7ffff6dc9348
    type <lang_type 0x7ffff6dadc78 init list VOID
        align:1 warn_if_not_align:0 symtab:0 alias-set -1 canonical-type 0x7ffff6dadc78>
    constant length:4
    val <non_lvalue_expr 0x7ffff6dcd540
        type <integer_type 0x7ffff6c415e8 int public type_6 SI
            size <integer_cst 0x7ffff6c43228 constant 32>
            unit-size <integer_cst 0x7ffff6c43240 constant 4>
            align:32 warn_if_not_align:0 symtab:0 alias-set -1 canonical-type 0x7ffff6c415e8 precision:32 min <integer_cst 0x7ffff6c431e0 -2147483648> max <integer_cst 0x7ffff6c431f8 2147483647>
            pointer_to_this <pointer_type 0x7ffff6c49b28>>
        constant public
        arg:0 <integer_cst 0x7ffff6c43390 constant 1>
        iter.cc:7:16 start: iter.cc:7:16 finish: iter.cc:7:16>
    val <non_lvalue_expr 0x7ffff6dcd560 type <integer_type 0x7ffff6c415e8 int>
        constant public
        arg:0 <integer_cst 0x7ffff6c43768 constant 2>
        iter.cc:7:18 start: iter.cc:7:18 finish: iter.cc:7:18>
    val <non_lvalue_expr 0x7ffff6dcd580 type <integer_type 0x7ffff6c415e8 int>
        constant public
        arg:0 <integer_cst 0x7ffff6c43780 constant 4>
        iter.cc:7:20 start: iter.cc:7:20 finish: iter.cc:7:20>
    val <non_lvalue_expr 0x7ffff6dcd5a0 type <integer_type 0x7ffff6c415e8 int>
        constant public
        arg:0 <integer_cst 0x7ffff6c437b0 constant 6>
        iter.cc:7:22 start: iter.cc:7:22 finish: iter.cc:7:22>>

which contains a lot of non_lvalue_expr, so it's surprising (to me) that we try to turn this into an lvalue, when it should/could be read-only.

Thanks in advance.  My apologies if this is a duplicate/known issue.
Ideally, GCC should be able to unroll this loop, but that's a different issue.


---


### compiler : `gcc`
### title : `improve_allocation() routine should consider save/restore cost of callee-save registers`
### open_at : `2023-06-01T07:55:01Z`
### last_modified_date : `2023-09-20T01:55:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110071
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `unknown`
### severity : `normal`
### contents :
For the following test:

long
foo (long i, long cond)
{
  if (cond)
    bar ();
  return i+1;
}
   
Input RTL to IRA:

BB2:
  set r123, r4
  set r122, r3
  set r120, compare(r123, 0)
  set r118, r122
  if r120 jump BB4 else jump BB3
BB3:
  call bar()
BB4:
  set r3, r118+1
  return r3

IRA assigns r31 to r118.
Since r31 is a callee save register in powerpc, we need to generate spill/restore code.

This assignment of r31 to r118 causes shrink wrap to fail for this test.
Since r31 is assigned to r118, BB2 requires a prolog and shrink wrap fails.

In the IRA pass, after graph coloring, r118 gets assigned to r3.
The routine improve_allocation() is called after graph coloring. This routine changes the assignment of r118 to r31.

In improve_allocation() routine, IRA checks for each allocno if spilling any conflicting allocnos can improve the allocation of this allocno. This routine computes the cost improvement for usage of each profitable hard register for a given allocno.

The existing code in improve_allocation() does not consider the save/restore costs of callee save registers while computing the cost improvement.

This bug is for adding save/restore costs while computing cost improvement.

Save/restore costs should be considered only for the first assignment of a callee save register. Subsequent assignments of the same register do not need to consider this cost.


---


### compiler : `gcc`
### title : `[13/14 Regression] Missed Dead Code Elimination at -Os when using __builtin_unreachable since r13-6945-g429a7a88438`
### open_at : `2023-06-01T13:54:43Z`
### last_modified_date : `2023-09-19T15:18:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110080
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Given: 

void foo(void);
static unsigned char a = 131;
static int *b;
static int **c = &b;
static void d(int e, unsigned f) {
    int *g;
    if (e){
        for (; a; ++a)
            for (e = 0; 0;)
                ;
        g = &e;
        int **h = &g;
        if (**h) {
            foo();
        }
    }
    *c = &e;
}
int main() { d(4 & a, a); }


gcc trunk/13.1/12.3 at -Os generate:

main:
        testb   $4, a(%rip)
        je      .L3
        movb    $0, a(%rip)
.L3:
        leaq    -4(%rsp), %rax
        movq    %rax, b(%rip)
        xorl    %eax, %eax
        ret
a:
        .byte   -125

If I include a __builtin_unreachable() to help the compiler:

void foo(void);
static unsigned char a = 131;
static int *b;
static int **c = &b;
static void d(int e, unsigned f) {
    int *g;
    if (f != 131) {
        __builtin_unreachable(); // <- THIS
    }
    if (!e){
        for (; a; ++a)
            for (e = 0; 0;)
                ;
        g = &e;
        int **h = &g;
        if (**h) {
            foo();
        }
    }
    *c = &e;
}
int main() { d(4 & a, a); }

gcc-12.3 at -Os generates better code:

main:
        leaq    -4(%rsp), %rax
        movb    $0, a(%rip)
        movq    %rax, b(%rip)
        xorl    %eax, %eax
        ret
a:
        .byte   -125

But gcc-13.1/trunk at -Os generate worse code:

main:
        subq    $24, %rsp
        movb    a(%rip), %al
        movl    %eax, %edx
        andl    $4, %edx
        movl    %edx, 12(%rsp)
        xorl    %edx, %edx
.L2:
        testb   %al, %al
        je      .L8
        incl    %eax
        movb    $1, %dl
        jmp     .L2
.L8:
        testb   %dl, %dl
        je      .L4
        movb    $0, a(%rip)
        jmp     .L5
.L4:
        cmpl    $0, 12(%rsp)
        je      .L5
        call    foo
.L5:
        leaq    12(%rsp), %rax
        movq    %rax, b(%rip)
        xorl    %eax, %eax
        addq    $24, %rsp
        ret
a:
        .byte   -125

https://godbolt.org/z/zvqshjEj3


Started with r13-6945-g429a7a88438


---


### compiler : `gcc`
### title : `Missing if conversion`
### open_at : `2023-06-02T09:39:39Z`
### last_modified_date : `2023-06-09T05:52:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110087
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
#include <stdbool.h>

_Bool foo (void);

_Bool bar (_Bool r)
{
  if (foo ())
    r = true;

  return r;
}
--cut here--

compiles for x86_64 target (-O2) to:

        movl    %edi, %ebx
        call    foo
        testb   %al, %al
        cmove   %ebx, %eax

More optimal code would be:

        movl    %edi, %ebx
        call    foo
        orb     %bl, %al


---


### compiler : `gcc`
### title : `[avr] Improve operation with const on l-reg after move from d-reg`
### open_at : `2023-06-02T10:26:41Z`
### last_modified_date : `2023-06-02T11:03:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110088
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `minor`
### contents :
The register allocator might generate code that moves a value from a d-reg to an l-reg, and then performs operation like PLUS, IOR, AND with a constant.  So we have a sequence like:

lreg = dreg
lreg = lreg <op> const

This is expensive because lreg cannot handle const and needs a QI scratch to load bytes of const.  Instead, if dreg dies after the first insn, better code is:

dreg = dreg <op> const
lreg = dreg

An RTL peephole can catch this.


---


### compiler : `gcc`
### title : `[12/13/14 Regression][avr] Move frenzy leading to code bloat`
### open_at : `2023-06-02T13:22:53Z`
### last_modified_date : `2023-08-30T14:04:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110093
### status : `NEW`
### tags : `missed-optimization, needs-bisection, ra`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
So here is a C test case:

long add (long aa, long bb, long cc)
{
    if (cc < 0)
        return aa - cc;
    return aa + bb;
}

$ avr-gcc-8 -S -Os -dp
compiles this to following assembly:

add:
	push r14		 ;  30	[c=4 l=1]  pushqi1/0
	push r15		 ;  31	[c=4 l=1]  pushqi1/0
	push r16		 ;  32	[c=4 l=1]  pushqi1/0
	push r17		 ;  33	[c=4 l=1]  pushqi1/0
/* prologue: function */
/* frame size = 0 */
/* stack size = 4 */
	sbrs r17,7	 ;  42	[c=28 l=2]  *sbrx_and_branchsi
	rjmp .L2	
	sub r22,r14	 ;  11	[c=16 l=4]  subsi3/0
	sbc r23,r15
	sbc r24,r16
	sbc r25,r17
.L1:
/* epilogue start */
	pop r17		 ;  36	[c=4 l=1]  popqi
	pop r16		 ;  37	[c=4 l=1]  popqi
	pop r15		 ;  38	[c=4 l=1]  popqi
	pop r14		 ;  39	[c=4 l=1]  popqi
	ret		 ;  40	[c=0 l=1]  return_from_epilogue
.L2:
	add r22,r18	 ;  16	[c=16 l=4]  addsi3/0
	adc r23,r19
	adc r24,r20
	adc r25,r21
	rjmp .L1		 ;  43	[c=4 l=1]  jump

Notice that the operations on aa in SI:22 can be done in place, no moves needed.  The superfluous PUSHes and POPs is PR109910, which is yet another issue...

The code from above then deteriorates with v12, v13, v14 20230501 to a move bonanza that starts moving stuff for no reason, leading to high register pressure,  required stack increases from 4 bytes to 14 bytes, code size increase from 20 instructions to 56:

add:
	push r4		 ;  85	[c=4 l=1]  pushqi1/0
	push r5		 ;  86	[c=4 l=1]  pushqi1/0
	push r6		 ;  87	[c=4 l=1]  pushqi1/0
	push r7		 ;  88	[c=4 l=1]  pushqi1/0
	push r8		 ;  89	[c=4 l=1]  pushqi1/0
	push r9		 ;  90	[c=4 l=1]  pushqi1/0
	push r10		 ;  91	[c=4 l=1]  pushqi1/0
	push r11		 ;  92	[c=4 l=1]  pushqi1/0
	push r12		 ;  93	[c=4 l=1]  pushqi1/0
	push r13		 ;  94	[c=4 l=1]  pushqi1/0
	push r14		 ;  95	[c=4 l=1]  pushqi1/0
	push r15		 ;  96	[c=4 l=1]  pushqi1/0
	push r16		 ;  97	[c=4 l=1]  pushqi1/0
	push r17		 ;  98	[c=4 l=1]  pushqi1/0
/* prologue: function */
/* frame size = 0 */
/* stack size = 14 */
.L__stack_usage = 14
	mov r4,r22	 ;  68	[c=4 l=1]  movqi_insn/0
	mov r5,r23	 ;  69	[c=4 l=1]  movqi_insn/0
	mov r6,r24	 ;  70	[c=4 l=1]  movqi_insn/0
	mov r7,r25	 ;  71	[c=4 l=1]  movqi_insn/0
	mov r8,r18	 ;  72	[c=4 l=1]  movqi_insn/0
	mov r9,r19	 ;  73	[c=4 l=1]  movqi_insn/0
	mov r10,r20	 ;  74	[c=4 l=1]  movqi_insn/0
	mov r11,r21	 ;  75	[c=4 l=1]  movqi_insn/0
	mov r12,r14	 ;  78	[c=4 l=1]  movqi_insn/0
	mov r13,r15	 ;  79	[c=4 l=1]  movqi_insn/0
	mov r14,r16	 ;  80	[c=4 l=1]  movqi_insn/0
	mov r15,r17	 ;  81	[c=4 l=1]  movqi_insn/0
	mov r25,r7	 ;  66	[c=4 l=4]  *movsi/0
	mov r24,r6
	mov r23,r5
	mov r22,r4
	sbrs r15,7	 ;  117	[c=28 l=2]  *sbrx_and_branchsi
	rjmp .L2	
	sub r22,r12	 ;  67	[c=16 l=4]  *subsi3/0
	sbc r23,r13
	sbc r24,r14
	sbc r25,r15
.L1:
/* epilogue start */
	pop r17		 ;  101	[c=4 l=1]  popqi
	pop r16		 ;  102	[c=4 l=1]  popqi
	pop r15		 ;  103	[c=4 l=1]  popqi
	pop r14		 ;  104	[c=4 l=1]  popqi
	pop r13		 ;  105	[c=4 l=1]  popqi
	pop r12		 ;  106	[c=4 l=1]  popqi
	pop r11		 ;  107	[c=4 l=1]  popqi
	pop r10		 ;  108	[c=4 l=1]  popqi
	pop r9		 ;  109	[c=4 l=1]  popqi
	pop r8		 ;  110	[c=4 l=1]  popqi
	pop r7		 ;  111	[c=4 l=1]  popqi
	pop r6		 ;  112	[c=4 l=1]  popqi
	pop r5		 ;  113	[c=4 l=1]  popqi
	pop r4		 ;  114	[c=4 l=1]  popqi
	ret		 ;  115	[c=0 l=1]  return_from_epilogue
.L2:
	add r22,r8	 ;  65	[c=16 l=4]  *addsi3/0
	adc r23,r9
	adc r24,r10
	adc r25,r11
	rjmp .L1		 ;  118	[c=4 l=1]  jump

Then finally, with v14 20230602, crazyness increases even more to even requires a stack frame and a frame pointer.  Register allocator starts to move stuff to a stack slot and back again.  Code size increases again from 56 instructions to 68, more stack usage:

add:
	push r4		 ;  84	[c=4 l=1]  pushqi1/0
	push r5		 ;  85	[c=4 l=1]  pushqi1/0
	push r6		 ;  86	[c=4 l=1]  pushqi1/0
	push r7		 ;  87	[c=4 l=1]  pushqi1/0
	push r8		 ;  88	[c=4 l=1]  pushqi1/0
	push r9		 ;  89	[c=4 l=1]  pushqi1/0
	push r10		 ;  90	[c=4 l=1]  pushqi1/0
	push r11		 ;  91	[c=4 l=1]  pushqi1/0
	push r14		 ;  92	[c=4 l=1]  pushqi1/0
	push r15		 ;  93	[c=4 l=1]  pushqi1/0
	push r16		 ;  94	[c=4 l=1]  pushqi1/0
	push r17		 ;  95	[c=4 l=1]  pushqi1/0
	push r28		 ;  96	[c=4 l=1]  pushqi1/0
	push r29		 ;  97	[c=4 l=1]  pushqi1/0
	 ; SP -= 4	 ;  101	[c=4 l=2]  *addhi3_sp
	rcall .	
	rcall .	
	in r28,__SP_L__	 ;  127	[c=4 l=2]  *movhi/7
	in r29,__SP_H__
/* prologue: function */
/* frame size = 4 */
/* stack size = 18 */
.L__stack_usage = 18
	mov r8,r22	 ;  69	[c=4 l=1]  movqi_insn/0
	mov r9,r23	 ;  70	[c=4 l=1]  movqi_insn/0
	mov r10,r24	 ;  71	[c=4 l=1]  movqi_insn/0
	mov r11,r25	 ;  72	[c=4 l=1]  movqi_insn/0
	std Y+1,r18	 ;  73	[c=4 l=1]  movqi_insn/2
	std Y+2,r19	 ;  74	[c=4 l=1]  movqi_insn/2
	std Y+3,r20	 ;  75	[c=4 l=1]  movqi_insn/2
	std Y+4,r21	 ;  76	[c=4 l=1]  movqi_insn/2
	mov r4,r14	 ;  77	[c=4 l=1]  movqi_insn/0
	mov r5,r15	 ;  78	[c=4 l=1]  movqi_insn/0
	mov r6,r16	 ;  79	[c=4 l=1]  movqi_insn/0
	mov r7,r17	 ;  80	[c=4 l=1]  movqi_insn/0
	sbrs r7,7	 ;  124	[c=28 l=2]  *sbrx_and_branchsi
	rjmp .L2	
	mov r25,r11	 ;  67	[c=4 l=4]  *movsi/0
	mov r24,r10
	mov r23,r9
	mov r22,r8
	sub r22,r4	 ;  68	[c=16 l=4]  *subsi3/0
	sbc r23,r5
	sbc r24,r6
	sbc r25,r7
.L1:
/* epilogue start */
	 ; SP += 4	 ;  107	[c=4 l=4]  *addhi3_sp
	pop __tmp_reg__
	pop __tmp_reg__
	pop __tmp_reg__
	pop __tmp_reg__
	pop r29		 ;  108	[c=4 l=1]  popqi
	pop r28		 ;  109	[c=4 l=1]  popqi
	pop r17		 ;  110	[c=4 l=1]  popqi
	pop r16		 ;  111	[c=4 l=1]  popqi
	pop r15		 ;  112	[c=4 l=1]  popqi
	pop r14		 ;  113	[c=4 l=1]  popqi
	pop r11		 ;  114	[c=4 l=1]  popqi
	pop r10		 ;  115	[c=4 l=1]  popqi
	pop r9		 ;  116	[c=4 l=1]  popqi
	pop r8		 ;  117	[c=4 l=1]  popqi
	pop r7		 ;  118	[c=4 l=1]  popqi
	pop r6		 ;  119	[c=4 l=1]  popqi
	pop r5		 ;  120	[c=4 l=1]  popqi
	pop r4		 ;  121	[c=4 l=1]  popqi
	ret		 ;  122	[c=0 l=1]  return_from_epilogue
.L2:
	ldd r22,Y+1	 ;  65	[c=16 l=4]  *movsi/2
	ldd r23,Y+2
	ldd r24,Y+3
	ldd r25,Y+4
	add r22,r8	 ;  66	[c=16 l=4]  *addsi3/0
	adc r23,r9
	adc r24,r10
	adc r25,r11
	rjmp .L1		 ;  125	[c=4 l=1]  jump

So we have the following results:

Optimal code:                 Size 12 Instr, no Stack
avr-gcc v8:                   Size 20 Instr,  4 Stack
avr-gcc v12, v13, v14 (May)   Size 56 Instr, 14 Stack
avr-gcc v14 (June)            Size 68 Instr, 18 Stack + Frame Pointer


Target: avr
Configured with: --target=avr --disable-nls --with-gnu-as --with-gnu-ld --disable-shared --enable-languages=c,c++


---


### compiler : `gcc`
### title : `gcc produces sub-optimal code for _addcarry_u64 chain`
### open_at : `2023-06-03T13:53:22Z`
### last_modified_date : `2023-07-07T11:29:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110104
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Consider the following code:

#include <x86intrin.h>
typedef unsigned long long u64;
typedef unsigned __int128 u128;
void testcase1(u64 *acc, u64 a, u64 b)
{
  u128 res = (u128)a*b;
  u64 lo = res, hi = res >> 64;
  unsigned char cf = 0;
  cf = _addcarry_u64(cf, lo, acc[0], acc+0);
  cf = _addcarry_u64(cf, hi, acc[1], acc+1);
  cf = _addcarry_u64(cf,  0, acc[2], acc+2);
}
void testcase2(u64 *acc, u64 a, u64 b)
{
  u128 res = (u128)a * b;
  u64 lo = res, hi = res >> 64;
  asm("add %[LO], %[D0]\n\t" "adc %[HI], %[D1]\n\t" "adc $0, %[D2]" :
  [D0] "+m" (acc[0]), [D1] "+m" (acc[1]), [D2] "+m" (acc[2]) :
  [LO] "r" (lo), [HI] "r" (hi) : "cc");
}

Compilation with either
gcc-trunk -Wall -Wextra -O3 -S testcase.c
gcc-trunk -Wall -Wextra -Os -S testcase.c
generate the same code:

// rdi = acc, rsi = a, rdx = b

testcase1:
  movq %rsi, %rax
  mulq %rdx
  addq %rax, (%rdi)
  movq %rdx, %rax
  adcq 8(%rdi), %rax
  adcq $0, 16(%rdi)
  movq %rax, 8(%rdi)
  ret

testcase2:
  movq %rsi, %rax	; rax = rsi = a
  mulq %rdx		; rdx:rax = rax*rdx = a*b
  add %rax, (%rdi)	; acc[0] += lo
  adc %rdx, 8(%rdi)	; acc[1] += hi + cf
  adc $0, 16(%rdi)	; acc[2] += cf
  ret


Conclusion:
gcc generates the expected code for testcase2.
However, the code for testcase1 is sub-optimal.

  movq %rdx, %rax
  adcq 8(%rdi), %rax
  movq %rax, 8(%rdi)

instead of

  adc %rdx, 8(%rdi)	; acc[1] += hi + cf


The copy of rdx to rax is useless.
The (load/add+store) ops can be merged into an load/add/store op.


---


### compiler : `gcc`
### title : `bool patterns that should produce a?b:c`
### open_at : `2023-06-04T08:15:18Z`
### last_modified_date : `2023-08-29T08:10:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110111
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
bool f(bool a, bool b, bool c)
{
        if (a)  return b;
        return c;
}
bool f1(bool a, bool b, bool c)
{
        return a & b | (!a & c);
}
bool f2(bool a, bool b, bool c)
{
        return a & b | (a < c);
}
```
All 3 should produce the same code.


---


### compiler : `gcc`
### title : `Miss CSE optimization for vptest after r14-1466-g3635e8c67e13e3da7e1e23a617dd9952218e93e0`
### open_at : `2023-06-05T01:55:01Z`
### last_modified_date : `2023-06-22T06:44:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110118
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include <immintrin.h>

int do_stuff(__m256i Y0, __m256i Y1, __m128i X2) {
  __m256i And01 = _mm256_and_si256(Y0, Y1);
  int TestResult = _mm256_testc_si256(And01, And01);
  int t1 = _mm256_testz_si256(And01, And01);
  return TestResult + t1;
}


GCC 12.2 generates

do_stuff:
        vpand   %ymm1, %ymm0, %ymm0
        xorl    %eax, %eax
        vptest  %ymm0, %ymm0
        sete    %al
        adcl    $0, %eax
        ret

GCC trunk generates

do_stuff:
        vpand   %ymm1, %ymm0, %ymm0
        xorl    %eax, %eax
        vptest  %ymm0, %ymm0
        setc    %al
        xorl    %edx, %edx
        vptest  %ymm0, %ymm0
        sete    %dl
        addl    %edx, %eax
        ret


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Missed Dead Code Elimination when using __builtin_unreachable since r12-6924-gc2b610e7c6c`
### open_at : `2023-06-05T18:35:14Z`
### last_modified_date : `2023-09-15T19:51:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110131
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/xr8Ka91rr

Given:

void foo(void);
static int a, d, g, m, *h = &a, *i = &a;
static char j, l, *n = &j;
static short k;
static short o(short b, short c) { return c == 0 || b && c == 1 ?: b / c; }
static char p(unsigned char e, char f) {
    if (!(((e) >= 1) && ((e) <= 255))) {
        __builtin_unreachable();
    }
    return e + f;
}
int main() {
    g = *h;
    *n = 0;
    int q = *i;
    l = p((k >= 0 ^ q >= k) - 1, 3);
    if (o(l >= j, g) + (unsigned)4294967291)
        o(0, d);
    else
        foo();
    p(&m != 0, 0);
}

gcc-11.4 -O2 generates:

main:
        movb    $0, j(%rip)
        xorl    %eax, %eax
        ret

but gcc-trunk -O2 generates:


main:
        movl    a(%rip), %eax
        movb    $0, j(%rip)
        cmpw    $1, %ax
        ja      .L8
.L4:
        xorl    %eax, %eax
        ret
.L8:
        cwtl
        leal    1(%rax), %edx
        cmpl    $2, %edx
        movl    $0, %edx
        cmova   %edx, %eax
        cmpl    $5, %eax
        jne     .L4
        pushq   %rax
        call    foo
        xorl    %eax, %eax
        popq    %rdx
        ret

(12 and 13 generate similar code)

Started with r12-6924-gc2b610e7c6c


---


### compiler : `gcc`
### title : `[10/11/12/13/14 Regression] (-unsigned1) != CST is not optimized to unsigned1 != CST at the gimple level`
### open_at : `2023-06-06T01:50:17Z`
### last_modified_date : `2023-06-07T03:02:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110134
### status : `RESOLVED`
### tags : `missed-optimization, patch, TREE`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
bool f1(int  a)
{
  unsigned t = a < 10;
  t = -t;
  return t != 0;
}

bool f2(int  a)
{
  signed t = a < 10;
  t = -t;
  return t != 0;
}
```

These should both optimize to just `a >= 9` in forwprop1 .
Both of these were working in GCC 5.
Starting GCC 6, neither was done and then in GCC 8, f2 was handled.

I noticed this while looking at PR 110131 (doing this does not fix that though).


---


### compiler : `gcc`
### title : `After optimization, the $r1 register will be broken when jumping to the jump table, resulting in a significant increase in the false prediction rate of branch prediction.`
### open_at : `2023-06-06T06:36:59Z`
### last_modified_date : `2023-06-15T08:51:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110136
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
Created attachment 55267
perlbench.ltrans15.ltrans.args.0

tag: releases/gcc-12.2.0
The code here replicates the problem.

$ ./libexec/gcc/loongarch64-linux-gnu/12.2.0/lto1 -quiet -dumpbase ./perlbench.ltrans15.ltrans -mabi=lp64d -march=loongarch64 -mfpu=64 -mcmodel=normal -mtune=loongarch64 -g -g -Ofast -Ofast -version -fno-openmp -fno-openacc -fcf-protection=none -fno-omit-frame-pointer -funroll-all-loops  -fltrans @./perlbench.ltrans15.ltrans.args.0 -fdump-rtl-all -o ./perlbench.ltrans15.ltrans.s -fpie

Perl_sv_upgrade:
...
 5908         addi.w  $r18,$r0,15                     # 0xf
 5909         bgtu    $r23,$r18,.L502
 5910         la.local        $r16,.L504
 5911         slli.d  $r19,$r23,3
 5912         ldx.d   $r20,$r16,$r19
 5913         add.d   $r1,$r16,$r20
 5914         jr      $r1
...

In the regrename passover optimization, replace the registers of lines 5193 and 5194 with $r1.

I tried debugging and found that the problem would be solved if hook HARD_REGNO_RENAME_OK was defined, but found that this was just an accident and there is no guarantee that this register will not be replaced with $r1 when jumping to the jump table.

The patch that defines the HARD_REGNO_RENAME_OK is as follows：
diff --git a/gcc/config/loongarch/loongarch.cc b/gcc/config/loongarch/loongarch.cc
index 5c9a33c14f7..0df0ae15c3e 100644
--- a/gcc/config/loongarch/loongarch.cc
+++ b/gcc/config/loongarch/loongarch.cc
@@ -5782,6 +5782,19 @@ loongarch_starting_frame_offset (void)
   return crtl->outgoing_args_size;
 }
 
+/* Return nonzero if register FROM_REGNO can be renamed to register
+   TO_REGNO.  */
+
+bool
+loongarch_hard_regno_rename_ok (unsigned from_regno ATTRIBUTE_UNUSED,
+                           unsigned to_regno)
+{
+  return df_regs_ever_live_p (to_regno);
+}
+
 /* Initialize the GCC target structure.  */
 #undef TARGET_ASM_ALIGNED_HI_OP
 #define TARGET_ASM_ALIGNED_HI_OP "\t.half\t"
diff --git a/gcc/config/loongarch/loongarch.h b/gcc/config/loongarch/loongarch.h
index f9de9a6e4fb..b22b439eaac 100644
--- a/gcc/config/loongarch/loongarch.h
+++ b/gcc/config/loongarch/loongarch.h
@@ -563,6 +563,8 @@ enum reg_class
 #define IMM_BITS 12
 #define IMM_REACH (1LL << IMM_BITS)
 
+#define HARD_REGNO_RENAME_OK(FROM, TO) loongarch_hard_regno_rename_ok (FROM, TO)
+
 /* True if VALUE is an unsigned 6-bit number.  */


Is there a way to make sure that the $r1 register is not corrupted when jumping to the table?


---


### compiler : `gcc`
### title : `implement clang -fassume-sane-operator-new`
### open_at : `2023-06-06T08:15:58Z`
### last_modified_date : `2023-08-02T08:24:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110137
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `14.0`
### severity : `enhancement`
### contents :
clang seems to have -fassume-sane-operator-new which allows to assert that
a call to the operator has no side-effects besides the allocation, in particular that it doesn't modify or inspect global memory.


---


### compiler : `gcc`
### title : `[14 Regression] TSVC s242 regression between g:c0df96b3cda5738afbba3a65bb054183c5cd5530 and g:e4c986fde56a6248f8fbe6cf0704e1da34b055d8`
### open_at : `2023-06-06T20:33:19Z`
### last_modified_date : `2023-09-26T15:04:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110148
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
Seen here:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=171.854.0 (benzen)
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=463.854.0 (helene)
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=283.854.0 (lntzen3)

So it seems to affect both intel (Helene) and zens
TSVC/s1244 is also regressing same day but only on zens


---


### compiler : `gcc`
### title : `Missing if conversion`
### open_at : `2023-06-07T12:21:05Z`
### last_modified_date : `2023-06-09T14:19:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110155
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
#include <stdbool.h>

_Bool foo (void);

int bar (int r)
{
  if (foo ())
    r++;

  return r;
}
--cut here--

compiles (gcc -O2) to:

        movl    %edi, %ebx
        call    foo
        cmpb    $1, %al
        sbbl    $-1, %ebx
        movl    %ebx, %eax

could be performed without compare as:

        movl    %edi, %ebx
        callq   foo
        movzbl  %al, %eax
        addl    %ebx, %eax


---


### compiler : `gcc`
### title : `[14 Regression] Comparing against a constant string is inefficient on some targets`
### open_at : `2023-06-07T20:33:11Z`
### last_modified_date : `2023-10-17T10:57:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110163
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Comparing against a constant string is expanded by inline_string_cmp and on some targets the generated code can be inefficient.  This can be seen in spec2017's omnetpp benchmark, particularly when the inline string comparison limits are increased.

The problem is the expansion code arranges to do all the arithmetic and tests in SImode.  On RV64 this introduces a sign extension for each test  due to how RV64 expresses 32bit ops.

It would be better to do all the computations in word_mode, then convert the final result to SImode, at least for RV64 and likely for other targets.

I experimented with starting to build out cost checks to determine what mode to use for the internal computations.  That ran afoul of x86 where the cost of a byte load is different than the cost of an extended byte load, even though they use the exact same instruction.

There's also a need to cost out the computations, test & branch in the different modes as well once the x86 hurdle is behind us.

I've set work on this aside for now.  But the discussion can be found in these two threads:

https://gcc.gnu.org/pipermail/gcc-patches/2023-June/620601.html
https://gcc.gnu.org/pipermail/gcc-patches/2023-June/620577.html

#include <string.h>
int
foo (char *x)
{
   return strcmp (x, "lowerLayout");
}

Compiled with -O2 --param builtin-string-cmp-inline-length=100 on rv64 should show the issue.


---


### compiler : `gcc`
### title : `[13/14 Regression] Missed Dead Code Elimination when using __builtin_unreachable`
### open_at : `2023-06-08T14:07:02Z`
### last_modified_date : `2023-08-25T23:44:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110173
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/ssja5dKdY


void foo(void);
static int e, f, n, *g, **h = &g;
static short m;
void __assert_fail() __attribute__((__noreturn__));
static short(a)(short b) { return b; }
static short(c)(short d) {
    if (!(((d) >= 1) && ((d) <= 1))) {
        __builtin_unreachable();
    }
    return 0;
}
static int *j();
static void k(int *l, char) {
    *l = 0;
    if (g == &e || g == 0 || g == &f);
    else {
        foo();
        __assert_fail();
    }
    c(f < 5);
}
static void i() { *h = j(); }
static int *j(unsigned o) {
    char p;
    if (g == &f || g == 0);
    else
        __assert_fail();
    p = m > (a(1) && o);
    k(&n, p);
    if (g == 0);
    else
        __assert_fail();
    return 0;
}
int main() { i(); }


gcc-trunk -Os -S: 

main:
        movq    g(%rip), %rax
        testq   %rax, %rax
        je      .L2
        cmpq    $f, %rax
        je      .L2
.L3:
        pushq   %rsi
        xorl    %eax, %eax
        call    __assert_fail
.L2:
        xorl    %ecx, %ecx
        movl    %ecx, n(%rip)
        testq   %rax, %rax
        jne     .L3
        xorl    %edx, %edx
        movq    %rdx, g(%rip)
        ret


main:
        pushq   %rsi
        movq    g(%rip), %rax
        cmpq    $f, %rax
        je      .L2
        testq   %rax, %rax
        jne     .L13
.L2:
        xorl    %ecx, %ecx
        movl    %ecx, n(%rip)
        cmpq    $f, %rax
        je      .L3
        testq   %rax, %rax
        je      .L11
        call    foo
.L13:
        xorl    %eax, %eax
        call    __assert_fail
.L3:
        testq   %rax, %rax
        jne     .L13
.L11:
        xorl    %eax, %eax
        movq    %rax, g(%rip)
        xorl    %eax, %eax
        popq    %rdx
        ret


Bisects to r14-569-g21e2ef2dc25


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Missed Dead Code Elimination when using __builtin_unreachable since r12-2305-g398572c1544`
### open_at : `2023-06-08T14:52:29Z`
### last_modified_date : `2023-06-24T08:48:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110177
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/d6zscbYYh

void foo(void);
static int c, *d = &c, *f, *i;
static unsigned e;
static int **g = &d, **j = &f,***h = &g;
static short k;
void __assert_fail() __attribute__((__noreturn__));
static short(a)(short b) {
    if (b != 0) {
        __builtin_unreachable();
    }
    return 0;
}
int main() {
    c = 0;
    for (; c <= 20; c = c + 9) {
        k = c << e;
        c &&a(c >> k);
        i = **h;
        *j = i;
        if (f)
            ;
        else
            __assert_fail();
    }
    e = 0;
    if (f);
    else
        foo();
}

gcc-11.4 -Os generates:

main:
        movq    g(%rip), %rax
        movl    $0, c(%rip)
        movq    (%rax), %rdx
        movq    %rdx, f(%rip)
        testq   %rdx, %rdx
        je      .L2
        movq    (%rax), %rax
        movq    %rax, f(%rip)
        testq   %rax, %rax
        jne     .L8
        movl    $9, c(%rip)
.L2:
        pushq   %rax
        xorl    %eax, %eax
        call    __assert_fail
.L8:
        movl    $27, c(%rip)
        xorl    %eax, %eax
        movl    $0, e(%rip)
        ret

gcc-trunk -Os generates:

main:
        xorl    %edi, %edi
        pushq   %rsi
        movq    g(%rip), %r8
        xorl    %eax, %eax
        movl    %edi, c(%rip)
        movl    e(%rip), %edi
        xorl    %edx, %edx
.L2:
        movl    %eax, %esi
        cmpl    $27, %eax
        je      .L23
        testl   %eax, %eax
        je      .L3
        movl    %edi, %ecx
        movl    %eax, %r9d
        movl    %eax, %r10d
        sall    %cl, %r9d
        movl    %r9d, %ecx
        sarl    %cl, %r10d
        testl   %r10d, %r10d
        je      .L3
        movl    %eax, c(%rip)
.L3:
        movq    (%r8), %rcx
        addl    $9, %eax
        movq    %rcx, f(%rip)
        testq   %rcx, %rcx
        jne     .L8
        testb   %dl, %dl
        je      .L4
        movl    %esi, c(%rip)
.L4:
        xorl    %eax, %eax
        call    __assert_fail
.L8:
        movb    $1, %dl
        jmp     .L2
.L23:
        testb   %dl, %dl
        je      .L6
        movl    $27, c(%rip)
.L6:
        xorl    %ecx, %ecx
        cmpq    $0, f(%rip)
        movl    %ecx, e(%rip)
        jne     .L7
        call    foo
.L7:
        xorl    %eax, %eax
        popq    %rdx
        ret

Bisects to r12-2305-g398572c1544


---


### compiler : `gcc`
### title : `longlong.h for RISCV should define count_leading_zeros and count_trailing_zeros and COUNT_LEADING_ZEROS_0 when ZBB is enabled`
### open_at : `2023-06-08T20:05:39Z`
### last_modified_date : `2023-06-08T22:18:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110181
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
I noticed that divdi3 on riscv32 with ZBB extensions always enabled still emits a lookup table but we really should just use __builtin_clzll/__builtin_clz (for the instruction).


---


### compiler : `gcc`
### title : `[x86] Missed optimisation: atomic operations should use PF, ZF and SF`
### open_at : `2023-06-08T22:55:32Z`
### last_modified_date : `2023-06-25T08:40:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110184
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.1`
### severity : `enhancement`
### contents :
Follow up from https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102566

The x86 locked ALU operations always set PF, ZF and SF, so the atomic builtins could use those to emit more optimal code instead of a cmpxchg loop.

Given:
template <auto Op> int atomic_rmw_op(std::atomic_int &i)
{
    int old = Op(i);
    if (old == 0)
        return 1;
    if (old < 0)
        return 2;
    return 0;
}

-------
Starting with the non-standard __atomic_OP_fetch, the current code for 

inline int andn_fetch_1(std::atomic_int &i)
{
    return __atomic_and_fetch((int *)&i, ~1, 0);
}

is

L33:
        movl    %eax, %edx
        andl    $-2, %edx
        lock cmpxchgl   %edx, (%rdi)
        jne     .L33
        movl    %edx, %eax
        shrl    $31, %eax
        addl    %eax, %eax      // eax = 2 if edx < 0
        testl   %edx, %edx
        movl    $1, %edx
        cmove   %edx, %eax

But it could be more optimally written as:

        movl    %ecx, 1
        movl    %edx, 2
        xorl    %eax, %eax
        lock andl    $-2, (%rdi)
        cmove   %ecx, %eax
        cmovs   %edx, %eax

The other __atomic_OP_fetch operations are very similar. I note that GCC already realises that if you perform __atomic_and_fetch(ptr, 1), the result can't have the sign bit set.

-------
For the standard atomic_fetch_OP operations, there are a couple of caveats:

fetch_and: if the retrieved value is ANDed again with the same pattern; for example:
    int pattern = 0x80000001;
    return i.fetch_and(pattern, std::memory_order_relaxed) & pattern;
This appears to be partially implemented, depending on what the pattern is. For example, it generates the optimal code for pattern = 3, 15, 0x7fffffff, 0x80000000. It appears to be related to testing for either SF or ZF, but not both.

fetch_or: always for SF, for the useful case when the pattern being ORed doesn't already contain the sign bit. If it does (a "non-useful case"), then the comparison is a constant, and likewise for ZF because it's never set if the pattern isn't zero.

fetch_xor: always, because the original value is reconstructible. Avoid generating unnecessary code in case the code already does the XOR itself, as in:

    return i.fetch_xor(1, std::memory_order_relaxed) ^ 1;


See https://gcc.godbolt.org/z/n9bMnaE4e for full results.


---


### compiler : `gcc`
### title : `gcc for RISC-V stack aligned error`
### open_at : `2023-06-09T07:27:10Z`
### last_modified_date : `2023-06-12T00:15:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110188
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
I use gcc(10.2.0) which is for RISC-V to compile a C language function, according to disassembly language, stack just consumed 3word(12bytes), RISC-V specification stipulate stack must aligned 16byte, but actual "addi sp,sp,-32" show that stack occupy 32bytes, why not 16bytes?
I have tried different optimization levels, but all occupy 32bytes.
I also have tried another compiler which is called zcc, that occupy 16bytes.

C code main.c is following
int myadd(int a1, int a2, int a3, int a4, int a5, int a6, int a7, int a8, int a9);
int main(void)
{
    return myadd(1,2,3,4,5,6,7,8,9);
}

command line is following
riscv64-unknown-elf-gcc -c main.c -o main.o -march=rv32imac -mpreferred-stack-boundary=4
riscv64-unknown-elf-objdump -d main.o > main.asm

diassembly main.asm is following
00000000 <main>:
   0:	1101                	addi	sp,sp,-32
   2:	ce06                	sw	ra,28(sp)
   4:	cc22                	sw	s0,24(sp)
   6:	1000                	addi	s0,sp,32
   8:	47a5                	li	a5,9
   a:	c03e                	sw	a5,0(sp)
   c:	48a1                	li	a7,8
   e:	481d                	li	a6,7
  10:	4799                	li	a5,6
  12:	4715                	li	a4,5
  14:	4691                	li	a3,4
  16:	460d                	li	a2,3
  18:	4589                	li	a1,2
  1a:	4505                	li	a0,1
  1c:	00000097          	auipc	ra,0x0
  20:	000080e7          	jalr	ra # 1c <main+0x1c>
  24:	87aa                	mv	a5,a0
  26:	853e                	mv	a0,a5
  28:	40f2                	lw	ra,28(sp)
  2a:	4462                	lw	s0,24(sp)
  2c:	6105                	addi	sp,sp,32
  2e:	8082                	ret


---


### compiler : `gcc`
### title : `Missed GIMPLE optimization with calls`
### open_at : `2023-06-09T10:08:57Z`
### last_modified_date : `2023-06-09T14:51:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110192
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
int *t;
long b;

void *
foo ()
{
  void *c;
  if (t)
    {
      c = operator new (b);
      return c;
    }
  void *d = operator new (b);
  return d;
}

has still in optimized dump
  t.0_1 = t;
  pretmp_13 = b;
  _14 = (long unsigned int) pretmp_13;
  if (t.0_1 != 0B)
    goto <bb 3>; [54.59%]
  else
    goto <bb 4>; [45.41%]

  <bb 3> [local count: 586155665]:
  _12 = operator new (_14); [tail call]
  goto <bb 5>; [100.00%]

  <bb 4> [local count: 487586160]:
  _10 = operator new (_14); [tail call]

  <bb 5> [local count: 1073741824]:
  # _6 = PHI <_12(3), _10(4)>
  return _6;
and only RTL optimizations manage (jump2) to just a single unconditional call.
In GCC 8 this causes ICE when t is thread_local (testcase reduced from skia) on powerpc64le because it isn't prepared to have all TLSLD references disappear during RTL optimizations (I think even trunk has a theoretical such problem where get_some_local_dynamic_name will return NULL if all such references are optimized out and it might misbehave in the callers).


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Missing VRP transformation with MIN_EXPR and known relation`
### open_at : `2023-06-09T22:28:13Z`
### last_modified_date : `2023-08-11T23:57:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110199
### status : `UNCONFIRMED`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
int test(int a, int b)
{
    if (a <= b)
        return a < b ? a : b;
    return 0;
}
```
This used to be optimized in GCC 11 and before via EVRP:

pushing new range for b_3(D): int [a_2(D), +INF]  EQUIVALENCES: { b_3(D) } (1 elements)
evrp visiting stmt _5 = MIN_EXPR <a_2(D), b_3(D)>;

extract_range_from_stmt visiting:
_5 = MIN_EXPR <a_2(D), b_3(D)>;
Intersecting
  int VARYING
and
  int VARYING
to
  int VARYING
Folding statement: _5 = MIN_EXPR <a_2(D), b_3(D)>;
Folded into: _5 = a_2(D);


But in GCC 12 and above it is missed:
Folding statement: _5 = MIN_EXPR <a_2(D), b_3(D)>;
 folding with relation a_2(D) <= b_3(D)
Not folded


---


### compiler : `gcc`
### title : `_mm512_ternarylogic_epi64 generates unnecessary operations`
### open_at : `2023-06-10T10:37:34Z`
### last_modified_date : `2023-08-05T15:32:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110202
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `enhancement`
### contents :
Consider the following two alternative implementations of a bitwise complement of an avx512 register.

#include <immintrin.h>

__m512i negate1(const __m512i *a)
{
    __m512i res;
    res = c(res, res, *a, 0x55);
    return res;
}

__m512i negate2(const __m512i *a)
{
    __m512i res;
    res = _mm512_xor_si512(*a, _mm512_set1_epi32(-1));
    return res;
}

which compiled with "-O3 -mavx512f" generates the asm listings (see godbolt: https://godbolt.org/z/jvrxEjW65)

negate1(long long __vector(8) const*):
        vpxor   xmm0, xmm0, xmm0
        vpternlogq      zmm0, zmm0, ZMMWORD PTR [rdi], 85
        ret
negate2(long long __vector(8) const*):
        vpternlogd      zmm0, zmm0, ZMMWORD PTR [rdi], 0x55
        ret

negate1 introduces an unnecessary xor operation. Probably this is because it does not recognize that, when vpternlogd is used with code 0x55, it only uses the third zmm argument.


---


### compiler : `gcc`
### title : `Sum should optimize to closed form`
### open_at : `2023-06-10T12:37:43Z`
### last_modified_date : `2023-06-11T01:03:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110203
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `enhancement`
### contents :
For both of

int foo(int num) {
    if(num == 1) return 1;
    return num + foo(num - 1);
}

int bar(int num) {
    int sum = 0;
    for(int i = 1; i <= num; i++) {
        sum += i;
    }
    return sum;
}


GCC emits loops:

foo(int):
        xor     eax, eax
        cmp     edi, 1
        je      .L8
.L2:
        mov     edx, edi
        sub     edi, 1
        add     eax, edx
        cmp     edi, 1
        jne     .L2
        add     eax, 1
        ret
.L8:
        mov     eax, 1
        ret
bar(int):
        test    edi, edi
        jle     .L12
        add     edi, 1
        mov     eax, 1
        xor     edx, edx
.L11:
        add     edx, eax
        add     eax, 1
        cmp     eax, edi
        jne     .L11
        mov     eax, edx
        ret
.L12:
        xor     edx, edx
        mov     eax, edx
        ret


GCC should optimize this to the closed form.

There is precedent for this transformation, llvm does it https://godbolt.org/z/Kfffe5aPz.


---


### compiler : `gcc`
### title : `[14 Regression] Suspicous warning when compiling ranges-v3 using GCC trunk (iteration 9223372036854775807 invokes undefined behavior)`
### open_at : `2023-06-10T14:41:03Z`
### last_modified_date : `2023-07-20T12:55:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110204
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, needs-bisection, needs-reduction`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Given code:

#include <vector>
#include <map>
#include <range/v3/all.hpp>

std::map<int, int> m;

std::vector<int> foo() {
  return ranges::to<std::vector>(m | ranges::views::keys);
}

GCC trunk reports:

/opt/compiler-explorer/gcc-trunk-20230610/include/c++/14.0.0/bits/stl_iterator_base_funcs.h:175:9: warning: iteration 9223372036854775807 invokes undefined behavior [-Waggressive-loop-optimizations]
  175 |         while (__n--)
      |         ^~~~~
/opt/compiler-explorer/gcc-trunk-20230610/include/c++/14.0.0/bits/stl_iterator_base_funcs.h:175:9: note: within this loop

while Clang and GCC 13 both say OK. I think this is a regression.
goldbolt link: https://godbolt.org/z/W33aMqr5c


---


### compiler : `gcc`
### title : `x86 backend lacks support for vec_pack_ssat_m and vec_pack_usat_m`
### open_at : `2023-06-12T05:53:39Z`
### last_modified_date : `2023-06-12T08:32:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110214
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
This is from PR108410:

> and the key thing to optimize is
> 
>   ivtmp_78 = ivtmp_77 + 4294967232; // -64
>   _79 = MIN_EXPR <ivtmp_78, 255>;
>   _80 = (unsigned char) _79;
>   _81 = {_80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80,
> _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80,
> _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80,
> _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80, _80,
> _80, _80, _80, _80, _80, _80};
> 
> that is we want to broadcast a saturated (to vector element precision) value.


Yes, backend needs to support vec_pack_ssat_m, vec_pack_usat_m.
But I didn't find optab for ss_truncate or us_truncate which might be used by BB vectorizer.

AVX512 support vpmov{u,}sqd, vpmov{u,}sdw, vpmov{u,}swb for demotion with signed/unsigned saturation.


---


### compiler : `gcc`
### title : `RA fails to allocate register when loop invariant lives across calls and eh`
### open_at : `2023-06-12T06:25:54Z`
### last_modified_date : `2023-06-27T06:00:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110215
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55305
A Testcase

Compiled with -Ofast, The innermost loop is

.L41:
        movups  (%rax), %xmm3
        movaps  (%rsp), %xmm0
        addq    $16, %rax
        subps   %xmm3, %xmm0
        andps   %xmm2, %xmm0
        movups  %xmm0, -16(%rax)
        addps   %xmm0, %xmm1
        cmpq    %rax, %rdx
        jne     .L41

While for Clang it produces

.LBB0_14:                               #   Parent Loop BB0_3 Depth=1
        movups  (%rbp,%rax), %xmm1
        movaps  %xmm3, %xmm2
        subps   %xmm1, %xmm2
        andps   %xmm4, %xmm2
        movups  %xmm2, (%rbp,%rax)
        addps   %xmm2, %xmm0
        addq    $16, %rax
        cmpq    %rax, %r12
        jne     .LBB0_14

The loop invariant `base` was spilled to stack in GCC, but for clang it can directly use a sse register.

Godbolt: https://godbolt.org/z/TTvG8M6E8


---


### compiler : `gcc`
### title : `[avr] SREG: use BSET and BCLR instead of load/modify/write`
### open_at : `2023-06-12T09:23:02Z`
### last_modified_date : `2023-07-12T20:01:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110217
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
Created attachment 55306
patch file for gcc 13.1

Hello.

I'd like to suggest a patch that makes the compiler use bset and bclr instructions when doing SREG &= 0x80 (or ~0x80) or similr. Right now, all versions, as far as I know, are using in/andi(ori)/out instructions to change the SREG.


---


### compiler : `gcc`
### title : `Missed optimization vectorizing booleans comparisons`
### open_at : `2023-06-12T13:04:32Z`
### last_modified_date : `2023-06-26T05:24:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110223
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
== truncate before bool

float a[1024], b[1024], c[1024], d[1024];
int k[1024];
_Bool res[1024];

int main ()
{
  int i;
  for (i = 0; i < 1024; i++)
    res[i] = k[i] != ((i - 3) == 0);
}

vectorizes but does the bit clear before the truncate. Due to the high unroll factor if done the other way around we can save the extra bitclears.

== reduce using unpack

float a[1024], b[1024], c[1024], d[1024];
_Bool k[1024];
_Bool res[1024];

int main ()
{
  int i;
  for (i = 0; i < 1024; i++)
    res[i] = k[i] != (i == 0);
}

Doesn't vectorize as the compiler doesn't know how to compare different boolean vector element sizes.  Because i is an integer the result is a V4SI backed boolean type, vs the V16QI one for k[i].  So it has to compare 4 V4SI vectors against 1 V16QI, it can do this by truncating the the 4 V4SI bools to 1 V16QI bool.

== mask vs non-mask type

_Bool k[1024];
_Bool res[1024];

int main ()
{
  char i;
  for (i = 0; i < 64; i++)
    res[i] = k[i] != (i == 0);
}

doesn't vectorize because the compiler doesn't know how to compare a boolean mask vs a non-mask boolean.  There's a comment in the source code that this can be done using a pattern (presumably casting the types earlier).

in my case I need these to work on gcond as well, not just assigns,  and since we don't codegen conds, it might be better to handle them in vectorizable_*.


---


### compiler : `gcc`
### title : `Unnecessary register move in indexed swap routine`
### open_at : `2023-06-13T18:59:50Z`
### last_modified_date : `2023-06-14T11:43:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110240
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
void swap (unsigned int * restrict a, unsigned int * restrict b)
{
  if (a[b[0]] > a[b[1]])
    {
      unsigned int tmp = b[0];
      b[0] = b[1];
      b[1] = tmp;
    }
}
$ gcc -O3 -S swap.c

gets me

swap:
.LFB0:
        .cfi_startproc
        movl    (%rsi), %ecx
        movl    4(%rsi), %r8d
        movq    %rcx, %rax
        movl    (%rdi,%rcx,4), %ecx
        cmpl    %ecx, (%rdi,%r8,4)
        jnb     .L1
        movl    %r8d, (%rsi)
        movl    %eax, 4(%rsi)
.L1:
        ret
        .cfi_endproc

where the

        movq    %rcx, %rax

is unneeded, because rcs is not overwritten.

(It is probably also a zero-latency operation due to register renaming,
but still).


---


### compiler : `gcc`
### title : `suboptimal code about `no_caller_saved_registers` on caller side`
### open_at : `2023-06-14T06:49:00Z`
### last_modified_date : `2023-06-15T03:52:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110247
### status : `UNCONFIRMED`
### tags : `documentation, missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Given:
(https://gcc.godbolt.org/z/xevzx56Y5)

```
int complex(int x, int y)
  __attribute__((__no_caller_saved_registers__));

int test(int x, int y, int z)
  {
    return complex(x, y) + complex(y, z) + complex(z, x);
  }
```


My understanding is that `__no_caller_saved_registers__` says no register will be clobbered by `complex`, so it is not necessary for GCC to establish a stack frame and push arguments there. This is gonna help a lot if the function will be inlined.


Clang generates much better assembly but I wonder whether it is valid to assume that arguments registers are also preserved, like

```
test:
  push rcx            ; align %rsp
                      ; %edi := x, %esi = y, %edx = z
  call complex        ;
  mov ecx, eax        ; %ecx = complex(x, y)
  xchg edi, esi       ; %edi = y
  xchg esi, edx       ; %esi = z, %edx = x
  call complex        ;
  add ecx, eax        ; %ecx += complex(y, z)
  mov edi, esi        ; %edi = z
  mov esi, edx        ; %esi = x
  call complex        ;
  add eax, ecx        ; %eax = %ecx + complex(z, x)
  pop rcx
  ret
```


---


### compiler : `gcc`
### title : `ivopts could under-cost for some addressing modes on len_{load,store}`
### open_at : `2023-06-14T09:20:36Z`
### last_modified_date : `2023-08-14T08:09:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110248
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `__builtin_unreachable helps optimisation at -O1 but not at -O2`
### open_at : `2023-06-14T11:01:34Z`
### last_modified_date : `2023-09-19T15:04:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110249
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
Sometimes it can be useful to use __builtin_unreachable() to give the compiler hints that can improve optimisation.  For example, it can be used here to tell the compiler that the parameter is always 8-byte aligned:

#include <stdint.h>
#include <string.h>

uint64_t read64(const uint64_t * p) {
    if ((uint64_t) p % 8 ) {
        __builtin_unreachable();
    }
     uint64_t value;
     memcpy( &value, p, sizeof(uint64_t) );
     return value;     
}

For some targets, such as 32-bit ARM and especially RISC-V, this can make a difference to the generated code.  In testing, when given -O1 the compiler takes advantage of the explicit undefined behaviour to see that the pointer is aligned, and generates a single 64-bit load.  With -O2, however, it seems that information is lost - perhaps due to earlier optimisation passes - and now slow unaligned load code is generated.

Ideally, such optimisation information from undefined behaviour (explicit via a builtin, or implicit via other code) should be kept - -O2 should have at least as much information as -O1.  An alternative would be the addition of a more directed "__builtin_assume" function that could be used.

(I know that in this particular case, __builtin_assume_aligned is available and works exactly as intended at -O1 and -O2, but I think this is a more general issue than just alignments.)


---


### compiler : `gcc`
### title : ``t < 0 ? 1 : min(t, 1)` is not simplified down to just `t != 0``
### open_at : `2023-06-15T03:18:06Z`
### last_modified_date : `2023-08-06T22:18:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110262
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int g(int min_need_stall)
{
  return  min_need_stall < 0 ? 1 : ((min_need_stall) < (1) ? (min_need_stall) : (1));
}
```
This should just be the same as `min_need_stall != 0` but currently is not.

Yes this code does shows up in real code, it is from sel-sched.cc:
          min_need_stall = min_need_stall < 0 ? 1 : MIN (min_need_stall, 1);

Note we currently miscompile it but that is due to PR 110252 (which I have a fix).


---


### compiler : `gcc`
### title : `[13 Regression] Missed Dead Code Elimination when using __builtin_unreachable since r13-4607-g2dc5d6b1e7e`
### open_at : `2023-06-15T16:22:17Z`
### last_modified_date : `2023-07-27T09:26:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110269
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/5qqPMsxEh

Given the following code:

void foo(void);
static int a = 1, c;
static int *b = &a;
static int **d = &b;
static int ***e = &d;
void __assert_fail() __attribute__((__noreturn__));
static int f() {
    if (a) return a;
    for (; c;) *e = 0;
    if (b) __assert_fail();
    return 6;
}
int main() {
    if (f()) {
        *d = 0;
        if (b == 0)
            ;
        else {
            __builtin_unreachable();
            __assert_fail();
        }
    }
    if (b == 0)
        ;
    else
        foo();
    ;
}

gcc-trunk -O3 generates:

main:
	subq	$8, %rsp
	movl	a(%rip), %eax
	testl	%eax, %eax
	jne	.L2
	cmpq	$0, b(%rip)
	jne	.L6
.L2:
	movq	d(%rip), %rax
	movq	$0, (%rax)
	cmpq	$0, b(%rip)
	je	.L3
	call	foo
.L3:
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
.L6:
	xorl	%eax, %eax
	call	__assert_fail

gcc-12.3.0 -O3 generates:

main:
	movl	a(%rip), %edx
	testl	%edx, %edx
	jne	.L2
	cmpq	$0, b(%rip)
	jne	.L7
.L2:
	movq	d(%rip), %rax
	movq	$0, (%rax)
	xorl	%eax, %eax
	ret
.L7:
	pushq	%rax
	xorl	%eax, %eax
	call	__assert_fail

Bisects to r13-4607-g2dc5d6b1e7e


---


### compiler : `gcc`
### title : `[14 Regression] FAIL: gcc.dg/tree-ssa/pr103257-1.c scan-tree-dump-times optimized "link_error" 0 from r14-1880-g827e208fa64771`
### open_at : `2023-06-16T08:31:39Z`
### last_modified_date : `2023-06-16T23:22:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110278
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
r14-1880-g827e208fa64771 causes

FAIL: gcc.dg/tree-ssa/pr103257-1.c scan-tree-dump-times optimized "link_error" 0

because we now fold more in .original:

-  if (a ((unsigned int) ((0, 1) && b != 0) > b, (int) (short int) c) != 0)
+  if (a ((unsigned int) (b != 0) > b, (int) (short int) c) != 0)

the call to link_error was optimized away in threadfull1 before this change,
after the change code is straight-line and no jump threading opportunity
is realized.

Before we enter threadfull1 with

  <bb 2> [local count: 1073741824]:
  c.0_1 = c;
  b.2_2 = b;
  if (b.2_2 != 0)
    goto <bb 4>; [50.00%]
  else
    goto <bb 3>; [50.00%]

  <bb 3> [local count: 536870913]:

  <bb 4> [local count: 1073741824]:
  # iftmp.1_5 = PHI <1(2), 0(3)>
  iftmp.3_3 = (unsigned int) iftmp.1_5;
  _4 = b.2_2 < iftmp.3_3;
  e.5_10 = (unsigned short) _4;
  f.6_11 = (unsigned short) c.0_1;
  _12 = e.5_10 * f.6_11;
  if (_12 != 0)
    goto <bb 5>; [33.00%]
  else
    goto <bb 6>; [67.00%]

  <bb 5> [local count: 354334800]:
  link_error ();

  <bb 6> [local count: 1073741824]:
  c = 0;
  return 0;

but afterwards the b != 0 condition is in straight-line code:

  <bb 2> [local count: 1073741824]:
  c.0_1 = c;
  b.1_2 = b;
  _3 = b.1_2 != 0;
  _4 = (unsigned int) _3;
  _5 = b.1_2 < _4;
  e.3_10 = (unsigned short) _5;
  f.4_11 = (unsigned short) c.0_1;
  _12 = e.3_10 * f.4_11;
  if (_12 != 0)
    goto <bb 3>; [33.00%]
  else
    goto <bb 4>; [67.00%]

  <bb 3> [local count: 354334800]:
  link_error ();

  <bb 4> [local count: 1073741824]:
  c = 0;
  return 0;


---


### compiler : `gcc`
### title : `[14 Regression] Regressions on aarch64 cause by handing FMA in reassoc (510.parest_r, 508.namd_r)`
### open_at : `2023-06-16T08:32:09Z`
### last_modified_date : `2023-08-09T18:09:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110279
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55339
[PATCH] Check for nested FMA chains in reassoc

After testing the recent patch "Handle FMA friendly in reassoc pass" (e5405f06) on some of our aarch64 machines, I found regressions in a few spec2017 fprate cases.

On ampere1, the patch introduced approximately 2% regression in 510.parest_r. Additionally, with fp_reassoc_width changed so that reassociation actually works on floating points additions (which brings about 1% overall benefit), there's approximately 5% regression in 508.namd_r on ampere1, and 2.6% on neoverse-n1.

The compile options we used is "-Ofast -mcpu=ampere1 -flto=32 --param avoid-fma-max-bits=512" for ampere1, and "-Ofast -mcpu=neoverse-n1 -flto=32" for neoverse-n1. The tests are single copy run.

Below is from my investigations.

1) From perf result, the regression in 510.parest_r is because the re-arranging in rank_ops_for_fma() produced 2 FMAs in a small loop, with the last FMA's result fed back into first one from PHI. With avoid-fma-max-bits, these candidates are dropped in widening_mul, causing 2% regression; without the parameter there is 1% regression.

Before the patch, the generated code looks like:
	label:  ....
	       fmul v2, v2, v3
	       fmla v2, v4, v5
	       fadd v1, v1, v2
	       ...
               b.ne  label

After the patch (without avoid-fma-max-bits):
	label:  ...
	       fmla v1, v2, v3
	       fmla v1, v4, v5
	       ...
               b.ne  label

2) As for 508.namd_r, there are slightly fewer FMAs generated. It seems the patch is not handling FMAs like ((a * b + c) * d + e) *... well. For example, below is a piece of CFG before reassoc2:

  _797 = A_788 * _796;
  fast_c = _797 + _1161;
  _815 = diffa * fast_d;
  _816 = fast_c + _815;
  _817 = diffa * _816;
  fast_dir = fast_b + _817;

Before the patch, optimized code looks like:

  fast_c = .FNMA (B_790, _798, _334);
  _816 = .FMA (diffa, fast_d, fast_c);
  fast_dir = .FMA (diffa, _816, fast_b);

After the patch:

  _815 = diffa * fast_d;
  _5910 = .FMA (A_788, _796, _815);
  _816 = _5909 + _5910;
  _817 = diffa * _816;
  _5908 = .FMA (A_788, _801, _817);
  fast_dir = _5907 + _5908;

I came out with a patch to solve this. I'll also attach here.


---


### compiler : `gcc`
### title : `Phiprop may be good idea in early opts`
### open_at : `2023-06-16T14:50:53Z`
### last_modified_date : `2023-06-24T08:50:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110289
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `enhancement`
### contents :
libstdc++ in push_back operation does equivalent of the following:

int max(int a, int b)
{
        int *ptr;
        if (a > b)
          ptr = &a;
        else
          ptr = &b;
        return *ptr;
}

this is a sily implementation of max operation that sadly is not optimized until late phiprop pass and confuses inlining heuristics: memory load is considered to be expensive.

I think this is win-win pass and not very expensive, so perhaps adding it to early opts is possible?


---


### compiler : `gcc`
### title : `Some `A CMP (A NEEQ 0)` is not simplified in some cases`
### open_at : `2023-06-16T23:22:57Z`
### last_modified_date : `2023-09-12T23:42:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110293
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
The full set is:
`x != (typeof x)(x == 0)` is always true (PR 110278)
`x == (typeof x)(x == 0)` is always false.
`x == (typeof x)(x != 0)` is `(unsigned_type)x <= 1`
`x != (typeof x)(x != 0)` is `(unsigned_type)x > 1`


`uns < (typeof uns)(uns != 0)` is always false (PR 110278)
`uns >= (typeof uns)(uns != 0)` is always true
`uns <= (typeof uns)(uns != 0)` is `uns <= 1`
`uns > (typeof uns)(uns != 0)` is `uns > 1`


---


### compiler : `gcc`
### title : `[14 Regression] x264 regression on  Neoverse-N1`
### open_at : `2023-06-18T20:11:42Z`
### last_modified_date : `2023-07-01T20:37:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110301
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
This happen between 
g:9e3607e19bcd34e1fc857ca44ae30a8a1a4f5e20
and
g:57446d1bc9757ee1fb030600d38fa9487231f2a4 (Jun 16 2023)

https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=581.377.0


---


### compiler : `gcc`
### title : `vector epilogue handling is inefficient`
### open_at : `2023-06-19T11:00:36Z`
### last_modified_date : `2023-07-04T07:08:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110310
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
It looks like we apply some analysis only when transforming the main vector loop.  In particular vect_do_peeling does the following which elides a
vector epilogue after costing.

  /* If we know the number of scalar iterations for the main loop we should
     check whether after the main loop there are enough iterations left over
     for the epilogue.  */
  if (vect_epilogues
      && LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo)
      && prolog_peeling >= 0
      && known_eq (vf, lowest_vf))
    {
      unsigned HOST_WIDE_INT eiters
        = (LOOP_VINFO_INT_NITERS (loop_vinfo)
           - LOOP_VINFO_PEELING_FOR_GAPS (loop_vinfo));

      eiters -= prolog_peeling;
      eiters
        = eiters % lowest_vf + LOOP_VINFO_PEELING_FOR_GAPS (loop_vinfo);

      while (!vect_update_epilogue_niters (epilogue_vinfo, eiters))
        {
          delete epilogue_vinfo;
          epilogue_vinfo = NULL;
          if (loop_vinfo->epilogue_vinfos.length () == 0)
            {
              vect_epilogues = false;
              break;
            }
          epilogue_vinfo = loop_vinfo->epilogue_vinfos[0];
          loop_vinfo->epilogue_vinfos.ordered_remove (0);
        }
      vect_epilogues_updated_niters = true;

So for example for the loop

void foo (int * __restrict a, int *b)
{
  for (int i = 0; i < 20; ++i)
    a[i] = b[i] + 42;
}

we end up with no vectorized epilogue when using AVX512 but instead of the
AVX2 epilogue which is discarded we'd like to use a SSE2 epilogue.  It
seems that vect_determine_partial_vectors_and_peeling as called from
vect_update_epilogue_niters should have been already determined when
analyzing the epilogue, but during the epilogue costing the loop_vinfo
still inherits the main loop NITER.

For the testcase at hand we're somewhat saved by BB vectorization but when
doing partial loop vectorization we unnecessarily get a AVX512 masked
epilogue here and the cost model doesn't get a chance to see the updated
known niter for the epilogue nor would there be a meaningful way to
do this when costs are compared because we have no way of estimating the
number of masked out lanes for example.


---


### compiler : `gcc`
### title : `Unused string literal is retained in assembler file`
### open_at : `2023-06-20T10:38:10Z`
### last_modified_date : `2023-06-20T17:08:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110318
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
~~~c
typedef typeof(sizeof 0) size_t;

int memcmp(const void *, const void *, size_t);

int demo(const char *);

int demo(const char *p) {
	const char *start = p;
	while (*p == 'C')
		p++;
	if (p - start == 2 && memcmp(start, "if", 2) == 0)
		return 2;
	if (p - start == 6 && memcmp(start, "ifndef", 6) == 0)
		return 6;
	return 0;
}
~~~

$ uname -a
NetBSD nbcurr.roland-illig.de 10.99.3 NetBSD 10.99.3 (GENERIC) #0: Fri Apr 21 02:17:32 UTC 2023
$ gcc-12.2.0 -O2 -S code.c
$ grep string code.s
        .string "ifndef"


The string "ifndef" is retained, even though it is not used in the code, because the comparison is inlined. Curiously, the string "if" is removed.

Both strings should be removed, as they are unnecessary.


---


### compiler : `gcc`
### title : `ELFv2 pc-rel ABI extension allows using r2 as a volatile register`
### open_at : `2023-06-20T13:40:45Z`
### last_modified_date : `2023-06-20T13:52:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110320
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
The PC-Relative ABI extension to the ELFv2 ABI defines register r2 (normally the TOC register in ELFv1 & ELFv2) as a volatile register available for use in the register allocator.  Currently GCC doesn't make use of that fact, so this is a missed-optimization opportunity.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Missed Dead Code Elimination when using __builtin_unreachable since r12-4790-g4b3a325f07a`
### open_at : `2023-06-20T17:11:37Z`
### last_modified_date : `2023-06-24T08:48:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110327
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/eT1jqzrT5

Given the following code:

void foo(void);
static int a, c;
static int *d, *e = &a;
static int **f = &d, **g = &e;
static int ***h = &g;
static int ****i = &h;
static short j;
static char(k)(char b) {
    if (!(((b) >= -28) && ((b) <= -28))) {
        __builtin_unreachable();
    }
    return 0;
}
static int l() {
    a = -28;
    for (; a != 2; a++) {
        j = c;
        c = 0;
        for (; c != 2; c = j)
            if (a) break;
        k(****i);
        if (****i >= 5 ^ 1) {
            ***i = *f;
            if (c < 1) return 1;
            if (a) foo();
            ;
        }
    }
    return a;
}
int main() { l(); }

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	movl	$-28, %eax
	movl	$-28, a(%rip)
	.p2align 4,,10
	.p2align 3
.L10:
	movl	c(%rip), %edx
	testl	%eax, %eax
	jne	.L19
	cmpw	$2, %dx
	je	.L4
.L5:
	jmp	.L5
	.p2align 4,,10
	.p2align 3
.L4:
	movl	$2, c(%rip)
	movl	$2, %esi
.L3:
	movq	h(%rip), %rdx
	movq	(%rdx), %rdx
	movq	(%rdx), %rcx
	cmpl	$4, (%rcx)
	jg	.L7
	movq	d(%rip), %rcx
	movq	%rcx, (%rdx)
	testl	%esi, %esi
	je	.L16
	testl	%eax, %eax
	jne	.L20
.L7:
	movl	a(%rip), %eax
	addl	$1, %eax
	movl	%eax, a(%rip)
	cmpl	$2, %eax
	jne	.L10
.L16:
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
	.p2align 4,,10
	.p2align 3
.L20:
	call	foo
	jmp	.L7
.L19:
	xorl	%edx, %edx
	xorl	%esi, %esi
	movl	%edx, c(%rip)
	jmp	.L3

gcc-11.4.0 -O2 eliminates the call to foo:

main:
	movl	$-28, a(%rip)
	movq	h(%rip), %rcx
	movl	$-28, %edx
	.p2align 4,,10
	.p2align 3
.L8:
	movzwl	c(%rip), %eax
	movl	$0, c(%rip)
	testl	%edx, %edx
	jne	.L2
	.p2align 4,,10
	.p2align 3
.L3:
	cmpw	$2, %ax
	jne	.L3
	movl	$2, c(%rip)
	movq	(%rcx), %rax
	movq	(%rax), %rdx
	cmpl	$4, (%rdx)
	jg	.L5
	movq	d(%rip), %rdx
	movq	%rdx, (%rax)
.L5:
	movl	a(%rip), %eax
	leal	1(%rax), %edx
	movl	%edx, a(%rip)
	cmpl	$2, %edx
	jne	.L8
.L13:
	xorl	%eax, %eax
	ret
.L2:
	movq	(%rcx), %rax
	movq	(%rax), %rdx
	cmpl	$4, (%rdx)
	jg	.L5
	movq	d(%rip), %rdx
	movq	%rdx, (%rax)
	jmp	.L13

Bisects to r12-4790-g4b3a325f07a


---


### compiler : `gcc`
### title : `ppc64 vec_extract with constant index is suboptimal on P8`
### open_at : `2023-06-21T03:19:41Z`
### last_modified_date : `2023-06-29T08:35:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110331
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
//test.c
#include <altivec.h>

#ifdef __BIG_ENDIAN__
#define LANE_B 3
#else
#define LANE_B 12
#endif

unsigned char foo1 (vector unsigned char v)
{
  return vec_extract (v, LANE_B);
}

Trunk generates:
        vspltb 2,2,3
        mfvsrd 3,34
        rlwinm 3,3,0,0xff

While it can be optimized as:
        mfvsrd 3,34
        rldicl 3,3,32,56


---


### compiler : `gcc`
### title : `int converted to double should be considered non-nan and non-nan*non-nan is still non-nan`
### open_at : `2023-06-21T07:35:41Z`
### last_modified_date : `2023-06-22T07:39:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110335
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
  void
  h (int n, int from, int to)
  {
    double dto = to;
    double damt = n * dto;
    if (damt != damt)
     __builtin_trap();
  }
  void
  h1 (double dto, double n)
  {
    if (dto != dto) __builtin_unreachable();
    if (n != n) __builtin_unreachable();
    double damt = n *  dto;
    if (damt != damt)
     __builtin_trap();
  }
```
I would assume this two functions be optimized to just `return;`

LLVM is able to optimize h but not h1.


---


### compiler : `gcc`
### title : `[13/14 Regression] Missed Dead Code Elimination when using __builtin_unreachable since r13-2020-g16b013c9d9b`
### open_at : `2023-06-22T11:40:40Z`
### last_modified_date : `2023-08-07T09:05:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110361
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/qab3679sq

Given the following code:

void foo(void);
static int a, b, d, p, k;
static int *c, *e, *g;
static short f;
static unsigned h;
static int **i = &c, **o = &g;
void __assert_fail() __attribute__((__noreturn__));
static void j(char, unsigned r) {
    h = r == 0 ?: 2 % r;
    int l = h;
    if (!(((l) >= 1) && ((l) <= 1))) {
        __builtin_unreachable();
    }
    int m;
    *i = &m;
}
static int *n();
static short q(short t) {
    *o = &p;
    return t;
}
static unsigned s() {
    int *j = n();
    if (e == 0 || e == &a || e == &d)
        ;
    else
        __assert_fail();
    return *j;
}
static int *n() {
    if (b) {
        if (c) __assert_fail();
        e = 0;
    }
    return &k;
}
int main() {
    j(q(s()), f);
    if (e == 0 || e == &a || e == &d)
        ;
    else
        foo();
    f = 0;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$24, %rsp
	movq	e(%rip), %rax
	movswl	f(%rip), %edx
	testq	%rax, %rax
	je	.L2
	cmpq	$a, %rax
	je	.L4
	cmpq	$d, %rax
	jne	.L17
.L4:
	movq	$p, g(%rip)
	testl	%edx, %edx
	je	.L18
	leaq	12(%rsp), %rdx
	movq	%rdx, c(%rip)
	cmpq	$a, %rax
	je	.L5
	cmpq	$d, %rax
	je	.L5
	call	foo
.L5:
	xorl	%eax, %eax
	movw	%ax, f(%rip)
	xorl	%eax, %eax
	addq	$24, %rsp
	ret
.L18:
	leaq	12(%rsp), %rax
	movq	%rax, c(%rip)
	jmp	.L5
.L2:
	movq	$p, g(%rip)
	leaq	12(%rsp), %rax
	movq	%rax, c(%rip)
	jmp	.L5
.L17:
	xorl	%eax, %eax
	call	__assert_fail

gcc-12.3.0 -O2 eliminates the call to foo:

main:
	subq	$24, %rsp
	movq	e(%rip), %rax
	testq	%rax, %rax
	je	.L2
	cmpq	$a, %rax
	je	.L2
	cmpq	$d, %rax
	jne	.L14
.L2:
	movq	$p, g(%rip)
	leaq	12(%rsp), %rax
	movq	%rax, c(%rip)
	xorl	%eax, %eax
	movw	%ax, f(%rip)
	xorl	%eax, %eax
	addq	$24, %rsp
	ret
.L14:
	xorl	%eax, %eax
	call	__assert_fail

Bisects to r13-2020-g16b013c9d9b


---


### compiler : `gcc`
### title : `Range information on lower bytes of __uint128_t`
### open_at : `2023-06-22T12:02:47Z`
### last_modified_date : `2023-07-06T12:37:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110362
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Consider the following example:

int test (__uint128_t a, __uint128_t b) {
  __uint128_t __a = b | (a << 32);
  return __a & 0xffffffff;
}

At the moment GCC-14 with -O2 generates the following assembly:

test(unsigned __int128, unsigned __int128):
        mov     rsi, rdi
        mov     rax, rdx
        sal     rsi, 32
        or      rax, rsi
        ret


Which could be simplified to just:

test(unsigned __int128, unsigned __int128):
        mov     rax, rdx
        ret

Godbolt playground: https://godbolt.org/z/K9x5vnhxq


---


### compiler : `gcc`
### title : `Early VRP and IPA-PROP should work out value ranges from __builtin_unreachable`
### open_at : `2023-06-23T15:37:29Z`
### last_modified_date : `2023-06-28T07:35:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110377
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.1.0`
### severity : `normal`
### contents :
In the following testcase
void test2(int);
void
test(int n)
{
        if (n > 5)
          __builtin_unreachable ();
        test2(n);
}
we should work out that value range of n passed to test2 is [INT_MIN,4].
This would help optimizing some code in libstdc++, which now uses similar constructs to ensure known value ranges.

I think it is a common case where such unreachable test can be retrofited to the SSA_NAME based on the fact that program can not terminate between definition and the conditional.

We currently get:
  function  test/0 parameter descriptors:
    param #0 n used undescribed_use
  Jump functions of caller  test/0:
    callsite  test/0 -> test2/2 : 
       param 0: PASS THROUGH: 0, op nop_expr
         value: 0x0, mask: 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
         Unknown VR
    callsite  test/0 -> __builtin_unreachable/1 :


---


### compiler : `gcc`
### title : `Reuse vector register for both scalar and vector value.`
### open_at : `2023-06-25T04:21:00Z`
### last_modified_date : `2023-06-25T13:24:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110400
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
From PR109812 #c18

Uroš Bizjak 2023-06-21 09:46:43 UTC
One interesting observation:

clang is able to do this:

  0.09 │     │  vmovddup     -0x8(%rdx,%rsi,1),%xmm3              ▒
  ...
  0.11 │     │  vfmadd231sd  %xmm2,%xmm3,%xmm1                    ▒
  ...
  0.74 │     │  vfmadd231pd  %xmm2,%xmm3,%xmm0                    ▒

It figures out that duplicated V2DFmode value in %xmm3 can also be accessed in the same register as DFmode value.

OTOH, current gcc does:

        vmovsd  (%rsi,%rax,8), %xmm1
        ...
        vmovddup        %xmm1, %xmm4
        ...
        vfmadd231pd     %xmm4, %xmm0, %xmm2
        ...
        vfmadd231sd     %xmm1, %xmm0, %xmm3

The above code needs two registers.

----------------------------------------------------

Similar with below testcase

typedef double v2df __attribute__((vector_size(16)));
v2df c;
double d;
void
foo (double* __restrict a)
{
    c = __extension__(v2df) {*a, *a};
    d = *a;
}

with option: -O2 -mavx2

GCC generates

foo(double*):
        vmovsd  (%rdi), %xmm0
        vmovddup        %xmm0, %xmm1
        vmovsd  %xmm0, d(%rip)
        vmovapd %xmm1, c(%rip)

Clang

foo(double*):                               # @foo(double*)
        vmovddup        (%rdi), %xmm0                   # xmm0 = mem[0,0]
        vmovaps %xmm0, c(%rip)
        vmovlps %xmm0, d(%rip)
        retq


---


### compiler : `gcc`
### title : `missing nonzerobits on branch`
### open_at : `2023-06-25T18:53:01Z`
### last_modified_date : `2023-06-25T18:53:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110405
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
void h(unsigned long);
void
f (unsigned long i)
{
  if ((i & 7) == 6)
    if(i & 1)
      h(0);
}
```

The call to h should be optimized away as we know on the branch (a&1) == 0 as we know the nonzerobits of i is ~1 .


---


### compiler : `gcc`
### title : `Missed optimization in DOM with a single value range`
### open_at : `2023-06-26T09:38:55Z`
### last_modified_date : `2023-07-19T16:36:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110410
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
The following code snippet:

#include <stdio.h>
int seed;
void hash(int *seed, int v) { *seed ^= v; }
unsigned int var_6 = 1341494070U;
int var_7 = 0;
unsigned short var_8 = 7472;
int var_13 = 518868156U;
unsigned var_17;
#define min(a, b) ((a) < (b) ? (a) : (b))
void test() {
    for (int ii_0 = 0; ii_0 < 20; ++ii_0)
        for (int ii_1 = 0; ii_1 < 10; ++ii_1)
            var_17 = min(var_6 ? var_8 >> (var_13 - 518868145U) : var_7, 1887837879 || 0);
}
int main() {
    test(2062689408U);
    hash(&seed, var_17);
    printf("%d\n", seed);
    return 0;
}

> $ /usr/gcc-trunk/bin/gcc -O0 bug.c; ./a.out
> 1
> $ /usr/gcc-trunk/bin/gcc -O2 bug.c; ./a.out
> 3

When compiled with -O2 or -O3, it prints 3 instead of 1. Earlier GCCs do not have this bug.

> $ /usr/gcc-trunk/bin/gcc --version
gcc (GCC) 14.0.0 20230619 (experimental) [master r14-1917-gf8e0270272]
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE


---


### compiler : `gcc`
### title : `[14 Regression] Missed Dead Code Elimination when using __builtin_unreachable since r14-1880-g827e208fa64`
### open_at : `2023-06-26T15:00:46Z`
### last_modified_date : `2023-09-02T20:52:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110413
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/75vvarEvs

Given the following code:

void foo(void);
static int a, b, d, e, g, h;
static short f;
static int *i, *k;
static int **l = &i;
void __assert_fail(char *, char *, int, char *) __attribute__((__noreturn__));
static int(c)() { return a + b; }
static int *j() {
    f | e & 4073709551611 && c();
    return &d;
}
int main() {
    *l = j();
    h = 5;
    for (; h; h--) {
        k = j();
        if (k && k <= &d)
            ;
        else {
            __builtin_unreachable();
            __assert_fail("", "", 0, __PRETTY_FUNCTION__);
        }
    }
    if (k || k == &g)
        ;
    else
        foo();
    ;
}

gcc-trunk -O1 does not eliminate the call to foo:

j:
	movl	$d, %eax
	ret
	

main:
	subq	$8, %rsp
	movl	$0, %eax
	call	j
	movq	%rax, i(%rip)
	movq	%rax, k(%rip)
	movl	$0, h(%rip)
	cmpq	$g, %rax
	je	.L3
	testq	%rax, %rax
	je	.L5
.L3:
	movl	$0, %eax
	addq	$8, %rsp
	ret
.L5:
	call	foo
	jmp	.L3

gcc-13.1.0 -O1 eliminates the call to foo:

j:
	movl	$d, %eax
	ret
	

main:
	movl	$0, %eax
	call	j
	movq	%rax, i(%rip)
	movq	%rax, k(%rip)
	movl	$0, h(%rip)
	movl	$0, %eax
	ret

Bisects to r14-1880-g827e208fa64


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression  since  r14-1127-g9e2017ae6ac`
### open_at : `2023-06-26T15:06:40Z`
### last_modified_date : `2023-09-02T20:51:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110414
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/5E795dPvY

Given the following code:

void foo(void);
static int a, b, e, l, m, n;
static int *c = &a, *j;
static int **d = &c;
static int ***f;
void __assert_fail(char *, char *, int, char *) __attribute__((__noreturn__));
static void g(char) {
    if (c == &b || c == &a)
        ;
    else
        __assert_fail("", "", 3, __PRETTY_FUNCTION__);
}
static void h();
static int *i();
static short k() {
    j = i();
    h();
    **f = &e;
    if (c == &b || c == &a || c == 0)
        __assert_fail("", "", 6, __PRETTY_FUNCTION__);
    return 0;
}
static void h() {
    if (l) {
        for (; l; l = l - 8) *d = &m;
        if (c == &a)
            ;
        else
            __assert_fail("", "", 9, __PRETTY_FUNCTION__);
    }
    if (c == &b || c == &a || c == 0)
        ;
    else {
        foo();
        __assert_fail("", "", 8, __PRETTY_FUNCTION__);
    }
}
static int *i() {
    if (c == &b || c == &a)
        ;
    else {
        __builtin_unreachable();
        __assert_fail("", "", 6, __PRETTY_FUNCTION__);
    }
    return &n;
}
int main() { g(a && k()); }

gcc-trunk -Os does not eliminate the call to foo:

main:
	pushq	%rcx
	cmpl	$0, a(%rip)
	je	.L2
	movl	l(%rip), %edx
	testl	%edx, %edx
	je	.L3
	movl	%edx, %eax
	movl	$__PRETTY_FUNCTION__.1, %ecx
	movq	$m, c(%rip)
	shrl	$3, %eax
	imull	$-8, %eax, %eax
	addl	%edx, %eax
	movl	$9, %edx
	movl	%eax, l(%rip)
	jmp	.L7
.L3:
	movq	c(%rip), %rax
	cmpq	$a, %rax
	sete	%dl
	cmpq	$b, %rax
	sete	%cl
	orb	%cl, %dl
	jne	.L4
	testq	%rax, %rax
	je	.L4
	call	foo
	movl	$__PRETTY_FUNCTION__.1, %ecx
	movl	$8, %edx
.L7:
	movl	$.LC0, %esi
	movq	%rsi, %rdi
	call	__assert_fail
.L4:
	movl	$__PRETTY_FUNCTION__.2, %ecx
	movl	$6, %edx
	jmp	.L7
.L2:
	movq	c(%rip), %rax
	cmpq	$a, %rax
	je	.L5
	cmpq	$b, %rax
	je	.L5
	movl	$__PRETTY_FUNCTION__.0, %ecx
	movl	$3, %edx
	jmp	.L7
.L5:
	xorl	%eax, %eax
	popq	%rdx
	ret

gcc-13.1.0 -Os eliminates the call to foo:

main:
	pushq	%rcx
	movq	c(%rip), %rdx
	cmpq	$a, %rdx
	sete	%al
	cmpq	$b, %rdx
	sete	%dl
	orl	%edx, %eax
	cmpl	$0, a(%rip)
	je	.L2
	movl	l(%rip), %edx
	testl	%edx, %edx
	je	.L3
	movl	%edx, %eax
	movl	$__PRETTY_FUNCTION__.1, %ecx
	movq	$m, c(%rip)
	shrl	$3, %eax
	imull	$-8, %eax, %eax
	addl	%edx, %eax
	movl	$9, %edx
	movl	%eax, l(%rip)
	jmp	.L7
.L2:
	testb	%al, %al
	jne	.L5
	movl	$__PRETTY_FUNCTION__.0, %ecx
	movl	$3, %edx
.L7:
	movl	$.LC0, %esi
	movq	%rsi, %rdi
	call	__assert_fail
.L3:
	movl	$__PRETTY_FUNCTION__.2, %ecx
	movl	$6, %edx
	jmp	.L7
.L5:
	xorl	%eax, %eax
	popq	%rdx
	ret

Bisects to r14-1127-g9e2017ae6ac


---


### compiler : `gcc`
### title : `Redundant constants not getting eliminated on RISCV.`
### open_at : `2023-06-27T00:54:46Z`
### last_modified_date : `2023-07-08T00:07:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110423
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Redundant constants, across basic blocks, don't seem to be eliminated robustly by gcc. I'd reported this last year [1] and this is just recapturing that info as a bugzilla PR.

[1] https://gcc.gnu.org/pipermail/gcc/2022-October/239645.html

When analyzing coremark build for RISC-V, noticed redundant constants 
not being eliminated. While this is a recurrent issue with RV, this 
specific instance is not unique to RV as I can trigger similar output on 
aarch64 with -fno-if-conversion, hence something which could be 
addressed in common passes.

-O3 -march=rv64gc_zba_zbb

crcu8:
	xor	a3,a0,a1
	andi	a3,a3,1
	srli	a4,a0,1
	srli	a5,a1,1
	beq	a3,zero,.L2

	li	a3,-24576	# 0xFFFF_A000
	addi	a3,a3,1		# 0xFFFF_A001
	xor	a5,a5,a3
	zext.h	a5,a5

.L2:
	xor	a4,a4,a5	
	andi	a4,a4,1	
	srli	a3,a0,2	
	srli	a5,a5,1	
	beq	a4,zero,.L3	

	li	a4,-24576	# 0xFFFF_A000
	addi	a4,a4,1		# 0xFFFF_A001
	xor	a5,a5,a4	
	zext.h	a5,a5		

.L3:
	xor	a3,a3,a5	
	andi	a3,a3,1	
	srli	a4,a0,3	
	srli	a5,a5,1	
	beq	a3,zero,.L4

	li	a3,-24576	# 0xFFFF_A000
	addi	a3,a3,1		# 0xFFFF_A001
[...]

.L8
	andi	a3,a5,1
	srli	a0,a5,1
	beq	a3,a4,.L9
	li	a5,-24576	# 0xFFFF_A000
	addi	a5,a5,1	        # 0xFFFF_A001
	xor	a0,a0,a5
	slli	a0,a0,48
	srli	a0,a0,48
.L9:
	ret


cse can't handle this: as explained by Jeff in [2] EBB can have jumps out but not jumps in, which misses the cfg paths needed to be traversed to find the equivalents.

[2] https://gcc.gnu.org/pipermail/gcc/2022-October/239646.html

Note that since gcc 13.1, this specific test generates different code since the match.pd change 6508d5e5a1a ("match.pd: rewrite select to branchless expression") now removes the branches and the arithmatic needing the large const. 
But this test is very convenient, so I'm continuing to use it and just revert the match.pd change in my local gcc build.


---


### compiler : `gcc`
### title : `missed CSE with VLA vectors`
### open_at : `2023-06-27T08:37:29Z`
### last_modified_date : `2023-06-27T10:34:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110428
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdint.h>

void __attribute__((noinline,noclone))
foo (uint16_t *out, uint16_t *res)
{
  int mask[] = { 0, 1, 1, 1, 1, 1, 1, 1 };
  int i;
  for (i = 0; i < 8; ++i)
    {
      if (mask[i])
        out[i] = 33;
    }
  uint16_t o0 = out[0];
  uint16_t o7 = out[3];
  uint16_t o14 = out[6];
  uint16_t o15 = out[7];
  res[0] = o0;
  res[2] = o7;
  res[4] = o14;
  res[6] = o15;
}

With -march=armv9.3-a -O3 -g0 -fno-vect-cost-model we fail to CSE the
out[] loads after vectorization.


---


### compiler : `gcc`
### title : `Redundant vector extract instruction on P9`
### open_at : `2023-06-27T09:12:14Z`
### last_modified_date : `2023-08-17T05:26:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110429
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
//test.c
#include <altivec.h>
void extract_int_2 (int *p, vector int a) { *p = vec_extract (a, 2); }

On P9 LE, it generates
        xxextractuw 34,34,4
        stxsiwx 34,0,3

The xxextractuw is unnecessary as the extracted int is just at word[1].


---


### compiler : `gcc`
### title : `Fail to CSE for LEN_MASK_STORE`
### open_at : `2023-06-27T09:18:57Z`
### last_modified_date : `2023-06-27T20:14:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110430
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
Consider this following case:

void __attribute__((noinline,noclone))
foo (int *out, int *res)
{
  int mask[] = { 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1 };
  int i;
  for (i = 0; i < 16; ++i)
    {
      if (mask[i])
        out[i] = i;
    }
  int o0 = out[0];
  int o7 = out[7];
  int o14 = out[14];
  int o15 = out[15];
  res[0] = o0;
  res[2] = o7;
  res[4] = o14;
  res[6] = o15;
}

-O3 -march=rv64gcv_zvl512b --param riscv-autovec-preference=fixed-vlmax
Current RVV auto-vectorization codegen:

foo:
        lui     a5,%hi(.LANCHOR0)
        vsetivli        zero,16,e32,m1,ta,ma
        addi    a5,a5,%lo(.LANCHOR0)
        vid.v   v1
        vlm.v   v0,0(a5)
        vsetvli a5,zero,e32,m1,ta,ma
        vse32.v v1,0(a0),v0.t
        lw      a2,0(a0)
        lw      a3,28(a0)
        lw      a4,56(a0)
        lw      a5,60(a0)
        sw      a2,0(a1)
        sw      a3,8(a1)
        sw      a4,16(a1)
        sw      a5,24(a1)
        ret

However, with this patch:
https://patchwork.sourceware.org/project/gcc/patch/20230627064737.16257-1-juzhe.zhong@rivai.ai/

We will end up with better codegen with CSE:

foo:
	lui	a5,%hi(.LANCHOR0)
	vsetivli	zero,16,e32,m1,ta,ma
	addi	a5,a5,%lo(.LANCHOR0)
	vid.v	v1
	vlm.v	v0,0(a5)
	vsetvli	a5,zero,e32,m1,ta,ma
	vse32.v	v1,0(a0),v0.t
	lw	a4,0(a0)
	lw	a5,56(a0)
	sw	a4,0(a1)
	sw	a5,16(a1)
	li	a4,7
	li	a5,15
	sw	a4,8(a1)
	sw	a5,24(a1)
	ret

2 "lw" should be CSE into 2 "li" instructions, gimple IR:

.LEN_MASK_STORE (out_10(D), 32B, 16, { 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1 }, { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 }, 0);
  o0_11 = *out_10(D);
  o14_13 = MEM[(int *)out_10(D) + 56B];
  *res_15(D) = o0_11;
  MEM[(int *)res_15(D) + 8B] = 7;
  MEM[(int *)res_15(D) + 16B] = o14_13;
  MEM[(int *)res_15(D) + 24B] = 15;
  mask ={v} {CLOBBER(eol)};

Since after discussion with Richi, 
this current possible fix patch can only hanlde VLS (fixed-length) vectors,
can not handle VLA (variable-length) vectors.

It's hard for us to create a C code testcase to produce CSE opportunity for
VL vectors.

So, open a BUG for now to make me won't forget such issue.
Will enhance LEN_MASK_STORE in CSE after I finished all RVV auto-vectorization
support.


---


### compiler : `gcc`
### title : `generating all-ones zmm needs dep-breaking pxor before ternlog`
### open_at : `2023-06-27T17:54:46Z`
### last_modified_date : `2023-07-18T03:33:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110438
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
VPTERNLOG is never a dependency-breaking instruction on existing x86 implementations, so generating a vector of all-ones via bare ternlog can stall waiting on destination register. GCC should emit a dependency-breaking PXOR, otherwise it will be a false-dependency-on-popcnt-lzcnt debacle all over again.

#include <immintrin.h>

__m512i g(void)
{
    return (__m512i){ 0 } - 1;
}

g:
        # waits until previous computation
        # of zmm0 has completed
        vpternlogd      zmm0, zmm0, zmm0, 0xFF
        ret


---


### compiler : `gcc`
### title : `Vect: use a small step to calculate the loop induction if the loop is unrolled during loop vectorization`
### open_at : `2023-06-28T09:22:40Z`
### last_modified_date : `2023-07-06T16:05:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110449
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
This is inspired by clang. Compile the follwing case with "-mcpu=neoverse-n2 -O3":

void foo(int *arr, int val, int step) {
  for (int i = 0; i < 1024; i++) {
    arr[i] = val;
    val += step;
  }
}

It will be unrolled by 2 during vectorization. GCC generates code:
	fmov	s29, w2                 # step
	shl	v27.2s, v29.2s, 3       # 8*step
	shl	v28.2s, v29.2s, 2       # 4*step
	...
.L2:
	mov	v30.16b, v31.16b
	add	v31.4s, v31.4s, v27.4s  # += 8*step
	add	v29.4s, v30.4s, v28.4s  # += 4*step
	stp	q30, q29, [x0]
	add	x0, x0, 32
	cmp	x1, x0
	bne	.L2

The v27 (i.e. "8*step") is actually not necessary. We can use v29 + v28 (i.e. "+ 4*step") and generate simpler code:
	fmov	s29, w2                 # step
	shl	v28.2s, v29.2s, 2       # 4*step
	...
.L2:
	add	v29.4s, v30.4s, v28.4s  # += 4*step
	stp	q30, q29, [x0]
	add	x0, x0, 32
	add	v30.4s, v29.4s, v28.4s  # += 4*step
	cmp	x1, x0
	bne	.L2

This has two benefits:
(1) Save 1 vector register and one "mov" instructon
(2) For floating point, the result value of small step should be closer to the original scalar result value than large step. I.e. "A + 4*step + ... + 4*step" should be closer to "A + step + ... + step" than "A + 8*step + ... 8*step".

Do you think if this is reasonable? 

I have a simple patch to enhance the tree-vect-loop.cc "vectorizable_induction()" to achieve this. Will send out the patch for code review later.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O2 since r14-261-g0ef3756adf0`
### open_at : `2023-06-28T10:21:15Z`
### last_modified_date : `2023-07-01T20:51:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110450
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/KqcYxfevj

Given the following code:

void foo(void);
static int a = 1;
static int *b = &a, *c = &a;
static short d, e;
static char f = 11;
static char(g)(char h, int i) { return h << i; }
int main() {
    if (f) *c = g(0 >= a, 3);
    e = *c;
    d = e % f;
    if (d) {
        __builtin_unreachable();
    } else if (*b)
        foo();
    ;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	movl	a(%rip), %eax
	xorl	%edx, %edx
	testl	%eax, %eax
	setle	%dl
	leal	0(,%rdx,8), %edx
	movl	%edx, a(%rip)
	jle	.L8
	xorl	%eax, %eax
	ret
.L8:
	pushq	%rax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	ret

gcc-13.1.0 -O2 eliminates the call to foo:

main:
	movl	a(%rip), %edx
	xorl	%eax, %eax
	testl	%edx, %edx
	setle	%al
	leal	0(,%rax,8), %eax
	movl	%eax, a(%rip)
	xorl	%eax, %eax
	ret

Bisects to r14-261-g0ef3756adf0


---


### compiler : `gcc`
### title : `LIM fails to hoist comparisons`
### open_at : `2023-06-28T11:00:52Z`
### last_modified_date : `2023-06-28T13:09:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110451
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
For SPEC bwaves or gfortran.dg/pr81303.f we interchange the loop nest which
produces a conditional in the innermost loop like the following to handle
a reduction with invariant initialization:

  <bb 14> [local count: 955630226]:
  # l_93 = PHI <l_94(21), 1(13)>
  # ivtmp_58 = PHI <ivtmp_84(21), _85(13)>
  _30 = (integer(kind=8)) l_93;
  _31 = _29 + _30;
  y__I_lsm.120_22 = (*y_139(D))[_31];
  _23 = m_95 != 1;
  y__I_lsm.120_33 = _23 ? y__I_lsm.120_22 : 0.0;
  _42 = _30 + _41;
  _43 = (*a_141(D))[_42];

currently the LIM cost modeling finds it not profitable to hoist the
invariant m_95 != 1 compare.  This causes vectorization to choose
a larger vectorization factor to accomodate vectorizing this compare.


---


### compiler : `gcc`
### title : `Bad vectorization of invariant masks`
### open_at : `2023-06-28T11:11:46Z`
### last_modified_date : `2023-06-29T08:35:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110452
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
When we have loop like

double a[1024], b[1024], c[1024];

void foo (int flag, int n)
{
  _Bool x = flag == 3;
  for (int i = 0; i < n; ++i)
    a[i] = (x ? b[i] : c[i]) * 42.;
}

and build it with -O2 -ftree-vectorize -march=znver4 (to avoid unswitching)
we get

  _55 = _2 ? -1 : 0;
  vect_cst__56 = {_55, _55, _55, _55, _55, _55, _55, _55};

  <bb 3> [local count: 567644343]:
  # i_14 = PHI <i_11(9), 0(21)>
  # vectp_b.10_49 = PHI <vectp_b.10_50(9), &b(21)>
  # vectp_c.13_52 = PHI <vectp_c.13_53(9), &c(21)>
  # vectp_a.18_62 = PHI <vectp_a.18_63(9), &a(21)>
  # ivtmp_65 = PHI <ivtmp_66(9), 0(21)>
  vect_iftmp.12_51 = MEM <vector(8) double> [(double *)vectp_b.10_49];
  iftmp.0_9 = b[i_14];
  vect_iftmp.15_54 = MEM <vector(8) double> [(double *)vectp_c.13_52];
  iftmp.0_8 = c[i_14];
  vect_patt_13.16_59 = VEC_COND_EXPR <vect_cst__56, vect_iftmp.12_51, vect_iftmp.15_54>;
  iftmp.0_3 = _2 ? iftmp.0_9 : iftmp.0_8;

so the invariant but not constant condition _2 on the COND_EXPR is vectorized as

  _55 = _2 ? -1 : 0;
  vect_cst__56 = {_55, _55, _55, _55, _55, _55, _55, _55};

unfortunately that leads to very bad generated code

        cmpl    $3, %edi
        sete    %cl
        movl    %ecx, %esi
        leal    (%rsi,%rsi), %eax
        leal    0(,%rsi,4), %r9d
        leal    0(,%rsi,8), %r8d
        orl     %esi, %eax
        orl     %r9d, %eax
        movl    %ecx, %r9d
        orl     %r8d, %eax
        movl    %ecx, %r8d
        sall    $4, %r9d
        sall    $5, %r8d
        sall    $6, %esi
        orl     %r9d, %eax
        orl     %r8d, %eax
        movl    %ecx, %r8d
        orl     %esi, %eax
        sall    $7, %r8d
        orl     %r8d, %eax
        kmovb   %eax, %k1


---


### compiler : `gcc`
### title : `Unnecessary movsx   eax, dil`
### open_at : `2023-06-28T15:05:52Z`
### last_modified_date : `2023-06-29T12:10:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110457
### status : `WAITING`
### tags : `ABI, missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
For the following code

int sample1(char c) {
  return (c << 4) + (c << 8) + (c << 16) + c;
}


GCC-14 with -O2 generates the assembly:

sample1(char):
 movsx  eax,dil
 imul   eax,eax,0x10111
 ret


However, it could be shortened to just:

sample1(char):
 imul   eax,edi,0x10111


Godbolt playground: https://godbolt.org/z/7GGdedEY8


---


### compiler : `gcc`
### title : `Trivial on stack variable was not optimized away`
### open_at : `2023-06-28T15:12:54Z`
### last_modified_date : `2023-07-21T22:59:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110459
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
Consider the example:

struct array {
    char data[4];
};

auto sample2(char c) {
  array buffer = {c, 0, 0, 0};
  return buffer;
}


With GCC-14 and -O2 it produces the following assembly:

sample2(char):
        xor     eax, eax
        mov     BYTE PTR [rsp-22], 0
        mov     WORD PTR [rsp-24], ax
        mov     eax, DWORD PTR [rsp-24]
        sal     eax, 8
        mov     al, dil
        ret


It could be further optimized to just:

sample2(char):
        movzx   eax, dil
        ret


Godbolt playground: https://godbolt.org/z/nxKhvo3ns


---


### compiler : `gcc`
### title : `60% slowdown with fwrapv when using openmp`
### open_at : `2023-06-29T01:38:44Z`
### last_modified_date : `2023-10-01T03:31:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110472
### status : `UNCONFIRMED`
### tags : `missed-optimization, openmp`
### component : `middle-end`
### version : `10.2.1`
### severity : `normal`
### contents :
Created attachment 55423
Reproduction file

Compiling the attached example with -fwrapv inhibits some optimizations and  results in a massive slowdown. It appears to be related to the use of OpenMP. I know that fwrapv can result in slowdowns; however, I do not believe that it needs to in this example.

In the loop nest below, gcc appears to believe the computations with the loop induction variables (ex. 'i2 = i * 21504 + i1 * 96;') will overflow. However, the code is looping over fixed size data and so I believe gcc should be able to determine that overflow is not possible. Perhaps some range analysis stops working across the openmp runtime boundary? The issue is fixed if I remove the openmp pragma.

The issue is also fixed if I change the loop induction variables to be declared as int64_t rather than int.

I also tried clang16 and did not observe the issue with fwrapv.

#pragma omp parallel for \
 num_threads(omp_get_max_threads()) \
 private(i1,u0,b_u0,i2,i3,i4,i5,i6,i7,i8,i10,i12,i14,i15) \
 firstprivate(r2)

  for (i = 0; i < 7; i++) {

    for (i1 = 0; i1 < 7; i1++) {
      u0 = i * -32 + 222;
      if (u0 > 32) {
        u0 = 32;
      }

      b_u0 = i1 * -32 + 222;
      if (b_u0 > 32) {
        b_u0 = 32;
      }

      i2 = i * 21504 + i1 * 96;
      i3 = i * 227328 + (i1 << 10);
      for (i4 = 0; i4 < u0; i4++) {
        for (i5 = 0; i5 < b_u0; i5++) {
          for (i6 = 0; i6 < 3; i6++) {
            i7 = i5 + i6;
            for (i8 = 0; i8 < 3; i8++) {
              for (i10 = 0; i10 < 3; i10++) {
                i12 = i4 + i10;
                for (i14 = 0; i14 < 32; i14++) {
                  i15 = ((i3 + 7104 * i4) + (i5 << 5)) + i14;
                  (r2)[i15] += ((float *)&in_0[0][0][0][0])[((i2 + 672 * i12) + 3 *
                    i7) + i8] * ((float *)&__constant_3x3x3x32xf32[0][0][0][0])
                    [((288 * i10 + 96 * i6) + (i8 << 5)) + i14];
                }
              }
            }
          }
        }
      }
    }
  }


Repro:
gcc -O3 -fopenmp -lpthread -fwrapv predict.i
./a.out

(Remove the -fwrapv to observe a major speedup)



Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/10/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none:amdgcn-amdhsa:hsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Debian 10.2.1-6' --with-bugurl=file:///usr/share/doc/gcc-10/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-10 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-10-Km9U7s/gcc-10-10.2.1/debian/tmp-nvptx/usr,amdgcn-amdhsa=/build/gcc-10-Km9U7s/gcc-10-10.2.1/debian/tmp-gcn/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 10.2.1 20210110 (Debian 10.2.1-6) 
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-fopenmp' '-fwrapv' '-mtune=generic' '-march=x86-64' '-pthread'
 /usr/lib/gcc/x86_64-linux-gnu/10/cc1 -E -quiet -v -imultiarch x86_64-linux-gnu -D_REENTRANT main.c -mtune=generic -march=x86-64 -fopenmp -fwrapv -O3 -fpch-preprocess -fasynchronous-unwind-tables -o main.i
ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/10/include-fixed"
ignoring nonexistent directory "/usr/lib/gcc/x86_64-linux-gnu/10/../../../../x86_64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/lib/gcc/x86_64-linux-gnu/10/include
 /usr/local/include
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-fopenmp' '-fwrapv' '-mtune=generic' '-march=x86-64' '-pthread'
 /usr/lib/gcc/x86_64-linux-gnu/10/cc1 -fpreprocessed main.i -quiet -dumpbase main.c -mtune=generic -march=x86-64 -auxbase main -O3 -version -fopenmp -fwrapv -fasynchronous-unwind-tables -o main.s
GNU C17 (Debian 10.2.1-6) version 10.2.1 20210110 (x86_64-linux-gnu)
	compiled by GNU C version 10.2.1 20210110, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.0, isl version isl-0.23-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
GNU C17 (Debian 10.2.1-6) version 10.2.1 20210110 (x86_64-linux-gnu)
	compiled by GNU C version 10.2.1 20210110, GMP version 6.2.1, MPFR version 4.1.0, MPC version 1.2.0, isl version isl-0.23-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 1f803793fa2e3418c492b25e7d3eac2f
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-fopenmp' '-fwrapv' '-mtune=generic' '-march=x86-64' '-pthread'
 as -v --64 -o main.o main.s
GNU assembler version 2.35.2 (x86_64-linux-gnu) using BFD version (GNU Binutils for Debian) 2.35.2
COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/10/:/usr/lib/gcc/x86_64-linux-gnu/10/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/10/:/usr/lib/gcc/x86_64-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/10/:/usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/10/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/10/../../../:/lib/:/usr/lib/
Reading specs from /usr/lib/gcc/x86_64-linux-gnu/10/libgomp.spec
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-fopenmp' '-fwrapv' '-mtune=generic' '-march=x86-64' '-pthread'
 /usr/lib/gcc/x86_64-linux-gnu/10/collect2 -plugin /usr/lib/gcc/x86_64-linux-gnu/10/liblto_plugin.so -plugin-opt=/usr/lib/gcc/x86_64-linux-gnu/10/lto-wrapper -plugin-opt=-fresolution=main.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lpthread -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu --as-needed -dynamic-linker /lib64/ld-linux-x86-64.so.2 -pie /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/Scrt1.o /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/10/crtbeginS.o /usr/lib/gcc/x86_64-linux-gnu/10/crtoffloadbegin.o -L/usr/lib/gcc/x86_64-linux-gnu/10 -L/usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/10/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/10/../../.. -lpthread main.o -lgomp -lgcc --push-state --as-needed -lgcc_s --pop-state -lpthread -lc -lgcc --push-state --as-needed -lgcc_s --pop-state /usr/lib/gcc/x86_64-linux-gnu/10/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/crtn.o /usr/lib/gcc/x86_64-linux-gnu/10/crtoffloadend.o
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-fopenmp' '-fwrapv' '-mtune=generic' '-march=x86-64' '-pthread'


---


### compiler : `gcc`
### title : `vec_convert for aarch64 seems to lower to something which should be improved`
### open_at : `2023-06-29T04:15:40Z`
### last_modified_date : `2023-07-24T19:00:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110473
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
typedef unsigned int v4si __attribute__ ((vector_size (4*sizeof(int))));
typedef unsigned short v4hi __attribute__ ((vector_size (4*sizeof(short))));

v4si f(v4si a, v4si b)
{
  v4hi t = __builtin_convertvector (a, v4hi);
  v4si t1 = __builtin_convertvector (t, v4si);
  return t1;
}
```

This gets lowered in veclower21 to
```
  _6 = BIT_FIELD_REF <_5, 64, 0>;
  t_2 = _6;
  _7 = BIT_FIELD_REF <t_2, 16, 0>;
  _8 = (unsigned int) _7;
  _9 = BIT_FIELD_REF <t_2, 16, 16>;
  _10 = (unsigned int) _9;
  _11 = BIT_FIELD_REF <t_2, 16, 32>;
  _12 = (unsigned int) _11;
  _13 = BIT_FIELD_REF <t_2, 16, 48>;
  _14 = (unsigned int) _13;
  t1_3 = {_8, _10, _12, _14};
```

And then forwprop optimizes this to:
```
  _6 = BIT_FIELD_REF <_5, 64, 0>;
  t1_3 = (v4si) _6;
```

And then combine comes along and optimizes that to:
(insn 9 8 11 2 (set (reg:V8HI 98)
        (vec_concat:V8HI (truncate:V4HI (reg:V4SI 102))
            (const_vector:V4HI [
                    (const_int 0 [0]) repeated x4
                ]))) "/app/example.cpp":7:8 5467 {truncv4siv4hi2_vec_concatz_le}
     (expr_list:REG_DEAD (reg:V4SI 102)
        (nil)))
(note 11 9 16 2 NOTE_INSN_DELETED)
(insn 16 11 17 2 (set (reg/i:V4SI 32 v0)
        (sign_extend:V4SI (subreg:V4HI (reg:V8HI 98) 0))) "/app/example.cpp":10:1 5459 {extendv4hiv4si2}
     (expr_list:REG_DEAD (reg:V8HI 98)
        (nil)))

But the first one is basically just (truncate:V4HI (reg:V4SI 102)) (due to the way the instruction works, the top parts is also zeros).
So we get in the end:
        xtn     v0.4h, v0.4s
        sxtl    v0.4s, v0.4h

Why couldn't vectlower could just do:
```
_6 = (v4hi)a_1(D);
t1_3 = (v4si) _6;
```
In the first place instead of depending on later optimizations (at least handle the second part)?

note with the above lowering we might hit an issue in match.pd where it is trying to turn that into t1_3 = t1_3 = {0xFFFF,0xFFFF,0xFFFF,0xFFFF} & _6;
(both because of TYPE_PRECISION and because it just uses wide_int_to_tree ...


---


### compiler : `gcc`
### title : `Unnecessary register move`
### open_at : `2023-06-29T08:03:18Z`
### last_modified_date : `2023-06-29T15:21:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110479
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
May be related to / a dup of PR110240.

The function

unsigned int bar(unsigned int a)
{
  return 1u << (((a >> 10) & 3) + 3);
}

is compiled, with a relatively recent trunk and -O3, to

bar:
.LFB12:
        .cfi_startproc
        shrl    $10, %edi
        movl    $1, %eax
        movl    %edi, %ecx
        andl    $3, %ecx
        addl    $3, %ecx
        sall    %cl, %eax
        ret

where the register move seems unnecessary.


---


### compiler : `gcc`
### title : `Possible improvements in dense switch statement returning values`
### open_at : `2023-06-29T08:18:35Z`
### last_modified_date : `2023-06-29T15:39:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110481
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Putting this provisionally into tree-optimization, although there may
be other aspects.

Consider

unsigned int foo(unsigned int a)
{
  switch ((a >> 10) & 3)
    {
    case 0:
      return 8;
    case 1:
      return 16;
    case 2:
      return 32;
    case 3:
      return 64;
    }
}

unsigned int bar(unsigned int a)
{
  return 1u << (((a >> 10) & 3) + 3);
}

unsigned int baz (unsigned int a)
{
  switch (a & (3 << 10))
    {
    case 0:
      return 8;
    case 1 << 10:
      return 16;
    case 2 << 10:
      return 32;
    case 3 << 10:
      return 64;
    }
}

which all do the same thing.

The code for bar is

bar:
.LFB1:
        .cfi_startproc
        shrl    $10, %edi
        movl    $1, %eax
        movl    %edi, %ecx
        andl    $3, %ecx
        addl    $3, %ecx
        sall    %cl, %eax
        ret

which is optimum except for the register move (submitted as PR110479).

The compiler does not recognize that foo or baz are equivalent to bar,
but that may be too much of a special case to really consider. 

The code for foo is

foo:
.LFB0:
        .cfi_startproc
        shrl    $10, %edi
        movl    $8, %eax
        andl    $3, %edi
        decl    %edi
        cmpl    $2, %edi
        ja      .L1
        movzbl  CSWTCH.1(%rdi), %eax
.L1:
        ret
        .cfi_endproc

[...]

CSWTCH.1:
        .byte   16
        .byte   32
        .byte   64

where it seems strange that there is a comparison and conditional
jump around the load.  A look at *.optimized shows

<bb 2> [local count: 1073741824]:
  _1 = a_4(D) >> 10;
  _2 = _1 & 3;
  _8 = _2 + 4294967295;
  if (_8 <= 2)
    goto <bb 3>; [50.00%]
  else
    goto <bb 4>; [50.00%]

  <bb 3> [local count: 536870913]:
  _6 = CSWTCH.1[_8];
  _5 = (unsigned int) _6;

  <bb 4> [local count: 1073741824]:
  # _3 = PHI <_5(3), 8(2)>
  return _3;

which assigns a probability of 50% to (a>>10)& 3 being zero.
Where this comes from is unclear to me.  A straightforward load
from a table which also includes the 8 seems more logical to me
(especially with -Os).

Finally, baz generates

baz:
.LFB2:
        .cfi_startproc
        andl    $3072, %edi
        movl    $32, %eax
        cmpl    $2048, %edi
        je      .L6
        ja      .L8
        movl    $8, %eax
        testl   %edi, %edi
        je      .L6
        movl    $16, %eax
        ret
.L8:
        movl    $64, %eax
.L6:
        ret

when transforming into something equivalent to foo (or even bar)
would seem advantageous.


---


### compiler : `gcc`
### title : `tree-ssa/clz-* and tree-ssa/ctz-* fail on s390x with arch14 (expression_expensive_p not handling if there is wider clz/ctz/popcount optab)`
### open_at : `2023-06-29T20:25:42Z`
### last_modified_date : `2023-06-29T22:29:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110490
### status : `NEW`
### tags : `missed-optimization, testsuite-fail`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
(Not sure what component is most appropriate for this.)

The following FAILs can be seen in
<https://gcc.gnu.org/pipermail/gcc-testresults/2023-June/786039.html> (s390x-ibm-linux-gnu arch14)
but not in
<https://gcc.gnu.org/pipermail/gcc-testresults/2023-June/786004.html> (s390x-ibm-linux-gnu default)

It would be great to fix them.

FAIL: gcc.dg/tree-ssa/clz-char.c scan-tree-dump-times optimized "__builtin_clz|\\\\.CLZ" 1
FAIL: gcc.dg/tree-ssa/clz-complement-char.c scan-tree-dump-times optimized "__builtin_clz|\\\\.CLZ" 1
FAIL: gcc.dg/tree-ssa/clz-complement-int.c scan-tree-dump-times optimized "__builtin_clz|\\\\.CLZ" 1
FAIL: gcc.dg/tree-ssa/clz-complement-long.c scan-tree-dump-times optimized "__builtin_clz|\\\\.CLZ" 1
FAIL: gcc.dg/tree-ssa/clz-int.c scan-tree-dump-times optimized "__builtin_clz|\\\\.CLZ" 1
FAIL: gcc.dg/tree-ssa/clz-long.c scan-tree-dump-times optimized "__builtin_clz|\\\\.CLZ" 1
FAIL: gcc.dg/tree-ssa/ctz-char.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1
FAIL: gcc.dg/tree-ssa/ctz-complement-char.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1
FAIL: gcc.dg/tree-ssa/ctz-complement-int.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1
FAIL: gcc.dg/tree-ssa/ctz-complement-long-long.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1
FAIL: gcc.dg/tree-ssa/ctz-complement-long.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1
FAIL: gcc.dg/tree-ssa/ctz-int.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1
FAIL: gcc.dg/tree-ssa/ctz-long-long.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1
FAIL: gcc.dg/tree-ssa/ctz-long.c scan-tree-dump-times optimized "__builtin_ctz|\\\\.CTZ" 1


---


### compiler : `gcc`
### title : `Attempted optimization of switch statement pessimizes it instead`
### open_at : `2023-06-30T02:52:22Z`
### last_modified_date : `2023-06-30T07:16:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110492
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.1.0`
### severity : `normal`
### contents :
This happens on my local GCC 11.3, but you can also see it on 13.1 at this godbolt link https://godbolt.org/z/G3qecWxPr

I'm creating a switch statement of hashed strings, which compiles to a binary search on the hashes, all well and good.  However, with -O3 specified, GCC peels back the last multiplication of the hash for some of the comparison branches, but is unable to do it for others, resulting in longer assembly with twice as many comparisons as is necessary.  Here is lines 15..19 in get_choice_1()

    (end of the hash loop)
        imul    esi, eax, 16777619
        test    dl, dl
        jne     .L3
    (start of the switch)
        cmp     eax, 1954414351
        je      .L8
        cmp     esi, 1901626525
        ja      .L4

eax is the hash without the last imul, and esi is the final hash.  If we prevent inlining of the hash function, the compiler can't make this "optimization" and gives the assembly I expect.  Here is lines 90..93 in get_choice_2()

        call    hash32_noinline(char const*)
        cmp     eax, 1901626525
        je      .L33
        jbe     .L46

Now it only does one comparison per entry and uses it for both the == and <= branches.

This isn't that important to my program but I thought you'd like to know.


---


### compiler : `gcc`
### title : `Spurious warnings stringop-overflow and array-bounds copying data as bytes into vector::reserve`
### open_at : `2023-06-30T12:21:11Z`
### last_modified_date : `2023-06-30T12:41:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110498
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `normal`
### contents :
Created attachment 55432
This is the preprocessed file (*.i*) that triggers the warning reported here.

The warning issue happens when using GCC v13.1.0 on Red Hat 7.9 3.10.0-1160.90.1.el7.x86_64.

The compiler configuration settings are:

/grid/common/test/gcc-v13.1.0d1rh74_lnx86/bin/gcc -v
Using built-in specs.
COLLECT_GCC=/grid/common/test/gcc-v13.1.0d1rh74_lnx86/bin/gcc
COLLECT_LTO_WRAPPER=/grid/common/test/gcc-v13.1.0d1rh74_lnx86/libexec/gcc/x86_64-redhat-linux/13.1.0/lto-wrapper
Target: x86_64-redhat-linux
Configured with: /tmp/gcc-v13.1.0d1rh74_lnx86/gcc.source/configure --prefix=/grid/common/test/gcc-v13.1.0d1rh74_lnx86 --with-pkgversion=Cadence --disable-libgcj --enable-threads=posix --enable-shared --with-system-zlib --enable-checking=release --enable-__cxa_atexit --disable-libunwind-exceptions --enable-languages=c,c++,fortran --disable-nls --enable-gnu-unique-object --enable-bootstrap --enable-plugin --enable-linker-build-id --enable-gnu-indirect-function --enable-install-libiberty --with-tune=generic --enable-initfini-array --enable-multiarch --with-linker-hash-style=gnu --with-ld=/grid/common/test/gcc-v13.1.0d1rh74_lnx86/bin/ld --with-as=/grid/common/test/gcc-v13.1.0d1rh74_lnx86/bin/as --build=x86_64-redhat-linux --host=x86_64-redhat-linux
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 13.1.0 (Cadence)

=========================================================

The command line to reproduce the warning and the compiler output:

/opt/gcc-v13.1.0d1rh74_lnx86/bin/gcc -c -fpic  -std=c++2b -pthread -D_GLIBCXX_USE_CXX11_ABI=1 -Wstrict-aliasing=3 -O3 -Wall -Werror -DNDEBUG -Wstrict-aliasing=3  -Wextra   -DET7 -DETX=7 -DCMG_QT_VERSION='"MAIN_WXE_23.05.539.d000-Z2, VEngineering"'  -DLINUX2    -I/opt/ua/3party/capnproto -I/opt/ua/Framework -I. -I.. -I/opt/ap/include -I/grid/cva/p4/eugene/gcc_wxe/ua/include -I/opt/ua/3party/xlm_inst/include  -MMD ../radbSerializer.C

In file included from /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/string:51,
                 from /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/stdexcept:39,
                 from ../radbSerializer.C:9:
In static member function 'static constexpr _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = long unsigned int; _Up = long unsigned int; bool _IsMove = false]',
    inlined from 'constexpr _OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:506:3 ,
    inlined from 'constexpr _OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:533:4 ,
    inlined from 'constexpr _OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:540:31,    inlined from 'constexpr _OI std::copy(_II, _II, _OI) [with _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:633:7,
    inlined from 'constexpr std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::_M_copy_aligned(const_iterator, const_iterator, iterator) [with _Alloc = std::allocator<bool>]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_bvector.h:1303:28,
    inlined from 'constexpr void std::vector<bool, _Alloc>::_M_reallocate(size_type) [with _Alloc = std::allocator<bool>]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/vector.tcc:851:40,
    inlined from 'constexpr void std::vector<bool, _Alloc>::reserve(size_type) [with _Alloc = std::allocator<bool>]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_bvector.h:1091:17,
    inlined from 'radb::ScopesSerializer::ScopesSerializer()' at ../radbSerializer.C:25:24:
/opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:437:30: error: 'void* __builtin_memmove(void*, const void*, long unsigned int)' forming offset 8 is out of the bounds [0, 8] [-Werror=array-bounds=]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In static member function 'static constexpr _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = long unsigned int; _Up = long unsigned int; bool _IsMove = false]',
    inlined from 'constexpr _OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:506:3 ,
    inlined from 'constexpr _OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:533:4 ,
    inlined from 'constexpr _OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:540:31,    inlined from 'constexpr _OI std::copy(_II, _II, _OI) [with _II = long unsigned int*; _OI = long unsigned int*]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:633:7,
    inlined from 'constexpr std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::_M_copy_aligned(const_iterator, const_iterator, iterator) [with _Alloc = std::allocator<bool>]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_bvector.h:1303:28,
    inlined from 'constexpr void std::vector<bool, _Alloc>::_M_reallocate(size_type) [with _Alloc = std::allocator<bool>]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/vector.tcc:851:40,
    inlined from 'constexpr void std::vector<bool, _Alloc>::reserve(size_type) [with _Alloc = std::allocator<bool>]' at /opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_bvector.h:1091:17,
    inlined from 'radb::VarsSerializer::VarsSerializer()' at ../radbSerializer.C:93:27:
/opt/gcc-v13.1.0d1rh74_lnx86/include/c++/13.1.0/bits/stl_algobase.h:437:30: error: 'void* __builtin_memmove(void*, const void*, long unsigned int)' forming offset 8 is out of the bounds [0, 8] [-Werror=array-bounds=]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cc1plus: all warnings being treated as errors

=========================================================

The the preprocessed file "radbSerializer.ii" is attached. This might means that this issue was still not totally fixed on GCC v13.1.0

Regards,
Rogerio


---


### compiler : `gcc`
### title : `malloc branch predictor is broken`
### open_at : `2023-06-30T13:53:01Z`
### last_modified_date : `2023-07-01T20:53:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110499
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `normal`
### contents :
Malloc branch predictor currently predicts that malloc() call likely returns 1. This is good for NULL pointer checks, but not good for checking pointers for equality:

#include <malloc.h>
void
test()
{
         if (malloc(10) == malloc(20))
                 printf ("Impossible!\n");
}

gets predicted as:

void test ()
{
  void * _1;
  void * _2;

  <bb 2> [local count: 1073741824]:
  _1 = malloc (10);
  _2 = malloc (20);
  if (_1 == _2)
    goto <bb 3>; [99.96%]
  else
    goto <bb 4>; [0.04%]

  <bb 3> [local count: 1073312329]:
  __builtin_puts (&"Impossible!"[0]);

  <bb 4> [local count: 1073741824]:
  return;

}

So we think that Impossible is output with 99.96 probability.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -Os since r14-1656-g55fcaa9a8bd`
### open_at : `2023-06-30T16:27:33Z`
### last_modified_date : `2023-09-16T06:21:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110502
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/T9rv1nsGr

Given the following code:

void foo(void);
static char e = -3L, k;
static int f;
static int *g;
static signed char *h = &e;
static signed char **i = &h;
static unsigned j;
static char(a)(char b, char c) { return b + c; }
static int(d)(int b, int c) { return b && b < -c ? b : b + c; }
static char l(char *n, unsigned short o) {
    char p;
    int q, s = 4104765887;
    int *r = &f;
    if (!(((o) >= 65533) && ((o) <= 65533))) {
        __builtin_unreachable();
    }
    for (; s <= 4; s = a(s, 5)) {
        int **t = &g;
        *t = &s;
        j++;
        p = *n;
        q = d(*g, p);
        *r = q;
    }
    if (g || f)
        ;
    else
        foo();
    return 0;
}
int main() {
    l(&e, e);
    char m = &k == (*i = &m);
}

gcc-trunk -Os does not eliminate the call to foo:

main:
	pushq	%rbx
	movl	$-190201409, %edx
	xorl	%esi, %esi
	subq	$16, %rsp
	movl	j(%rip), %ecx
	movl	$-190201409, 12(%rsp)
	leal	15(%rcx), %r8d
.L2:
	leal	1(%rcx), %edi
	cmpl	%r8d, %edi
	je	.L8
	testl	%edx, %edx
	movb	$1, %sil
	sete	%al
	cmpl	$2, %edx
	setg	%cl
	orl	%ecx, %eax
	movl	%edi, %ecx
	movzbl	%al, %eax
	negl	%eax
	andl	$-3, %eax
	addl	%edx, %eax
	addl	$5, %edx
	movsbl	%dl, %edx
	jmp	.L2
.L8:
	testb	%sil, %sil
	leaq	12(%rsp), %rbx
	je	.L4
	movq	%rbx, g(%rip)
	movl	%ecx, j(%rip)
	movl	%eax, f(%rip)
	jmp	.L5
.L4:
	cmpq	$0, g(%rip)
	jne	.L5
	cmpl	$0, f(%rip)
	jne	.L5
	call	foo
.L5:
	movq	%rbx, h(%rip)
	addq	$16, %rsp
	xorl	%eax, %eax
	popq	%rbx
	ret

gcc-13.1.0 -Os eliminates the call to foo:

main:
	movl	j(%rip), %edx
	movl	$-190201409, %eax
	leal	14(%rdx), %esi
.L4:
	testl	%eax, %eax
	je	.L6
	movl	%eax, %ecx
	cmpl	$2, %eax
	jle	.L2
.L6:
	leal	-3(%rax), %ecx
.L2:
	addl	$5, %eax
	incl	%edx
	movsbl	%al, %eax
	cmpl	%esi, %edx
	jne	.L4
	leaq	-4(%rsp), %rax
	movl	%ecx, f(%rip)
	movq	%rax, g(%rip)
	movq	%rax, h(%rip)
	xorl	%eax, %eax
	movl	%edx, j(%rip)
	ret

Bisects to r14-1656-g55fcaa9a8bd


---


### compiler : `gcc`
### title : `[13/14 Regression] Dead Code Elimination Regression at -O3 since  r13-322-g7f04b0d786e`
### open_at : `2023-06-30T16:34:39Z`
### last_modified_date : `2023-08-26T00:13:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110503
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/h3G76EEW5

Given the following code:

void foo(void);
static int f = 10, g, h;
static int *i = &g, *j = &f, *k = &h;
static char(a)(int b) {
    if (!(((b) >= 1) && ((b) <= 3))) {
        __builtin_unreachable();
    }
    return 0;
}
static char(c)(char d, char e) { return d % e; }
static void l() {
    char m, n;
    unsigned o;
    int *p = &g;
    o = -19;
    for (; o > 22; o = o + 8) {
        int *q = &g;
        g = 30;
        unsigned char d = 60, e = o;
        m = d / e;
        if ((0 || *q) & m == 0)
            ;
        else {
            foo();
            n = c(0 != p, *p);
            a(n);
            *q = 0;
        }
    }
    *i = a(3);
    *k = 0 != *j;
}
int main() {
    l();
    a(h);
}

gcc-trunk -O3 does not eliminate the call to foo:

main:
	pushq	%rbx
	movl	$-19, %ebx
	jmp	.L5
	.p2align 4,,10
	.p2align 3
.L8:
	call	foo
	addl	$8, %ebx
.L5:
	movl	$30, g(%rip)
	cmpb	$60, %bl
	jbe	.L8
	addl	$8, %ebx
	cmpl	$5, %ebx
	jne	.L5
	movl	f(%rip), %edx
	xorl	%eax, %eax
	popq	%rbx
	movl	$0, g(%rip)
	testl	%edx, %edx
	setne	%al
	movl	%eax, h(%rip)
	xorl	%eax, %eax
	ret

gcc-12.3.0 -O3 eliminates the call to foo:

main:
	movl	f(%rip), %edx
	xorl	%eax, %eax
	movl	$0, g(%rip)
	testl	%edx, %edx
	setne	%al
	movl	%eax, h(%rip)
	xorl	%eax, %eax
	ret

Bisects to r13-322-g7f04b0d786e


---


### compiler : `gcc`
### title : `Optimize `for` loop that only assigns to a local variable`
### open_at : `2023-07-02T09:28:00Z`
### last_modified_date : `2023-07-02T20:50:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110519
### status : `NEW`
### tags : `missed-optimization`
### component : `c`
### version : `12.0`
### severity : `enhancement`
### contents :
~~~c
struct symbol {
    struct symbol *next;
};

void f(const struct symbol *sym)
{
    for (const struct symbol *s = sym; s != (void *)0; s = s->next)
        do { } while (0);
}
~~~

The above code was extracted from <https://github.com/NetBSD/src/blob/f74666e8d086952b27d76155223a5d497e69f5a8/usr.bin/xlint/lint1/decl.c#L1319>.

Several other compilers know that this loop has no side effects, even at low optimization levels, see <https://godbolt.org/z/Y85EGcW3d>.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since  r14-368-ge1366a7e4ce`
### open_at : `2023-07-04T09:20:49Z`
### last_modified_date : `2023-08-09T07:29:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110538
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/53Ys78do1

Given the following code:

void foo(void);
static int b, d, f;
static int *c, *e = &d;
static unsigned g;
void a();
int main() {
    g = -19;
    for (; g; ++g) {
        int h = g;
        int *i = &b;
        int **j = &i;
        if (d) {
            int **k = &i;
            j = &c;
            *k = &f;
        } else
            *e = 0;
        if (!(((h) >= -19) && ((h) <= -1))) {
            __builtin_unreachable();
        }
        if (i)
            ;
        else
            a();
        if (j == &i || j == &c)
            ;
        else
            foo();
        ;
    }
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$24, %rsp
	movl	$-19, g(%rip)
	.p2align 4,,10
	.p2align 3
.L4:
	movl	d(%rip), %edx
	movl	$c, %eax
	testl	%edx, %edx
	jne	.L2
	leaq	8(%rsp), %rax
.L2:
	cmpq	$c, %rax
	je	.L5
	leaq	8(%rsp), %rdx
	cmpq	%rdx, %rax
	je	.L5
	call	foo
.L5:
	addl	$1, g(%rip)
	jne	.L4
	xorl	%eax, %eax
	addq	$24, %rsp
	ret

gcc-13.1.0 -O2 eliminates the call to foo:

main:
	movl	$-19, g(%rip)
	.p2align 4,,10
	.p2align 3
.L2:
	movl	g(%rip), %eax
	addl	$1, %eax
	movl	%eax, g(%rip)
	jne	.L2
	ret

Bisects to r14-368-ge1366a7e4ce


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at since r14-338-g1dd154f6407`
### open_at : `2023-07-04T09:38:04Z`
### last_modified_date : `2023-07-13T14:55:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110539
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/h9vrrj5je

Given the following code:

void foo(void);
static int a, c = 1;
static short b;
static int *d = &c, *e = &a;
static int **f = &d;
void __assert_fail() __attribute__((__noreturn__));
static void g(short h) {
    if (*d)
        ;
    else {
        if (e) __assert_fail();
        if (a) {
            __builtin_unreachable();
        } else
            __assert_fail();
    }
    if ((((0, 0) || h) == h) + b) *f = 0;
}
int main() {
    int i = 0 != 10 & a;
    g(i);
    *e = 9;
    e = 0;
    if (d == 0)
        ;
    else
        foo();
    ;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	movq	d(%rip), %rax
	movq	e(%rip), %rdx
	movl	(%rax), %edi
	testl	%edi, %edi
	jne	.L2
	xorl	%eax, %eax
	call	__assert_fail
.L2:
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	movq	%rcx, d(%rip)
	cmpq	$0, d(%rip)
	movl	$9, (%rdx)
	movq	%rsi, e(%rip)
	je	.L3
	call	foo
.L3:
	xorl	%eax, %eax
	popq	%rdx
	ret

gcc-13.1.0 -O2 eliminates the call to foo:

main:
	movq	d(%rip), %rax
	movq	e(%rip), %rdx
	movl	(%rax), %esi
	testl	%esi, %esi
	jne	.L2
	pushq	%rcx
	xorl	%eax, %eax
	call	__assert_fail
.L2:
	xorl	%eax, %eax
	movq	%rax, d(%rip)
	xorl	%eax, %eax
	movl	$9, (%rdx)
	xorl	%edx, %edx
	movq	%rdx, e(%rip)
	ret

Bisects to r14-338-g1dd154f6407


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-1163-gd8b058d3ca4`
### open_at : `2023-07-04T09:44:49Z`
### last_modified_date : `2023-09-14T18:24:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110540
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/WP14EKx7M

Given the following code:

void foo(void);
static int a = 1;
static int *d = &a;
static int **f = &d;
static int ***g = &f;
static char(h)(char b, char c) { return b - c; }
static char(i)(int e) {
    if (!(((e) >= 1) && ((e) <= 254))) {
        __builtin_unreachable();
    }
    return 0;
}
int main() {
    char j = 0;
    for (; j != -9; j = h(j, 9)) {
        i(1);
        *d = a ^ 255;
        if (i(a) >= **f) **f = 0;
    }
    if (**g)
        ;
    else
        foo();
    ;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	movl	a(%rip), %eax
	movq	f(%rip), %rdx
	movq	d(%rip), %rcx
	xorb	$-1, %al
	movl	%eax, (%rcx)
	movq	(%rdx), %rax
	movl	(%rax), %ecx
	testl	%ecx, %ecx
	jle	.L8
.L4:
	xorl	%eax, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L8:
	cmpq	$0, (%rdx)
	movl	$0, (%rax)
	jne	.L4
	pushq	%rax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	ret

gcc-13.1.0 -O2 eliminates the call to foo:

main:
	movl	a(%rip), %eax
	movq	d(%rip), %rdx
	xorb	$-1, %al
	movl	%eax, (%rdx)
	movq	f(%rip), %rax
	movq	(%rax), %rax
	movl	(%rax), %ecx
	testl	%ecx, %ecx
	jg	.L2
	xorl	%edx, %edx
	movl	%edx, (%rax)
.L2:
	xorl	%eax, %eax
	ret

Bisects to r14-1163-gd8b058d3ca4


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] an extra mov when doing 128bit multiply`
### open_at : `2023-07-04T17:26:14Z`
### last_modified_date : `2023-10-18T19:11:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110551
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
https://godbolt.org/z/3hdondY6n

Codegen for the code shared above (which is a mixing step in boost.Unordered when a non-avalanching hash function is being used [1] ) regressed since GCC 11. I believe there are 2 regressions:

Regression 1:

A redundant move is introduced:


        movabs  rcx, -7046029254386353131
        mov     rax, rcx


The regression seems to be present at all optimization levels above -O0 (including -Os and -Og).

Possibly a duplicate of https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94804


Regression 2

When using -march=haswell or newer, GCC >= 11 emits mulx. The resulting code is longer (by 1 instruction) with no clear benefit to my untrained eyes. It looks to me like the code generated by GCC 10 is optimal, even for haswell and newer.


I am reporting both issues in the same bug report because they seem related enough. Let me know if you want me to split them into 2 bug reports instead.

[1] https://github.com/boostorg/unordered/blob/9a7d1d336aaa73ad8e5f7c07bdb81b2e793f8d93/include/boost/unordered/detail/mulx.hpp#L111


---


### compiler : `gcc`
### title : `Bad mask_load/mask_store codegen of RVV`
### open_at : `2023-07-05T11:30:20Z`
### last_modified_date : `2023-08-25T13:30:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110559
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdint-gcc.h>

#define INDEX8 uint8_t
#define INDEX16 uint16_t
#define INDEX32 uint32_t
#define INDEX64 uint64_t

#define TEST_LOOP(DATA_TYPE, BITS)                                             \
  void __attribute__ ((noinline, noclone))                                     \
  f_##DATA_TYPE (DATA_TYPE *restrict dest, DATA_TYPE *restrict src,            \
		 INDEX##BITS *restrict indices, INDEX##BITS *restrict cond)    \
  {                                                                            \
    for (int i = 0; i < 128; ++i)                                              \
      if (cond[i])                                                             \
	dest[i] += src[i];                                            \
  }

#define TEST_ALL(T)                                                            \
  T (int8_t, 8)                                                                \
  T (uint8_t, 8)                                                               \
  T (int16_t, 16)                                                              \
  T (uint16_t, 16)                                                             \
  T (_Float16, 16)                                                             \
  T (int32_t, 32)                                                              \
  T (uint32_t, 32)                                                             \
  T (float, 32)                                                                \
  T (int64_t, 64)                                                              \
  T (uint64_t, 64)                                                             \
  T (double, 64)

TEST_ALL (TEST_LOOP)

riscv32： --param riscv-autovec-preference=fixed-vlmax -O3：
f_int8_t:
	addi	sp,sp,-48
	sw	s0,44(sp)
	sw	s1,40(sp)
	sw	s2,36(sp)
	sw	s3,32(sp)
	sw	s4,28(sp)
	sw	s5,24(sp)
	sw	s6,20(sp)
	sw	s7,16(sp)
	sw	s8,12(sp)
	sw	s9,8(sp)
	vsetivli	zero,16,e8,m1,ta,ma
	addi	s9,a3,16
	vmv.v.i	v1,0
	vl1re8.v	v8,0(a3)
	vmsne.vv	v8,v8,v1
	vmv1r.v	v0,v8
	vl1re8.v	v7,0(s9)
	vle8.v	v9,0(a0),v0.t
	vmsne.vv	v7,v7,v1
	vle8.v	v15,0(a1),v0.t
	addi	s8,a1,16
	vmv1r.v	v0,v7
	addi	s7,a3,32
	vle8.v	v14,0(s8),v0.t
	vl1re8.v	v6,0(s7)
	addi	s5,a1,32
	vmsne.vv	v6,v6,v1
	addi	s6,a3,48
	vmv1r.v	v0,v6
	vl1re8.v	v5,0(s6)
	vle8.v	v13,0(s5),v0.t
	vmsne.vv	v5,v5,v1
	addi	s4,a1,48
	vmv1r.v	v0,v5
	addi	s3,a3,64
	vle8.v	v12,0(s4),v0.t
	vl1re8.v	v4,0(s3)
	addi	s1,a1,64
	vmsne.vv	v4,v4,v1
	addi	s2,a3,80
	vmv1r.v	v0,v4
	addi	s0,a1,80
	addi	t2,a3,96
	vle8.v	v11,0(s1),v0.t
	vl1re8.v	v2,0(s2)
	vl1re8.v	v3,0(t2)
	vmsne.vv	v2,v2,v1
	vmsne.vv	v3,v3,v1
	vmv1r.v	v0,v2
	vadd.vv	v9,v9,v15
	vle8.v	v10,0(s0),v0.t
	vsetvli	a5,zero,e8,m1,ta,ma
	vmv1r.v	v0,v8
	addi	t4,a0,16
	vse8.v	v9,0(a0),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	vmv1r.v	v0,v7
	vle8.v	v8,0(t4),v0.t
	vadd.vv	v8,v8,v14
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	t3,a0,32
	vse8.v	v8,0(t4),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	vmv1r.v	v0,v6
	vle8.v	v7,0(t3),v0.t
	vadd.vv	v7,v7,v13
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	t1,a0,48
	vse8.v	v7,0(t3),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	vmv1r.v	v0,v5
	vle8.v	v6,0(t1),v0.t
	vadd.vv	v6,v6,v12
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	a7,a0,64
	vse8.v	v6,0(t1),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	vmv1r.v	v0,v4
	vle8.v	v5,0(a7),v0.t
	vadd.vv	v5,v5,v11
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	a6,a0,80
	vse8.v	v5,0(a7),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	vmv1r.v	v0,v2
	vle8.v	v4,0(a6),v0.t
	vadd.vv	v4,v4,v10
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	a2,a0,96
	vse8.v	v4,0(a6),v0.t
	addi	t0,a1,96
	vsetivli	zero,16,e8,m1,ta,ma
	addi	t6,a3,112
	vmv1r.v	v0,v3
	vl1re8.v	v2,0(t6)
	vle8.v	v4,0(a2),v0.t
	vle8.v	v5,0(t0),v0.t
	vmsne.vv	v1,v2,v1
	vadd.vv	v4,v4,v5
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	a4,a0,112
	vse8.v	v4,0(a2),v0.t
	addi	t5,a1,112
	vsetivli	zero,16,e8,m1,ta,ma
	vmv1r.v	v0,v1
	vle8.v	v2,0(a4),v0.t
	vle8.v	v3,0(t5),v0.t
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	vse8.v	v2,0(a4),v0.t
	lw	s0,44(sp)
	lw	s1,40(sp)
	lw	s2,36(sp)
	lw	s3,32(sp)
	lw	s4,28(sp)
	lw	s5,24(sp)
	lw	s6,20(sp)
	lw	s7,16(sp)
	lw	s8,12(sp)
	lw	s9,8(sp)
	addi	sp,sp,48
	jr	ra

This codegen is very ugly and bad, to many "lw" "sw" and "vmv1r.v"

But with -fno-schedule-insns, codegen becomes much more reasonable:

f_uint8_t:
	vsetivli	zero,16,e8,m1,ta,ma
	addi	a2,a3,16
	vmv.v.i	v1,0
	vl1re8.v	v0,0(a3)
	vmsne.vv	v0,v0,v1
	vle8.v	v2,0(a0),v0.t
	vle8.v	v3,0(a1),v0.t
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	a4,a0,16
	vse8.v	v2,0(a0),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	vl1re8.v	v0,0(a2)
	addi	a2,a1,16
	vmsne.vv	v0,v0,v1
	vle8.v	v3,0(a4),v0.t
	vle8.v	v2,0(a2),v0.t
	addi	a2,a3,32
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	vse8.v	v2,0(a4),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	addi	a4,a0,32
	vl1re8.v	v0,0(a2)
	addi	a2,a1,32
	vmsne.vv	v0,v0,v1
	vle8.v	v2,0(a4),v0.t
	vle8.v	v3,0(a2),v0.t
	addi	a2,a3,48
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	vse8.v	v2,0(a4),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	addi	a4,a0,48
	vl1re8.v	v0,0(a2)
	addi	a2,a1,48
	vmsne.vv	v0,v0,v1
	vle8.v	v2,0(a4),v0.t
	vle8.v	v3,0(a2),v0.t
	addi	a2,a3,64
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	vse8.v	v2,0(a4),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	addi	a4,a0,64
	vl1re8.v	v0,0(a2)
	addi	a2,a1,64
	vmsne.vv	v0,v0,v1
	vle8.v	v2,0(a4),v0.t
	vle8.v	v3,0(a2),v0.t
	addi	a2,a3,80
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	vse8.v	v2,0(a4),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	addi	a4,a0,80
	vl1re8.v	v0,0(a2)
	addi	a2,a1,80
	vmsne.vv	v0,v0,v1
	vle8.v	v2,0(a4),v0.t
	vle8.v	v3,0(a2),v0.t
	addi	a2,a3,96
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	vse8.v	v2,0(a4),v0.t
	vsetivli	zero,16,e8,m1,ta,ma
	addi	a4,a0,96
	vl1re8.v	v0,0(a2)
	addi	a2,a1,96
	vmsne.vv	v0,v0,v1
	vle8.v	v2,0(a4),v0.t
	vle8.v	v3,0(a2),v0.t
	vadd.vv	v2,v2,v3
	vsetvli	a5,zero,e8,m1,ta,ma
	addi	a0,a0,112
	vse8.v	v2,0(a4),v0.t
	addi	a3,a3,112
	vsetivli	zero,16,e8,m1,ta,ma
	addi	a1,a1,112
	vl1re8.v	v0,0(a3)
	vmsne.vv	v0,v0,v1
	vle8.v	v1,0(a0),v0.t
	vle8.v	v2,0(a1),v0.t
	vadd.vv	v1,v1,v2
	vsetvli	a5,zero,e8,m1,ta,ma
	vse8.v	v1,0(a0),v0.t
	ret


---


### compiler : `gcc`
### title : `GCC fails using a register to initialize multiple variables with a constant`
### open_at : `2023-07-05T22:09:08Z`
### last_modified_date : `2023-07-24T21:54:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110567
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
This is about a missed optimization for an obvious case.
GCC fails to use a register to initialize several global variables with the same constant value.

$ cat foo.c
extern long init;
extern long g1, g2, g3, g4, g5;

void f(void)
{
    long val = 12345678;

    g1 = val;
    g2 = val;
    g3 = val;
    g4 = val;
    g5 = val;
}

$ m68k-elf-gcc -S foo.c -o - -Os
#NO_APP
        .file   "foo.c"
        .text
        .align  2
        .globl  f
        .type   f, @function
f:
        move.l #12345678,g1
        move.l #12345678,g2
        move.l #12345678,g3
        move.l #12345678,g4
        move.l #12345678,g5
        rts
        .size   f, .-f
        .ident  "GCC: (GNU) 13.1.0"

We can see that 12345678 is used many times. This is a waste of space and time.

On the other hand, if I replace 12345678 by "init" (an external global variable) then GCC produces the expected optimized output:

$ m68k-elf-gcc -S foo.c -o - -Os
#NO_APP
        .file   "foo.c"
        .text
        .align  2
        .globl  f
        .type   f, @function
f:
        move.l init,%d0
        move.l %d0,g1
        move.l %d0,g2
        move.l %d0,g3
        move.l %d0,g4
        move.l %d0,g5
        rts
        .size   f, .-f
        .ident  "GCC: (GNU) 13.1.0"

GCC should be able to optimize a constant initializer, just as it does with a variable initializer.

I can reproduce the same wrong behaviour with GCC 11.4.0 for x86_64-pc-cygwin host with -m32:

$ gcc -m32 -S foo.c -o - -Os
        .file   "foo.c"
        .text
        .globl  _f
        .def    _f;     .scl    2;      .type   32;     .endef
_f:
        movl    $12345678, _g1
        movl    $12345678, _g2
        movl    $12345678, _g3
        movl    $12345678, _g4
        movl    $12345678, _g5
        ret
        .ident  "GCC: (GNU) 11.4.0"


---


### compiler : `gcc`
### title : `branch delay slots are not filled with atomic stores`
### open_at : `2023-07-06T13:58:46Z`
### last_modified_date : `2023-07-06T23:49:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110573
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.1.0`
### severity : `normal`
### contents :
This is my first report for GCC so please forgive me if I make a mistake. This is an enhancement report - the behaviour of the program is ok, but an instruction could be removed to be consistent with the non-atomic variant of the below code.

Consider the code in GCC 13.1.0 built for MIPS64 (https://godbolt.org/z/as68sEWda)

```
void P1() {
  int r0;
  r0 = *y;
  if(r0 == (1))   {
    atomic_store_explicit(x,1,memory_order_release);  }

  *P1_r0 = r0;
}
```
When compiled using `-O1 -pthread -std=c11 -g -c` the branch to label L7 loads a pointer to `P1_r0` using the delay slot. Likewise `P1_r0` is loaded the line above L7 when the branch is taken. 


```
                                            #... (code in if branch)
        ld      $3,%got_disp(x)($5)

        ld      $3,%got_disp(P1_r0)($5).    # ld P1_r0 on branch taken
.L7:
        ld      $3,0($3)
        jr      $31
        sw      $2,0($3)

.L6:
        ld      $3,0($3)
        sync
        li      $4,1                        # 0x1
        sw      $4,0($3)
        b       .L7
        ld      $3,%got_disp(P1_r0)($5).  # ld P1_r0 on branch not taken
```


The ld could be moved into L7, thus saving one instruction:


```
                                            #... (code in if branch)
        ld      $3,%got_disp(x)($5)
.L7:
        ld      $3,0($3)
        jr      $31
        sw      $2,0($3)

.L6:    ld      $3,%got_disp(P1_r0)($5). 
        ld      $3,0($3)
        sync
        li      $4,1                        # 0x1
        b       .L7
        sw      $4,0($3)
```

The above optimisation already occurs if x is non-atomic (see https://godbolt.org/z/8dhxvsE18)

The optimisation can also be applied for `-O2` (https://godbolt.org/z/8aMj6xqTq)
as well.

I hope this helps.


---


### compiler : `gcc`
### title : `[x86] missed optimizations in vector concatenation patterns`
### open_at : `2023-07-07T08:50:49Z`
### last_modified_date : `2023-07-07T08:50:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110583
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Test case (https://godbolt.org/z/f8GdzfjbW):
Compile with e.g. `-O2 -std=gnu++20 -march=skylake`

using short4 [[gnu::vector_size(4 * sizeof(short))]] = short;
using short8 [[gnu::vector_size(8 * sizeof(short))]] = short;
using int4 [[gnu::vector_size(4 * sizeof(int))]] = int;
using int8 [[gnu::vector_size(8 * sizeof(int))]] = int;
using float4 [[gnu::vector_size(4 * sizeof(float))]] = float;
using float8 [[gnu::vector_size(8 * sizeof(float))]] = float;
using double1 [[gnu::vector_size(1 * sizeof(double))]] = double;
using double4 [[gnu::vector_size(4 * sizeof(double))]] = double;

// ------------------------------------------------
// vpunpcklqdq xmm0, xmm0, xmm1
// ret
short8 vpunpcklqdq_1(short4 a, short4 b)
{ return short8{a[0], a[1], a[2], a[3], b[0], b[1], b[2], b[3]}; }

short8 vpunpcklqdq_2(short4 a, short4 b)
{ return __builtin_shufflevector(a, b, 0, 1, 2, 3, 4, 5, 6, 7); }

// ------------------------------------------------
// vinserti128 ymm0, ymm0, xmm1, 1
// ret
int8 vinserti128_1(int4 a, int4 b)
{ return int8{a[0], a[1], a[2], a[3], b[0], b[1], b[2], b[3]}; }

int8 vinserti128_2(int4 a, int4 b)
{ return __builtin_shufflevector(a, b, 0, 1, 2, 3, 4, 5, 6, 7); }

// ------------------------------------------------
// vinsertf128 ymm0, ymm0, xmm1, 1
// ret
float8 vinsertf128_good(float4 a, float4 b)
{ return float8{a[0], a[1], a[2], a[3], b[0], b[1], b[2], b[3]}; }

float8 vinsertf128_bad(float4 a, float4 b)
{ return __builtin_shufflevector(a, b, 0, 1, 2, 3, 4, 5, 6, 7); }

// ------------------------------------------------
// vbroadcastsd    ymm1, xmm1
// vblendps        ymm0, ymm0, ymm1, 192
// ret
double4 broadcast_blend_0(double4 a, double b)
{ return double4{a[0], a[1], a[2], b}; }

// ------------------------------------------------
// vbroadcastsd    ymm1, QWORD PTR [rsp+8]
// vblendps        ymm0, ymm0, ymm1, 192
// ret
double4 broadcast_blend_1(double4 a, double1 b)
{ return double4{a[0], a[1], a[2], b[0]}; }

double4 broadcast_blend_2(double4 a, double1 b)
{ return __builtin_shufflevector(a, b, 0, 1, 2, 4); }


These functions should compile to the asm in the comments above them. Only vinsertf128_good is fine, however I added it because it should be equivalent to vinsertf128_bad (with the latter having to change ;) ).


---


### compiler : `gcc`
### title : `[14 Regression] 10% fatigue2 regression on zen since r14-2369-g3a61ca1b925653 (bad LRA&scheduling)`
### open_at : `2023-07-07T09:46:00Z`
### last_modified_date : `2023-07-18T15:56:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110586
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
seen here:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=283.767.0
on zen3 and
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=171.767.0
on zen2
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=171.767.0
on zen1


---


### compiler : `gcc`
### title : `btl (on x86_64) not always generated`
### open_at : `2023-07-07T10:02:00Z`
### last_modified_date : `2023-07-23T05:06:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110588
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
Take C+ code (since I am too lazy to figure out the types):
```
unsigned char clr1_bb (unsigned char x, int y)
{
  return (x & (1<<y)) == 0;
}


unsigned char clr1_bb1 (unsigned char x, int y)
{
  auto  _1 = (int) x;
 auto _2 = _1 >> y;
 auto _3 = _2 & 1;
 auto _8 = (unsigned char) _3;
auto  _6 = _8 ^ 1;

  return _6;
}

unsigned char clr1_bb2 (unsigned char x, int y)
{
  auto  _1 = (int) x;
 auto _2 = _1 >> y;
 auto _3 = _2 & 1;
auto  _6 = _3 ^ 1;
 auto _8 = (unsigned char) _6;

  return _6;
}
```

Right now clr1_bb and clr1_bb2 produce the btl/setnc but clr1_bb1 does not.

Note I noticed this while working on PR 110539 (I changed my patch to put the cast after the ^1 rather than before so this is not blocking that patch).


---


### compiler : `gcc`
### title : `Missed optimization with call-clobbered restrict qualified references`
### open_at : `2023-07-07T15:26:37Z`
### last_modified_date : `2023-07-07T17:49:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110589
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
I see this on GCC trunk and do not think it is a regression, the optimization is being taken by Clang.
https://godbolt.org/z/G8qrzaKn9


extern void foo(void);

int test_clobber_by_call (int * restrict val_ptr) {
    *val_ptr = 1;

    foo();

    return *val_ptr;
}

GCC 14.0 -O3 produces:
test_clobber_by_call:
        push    rbx
        mov     rbx, rdi
        mov     BYTE PTR [rdi], 1
        call    foo
        movzx   eax, BYTE PTR [rbx] ; <--- Not expected
        pop     rbx
        ret


I would expect restrict to be a guarantee that foo() will not alias val_ptr, producing:
test_clobber_by_call:
        mov     DWORD PTR [rdi], 1
        call    foo
        mov     eax, 1
        ret

This is indeed the output when the compiler recognizes that foo does not alias, as in:
__attribute((noinline)) void foo(int *a) {
    *(a+10) = 0;
}


int test_clobber_by_call (int * val_ptr) {
    *val_ptr = 1;

    foo(val_ptr);

    return *val_ptr;
}


-------------------------

It looks to me as if tree-ssa-alias.c#call_may_clobber_ref_p is not considering a restrict qualified reference. 

The following patch produces optimized code for the above example. I do not claim that it is correct, but it does reflect what I expected to see:


diff --git a/gcc/tree-ssa-alias.c b/gcc/tree-ssa-alias.c
index c3f43dc..277a21e 100644
--- a/gcc/tree-ssa-alias.c
+++ b/gcc/tree-ssa-alias.c
@@ -3037,6 +3037,16 @@ call_may_clobber_ref_p_1 (gcall *call, ao_ref *ref, bool tbaa_p)
       && SSA_NAME_POINTS_TO_READONLY_MEMORY (TREE_OPERAND (base, 0)))
     return false;
 
+  /* perhaps should be moved further up */
+  if ((TREE_CODE (base) == MEM_REF
+       || TREE_CODE (base) == TARGET_MEM_REF)
+       && TREE_CODE(TREE_OPERAND (base, 0)) == SSA_NAME)
+    {
+      struct ptr_info_def *pi = SSA_NAME_PTR_INFO (TREE_OPERAND (base, 0));
+      if (pi && pi->pt.vars_contains_restrict)
+        return false;
+    }
+
   if (int res = check_fnspec (call, ref, true))
     {
       if (res == 1)


test_clobber_by_call:
        sub     rsp, 8
        mov     BYTE PTR [rdi], 1
        call    may_alias
        mov     eax, 1   ; <----- deref gone
        add     rsp, 8
        ret


---


### compiler : `gcc`
### title : `optimization about combined store of adjacent bitfields`
### open_at : `2023-07-10T14:47:13Z`
### last_modified_date : `2023-07-29T21:59:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110613
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
This is a piece of code taken from a WebSocket frame parser:
```
#include <stdint.h>

struct Header 
  {
    // 1st byte
    uint8_t opcode : 4;
    uint8_t rsv3 : 1;
    uint8_t rsv2 : 1;
    uint8_t rsv1 : 1;
    uint8_t fin : 1;
     
    // 2nd byte
    uint8_t reserved_1 : 7;
    uint8_t mask : 1;

    // 3rd and 4th bytes
    uint8_t reserved_2;
    uint8_t reserved_3;

    // 5th to 7th bytes
    union {
      char mask_key[4];
      uint32_t mask_key_u32;
    };

    // 8th to 15th bytes
    uint64_t payload_len;
  };

void
set_header(Header* ph, const uint8_t* bptr)
  {
    uint8_t f = bptr[0];
    uint8_t t = bptr[1];

    ph->fin = f >> 7;
    ph->rsv1 = f >> 6;
    ph->rsv2 = f >> 5;
    ph->rsv3 = f >> 4;
    ph->opcode = f;

    ph->mask = t >> 7;
    ph->reserved_1 = t;
  }
```

The structure is designed to match x86_64 ABI (little endian), so
```
    ph->fin = f >> 7;
    ph->rsv1 = f >> 6;
    ph->rsv2 = f >> 5;
    ph->rsv3 = f >> 4;
    ph->opcode = f;
```
should be a simple move (https://gcc.godbolt.org/z/9vTqs7axj), and
```
    ph->mask = t >> 7;
    ph->reserved_1 = t;
```
should also be a simple move (https://gcc.godbolt.org/z/KdchWvEn1), but!

When put these two pieces of code together, guess what?:
(godbolt: https://gcc.godbolt.org/z/hbEaeb3MT)
```
set_header(Header*, unsigned char const*):
        movzx   edx, BYTE PTR [rsi]
        movzx   ecx, BYTE PTR [rsi+1]
        mov     eax, edx
        mov     esi, edx
        shr     al, 4
        and     esi, 15
        and     eax, 1
        sal     eax, 4
        or      eax, esi
        mov     esi, edx
        shr     sil, 5
        and     esi, 1
        sal     esi, 5
        or      eax, esi
        mov     esi, edx
        shr     dl, 7
        shr     sil, 6
        movzx   edx, dl
        and     esi, 1
        sal     edx, 7
        sal     esi, 6
        or      eax, esi
        or      eax, edx
        mov     edx, ecx
        shr     cl, 7
        and     edx, 127
        sal     ecx, 15
        sal     edx, 8
        or      eax, edx
        or      eax, ecx
        mov     WORD PTR [rdi], ax
        ret
```


---


### compiler : `gcc`
### title : `[AArch64] Vect: SLP fails to vectorize a loop as the reduction_latency calculated by new costs is too large`
### open_at : `2023-07-11T09:15:52Z`
### last_modified_date : `2023-08-04T02:34:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110625
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
This problem causes a performance regression in SPEC2017 538.imagick.  For the following simple case (modified from pr96208):

    typedef struct {
        unsigned short m1, m2, m3, m4;
    } the_struct_t;
    typedef struct {
        double m1, m2, m3, m4, m5;
    } the_struct2_t;

    double bar1 (the_struct2_t*);

    double foo (double* k, unsigned int n, the_struct_t* the_struct) {
        unsigned int u;
        the_struct2_t result;
        for (u=0; u < n; u++, k--) {
            result.m1 += (*k)*the_struct[u].m1;
            result.m2 += (*k)*the_struct[u].m2;
            result.m3 += (*k)*the_struct[u].m3;
            result.m4 += (*k)*the_struct[u].m4;
        }
        return bar1 (&result);
    }


Compile it with "-Ofast -S -mcpu=neoverse-n2 -fdump-tree-vect-details -fno-tree-slp-vectorize". SLP fails to vectorize the loop as the vector body cost is increased due to the too large "reduction latency".  See the dump of vect pass:

    Original vector body cost = 51
    Scalar issue estimate:
      ...
      reduction latency = 2
      estimated min cycles per iteration = 2.000000
      estimated cycles per vector iteration (for VF 2) = 4.000000
    Vector issue estimate:
      ...
      reduction latency = 8      <-- Too large
      estimated min cycles per iteration = 8.000000
    Increasing body cost to 102 because scalar code would issue more quickly
    Cost model analysis: 
    Vector inside of loop cost: 102
    ...
    Scalar iteration cost: 44
    ...
    missed:  cost model: the vector iteration cost = 102 divided by the scalar iteration cost = 44 is greater or equal to the vectorization factor = 2.
    missed:  not vectorized: vectorization not profitable.


SLP will success with "-mcpu=neoverse-n1", as N1 doesn't use the new vector costs and vector body cost is not increased. The "reduction latency" is calculated in aarch64.cc count_ops():
      /* ??? Ideally we'd do COUNT reductions in parallel, but unfortunately
	 that's not yet the case.  */
      ops->reduction_latency = MAX (ops->reduction_latency, base * count);

For this case, the "base" is 2 and "count" is 4 .  To my understanding, the "count" of SLP means the number of scalar stmts (i.e. results.m1 +=, ...) in a permutation group to be merged into a vector stmt.  It seems not reasonable to multiply cost by "count" (maybe it doesn't consider about the SLP situation).  So, I'm thinking to calcualte it differently for SLP situation, e.g.

      unsigned int latency = PURE_SLP_STMT(stmt_info) ? base : base * count;
      ops->reduction_latency = MAX (ops->reduction_latency, latency);

Is this reasonable?


---


### compiler : `gcc`
### title : `Missed optimization: bb-slp-pr95839.c not vectorised with V2SF targets`
### open_at : `2023-07-11T14:49:17Z`
### last_modified_date : `2023-07-12T11:03:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110630
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
With targets that only support the V2SF vector mode such as:

mips-linux-gnu-gcc -march=mips64 -mabi=64 -mpaired-single

scalar code is produced for V4SF data with bb-slp-pr95839.c while a
pair of V2SF operations would be more efficient.


---


### compiler : `gcc`
### title : `(type)(zeroonep !=/== 0) should be optimized as (type)zeroonep/zeroonep^1 earlier than vrp`
### open_at : `2023-07-12T07:25:56Z`
### last_modified_date : `2023-10-25T13:43:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110637
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(int t)
{
 t &= 1;
 return t != 0;
}
```
Currently this only gets optimized at -O2 (due to VRP) to:
  _4 = t_1(D) & 1;
  return _4;

Likewise for:
```
int g(int t)
{
 t &= 1;
 return t == 0;
}
```

This was originally how I was going to solve PR 110539 but when we should optimize to those gets in the way of other optimizations so I decided to fix PR 110539 a different way.


---


### compiler : `gcc`
### title : `[13 regression] memcpy should be inlined with sve loop`
### open_at : `2023-07-12T07:42:11Z`
### last_modified_date : `2023-07-12T07:55:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110638
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
* test:https://gcc.godbolt.org/z/39KddjbE4
```
void va(struct args_t * func_args)
{
    for (int nl = 0; nl < iterations; nl++) {
        for (int i = 0; i < LEN_1D; i++) {
            a[i] = b[i];
        }
        dummy(a, b);
    }
    return ;
}
```

* gcc12: don't transform the loop into a memcpy call, so it can make use of the new sve feature.

* gcc13: use a normal 'bl      memcpy'

The above regression start from gcc13.


---


### compiler : `gcc`
### title : `[14 Regression] 66% TSVC/s2712 regressoin on N1-neoverse between g:620a35b24a2b6edb (2023-07-01 07:24) and g:80ae426a195a0d03 (2023-07-02 01:37)`
### open_at : `2023-07-12T20:53:09Z`
### last_modified_date : `2023-07-16T16:25:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110647
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=356.884.0

this seems to be off noise.


---


### compiler : `gcc`
### title : `[14 Regression] 25% sphinx3 spec2006 regression on Ice Lake and zen since g:r14-2369-g3a61ca1b925653  (2023-07-06)`
### open_at : `2023-07-12T22:35:22Z`
### last_modified_date : `2023-07-18T15:56:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110649
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
It is seen here:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=798.280.0
with -Ofast -march=native

Weekly testers seems to show regressions too (all just 1 run so far, so may be a noise)

Intel
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=790.280.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=790.280.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=785.280.0

zen1
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=301.280.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=299.280.0

zen3
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=467.280.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=468.280.0


---


### compiler : `gcc`
### title : `Missed optimization opportunity with countr_zero`
### open_at : `2023-07-15T22:17:11Z`
### last_modified_date : `2023-07-15T22:34:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110679
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.2.1`
### severity : `enhancement`
### contents :
Clang better optimizes the following code:

```
int tst(std::uint8_t x) {
  if (!x) return 8;
  return std::countr_zero(x);
}
```

https://godbolt.org/z/dqr5Eddn4


---


### compiler : `gcc`
### title : `epilogues for loop which can be also vectorized with half size can be improved.`
### open_at : `2023-07-16T19:31:27Z`
### last_modified_date : `2023-07-17T07:45:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110692
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `normal`
### contents :
for:
int a[99];
test()
{
        for (int i = 0 ; i < 99; i++)
                a[i]++;
}
we produce:
   0:   66 0f 6f 0d 00 00 00    movdqa 0x0(%rip),%xmm1        # 8 <test+0x8>
   7:   00 
   8:   b8 00 00 00 00          mov    $0x0,%eax
   d:   ba 00 00 00 00          mov    $0x0,%edx
  12:   66 0f 1f 44 00 00       nopw   0x0(%rax,%rax,1)
  18:   66 0f 6f 00             movdqa (%rax),%xmm0
  1c:   48 83 c0 10             add    $0x10,%rax
  20:   66 0f fe c1             paddd  %xmm1,%xmm0
  24:   0f 29 40 f0             movaps %xmm0,-0x10(%rax)
  28:   48 39 c2                cmp    %rax,%rdx
  2b:   75 eb                   jne    18 <test+0x18>
  2d:   f3 0f 7e 05 00 00 00    movq   0x0(%rip),%xmm0        # 35 <test+0x35>
  34:   00 
  35:   83 05 00 00 00 00 01    addl   $0x1,0x0(%rip)        # 3c <test+0x3c>
  3c:   f3 0f 7e 0d 00 00 00    movq   0x0(%rip),%xmm1        # 44 <test+0x44>
  43:   00 
  44:   66 0f fe c1             paddd  %xmm1,%xmm0
  48:   66 0f d6 05 00 00 00    movq   %xmm0,0x0(%rip)        # 50 <test+0x50>
  4f:   00 
  50:   c3                      ret

which does the 4x vectorized loop followed by 2x vectorized loopless epilogue and copy of remaining byte.

When bound is unknow we do:
int a[99];
test(int n)
{
        for (int i = 0 ; i < n; i++)
                a[i]++;
}
   0:   85 ff                   test   %edi,%edi
   2:   7e 70                   jle    74 <test+0x74>
   4:   8d 47 ff                lea    -0x1(%rdi),%eax
   7:   83 f8 02                cmp    $0x2,%eax
   a:   76 6d                   jbe    79 <test+0x79>
   c:   89 fa                   mov    %edi,%edx
   e:   66 0f 6f 0d 00 00 00    movdqa 0x0(%rip),%xmm1        # 16 <test+0x16>
  15:   00
  16:   31 c0                   xor    %eax,%eax
  18:   c1 ea 02                shr    $0x2,%edx
  1b:   48 c1 e2 04             shl    $0x4,%rdx
  1f:   66 0f 6f 80 00 00 00    movdqa 0x0(%rax),%xmm0
  26:   00
  27:   48 83 c0 10             add    $0x10,%rax
  2b:   66 0f fe c1             paddd  %xmm1,%xmm0
  2f:   0f 29 80 00 00 00 00    movaps %xmm0,0x0(%rax)
  36:   48 39 d0                cmp    %rdx,%rax
  39:   75 e4                   jne    1f <test+0x1f>
  3b:   89 f8                   mov    %edi,%eax
  3d:   83 e0 fc                and    $0xfffffffc,%eax
  40:   40 f6 c7 03             test   $0x3,%dil
  44:   74 32                   je     78 <test+0x78>
  46:   48 63 d0                movslq %eax,%rdx
  49:   83 04 95 00 00 00 00    addl   $0x1,0x0(,%rdx,4)
  50:   01
  51:   8d 50 01                lea    0x1(%rax),%edx
  54:   39 d7                   cmp    %edx,%edi
  56:   7e 1c                   jle    74 <test+0x74>
  58:   48 63 d2                movslq %edx,%rdx
  5b:   83 c0 02                add    $0x2,%eax
  5e:   83 04 95 00 00 00 00    addl   $0x1,0x0(,%rdx,4)
  65:   01
  66:   39 c7                   cmp    %eax,%edi
  68:   7e 0a                   jle    74 <test+0x74>
  6a:   48 98                   cltq
  6c:   83 04 85 00 00 00 00    addl   $0x1,0x0(,%rax,4)
  73:   01
  74:   c3                      ret
  75:   0f 1f 00                nopl   (%rax)
  78:   c3                      ret
  79:   31 c0                   xor    %eax,%eax
  7b:   eb c9                   jmp    46 <test+0x46>

the profitability threshold is 4.
Producing the loopless epilogue just as in the first case with additional tests for block size would work better. The code looks quite bad for small trip counts since there is extra jump down to 79.


---


### compiler : `gcc`
### title : `possible missed optimization for std::max with -march=znver2`
### open_at : `2023-07-18T10:11:08Z`
### last_modified_date : `2023-07-19T08:41:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110711
### status : `NEW`
### tags : `missed-optimization, openmp`
### component : `middle-end`
### version : `13.1.0`
### severity : `normal`
### contents :
I think I found a missed optimization involving std::max() for -march=znver2 (sorry if it was already reported, but I didn't find anything related in the bug tracker).

I have two functions that compute the maximum element of an array:

- function k_std_max uses std::max() and is never vectorized
- function k_max uses conditional assignment and is vectorized, when the optimization flags allow for it

The code (also https://godbolt.org/z/hW49nbqMY):

#include <cassert>
#include <algorithm>

double k_std_max(size_t n_els, double * a)
{
    assert(n_els > 0);
    double m = a[0];
    #ifdef _OPENMP
        #pragma omp simd reduction(max:m)
    #endif
    for (size_t i = 1; i < n_els; ++i) {
        m = std::max(m, a[i]);
    }

    return m;
}

double k_max(size_t n_els, double * a)
{
    assert(n_els > 0);
    double m = a[0];
    #ifdef _OPENMP
        #pragma omp simd reduction(max:m)
    #endif
    for (size_t i = 1; i < n_els; ++i) {
        m = m < a[i] ? a[i] : m;
    }

    return m;
}

Compiling with "-O3 -fopenmp -march=znver2 -Wall -Wextra -DNDEBUG" vectorizes k_max:

.L19:
        vmovupd ymm3, YMMWORD PTR [rax+8]
        add     rax, 32
        vmaxpd  ymm1, ymm3, ymm1
        cmp     rax, rdx
        jne     .L19
        
but for k_std_max still scalar instructions are used:

.L3:
        vmovsd  xmm0, QWORD PTR [rax]
        add     rax, 8
        vmaxsd  xmm0, xmm0, xmm1
        cmp     rdx, rax
        jne     .L5
        
Note that I had to use -fopenmp as using only -fopenmp-simd did not vectorize k_max.

Even when I use "-Ofast" or "-Ofast -fopenmp" instead of "-O3" k_std_max is not vectorized:

.L3:
        vmaxsd  xmm0, xmm0, QWORD PTR [rax]
        add     rax, 8
        cmp     rdx, rax
        jne     .L3


---


### compiler : `gcc`
### title : `Fatigue2 runs twice as fast with increased inlining limits`
### open_at : `2023-07-18T11:03:02Z`
### last_modified_date : `2023-07-19T08:18:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110713
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.1.0`
### severity : `normal`
### contents :
jh@ryzen3:~/pb11/lin/source> ~/trunk-histogram/bin/gfortran fatigue2.f90 -Ofast -march=native -fdump-tree-all-details-blocks -fdump-rtl-all-details -fdump-ipa-all-details --param max-inline-insns-auto=110 ; perf stat ./a.out >/dev/null

 Performance counter stats for './a.out':

          13937.07 msec task-clock:u                     #    1.000 CPUs utilized             
                 0      context-switches:u               #    0.000 /sec                      
                 0      cpu-migrations:u                 #    0.000 /sec                      
               138      page-faults:u                    #    9.902 /sec                      
       67489472294      cycles:u                         #    4.842 GHz                         (83.33%)
          38791427      stalled-cycles-frontend:u        #    0.06% frontend cycles idle        (83.33%)
           2351353      stalled-cycles-backend:u         #    0.00% backend cycles idle         (83.33%)
      147268347462      instructions:u                   #    2.18  insn per cycle            
                                                  #    0.00  stalled cycles per insn     (83.33%)
        5705431257      branches:u                       #  409.371 M/sec                       (83.35%)
          13638274      branch-misses:u                  #    0.24% of all branches             (83.35%)

      13.941876147 seconds time elapsed

      13.933226000 seconds user
       0.003999000 seconds sys


jh@ryzen3:~/pb11/lin/source> ~/trunk-histogram/bin/gfortran fatigue2.f90 -Ofast -march=native -fdump-tree-all-details-blocks -fdump-rtl-all-details -fdump-ipa-all-details  ; perf stat ./a.out >/dev/null

 Performance counter stats for './a.out':

          31300.68 msec task-clock:u                     #    1.000 CPUs utilized             
                 0      context-switches:u               #    0.000 /sec                      
                 0      cpu-migrations:u                 #    0.000 /sec                      
               138      page-faults:u                    #    4.409 /sec                      
      150619261261      cycles:u                         #    4.812 GHz                         (83.32%)
         779861463      stalled-cycles-frontend:u        #    0.52% frontend cycles idle        (83.33%)
           4695025      stalled-cycles-backend:u         #    0.00% backend cycles idle         (83.34%)
      242822794319      instructions:u                   #    1.61  insn per cycle            
                                                  #    0.00  stalled cycles per insn     (83.34%)
       13542051898      branches:u                       #  432.644 M/sec                       (83.34%)
          14587945      branch-misses:u                  #    0.11% of all branches             (83.34%)

      31.301169341 seconds time elapsed

      31.296826000 seconds user
       0.003999000 seconds sys

The main differnece is inlning generalized_hookes_law. While it looks quite big at release_ssa time, after vectorization it gets loopless and inlining is a big win.

      function generalized_hookes_law (strain_tensor, lambda, mu) result (stress_tensor)
!
!      Author:       Dr. John K. Prentice
!      Affiliation:  Quetzal Computational Associates, Inc.
!      Dates:        28 November 1997
!
!      Purpose:      Apply the generalized Hooke's law for elasticity to the strain tensor
!                    (or strain rate tensor) to compute the stress tensor (or stress rate
!                    tensor)
!
!############################################################################################
!
!      Input:
!
!        strain_tensor                [selected_real_kind(15,90), dimension(3,3)]
!                                     stress tensor
!
!        lambda                       [selected_real_kind(15,90)]
!                                     Lame constant Lambda
!
!        mu                           [selected_real_kind(15,90)]
!                                     Lame constant mu
!
!     Output:
!
!        stress_tensor                [selected_real_kind(15,90), dimension(3,3)]
!                                     stress tensor
!
!############################################################################################
!
!
!=========== formal variables =============
!
      real (kind = LONGreal), dimension(:,:), intent(in) :: strain_tensor
      real (kind = LONGreal), intent(in) :: lambda, mu
      real (kind = LONGreal), dimension(3,3) :: stress_tensor
!
!========== internal variables ============
!
      real (kind = LONGreal), dimension(6) ::generalized_strain_vector,                     &
                                             generalized_stress_vector
      real (kind = LONGreal), dimension(6,6) :: generalized_constitutive_tensor
      integer :: i
!
!        construct the generalized constitutive tensor for elasticity
!
      generalized_constitutive_tensor(:,:) = 0.0_LONGreal
      generalized_constitutive_tensor(1,1) = lambda + 2.0_LONGreal * mu
      generalized_constitutive_tensor(1,2) = lambda
      generalized_constitutive_tensor(1,3) = lambda
      generalized_constitutive_tensor(2,1) = lambda
      generalized_constitutive_tensor(2,2) = lambda + 2.0_LONGreal * mu
      generalized_constitutive_tensor(2,3) = lambda
      generalized_constitutive_tensor(3,1) = lambda
      generalized_constitutive_tensor(3,2) = lambda
      generalized_constitutive_tensor(3,3) = lambda + 2.0_LONGreal * mu
      generalized_constitutive_tensor(4,4) = mu
      generalized_constitutive_tensor(5,5) = mu
      generalized_constitutive_tensor(6,6) = mu
!
!        construct the generalized strain vector (using double index notation)
!
      generalized_strain_vector(1) = strain_tensor(1,1)
      generalized_strain_vector(2) = strain_tensor(2,2)
      generalized_strain_vector(3) = strain_tensor(3,3)
      generalized_strain_vector(4) = strain_tensor(2,3)
      generalized_strain_vector(5) = strain_tensor(1,3)
      generalized_strain_vector(6) = strain_tensor(1,2)
!
!        compute the generalized stress vector
!
      do i = 1, 6
          generalized_stress_vector(i) = dot_product(generalized_constitutive_tensor(i,:),  &
                                                                generalized_strain_vector(:))
      end do
!
!        update the stress tensor 
!
      stress_tensor(1,1) = generalized_stress_vector(1)
      stress_tensor(2,2) = generalized_stress_vector(2)
      stress_tensor(3,3) = generalized_stress_vector(3)
      stress_tensor(2,3) = generalized_stress_vector(4)
      stress_tensor(1,3) = generalized_stress_vector(5)
      stress_tensor(1,2) = generalized_stress_vector(6)
      stress_tensor(3,2) = stress_tensor(2,3)
      stress_tensor(3,1) = stress_tensor(1,3)
      stress_tensor(2,1) = stress_tensor(1,2)
!
      end function generalized_hookes_law


---


### compiler : `gcc`
### title : `Double-word sign-extension missed-optimization`
### open_at : `2023-07-18T14:32:17Z`
### last_modified_date : `2023-10-12T22:45:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110717
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
While working on _BitInt, I've noticed that we don't emit very good code at least on x86_64 -m64/-m32 -O2 for:
#ifdef __SIZEOF_INT128__
unsigned __int128
foo (unsigned __int128 x)
{
  x <<= 59;
  return ((__int128) x) >> 59;
}
#else
unsigned long long
foo (unsigned long long x)
{
  x <<= 27;
  return ((long long) x) >> 27;
}
#endif

The sign-extension from 69 resp. 37 bits could be limited solely to the upper word,
but we uselessly shift the lower word with it as well:
	movq	%rdi, %rax
	movq	%rsi, %rdx
	shldq	$59, %rdi, %rdx
	salq	$59, %rax
	shrdq	$59, %rdx, %rax
	sarq	$59, %rdx
	ret
for -m64 and
	movl	4(%esp), %eax
	movl	8(%esp), %edx
	shldl	$27, %eax, %edx
	sall	$27, %eax
	shrdl	$27, %edx, %eax
	sarl	$27, %edx
	ret
for -m32.
LLVM emits even more horrible code for -m64, but
        movl    4(%esp), %eax
        movl    8(%esp), %edx
        shll    $27, %edx
        sarl    $27, %edx
        retl
for -m32, which looks to me like what we want.


---


### compiler : `gcc`
### title : `FP is Saved/Restored around inline assembly`
### open_at : `2023-07-18T15:35:32Z`
### last_modified_date : `2023-07-19T06:40:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110722
### status : `UNCONFIRMED`
### tags : `inline-asm, missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
I'm not sure if this is some ABI-related requirement that I've managed to forget about, but it looks like we're saving/restoring FP around inline assembly.

long fp_asm(long arg0)
{
    asm volatile ("addi a0, a0, 1" : "+r"(arg0));
    return arg0;
}

produces

fp_asm(long):
        addi    sp,sp,-16
        sd      s0,8(sp)
        addi    s0,sp,16
        addi a0, a0, 1
        ld      s0,8(sp)
        addi    sp,sp,16
        jr      ra

We've got a ton of inline assembly in Linux these days and defconfig has `-fno-omit-frame-pointer`, so this probably manifests as a performance issue for someone somewhere -- though Clement just ran into it because he was curious, so I don't have anything concrete.


---


### compiler : `gcc`
### title : `Unnecessary alignment on branch to unconditional branch targets`
### open_at : `2023-07-18T17:11:50Z`
### last_modified_date : `2023-07-19T08:26:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110724
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/f7qMxxfMj

void duff(int * __restrict to, const int * __restrict from, const int count)
{
    int n = (count+7) / 8;
    switch(count%8)
    {
       case 0: do { *to++ = *from++;
       case 7:      *to++ = *from++;
       case 6:      *to++ = *from++;
       case 5:      *to++ = *from++;
       case 4:      *to++ = *from++;
       case 3:      *to++ = *from++;
       case 2:      *to++ = *from++;
       [[likely]] case 1:      *to++ = *from++;
        } while (--n>0);
    }
}

Trunk with O3:
        jle     .L1
        [...]
        lea     rax, [rax+4]
        jmp     .L5            # <-- no fall-through to ret
        .p2align 4,,7          # <-- unnecessary alignment
        .p2align 3
.L1:
        ret


I believe this 16-byte alignment is done to put the branch target at the beginning of a front-end instruction fetch block. That however seems unnecessary when the branch target is itself an unconditional branch, as the instructions to follow will not retire.
 
In this example the degrade is code size / instruction caching only, as there is no possible fall-through to .L1 that would cause nop's to be consumed. Changing the C++ attribute to [[unlikely]] introduces fall-through, and GCC seems to remove the padding, which is great.


---


### compiler : `gcc`
### title : `RISC-V: optimize store of DF 0.0`
### open_at : `2023-07-20T06:13:50Z`
### last_modified_date : `2023-08-15T17:13:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110748
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Currently a store of int 0 is optimized by using reg x0.

void zi(int *i) {    *i = 0;    }

-O2 =march=rv64gc

  sw  zero,0(a0)
  ret

However a store of positive DF 0.0 generates 2 insns.

void zd(double *d) { *d = 0.0;  }

  fmv.d.x fa5,zero
  fsd     fa5,0(a0)
  ret

Since +0.0 is all zero bits, this could be generated as an int store
   sw zero, 0(a0) 

This is 1 less insn and avoids the FPU thus overall a win.

This came up when discussing an ICE in anewly proposed pass f-m-o by Manolis. Turns out that it could be an independent optimization opportunity [1].

[1] https://gcc.gnu.org/pipermail/gcc-patches/2023-July/624935.html


---


### compiler : `gcc`
### title : `[14 Regression] 7% parest regression on zen3 -Ofast -march=native -flto between g:4dbb3af1efe55174 (2023-07-14 00:54) and g:a5088dc3f5ef73c8 (2023-07-17 03:24)`
### open_at : `2023-07-20T21:55:47Z`
### last_modified_date : `2023-07-26T07:15:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110757
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
seems there are two commits producing this regression. Run in between is d76d19c9bc5ef113 (2023-07-16 00:16)

https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=475.457.0

There are earlier two jumps between g:52577a301ef1b86d (2023-05-30 02:20) and g:d0c064c3eabc75cf (2023-05-31 16:46)
and between g:7ebd4a1d61993c0a (2023-04-28 07:23) and g:977a3be3ccbc7f17 (2023-05-01 13:40)

8% regression is also seen on zen1 machine:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=287.457.0


---


### compiler : `gcc`
### title : `[14 Regression] 8% hmmer regression on zen1/3 with -Ofast -march=native -flto between g:8377cf1bf41a0a9d (2023-07-05 01:46) and g:3a61ca1b9256535e (2023-07-06 16:56); g:d76d19c9bc5ef113 (2023-07-16 00:16) and g:a5088dc3f5ef73c8 (2023-07-17 03:24)`
### open_at : `2023-07-20T22:02:24Z`
### last_modified_date : `2023-07-31T07:39:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110758
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=476.180.0
the earlier jump looks like random code layout change.
Later jump is also seen with PGO
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=474.180.0
and -O2
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=469.180.0

zen1 machine
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=289.180.0


---


### compiler : `gcc`
### title : `vec_{fm,}{addsub,subadd} are missing for AVX512 modes`
### open_at : `2023-07-21T12:27:34Z`
### last_modified_date : `2023-07-21T12:27:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110767
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
See PR54939 for a testcase.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-2623-gc11a3aedec2`
### open_at : `2023-07-21T13:59:15Z`
### last_modified_date : `2023-09-10T19:46:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110768
### status : `UNCONFIRMED`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/GsTMz1G9c

Given the following code:

void foo(void);
static int a, b;
int main() {
    {
        short c = 45127;
        char d;
        b = 0;
        for (; b <= 3; b++) {
            if (b) continue;
            d = 0;
            for (; d <= 3; d++) {
                if (!(((c) >= -20409) && ((c) <= 1))) {
                    __builtin_unreachable();
                }
                if (~(0 == a) & 1) return b;
                c = 0;
                for (; c <= 0; c++) a = 3;
            }
        }
        foo();
    }
}

gcc-trunk -Os does not eliminate the call to foo:

main:
	xorl	%r9d, %r9d
	movl	a(%rip), %eax
	xorl	%edx, %edx
	xorl	%esi, %esi
	movl	%r9d, b(%rip)
	xorl	%ecx, %ecx
.L2:
	cmpl	$4, %edx
	je	.L31
	testl	%edx, %edx
	jne	.L3
	testl	%eax, %eax
	je	.L4
.L24:
	testb	%sil, %sil
	je	.L5
	xorl	%r8d, %r8d
	movl	%r8d, b(%rip)
.L5:
	testb	%cl, %cl
	je	.L27
	movl	%eax, a(%rip)
	jmp	.L27
.L4:
	movl	$3, %eax
	movb	$1, %cl
	jmp	.L24
.L3:
	incl	%edx
	movb	$1, %sil
	jmp	.L2
.L31:
	pushq	%rdi
	testb	%sil, %sil
	je	.L9
	movl	$4, b(%rip)
.L9:
	testb	%cl, %cl
	je	.L10
	movl	%eax, a(%rip)
.L10:
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	ret
.L27:
	xorl	%eax, %eax
	ret

gcc-13.1.0 -Os eliminates the call to foo:

main:
	xorl	%eax, %eax
	cmpl	$0, a(%rip)
	movl	%eax, b(%rip)
	jne	.L2
	movl	$3, a(%rip)
.L2:
	xorl	%eax, %eax
	ret

Bisects to r14-2623-gc11a3aedec2


---


### compiler : `gcc`
### title : `aarch64 NEON redundant displaced ld3`
### open_at : `2023-07-23T20:58:15Z`
### last_modified_date : `2023-07-24T19:12:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110780
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Compile the following with gcc 14.0.0 20230723 on aarch64 with -O3:

#include <stdint.h>
void CSI2toBE12(uint8_t* pCSI2, uint8_t* pBE, uint8_t* pCSI2LineEnd)
{
    while (pCSI2 < pCSI2LineEnd) {
        pBE[0] = pCSI2[0];
        pBE[1] = ((pCSI2[2] & 0xf) << 4) | (pCSI2[1] >> 4);
        pBE[2] = ((pCSI2[1] & 0xf) << 4) | (pCSI2[2] >> 4);
        pCSI2 += 3;
        pBE += 3;
    }
}

Godbolt: https://godbolt.org/z/WshTPKzY5

In the inner loop (.L5 of the godbolt asm) we have

        ld3     {v25.16b - v27.16b}, [x3]
        add     x6, x3, 1
        // no intervening stores
        ld3     {v25.16b - v27.16b}, [x6]

The second load is redundant.  v25, v26 are the same as what was already in v26, v27 respectively.  The value loaded into v27 is new but it is not used in the subsequent code.

This might also account for some extra later complexity, because it means that the last 48 bytes of the input can't be handled by this loop (or else the second load would be out of bounds by one byte) and so must be handled specially.


---


### compiler : `gcc`
### title : `Spilling to mask register for GPR vec_duplicate`
### open_at : `2023-07-24T08:40:29Z`
### last_modified_date : `2023-07-28T02:30:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110788
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
double a[1024], b[1024];

void foo (int n)
{
  for (int i = 0; i < n; ++i)
    a[i] = b[i] * 3.;
}

compiled with -O3 -march=cascadelake --param vect-partial-vector-usage=2

produces the inner loop

.L3:
        vmovapd b(%rax), %ymm0{%k1}
        movl    %edx, %ecx
        subl    $4, %edx
        kmovw   %edx, %k0
        vmulpd  %ymm3, %ymm0, %ymm1{%k1}{z}
        vmovapd %ymm1, a(%rax){%k1}
        vpbroadcastmw2d %k0, %xmm1
        addq    $32, %rax
        vpcmpud $6, %xmm2, %xmm1, %k1
        cmpw    $4, %cx
        ja      .L3

where we implement the splat of %edx as

        kmovw   %edx, %k0
        vpbroadcastmw2d %k0, %xmm1

instead of

        vpbroadcastw    %edx, %xmm1

we expand to

(insn 14 13 15 (set (reg:V4SI 96)
        (vec_duplicate:V4SI (reg:SI 93 [ _27 ]))) 8167 {*avx512vl_vec_dup_gprv4si}
     (nil))

but at IRA time we instead match that do

(insn 14 13 15 3 (set (reg:V4SI 96)
        (vec_duplicate:V4SI (zero_extend:SI (subreg:HI (reg/v:SI 95 [ n ]) 0)))) 8247 {avx512cd_maskw_vec_dupv4si}
     (expr_list:REG_DEAD (reg/v:SI 95 [ n ]) 
        (nil)))

where combine created this via

Trying 13 -> 14:
   13: r93:SI=zero_extend(r95:SI#0)
      REG_DEAD r95:SI
   14: r96:V4SI=vec_duplicate(r93:SI)
      REG_DEAD r93:SI
Successfully matched this instruction:
(set (reg:V4SI 96)
    (vec_duplicate:V4SI (zero_extend:SI (subreg:HI (reg/v:SI 95 [ n ]) 0))))
allowing combination of insns 13 and 14
original costs 4 + 4 = 8
replacement cost 4

but it didn't anticipate that reg 95 could be allocated to a GPR?  The
vectorizer uses an unsigned short IV for the loop, that's possibly
sub-optimal in this case but important in others.

I suppose it could also be a missed optimization in REE since I think
the HImode regs should already be zero-extended?


---


### compiler : `gcc`
### title : `std::format code runs slower than equivalent {fmt} code`
### open_at : `2023-07-25T10:58:49Z`
### last_modified_date : `2023-08-17T17:26:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110801
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.1.1`
### severity : `normal`
### contents :
Using the benchmark from https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0645r10.html#Benchmarks and adding:

#if __has_include(<format>)
#include <format>
#endif

#ifdef __cpp_lib_format
void std_format(benchmark::State& s) {
  size_t result = 0;
  while (s.KeepRunning()) {
    for (auto i: data)
      result += std::format("{}", i).size();
  }
  benchmark::DoNotOptimize(result);
}
BENCHMARK(std_format);

void std_format_to(benchmark::State& s) {
  size_t result = 0;
  while (s.KeepRunning()) {
    for (auto i: data) {
      char buffer[12];
      result += std::format_to(buffer, "{}", i) - buffer;
    }
  }
  benchmark::DoNotOptimize(result);
}
BENCHMARK(std_format_to);
#endif

I get:

Benchmark              Time             CPU   Iterations
--------------------------------------------------------
sprintf           708600 ns       706474 ns          946
ostringstream    1216025 ns      1210087 ns          589
to_string         178579 ns       178088 ns         3824
format            306344 ns       305365 ns         2345
format_to         145606 ns       145223 ns         4940
std_format        514969 ns       513563 ns         1376
std_format_to     436502 ns       435402 ns         1567

The std::to_string performance is good, but std::format is much slower than fmt::format.


---


### compiler : `gcc`
### title : `[13 Regression] Copy list initialisation of a vector<bool> raises a warning with -O2`
### open_at : `2023-07-25T18:26:57Z`
### last_modified_date : `2023-07-31T21:07:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110807
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `libstdc++`
### version : `13.1.0`
### severity : `normal`
### contents :
Created attachment 55631
the source coded needed to reproduce the problem

If you have a class with a vector<bool> member:

struct Foo {
    std::vector<bool> byCallSpread;
};

And try to initialise it with copy list initialisation:

    Foo() { byCallSpread = {true, false}; }

Then that works fine with the default optimisation level, but gets a warning at -O2 saying:

/usr/local/include/c++/13.1.0/bits/stl_algobase.h:437:30: warning: 'void* __builtin_memmove(void*, const void*, long unsigned int)' writing between 9 and 9223372036854775807 bytes into a region of size 8 overflows the destination [-Wstringop-overflow=]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

(i believe this is the relevant error - i am not very good at reading error messages, so apologies if not)

The warning goes away if the initialisation uses an explicit constructor:

    Foo() { byCallSpread = std::vector<bool>({true, false}); }

I did not get this warning with GCC 7.2.0. According to Compiler Explorer, it does not occur with GCC 12.3.

Here is a transcript of a self-contained session using the GCC 13.1.0 official Docker image (official for Docker, perhaps not for GCC!) which demonstrates the problem:

$ sudo docker run -it gcc:13.1.0
root@4694030a8bea:/# gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-linux-gnu/13.1.0/lto-wrapper
Target: x86_64-linux-gnu
Configured with: /usr/src/gcc/configure --build=x86_64-linux-gnu --disable-multilib --enable-languages=c,c++,fortran,go
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 13.1.0 (GCC) 
root@4694030a8bea:/# cat >foo.cpp
#include <vector>

struct Foo {
    std::vector<bool> byCallSpread;

    Foo() { byCallSpread = {true, false}; }
};

Foo f;

int main(int argc, char** argv) {}
root@4694030a8bea:/# g++ foo.cpp 
root@4694030a8bea:/# g++ -O2 foo.cpp 
In file included from /usr/local/include/c++/13.1.0/vector:62,
                 from foo.cpp:1:
In static member function 'static _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = long unsigned int; _Up = long unsigned int; bool _IsMove = false]',
    inlined from '_OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /usr/local/include/c++/13.1.0/bits/stl_algobase.h:506:30,
    inlined from '_OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /usr/local/include/c++/13.1.0/bits/stl_algobase.h:533:42,
    inlined from '_OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /usr/local/include/c++/13.1.0/bits/stl_algobase.h:540:31,
    inlined from '_OI std::copy(_II, _II, _OI) [with _II = long unsigned int*; _OI = long unsigned int*]' at /usr/local/include/c++/13.1.0/bits/stl_algobase.h:633:7,
    inlined from 'std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::_M_copy_aligned(const_iterator, const_iterator, iterator) [with _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:1303:28,
    inlined from 'void std::vector<bool, _Alloc>::_M_insert_range(iterator, _ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const bool*; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/vector.tcc:915:33,
    inlined from 'std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::insert(const_iterator, _InputIterator, _InputIterator) [with _InputIterator = const bool*; <template-parameter-2-2> = void; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:1180:19,
    inlined from 'void std::vector<bool, _Alloc>::_M_assign_aux(_ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const bool*; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:1440:14,
    inlined from 'void std::vector<bool, _Alloc>::assign(_InputIterator, _InputIterator) [with _InputIterator = const bool*; <template-parameter-2-2> = void; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:935:17,
    inlined from 'std::vector<bool, _Alloc>& std::vector<bool, _Alloc>::operator=(std::initializer_list<bool>) [with _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:915:14,
    inlined from 'Foo::Foo()' at foo.cpp:6:40,
    inlined from 'void __static_initialization_and_destruction_0()' at foo.cpp:9:5,
    inlined from '(static initializers for foo.cpp)' at foo.cpp:11:34:
/usr/local/include/c++/13.1.0/bits/stl_algobase.h:437:30: warning: 'void* __builtin_memmove(void*, const void*, long unsigned int)' writing between 9 and 9223372036854775807 bytes into a region of size 8 overflows the destination [-Wstringop-overflow=]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /usr/local/include/c++/13.1.0/x86_64-linux-gnu/bits/c++allocator.h:33,
                 from /usr/local/include/c++/13.1.0/bits/allocator.h:46,
                 from /usr/local/include/c++/13.1.0/vector:63:
In member function '_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = long unsigned int]',
    inlined from 'static _Tp* std::allocator_traits<std::allocator<_Tp1> >::allocate(allocator_type&, size_type) [with _Tp = long unsigned int]' at /usr/local/include/c++/13.1.0/bits/alloc_traits.h:482:28,
    inlined from 'std::_Bvector_base<_Alloc>::_Bit_pointer std::_Bvector_base<_Alloc>::_M_allocate(std::size_t) [with _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:643:48,
    inlined from 'void std::vector<bool, _Alloc>::_M_insert_range(iterator, _ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const bool*; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/vector.tcc:913:39,
    inlined from 'std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::insert(const_iterator, _InputIterator, _InputIterator) [with _InputIterator = const bool*; <template-parameter-2-2> = void; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:1180:19,
    inlined from 'void std::vector<bool, _Alloc>::_M_assign_aux(_ForwardIterator, _ForwardIterator, std::forward_iterator_tag) [with _ForwardIterator = const bool*; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:1440:14,
    inlined from 'void std::vector<bool, _Alloc>::assign(_InputIterator, _InputIterator) [with _InputIterator = const bool*; <template-parameter-2-2> = void; _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:935:17,
    inlined from 'std::vector<bool, _Alloc>& std::vector<bool, _Alloc>::operator=(std::initializer_list<bool>) [with _Alloc = std::allocator<bool>]' at /usr/local/include/c++/13.1.0/bits/stl_bvector.h:915:14,
    inlined from 'Foo::Foo()' at foo.cpp:6:40,
    inlined from 'void __static_initialization_and_destruction_0()' at foo.cpp:9:5,
    inlined from '(static initializers for foo.cpp)' at foo.cpp:11:34:
/usr/local/include/c++/13.1.0/bits/new_allocator.h:147:55: note: destination object of size 8 allocated by 'operator new'
  147 |         return static_cast<_Tp*>(_GLIBCXX_OPERATOR_NEW(__n * sizeof(_Tp)));
      |                                                       ^

I have attached the code from this session to this issue.


---


### compiler : `gcc`
### title : `under-optimized code for x86_64`
### open_at : `2023-07-26T07:46:54Z`
### last_modified_date : `2023-07-26T09:54:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110811
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.1`
### severity : `normal`
### contents :
The following example is not properly optimized, resulting in low quality code：

long a;
int b;
void foo(int c) {
    int t = c > 0;
    b += t;
    a += t;
}

Compile command: gcc -O3 foo.c -S

Output:
foo:
	testl	%edi, %edi
	setg	%al
	setg	%dl
	movzbl	%dl, %edx
	movzbl	%al, %eax
	addl	%edx, b(%rip)
	addq	%rax, a(%rip)
	ret

There are redundant setg and movzbl.


---


### compiler : `gcc`
### title : `Missed optimization: when vector's size is 0 but vector::reserve has been called.`
### open_at : `2023-07-26T16:37:39Z`
### last_modified_date : `2023-07-29T07:15:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110819
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.1`
### severity : `enhancement`
### contents :
#include<vector>

void f(int);

void use_idx_const_size_reserve() {
    std::vector<int> v;
    v.reserve(100000);
    auto s = v.size();
    for (std::vector<int>::size_type i = 0; i < s; i++)
        f(v[i]);
}

$ g++ -O3

use_idx_const_size_reserve():
        sub     rsp, 8
        mov     edi, 400000
        call    operator new(unsigned long)
        mov     esi, 400000
        add     rsp, 8
        mov     rdi, rax
        jmp     operator delete(void*, unsigned long)


$ clang++ -O3 -stdlib=libc++

use_idx_const_size_reserve():        # @use_idx_const_size_reserve()
        ret


---


### compiler : `gcc`
### title : `[missed optimization] >50% speedup for x86-64 ASCII processing a la GNU diffutils`
### open_at : `2023-07-26T19:37:22Z`
### last_modified_date : `2023-08-25T14:33:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110823
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.1.1`
### severity : `enhancement`
### contents :
Created attachment 55643
proprocessed source code inspired by GNU diffutils

This is GCC 13.1.1 20230614 (Red Hat 13.1.1-4) on x86-64.

While tuning GNU diffutils I noticed that its loops to process mostly-ASCII text were not compiled well by GCC on x86-64. For a stripped-down example of the problem, compile the attached program with:

gcc -O2 -S code-mbcel1.i

The result is in the attached file code-mbcel1.s. Its loop kernel assuming ASCII text (starting on line 212) looks like this:

  .L33:
        testb   %al, %al
        js      .L30
        movl    $1, %edx
  .L31:
        movl    %eax, %eax
        addq    %rdx, %rbx
        addq    %rax, %rbp
        movsbl  (%rbx), %eax
        testb   %al, %al
        jne     .L33

As I understand it the "movl %eax, %eax" is unnecessary, as all code that reaches .L31 guarantees that %rax's top 32 bits are zero.

Also, the loop body executes "testb %al, %al" twice when once would suffice. (As a minor point, since the compiler can easily tell that %al is positive when the loop is entered, it can omit the first testb.)

Suppose we change the above code to the following, as is done in the attached file code-mbcel1-opt.s:

  .L33:
        movl    $1, %edx
  .L31:
        addq    %rdx, %rbx
        addq    %rax, %rbp
        movsbl  (%rbx), %eax
        testb   %al, %al
        jg      .L33
        js      .L30

This small change improves performance significantly: for me, the test program runs 55% faster on a circa-2021 Intel Xeon W-1350, and 74% faster on a circa-2010 AMD Phenom II x4 910e, using the following commands to benchmark:

gcc -O2 code-mbcel1.i -o code-mbcel1
gcc -O2 code-mbcel1-opt.s -o code-mbcel1-opt
time ./code-mbcel1
time ./code-mbcel1-opt


---


### compiler : `gcc`
### title : `[13/14 Regression] FAIL: gcc.dg/pr56837.c scan-tree-dump-times optimized "memset ..c, 68, 16384.;"  1`
### open_at : `2023-07-27T10:33:40Z`
### last_modified_date : `2023-07-27T10:37:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110829
### status : `UNCONFIRMED`
### tags : `missed-optimization, xfail`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `normal`
### contents :
After r13-1762-gf9d4c3b45c5ed5 the 

_Complex double c[N];

void
fc (void)
{
  int i;
  for (i = 0; i < N; i++)
    c[i] = 747708026454360457216.0 + 747708026454360457216.0i;
}

part of the testcase fails to be recognized as memset since we lower
the complex store to a component-wise store to enable vectorization.

loop distribution doesn't yet handle merging stores as part of
its pattern recognition for memset, memcpy or memmove.


---


### compiler : `gcc`
### title : `[14 Regression] 14% capacita -O2 regression between g:9fdbd7d6fa5e0a76 (2023-07-26 01:45) and g:ca912a39cccdd990 (2023-07-27 03:44) on zen3 and core`
### open_at : `2023-07-27T17:27:48Z`
### last_modified_date : `2023-10-09T22:21:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110832
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
Biggest regression is seen here
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=466.758.0
zen3
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=466.758.0

Curiously zen2 improves:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=171.758.0

I can see instruction count differnece in perfs:
 Performance counter stats for './a.out':

          10923.70 msec task-clock:u                     #    1.000 CPUs utilized             
                 0      context-switches:u               #    0.000 /sec                      
                 0      cpu-migrations:u                 #    0.000 /sec                      
             15510      page-faults:u                    #    1.420 K/sec                     
       59062937176      cycles:u                         #    5.407 GHz                         (83.33%)
          12607081      stalled-cycles-frontend:u        #    0.02% frontend cycles idle        (83.34%)
         122404896      stalled-cycles-backend:u         #    0.21% backend cycles idle         (83.34%)
      112648123380      instructions:u                   #    1.91  insn per cycle            
                                                  #    0.00  stalled cycles per insn     (83.34%)
        9666338531      branches:u                       #  884.896 M/sec                       (83.34%)
           2937216      branch-misses:u                  #    0.03% of all branches             (83.31%)

      10.924108973 seconds time elapsed

      10.912056000 seconds user
       0.012000000 seconds sys


 Performance counter stats for './b.out':

          11025.38 msec task-clock:u                     #    1.000 CPUs utilized             
                 0      context-switches:u               #    0.000 /sec                      
                 0      cpu-migrations:u                 #    0.000 /sec                      
             14998      page-faults:u                    #    1.360 K/sec                     
       59436352848      cycles:u                         #    5.391 GHz                         (83.31%)
           9217660      stalled-cycles-frontend:u        #    0.02% frontend cycles idle        (83.32%)
         210162784      stalled-cycles-backend:u         #    0.35% backend cycles idle         (83.35%)
      131604240004      instructions:u                   #    2.21  insn per cycle            
                                                  #    0.00  stalled cycles per insn     (83.35%)
        9657712171      branches:u                       #  875.953 M/sec                       (83.35%)
           3146487      branch-misses:u                  #    0.03% of all branches             (83.33%)

      11.025701172 seconds time elapsed

      11.005646000 seconds user
       0.020002000 seconds sys

however perf report does not show clear differences in times of functions.
I


---


### compiler : `gcc`
### title : `gamess regression on Ice Lake with -Ofast -march=native between g:1c6231c05bdccab3 (2023-07-21 03:06) and g:bbc1a102735c72e3 (2023-07-23 04:55)`
### open_at : `2023-07-27T18:38:19Z`
### last_modified_date : `2023-07-28T06:22:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110833
### status : `UNCONFIRMED`
### tags : `missed-optimization, needs-bisection`
### component : `middle-end`
### version : `13.1.0`
### severity : `normal`
### contents :
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=798.50.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=790.50.0

It may be interesting to know why it improved and now regressed again.


---


### compiler : `gcc`
### title : `Bogus -Wreturn-type for unreachable switch default case`
### open_at : `2023-07-28T07:36:04Z`
### last_modified_date : `2023-09-21T17:11:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110839
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `13.1.1`
### severity : `normal`
### contents :
unsigned char random_ui8(void);
int f16RandomQOutP3 (void);
int f16RandomQOutPInf (void);
int f16RandomQInfP3 (void);
int f16RandomQInfPInf (void);
int f16Random( void )
{   
    
    switch ( random_ui8() & 7 ) {
     case 0:
     case 1:
     case 2:
        return f16RandomQOutP3();
     case 3:
        return f16RandomQOutPInf();
     case 4:
     case 5:
     case 6:
        return f16RandomQInfP3();
     case 7:
        return f16RandomQInfPInf();
    }
}

diagnoses

gcc -S t.c -O2 -Wreturn-type
t.c: In function 'f16Random':
t.c:23:1: warning: control reaches end of non-void function [-Wreturn-type]
   23 | }
      | ^

this is because pass_warn_function_return runs after lowering before any
optimization is done and the gimplifier adds a default: case that
falls through to the function end.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-2675-gef28aadad6e`
### open_at : `2023-07-28T10:10:58Z`
### last_modified_date : `2023-08-07T09:16:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110841
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/YondMzhT8

Given the following code:

void foo(void);
static int b, c, d;
static char(a)(char e, char f) { return e - f; }
int main() {
    for (; b <= 4; b++)
        ;
    c = 0;
    for (; c >= -16; c = a(c, 4))
        ;
    char g = b;
    for (; d <= 0; d++) {
        if (!(((g) >= 5) && ((g) <= 5))) {
            __builtin_unreachable();
        }
        if (c) return 0;
        g = 0;
        for (;;) {
            foo();
            break;
        }
    }
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	cmpl	$4, b(%rip)
	jg	.L2
	movl	$5, b(%rip)
.L2:
	movl	$-20, c(%rip)
	movl	d(%rip), %ecx
	testl	%ecx, %ecx
	jg	.L10
	cmpl	$0, c(%rip)
	jne	.L10
	pushq	%rax
	jmp	.L11
	.p2align 4,,10
	.p2align 3
.L3:
	movl	c(%rip), %edx
	testl	%edx, %edx
	jne	.L5
.L11:
	call	foo
	movl	d(%rip), %eax
	addl	$1, %eax
	movl	%eax, d(%rip)
	testl	%eax, %eax
	jle	.L3
.L5:
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
.L10:
	xorl	%eax, %eax
	ret

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	cmpl	$4, b(%rip)
	jg	.L2
	movl	$5, b(%rip)
.L2:
	movl	$-20, c(%rip)
	xorl	%eax, %eax
	ret

Bisects to r14-2675-gef28aadad6e


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O2 since r14-376-g47a76439911`
### open_at : `2023-08-02T12:34:39Z`
### last_modified_date : `2023-08-25T23:48:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110873
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static int b;
static short c;
static char d;
static int *e = &b;
void foo(void);  
void bar110_(void);
static char(a)(char f, char g) { return f + g; }
int main() {
    *e = 3;
    unsigned char h;
    c = 0;
i:
    if (!b)
        foo();
    h = -25;
    for (; h >= 7; h = a(h, 6))
        for (; d; d--)
            ;
    if (!(h >= 5))
        bar110_();
    if (c)
        goto i;
}
-----------------------------------------------------------------------
gcc-68c3aa7510b (trunk) -O2 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O2 can
-----------------------------------------------------------------------
gcc-68c3aa7510b2f45f44379ecd77e97c88780a84ed -O2 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movl	$3, b(%rip)
	xorl	%esi, %esi
	movw	%si, c(%rip)
.L11:
	cmpb	$0, d(%rip)
	je	.L13
	movb	$0, d(%rip)
.L13:
	cmpw	$0, c(%rip)
	je	.L15
	movl	b(%rip), %eax
	testl	%eax, %eax
	jne	.L11
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
.L6:
	call	foo
.L2:
	cmpb	$0, d(%rip)
	je	.L4
	movb	$0, d(%rip)
.L4:
	cmpw	$0, c(%rip)
	je	.L5
	movl	b(%rip), %ecx
	testl	%ecx, %ecx
	jne	.L2
	jmp	.L6
.L15:
	.cfi_def_cfa_offset 8
	xorl	%eax, %eax
	ret
.L5:
	.cfi_def_cfa_offset 16
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O2 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movl	$3, b(%rip)
	xorl	%eax, %eax
	cmpb	$0, d(%rip)
	movw	%ax, c(%rip)
	je	.L3
	movb	$0, d(%rip)
.L3:
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-376-g47a76439911


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-2501-g285c9d042e9`
### open_at : `2023-08-02T13:56:34Z`
### last_modified_date : `2023-09-07T19:26:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110875
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/rjxT5PfzY

Given the following code:

void foo(void);
static int a, b;
static int *c = &a, *d;
static unsigned e;
static short f;
static unsigned g(unsigned char h, char i) { return h + i; }
int main() {
    d = &a;
    int *j = d;
    e = -27;
    for (; e > 18; e = g(e, 6)) {
        a = 0;
        for (; a != -3; a--) {
            if (0 != a ^ *j)
                for (; b; b++) f = -f;
            else if (*c) {
                foo();
                break;
            }
            if (!(((e) >= 235) && ((e) <= 4294967269))) {
                __builtin_unreachable();
            }
            b = 0;
        }
    }
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	movl	$-27, e(%rip)
	.p2align 4,,10
	.p2align 3
.L5:
	xorl	%eax, %eax
.L3:
	subl	$1, %eax
	cmpl	$-3, %eax
	je	.L2
	cmpl	$1, %eax
	jne	.L3
	movl	$1, a(%rip)
	movl	$0, b(%rip)
	call	foo
.L6:
	movzbl	e(%rip), %eax
	addl	$6, %eax
	movl	%eax, e(%rip)
	cmpl	$18, %eax
	jg	.L5
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
	.p2align 4,,10
	.p2align 3
.L2:
	movl	$-3, a(%rip)
	movl	$0, b(%rip)
	jmp	.L6

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	movl	$-27, e(%rip)
	movl	$-27, %esi
	.p2align 4,,10
	.p2align 3
.L9:
	movzwl	f(%rip), %r11d
	xorl	%ecx, %ecx
	xorl	%r10d, %r10d
	xorl	%edx, %edx
	movl	$0, a(%rip)
	movl	b(%rip), %eax
	leal	-235(%rsi), %r8d
	jmp	.L7
	.p2align 4,,10
	.p2align 3
.L2:
	cmpl	$-262, %r8d
	ja	.L28
	subl	$1, %edx
	xorl	%eax, %eax
	movl	$1, %ecx
	cmpl	$-3, %edx
	je	.L32
	movl	%edx, %r9d
.L7:
	xorl	%esi, %esi
	testl	%edx, %edx
	movl	%ecx, %edi
	setne	%sil
	cmpl	%edx, %esi
	je	.L2
	testl	%eax, %eax
	je	.L2
	testb	$1, %al
	je	.L3
	negl	%r11d
	addl	$1, %eax
	je	.L30
	.p2align 4,,10
	.p2align 3
.L3:
	addl	$2, %eax
	jne	.L3
.L30:
	movl	$1, %edi
	movl	$1, %r10d
	jmp	.L2
	.p2align 4,,10
	.p2align 3
.L32:
	movl	$-3, a(%rip)
	movl	$0, b(%rip)
	testb	%r10b, %r10b
	je	.L8
	movw	%r11w, f(%rip)
.L8:
	movzbl	e(%rip), %esi
	addl	$6, %esi
	movl	%esi, e(%rip)
	cmpl	$18, %esi
	jg	.L9
	xorl	%eax, %eax
	ret
	.section	.text.unlikely
	.type	main.cold, @function
main.cold:
.L28:
	testb	%cl, %cl
	je	.L5
	movl	%r9d, a(%rip)
.L5:
	testb	%dil, %dil
	je	.L6
	movl	%eax, b(%rip)
.L6:
	movw	%r11w, f(%rip)
	.section	.text.startup

Bisects to  r14-2501-g285c9d042e9


---


### compiler : `gcc`
### title : `[14 Regression] Unnecessary reread from memory in a loop with std::vector`
### open_at : `2023-08-02T19:26:20Z`
### last_modified_date : `2023-09-01T16:12:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110879
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `libstdc++`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55678
preprocessed file by g++ from revision dd2eb972a

I've found a strange regression in optimization. Trunk version of g++ produces less optimal assembly. It rereads same memory location in every iteration of a loop. More specifically, it rereads fields _M_finish and _M_end_of_storage of a vector from memory every push_back call, although it is not necessary.
Released version 13.2 doesn't do that, and just uses values from registers.

I'm compiling following code:

#include <vector>

std::vector<int> f(std::size_t n) {
    std::vector<int> res;
    res.reserve(n);
    for (std::size_t i = 0; i < n; ++i) {
        res.push_back(i*i);
    }
    return res;
}

The main body of a loop looks like this:
~/.local/gcc/bin/g++ -S -fverbose-asm -O3 -std=c++20 pb.cpp

>.L41:
># /home/scaiper/.local/gcc/include/c++/14.0.0/bits/stl_construct.h:97:     { return ::new((void*)__location) _Tp(std::forward<_Args>(__args)...); }
>        movl    %r15d, (%rbx)   # _3, *prephitmp_51
># /home/scaiper/.local/gcc/include/c++/14.0.0/bits/vector.tcc:119:          ++this->_M_impl._M_finish;
>        addq    $4, %rbx        #, tmp135
>        movq    %rbx, 8(%rbp)   # tmp135, res_8(D)->D.35756._M_impl.D.35067._M_finish
>.L8:
># pb.cpp:6:     for (std::size_t i = 0; i < n; ++i) {
>        addq    $1, %r13        #, i
># pb.cpp:6:     for (std::size_t i = 0; i < n; ++i) {
>        cmpq    %r13, %r12      # i, n
>        je      .L1     #,
># /home/scaiper/.local/gcc/include/c++/14.0.0/bits/vector.tcc:114:      if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
>        movq    8(%rbp), %rbx   # res_8(D)->D.35756._M_impl.D.35067._M_finish, prephitmp_51
># /home/scaiper/.local/gcc/include/c++/14.0.0/bits/vector.tcc:114:      if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
>        movq    16(%rbp), %rax  # res_8(D)->D.35756._M_impl.D.35067._M_end_of_storage, pretmp_52
>.L16:
># pb.cpp:7:         res.push_back(i*i);
>        movl    %r13d, %r15d    # i, _3
>        imull   %r13d, %r15d    # i, _3
># /home/scaiper/.local/gcc/include/c++/14.0.0/bits/vector.tcc:114:      if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
>        cmpq    %rax, %rbx      # pretmp_52, prephitmp_51
>        jne     .L41    #,

Same loop as produced by 13.2:
~/.local/gcc-13.2/bin/g++ -v -S -fverbose-asm -O3 -std=c++20 pb.cpp

>.L43:
># /home/scaiper/.local/gcc-13.2/include/c++/13.2.0/bits/stl_construct.h:97:     { return ::new((void*)__location) _Tp(std::forward<_Args>(__args)...); }
>        movl    %r12d, (%rcx)   # _3, *prephitmp_4
># /home/scaiper/.local/gcc-13.2/include/c++/13.2.0/bits/vector.tcc:119:             ++this->_M_impl._M_finish;
>        addq    $4, %rcx        #, prephitmp_4
>        movq    %rcx, 8(%rbp)   # prephitmp_4, res_8(D)->D.35699._M_impl.D.35010._M_finish
>.L8:
># pb.cpp:6:     for (std::size_t i = 0; i < n; ++i) {
>        addq    $1, %rbx        #, i
># pb.cpp:6:     for (std::size_t i = 0; i < n; ++i) {
>        cmpq    %rbx, %r13      # i, n
>        je      .L1     #,
>.L18:
># pb.cpp:7:         res.push_back(i*i);
>        movl    %ebx, %r12d     # i, _3
>        imull   %ebx, %r12d     # i, _3
># /home/scaiper/.local/gcc-13.2/include/c++/13.2.0/bits/vector.tcc:114:         if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
>        cmpq    %r8, %rcx       # prephitmp_74, prephitmp_4
>        jne     .L43    #,

Notice this extra commands in the first snippet:
movq    8(%rbp), %rbx
movq    16(%rbp), %rax

I've bisected this problem to the commit dd2eb972a (libstdc++: Use RAII in std::vector::_M_realloc_insert) (https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=dd2eb972a5b063e10c83878d5c9336a818fa8291).
It doesn't look like commit is the problem. Code looks pretty equivalent. But for some reason compiler produces different result.

I'm using version built from aforementioned commit dd2eb972a:
Target: x86_64-pc-linux-gnu
Configured with: ../gcc/configure --enable-languages=c++ --disable-multilib --prefix=/home/scaiper/.local/gcc
gcc version 14.0.0 20230623 (experimental) (GCC)
COLLECT_GCC_OPTIONS='-v' '-S' '-fverbose-asm' '-O3' '-std=c++20' '-shared-libgcc' '-mtune=generic' '-march=x86-64'

Comparing with 13.2:
Target: x86_64-pc-linux-gnu
Configured with: ../gcc/configure --enable-languages=c++ --disable-multilib --prefix=/home/scaiper/.local/gcc-13.2
gcc version 13.2.0 (GCC)
COLLECT_GCC_OPTIONS='-v' '-S' '-fverbose-asm' '-O3' '-std=c++20' '-shared-libgcc' '-mtune=generic' '-march=x86-64'


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-2674-gd0de3bf9175`
### open_at : `2023-08-03T16:01:08Z`
### last_modified_date : `2023-08-28T06:53:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110891
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/njdE3xof4

Given the following code:

void foo(void);
static int a, c = 7, d, o, q;
static int *b = &a, *f, *j = &d, *n = &c, *ae;
static short e, m;
static short *i = &e;
static char r;
void __assert_fail(char *, char *, int, char *) __attribute__((__noreturn__));
static const short g();
static void h();
static int *k(int) {
    (*i)++;
    *j ^= *b;
    return &a;
}
static void l(unsigned p) {
    int *aa = &o;
    h();
    o = 5 ^ g() && p;
    if (f == &d || f == &c || f == &a)
        ;
    else {
        foo();
        __assert_fail("", "", 3, __PRETTY_FUNCTION__);
    }
    *aa ^= *n;
    if (*aa)
        if (!(((p) >= 0) && ((p) <= 0))) {
            __builtin_unreachable();
        }
    k(p);
}
static const short g() { return q; }
static void h() {
    unsigned ag = c;
    d = ag > r ? ag : 0;
    ae = k(c);
    f = ae;
    if (ae == &d || ae == &c || ae == &a)
        ;
    else
        __assert_fail("", "", 4, __PRETTY_FUNCTION__);
}
int main() {
    l(a);
    m || (*b |= 64);
    *b &= 5;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	movl	c(%rip), %eax
	movl	%eax, d(%rip)
	call	k.isra.0
	cmpq	$c, %rax
	movq	%rax, f(%rip)
	sete	%cl
	cmpq	$a, %rax
	sete	%dil
	cmpq	$d, %rax
	sete	%dl
	orb	%cl, %dl
	jne	.L4
	testb	%dil, %dil
	je	.L15
.L4:
	cmpq	$d, %rax
	je	.L6
	testb	%cl, %cl
	jne	.L6
	testb	%dil, %dil
	je	.L16
.L6:
	call	k.isra.0
	andl	$5, a(%rip)
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
.L16:
	call	foo
	movl	$.LC0, %esi
	movl	$__PRETTY_FUNCTION__.1, %ecx
	movl	$3, %edx
	movq	%rsi, %rdi
	call	__assert_fail
.L15:
	movl	$.LC0, %esi
	movl	$__PRETTY_FUNCTION__.0, %ecx
	movl	$4, %edx
	movq	%rsi, %rdi
	call	__assert_fail

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	subq	$8, %rsp
	movl	c(%rip), %eax
	movl	%eax, d(%rip)
	call	k.isra.0
	cmpq	$c, %rax
	movq	%rax, f(%rip)
	sete	%dl
	cmpq	$d, %rax
	sete	%cl
	orb	%cl, %dl
	jne	.L4
	cmpq	$a, %rax
	jne	.L6
.L4:
	call	k.isra.0
	andl	$5, a(%rip)
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
.L6:
	movl	$.LC0, %esi
	movl	$__PRETTY_FUNCTION__.0, %ecx
	movl	$4, %edx
	movq	%rsi, %rdi
	call	__assert_fail

Bisects to r14-2674-gd0de3bf9175


---


### compiler : `gcc`
### title : `[14 Regression] Risc-V rvv testsuite failures caused by conditional move costing`
### open_at : `2023-08-03T16:48:24Z`
### last_modified_date : `2023-08-03T20:58:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110892
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55682
testsuite failures

Seeing 129 new failures on 36 rvv test cases.

Bisected using https://github.com/patrick-rivos/riscv-gnu-toolchain/issues/127

Confirmed to work: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=cf58b896a26390c98ae5382377fcdf44e8ae209f
Confirmed to fail - rv32 newlib: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=e15d0b6680d10d7666195e9db65581364ad5e5df
Confirmed to fail - other linux/newlib risc-v targets: https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=2d73f2eb80caf328bc4dd1324d475e7bf6b56837

Seems to be caused by https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=e15d0b6680d10d7666195e9db65581364ad5e5df "[PATCH 3/5] [RISC-V] RISC-V Conditional Move costing [was:Generate Zicond instruction for select pattern with condition eq or neq to 0]"

Most(all?) failures appear to be regex match failures.


---


### compiler : `gcc`
### title : `RISC-V: Fail to vectorize shift`
### open_at : `2023-08-03T23:18:38Z`
### last_modified_date : `2023-08-07T10:21:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110897
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Since this commit:
https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=29370f1387274ad5a35a020db6a5d06c0324e6c1

causes RISCV a bunch of fails related to shift vectorization:
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv32gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsrl\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/shift-rv64gcv.c scan-assembler-times \\tvsra\\.vv 4
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsra\\.wv 6
FAIL: gcc.target/riscv/rvv/autovec/binop/narrow-1.c scan-assembler-times \\tvnsrl\\.wv 5


When trying this simple case:
#define TEST1_TYPE(TYPE)					\
  __attribute__((noipa))					\
  void vshl_##TYPE (TYPE *dst, TYPE *a, TYPE *b, int n)		\
  {								\
    for (int i = 0; i < n; i++)					\
      dst[i] = a[i] << b[i];					\
  }

#define TEST2_TYPE(TYPE)					\
  __attribute__((noipa))					\
  void vshiftr_##TYPE (TYPE *dst, TYPE *a, TYPE *b, int n)	\
  {								\
    for (int i = 0; i < n; i++)					\
      dst[i] = a[i] >> b[i];					\
  }

#define TEST_ALL()	\
 TEST2_TYPE(uint16_t)	\


TEST_ALL()

rvv.c:23:1: missed: couldn't vectorize loop
rvv.c:23:1: missed: not vectorized: relevant stmt not supported: patt_29 = _4 >> patt_30;


It failed to vectorize.

Could you tell me how to adapt RISC-V port to make it vectorized ?

Thanks.


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Dead Code Elimination Regression`
### open_at : `2023-08-04T17:39:23Z`
### last_modified_date : `2023-08-04T18:37:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110903
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/7of4jjM3K

Given the following code:

void foo(void);
static char b, c;
static short e, f;
static int g = 41317;
static int(a)(int h, int i) { return h + i; }
static int(d)(int h, int i) { return i ? h : 0; }
int main() {
    {
        char j;
        short k;
        for (; g >= 10; g = (short)g) {
            int l = 1, m = 0;
            j = 8 * k;
            k = j <= 0;
            f = c + 3;
            for (; c < 2; c = f) {
                char n = 4073709551615;
                if (!(((m) >= 0) && ((m) <= 0))) {
                    __builtin_unreachable();
                }
                if (g)
                    ;
                else {
                    if ((m = k, (b = a(d(l, k), e) && n) || l) < k) foo();
                    e = l = 0;
                }
            }
        }
    }
}

gcc-trunk -O3 does not eliminate the call to foo:

main:
	movl	g(%rip), %edi
	cmpl	$9, %edi
	jle	.L25
	pushq	%rbp
	movl	%edi, %ecx
	movl	$1, %ebp
	movl	$1, %esi
	pushq	%rbx
	movl	$1, %ebx
	subq	$8, %rsp
	movzbl	c(%rip), %edx
	movsbw	%dl, %ax
	addl	$3, %eax
	movw	%ax, f(%rip)
	cmpb	$1, %dl
	jg	.L12
	.p2align 4,,10
	.p2align 3
.L6:
	testl	%edi, %edi
	je	.L7
	movb	%al, c(%rip)
	movsbw	%al, %dx
	cmpb	$1, %al
	jle	.L6
.L9:
	movswl	%di, %ecx
	movl	%ecx, g(%rip)
	cmpl	$9, %ecx
	jle	.L17
	addl	$3, %edx
	movw	%dx, f(%rip)
.L12:
	movswl	%cx, %eax
	cmpw	$9, %cx
	jle	.L29
.L4:
	jmp	.L4
	.p2align 4,,10
	.p2align 3
.L7:
	movswl	e(%rip), %ecx
	movl	%ebx, %edx
	andl	%esi, %edx
	addl	%ecx, %edx
	orl	%esi, %edx
	jne	.L10
	testb	%bpl, %bpl
	jne	.L30
.L10:
	xorl	%edx, %edx
	movb	%al, c(%rip)
	movw	%dx, e(%rip)
	movsbw	%al, %dx
	cmpb	$1, %al
	jg	.L9
	xorl	%esi, %esi
	jmp	.L6
	.p2align 4,,10
	.p2align 3
.L30:
	call	foo
	movzwl	f(%rip), %eax
	movl	g(%rip), %edi
	jmp	.L10
.L29:
	movl	%eax, g(%rip)
.L17:
	addq	$8, %rsp
	xorl	%eax, %eax
	popq	%rbx
	popq	%rbp
	ret
.L25:
	xorl	%eax, %eax
	ret

gcc-13.2.0 -O3 eliminates the call to foo:

main:
	movl	g(%rip), %esi
	movl	%esi, %ecx
	cmpl	$9, %esi
	jle	.L14
	movzbl	c(%rip), %eax
	movsbw	%al, %dx
	addl	$3, %edx
	movw	%dx, f(%rip)
	cmpb	$1, %al
	jg	.L12
	xorl	%eax, %eax
	testb	%al, %al
	movl	%edx, %eax
	je	.L6
	cmpb	$1, %dl
	jg	.L22
.L7:
	jmp	.L7
	.p2align 4,,10
	.p2align 3
.L22:
	movb	%dl, c(%rip)
.L8:
	movswl	%si, %ecx
	movl	%ecx, g(%rip)
	cmpl	$9, %ecx
	jle	.L14
	addl	$3, %eax
	cbtw
	movw	%ax, f(%rip)
.L12:
	movswl	%cx, %eax
	cmpw	$9, %cx
	jle	.L23
.L4:
	jmp	.L4
	.p2align 4,,10
	.p2align 3
.L6:
	movb	%dl, c(%rip)
	cmpw	$1, %dx
	jg	.L8
	.p2align 4,,10
	.p2align 3
.L9:
	movl	g(%rip), %eax
	testl	%eax, %eax
	jne	.L9
	movw	$0, e(%rip)
	movb	%dl, c(%rip)
.L23:
	movl	%eax, g(%rip)
.L14:
	xorl	%eax, %eax
	ret

Bisects to r14-1597-g64d90d06d2d


---


### compiler : `gcc`
### title : `math-errno in optimization attribute has issues`
### open_at : `2023-08-04T21:01:33Z`
### last_modified_date : `2023-08-04T21:16:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110906
### status : `UNCONFIRMED`
### tags : `missed-optimization, wrong-code`
### component : `middle-end`
### version : `13.2.0`
### severity : `normal`
### contents :
Consider this C++ code compiled with -O3:

double g(double x) {
  return std::sqrt(x);
}

Usually this does call the library function std::sqrt because x might be negative and errno needs to be set accordingly. Moreover, with -fno-math-errno a single sqrtsd instruction is emitted. However, annotating g with

__attribute__((optimize("no-math-errno")))

has no effect. This attribute (and #pragma GCC optimize("no-math-errno") ) used to work up to gcc 5.5.

https://godbolt.org/z/T1nb11bv5


---


### compiler : `gcc`
### title : `Missed optimization: Suboptimal codegen in vector copy assignment`
### open_at : `2023-08-05T05:11:55Z`
### last_modified_date : `2023-08-05T13:37:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110909
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `unknown`
### severity : `normal`
### contents :
#include<vector>

using Container = std::vector<int>;
int copy_assignment(const Container &v1, Container &v2) {
  v2 = v1;
  return 0;
}


I'd expect this to only generate a memcpy. but i'm not sure why memmoves are generated?

$ gcc -std=c++2a -O3  -fno-exceptions

copy_assignment(std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> >&):
        cmp     rsi, rdi
        je      .L21
        push    r13
        push    r12
        push    rbp
        mov     rbp, rdi
        push    rbx
        mov     rbx, rsi
        sub     rsp, 8
        mov     rax, QWORD PTR [rdi+8]
        mov     r13, QWORD PTR [rdi]
        mov     rdx, QWORD PTR [rsi+16]
        mov     rdi, QWORD PTR [rsi]
        mov     r12, rax
        sub     r12, r13
        sub     rdx, rdi
        cmp     rdx, r12
        jb      .L25
        mov     rcx, QWORD PTR [rsi+8]
        mov     rdx, rcx
        sub     rdx, rdi
        cmp     rdx, r12
        jnb     .L26
        cmp     rdx, 4
        jle     .L12
        mov     rsi, r13
        call    memmove
        mov     rcx, QWORD PTR [rbx+8]
        mov     rdi, QWORD PTR [rbx]
        mov     rax, QWORD PTR [rbp+8]
        mov     r13, QWORD PTR [rbp+0]
        mov     rdx, rcx
        sub     rdx, rdi
.L13:
        lea     rsi, [r13+0+rdx]
        sub     rax, rsi
        mov     rdx, rax
        cmp     rax, 4
        jle     .L14
        mov     rdi, rcx
        call    memmove
        mov     rax, QWORD PTR [rbx]
        add     rax, r12
.L8:
        mov     QWORD PTR [rbx+8], rax
        add     rsp, 8
        xor     eax, eax
        pop     rbx
        pop     rbp
        pop     r12
        pop     r13
        ret
.L21:
        xor     eax, eax
        ret
.L25:
        movabs  rax, 9223372036854775804
        cmp     rax, r12
        jb      .L27
        mov     rdi, r12
        call    operator new(unsigned long)
        mov     rbp, rax
        cmp     r12, 4
        jle     .L5
        mov     rdx, r12
        mov     rsi, r13
        mov     rdi, rax
        call    memcpy
.L6:
        mov     rdi, QWORD PTR [rbx]
        test    rdi, rdi
        je      .L7
        mov     rsi, QWORD PTR [rbx+16]
        sub     rsi, rdi
        call    operator delete(void*, unsigned long)
.L7:
        lea     rax, [rbp+0+r12]
        mov     QWORD PTR [rbx], rbp
        mov     QWORD PTR [rbx+16], rax
        jmp     .L8
.L26:
        cmp     r12, 4
        jle     .L10
        mov     rdx, r12
        mov     rsi, r13
        call    memmove
        mov     rax, QWORD PTR [rbx]
        add     rax, r12
        jmp     .L8
.L14:
        lea     rax, [rdi+r12]
        jne     .L8
        mov     edx, DWORD PTR [rsi]
        mov     DWORD PTR [rcx], edx
        jmp     .L8
.L12:
        jne     .L13
        mov     esi, DWORD PTR [r13+0]
        mov     DWORD PTR [rdi], esi
        jmp     .L13
.L10:
        lea     rax, [rdi+r12]
        jne     .L8
        mov     edx, DWORD PTR [r13+0]
        mov     DWORD PTR [rdi], edx
        jmp     .L8
.L5:
        mov     eax, DWORD PTR [r13+0]
        mov     DWORD PTR [rbp+0], eax
        jmp     .L6
.L27:
        call    std::__throw_bad_array_new_length()



Ideally, the above C++ code should translate to an equivalent of the following C++ code:

using Container = std::vector<int>;
int copy_assignment(const Container &v1, Container &v2) {
  v2.reserve(v1.size());
  std::memcpy(&v2[0], &v1[0], v1.size()*sizeof(int));
  // change the size: v2.size() = v1.size()
  return 0;
}


---


### compiler : `gcc`
### title : `vector version of `x == MIN & x > y` is not optimized`
### open_at : `2023-08-05T17:11:12Z`
### last_modified_date : `2023-09-01T12:19:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110915
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Example:
```
#include <limits.h>

#define vector __attribute__((vector_size(sizeof(unsigned)*2)))

vector signed fs(vector signed x, vector signed y)
{
  vector signed t = x == INT_MIN;
  vector signed t1 = x > y;
  return t & t1;
}
vector signed fu(vector unsigned x, vector unsigned y)
{
  vector signed t = x == 0;
  vector signed t1 = x > y;
  return t & t1;
}

 signed f2_1( signed x,  signed y)
{
   signed t = x == INT_MIN;
   signed t1 = x > y;
  return t & t1;
}
```

I am filing this as I am fixing the pointer version (PR 96695) and don't want to lose track of this either.


---


### compiler : `gcc`
### title : `Architecture-dependent missed optimizations for double swapping`
### open_at : `2023-08-05T22:44:08Z`
### last_modified_date : `2023-08-06T12:40:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110916
### status : `NEW`
### tags : `alias, missed-optimization, TREE`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
GCC's ability to eliminate redundant stores and loads is oddly dependent on the architecture. Even on the same overall arch, compiling for Skylake in particular performs always performs best.

On x86_64 -march=x86-64-v2, GCC 11 provides the optimal output. GCC 12/13/14 provide suboptimal output compared to -march=skylake.

On ARM64, a strange load and store from/to the same register is emitted. This is the case for all version of GCC available on Compiler Explorer.

## Code to Reproduce (https://godbolt.org/z/d7Kcdn8fo)

static void swap(int* restrict a, int* restrict b) {
    const int tmp = *a;
    *a = *b;
    *b = tmp;
}

void double_swap_alias(int* a, int* b) {
    swap(a, b);
    swap(a, b);
}

## Expected Output (x86_64 GCC 14 -O3 -march=skylake)

ret


## Actual Output (x86_64 GCC 14 -O3 -march=x86-64-v2)

mov     edx, DWORD PTR [rsi]
mov     eax, DWORD PTR [rdi]
mov     DWORD PTR [rdi], edx
mov     DWORD PTR [rsi], eax
mov     edx, DWORD PTR [rdi]
mov     DWORD PTR [rdi], eax
mov     DWORD PTR [rsi], edx
ret


## Actual Output (ARM64 GCC 14 -O3)

ldr     w0, [x1]
str     w0, [x1]
ret


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O3 since r14-2331-g018e7f16408`
### open_at : `2023-08-06T10:58:51Z`
### last_modified_date : `2023-08-25T19:07:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110918
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static char b = 53;
static unsigned c;
void foo(void);
static int(a)(int d, int e) { return (d ^ e) < 0 ? d : d - e; }
int main() {
  {
    int f = 2;
    c = b;
    b = 0;
    for (; c <= 6;) {
      if (f >= 2)
        f = 0;
      for (; f >= -9; f = a(f, 8))
        if (!(f >= -8 && f <= 0))
          foo();
    }
  }
}

gcc-87b0749cfb9 (trunk) -O3 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O3 can.
-----------------------------------------------------------------------
gcc-87b0749cfb9406790b108208b466cf507ae3c431 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movsbl	b(%rip), %eax
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	xorl	%ebx, %ebx
	movb	$0, b(%rip)
	movl	%eax, c(%rip)
	cmpl	$6, %eax
	ja	.L11
	.p2align 4,,10
	.p2align 3
.L2:
	leal	8(%rbx), %eax
	cmpl	$8, %eax
	ja	.L16
.L3:
	cmpl	$-8, %ebx
	je	.L5
	cmpl	$-1, %ebx
	jl	.L6
	movl	$-8, %ebx
	leal	8(%rbx), %eax
	cmpl	$8, %eax
	jbe	.L3
.L16:
	call	foo
	subl	$8, %ebx
	jmp	.L2
.L5:
	jmp	.L5
	.p2align 4,,10
	.p2align 3
.L6:
	cmpl	$6, c(%rip)
	ja	.L11
.L8:
	jmp	.L8
	.p2align 4,,10
	.p2align 3
.L11:
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	movsbl	b(%rip), %edx
	xorl	%eax, %eax
	movb	$0, b(%rip)
	movl	%edx, c(%rip)
	cmpl	$6, %edx
	ja	.L8
	.p2align 4,,10
	.p2align 3
.L2:
	testl	%eax, %eax
	js	.L3
	subl	$8, %eax
	cmpl	$-9, %eax
	jge	.L2
	cmpl	$6, c(%rip)
	ja	.L8
.L6:
	jmp	.L6
.L3:
	jmp	.L3
	.p2align 4,,10
	.p2align 3
.L8:
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-2331-g018e7f16408


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O3 since r14-1691-gbc5a2c2e793`
### open_at : `2023-08-06T11:05:46Z`
### last_modified_date : `2023-09-15T04:44:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110919
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static short a, b;
static int e;
static int *j = &e;
void bar151_(void);
void foo(void);
void bar162_(void);
static int(d)(int k, int l) { return l == k && l - 1 ? k : 0; }
static void f();
static void g(unsigned short, unsigned);
static void h() {
  g(0, 4);
  f();
  int i = -1;
  g(0, i);
}
static void f(int k) { g(k, 0); }
static void g(unsigned short k, unsigned l) {
  if (k)
    bar151_();
  if (l)
    if (k)
      bar162_();
  short c = l;
  b = c == 0 || a;
  if (d(*j, b))
    foo();
}
int main() { h(); }

gcc-87b0749cfb9 (trunk) -O3 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O3 can.
-----------------------------------------------------------------------
gcc-87b0749cfb9406790b108208b466cf507ae3c431 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB4:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	$4, %esi
	xorl	%edi, %edi
	call	g
	xorl	%esi, %esi
	xorl	%edi, %edi
	call	g
	movl	$-1, %esi
	xorl	%edi, %edi
	call	g
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB4:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	$4, %esi
	xorl	%edi, %edi
	call	g
	xorl	%esi, %esi
	xorl	%edi, %edi
	call	g
	movl	$-1, %esi
	xorl	%edi, %edi
	call	g
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-1691-gbc5a2c2e793


---


### compiler : `gcc`
### title : ``(a == b) & (a == c) & (b != c)` is not optimized to false`
### open_at : `2023-08-06T17:18:20Z`
### last_modified_date : `2023-08-21T07:46:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110922
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
I noticed this while looking at PR 110919:
```
int f(int a, int b)
{
  _Bool t = a == b;
  _Bool t1 = a == 0;
  _Bool t2 = b != 0;
  return t&t1&t2;
}
```
is not optimized to false.


---


### compiler : `gcc`
### title : `Unnecessary dynamic initialization in trivial cases`
### open_at : `2023-08-06T23:27:40Z`
### last_modified_date : `2023-08-06T23:36:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110925
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `14.0`
### severity : `normal`
### contents :
## Code to Reproduce

int z = 0;
int x = z;

## Expected Output (delivered by clang trunk -O3)

x:
        .zero   4
z:
        .zero   4

## Actual Output (x86_64 GCC 14 -O3) (https://godbolt.org/z/95d9hj3he)

_GLOBAL__sub_I_z:
        mov     eax, DWORD PTR z[rip]
        mov     DWORD PTR x[rip], eax
        ret
x:
        .zero   4
z:
        .zero   4


## Explanation

The implementation is allowed to turn this into static initialization. See https://eel.is/c++draft/basic.start.static#3. However, GCC emits unnecessary dynamic initialization code.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-2890-gcc2003cd875`
### open_at : `2023-08-07T10:33:57Z`
### last_modified_date : `2023-09-14T18:18:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110931
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/zPTeo8Mj9

Given the following code:

void foo(void);
static int a = 7, c;
static int *d = &a, *e;
static int **f = &e;
static short g;
static char(h)(char b) {
    if (!(((b) >= 1) && ((b) <= 1))) {
        __builtin_unreachable();
    }
    return 0;
}
static void k(unsigned i) {
    short j = i;
    if (j)
        ;
    else {
        h(i);
        if ((e && i) <= 0)
            ;
        else
            foo();
        *f = &c;
    }
    h(0 >= 0);
}
int main() {
    int l = *d;
    g = l;
    k(g);
}

gcc-trunk -O3 does not eliminate the call to foo:

main:
	cmpw	$0, a(%rip)
	jne	.L5
	cmpq	$0, e(%rip)
	je	.L6
	pushq	%rax
	call	foo
	xorl	%eax, %eax
	movq	$c, e(%rip)
	popq	%rdx
	ret
.L6:
	movq	$c, e(%rip)
.L5:
	xorl	%eax, %eax
	ret

gcc-13.2.0 -O3 eliminates the call to foo:

main:
	xorl	%eax, %eax
	ret

Bisects to r14-2890-gcc2003cd875


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-2230-g7e904d6c7f2`
### open_at : `2023-08-07T10:42:45Z`
### last_modified_date : `2023-09-15T04:44:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110932
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/fs5obnhPP

Given the following code:

void foo(void);
static short a;
static int b, c, f, h, i, k;
static char d, m;
static int *j, *o = &f;
static int **l = &j, **n = &j;
static unsigned p(short g, short) { return g; }
static short(q)(unsigned e) {
    if (!(((e) >= 1) && ((e) <= 65531))) {
        __builtin_unreachable();
    }
    return 0;
}
static short r() {
    q(&k != 0 | f);
    *l = o;
    return h;
}
static int *s() {
    m = c ?: d ?: m;
    b = b ?: a;
    a = 0;
    return *l;
}
int main() {
    char t = 0;
    for (; t <= 4; t = t + 2) {
        int *u = &i;
        if (p(~(r() <= t), *u) > i == i) {
            **n = -5L;
            if (*u) continue;
            **n = 0;
            if (t) foo();
            break;
        }
        s();
        *u = u != 0;
    }
    s();
    h = 0;
}

gcc-trunk -O3 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	movl	h(%rip), %ecx
	xorl	%edx, %edx
	movl	i(%rip), %eax
	movq	$f, j(%rip)
	testw	%cx, %cx
	setle	%dl
	notl	%edx
	movswl	%dx, %edx
	cmpl	%edx, %eax
	setb	%dl
	movzbl	%dl, %edx
	cmpl	%eax, %edx
	jne	.L22
	testl	%eax, %eax
	jne	.L23
	xorl	%edx, %edx
	movl	%edx, f(%rip)
	jmp	.L14
.L22:
	xorl	%eax, %eax
	call	s.isra.0
	movl	$1, %eax
	movl	$1, i(%rip)
.L6:
	movq	$f, j(%rip)
	xorl	%edx, %edx
	cmpw	$2, %cx
	setle	%dl
	notl	%edx
	movswl	%dx, %edx
	cmpl	%edx, %eax
	setb	%dl
	movzbl	%dl, %edx
	cmpl	%edx, %eax
	je	.L8
	xorl	%eax, %eax
	call	s.isra.0
	movl	$1, %eax
	movl	$1, i(%rip)
.L9:
	movq	$f, j(%rip)
	xorl	%edx, %edx
	cmpw	$4, %cx
	setle	%dl
	notl	%edx
	movswl	%dx, %edx
	cmpl	%edx, %eax
	setb	%dl
	movzbl	%dl, %edx
	cmpl	%edx, %eax
	jne	.L11
	testl	%eax, %eax
	je	.L10
	movl	$-5, f(%rip)
	jmp	.L14
.L8:
	testl	%eax, %eax
	jne	.L24
.L10:
	xorl	%eax, %eax
	movl	%eax, f(%rip)
	call	foo
.L14:
	xorl	%eax, %eax
	call	s.isra.0
	xorl	%eax, %eax
	movl	$0, h(%rip)
	addq	$8, %rsp
	ret
.L11:
	xorl	%eax, %eax
	call	s.isra.0
	movl	$1, i(%rip)
	jmp	.L14
.L23:
	movl	$-5, f(%rip)
	jmp	.L6
.L24:
	movl	$-5, f(%rip)
	jmp	.L9

gcc-13.2.0 -O3 eliminates the call to foo:

main:
	xorl	%eax, %eax
	cmpw	$0, h(%rip)
	movl	i(%rip), %edx
	movq	$f, j(%rip)
	setle	%al
	notl	%eax
	cwtl
	cmpl	%eax, %edx
	setb	%al
	movzbl	%al, %eax
	cmpl	%edx, %eax
	jne	.L11
	testl	%edx, %edx
	jne	.L6
.L7:
	xorl	%eax, %eax
	movl	%edx, f(%rip)
	call	s.isra.0
	xorl	%eax, %eax
	movl	$0, h(%rip)
	ret
.L11:
	xorl	%eax, %eax
	call	s.isra.0
	movl	$1, i(%rip)
.L6:
	movq	$f, j(%rip)
	movl	$-5, %edx
	jmp	.L7

Bisects to r14-2230-g7e904d6c7f2


---


### compiler : `gcc`
### title : `Missed BB reduction vectorization because of missed eliding of a permute`
### open_at : `2023-08-07T13:28:26Z`
### last_modified_date : `2023-09-12T07:43:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110935
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `normal`
### contents :
double vals[16];
double test ()
{
  vals[0]++;
  return vals[2] + vals[4] + vals[1] + vals[3];
}

has the reduction not vectorized with -ffast-math because

t.c:5:38: note:   === vect_slp_analyze_operations ===
t.c:5:38: note:   ==> examining statement: _8 = vals[3];
t.c:5:38: missed:   BB vectorization with gaps at the end of a load is not supported
t.c:5:44: missed:   not vectorized: relevant stmt not supported: _8 = vals[3];
t.c:5:38: note:   removing SLP instance operations starting from: _11 = _7 + _8;
t.c:5:38: missed:  not vectorized: bad operation in basic block.

we fail to elide the load permutation (BB vect allows a consecutive
sub-set):

t.c:5:38: note:   Final SLP tree for instance 0x51c8d60:
t.c:5:38: note:   node 0x5285860 (max_nunits=2, refcnt=2) vector(2) double
t.c:5:38: note:   op template: _8 = vals[3];
t.c:5:38: note:         stmt 0 _8 = vals[3];
t.c:5:38: note:         stmt 1 _6 = vals[1];
t.c:5:38: note:         stmt 2 _3 = vals[2];
t.c:5:38: note:         stmt 3 _4 = vals[4];
t.c:5:38: note:         load permutation { 3 1 2 4 }
t.c:5:38: note:    === vect_match_slp_patterns ===
t.c:5:38: note:    Analyzing SLP tree 0x5285860 for patterns
t.c:5:38: note:  SLP optimize permutations:
t.c:5:38: note:    1: { 2, 0, 1, 3 }
t.c:5:38: note:  SLP optimize partitions:
t.c:5:38: note:    -------------
t.c:5:38: note:    partition 0 (layout 0):
t.c:5:38: note:      nodes:
t.c:5:38: note:        - 0x5285860:
t.c:5:38: note:            weight: 1.000000
t.c:5:38: note:            op template: _8 = vals[3];
t.c:5:38: note:      edges:
t.c:5:38: note:      layout 0: (*)
t.c:5:38: note:          {depth: 0.000000, total: 0.000000}
t.c:5:38: note:        + {depth: 1.000000, total: 1.000000}
t.c:5:38: note:        + {depth: 0.000000, total: 0.000000}
t.c:5:38: note:        = {depth: 1.000000, total: 1.000000}
t.c:5:38: note:      layout 1:
t.c:5:38: note:          {depth: 0.000000, total: 0.000000}
t.c:5:38: note:        + {depth: 1.000000, total: 1.000000}
t.c:5:38: note:        + {depth: 0.000000, total: 0.000000}
t.c:5:38: note:        = {depth: 1.000000, total: 1.000000}
t.c:5:38: note:  recording new base alignment for &vals
  alignment:    32
  misalignment: 0
  based on:     _1 = vals[0];
t.c:5:38: note:   === vect_slp_analyze_instance_alignment ===


---


### compiler : `gcc`
### title : `(bool0 ? bool1^1 : bool1) is not optimized to bool0 ^ bool1`
### open_at : `2023-08-07T16:52:17Z`
### last_modified_date : `2023-08-09T19:27:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110937
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
_Bool f2(_Bool a, _Bool b)
{
        if (a)
          return !b;
        return b;
}
```
This should be optimized to just:
```
_Bool f2_(_Bool a, _Bool b)
{
  return a ^ b;
}
```


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O3 since r14-2379-gc496d15954c`
### open_at : `2023-08-08T09:11:11Z`
### last_modified_date : `2023-09-16T07:46:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110941
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static int a;
void foo(void);
void bar349_(void);
void bar363_(void);
void bar275_(void);
int main() {
  {
    {
      short b = 26;
      for (; b >= 1; b = b - 4) {
        if (b >= 2 && b <= 26)
          bar275_();
        if (a)
          bar363_();
        if (a)
          bar349_();
        int c = b;
        if (!(c >= 2 && c <= 26))
          foo();
      }
    }
    a = 0;
  }
}

gcc-25c4b1620eb (trunk) -O3 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O3 can.
-----------------------------------------------------------------------
gcc-25c4b1620ebc10fceabd86a34fdbbaf8037e7e82 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$24, %ebx
	.p2align 4,,10
	.p2align 3
.L8:
	cmpw	$24, %bx
	jbe	.L2
	movl	a(%rip), %edx
	testl	%edx, %edx
	jne	.L3
.L7:
	call	foo
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	movl	$0, a(%rip)
	ret
	.p2align 4,,10
	.p2align 3
.L3:
	.cfi_restore_state
	call	bar363_
	movl	a(%rip), %eax
	testl	%eax, %eax
	je	.L7
	call	bar349_
	jmp	.L7
.L2:
	call	bar275_
	cmpl	$0, a(%rip)
	jne	.L15
	.p2align 4,,10
	.p2align 3
.L6:
	subl	$4, %ebx
	cmpl	$-4, %ebx
	jne	.L8
	movl	$0, a(%rip)
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L15:
	.cfi_restore_state
	call	bar363_
	cmpl	$0, a(%rip)
	je	.L6
	call	bar349_
	cmpl	$24, %ebx
	jbe	.L6
	jmp	.L7
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$24, %ebx
.L10:
	cmpw	$24, %bx
	jbe	.L2
	movl	a(%rip), %edx
	testl	%edx, %edx
	jne	.L17
.L3:
	movl	$0, a(%rip)
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L17:
	.cfi_restore_state
	call	bar363_
	movl	a(%rip), %eax
	testl	%eax, %eax
	jne	.L4
.L15:
	subl	$4, %ebx
	jmp	.L10
	.p2align 4,,10
	.p2align 3
.L4:
	call	bar349_
	jmp	.L15
.L2:
	call	bar275_
	cmpl	$0, a(%rip)
	jne	.L18
	.p2align 4,,10
	.p2align 3
.L9:
	subl	$4, %ebx
	cmpw	$-4, %bx
	jne	.L10
	jmp	.L3
.L18:
	call	bar363_
	cmpl	$0, a(%rip)
	je	.L9
	call	bar349_
	jmp	.L9
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-2379-gc496d15954c


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O3 since r14-1165-g257c2be7ff8`
### open_at : `2023-08-08T09:24:18Z`
### last_modified_date : `2023-08-16T20:04:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110942
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static int b, c;
static short d;
static char e;
void bar49_(void);
void bar115_(void);
void bar42_(void);
void foo(void);
static short(a)(short f, short g) { return f + g; }
static int h() {
  short i = 0;
  char j;
  int k;
l:
  k = 0;
  if (i)
    bar42_();
  i = 0;
  for (; i != 8; i = a(i, 8)) {
    if (e)
      bar49_();
    if (0 >= i)
      c = 0;
    if (!k) {
      j = 6;
      for (; j >= 0; j--) {
        k = 0;
        for (; k <= 0; k++) {
          if (!(j >= 5))
            bar115_();
          for (; d; d = 0) {
            if (!j)
              foo();
            if (j)
              break;
          }
          if (i)
            break;
        }
        if (c)
          return j;
        c = 2;
        if (b)
          goto l;
      }
    }
  }
  return 0;
}
int main() { h(); }

gcc-25c4b1620eb (trunk) -O3 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O3 can.
-----------------------------------------------------------------------
gcc-25c4b1620ebc10fceabd86a34fdbbaf8037e7e82 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	xorl	%eax, %eax
	movl	$6, %ebx
.L2:
	movl	%eax, c(%rip)
	cmpb	$4, %bl
	jne	.L18
.L7:
	call	bar115_
	cmpw	$0, d(%rip)
	je	.L8
	testb	%bl, %bl
	jne	.L8
	call	foo
	movl	c(%rip), %edx
	xorl	%eax, %eax
	movw	%ax, d(%rip)
	testl	%edx, %edx
	jne	.L5
	movl	$2, c(%rip)
.L5:
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L18:
	.cfi_restore_state
	testl	%eax, %eax
	jne	.L5
	subl	$1, %ebx
	movl	$2, %eax
	jmp	.L2
.L8:
	cmpl	$0, c(%rip)
	jne	.L5
	movl	$2, c(%rip)
	subl	$1, %ebx
	cmpb	$-1, %bl
	jne	.L7
	jmp	.L5
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	movl	$2, c(%rip)
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-1165-g257c2be7ff8


---
