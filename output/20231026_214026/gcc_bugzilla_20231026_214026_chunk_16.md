### Total Bugs Detected: 4649
### Current Chunk: 16 of 30
### Bugs in this Chunk: 160 (From bug 2401 to 2560)
---


### compiler : `gcc`
### title : `x86 lacks vector extend / truncate`
### open_at : `2019-11-25T13:38:32Z`
### last_modified_date : `2023-06-03T00:09:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92658
### status : `NEW`
### tags : `easyhack, missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
After

2019-11-14  Richard Sandiford  <richard.sandiford@arm.com>

        * tree-cfg.c (verify_gimple_assign_unary): Handle conversions
        between vector types.
        * tree-vect-stmts.c (vectorizable_conversion): Extend the
        non-widening and non-narrowing path to handle standard
        conversion codes, if the target supports them.
        * expr.c (convert_move): Try using the extend and truncate optabs
        for vectors.
        * optabs-tree.c (supportable_convert_operation): Likewise.
        * config/aarch64/iterators.md (Vnarroqw): New iterator.
        * config/aarch64/aarch64-simd.md (<optab><Vnarrowq><mode>2)
        (trunc<mode><Vnarrowq>2): New patterns.

it would now be possible to BB vectorize the following but the x86 backend
lacks appropriate patterns for the vector mode variants (here V8HI -> V8QI
narrowing).

typedef unsigned char v16qi __attribute__((vector_size(16)));
typedef unsigned short v8hi __attribute__((vector_size(16)));

void bar (v8hi *dst, v16qi * __restrict src)
{
  unsigned short tem[8];
  tem[0] = (*src)[0];
  tem[1] = (*src)[1];
  tem[2] = (*src)[2];
  tem[3] = (*src)[3];
  tem[4] = (*src)[4];
  tem[5] = (*src)[5];
  tem[6] = (*src)[6];
  tem[7] = (*src)[7];
  dst[0] = *(v8hi *)tem;
}
void foo (v8hi *dst, v16qi src)
{
  unsigned short tem[8];
  tem[0] = src[0];
  tem[1] = src[1];
  tem[2] = src[2];
  tem[3] = src[3];
  tem[4] = src[4];
  tem[5] = src[5];
  tem[6] = src[6];
  tem[7] = src[7];
  dst[0] = *(v8hi *)tem;
}


---


### compiler : `gcc`
### title : `[AArch64] low lanes select not optimized out for vmlal intrinsics`
### open_at : `2019-11-25T18:59:10Z`
### last_modified_date : `2021-01-29T08:43:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92665
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
With gcc as of today I see dup instructions that could be optimized out:

$ cat red.c
#include "arm_neon.h"

int32x4_t fun(int32x4_t a, int16x8_t b, int16x8_t c) {
  a = vmlal_s16(a, vget_low_s16(b), vget_low_s16(c));
  a = vmlal_high_s16(a, b, c);
  return a;
}

$ gcc -O3 -S -o- red.c
fun:
	dup	d3, v1.d[0]
	dup	d4, v2.d[0]
	smlal v0.4s,v3.4h,v4.4h
	smlal2 v0.4s,v1.8h,v2.8h
	ret

$ clang -O3 -S -o- red.c
fun:
	smlal	v0.4s, v1.4h, v2.4h
	smlal2	v0.4s, v1.8h, v2.8h
	ret


---


### compiler : `gcc`
### title : `SRA confuses FRE`
### open_at : `2019-11-28T12:08:32Z`
### last_modified_date : `2022-11-16T12:13:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92706
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Reduced from one of the issues in PR92645

struct S { int i[4]; } __attribute__((aligned(128)));
typedef __int128_t my_int128 __attribute__((may_alias));
__int128_t load (void *p)
{
  struct S v;
  __builtin_memcpy (&v, p, sizeof (struct S));
  struct S u;
  u = v;
  struct S w;
  w = u;
  return *(my_int128 *)&w;
}


---


### compiler : `gcc`
### title : `GCC 10 libxul.so -fprofile-generate binary is 360MB while clang needs only 163MB.`
### open_at : `2019-11-28T16:03:31Z`
### last_modified_date : `2020-01-30T10:59:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92711
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
It seems that profiling became more expensive in GCC10 compared to clang or previous GCC releases.
Clang binary is here https://firefox-ci-tc.services.mozilla.com/api/queue/v1/task/H_iSouCVTha9mEw9y5XO5Q/runs/0/artifacts/public/build/target.tar.bz2
more or less comparable GCC build is here 
https://firefox-ci-tc.services.mozilla.com/api/queue/v1/task/NOUqVShcSMaJn5j3g5nEYg/runs/0/artifacts/public/build/target.tar.bz2
It also seems that profile streaming is slower in GCC build (which is important since Firefox forks multiple times on startup and then when creating new tab and that triggers profile data streamout).


---


### compiler : `gcc`
### title : `[9 Regression] Performance regression with assumed values`
### open_at : `2019-11-28T17:18:53Z`
### last_modified_date : `2022-05-27T08:40:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92712
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
The following code generates progressively worse code from GCC 7.5 to GCC 8.3 to GCC 9.1 (and trunk):

static void func_base(int t, const int v) {
    int x = 0;
    for (int i = 0; i < t; ++i) {
        x += v;
    }
    volatile int d = x;
}

void func_default(int t, const int v) {
    func_base(t, v);
}

void func_assumed(int t, const int v) {
    if (t < 0) __builtin_unreachable();
    func_base(t, v);
}

On GCC 7.5 (-O2):

func_default(int, int):
  test edi, edi
  jle .L3
  imul edi, esi
  mov DWORD PTR [rsp-4], edi
  ret
.L3:
  xor edi, edi
  mov DWORD PTR [rsp-4], edi
  ret
func_assumed(int, int):
  imul edi, esi
  mov DWORD PTR [rsp-4], edi
  ret

On GCC 8.3 (-O2):

func_default(int, int):
  test edi, edi
  jle .L3
  imul edi, esi
  mov DWORD PTR [rsp-4], edi
  ret
.L3:
  xor edi, edi
  mov DWORD PTR [rsp-4], edi
  ret
func_assumed(int, int):
  test edi, edi
  je .L6
  imul edi, esi
.L6:
  mov DWORD PTR [rsp-4], edi
  ret

On GCC 9.1 and trunk (-O2):

func_default(int, int):
  test edi, edi
  jle .L3
  sub edi, 1
  imul edi, esi
  add esi, edi
  mov DWORD PTR [rsp-4], esi
  ret
.L3:
  xor esi, esi
  mov DWORD PTR [rsp-4], esi
  ret
func_assumed(int, int):
  test edi, edi
  je .L6
  sub edi, 1
  imul edi, esi
  add edi, esi
.L6:
  mov DWORD PTR [rsp-4], edi
  ret

This occurs regardless of if `func_base` is allowed to inline, or if it is manually inlined.

It does not occur in LLVM-Clang or in Microsoft Visual C++.


---


### compiler : `gcc`
### title : `[missed-optimization] aggregate initialization of an array fills the whole array with zeros first, including leading non-zero elements`
### open_at : `2019-11-28T18:39:08Z`
### last_modified_date : `2021-12-22T10:06:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92714
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.1.0`
### severity : `enhancement`
### contents :
void *sink;
void bar() {
    int a[100]{1,2,3,4};
    sink = a;             // a escapes the function
    asm("":::"memory");   // and compiler memory barrier
    // forces the compiler to materialize a[] in memory instead of optimizing away
}

gcc 8.1 and gcc 9.2 both make asm like this (even with -O3):

bar():
    push    edi                       # save call-preserved EDI which rep stos uses
    xor     eax, eax                  # eax=0
    mov     ecx, 100                  # repeat-count = 100
    sub     esp, 400                  # reserve 400 bytes on the stack
    mov     edi, esp                  # dst for rep stos
        mov     DWORD PTR sink, esp       # sink = a
    rep stosd                         # memset(a, 0, 400) 

    mov     DWORD PTR [esp], 1        # then store the non-zero initializers
    mov     DWORD PTR [esp+4], 2      # over the zeroed part of the array
    mov     DWORD PTR [esp+8], 3
    mov     DWORD PTR [esp+12], 4

    add     esp, 400                  # cleanup the stack
    pop     edi                       # and restore caller's EDI
    ret


---


### compiler : `gcc`
### title : `-Os doesn't inline byteswap function even though it's a single instruction`
### open_at : `2019-11-28T20:29:49Z`
### last_modified_date : `2023-08-08T15:57:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92716
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `enhancement`
### contents :
I compiled the following test code for both x86_64 and aarch64 on gcc 8.3.0:

static inline unsigned int byteswap(unsigned int x)                                              
{                                                                                                
        return (((x >> 24) & 0xff) << 0) |                                                       
               (((x >> 16) & 0xff) << 8) |                                                       
               (((x >> 8) & 0xff) << 16) |                                                       
               (((x >> 0) & 0xff) << 24);                                                        
}                                                                                                
                                                                                                 
unsigned int test(unsigned int a, unsigned int b, unsigned int c) {                              
        return byteswap(a) + byteswap(b) + byteswap(c);                                                                                                                                 
}

On x86_64 I get:

0000000000000000 <byteswap> (File Offset: 0x40):
   0:   89 f8                   mov    %edi,%eax
   2:   0f c8                   bswap  %eax
   4:   c3                      retq   

0000000000000005 <test> (File Offset: 0x45):
   5:   e8 f6 ff ff ff          callq  0 <byteswap> (File Offset: 0x40)
   a:   89 f7                   mov    %esi,%edi
   c:   89 c1                   mov    %eax,%ecx
   e:   e8 ed ff ff ff          callq  0 <byteswap> (File Offset: 0x40)
  13:   89 d7                   mov    %edx,%edi
  15:   01 c1                   add    %eax,%ecx
  17:   e8 e4 ff ff ff          callq  0 <byteswap> (File Offset: 0x40)
  1c:   01 c8                   add    %ecx,%eax
  1e:   c3                      retq   

And on aarch64 I get:

0000000000000000 <byteswap> (File Offset: 0x40):
   0:   5ac00800        rev     w0, w0
   4:   d65f03c0        ret

0000000000000008 <test> (File Offset: 0x48):
   8:   a9bf7bfd        stp     x29, x30, [sp,#-16]!
   c:   910003fd        mov     x29, sp
  10:   97fffffc        bl      0 <byteswap> (File Offset: 0x40)
  14:   2a0003e3        mov     w3, w0
  18:   2a0103e0        mov     w0, w1
  1c:   97fffff9        bl      0 <byteswap> (File Offset: 0x40)
  20:   0b000063        add     w3, w3, w0
  24:   2a0203e0        mov     w0, w2
  28:   97fffff6        bl      0 <byteswap> (File Offset: 0x40)
  2c:   0b000060        add     w0, w3, w0
  30:   a8c17bfd        ldp     x29, x30, [sp],#16
  34:   d65f03c0        ret

So the good news is that GCC recognized this code as a byteswap function that can be implemented with a single instruction on both of these platforms. The bad news is that it then doesn't seem to realize that inlining this single instruction leads to smaller code size than wrapping it in a function and calling it, even if it is called many times. If I instead compile with -O2, the function is inlined as expected. (I also tried with clang 8.0.1 which manages to inline correctly even with -Os.)


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Bogus Wstringop-overflow in __builtin_memset() of an element of array of size 1 of struct`
### open_at : `2019-11-28T21:28:26Z`
### last_modified_date : `2023-07-07T10:36:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92718
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `8.3.0`
### severity : `normal`
### contents :
Created attachment 47392
Minimized test case

Gcc is emitting bogus, or maybe pessimistic, Wstringop-overflow or
Warray-bounds warnings (depending on warning flags).

$ cat min.c
struct s {
	int x;
};

extern int n;
struct s a[1];

void
f(void)
{
	struct s *ps;
	int i;

	for (i = 0; i < n; i++) {
		ps = &a[i];
		__builtin_memset(ps, 0, sizeof(*ps));
		ps->x = 1;
	}
}

$ /tmp/gcc/bin/bin/gcc -O -c min.c
min.c: In function ‘f’:
min.c:16:3: warning: ‘__builtin_memset’ writing 4 bytes into a region of size 0 overflows the destination [-Wstringop-overflow=]
   16 |   __builtin_memset(ps, 0, sizeof(*ps));
      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

$ /tmp/gcc/bin/bin/gcc -Wall -O -c min.c
min.c: In function ‘f’:
min.c:16:3: warning: ‘__builtin_memset’ offset [4, 7] is out of the bounds [0, 4] of object ‘a’ with type ‘struct s[1]’ [-Warray-bounds]
   16 |   __builtin_memset(ps, 0, sizeof(*ps));
      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
min.c:6:10: note: ‘a’ declared here
    6 | struct s a[1];
      |          ^

My unprofessional opinion is that it seems like the compiler is emitting
a warning for when i >= 1, but the compiler does not know whether that
is actually possible.  Note that a warning is not emitted if any of
these changes are made:
 - The size of array "a" is changed from 1 to 2.
 - A check of "n" against the array size is inserted:
	if (n > 1)
		return;
 - The assignment of "x" after __builtin_memset() is deleted.
 - The array of struct is changed to array of int.

The warning is emitted as -Wstringop-overflow with the default warning
options.  In latest source, with -Wall, the warning is emitted as
Warray-bounds, but in previous versions (I checked 8.3.0) -Wall emits
Wstringop-overflow.  I'm not sure when this aspect may have changed.

I have tested this case and seen a warning on:
 - gcc 7.4.0
 - gcc 8.3.0
 - gcc current sources (r278812, "10.0.0")

I looked through the bugs linked to bug 88443 and was not immediately
able to identify this as a duplicate of one.  Apologies if it is.


---


### compiler : `gcc`
### title : `gcc considers "padding" byte of empty lambda to be uninitialized`
### open_at : `2019-11-29T06:54:03Z`
### last_modified_date : `2022-03-11T00:32:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92722
### status : `RESOLVED`
### tags : `c++-lambda, diagnostic, missed-optimization`
### component : `c++`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 47394
testcase

seastar has a variant of std::function that cannot be copied and is called noncopyable_function. It also has a small buffer so that small functions don't need an indirection.

The implementation is careful to only move sizeof(Func) bytes to avoid using uninitialized memory. It looks like gcc is careful enough to consider padding bytes to be initialized, but when given an empty lambda, sizeof(Func) is 1 but no bytes are actually used and gcc ends up issuing a warning.

A reduced testcase is attached. Compiling with g++ -S failure_injector_test.ii -O1 -Wall produces the warning:

failure_injector_test.ii:22:27: warning: ‘a.noncopyable_function::direct[0]’ is used uninitialized in this function [-Wuninitialized]

This seems to be generated because

    noncopyable_function(Func&& func) {
        static_assert(sizeof(Func) == 1);
        new (reinterpret_cast<Func*>(direct)) Func(std::move(func));
    }

Doesn't initialize any bytes, but sizeof(Func) is 1, so trivial_direct_move still uses direrct[0]


---


### compiler : `gcc`
### title : `Missing match.pd simplification done by fold_binary_loc on generic`
### open_at : `2019-11-29T19:33:57Z`
### last_modified_date : `2019-12-04T09:39:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92734
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
While working on the PR92712 testcases, I had to remove one testcase I wrote, because we haven't simplified stuff in GIMPLE in that case (we do on RTL):
int
foo (int t)
{
  return 1 - (int) (1U - t);
}

int
bar (int t)
{
  int a = 1U - t;
  return 1 - a;
}

The first function is optimized into return t during fold_binary_loc associate:
but there isn't anything in match.pd that handles this (or should it be some special pass?).


---


### compiler : `gcc`
### title : `[10 regression] Large code size growth for -O2 binaries between 2019-05-19...2019-05-29`
### open_at : `2019-11-30T19:12:21Z`
### last_modified_date : `2019-12-20T23:51:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92738
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `induct2 (from polyhedron) regresses 267% with -O2 -ftree-vectorize -ftree-slp-vectorize -fvect-cost-modes=dynamic or cheap compared to -O2`
### open_at : `2019-12-01T01:06:14Z`
### last_modified_date : `2021-08-25T02:06:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92740
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
This is on zen2 hardware.


---


### compiler : `gcc`
### title : `[10 Regression] ICE: verify_gimple failed (error: invalid vector types in nop conversion)`
### open_at : `2019-12-01T04:56:14Z`
### last_modified_date : `2019-12-02T17:52:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92741
### status : `RESOLVED`
### tags : `ice-on-valid-code, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
gcc-10.0.0-alpha20191124 snapshot (r278660) ICEs when compiling the following testcase reduced from testsuite/gcc.target/i386/sse2-mmx-pinsrw.c w/ -O2 -fexceptions -fnon-call-exceptions -fno-inline:

typedef int vh __attribute__ ((__vector_size__ (2 * sizeof (int))));
typedef short int cq __attribute__ ((__vector_size__ (4 * sizeof (short int))));

static void
id (int *r8, vh *tu)
{
  *(vh *) r8 = *tu;
}

void
mr (void)
{
  int r8;
  cq he = { 0, };

  id (&r8, (vh *) &he);
}

% x86_64-unknown-linux-gnu-gcc-10.0.0-alpha20191124 -O2 -fexceptions -fnon-call-exceptions -fno-inline -c pi4vkh1k.c
pi4vkh1k.c: In function 'id.constprop':
pi4vkh1k.c:5:1: error: invalid vector types in nop conversion
    5 | id (int *r8, vh *tu)
      | ^~
vector(2) int
cq
_2 = (vector(2) int) { 0, 0, 0, 0 };
during GIMPLE pass: fixup_cfg
pi4vkh1k.c:5:1: internal compiler error: verify_gimple failed
0xd57c08 verify_gimple_in_cfg(function*, bool)
	/var/tmp/portage/sys-devel/gcc-10.0.0_alpha20191124/work/gcc-10-20191124/gcc/tree-cfg.c:5445
0xc3af99 execute_function_todo
	/var/tmp/portage/sys-devel/gcc-10.0.0_alpha20191124/work/gcc-10-20191124/gcc/passes.c:1983
0xc3bcb7 do_per_function
	/var/tmp/portage/sys-devel/gcc-10.0.0_alpha20191124/work/gcc-10-20191124/gcc/passes.c:1638
0xc3bcb7 execute_todo
	/var/tmp/portage/sys-devel/gcc-10.0.0_alpha20191124/work/gcc-10-20191124/gcc/passes.c:2037

W/ -fopenacc the check fails during IPA pass pta instead.


---


### compiler : `gcc`
### title : `x86 vector builtins throw exceptions`
### open_at : `2019-12-02T10:27:14Z`
### last_modified_date : `2019-12-02T14:08:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92747
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Looks like all x86 vector builtins used by intrinsics throw exceptions.  Seen
with the testcase for PR92645 and __builtin_ia32_packuswb128 for example.


---


### compiler : `gcc`
### title : `DSE fails to remove all dead clobbers`
### open_at : `2019-12-02T15:16:22Z`
### last_modified_date : `2020-03-05T08:14:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92750
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
In PR92645 I see DSE1 doing

   MEM[(struct Vec *)&s] = _74;
   MEM[(struct Vec *)&a] ={v} {CLOBBER};
-  MEM[(struct Vec *)&a] ={v} {CLOBBER};
-  MEM[(struct Vec *)&a] ={v} {CLOBBER};
-  MEM[(struct Vec *)&a] ={v} {CLOBBER};
-  MEM[(struct Vec *)&a] ={v} {CLOBBER};
   _73 = (unsigned char) invA_8;
   MEM[(struct Vec *)&a].val = _73;
   MEM[(struct Vec *)&a + 1B] ={v} {CLOBBER};
   MEM[(struct Vec *)&a + 1B].val = _73;
   MEM[(struct Vec *)&a + 2B] ={v} {CLOBBER};
-  MEM[(struct Vec *)&a + 2B] ={v} {CLOBBER};
   MEM[(struct Vec *)&a + 2B].val = _73;
   MEM[(struct Vec *)&a + 3B] ={v} {CLOBBER};
   MEM[(struct Vec *)&a + 3B].val = _73;
   MEM[(struct Vec *)&a + 4B] ={v} {CLOBBER};
-  MEM[(struct Vec *)&a + 4B] ={v} {CLOBBER};
-  MEM[(struct Vec *)&a + 4B] ={v} {CLOBBER};
   MEM[(struct Vec *)&a + 4B].val = _73;
   MEM[(struct Vec *)&a + 5B] ={v} {CLOBBER};
...

as you can see it removes all but one clobber but there's still a real must-def
that should make the last clobber dead as well.


---


### compiler : `gcc`
### title : `VN partial def support confused about clobbers`
### open_at : `2019-12-02T15:17:45Z`
### last_modified_date : `2019-12-03T10:48:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92751
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
FRE doesn't optimize

  MEM[(struct Vec *)&D.195309] ={v} {CLOBBER};
  MEM[(struct Vec *)&D.195309].val = 128;
  MEM[(struct Vec *)&D.195309 + 2B] ={v} {CLOBBER};
  MEM[(struct Vec *)&D.195309 + 2B].val = 128;
...
  MEM[(struct Vec *)&D.195309 + 28B].val = 128;
  MEM[(struct Vec *)&D.195309 + 30B] ={v} {CLOBBER};
  MEM[(struct Vec *)&D.195309 + 30B].val = 128;
  _127 = MEM <vector(16) short unsigned int> [(char * {ref-all})&D.195309];

to a vector constant because it's confused by the CLOBBERs.


---


### compiler : `gcc`
### title : `gcc generate extra move for the snippet code along with lea instruction.`
### open_at : `2019-12-04T16:51:46Z`
### last_modified_date : `2021-10-20T15:15:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92807
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `sprintf result not used by strlen pass`
### open_at : `2019-12-04T23:19:07Z`
### last_modified_date : `2021-09-15T01:22:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92813
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
With the integration of the sprintf pass into strlen, GCC 10 makes use of string lengths computed by the strlen pass to also compute the size of sprintf (and snprintf) output.  This makes it possible to optimize calls like the one if function f() below.  But the integration exposes string lengths only in one direction, and doesn't also make the lengths computed by the sprintf pass available to the strlen pass.  This can be seen in the dump for function g() below which should be optimized away just like f() but isn't.

$ cat t.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout t.c
void f (void)
{
  char a[8];
  __builtin_strcpy (a, "1234");
  if (__builtin_snprintf (0, 0, "%s", a) != 4)
    __builtin_abort ();
}

void g (void)
{
  char a[8];
  __builtin_sprintf (a, "1234");
  if (__builtin_strlen (a) != 4)
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1930, cgraph_uid=1, symbol_order=0)

f ()
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1934, cgraph_uid=2, symbol_order=1)

g ()
{
  char a[8];
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (&a, "1234", 5);
  _1 = __builtin_strlen (&a);
  if (_1 != 4)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  a ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `Typo in vec_perm -> bit_insert pattern`
### open_at : `2019-12-05T11:08:04Z`
### last_modified_date : `2019-12-05T13:08:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92818
### status : `RESOLVED`
### tags : `missed-optimization, wrong-code`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Index: gcc/match.pd
===================================================================
--- gcc/match.pd        (revision 278992)
+++ gcc/match.pd        (working copy)
@@ -6049,7 +6049,7 @@ (define_operator_list COND_TERNARY
                    break;
                if (at < encoded_nelts && sel.series_p (at + 1, 1, at + 1, 1))
                  {
-                   if (known_lt (at, nelts))
+                   if (known_lt (poly_uint64 (sel[at]), nelts))
                      ins = fold_read_from_vector (cop0, sel[at]);
                    else
                      ins = fold_read_from_vector (cop1, sel[at] - nelts);


---


### compiler : `gcc`
### title : `[10 Regression] vfma_laneq_f32 and vmul_laneq_f32 are broken on aarch64 after r278938`
### open_at : `2019-12-05T13:03:17Z`
### last_modified_date : `2020-01-28T11:50:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92822
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
After commit r278938, I've noticed regressions on aarch64:

FAIL: gcc.target/aarch64/fmla_intrinsic_1.c scan-assembler-times fmla\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s\\[[0-9]+\\] 2
FAIL: gcc.target/aarch64/fmls_intrinsic_1.c scan-assembler-times fmls\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s\\[[0-9]+\\] 2
FAIL: gcc.target/aarch64/fmul_intrinsic_1.c scan-assembler-times fmul\\td[0-9]+, d[0-9]+, d[0-9]+ 1
FAIL: gcc.target/aarch64/fmul_intrinsic_1.c scan-assembler-times fmul\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.d\\[[0-9]+\\] 3
FAIL: gcc.target/aarch64/fmul_intrinsic_1.c scan-assembler-times fmul\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.s\\[[0-9]+\\] 2
FAIL: gcc.target/aarch64/simd/vmulx_laneq_f32_1.c scan-assembler-times fmulx[ \t]+[vV][0-9]+.2[sS], ?[vV][0-9]+.2[sS], ?[vV][0-9]+.[sS]\\[0\\]\n 1
FAIL: gcc.target/aarch64/simd/vmulx_laneq_f32_1.c scan-assembler-times fmulx[ \t]+[vV][0-9]+.2[sS], ?[vV][0-9]+.2[sS], ?[vV][0-9]+.[sS]\\[1\\]\n 1
FAIL: gcc.target/aarch64/simd/vmulx_laneq_f32_1.c scan-assembler-times fmulx[ \t]+[vV][0-9]+.2[sS], ?[vV][0-9]+.2[sS], ?[vV][0-9]+.[sS]\\[2\\]\n 1
FAIL: gcc.target/aarch64/simd/vmulx_laneq_f32_1.c scan-assembler-times fmulx[ \t]+[vV][0-9]+.2[sS], ?[vV][0-9]+.2[sS], ?[vV][0-9]+.[sS]\\[3\\]\n 1
FAIL: gcc.target/aarch64/simd/vmulx_laneq_f64_1.c scan-assembler-times fmulx[ \t]+[dD][0-9]+, ?[dD][0-9]+, ?[vV][0-9]+.[dD]\\[0\\]\n 1
FAIL: gcc.target/aarch64/simd/vmulx_laneq_f64_1.c scan-assembler-times fmulx[ \t]+[dD][0-9]+, ?[dD][0-9]+, ?[vV][0-9]+.[dD]\\[1\\]\n 1
FAIL: gcc.target/aarch64/singleton_intrinsics_1.c scan-assembler-times aarch64_get_lanev2di 2


---


### compiler : `gcc`
### title : `Is that possible to optimize C++ exception??????????? I always do not like 2 phases of exception unwind since it does not call destructors.`
### open_at : `2019-12-05T13:04:59Z`
### last_modified_date : `2021-12-20T04:07:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92823
### status : `UNCONFIRMED`
### tags : `EH, missed-optimization`
### component : `c++`
### version : `10.0`
### severity : `enhancement`
### contents :
This paper shows the possibility of optimizing the current exception model. Just optimize lippincott function will yield huge benefits.

http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1676r0.pdf

Here is the proposal of Bjarne Stroustrup:

http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1947r0.pdf

Same with clang. It also does 2 phase unwinding.

Stroustrup C++ exceptions and alternatives P1947

I think that the current implementations of C++ exception handling are sub-optimal from a performance
standpoint and could be improved:
• GCC always (even in optimized modes) walks the stack twice to provide better debug support.
• Implementations use complete RTTI implementations to do type matching (ostensibly to
simplify use of dynamic linking).
• Cross-function optimizations of exception handling are still rare.
• Special-purpose stack-unwinding and type-matching algorithms are not used for systems with
special requirements (e.g., with no dynamic linking or tight memories).
• Some exception handling mechanisms cater for generalizations and alternatives not mandated
by the standard.
• There seem to be no optimization of the original case of passing and catching simple exceptions
by value. Given final, we can know that a given exception type isn’t the root of a hierarchy.
• Use of the general free store for all exception objects, rather than pre-allocated memory for
common and important exceptions as anticipated in the original design.
One reason for the relative poverty of optimizations seems to be that since exceptions were considered
slow there seemed no reason to optimize them. Thus, exception handling is now relatively slower than it
was in the 1990s. Other kinds of code have been significantly optimized since then.
As an example of a missed opportunity, I can mention the Chinese-remainder fast constant-time type
matching algorithm that I published in 2005 [Gibbs]. For closed systems and relatively small class
hierarchies (as are increasingly common in embedded systems), this eliminates almost all of the type
matching cost.
It may also be the case that the table-based implementation approach is sub-optimal for tiny memories
compared to error-code/stack-marking implementation approaches. The possibility of special-purpose
implementations of exception handling for special-purpose systems ought to be explored. The
Edinburgh experiments [Renwick] are encouraging for this direction of exploration.


---


### compiler : `gcc`
### title : `Unnecesary stack protection in Firefox's LightPixel.`
### open_at : `2019-12-05T13:21:21Z`
### last_modified_date : `2019-12-11T08:24:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92825
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 47428
full testcase

uint32_t DiffuseLightingSoftware::LightPixel(const Point3D& aNormal,
                                             const Point3D& aVectorToLight,
                                             uint32_t aColor) {
  Float dotNL = std::max(0.0f, aNormal.DotProduct(aVectorToLight));
  Float diffuseNL = mDiffuseConstant * dotNL;

  union {
    uint32_t bgra;
    uint8_t components[4];
  } color = {aColor};
  color.components[B8G8R8A8_COMPONENT_BYTEOFFSET_B] = umin(
      uint32_t(diffuseNL * color.components[B8G8R8A8_COMPONENT_BYTEOFFSET_B]),
      255U);
  color.components[B8G8R8A8_COMPONENT_BYTEOFFSET_G] = umin(
      uint32_t(diffuseNL * color.components[B8G8R8A8_COMPONENT_BYTEOFFSET_G]),
      255U);
  color.components[B8G8R8A8_COMPONENT_BYTEOFFSET_R] = umin(
      uint32_t(diffuseNL * color.components[B8G8R8A8_COMPONENT_BYTEOFFSET_R]),
      255U);
  color.components[B8G8R8A8_COMPONENT_BYTEOFFSET_A] = 255;
  return color.bgra;
}

(full testcase attached)
Built with -O3 -fstack-protection-strong results in slower code with gcc10 than with gcc9 or clang.

GCC produces:
       │     0000000004390e20 <mozilla::gfx::(anonymous namespace)::SpecularLightingSoftware::LightPixel(mozilla::gfx::Point3DTyped<mozilla::gfx::UnknownUnits, float> const&,
       │     _ZN7mozilla3gfx12_GLOBAL__N_124SpecularLightingSoftware10LightPixelERKNS0_12Point3DTypedINS0_12UnknownUnitsEfEES7_j():
  0.19 │       push      %rbp
  0.60 │       pxor      %xmm5,%xmm5
  0.05 │       mov       %rsp,%rbp
  0.12 │       push      %rbx
  0.65 │       sub       $0x18,%rsp
  0.33 │       movss     0x4(%rdx),%xmm0
  0.10 │       movss     (%rdx),%xmm1
  0.58 │       mov       %fs:0x28,%rax
  0.03 │       mov       %rax,-0x18(%rbp)
  0.22 │       xor       %eax,%eax
  0.07 │       movss     pw_32+0x1588,%xmm3
  1.58 │       addss     0x8(%rdx),%xmm3
  0.67 │       addss     %xmm5,%xmm0
  0.23 │       addss     %xmm5,%xmm1
       │       movaps    %xmm0,%xmm2
  0.41 │       movaps    %xmm1,%xmm4
  0.87 │       mulss     %xmm0,%xmm2
  0.28 │       mulss     %xmm1,%xmm4
  3.71 │       addss     %xmm2,%xmm4
  0.14 │       movaps    %xmm3,%xmm2
  0.04 │       mulss     %xmm3,%xmm2
  1.99 │       addss     %xmm2,%xmm4
  0.15 │       movss     0x4(%rsi),%xmm2
  9.39 │       sqrtss    %xmm4,%xmm4
  8.90 │       divss     %xmm4,%xmm0
  2.10 │       divss     %xmm4,%xmm3
  1.08 │       mulss     %xmm0,%xmm2
  0.01 │       movss     0x8(%rsi),%xmm0

while clang
Percent│     _ZN7mozilla3gfx12_GLOBAL__N_124SpecularLightingSoftware10LightPixelERKNS0_12Point3DTypedINS0_12UnknownUnitsEfEES7_j():
  0.11 │       xorps     %xmm0,%xmm0
  0.83 │       movss     0x4(%rdx),%xmm1
  3.29 │       addss     %xmm0,%xmm1
  0.03 │       movss     (%rdx),%xmm2
  0.08 │       movss     0x8(%rdx),%xmm3
  0.04 │       unpcklps  %xmm2,%xmm3
  0.59 │       movss     mozilla::gfx::ConvertComponentTransferFunctionToFilter(mozilla::gfx::ComponentTransferAttributes const&, int, int, mozilla::gfx::DrawTarget*, RefPtr<m
  1.00 │       addps     %xmm2,%xmm3
  0.10 │       movaps    %xmm3,%xmm4
  0.82 │       shufps    $0xe5,%xmm3,%xmm4
  3.05 │       mulss     %xmm4,%xmm4
  0.09 │       movaps    %xmm1,%xmm5
  0.12 │       mulss     %xmm1,%xmm5
  2.77 │       addss     %xmm4,%xmm5
  0.06 │       movaps    %xmm3,%xmm4
  0.00 │       mulss     %xmm3,%xmm4
  2.95 │       addss     %xmm5,%xmm4
  9.54 │       sqrtss    %xmm4,%xmm4
  8.84 │       divss     %xmm4,%xmm1
  0.08 │       shufps    $0xe0,%xmm4,%xmm4
  2.45 │       divps     %xmm4,%xmm3
  0.88 │       mulss     0x4(%rsi),%xmm1
  0.01 │       movss     (%rsi),%xmm4
       │       movss     0x8(%rsi),%xmm5
  0.02 │       unpcklps  %xmm4,%xmm5
  2.82 │       mulps     %xmm3,%xmm5
  0.03 │       movaps    %xmm5,%xmm3
  0.88 │       shufps    $0xe5,%xmm5,%xmm3
  3.47 │       addss     %xmm1,%xmm3
  3.39 │       addss     %xmm5,%xmm3
  3.09 │       cmpless   %xmm3,%xmm0
  1.77 │       andps     %xmm2,%xmm0
  3.01 │       mulss     %xmm3,%xmm0
  3.25 │       mulss     mozIGeckoMediaPluginService::COMTypeInfo<mozIGeckoMediaPluginService, void,%xmm0
  4.85 │       cvttss2si %xmm0,%eax


---


### compiler : `gcc`
### title : `misssed SLP vectorization in LightPixel`
### open_at : `2019-12-05T20:18:13Z`
### last_modified_date : `2023-09-21T12:26:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92834
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 47431
simplified testcase

Clang is able to vectorize LightPixel which leads to about 10% improvements in rasterflood-svg Firefox benchmark.


---


### compiler : `gcc`
### title : `Optimize -fstack-protector-strong code generation a bit`
### open_at : `2019-12-06T11:52:50Z`
### last_modified_date : `2019-12-20T08:24:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92841
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 47435
prepocessed source

Hi,

I'm building the linux kernel with

gcc (Debian 9.2.1-8) 9.2.1 20190909

and -fstack-protector-strong on x86 and from looking at what it generates, it could be optimized a little:

# arch/x86/kernel/cpu/scattered.c:48: {
        movq    %gs:40, %rax    # MEM[(<address-space-2> long unsigned int *)40B], tmp125
        movq    %rax, 16(%rsp)  # tmp125, D.21425
        xorl    %eax, %eax      # tmp125
        movl    $6, %eax        #, pretmp_35
        jmp     .L6     #

AFAICT, the stack protector part would load the canary value from the %gs offset and use a temp register to stick it on the stack and then clear that temp register, %rax in this case.

As part of further function processing, it would stick a value in that same register, which makes the previous clearing superfluous.

I'm being told this could be taken care of in a "late peephole". :)

Attaching preprocessed source.

Thx.


---


### compiler : `gcc`
### title : `Use ERF_RETURNS_ARG in more places`
### open_at : `2019-12-09T11:09:57Z`
### last_modified_date : `2021-12-27T04:25:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92867
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Shouldn't we use the fact that some functions returns one of its arguments unmodified in more places?

E.g. for tail calls:
char buf[128] = { 1 };                                                                                                                             
                                                                                                                                                   
char *                                                                                                                                             
foo (int n)                                                                                                                                        
{                                                                                                                                                  
  return __builtin_memset (buf, ' ', n);                                                                                                                     
}                                                                                                                                                  
                                                                                                                                                   
char *                                                                                                                                             
bar (int n)                                                                                                                                        
{                                                                                                                                                  
  __builtin_memset (buf, ' ', n);                                                                                                                            
  return buf;                                                                                                                                      
}                                                                                                                                                  

we get better code (tail call) just in the first case and not the second one.

Shouldn't we perform IPA discovery of calls that return one of their arguments?

And, shouldn't we have some user attribute which would allow to specify this?


---


### compiler : `gcc`
### title : `[10/11 Regression] incorrect warning of __builtin_memset offset is out of the bounds on zero-size allocation and initialization`
### open_at : `2019-12-10T04:13:00Z`
### last_modified_date : `2021-11-10T21:51:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92879
### status : `RESOLVED`
### tags : `alias, diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
DIAGNOSTIC:

g++ prog.cpp -c -Wall -O3
In constructor ‘S::S(int)’,
    inlined from ‘(static initializers for prog.cpp)’ at prog.cpp:16:6:
prog.cpp:13:30: warning: ‘void* __builtin_memset(void*, int, long unsigned int)’ offset [0, 3] is out of the bounds [0, 0] []8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Warray-bounds-Warray-bounds]8;;]
   13 |         for(int i=0;i<m;i++) new(p+i)int();
      |                              ^~~~~~~~~~~~~


CODE:

inline void* operator new(long unsigned int, void* v) noexcept
{
    return v;
}
struct S
{
    int* p;
    int m;
    S(int i)
    {
        m=i;
        p=(int*)new unsigned char[sizeof(int)*m];
        for(int i=0;i<m;i++) new(p+i)int();
    }
};
S a(0);


COMPILER:

Using built-in specs.
COLLECT_GCC=/home/craig/new-gcc/i-trunk/bin/g++
COLLECT_LTO_WRAPPER=/home/craig/new-gcc/i-trunk/libexec/gcc/x86_64-pc-linux-gnu/10.0.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../s-trunk/configure --prefix=/home/craig/new-gcc/i-trunk --disable-multilib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.0.0 20191209 (experimental) (GCC) 


EXPLANATION:

The body of the initialization loop never executes, so no out-of-bounds is possible.


---


### compiler : `gcc`
### title : `[SVE] Add support for chained extract-last reductions`
### open_at : `2019-12-10T10:32:12Z`
### last_modified_date : `2020-02-17T09:29:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92884
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Extract-last (i.e. CLASTB) reductions can't yet handle chained
conditions, such as those seen in gcc.dg/vect/vect-cond-reduc-5.c.
We just fall back to the normal COND_REDUCTION handling instead.

If we have:

    res_0 = PHI <res_n(latch), init(entry)>;
    res_1 = COND_EXPR <cond_1, res_0, val_1>;
    res_2 = COND_EXPR <cond_2, res_1, val_2>;
    ...
    res_n = COND_EXPR <cond_n, res_{n-1}, val_n>;

one alternative would be (pseudo-code):

    res_0 = PHI <res_n(latch), init(entry)>;
    vec.res_1 = vec.val_1;
    vec.res_2 = VEC_COND_EXPR <vec.cond_2, vec.res_1, vec.val_2>;
    ...
    vec.res_n = VEC_COND_EXPR <vec.cond_n, vec.res_{n-1}, vec.val_n>;
    vec.cond_any = IOR_EXPR <vec.cond_1, ..., vec.cond_n>;
    res_n = .EXTRACT_LAST (res_0, vec.cond_any, vec.res_n);

Perhaps it would make sense to move the IFN_EXTRACT_LAST generation
from vectorizable_condition to vect_create_epilog_for_reduction.
All vectorizable_condition would need to do differently from
COND_REDUCTION is to handle the special case of:

    vec.res_1 = vec.val_1;

instead of using a VEC_COND_EXPR between vec.val_1 and vec.res_0.
(res_0 isn't vectorised for EXTRACT_LAST_REDUCTION.)


---


### compiler : `gcc`
### title : `[AARCH64] TBL-based permutations can be implemented more efficiently for 2-element vectors`
### open_at : `2019-12-10T16:48:59Z`
### last_modified_date : `2020-02-13T01:05:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92892
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
Current vector elements permutation implementation generates different instructions depending on specific permutation form. For permutations like: "target[0] = src1[0]; target[1] = src2[1];" the TBL instruction is used and following instructions sequence is generated:

mov tmpReg1, src1;
mov tmpReg2, src2;
tbl target, {tmpReg1, tmpReg2}, ...
// the tmpReg1 and tmpReg2 registers which are numbered consecutively, as required by tbl instruction

For 2-element vectors this sequence can be reduced to:

mov target[0], src1[0]
mov target[1], src2[1]


And it can be reduced to a single mov in case target = src, which is already implemented in patch prototype I'm working on.


---


### compiler : `gcc`
### title : `Cannot elide byteswap when only needed to compare to multiple constants`
### open_at : `2019-12-10T22:57:44Z`
### last_modified_date : `2023-08-08T07:26:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92903
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `enhancement`
### contents :
I compiled the following test code on GCC 8.3.0:

int test(int a)                                                                                  
{                                                                                                
        const int swapped = __builtin_bswap32(a);                                                
                                                                                                 
        if (swapped == 0x12345678 || swapped == 0x23456789)                                                                                                                                
                return 1;                                                                        
        return 0;                                                                                
}

On x86_64, I get:

0000000000000000 <test> (File Offset: 0x40):
   0:   0f cf                   bswap  %edi
   2:   81 ff 78 56 34 12       cmp    $0x12345678,%edi
   8:   0f 94 c0                sete   %al
   b:   81 ff 89 67 45 23       cmp    $0x23456789,%edi
  11:   0f 94 c2                sete   %dl
  14:   09 d0                   or     %edx,%eax
  16:   0f b6 c0                movzbl %al,%eax
  19:   c3                      retq   

And on aarch64, I get:

0000000000000000 <test> (File Offset: 0x40):
   0:   528acf01        mov     w1, #0x5678                     // #22136
   4:   5ac00800        rev     w0, w0
   8:   72a24681        movk    w1, #0x1234, lsl #16
   c:   6b01001f        cmp     w0, w1
  10:   528cf121        mov     w1, #0x6789                     // #26505
  14:   72a468a1        movk    w1, #0x2345, lsl #16
  18:   7a411004        ccmp    w0, w1, #0x4, ne
  1c:   1a9f17e0        cset    w0, eq
  20:   d65f03c0        ret

In both of those cases it would have been better to omit the byteswap instruction and instead embed the constants to compare to in their swapped form right away. This works correctly when comparing to a single constant, like this:

int test(int a)                                                                                  
{                                                                                                
        const int swapped = __builtin_bswap32(a);                                                
                                                                                                 
        if (swapped == 0x12345678)                                                                                                                                                         
                return 1;                                                                        
        return 0;                                                                                
} 

0000000000000000 <test> (File Offset: 0x40):
   0:   52868241        mov     w1, #0x3412                     // #13330
   4:   72af0ac1        movk    w1, #0x7856, lsl #16
   8:   6b01001f        cmp     w0, w1
   c:   1a9f17e0        cset    w0, eq
  10:   d65f03c0        ret

But comparing the swapped value to more than one constant somehow makes GCC miss this optimization, even if neither the swapped value nor the constants are used for anything else.


---


### compiler : `gcc`
### title : `[10 Regression] Spills float-int union to memory`
### open_at : `2019-12-11T08:57:25Z`
### last_modified_date : `2019-12-21T15:06:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92905
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
gcc-10 branch regressed for code that needs bitwise operations on floats:

float f(float x)
{
    union {float f; unsigned i;} u = {x};
    u.i |= 0x80000000;
    return u.f;
}

float my_copysign(float x, float y)
{
    union {float f; unsigned i;} ux = {x}, uy ={y};
    ux.i &= 0x7fffffff;
    ux.i |= 0x80000000 & uy.i;
    return ux.f;
}


For function 'f' gcc-10 -O2 -mtune=intel generates
f:
        movd    %xmm0, -4(%rsp)
        movl    $-2147483648, %eax
        orl     -4(%rsp), %eax
        movd    %eax, %xmm0
        ret

while gcc-9 and earlier generate code without stack use, even without -mtune=intel:
f:
        movd    %xmm0, %eax
        orl     $-2147483648, %eax
        movd    %eax, %xmm0
        ret

Likewise for the more realistic my_copysign, where ux is spilled, but uy is not.

Eventually it would be nicer to use SSE bitwise operations for this, for example LLVM already generates
f:
        orps    .LCPI0_0(%rip), %xmm0


---


### compiler : `gcc`
### title : `RTL expansion throws away misalignment info`
### open_at : `2019-12-12T23:52:11Z`
### last_modified_date : `2021-12-16T06:27:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92925
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Hi,
this testcase originally started as attempt to produce self contained reproducer for ipa-cp bug.  Problem is that RTL expansion is too limited and refuses to produce aligned moves for me.  
struct a {long a1; long a2;};
struct b {long b; struct a a[10];};
struct c {long c; struct b b;__int128 e;};
int l;
__attribute__ ((noinline))
static void
set(struct b *bptr)
{
  for (int i=0;i<l;i+=2)
    bptr->a[i]=(struct a){};
}
test ()
{
  struct c c;
  set (&c.b);
}

Here ipa-cp propagates that BPTR is always aligned to 16 with misaligment 8. This should let expansion to use movaps for the "bptr->a[i]=(struct a){};" constructions but it does not.

set:
.LFB0:
        .cfi_startproc
        movl    l(%rip), %ecx
        testl   %ecx, %ecx
        jle     .L1
        xorl    %eax, %eax
        .p2align 4,,10
        .p2align 3
.L3:
        movslq  %eax, %rdx
        pxor    %xmm0, %xmm0
        addl    $2, %eax
        salq    $4, %rdx
        movups  %xmm0, 8(%rdi,%rdx)
        cmpl    %ecx, %eax
        jl      .L3
.L1:

Overall the loop codegen is quite bad.


---


### compiler : `gcc`
### title : `bswap/store merging does not handle BIT_INSERT_EXPR/BIT_FIELD_REF`
### open_at : `2019-12-15T21:28:01Z`
### last_modified_date : `2020-01-12T09:38:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92949
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
While working on lowering bit-field accesses (to allow better optimizations on the tree level rather than just on the RTL level), I find the bswap/store merging passes don't handle BIT_INSERT_EXPR.  So we don't transform some things now.
I could not understand how symbolic_number works so I am filing this bug.


---


### compiler : `gcc`
### title : `Undesired if-conversion with overflow builtins`
### open_at : `2019-12-16T11:04:58Z`
### last_modified_date : `2023-09-20T06:18:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92953
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider:

/* Return 0 if a==b, any positive value if a>b, any negative value otherwise. */
int foo(int a, int b)
{
    int c;
    if (__builtin_sub_overflow(a, b, &c))
        c = 1 | ~c;
    return c;
}

(suggestions for implementations that would be more efficient on x86 welcome)

on x86 with -Os gives the expected

foo:
        subl    %esi, %edi
        movl    %edi, %eax
        jno     .L1
        notl    %eax
        orl     $1, %eax
.L1:
        ret

but with -O2 there's if-conversion despite internal-fn.c marking the branch as "very_unlikely":

foo:
        xorl    %edx, %edx
        subl    %esi, %edi
        movl    %edi, %eax
        seto    %dl
        notl    %eax
        orl     $1, %eax
        testl   %edx, %edx
        cmove   %edi, %eax
        ret

Adding __builtin_expect to the source doesn't help. Adding __builtin_expect_with_probability helps when specified probability is very low (<3%), but I feel that shouldn't be required here.

Looking at expand dump, on RTL we start with two branches, first from expanding the internal fn to calculate a 0/1 predicate value, the second corresponding to the "if" in the source, branching on testing that predicate against 0. At -Os, we rely on first if-conversion pass to eliminate the first branch, and then on combine to optimize the second branch.

Is it possible to expand straight to one branch by noticing that the predicate is only used in the gimple conditional that follows immediately?


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] False positive stringop-overflow warning with vectorization and loop unrolling`
### open_at : `2019-12-16T16:03:29Z`
### last_modified_date : `2023-07-07T10:18:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92955
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Executing on host: /home/seurer/gcc/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/build/gcc-test/gcc/ /home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -fdiagnostics-urls=never   -maltivec -mpower8-vector -ftree-vectorize -fno-tree-loop-distribute-patterns -fno-vect-cost-model -fno-common -O2 -fdump-tree-vect-details -Wall -Werror -S -o pr60505.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/build/gcc-test/gcc/ /home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -maltivec -mpower8-vector -ftree-vectorize -fno-tree-loop-distribute-patterns -fno-vect-cost-model -fno-common -O2 -fdump-tree-vect-details -Wall -Werror -S -o pr60505.s
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c: In function 'foo':
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset [16, 2147483632] to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 17 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 18 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 19 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 20 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 21 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 22 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 23 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 24 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 25 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 26 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 27 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 28 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 29 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 30 to object 'ovec' with size 16 declared here
cc1: all warnings being treated as errors
compiler exited with status 1
Executing on host: /home/seurer/gcc/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/build/gcc-test/gcc/ exceptions_enabled78029.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -fdiagnostics-urls=never  -S -o exceptions_enabled78029.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/build/gcc-test/gcc/ exceptions_enabled78029.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -S -o exceptions_enabled78029.s
exceptions_enabled78029.c: In function 'foo':
exceptions_enabled78029.c:4:7: error: 'throw' undeclared (first use in this function)
exceptions_enabled78029.c:4:7: note: each undeclared identifier is reported only once for each function it appears in
exceptions_enabled78029.c:4:12: error: expected ';' before numeric constant
compiler exited with status 1
FAIL: gcc.dg/vect/pr60505.c (test for excess errors)
Excess errors:
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
cc1: all warnings being treated as errors

Executing on host: /home/seurer/gcc/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/build/gcc-test/gcc/ /home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -fdiagnostics-urls=never  -flto -ffat-lto-objects -maltivec -mpower8-vector -ftree-vectorize -fno-tree-loop-distribute-patterns -fno-vect-cost-model -fno-common -O2 -fdump-tree-vect-details -Wall -Werror -S -o pr60505.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/build/gcc-test/gcc/ /home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -fdiagnostics-urls=never -flto -ffat-lto-objects -maltivec -mpower8-vector -ftree-vectorize -fno-tree-loop-distribute-patterns -fno-vect-cost-model -fno-common -O2 -fdump-tree-vect-details -Wall -Werror -S -o pr60505.s
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c: In function 'foo':
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset [16, 2147483632] to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 17 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 18 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 19 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 20 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 21 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 22 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 23 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 24 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 25 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 26 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 27 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 28 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 29 to object 'ovec' with size 16 declared here
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:7:8: note: at offset 30 to object 'ovec' with size 16 declared here
cc1: all warnings being treated as errors
compiler exited with status 1
FAIL: gcc.dg/vect/pr60505.c -flto -ffat-lto-objects (test for excess errors)
Excess errors:
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
/home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/pr60505.c:10:23: error: writing 1 byte into a region of size 0 [-Werror=stringop-overflow=]
cc1: all warnings being treated as errors

testcase /home/seurer/gcc/gcc-test/gcc/testsuite/gcc.dg/vect/vect.exp completed in 0 seconds

		=== gcc Summary ===

# of unexpected failures	2


---


### compiler : `gcc`
### title : `gcc produces a duplicated load for restrict point in some cases`
### open_at : `2019-12-16T17:22:00Z`
### last_modified_date : `2021-11-02T10:24:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92957
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Created attachment 47507
testcase

In the attached testcase gcc produces

_Z3fooPN7seastar12future_stateIJiEEES2_:
	movq	(%rsi), %rax
	movq	%rax, (%rdi)
	movq	$0, (%rsi)
	movq	(%rdi), %rax
	subq	$2, %rax
	cmpq	$1, %rax
	ja	.L1
	movl	8(%rsi), %eax
	movl	%eax, 8(%rdi)
.L1:
	ret

The "movq	(%rdi), %rax" load is redundant and clang avoids it.


---


### compiler : `gcc`
### title : `order of base class members generates vastly different code`
### open_at : `2019-12-16T22:12:32Z`
### last_modified_date : `2021-11-29T01:50:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92964
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 47510
reproducible c++ source

I'm new at submitting bug reports, sorry if this one is invalid.D

I have attached a repro file that actually exhibits what I believe to be two issues, but I'm unsure of the second. If I need to open a second report for it (or if it's really not a problem) just let me know. I've reproduced this behavior on gcc 8.3, gcc 9.2, and gcc trunk.

Godbolt link if interested: https://godbolt.org/z/F756Pe

Anyway, looking at the generated assembly when compiling the program with -std=c++17 -O2 (and -O3) seems to indicate a missed optimization of some kind.

bar1() and bar3() are equivalent, other than the order of the union and the bool in the base storage class. I believe gcc should be able to generate basically the same assembly as well (other than swapping %rax/%rdx), but bar3 utilizes the stack, even using SIMD instructions for the false branch. bar1 doesn't do this. clang generates equivalent code for bar1/bar3.

I'm assuming it has something to do with alignment, but what's strange is that if the intermediary Payload class is bypassed by using a PayloadBase directly in Foo<>, bar1 and bar3 are generated (almost) equivalently as expected (no SIMD instructions or stack usage).



The other issue (please let me know if I should just open another report or if this is expected), is the difference between bar1/bar2 and bar3/bar4. They only differ in how they return the Foo<> object. bar1/bar3 return a default constructed Foo<>, but bar2/bar4 return a sentinel and rely on converting to Foo<> using the user-defined constructor. Generated assembly for these pairs of functions are very different (at least at -O2 optimization).

With the -O2 level optimization, bar2 does not set %rdx in the false branch (bar1 does). clang sets it in both. I realize it's not strictly necessary to ensure that a value is returned in %rdx since the empty union member is the one being initialized, but bar1 takes care to make sure it's set to 0 (as does clang for both). Bumping the optimization level up to -O3 actually causes bar2 to ensure both are set to 0 explicitly.

bar3 and bar4, however, both intentionally populate %rax with uninitialized junk from the stack in the false branch (both -O2 and -O3), though it's clearer in bar4 since it doesn't use SIMD instructions there (related to the first point?) If initializing %rax isn't necessary, why does it move memory from the uninitialized stack into the return register at all?

I apologize if this is not well-thought-out or if there's something obvious I'm missing. Let me know if you need me to provide any more information.


---


### compiler : `gcc`
### title : `bswap not finding a bswap with a memory load at the beginging of the instruction stream`
### open_at : `2019-12-17T21:31:13Z`
### last_modified_date : `2021-10-03T00:29:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92979
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Take these two functions:
unsigned g(unsigned *a)
{
  unsigned M0 = *a & 0xff;
  unsigned M1 = (*a>>8) & 0xff;
  unsigned M2 = (*a>>16) & 0xff;
  unsigned M3 = (*a>>24) & 0xff;
  unsigned t = 0;
  t |= M0;
  t <<= 8;
  t |= M1;
  t <<= 8;
  t |= M2;
  t <<= 8;
  t |= M3;
  return t;
}
unsigned g1(unsigned a)
{
  unsigned M0 = a & 0xff;
  unsigned M1 = (a>>8) & 0xff;
  unsigned M2 = (a>>16) & 0xff;
  unsigned M3 = (a>>24) & 0xff;
  unsigned t = 0;
  t |= M0;
  t <<= 8;
  t |= M1;
  t <<= 8;
  t |= M2;
  t <<= 8;
  t |= M3;
  return t;
}
----- CUT ---

Only g1 is detected as bswap while g is not.  The problem is the way bswap is too "agressive" in looking through the instruction stream to find loads.

I found this while implementing lowering of bit-fields.


---


### compiler : `gcc`
### title : `[miss optimization]redundant load missed by fre.`
### open_at : `2019-12-18T01:27:25Z`
### last_modified_date : `2022-08-06T20:03:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92980
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
cat test.c

int foo(unsigned int *__restrict src1, int i, int k, int n)
{
  int j = k + n;
  int sum = src1[j];
  sum += src1[j-1];
  if (i <= k)
    {
      j+=2;
      sum += src1[j-3];
    }
  return sum + j;
}


x86_64_gcctrunk -Ofast test.c -S
got 

foo:
.LFB0:
	.cfi_startproc
	addl	%edx, %ecx
	movl	%esi, %r8d
	movslq	%ecx, %rsi
	movl	(%rdi,%rsi,4), %eax
	addl	-4(%rdi,%rsi,4), %eax
	cmpl	%r8d, %edx
	jl	.L3
	addl	$2, %ecx
	movslq	%ecx, %rdx
	addl	-12(%rdi,%rdx,4), %eax   ---- redudant load, it's actual a[j-1] which is loaded before.
.L3:
	addl	%ecx, %eax
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 10.0.0 20191117 (experimental)"
	.section	.note.GNU-stack,"",@progbits

it could be better like

foo:
.LFB0:
	.cfi_startproc
	addl	%edx, %ecx
	movl	%esi, %r9d
	movslq	%ecx, %rsi
	movl	-4(%rdi,%rsi,4), %r8d
	movl	(%rdi,%rsi,4), %eax
	addl	%r8d, %eax
	cmpl	%r9d, %edx
	jl	.L3
	addl	$2, %ecx
	addl	%r8d, %eax ----> reuse earlir load result.
.L3:
	addl	%ecx, %eax
	ret
	.cfi_endproc


---


### compiler : `gcc`
### title : `missed optimization opportunity for switch linear transformation`
### open_at : `2019-12-18T13:24:12Z`
### last_modified_date : `2021-11-29T20:41:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92985
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
From Peter Dimov on Slack:

#include <cstddef>

struct X
{
    int x, y, z;

    int operator[]( std::size_t i ) const noexcept
    {
        switch( i )
        {
            case 0: return x;
            case 1: return y;
            case 2: return z;
            default: __builtin_unreachable();
        }
    }
};

int f( X const& x, std::size_t i )
{
    return x[ i ];
}

compiled with -O2 (or -O3) emits:

f(X const&, unsigned long):
        cmp     rsi, 1
        je      .L2
        cmp     rsi, 2
        jne     .L6
        mov     eax, DWORD PTR [rdi+8]
        ret
.L2:
        mov     eax, DWORD PTR [rdi+4]
        ret
.L6:
        mov     eax, DWORD PTR [rdi]
        ret

But it should be able to just emit:
        mov     eax, DWORD PTR [rdi+rsi*4]
        ret


---


### compiler : `gcc`
### title : `while(i--) optimization`
### open_at : `2019-12-19T05:40:56Z`
### last_modified_date : `2021-12-21T11:43:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93002
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `enhancement`
### contents :
while (i--) can be compiled into `sub ecx, 1 / jnc code` rather than `dec ecx / cmp ecx, -1 / jne code`.

See https://stackoverflow.com/questions/54278070/ for discussion (I'm asker)


---


### compiler : `gcc`
### title : `Better code with a-- == 0 rather than --a == -1`
### open_at : `2019-12-19T11:18:19Z`
### last_modified_date : `2023-09-05T08:34:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93006
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
unsigned int a;

unsigned
foo (unsigned b, unsigned c)
{
  return b + c + (--a == -1);
}

unsigned
bar (unsigned b, unsigned c)
{
  return b + c + (a-- == 0);
}

unsigned
baz (unsigned b, unsigned c)
{
  return b + c + (--a != -1);
}

unsigned
qux (unsigned b, unsigned c)
{
  return b + c + (a-- != 0);
}

where foo is equivalent to bar and baz to qux generates better code (just 6 instructions) in bar/qux and 9 in foo/baz on x86_64-linux.


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] pr77698.c testcase fails due to block commoning`
### open_at : `2019-12-19T13:19:38Z`
### last_modified_date : `2023-07-07T10:36:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93007
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Since r276960 we see this failure on Arm:

FAIL: gcc.dg/tree-prof/pr77698.c scan-rtl-dump-times alignments "internal loop alignment added" 1

The issue appears to be that basic block commoning works on an unrolled loop, which is unlikely to be beneficial for performance:

.L17:
        adds    r0, r0, #1
        b       .L27
.L6:
        ldr     r4, [r2, #12]
        adds    r0, r0, #4
        ldr     lr, [r1]
        str     lr, [r3, r4, lsl #2]
        ldr     r4, [r2, #12]
        ldr     lr, [r1]
        str     lr, [r3, r4, lsl #2]
        ldr     r4, [r2, #12]
        ldr     lr, [r1]
        str     lr, [r3, r4, lsl #2]
.L27:
        ldr     r4, [r2, #12]
        cmp     ip, r0
        ldr     lr, [r1]
        str     lr, [r3, r4, lsl #2]
        bne     .L6
        pop     {r4, pc}

The test could be easily fixed, but ensuring block commoning takes loops and execution frequencies into account would be better overall.


---


### compiler : `gcc`
### title : `Need a way to make inlining heuristics ignore whether a function is inline`
### open_at : `2019-12-19T13:42:31Z`
### last_modified_date : `2022-04-22T16:41:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93008
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.0`
### severity : `enhancement`
### contents :
Currently GCC considers the 'inline' keyword as a hint for inlining, not just a request for comdat semantics.

This is a problem in C++20 where arbitrarily complex functions are marked 'constexpr' so that they can be used at compile time, and a constexpr function is implicitly an inline function. But we do not want the function to get additional weighting for runtime optimizations just because it is eligible for use in compile-time constant expressions.

It would be useful to have a new attribute that tells the inliner to ignore whether the function is 'inline' and only consider its size, use in hot/cold regions etc.

Alternatively, do not interpret 'constexpr' on its own as an inlining hint, and only treat it as a hint when a constexpr function is explicitly declared with the 'inline' specifier (or is defined in the class body).


constexpr void f() { }  // not an inline hint
constexpr inline void g() { } // inline hint
inline constexpr void h() { } // inline hint

struct X {
  constexpr void i() { } // inline hint
};


---


### compiler : `gcc`
### title : `PPC: inefficient 64-bit constant generation (upper = lower)`
### open_at : `2019-12-19T15:37:43Z`
### last_modified_date : `2021-12-15T08:03:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93012
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
unsigned long long msk66()
{
   return 0x6666666666666666ULL;
}

gcc -maix64 -O2 const.C -save-temps

Output:
._Z5msk66v:
LFB..0:
        lis 3,0x6666
        ori 3,3,0x6666
        sldi 3,3,32
        oris 3,3,0x6666
        ori 3,3,0x6666
        blr

Any 64-bit constant that has matching upper 32-bit and lower 32-bit can be created using 3 instructions construct 32-bit lower part and then use rldimi to duplicate into upper part of register.

Sample:
        lis 3, 26214
        ori 3, 3, 26214
        rldimi 3, 3, 32, 0


---


### compiler : `gcc`
### title : `give preference to address iv without offset in ivopts`
### open_at : `2019-12-20T09:46:19Z`
### last_modified_date : `2020-01-08T14:42:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93023
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
From address IVs with same base and index, ivopts always pick up one with non-zero offset. This does not incur extra cost on architecture like X86, which has LEA instruction to combine offset into address computation. But on ARM, one more add-with-offset instruction is required.

   X86:   lea addr_reg, base[index + offset]

   ARM:   add addr_reg, base, index
          add addr_reg, addr_reg, offset

So choosing IV w/o offset can save one instruction in most situations.
Here is an example, compile it on aarch64.

  int data[100];
  int fn1 ();

  void fn2 (int b, int n)
  {
    int i;

    for (i = 0; i < n; i++, b++)
      {
        data[b + 10] = 1;
        fn1 ();
        data[b + 3] = 2;
      }
  }

Analysis into ivopts shows that those address IVs have same in-loop cost, and IV w/o offset does have smaller pre-loop setup cost. But since the setup cost will be averaged to each iteration, the minor cost difference will go due to round-off by integer division. To fix this round-off error, cost can be represented in a more accurate way, such as adding a fraction part to make it a fixpoint number.


---


### compiler : `gcc`
### title : `Slow 'while' loop unrolling`
### open_at : `2019-12-21T08:20:04Z`
### last_modified_date : `2020-01-08T14:46:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93037
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
Created attachment 47538
Adobe_C++Benchmarks

Compile and run 'loop unroll test' in Adobe C++ benchmark (https://stlab.adobe.com/performance/). We find slowdown in GCC-8(docker) when adding '-O3':

╔════════════════════════════════════════╦═════╦═════╦═════╦══════════╗
║ Total absolute execution time (sec.)   ║ O1  ║ O2  ║ O3  ║ comments ║
╠═════════╦══════════════════════════════╬═════╬═════╬═════╬══════════╣
║ gcc-8   ║ int32_t for loop unrolling   ║ 259 ║ 167 ║ 149 ║ ok       ║
║         ╠══════════════════════════════╬═════╬═════╬═════╬══════════╣
║         ║ int32_t while loop unrolling ║ 257 ║ 164 ║ 204 ║ SLOW     ║
╠═════════╬══════════════════════════════╬═════╬═════╬═════╬══════════╣
║ clang-6 ║ int32_t for loop unrolling   ║ 326 ║ 206 ║ 181 ║ ok       ║
║         ╠══════════════════════════════╬═════╬═════╬═════╬══════════╣
║         ║ int32_t while loop unrolling ║ 347 ║ 206 ║ 181 ║ ok       ║
╚═════════╩══════════════════════════════╩═════╩═════╩═════╩══════════╝

The this performance issue do not occur in 'for' loop unrolling test or clang either.


---


### compiler : `gcc`
### title : `Fails to use SSE bitwise ops for float-as-int manipulations`
### open_at : `2019-12-21T15:05:58Z`
### last_modified_date : `2021-11-28T06:31:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93039
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
(the non-regression part of PR 92905)

libm functions need to manipulate individual bits of float/double representations with good efficiency, but on x86 gcc typically does them on gprs even when it results in sse-gpreg-sse move chain:

float foo(float x)
{
    union {float f; unsigned i;} u = {x};
    u.i &= ~0x80000000;
    return u.f;
}

foo:
        movd    eax, xmm0
        and     eax, 2147483647
        movd    xmm0, eax
        ret

It's good to use bitwise ops on general registers if the source or destination needs to be in a general registe, but for cases like the above creating a roundtrip is not desirable.

(GCC gets this example right on aarch64; LLVM on x86 compiles this to SSE/AVX bitwise 'and', taking the immediate from memory)


---


### compiler : `gcc`
### title : `gcc doesn't optimize unaligned accesses to a 16-bit value on the x86 as well as it does a 32-bit value (or clang)`
### open_at : `2019-12-22T00:47:38Z`
### last_modified_date : `2021-10-03T00:29:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93040
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Given the following code:

    unsigned short get_unaligned_16 (unsigned char *p)
    {
	return p[0] | (p[1] << 8);
    }

    unsigned int get_unaligned_32 (unsigned char *p)
    {
	return get_unaligned_16 (p) | (get_unaligned_16 (p + 2) << 16);
    }

    unsigned int get_unaligned_32_alt (unsigned char *p)
    {
	return p[0] | (p[1] << 8) | (p[2] << 16) | (p[3] << 24);
    }


... Clang/LLVM (trunk, but it has the same results many versions back)
generates the following very nice output:

    get_unaligned_16:                       # @get_unaligned_16
	    movzx   eax, word ptr [rdi]
	    ret
    get_unaligned_32:                       # @get_unaligned_32
	    mov     eax, dword ptr [rdi]
	    ret
    get_unaligned_32_alt:                   # @get_unaligned_32_alt
	    mov     eax, dword ptr [rdi]
	    ret


Whereas gcc (trunk but ditto) generates:

    get_unaligned_16:
	    movzx   eax, BYTE PTR [rdi+1]
	    sal     eax, 8
	    mov     edx, eax
	    movzx   eax, BYTE PTR [rdi]
	    or      eax, edx
	    ret
    get_unaligned_32:
	    movzx   eax, BYTE PTR [rdi+3]
	    sal     eax, 8
	    mov     edx, eax
	    movzx   eax, BYTE PTR [rdi+2]
	    or      eax, edx
	    movzx   edx, BYTE PTR [rdi+1]
	    sal     eax, 16
	    mov     ecx, edx
	    movzx   edx, BYTE PTR [rdi]
	    sal     ecx, 8
	    or      edx, ecx
	    movzx   edx, dx
	    or      eax, edx
	    ret
    get_unaligned_32_alt:
	    mov     eax, DWORD PTR [rdi]
	    ret


Notice that in the "get_unaligned_32_alt" version, gcc _does_ detect
that this is really an unaligned access to a 32-bit integer and
reduces it to a single instruction on the x86, as that architecture
supports unaligned accesses.

However the 16-bit version, "get_unaligned_16", and get_unaligned_32
derived from that, it just uses the component bit-munching operations.

It does seem curious that gcc manages the 32-bit case, but fails on
the 16-bit case...

I tested gcc on godbolt.com, and Clang locally (and on godbolt).

Flags used:

   -O2 -march=skylake

-Os and -O3 yield the same results.

Versions:

   gcc (Compiler-Explorer-Build) 10.0.0 20191220 (experimental)
   clang version 10.0.0 (https://github.com/llvm/llvm-project.git b4dfa74a5d80b3602a5315fac2ef5f98b0e63708)


---


### compiler : `gcc`
### title : `bit-field optimizations get in the way of interchange`
### open_at : `2019-12-22T09:10:40Z`
### last_modified_date : `2023-07-19T04:14:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93042
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:
#define N 100
#define M 1111
struct S { int a : 3; int b : 17; int c : 12; };
struct S A[N][M];

int __attribute__((noinline))
foo (void)
{
  int i, j;

  for( i = 0; i < M; i++)
    for( j = 0; j < N; j++)
      {
	int t = 0;
	struct S s1 = A[j][i];
	if (s1.b)
	  t++;
	A[j][i].a = t;
      }

  return A[0][0].a + A[N-1][M-1].a;
}
----- CUT ----
If we change the loop slightly, to not to use a temp struct or even add a temp var to hold s1.b, the loop can be interchanged.
That is:
int __attribute__((noinline))
foo1 (void)
{
  int i, j;

  for( i = 0; i < M; i++)
    for( j = 0; j < N; j++)
      {
	int t = 0;
	if (A[j][i].b)
	  t++;
	A[j][i].a = t;
      }

  return A[0][0].a + A[N-1][M-1].a;
}
int __attribute__((noinline))
foo2 (void)
{
  int i, j;

  for( i = 0; i < M; i++)
    for( j = 0; j < N; j++)
      {
	int t = 0;
	struct S s1 = A[j][i];
	int t1 = s1.b;
	if (t1)
	  t++;
	A[j][i].a = t;
      }

  return A[0][0].a + A[N-1][M-1].a;
}
----- CUT ----
This shows up slightly different when I add lowering of bit-field accesses but in a different way.


---


### compiler : `gcc`
### title : `extra cast is not removed`
### open_at : `2019-12-22T12:06:31Z`
### last_modified_date : `2023-08-01T00:08:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93044
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
While looking into some bit-field code generation, I found a case where we don't remove a cast.  A slightly different testcase GCC does.
Take:
void f(signed char *a, unsigned char *c)
{
  unsigned short b = *a;
  *c = ((unsigned char)b);
}

---- CUT ---
We are not able to remove the cast there.

But if we change it to:

void f1(signed char *a, unsigned char *c)
{
  signed short b = *a;
  *c = ((unsigned char)b);
}
---- CUT ----
We are able to remove it.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] accumulation loops in stepanov_vector benchmark use more instruction level parpallelism`
### open_at : `2019-12-23T19:28:48Z`
### last_modified_date : `2023-07-07T10:36:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93055
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
stepanov_vector benchmark form https://gitlab.com/chriscox/CppPerformanceBenchmarks gets poor codegen on TestOneType<double>

Built with -march=bdver1 -O3 (but the regression happens on core too)

Clang compiles accumulation loops for testOneType<int> as follows:

       │        vpxor  %xmm0,%xmm0,%xmm0
       │        vpxor  %xmm1,%xmm1,%xmm1 
       │        vpxor  %xmm2,%xmm2,%xmm2
  0.05 │        vpxor  %xmm3,%xmm3,%xmm3=
       │        data16 nopw %cs:0x0(%rax,%rax,1)
  6.95 │ 300:┌─→vpaddd 0x5f0(%rsp,%rcx,4),%xmm0,%xmm0 
  0.05 │     │  vpaddd 0x600(%rsp,%rcx,4),%xmm1,%xmm1
  7.13 │     │  vpaddd 0x610(%rsp,%rcx,4),%xmm2,%xmm2
  0.16 │     │  vpaddd 0x620(%rsp,%rcx,4),%xmm3,%xmm3
       │     │  add    $0x10,%rcx
       │     │  cmp    $0x7dc,%rcx
  7.04 │     └──jne    300
  0.07 │        vpaddd %xmm0,%xmm1,%xmm0
  1.61 │        vpaddd %xmm0,%xmm2,%xmm0
       │        vpaddd %xmm0,%xmm3,%xmm0
       │        vpshuf $0x4e,%xmm0,%xmm1
  0.07 │        vpaddd %xmm1,%xmm0,%xmm0 
  0.02 │        vpshuf $0xe5,%xmm0,%xmm1

while GCC10 does:

       │ 1c0:   vxorps %xmm0,%xmm0,%xmm0 
       │        mov    %rbx,%rax
       │        nop
  2.25 │ 1d0:┌─→vpaddd (%rax),%xmm0,%xmm0 
  0.01 │     │  lea    0x2100(%rsp),%rdi
  0.95 │     │  add    $0x10,%rax
  1.04 │     │  cmp    %rax,%rdi
  2.24 │     └──jne    1d0  

Which runs slower:

test                                        description   absolute   operations   ratio with
number                                                    time       per second   test0
                                                                                
 0                 "int32_t accumulate pointer verify2"   1.06 sec   12440.17 M     1.00
 1                 "int32_t accumulate vector iterator"   1.06 sec   12458.15 M     1.00
 2         "int32_t accumulate pointer reverse reverse"   1.06 sec   12440.34 M     1.00
 3 "int32_t accumulate vector reverse_iterator reverse"   1.05 sec   12602.74 M     0.99
 4 "int32_t accumulate vector iterator reverse reverse"   1.04 sec   12749.27 M     0.98
 5 "int32_t accumulate array Riterator reverse reverse"   1.06 sec   12486.26 M     1.00
                                                                                
Total absolute time for int32_t Vector Accumulate: 6.32 sec                     
                                                                                
int32_t Vector Accumulate Penalty: 0.99                                         

compared to:
test                                        description   absolute   operations   ratio with
number                                                    time       per second   test0
                                                                                
 0                 "int32_t accumulate pointer verify2"   2.29 sec   5773.60 M     1.00
 1                 "int32_t accumulate vector iterator"   2.27 sec   5806.96 M     0.99
 2         "int32_t accumulate pointer reverse reverse"   2.26 sec   5830.72 M     0.99
 3 "int32_t accumulate vector reverse_iterator reverse"   2.27 sec   5827.45 M     0.99
 4 "int32_t accumulate vector iterator reverse reverse"   2.27 sec   5821.29 M     0.99
 5 "int32_t accumulate array Riterator reverse reverse"   2.27 sec   5826.58 M     0.99
                                                                                
Total absolute time for int32_t Vector Accumulate: 13.62 sec                    
                                                                                
int32_t Vector Accumulate Penalty: 0.99


---


### compiler : `gcc`
### title : `Poor codegen for heapsort in stephanov_vector benchmark`
### open_at : `2019-12-23T20:19:04Z`
### last_modified_date : `2023-05-05T07:07:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93056
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 47543
preprocessed benchmark with other tests disabled.

heap sort test in stepanov_vector benchmark runs about 50% slower when built with GCC10 compared to clang8 (with -O3 -march=native, on bdver1 hardware)

Clang profile is:
  33.19%  stepanov_vector  stepanov_vector    [.] benchmark::heapsort<std::reverse_iterator<std::reverse_iterator<__gnu_cxx::__normal_iterator<double*, std::vector<double, std
  16.73%  stepanov_vector  stepanov_vector    [.] benchmark::heapsort<reverse_iterator<reverse_iterator<__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocato
  16.07%  stepanov_vector  stepanov_vector    [.] benchmark::heapsort<__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > > >
  15.61%  stepanov_vector  stepanov_vector    [.] benchmark::heapsort<double*>
  15.61%  stepanov_vector  stepanov_vector    [.] benchmark::heapsort<std::reverse_iterator<std::reverse_iterator<double*> > >

while GCC profile is:

  32.90%  stepanov_vector  stepanov_vector    [.] benchmark::__sift_in<std::reverse_iterator<std::reverse_iterator<__gnu_cxx::__normal_iterator<double*, std::vector<double, st
  16.25%  stepanov_vector  stepanov_vector    [.] benchmark::__sift_in<std::reverse_iterator<std::reverse_iterator<double*> >, double>
  16.01%  stepanov_vector  stepanov_vector    [.] benchmark::__sift_in<reverse_iterator<reverse_iterator<__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocat
  15.94%  stepanov_vector  stepanov_vector    [.] benchmark::__sift_in<double*, double>
  15.73%  stepanov_vector  stepanov_vector    [.] benchmark::__sift_in<__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, double>


In the hottest function the internal iterator loop leads to worse code:

Clang:
       │       xor    %edx,%edx
       │             if ( *(begin+(i-1)) < *(begin+i)) 
  0.88 │180:┌─→vmovsd (%rax,%rsi,8),%xmm1
  6.16 │    │  xor    %edi,%edi
  6.45 │    │  vucomi -0x8(%rax,%rsi,8),%xmm1 
 22.87 │    │  seta   %dil 
  6.74 │    │  or     %rsi,%rdi
       │    │        *(begin + free) = *(begin+(i-1));
  7.62 │    │  mov    -0x8(%rax,%rdi,8),%rcx
 14.08 │    │  mov    %rcx,(%rax,%rdx,8)
  0.59 │    │  lea    -0x1(%rdi),%rdx 
       │    │    for ( i = 2*(free+1); i < count; i += i) {
  0.59 │    │  add    %rdi,%rdi
  2.05 │    │  mov    %rdi,%rsi
  2.05 │    │  cmp    %r11,%rdi
  0.29 │    └──jl     180
       │         if (i == count) { 

GCC:
       │      nop 
  1.81 │18:┌─→mov    %rax,%r10
       │   │        if ( *(begin+(i-1)) < *(begin+i))
  1.21 │1b:│  lea    -0x1(%rcx),%rax
       │   │_ZNK9__gnu_cxx17__normal_iteratorIPdSt6vectorIdSaIdEEEplEl(): 
  2.02 │   │  lea    0x0(,%rax,8),%r9
  0.40 │   │  lea    (%rsi,%r9,1),%r8
  0.40 │   │  lea    0x8(%rsi,%r9,1),%r9
       │   │_ZN9benchmark9__sift_inISt16reverse_iteratorIS1_IN9__gnu_cxx17__normal_iteratorIPdSt6vectorIdSaIdEEEEEEdEEvlT_lT0_():
  3.23 │   │  vmovsd (%r8),%xmm1
  7.66 │   │  vmovsd (%r9),%xmm2 
  0.40 │   │  vcomis %xmm1,%xmm2
  8.67 │   │↓ jbe    4d 
  4.64 │   │  vmovap %xmm2,%xmm1 
       │   │            i++;
 18.55 │   │  mov    %rcx,%rax 
       │   │  mov    %r9,%r8
  1.01 │   │  inc    %rcx 
       │   │    for ( i = 2*(free+1); i < count; i += i) {
  4.64 │4d:│  add    %rcx,%rcx
       │   │        *(begin + free) = *(begin+(i-1));
 19.76 │   │  vmovsd %xmm1,(%rsi,%r10,8)
       │   │    for ( i = 2*(free+1); i < count; i += i) {
  8.06 │   │  cmp    %rcx,%rdi 
       │   └──jg     18
       │            free = i-1; 
       │        } 

The code is:
void __sift_in(ptrdiff_t count, RandomAccessIterator begin, ptrdiff_t free_in, T next)
{
    ptrdiff_t i;
    ptrdiff_t free = free_in;

    // sift up the free node 
    for ( i = 2*(free+1); i < count; i += i) {
        if ( *(begin+(i-1)) < *(begin+i))
            i++;
        *(begin + free) = *(begin+(i-1));
        free = i-1;
    }

    // special case in sift up if the last inner node has only 1 child
    if (i == count) {
        *(begin + free) = *(begin+(i-1));
        free = i-1;
    }

    // sift down the new item next
    i = (free-1)/2;
    while( (free > free_in) && *(begin+i) < next) {
        *(begin + free) = *(begin+i);
        free = i;
        i = (free-1)/2;
    }

    *(begin + free) = next;
}


---


### compiler : `gcc`
### title : `char and char8_t does not talk with each other with memcpy. std::copy std::copy_n, std::fill, std::fill_n, std::uninitialized_copy std::uninitialized_copy_n, std::fill, std::uninitialized_fill_n fails to convert to memxxx functions`
### open_at : `2019-12-23T22:10:53Z`
### last_modified_date : `2021-10-06T10:40:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93059
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.0`
### severity : `normal`
### contents :
https://godbolt.org/z/SPktTz

All these functions should generate exactly the same assembly but they do not. GCC does not treat char and char8_t the same because libstdc++ does not do this check. I did my native manually fix and it works. (does not do mul_overflow something)

//g++ -S copy.cc -Ofast -std=c++2a
#include<cstring>
#include<algorithm>
#include<array>
#include<concepts>
#include<iterator>

auto copy_char8_t_array(char* out,std::array<char8_t,2> const& bits)
{
	return std::copy_n(bits.data(),bits.size(),out);
}


auto memcpy_char8_t_array(char* out,std::array<char8_t,2> const& bits)
{
	std::memcpy(out,bits.data(),bits.size());
	return bits.size();
}


auto copy_char_array(char* out,std::array<char,2> const& bits)
{
	return std::copy_n(bits.data(),bits.size(),out);
}


auto memcpy_char_array(char* out,std::array<char,2> const& bits)
{
	std::memcpy(out,bits.data(),bits.size());
	return bits.size();
}

auto copy_char_array_chars(char8_t* out,std::array<char8_t,2> const& bits)
{
	return std::copy_n(bits.data(),bits.size(),out);
}


auto memcpy_char_array_array_chars(char8_t* out,std::array<char8_t,2> const& bits)
{
	std::memcpy(out,bits.data(),bits.size());
	return bits.size();
}


auto copy_char_array(char8_t* out,std::array<char,2> const& bits)
{
	return std::copy_n(bits.data(),bits.size(),out);
}


auto memcpy_char_array(char8_t* out,std::array<char,2> const& bits)
{
	std::memcpy(out,bits.data(),bits.size());
	return bits.size();
}
template<std::input_iterator input_iter,std::input_iterator output_iter>
inline constexpr output_iter my_copy_n(input_iter first,std::size_t count,output_iter result)
{
	if constexpr(std::contiguous_iterator<input_iter>&&
		std::contiguous_iterator<output_iter>&&
		std::is_trivially_copyable_v<typename std::iterator_traits<input_iter>::value_type>&&
		std::is_trivially_copyable_v<typename std::iterator_traits<output_iter>::value_type>)
	{
		if constexpr(sizeof(std::is_trivially_copyable_v<typename std::iterator_traits<input_iter>::value_type>)
		==sizeof(std::is_trivially_copyable_v<typename std::iterator_traits<output_iter>::value_type>))
		{
			memcpy(std::to_address(result),std::to_address(first),
				sizeof(typename std::iterator_traits<input_iter>::value_type)*count);
			return result+count;
		}
	}
	return std::copy_n(first,count,result);
}

auto my_copy_char_array(char8_t* out,std::array<char,2> const& bits)
{
	return my_copy_n(bits.data(),bits.size(),out);
}
auto my_copy_char_array(char* out,std::array<char8_t,2> const& bits)
{
	return my_copy_n(bits.data(),bits.size(),out);
}


auto uninit_copy_char_array(char8_t* out,std::array<char,2> const& bits)
{
	return std::uninitialized_copy_n(bits.data(),bits.size(),out);
}




_Z29memcpy_char_array_array_charsPDuRKSt5arrayIDuLm2EE:
        movzwl  (%rsi), %eax
        movw    %ax, (%rdi)
        movl    $2, %eax
        ret
std::copy_n generates more assembly than it should
_Z15copy_char_arrayPDuRKSt5arrayIcLm2EE:
        movzbl  (%rsi), %eax
        movb    %al, (%rdi)
        movzbl  1(%rsi), %eax
        movb    %al, 1(%rdi)
        leaq    2(%rdi), %rax
        ret


---


### compiler : `gcc`
### title : `Loop distribution and NOP conversions`
### open_at : `2019-12-24T10:37:34Z`
### last_modified_date : `2020-01-09T09:11:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93063
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
This comes from PR 93059.

void f(signed short*__restrict p,unsigned short*__restrict q,int n){
  for(int i=0;i<n;++i)
    *p++=*q++;
}

It would be nice if ldist could handle the NOP conversion and produce memcpy/memmove as in the case without a conversion.


---


### compiler : `gcc`
### title : `std::__lg (and all functions that use it) generates suboptimal code`
### open_at : `2019-12-25T06:05:17Z`
### last_modified_date : `2020-11-16T21:54:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93071
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
On Godbolt, the following code:

```
#include<algorithm>

int f(int x){
    return std::__lg(x);
}
int g(int x){
    return 31-__builtin_clz(x);
}
int h(int x){
    return 31^__builtin_clz(x);
}
```

compiles to:

```
f(int):
        bsr     edi, edi
        mov     eax, 31
        xor     edi, 31
        sub     eax, edi
        ret
g(int):
        bsr     edi, edi
        mov     eax, 31
        xor     edi, 31
        sub     eax, edi
        ret
h(int):
        bsr     eax, edi
        ret
```

Only `h` are optimal. `f` and `g` has a tiny slowdown with redundant `31-` and `31^`.

Suggestion: The better option would be to recognize `(power of 2 - 1) ^ x` and `(power of 2 - 1) - x` as equal, but otherwise it's possible to just change the `__lg` implementation to use `^` instead of `-` when the size is a power of 2.


---


### compiler : `gcc`
### title : `Missing fma and round functions auto-vectorization with x86-64 (sse2)`
### open_at : `2019-12-26T22:44:17Z`
### last_modified_date : `2022-10-21T16:48:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93078
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
The next code (with -Ofast):

#include <cmath>

using namespace std;

float a[4], b[4], c[4];

void vec_fma() {
    for (int i = 0; i < 4; ++i) c[i] = fma(a[i], b[i], c[i]);
}

void vec_round() {
    for (int i = 0; i < 4; ++i) c[i] = round(a[i]);
}

void vec_floor() {
    for (int i = 0; i < 4; ++i) c[i] = floor(a[i]);
}

void vec_ceil() {
    for (int i = 0; i < 4; ++i) c[i] = ceil(a[i]);
}

void vec_trunc() {
    for (int i = 0; i < 4; ++i) c[i] = trunc(a[i]);
}

void vec_rint() {
    for (int i = 0; i < 4; ++i) c[i] = rint(a[i]);
}

void vec_nearbyint() {
    for (int i = 0; i < 4; ++i) c[i] = nearbyint(a[i]);
}

Compiles without auto-vectorization:

vec_fma():
        sub     rsp, 8
        movss   xmm2, DWORD PTR c[rip]
        movss   xmm1, DWORD PTR b[rip]
        movss   xmm0, DWORD PTR a[rip]
        call    fmaf
        movss   xmm2, DWORD PTR c[rip+4]
        movss   xmm1, DWORD PTR b[rip+4]
        movss   DWORD PTR c[rip], xmm0
        movss   xmm0, DWORD PTR a[rip+4]
        call    fmaf
        movss   xmm2, DWORD PTR c[rip+8]
        movss   xmm1, DWORD PTR b[rip+8]
        movss   DWORD PTR c[rip+4], xmm0
        movss   xmm0, DWORD PTR a[rip+8]
        call    fmaf
        movss   xmm2, DWORD PTR c[rip+12]
        movss   xmm1, DWORD PTR b[rip+12]
        movss   DWORD PTR c[rip+8], xmm0
        movss   xmm0, DWORD PTR a[rip+12]
        call    fmaf
        movss   DWORD PTR c[rip+12], xmm0
        add     rsp, 8
        ret
vec_round():
        movss   xmm3, DWORD PTR a[rip]
        movss   xmm0, DWORD PTR .LC1[rip]
        movss   xmm2, DWORD PTR .LC0[rip]
        movaps  xmm4, xmm0
        movaps  xmm1, xmm3
        andps   xmm1, xmm0
        comiss  xmm2, xmm1
        jbe     .L5
        addss   xmm1, DWORD PTR .LC2[rip]
        andnps  xmm4, xmm3
        movaps  xmm3, xmm4
        cvttss2si       eax, xmm1
        pxor    xmm1, xmm1
        cvtsi2ss        xmm1, eax
        orps    xmm3, xmm1
.L5:
        movss   DWORD PTR c[rip], xmm3
        movss   xmm3, DWORD PTR a[rip+4]
        movaps  xmm4, xmm0
        movaps  xmm1, xmm3
        andps   xmm1, xmm0
        comiss  xmm2, xmm1
        jbe     .L6
        addss   xmm1, DWORD PTR .LC2[rip]
        andnps  xmm4, xmm3
        movaps  xmm3, xmm4
        cvttss2si       eax, xmm1
        pxor    xmm1, xmm1
        cvtsi2ss        xmm1, eax
        orps    xmm3, xmm1
.L6:
        movss   DWORD PTR c[rip+4], xmm3
        movss   xmm3, DWORD PTR a[rip+8]
        movaps  xmm4, xmm0
        movaps  xmm1, xmm3
        andps   xmm1, xmm0
        comiss  xmm2, xmm1
        jbe     .L7
        addss   xmm1, DWORD PTR .LC2[rip]
        andnps  xmm4, xmm3
        movaps  xmm3, xmm4
        cvttss2si       eax, xmm1
        pxor    xmm1, xmm1
        cvtsi2ss        xmm1, eax
        orps    xmm3, xmm1
.L7:
        movss   DWORD PTR c[rip+8], xmm3
        movss   xmm3, DWORD PTR a[rip+12]
        movaps  xmm1, xmm3
        andps   xmm1, xmm0
        comiss  xmm2, xmm1
        jbe     .L8
        addss   xmm1, DWORD PTR .LC2[rip]
        andnps  xmm0, xmm3
        cvttss2si       eax, xmm1
        pxor    xmm1, xmm1
        cvtsi2ss        xmm1, eax
        movaps  xmm3, xmm1
        orps    xmm3, xmm0
.L8:
        movss   DWORD PTR c[rip+12], xmm3
        ret

...

vec_nearbyint():
        sub     rsp, 8
        movss   xmm0, DWORD PTR a[rip]
        call    nearbyintf
        movss   DWORD PTR c[rip], xmm0
        movss   xmm0, DWORD PTR a[rip+4]
        call    nearbyintf
        movss   DWORD PTR c[rip+4], xmm0
        movss   xmm0, DWORD PTR a[rip+8]
        call    nearbyintf
        movss   DWORD PTR c[rip+8], xmm0
        movss   xmm0, DWORD PTR a[rip+12]
        call    nearbyintf
        movss   DWORD PTR c[rip+12], xmm0
        add     rsp, 8
        ret

In comparison, the icc compiler also fails to auto-vectorize fma in sse2 mode (without vfmadd132ps native instruction of fma), but it does have vectorized versions of rounding functions (in sse2 mode, withtout roundps native instruction of sse4.1):

vec_round():
        push      rsi
        movups    xmm0, XMMWORD PTR a[rip]
        call      QWORD PTR [__svml_roundf4@GOTPCREL+rip]
        movups    XMMWORD PTR c[rip], xmm0
        pop       rcx
        ret

...

vec_nearbyint():
        push      rsi
        movups    xmm0, XMMWORD PTR a[rip]
        call      QWORD PTR [__svml_nearbyintf4@GOTPCREL+rip]
        movups    XMMWORD PTR c[rip], xmm0
        pop       rcx
        ret

Compiler Explorer Code: https://gcc.godbolt.org/z/xwKluO

With the -msse4.1 flag the gcc compiler stills fail to auto-vectorize fma and nearbyint, i not sure why dont auto-vectorize the function round directly to "roundps xmm0, XMMWORD PTR a[rip], 0":

vec_round():
        movaps  xmm0, XMMWORD PTR a[rip]
        andps   xmm0, XMMWORD PTR .LC1[rip]
        orps    xmm0, XMMWORD PTR .LC0[rip]
        addps   xmm0, XMMWORD PTR a[rip]
        roundps xmm0, xmm0, 3
        movaps  XMMWORD PTR c[rip], xmm0
        ret
vec_floor():
        roundps xmm0, XMMWORD PTR a[rip], 1
        movaps  XMMWORD PTR c[rip], xmm0
        ret
vec_ceil():
        roundps xmm0, XMMWORD PTR a[rip], 2
        movaps  XMMWORD PTR c[rip], xmm0
        ret
vec_trunc():
        roundps xmm0, XMMWORD PTR a[rip], 3
        movaps  XMMWORD PTR c[rip], xmm0
        ret
vec_rint():
        roundps xmm0, XMMWORD PTR a[rip], 4
        movaps  XMMWORD PTR c[rip], xmm0
        ret
vec_nearbyint():
        mov     eax, OFFSET FLAT:a
        movss   xmm0, DWORD PTR [rax]
        roundss xmm0, xmm0, 12
        movss   DWORD PTR c[rip], xmm0
        movss   xmm0, DWORD PTR [rax+4]
        roundss xmm0, xmm0, 12
        movss   DWORD PTR c[rip+4], xmm0
        movss   xmm0, DWORD PTR [rax+8]
        roundss xmm0, xmm0, 12
        movss   DWORD PTR c[rip+8], xmm0
        movss   xmm0, DWORD PTR [rax+12]
        roundss xmm0, xmm0, 12
        movss   DWORD PTR c[rip+12], xmm0
        ret

Compiler Explorer Code: https://gcc.godbolt.org/z/Rc63b9

With -mfma flag, the nearbyint function continues without auto-vectorization:

vec_fma():
        vmovaps xmm1, XMMWORD PTR c[rip]
        vmovaps xmm0, XMMWORD PTR a[rip]
        vfmadd132ps     xmm0, xmm1, XMMWORD PTR b[rip]
        vmovaps XMMWORD PTR c[rip], xmm0
        ret
vec_round():
        vmovaps xmm1, XMMWORD PTR a[rip]
        vandps  xmm0, xmm1, XMMWORD PTR .LC1[rip]
        vorps   xmm0, xmm0, XMMWORD PTR .LC0[rip]
        vaddps  xmm0, xmm0, xmm1
        vroundps        xmm0, xmm0, 3
        vmovaps XMMWORD PTR c[rip], xmm0
        ret
vec_floor():
        vroundps        xmm0, XMMWORD PTR a[rip], 1
        vmovaps XMMWORD PTR c[rip], xmm0
        ret
vec_ceil():
        vroundps        xmm0, XMMWORD PTR a[rip], 2
        vmovaps XMMWORD PTR c[rip], xmm0
        ret
vec_trunc():
        vroundps        xmm0, XMMWORD PTR a[rip], 3
        vmovaps XMMWORD PTR c[rip], xmm0
        ret
vec_rint():
        vroundps        xmm0, XMMWORD PTR a[rip], 4
        vmovaps XMMWORD PTR c[rip], xmm0
        ret
vec_nearbyint():
        mov     eax, OFFSET FLAT:a
        vmovss  xmm1, DWORD PTR [rax]
        vmovss  xmm2, DWORD PTR [rax+4]
        vmovss  xmm3, DWORD PTR [rax+8]
        vmovss  xmm4, DWORD PTR [rax+12]
        vroundss        xmm0, xmm1, xmm1, 12
        vmovss  DWORD PTR c[rip], xmm0
        vroundss        xmm0, xmm2, xmm2, 12
        vmovss  DWORD PTR c[rip+4], xmm0
        vroundss        xmm0, xmm3, xmm3, 12
        vmovss  DWORD PTR c[rip+8], xmm0
        vroundss        xmm0, xmm4, xmm4, 12
        vmovss  DWORD PTR c[rip+12], xmm0
        ret

Compiler Explorer Code: https://gcc.godbolt.org/z/_WeniA


---


### compiler : `gcc`
### title : `insert of an extraction on the same location is not optimized`
### open_at : `2019-12-27T11:52:38Z`
### last_modified_date : `2023-08-22T09:36:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93080
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
testcase:
/* { dg-do compile } */
/* { dg-options "-O2 -fdump-tree-optimized" } */
#define vector __attribute__((__vector_size__(16) ))


vector int g(vector int a)
{
  int b = a[0];
  a[0] = b;
  return a;
}

/* { dg-final { scan-tree-dump-times "BIT_INSERT_EXPR" 0 "optimized" } } */
/* { dg-final { scan-tree-dump-times "BIT_FIELD_REF" 0 "optimized" } } */
---- CUT ----


---


### compiler : `gcc`
### title : `insertation followed by another inseration to the same location is not optimized away`
### open_at : `2019-12-27T11:54:56Z`
### last_modified_date : `2021-07-28T07:04:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93081
### status : `ASSIGNED`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Testcase:
/* { dg-do compile } */
/* { dg-options "-O2 -fdump-tree-optimized" } */


#define vector __attribute__((__vector_size__(16) ))

vector int f(vector int a, int b, int c)
{
  a[0] = b;
  a[0] = c;
  return a;
}


/* { dg-final { scan-tree-dump-times "BIT_INSERT_EXPR" 1 "optimized" } } */

--- CUT ---
Code added to match.pd which will fix it:
/* Inserting into the same location as previously just over writes the location. */
(simplify
 (bit_insert (bit_insert @0 @1 @2) @3 @2)
 (bit_insert @0 @3 @2))


---


### compiler : `gcc`
### title : `[optimization] is it legal to avoid accessing const local array from stack ?`
### open_at : `2019-12-30T11:03:33Z`
### last_modified_date : `2021-09-25T01:34:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93102
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
test code:

int foo (int m, int n)
{
  const int C00002029[10] = {0,1,2,3,2,3,0,1,2,3};
	
  int index, sum = 0;
  
  for (index=m; index<n; index ++)
    sum += C00002029[index];

  return sum;
}

in general, we may store the const value {0,1,2,3,2,3,0,1,2,3} in rodata section, then use memcpy to assign the local array C00002029.

But as we known, in above code, the array is only accessed one time, so can we optimize it such as following logic? this can avoid initialization, and the performance can be better.

const int C00002029_xxx[10] = {0,1,2,3,2,3,0,1,2,3};
int foo (int m, int n)
{
  int index, sum = 0;
  
  for (index=m; index<n; index ++)
    sum += C00002029_xxx[index];

  return sum;
}


---


### compiler : `gcc`
### title : `match.pd uses convert/tree_nop_conversion_p instead of nop_convert`
### open_at : `2020-01-01T22:22:57Z`
### last_modified_date : `2023-07-22T06:35:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93116
### status : `ASSIGNED`
### tags : `internal-improvement, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
There are a few places in match.pd which uses convert followed by a check to tree_nop_conversion_p but they could be simplified down to just using nop_convert instead.


---


### compiler : `gcc`
### title : `>>32<<32 is not always converted into &~0ffffffffull at the tree level`
### open_at : `2020-01-01T23:05:13Z`
### last_modified_date : `2020-01-09T10:26:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93118
### status : `RESOLVED`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:

typedef unsigned long long u64;
void f(u64 *aa, u64 a)
{
  u64 b = a>>32;
  int c = b;
  u64 d = c;
  *aa = d<<32;
}

This should produce the same as:
typedef unsigned long long u64;
void f1(u64 *aa, u64 a)
{
  u64 b = a>>32;
  unsigned c = b;
  u64 d = c;
  *aa = d<<32;
}

But the first produces in .optimized:
  b_3 = a_2(D) >> 32;
  c_4 = (int) b_3;
  d_5 = (u64) c_4;
  _1 = d_5 << 32;
  *aa_7(D) = _1;

While the second one produces:
  _1 = a_2(D) & 18446744069414584320;
  *aa_4(D) = _1;


---


### compiler : `gcc`
### title : `Lacking basic optimization around __int128 bitwise operations against constants`
### open_at : `2020-01-02T07:57:55Z`
### last_modified_date : `2022-03-08T16:20:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93123
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `enhancement`
### contents :
unsigned __int128 and128WithConst(unsigned __int128 a)
{
   unsigned __int128 c128 = (((unsigned __int128)(~0ULL)) << 64) | ((unsigned __int128)(~0xFULL));
   return a & c128;
}

gcc -O2 -maix64 -save-temps int128.C

Output:
._Z12andWithConsto:
LFB..0:
        li 10,-1
        li 11,-16
        and 3,3,10
        and 4,4,11
        blr

Expected result: Single instruction: rldicr on low part (register 4).
Bitwise and with 0xFF..FF is a no-op.
Bitwise and with 0xFF..F0 can be done using rldicr.


---


### compiler : `gcc`
### title : `PPC altivec vec_promote creates unnecessary xxpermdi instruction`
### open_at : `2020-01-02T08:30:24Z`
### last_modified_date : `2022-01-14T08:37:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93127
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
vec_promote can leave half of register undefined and therefore should not issue extra instruction.

Input:
#include <altivec.h>

double vmax(double a, double b)
{
#ifdef _BIG_ENDIAN
   const int PREF = 0;
#else
   const int PREF = 1;
#endif
   vector double va = vec_promote(a,PREF);
   vector double vb = vec_promote(b,PREF);
   return vec_extract(vec_max(va, vb), PREF);
}

Output:
._Z4vmaxdd:
LFB..0:
        xxpermdi 2,2,2,0
        xxpermdi 1,1,1,0
        xvmaxdp 1,1,2
         # vec_extract to same register
        blr

Both xxpermdi are unnecessary.


---


### compiler : `gcc`
### title : `PPC small floating point constants can be constructed using vector operations`
### open_at : `2020-01-02T09:04:43Z`
### last_modified_date : `2020-01-06T16:18:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93128
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
Input:
#include <altivec.h>
double d2()
{
  return 2.0;
}

vector double v2()
{
   return vec_splats(2.0);
}

gcc -O2 -maix64 -mcpu=power7 -maltivec const.C

gcc uses load from constant area.

Better alternative for "integer" values -15.0..+16.0.
        vspltisw 0,<integer value>
        xvcvsxwdp 1,32

0.0 already get constructed using xxlxor, which is great.

Similar things can be done for 
vector float v2f()
{
   return vec_splats(2.0f);
}


---


### compiler : `gcc`
### title : `PPC memset not using vector instruction on >= Power8`
### open_at : `2020-01-02T09:34:41Z`
### last_modified_date : `2022-03-08T16:21:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93129
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
Input:
void memclear16(char *p)
{
  memset(p, 0, 16);
}

void memFF16(char *p)
{
  memset(p, 0xFF, 16);
}

Output:
._Z10memclear16Pc:
LFB..0:
        li 9,0
        std 9,0(3)
        std 9,8(3)
        blr

._Z7memFF16Pc:
LFB..1:
        mflr 0
        li 5,16
        li 4,255
        std 0,16(1)
        stdu 1,-112(1)
LCFI..0:
        bl .memset
        nop
        addi 1,1,112
LCFI..1:
        ld 0,16(1)
        mtlr 0
LCFI..2:
        blr

Expected output:
vsplitb + vector store

Unaligned vector stores only perform on >= Power8, vector stores should be used only on >= Power8. On Power7 you would need to know it is at least 8-byte aligned.

vsplitb has the -16..+15 limit. On Power9 a splat for 0.255 exists.


---


### compiler : `gcc`
### title : `PPC simple memset not inlined`
### open_at : `2020-01-02T09:56:24Z`
### last_modified_date : `2022-03-08T16:20:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93130
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `enhancement`
### contents :
Input:
void memspace16(char *p)
{
  memset(p, ' ', 16);
}


Expected result:
li 4,0x2020
rldimi 4,4,16,0
rldimi 4,4,32,0
std 4,0(3)

Splatting the memset input to 64-bit can be done using li + 2xrldimi.
But also

._Z13memspace16OptPc:
LFB..3:
        lis 9,0x2020
        ori 9,9,0x2020
        sldi 9,9,32
        oris 9,9,0x2020
        ori 9,9,0x2020
        std 9,0(3)
        std 9,8(3)
        blr

would perform better than the function call to memset.


---


### compiler : `gcc`
### title : `((a&8) == 8) && ((a&2) == 2) is not optimized to (a&(8|2)) == (8|2)`
### open_at : `2020-01-02T10:40:24Z`
### last_modified_date : `2023-06-13T15:40:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93131
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
int f(int a)
{
  char b = (a&8) == 8;
  char c = (a&2) == 2;
  char d = (a&4) == 4;
  return b&c&d;
}

int f1(int a)
{
  return (a&(8|2|4)) == (8|2|4);
}


---


### compiler : `gcc`
### title : `Missed optimization : Use of adc when checking overflow`
### open_at : `2020-01-03T12:26:15Z`
### last_modified_date : `2022-11-08T00:13:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93141
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
Created attachment 47584
Complete code for computing dot product (same as the godbolt link)

Consider emulating 192-bit integer using a 128-bit integer and a 64-bit integer. In the code sample this emulated integer is used to compute dot product of two uint64_t vectors of length N. 

    // function to compute dot product of two vectors
    using u128 = unsigned __int128;
    const int N = 2048;
    uint64_t a[N], b[N];
    u128 sum = 0;
    uint64_t overflow = 0;
    for(int i=0;i<N;++i){
        u128 prod = (u128) a[i] * (u128) b[i];
        sum += prod;
        // gcc branches, clang just uses: adc overflow, 0
        overflow += sum<prod;
    }

To check for overflow in 128-bit addition a branch statement is produced in assembly while it can be substituted with `adc`. This idiom of manual checking of Carry Flag works with clang.

Thus a branch statement can be eliminated in this case.

g++ 9.2.0 with -O3 -Wall -Wextra -march=broadwell -fno-unroll-loops produces:
.L3:
        mov     rdx, QWORD PTR [rdi+rcx]
        mulx    r9, r8, QWORD PTR [rsi+rcx]
        add     r14, r8
        adc     r15, r9

        cmp     r14, r8    <----- branch because of : overflow += sum < prod;
        mov     rax, r15
        sbb     rax, r9
        adc     r10, 0

        add     rcx, 8
        cmp     rcx, 16384
        jne     .L3

whereas clang++ 9.0.0 with -O3 -Wall -Wextra -march=broadwell -fno-unroll-loops produces:

.LBB0_1:               
        mov     rax, qword ptr [rsi + 8*rcx]
        mul     qword ptr [rdi + 8*rcx]
        add     r10, rax
        adc     r9, rdx

        adc     r11, 0     <------- branch eliminated in clang

        inc     rcx
        cmp     rcx, 2048
        jne     .LBB0_1

For complete source code please visit: https://godbolt.org/z/ktdA4b


---


### compiler : `gcc`
### title : `Missed optimization : Use of adc when checking overflow`
### open_at : `2020-01-03T12:29:11Z`
### last_modified_date : `2020-01-09T10:43:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93142
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
Created attachment 47585
Complete code for computing dot product (same as the godbolt link)

Consider emulating 192-bit integer using a 128-bit integer and a 64-bit integer. In the code sample this emulated integer is used to compute dot product of two uint64_t vectors of length N. 

    // function to compute dot product of two vectors
    using u128 = unsigned __int128;
    const int N = 2048;
    uint64_t a[N], b[N];
    u128 sum = 0;
    uint64_t overflow = 0;
    for(int i=0;i<N;++i){
        u128 prod = (u128) a[i] * (u128) b[i];
        sum += prod;
        // gcc branches, clang just uses: adc overflow, 0
        overflow += sum<prod;
    }

To check for overflow in 128-bit addition a branch statement is produced in assembly while it can be substituted with `adc`. This idiom of manual checking of Carry Flag works with clang.

Thus a branch statement can be eliminated in this case.

g++ 9.2.0 with -O3 -Wall -Wextra -march=broadwell -fno-unroll-loops produces:
.L3:
        mov     rdx, QWORD PTR [rdi+rcx]
        mulx    r9, r8, QWORD PTR [rsi+rcx]
        add     r14, r8
        adc     r15, r9

        cmp     r14, r8    <----- branch because of : overflow += sum < prod;
        mov     rax, r15
        sbb     rax, r9
        adc     r10, 0

        add     rcx, 8
        cmp     rcx, 16384
        jne     .L3

whereas clang++ 9.0.0 with -O3 -Wall -Wextra -march=broadwell -fno-unroll-loops produces:

.LBB0_1:               
        mov     rax, qword ptr [rsi + 8*rcx]
        mul     qword ptr [rdi + 8*rcx]
        add     r10, rax
        adc     r9, rdx

        adc     r11, 0     <------- branch eliminated in clang

        inc     rcx
        cmp     rcx, 2048
        jne     .LBB0_1

For complete source code please visit: https://godbolt.org/z/ktdA4b


---


### compiler : `gcc`
### title : `(A&N) == CST1 &( ((A&M)==CST2) | ((A&O)==CST3) ) is not simplified`
### open_at : `2020-01-04T09:36:31Z`
### last_modified_date : `2023-06-13T15:40:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93150
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
(x&N) == CST1 & ( ((x&M)==CST2) | ((x&O)==CST3) )
Should be simplified into:
((x&(N|M)) == (CST1|CST2)) | ((x&(N|O)==(CST1|CST3))

Note this can be done for all &/| and ==/!= .
That is:
(for bitop1 (bit_and bit_or)
     bitop2 (bit_or bit_and)
 (for cmp (eq ne)
  (simplify
   (bitop1:c
    (cmp (bit_and @0 INTEGER_CST@N) INTEGER_CST@CST1)
    (bitop2
     (cmp (bit_and @0 INTEGER_CST@N) INTEGER_CST@CST2)
     (cmp (bit_and @0 INTEGER_CST@N) INTEGER_CST@CST3)))
....


---


### compiler : `gcc`
### title : `rldimi is sometimes not produced because combine gets in the way`
### open_at : `2020-01-06T13:55:31Z`
### last_modified_date : `2020-01-06T13:55:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93171
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:
typedef unsigned long long uint64_t;
typedef uint64_t cvmx_pow_tag_type_t;
typedef uint64_t cvmx_pow_tag_op_t;
typedef union
{
    uint64_t u64;
    struct
    {

        uint64_t no_sched : 1;
        uint64_t unused : 2;
        uint64_t index :13;
        cvmx_pow_tag_op_t op : 4;
        uint64_t unused2 : 2;
        uint64_t qos : 3;
        uint64_t grp : 4;
        cvmx_pow_tag_type_t type : 3;
        uint64_t tag :32;
    } s_cn38xx;
    struct {

        uint64_t no_sched : 1;
        cvmx_pow_tag_op_t op : 4;
        uint64_t unused1 : 16;
        uint64_t grp : 6;
        uint64_t unused3 : 3;
        cvmx_pow_tag_type_t type : 2;
        uint64_t tag :32;
    } s_cn68xx_other;

} cvmx_pow_tag_req_t;

int c;

#define CVMX_POW_TAG_OP_SWTAG 0

void f(uint64_t *a, int tag, int tag_type)
{
  cvmx_pow_tag_req_t tag_req;
  tag_req.u64 = 0;
  if (c)
    {
      tag_req.s_cn68xx_other.op = 0;
      tag_req.s_cn68xx_other.tag = tag;
  asm("":"+r"(tag_req.u64));
      tag_req.s_cn68xx_other.type = tag_type;
    }
  else
    {
      tag_req.s_cn38xx.op = 0;
      tag_req.s_cn38xx.tag = tag;
//  asm("":"+r"(tag_req.u64));
      tag_req.s_cn38xx.type = tag_type;
    }

  *a = tag_req.u64;
}
--- CUT ---
On the path which contains the asm, we get:
        rldimi 4,5,32,30

While on the other path, we get:
        rldic 5,5,32,29
        or 4,4,5

NOTE the asm is just there to force combine not to happen :).


---


### compiler : `gcc`
### title : `with AVX512 masked mov assigning zero can use {z}`
### open_at : `2020-01-06T14:06:58Z`
### last_modified_date : `2020-01-20T14:35:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93172
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Testcase (cf. https://godbolt.org/z/DMQf9-):

#include <x86intrin.h>

// missed optimization:
__m512 f(__m512 x, __mmask16 k) {
    return _mm512_mask_mov_ps(x, _knot_mask16(k), __m512());
}

// f should be translated like this:
__m512 g(__m512 x, __mmask16 k) {
    return _mm512_maskz_mov_ps(k, x);
}

GCC translates f to:

  vxorps xmm1, xmm1, xmm1
  kmovw k1, edi
  vmovaps zmm0{k1}, zmm1

. It could use:

  kmovd k0, edi
  knotw k1, k0
  vmovaps zmm0 {k1} {z}, zmm0

like g does. I.e. whenever a constant zero is assigned under a negated write-mask, the {z} variant of vmovaps should be used.

Clang even uses {z} for `_mm512_mask_mov_ps(x, k, __m512())` (i.e. without negation of the mask), which is unclear whether that's actually a pessimization: https://godbolt.org/z/Nn4qXz


---


### compiler : `gcc`
### title : `PPC: inefficient 64-bit constant consecutive ones`
### open_at : `2020-01-06T20:12:41Z`
### last_modified_date : `2023-10-08T02:36:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93176
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
All 64-bit constants containing a sequence of ones can be constructed with 2 instructions (li/lis + rldicl). gcc creates up to 5 instructions.

Input:
unsigned long long onesLI()
{
   return 0x00FFFFFFFFFFFF00ULL; // expected: li 3,0xFF00 ; rldicl 3,3,0,8
}

unsigned long long onesLIS()
{
   return 0x00FFFFFFFF000000ULL; // expected: lis 3,0xFF00 ; rldicl 3,3,0,8
}

unsigned long long onesHI()
{
   return 0x00FFFF0000000000ULL; // expected: lis 3,0xFFFF ; rldicl 3,3,8,8
}

Command line:
gcc -O2 -maix64 -save-temps const.C

Output:
._Z6onesLIv:
LFB..2:
        lis 3,0xff
        ori 3,3,0xffff
        sldi 3,3,32
        oris 3,3,0xffff
        ori 3,3,0xff00
        blr


._Z7onesLISv:
LFB..3:
        lis 3,0xff
        ori 3,3,0xffff
        sldi 3,3,32
        oris 3,3,0xff00
        blr

._Z6onesHIv:
LFB..4:
        lis 3,0xff
        ori 3,3,0xff00
        sldi 3,3,32
        blr


---


### compiler : `gcc`
### title : `PPC: inefficient 64-bit constant generation if msb is off in low 16 bit`
### open_at : `2020-01-06T20:22:49Z`
### last_modified_date : `2023-05-10T09:01:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93178
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
Input:
unsigned long long hi16msbon_low16msboff()
{
   return 0x87654321ULL; // expected: li 3,0x4321 ; oris 3,0x8765
}

Command line:
gcc -O2 -maix64 -save-temps const.C

Output:
._Z21hi16msbon_low16msboffv:
LFB..1:
        lis 3,0x8765
        ori 3,3,0x4321
        rldicl 3,3,0,32
        blr


---


### compiler : `gcc`
### title : `SVE does not use neg as conditional`
### open_at : `2020-01-07T10:35:15Z`
### last_modified_date : `2023-08-12T00:33:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93183
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:
typedef unsigned char uint8_t;

static inline uint8_t x264_clip_uint8( uint8_t x )
{
  uint8_t t = -x;
  t = t>>7;
  uint8_t t1 = x&(~63);
  return t1 ? t : x;
}


void mc_weight( uint8_t *__restrict dst, uint8_t *__restrict src, int n)
{
        for( int x = 0; x < n*16; x++ )
            dst[x] = x264_clip_uint8(src[x]);
}

---- CUT ----
Currently (with -O3 -mcpu=generic+sve), we get for the inner loop:
.L3:
        ld1b    z0.b, p0/z, [x1, x3]
        movprfx z2, z0
        and     z2.b, z2.b, #0xc0
        neg     z1.b, p1/m, z0.b
        cmpne   p2.b, p1/z, z2.b, #0
        movprfx z0.b, p2/m, z1.b
        lsr     z0.b, p2/m, z0.b, #7
        st1b    z0.b, p0, [x0, x3]
        incb    x3
        whilelo p0.b, w3, w2
        b.any   .L3

--- CUT ---
But we should be able to do:
.L3:
        ld1b    z0.b, p0/z, [x1, x3]
        and     z2.b, z0.b, #0xc0
        cmpne   p2.b, p1/z, z2.b, #0 ;;; << I think p1/z here really should be p0/z also
        neg     z0.b, p2/m, z0.b ;;; <<< This one should be conditional
        lsr     z0.b, p2/m, z0.b, #7
        st1b    z0.b, p0, [x0, x3]
        incb    x3
        whilelo p0.b, w3, w2
        b.any   .L3


---


### compiler : `gcc`
### title : `Sub-optimal code optimization on struct/combound constexpr (gcc vs. clang)`
### open_at : `2020-01-09T09:46:25Z`
### last_modified_date : `2021-04-15T06:05:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93210
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
Created attachment 47617
Sample-Code to show different optimization of gcc vs. clang

Hi,

The attached source-code shows that the current elease of GCC does not use the given constexpr for a direct value-compare and needs a definition for a indirect (mem-) operation. But, if the same compare uses a temporary copy, the emitted code is optimal.

Additionaly, the comparision of assembler-output between gcc 9.2 to clang 9.0 will show, that clang will generate in booth cases a direct value-compare.
IMHO, a "more" sub-optimal code will be generated with the flag -fPIC; I.e. two additional read-cycles.

Assembler-Code output 1 (x86-64 gcc 8.2, -O2):
[sub-optimal code of Bar::CheckId1]

    Bar::CheckId1(Foo const&) const:
        mov     rax, QWORD PTR [rsi]
        cmp     QWORD PTR Bar::Should[rip], rax
        sete    al
        ret
    Bar::CheckId2(Foo const&) const:
        movabs  rax, 180388626445
        cmp     rax, QWORD PTR [rsi]
        sete    al
        ret
    Bar::Should:
        .long   13
        .long   42
--------

Assembler-Code output 2 (x86-64 clang 9.0, -O2):
[same code for Bar::CheckId1 and CheckId2]

    Bar::CheckId1(Foo const&) const:
        movabs  rax, 180388626445
        cmp     qword ptr [rsi], rax
        sete    al
        ret
    Bar::CheckId2(Foo const&) const:
        movabs  rax, 180388626445
        cmp     qword ptr [rsi], rax
        sete    al
        ret
Bar::Should:
        .long   13                      # 0xd
        .long   42                      # 0x2a
--------

Best regards from Salzburg,
Markus


---


### compiler : `gcc`
### title : `vector defined using inserts is not converted into constructors`
### open_at : `2020-01-12T12:58:35Z`
### last_modified_date : `2023-05-16T20:33:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93237
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take this stupid code:
/* { dg-do compile } */
/* { dg-options "-O2 -fdump-tree-optimized" } */


#define vector __attribute__((__vector_size__(4*sizeof(int)) ))

vector int f(int b)
{
  vector int a = {0,0,0,0};
  a[0] = b;
  a[1] = b;
  a[2] = b;
  a[3] = b;
  return a;
}


/* { dg-final { scan-tree-dump-times "BIT_INSERT_EXPR" 0 "optimized" } } */

----- CUT ---
In the end, we should generate a CONSTRUCTOR with just b's.


---


### compiler : `gcc`
### title : `misoptimization: minor changes of the code leads change up to +/- 30% performance on x86_64, -Os faster than -Ofast/O2/O3`
### open_at : `2020-01-12T23:47:38Z`
### last_modified_date : `2020-01-15T13:45:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93243
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
Briefly:

./heapsort-bench, cc 9.2.1 20191008
pass 1, small:
  1.138047 seconds, baseline
  1.090476 seconds, case-1, 95.8% of baseline
  0.957207 seconds, case-2, 84.1% of baseline
  1.163323 seconds, case-1+2, 102.2% of baseline
pass 1, large:
  2.766881 seconds, baseline
  2.677642 seconds, case-1, 96.8% of baseline
  3.230149 seconds, case-2, 116.7% of baseline
  2.758408 seconds, case-1+2, 99.7% of baseline

./heapsort-bench, cc Clang 9.0.0 (tags/RELEASE_900/final)
pass 1, small:
  1.048489 seconds, baseline
  1.050220 seconds, case-1, 100.2% of baseline
  1.056953 seconds, case-2, 100.8% of baseline
  1.050501 seconds, case-1+2, 100.2% of baseline
pass 1, large:
  2.588565 seconds, baseline
  2.585488 seconds, case-1, 99.9% of baseline
  2.610508 seconds, case-2, 100.8% of baseline
  2.587282 seconds, case-1+2, 100.0% of baseline

./heapsort-bench, gcc 7.4.0 (ubuntu)
pass 1, small:
  0.893917 seconds, baseline
  1.135796 seconds, case-1, 127.1% of baseline
  0.920338 seconds, case-2, 103.0% of baseline
  1.140505 seconds, case-1+2, 127.6% of baseline
pass 1, large:
  3.804271 seconds, baseline
  2.955773 seconds, case-1, 77.7% of baseline
  3.908621 seconds, case-2, 102.7% of baseline
  2.925845 seconds, case-1+2, 76.9% of baseline

The diffs in the source code are:
#if CASE & 1
#define CMP(a, b) ((a) < (b))
#else
#define CMP(a, b) (((a) - (b)) < 0)
#endiF

#if CASE & 2
  for (size_t root = from; (root + root) <= to;) {
    size_t child = root << 1;
#else
  for (size_t child, root = from; (child = root + root) <= to;) {
#endif

gcc 9.x and clang 9.x shows (nearly) the same results on Fedora 31 and Ubunto 19.10.
gcc 7.4 probed only on ubuntu, moreover clang 6.0 shown stable results like clang 9.

Source code of testcase at https://github.com/leo-yuriev/gcc-issues
$ wc heapsort.c
 165  528 4309 heapsort.c

Using PGO (included in the testcase) does not significantly change the result.

Basically these words is seems enough, but more ones I will add tomorrow (likely after afternoon UTC+03).

Regards, 
Leonid.


---


### compiler : `gcc`
### title : `[10 regression] Missed constant folding from constructor`
### open_at : `2020-01-14T09:21:13Z`
### last_modified_date : `2020-01-17T12:46:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93258
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
In the testcase (reduced from folly by Mark Williams):

typedef int a;
template <typename b, b c> struct d { static constexpr b e = c; };
template <bool c> using f = d<bool, c>;
template <typename...> struct g;
template <typename h, typename i> struct g<h, i> : h {};
template <typename> struct j : f<!bool()> {};
template <bool> struct an;
class m {
  struct n {};

public:
  using bm = an<g<j<int>, int>::e>;
  using bn = n;
  template <typename bo> m(bn, bo);
};
class o {
  m bq;
  using bn = m::bn;
  bn k;
  o(o &&) : bq(k, 0) {}
};
namespace bw {
template <class bx> class by {
public:
  constexpr by(bx ca, a cb) : cc(ca), cd(ca + cb) {}
  long cb() { return cd - cc; }
  bx cc, cd;
};
typedef by<const char *> ch;
class ci {
public:
  static o cj(void *, long);
  static o cj(by<const char *> p) {
    long l = p.cb();
    cj(0, l);
  }
};
}
constexpr bw::ch ck{"", 6};
auto fn1() { bw::ci::cj(ck); }

compiled with -O3 both GCC 9 and 10 arrive to the following code before fre3:

;; Function fn1 (_Z3fn1v, funcdef_no=9, decl_uid=2560, cgraph_uid=8, symbol_order=8) (executed once)
                                                                                
fn1 ()                                                                          
{                                                                               
  struct o D.2587;                                                              
  struct by p;                                                                  
  const char * _3;                                                              
  const char * _4;                                                              
  long int _5;                                                                  
                                                                                
  <bb 2> [local count: 1073741824]:                                             
  p = ck;                                                                       
  _3 = MEM[(const char * *)&p];                                                 
  _4 = MEM[(const char * *)&p + 8B];                                            
  _5 = _4 - _3;                                                                 
  D.2587 = bw::ci::cj (0B, _5); [return slot optimization]                      
  D.2587 ={v} {CLOBBER};                                                        
  __builtin_unreachable ();                                                     
                                                                                
}                                                                               

Now GCC9 optimizes in fre3 to:
fn1 ()
{
  struct o D.2585;
  struct by p;

  <bb 2> [local count: 1073741824]:
  p = ck;
  D.2585 = bw::ci::cj (0B, 6); [return slot optimization]
  D.2585 ={v} {CLOBBER};
  __builtin_unreachable ();

}


and GCC10:
fn1 ()
{
  struct o D.2587;
  struct by p;
  const char * _4;
  long int _5;

  <bb 2> [local count: 1073741824]:
  p = ck;
  _4 = MEM[(const char * *)&p + 8B];
  _5 = _4 - "";
  D.2587 = bw::ci::cj (0B, _5); [return slot optimization]
  D.2587 ={v} {CLOBBER};
  __builtin_unreachable ();

}

This seems to be caused by fact that the constructor gets is not formed as valid min invariant:
(gdb) p debug_generic_stmt (res)
(const char *) "" + 6;
$19 = void
(gdb) n
1487                      if (is_gimple_min_invariant (res))
(gdb)
1501      return NULL_TREE;

Is this missed gimplification? GCC9 gets it as &MEM[(void *)"" + 6B]

Mark bisected it to the following:
                                                                                                                                                                               
commit 7a429a9d52a77e4db2eac6d23d8edb69d26d4f9b (HEAD, refs/bisect/good-7a429a9d52a77e4db2eac6d23d8edb69d26d4f9b)                                                              
Author:     jason <jason@138bc75d-0d04-0410-961f-82ee72b054a4>                                                                                                                 
AuthorDate: Mon Jun 10 19:31:49 2019 +0000                                                                                                                                     
Commit:     jason <jason@138bc75d-0d04-0410-961f-82ee72b054a4>                                                                                                                 
CommitDate: Mon Jun 10 19:31:49 2019 +0000                                                                                                                                     
                                                                                                                                                                               
            Reduce constexpr_call memory consumption.                                                                                                                          
                                                                                                                                                                               
            * constexpr.c (cxx_bind_parameters_in_call): Use TREE_VEC rather                                                                                                   
            than TREE_LIST.                                                                                                                                                    
            (constexpr_call_hasher::equal, cxx_bind_parameters_in_call)                                                                                                        
            (cxx_eval_call_expression): Adjust.                                                                                                                                
                                                                                                                                                                               
    git-svn-id: svn+ssh://gcc.gnu.org/svn/gcc/trunk@272125 138bc75d-0d04-0410-961f-82ee72b054a4


---


### compiler : `gcc`
### title : `fold strstr(a, b) to zero when b is longer than a`
### open_at : `2020-01-14T11:57:55Z`
### last_modified_date : `2020-01-20T13:45:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93261
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Similarly to pr90879, pr90626, pr90876 and pr83026, the expression strstr(a, b) can be folded to null when the string b is longer than the string a, or the object in which the string a is stored.  The latter should avoid problems like some of those discussed in pr92765.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
void f (const char *s)
{
  if (__builtin_strlen (s) > 2)
    return;

  if (__builtin_strstr (s, "bar"))   // can be folded to false
    __builtin_abort ();
}

void g (void)
{
  extern char a[6];   // strlen (a) must be less than 6

  if (__builtin_strstr (a, "foobar"))   // can be folded to false
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1930, cgraph_uid=1, symbol_order=0)

Removing basic block 6
Removing basic block 7
f (const char * s)
{
  long unsigned int _1;
  char * _2;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strlen (s_4(D));
  if (_1 > 2)
    goto <bb 5>; [34.00%]
  else
    goto <bb 3>; [66.00%]

  <bb 3> [local count: 708669605]:
  _2 = __builtin_strstr (s_4(D), "bar");
  if (_2 != 0B)
    goto <bb 4>; [0.00%]
  else
    goto <bb 5>; [100.00%]

  <bb 4> [count: 0]:
  __builtin_abort ();

  <bb 5> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1933, cgraph_uid=2, symbol_order=1)

g ()
{
  char * _1;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strstr (&a, "foobar");
  if (_1 != 0B)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `memcmp comparisons of structs wrapping a primitive type not as compact/efficient as direct comparisons of the underlying primitive type under -Os`
### open_at : `2020-01-14T14:10:43Z`
### last_modified_date : `2023-05-01T08:22:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93265
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
`memcmp` comparisons of types such as `struct Int { int x; };` generate full `memcmp` calls under `-Os` (also `-O1`)
These are much larger (/less efficient) than the direct primitive-type comparisons that could have been used.

Example code:

#include <string.h>
//a contiguous struct wrapping a primitive type
typedef struct a_tp { int x; }a_tp; 
_Static_assert(sizeof(a_tp)==sizeof(int),"");

//compare a contiguous lvalue
#define CONTIG_EQ_EH(Ap,Bp) (!memcmp(Ap,Bp,sizeof(*(1?(Ap):(Bp))))) 

/////////////////////////////////////////
//>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
//A FULL MEMCPY :( under -Os (and -O1)
_Bool a_is42(a_tp X) {return CONTIG_EQ_EH(&X,&(a_tp const){42});}
//>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
/////////////////////////////////////////

_Bool i_is42(int X) {return X==42; } //direct cmp
_Bool i2_is42(a_tp X) {return X.x==42; } //same
_Bool i3_is42(a_tp X) {return CONTIG_EQ_EH(&X,&(int const){42});} //still a direct cmp

https://gcc.godbolt.org/z/BC_QsN


---


### compiler : `gcc`
### title : `strlen pass could optimize strncpy with known strlen (src) == 0 into memset`
### open_at : `2020-01-14T14:41:25Z`
### last_modified_date : `2020-01-14T17:52:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93266
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
In:

char b[32];

void
foo ()
{
  char a[16];
  __builtin_memcpy (a, "\0foobarbazquuxy", 16);
  __builtin_strncpy (b, a, 32);
}

we could turn the strncpy into memset (b, ' ', 32);


---


### compiler : `gcc`
### title : `switch optimisation of multiple jumptables into a lookup`
### open_at : `2020-01-20T07:16:10Z`
### last_modified_date : `2021-09-05T00:29:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93326
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `enhancement`
### contents :
Attached two code samples are equivalent, yet they are optimised wildly differently (with -O3 -fno-pic).

Apparently, GCC is capable of recognising that a switch/return realised by a single jumptable can be optimised into a lookup table, but is unable to do so when a switch is realised by multiple jumptables.

When compiling in PIC mode, neither switch is optimised into a lookup table; I don't see a reason why it couldn't be.


---


### compiler : `gcc`
### title : `missed optimization opportunity in deserialization code`
### open_at : `2020-01-20T10:53:20Z`
### last_modified_date : `2023-10-12T06:28:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93328
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Deserialization code often deals with endianness and alignment. However, in some cases, the protocol endianness is the same as the host endianness, and your platform does not care about alignment or has an unaligned load instruction. Take this code, for example:

unsigned int foo(const unsigned char* c) {
  return c[0] + c[1]*0x100 + c[2]*0x10000 + c[3]*0x1000000;
}

On i386 or x86_64, this could just be compiled into a single load.
In fact, clang does compile this into a single load.
gcc however turns it into four loads and three shifts.

For some use cases this optimization could be a huge improvement. In fact, even if the alignment does not match, this could be a huge improvement. The compiler could turn it into an load + bswap.
In fact, clang does compile the big endian version into a load + bswap.


---


### compiler : `gcc`
### title : `-O3 generates useless code checking for overlapping memset ?`
### open_at : `2020-01-20T15:23:59Z`
### last_modified_date : `2021-12-20T04:48:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93334
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
It seems that trying to zero out two arrays in the same loop results in poor code beeing generated by -O3.
If I understand it correctly, the generated code tries to identify if the arrays overlap. If it is the case the code then falls back to simple loops instead of calls to memset.

I wonder why overlapping memset is an issue?
I this some inherited behaviour from dealing with memcpy?

In case 4 arrays are zeroed together, about 40 instructions are generated to check for mutual overlap... This does not seem to be necessary.
Other compilers (clang, icc) don't do that.

See issue here, with assembly generated:
https://godbolt.org/z/SSWVhm

And I copy the code below for reference too:

void test_simple_code(long l, double* mem, long ofs2) {
        for (long k=0; k<l; k++) {
			mem[k] = 0.0;
			mem[ofs2 +k] = 0.0;
        }
}

void test_crazy_code(long l, double* mem, long ofs2, long ofs3, long ofs4) {
        for (long k=0; k<l; k++) {
			mem[k] = 0.0;
			mem[ofs2 +k] = 0.0;
			mem[ofs3 +k] = 0.0;
			mem[ofs4 +k] = 0.0;
        }
}

void test_ok_code(long l, double* mem, long ofs2, long ofs3, long ofs4) {
        for (long k=0; k<l; k++)
			mem[k] = 0.0;
        for (long k=0; k<l; k++)
			mem[ofs2 +k] = 0.0;
        for (long k=0; k<l; k++)
			mem[ofs3 +k] = 0.0;
        for (long k=0; k<l; k++)
			mem[ofs4 +k] = 0.0;
}


---


### compiler : `gcc`
### title : `interchange does not work when using the address rather than direct array`
### open_at : `2020-01-20T23:27:44Z`
### last_modified_date : `2020-01-21T08:48:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93344
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:
#define N 100
#define M 1111
struct S { int a ; int b ; int c ; };
struct S A[N][M];

int __attribute__((noinline))
foo (void)
{
  int i, j;

  for( i = 0; i < M; i++)
    for( j = 0; j < N; j++)
     {
      struct S *a = A[j];
      a[i].b = 5 * a[i].b;
     }

  return A[0][0].b + A[N-1][M-1].b;
}

int __attribute__((noinline))
foo1 (void)
{
  int i, j;

  for( i = 0; i < M; i++)
    for( j = 0; j < N; j++)
     {
      A[j][i].b = 5 * A[j][i].b;
     }

  return A[0][0].b + A[N-1][M-1].b;
}

---- CUT ----
foo does not work but foo1 does work though.


---


### compiler : `gcc`
### title : `gcc does not generate BZHI`
### open_at : `2020-01-21T04:27:40Z`
### last_modified_date : `2021-09-04T22:01:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93346
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `[10/11/12 Regression] 447.dealII regresses by 15% after r10-6025-gf5b25e15165adde1356af42e9066ab75c5b37a19`
### open_at : `2020-01-21T15:07:43Z`
### last_modified_date : `2021-11-21T15:21:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93358
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `lto`
### version : `10.0`
### severity : `normal`
### contents :
Fails with -march=znver2 on znver1 or znver2 machine:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=289.140.0

Options:
-Ofast -march=znver2 -g -flto=16


---


### compiler : `gcc`
### title : `cris performance regressions due to de-cc0 work`
### open_at : `2020-01-22T04:13:38Z`
### last_modified_date : `2021-04-27T14:54:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93372
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
This is a placeholder tracking all cris-specific performance-regressions related to work of getting rid of "cc0" in the CRIS port.  The initial regressions are just incidental pruning of various instruction-matching patterns due to simplifications of the port, but findings due to generic parts of gcc, is also on topic.


---


### compiler : `gcc`
### title : `AARCH64: FP move costs needs improvements for ThunderX2`
### open_at : `2020-01-22T16:21:22Z`
### last_modified_date : `2021-01-13T06:15:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93390
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Current cpu_regmove_cost for thunderx2t99 seems to be not optimal. Preliminary experiments and benchmarks (SPEC) shows that a little bit lower values for FP-related entries get better results. I'd like to proceed on this one.


---


### compiler : `gcc`
### title : `AVX2 missed optimization : _mm256_permute_pd() is unfortunately translated into the more expensive VPERMPD instead of the cheap VPERMILPD`
### open_at : `2020-01-22T20:58:25Z`
### last_modified_date : `2020-01-24T21:53:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93395
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
According to Agner Fog's instruction timing tables
and to my own measurements, VPERMPD has a 3 cycle latency, while VPERMILPD has a 1 cycle latency on most CPUs.

Yet, the intrinsic _mm256_permute_pd() is always translated with VPERMPD, while this intrinsic maps directly to the VPERMILPD instruction.
This makes the code SLOWER.

It should be the opposite: the _mm256_permute4x64_pd() intrinsic, which maps the VPERMPD instruction should, when possible, be translated into VPERMILPD.

Note that clang does the right thing here.

The same problem arises for AVX-512.

See assembly generated here: https://godbolt.org/z/VZe8qk

I replicate the code here for completeness:

#include <immintrin.h>


// translated into   "vpermpd ymm0, ymm0, 177"
// which is OK, but  "vpermilpd ymm0, ymm0, 5"   does the same thing faster.
__m256d perm_missed_optimization(__m256d a) {
    return _mm256_permute4x64_pd(a,0xB1);
}

// translated into   "vpermpd ymm0, ymm0, 177"
// which is 3 times slower than the original intent of   "vpermilpd ymm0, ymm0, 5"
__m256d perm_pessimization(__m256d a) {
    return _mm256_permute_pd(a,0x5);
}

// adequately translated into  "vshufpd ymm0, ymm0, ymm0, 5"
// which does the same as   "vpermilpd ymm0, ymm0, 5"   at the same speed.
__m256d perm_workaround(__m256d a) {
    return _mm256_shuffle_pd(a, a, 5);
}


---


### compiler : `gcc`
### title : `[RX] tail call optimization does not work with indirect call`
### open_at : `2020-01-23T04:42:41Z`
### last_modified_date : `2021-09-05T05:05:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93396
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `enhancement`
### contents :
Compiling the following on RX with -O2

[[gnu::noinline]]
int test_1 (int x)
{
  return x + 1;
}

[[gnu::noinline]]
int test_2 (int x)
{
  return test_1 (x);
}

[[gnu::noinline]]
int test_3 (int (*func)(int), int x)
{
  return func (x);
}

results in:

	.global	__Z6test_1i
	.type	__Z6test_1i, @function
__Z6test_1i:
	add	#1, r1
	rts
	.size	__Z6test_1i, .-__Z6test_1i
	.global	__Z6test_2i
	.type	__Z6test_2i, @function
__Z6test_2i:
	bra	__Z6test_1i
	.size	__Z6test_2i, .-__Z6test_2i
	.global	__Z6test_3PFiiEi
	.type	__Z6test_3PFiiEi, @function
__Z6test_3PFiiEi:
	mov.L	r1, r14
	mov.L	r2, r1
	jsr	r14
	rts
	.size	__Z6test_3PFiiEi, .-__Z6test_3PFiiEi

It seems something is missing for the tail call optimization to work.


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] -O2 and -O2 -flto SPEC2006 and 2017 code size regression`
### open_at : `2020-01-23T15:37:22Z`
### last_modified_date : `2023-07-07T10:36:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93404
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
I am looking into this for a while, but lets create a tracking bug to record some info on the problem.

-O2 code size seems to keep getting bigger which is not a good thing. 
From https://lnt.opensuse.org/db_default/v4/SPEC/spec_report/branch?sorting=&all_elf_detail_stats=on
We get following comparing text segment sizes to gcc 6

                       GCC 7  GCC 8  GCC 9  GCC 10
SPECint2006 -O2:       ~      ~      ~      ~
SPECfp2006 -O2:        ~      ~      ~      6.94%
SPECint2006 -O2 -flto: ~      ~      ~      11.72%
SPECfp2006  -O2 -flto: ~      ~      ~      8.84%
SPECint2017 -O2:       ~      ~      2.58%  10.33%
SPECfp2018 -O2:        ~      ~      3.56%  14.47%
SPECint2017 -O2:       ~      ~      ~      9.77%
SPECfp2018 -O2:        ~      2.42%  ~      18.97%

Of course, -O3 also gets progressively bigger. LNT uses ~ for values which it considers noise (that is bit funny for sizes that are noise free), but clearly GCC 10 regressed more than previous releases.

In GCC 10 timeframe a large portion of -flto -O2 growth is enabling auto-inlining, but it is not the only issue here, because w/o LTO the change actually improved code size.

In December I compared current -O2 inlining to GCC 9 style inlining and also to auto inlining with -O3 params:

SPEC text segment effect
Benchmark                       -O2     -O2 with -O3 style inlining
SPECint2006 -O2 generic         0.32%   12.08%
SPECfp2006  -O2 generic         -0.16%  2.99%
SPECint2006 -O2 generic LTO     11.79%  29.33%
SPECfp2006  -O2 generic LTO     1.72%   6.24%   
SPECint2017 -O2 generic         -1.49%  11.17%
SPECfp2017  -O2 generic         -0.33%  3.76%   
SPECint2017 -O2 generic LTO     10.99%  20.63%
SPECfp2017  -O2 generic LTO     4.07%   11.09%

First column compares GCC 9 -O2 inliner setting to one after patch and second to -O3 settings.  Things has improved a bit since December, so i will try to get fresh numbers.


---


### compiler : `gcc`
### title : `Dead partial memset not optimized away (clang does that)`
### open_at : `2020-01-23T17:20:18Z`
### last_modified_date : `2022-01-31T08:03:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93407
### status : `WAITING`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
While analyzing GCC for PR93404 I noticed that gen_rtx_fmt_ee
     34 rtx
     35 gen_rtx_fmt_ee (code, mode, arg0, arg1)
     36      RTX_CODE code;
     37      enum machine_mode mode;
     38      rtx arg0;
     39      rtx arg1;
     40 {
     41   rtx rt;
     42   rt = ggc_alloc_rtx (2);
     43   memset (rt, 0, sizeof (struct rtx_def) - sizeof (rtunion));
     44 
     45   PUT_CODE (rt, code);
     46   PUT_MODE (rt, mode);
     47   XEXP (rt, 0) = arg0;
     48   XEXP (rt, 1) = arg1;
     49 
     50   return rt;
     51 }
gets quite odd codegen which gets inlined many times:

gen_rtx_fmt_ee():
/aux/hubicka/403.gcc/src/genrtl.c:41
  4036ae:       bf 18 00 00 00          mov    $0x18,%edi
init_reload():
/aux/hubicka/403.gcc/src/reload1.c:500
  4036b3:       41 0f 95 c6             setne  %r14b
  4036b7:       49 89 c5                mov    %rax,%r13
gen_rtx_fmt_ee():
/aux/hubicka/403.gcc/src/genrtl.c:41
  4036ba:       e8 d1 64 0d 00          callq  4d9b90 <ggc_alloc>
init_reload():
/aux/hubicka/403.gcc/src/reload1.c:500
  4036bf:       41 83 c6 04             add    $0x4,%r14d
plus_constant_wide():
/aux/hubicka/403.gcc/src/genrtl.c:41
  4036c3:       be 04 00 00 00          mov    $0x4,%esi
gen_rtx_fmt_ee():
/aux/hubicka/403.gcc/src/genrtl.c:42
  4036c8:       c7 40 03 00 00 00 00    movl   $0x0,0x3(%rax)
/aux/hubicka/403.gcc/src/genrtl.c:41
  4036cf:       48 89 c7                mov    %rax,%rdi
/aux/hubicka/403.gcc/src/genrtl.c:42
  4036d2:       c6 40 07 00             movb   $0x0,0x7(%rax)
/aux/hubicka/403.gcc/src/genrtl.c:44
  4036d6:       b8 4b 00 00 00          mov    $0x4b,%eax
  4036db:       66 89 07                mov    %ax,(%rdi)
/aux/hubicka/403.gcc/src/genrtl.c:45
  4036de:       44 88 77 02             mov    %r14b,0x2(%rdi)
/aux/hubicka/403.gcc/src/genrtl.c:46
  4036e2:       4c 89 6f 08             mov    %r13,0x8(%rdi)
/aux/hubicka/403.gcc/src/genrtl.c:47
  4036e6:       4c 89 67 10             mov    %r12,0x10(%rdi)

For exmaple
  4036c8:       c7 40 03 00 00 00 00    movl   $0x0,0x3(%rax)
seems to be leftover of the memset code. Clang does:
gen_rtx_fmt_ee():
/aux/hubicka/403.gcc/src/genrtl.c:43
  67c72f:       bf 18 00 00 00          mov    $0x18,%edi
  67c734:       e8 a7 1a eb ff          callq  52e1e0 <ggc_alloc>
  67c739:       49 89 c6                mov    %rax,%r14
/aux/hubicka/403.gcc/src/genrtl.c:44
  67c73c:       48 c7 00 00 00 00 00    movq   $0x0,(%rax)
/aux/hubicka/403.gcc/src/genrtl.c:46
  67c743:       8d 85 4b 00 04 00       lea    0x4004b(%rbp),%eax
/aux/hubicka/403.gcc/src/genrtl.c:47
  67c749:       41 89 06                mov    %eax,(%r14)
/aux/hubicka/403.gcc/src/genrtl.c:48
  67c74c:       49 89 5e 08             mov    %rbx,0x8(%r14)
/aux/hubicka/403.gcc/src/genrtl.c:49
  67c750:       4d 89 7e 10             mov    %r15,0x10(%r14)

which simply fills in the rtx header and the two XEXP pointers.


---


### compiler : `gcc`
### title : `variable reference to constant array with same elements not folded`
### open_at : `2020-01-24T11:19:33Z`
### last_modified_date : `2020-01-28T16:54:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93411
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
This was prompted by pr86912 (not sure if the problem there would be solved by solving this one).  The following test case shows that while constant references to constant arrays are folded (both in-bounds and out-of-bounds), non-constant references are not even though they could be.

$ cat u.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout u.c
const int a[] = { 0, 0, 0 };

int idx_cst_in_bounds (void)
{
  return a[1];   // valid, folded to 0 (good)
}

int idx_cst_out_of_bounds (void)
{
  return a[-123];   // invalid, folded to 0 without a warning (see pr86691)
}

int idx_rng_in_bounds (int i)
{
  if (i < 0 || 3 < i) i = 0;
  return a[i];   // valid, not folded but could/should be
}

int idx_unknown (int i)
{
  return a[i];   // valid, not folded but could/should be
}

int idx_rng_out_of_bounds (int i)
{
  if (i >= 0) i = -1;
  return a[i];   // invalid, not folded with warning; should it be?
}

;; Function idx_cst_in_bounds (idx_cst_in_bounds, funcdef_no=0, decl_uid=1931, cgraph_uid=1, symbol_order=1)

idx_cst_in_bounds ()
{
  <bb 2> [local count: 1073741824]:
  return 0;

}



;; Function idx_cst_out_of_bounds (idx_cst_out_of_bounds, funcdef_no=6, decl_uid=1934, cgraph_uid=2, symbol_order=2)

idx_cst_out_of_bounds ()
{
  <bb 2> [local count: 1073741824]:
  return 0;

}



;; Function idx_rng_in_bounds (idx_rng_in_bounds, funcdef_no=2, decl_uid=1937, cgraph_uid=3, symbol_order=3)

Removing basic block 5
idx_rng_in_bounds (int i)
{
  unsigned int i.0_1;
  int pretmp_6;
  int prephitmp_7;

  <bb 2> [local count: 1073741824]:
  i.0_1 = (unsigned int) i_3(D);
  if (i.0_1 > 3)
    goto <bb 4>; [50.00%]
  else
    goto <bb 3>; [50.00%]

  <bb 3> [local count: 536870912]:
  pretmp_6 = a[i_3(D)];

  <bb 4> [local count: 1073741824]:
  # prephitmp_7 = PHI <pretmp_6(3), 0(2)>
  return prephitmp_7;

}



;; Function idx_unknown (idx_unknown, funcdef_no=3, decl_uid=1940, cgraph_uid=4, symbol_order=4)

idx_unknown (int i)
{
  int _3;

  <bb 2> [local count: 1073741824]:
  _3 = a[i_2(D)];
  return _3;

}


u.c: In function ‘idx_rng_out_of_bounds’:
u.c:27:11: warning: array subscript -1 is below array bounds of ‘const int[3]’ [-Warray-bounds]
   27 |   return a[i];   // not folded with warning; should it be?
      |          ~^~~
u.c:1:11: note: while referencing ‘a’
    1 | const int a[] = { 0, 0, 0 };
      |           ^

;; Function idx_rng_out_of_bounds (idx_rng_out_of_bounds, funcdef_no=4, decl_uid=1943, cgraph_uid=5, symbol_order=5)

idx_rng_out_of_bounds (int i)
{
  int _4;

  <bb 2> [local count: 1073741824]:
  i_2 = MIN_EXPR <i_1(D), -1>;
  _4 = a[i_2];
  return _4;

}


---


### compiler : `gcc`
### title : `scalar unrolled loop makes vectorized code unreachable`
### open_at : `2020-01-26T12:26:03Z`
### last_modified_date : `2020-01-31T17:00:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93440
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 47711
generated assemby code, showing unreachable SIMD vector code for transform_reduce

With gcc version 9.2.1 20190827 (Red Hat 9.2.1-1) (GCC) on x86-64, "-O3 -std=gnu++17 -march=core-avx2" emits both SIMD vectorized code and an unrolled loop for "std::transform_reduce". The unrolled loop prevents the SIMD vectorized code from executing for reasonable vector sizes. In contrast, "std::inner_product" produces reachable vectorized code.

Minimal reproducing c++ code:

    #include <vector>
    #include <algorithm>
    #include <numeric>
    
    auto workingvector(std::vector<int> const& a, std::vector<int> const& b) 
    {
      return std::inner_product(cbegin(a), cend(a), cbegin(b), 0, std::plus<>{}, std::multiplies<>{});
    }

    auto brokenvector(std::vector<int> const& a, std::vector<int> const& b) 
    {
      return std::transform_reduce(cbegin(a), cend(a), cbegin(b), 0, std::plus<>{},std::multiplies<>{});
    }



Details:
The generated assembly for the "transform_reduce" checks for short vectors with a signed comparison, so the vectorized code is *technically* reachable (for completely infeasible vector sizes on a 64-bit address-space). If the vector size is large enough for the unrolled scalar loop, the scalar loop processes the entire vector, never allowing the SIMD vector code to execute.


---


### compiler : `gcc`
### title : `Value range propagation not working at -Os`
### open_at : `2020-01-26T22:34:30Z`
### last_modified_date : `2023-05-06T21:49:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93447
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
I have a lot of cases where I'd like to translate boolean conditions to negated errno codes (possibly wrapped in an struct or a trivial union).

If this translation is inlinable, I'd expect that undoing it to get a boolean again (bool => negated errno => bool) would eliminate the roundtrip.

GCC indeed does this at -O2 and -O3, but the optimization's failing to kick in at -Os, leading to code size increases.

Clang succeeds at eliminating the roundrip at -Os (and it does this optimization already at -O1).

A simple example that generates unnecessary code at -Os:

#include <errno.h>

_Bool addb_simple(int A, int B, int *Rp)
{
    int ec=0;
    if(__builtin_add_overflow(A,B,Rp)) 
        ec = -ERANGE;
    return !!ec;
}

https://gcc.godbolt.org/z/tGkbtD


Thanks for looking into it!


---


### compiler : `gcc`
### title : `PPC: rldimi not taken into account to avoid shift+or`
### open_at : `2020-01-27T12:55:41Z`
### last_modified_date : `2022-06-09T05:36:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93453
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
2 samples:

unsigned long long load8r(unsigned long long *in)
{
   return __builtin_bswap64(*in);
}

unsigned long long rldimi(unsigned int hi, unsigned int lo)
{
   return (((unsigned long long)hi) << 32) | ((unsigned long long)lo);
}

Command line:
gcc -maix64 -mcpu=power6 -save-temps -O2 rldimi.C 

Even if number range is known to not cause conflicts shift+or does not get replaces by rldimi.

Output:
._Z6load8rPy:
LFB..0:
        addi 9,3,4
        lwbrx 3,0,3
        lwbrx 10,0,9
        sldi 10,10,32
        or 3,3,10
        blr

._Z6rldimijj:
LFB..1:
        sldi 3,3,32
        or 3,3,4
        blr


---


### compiler : `gcc`
### title : `aarch64: Q constraint address is recomputed`
### open_at : `2020-01-27T14:56:35Z`
### last_modified_date : `2021-10-08T17:03:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93455
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
gcc may recompute the address used in a Q constraint
(which may be used for atomic load and stores).

static volatile int x[1];
int f()
{
	int r;
	asm volatile ("A %w0 %1" : "=r"(r) : "Q"(*x));
	asm volatile ("B %0" : "=Q"(*x));
	return r;
}

with -O3 gcc generates

f:
        adrp    x1, .LANCHOR0
        add     x0, x1, :lo12:.LANCHOR0
        A w0 [x0]
        add     x1, x1, :lo12:.LANCHOR0
        B [x1]
        ret

i expected one address computation.


---


### compiler : `gcc`
### title : `ix86_fold_builtin should handle __builtin_ia32_pcmpeqd128_mask and similar builtins`
### open_at : `2020-01-27T16:50:54Z`
### last_modified_date : `2021-12-20T05:34:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93459
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
Just noticed we don't fold e.g.
#include <x86intrin.h>

void link_error (void);

void
foo (void)
{
  __m128i a = _mm_setr_epi32 (0xffff0000U, 0x80000000U, 0, 0xfffffff8U);
  if (_mm_cmpeq_epi32_mask (a, a) != 15)
    link_error ();
}

with -O2 -mavx512vl, neither at GIMPLE time (ix86_fold_builtin), nor at RTL opt time.


---


### compiler : `gcc`
### title : `Missed tail-call optimizations`
### open_at : `2020-01-29T10:10:52Z`
### last_modified_date : `2022-11-01T19:50:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93487
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Given, for example:

	#include <stdint.h>
	typedef union lp_tp {
		long l;
		void *p;
	} lp_tp ;
	typedef union ilp_tp{
		int i;
		long l;
		void *p;
		lp_tp lp;
	} ilp_tp;

	long lcallee(void);
	int icallee(void);
	void *pcallee(void);
	lp_tp lpcallee(void);

	//tail calls on clang but not on gcc
	int l2i(void){ return lcallee(); }
	ilp_tp l_caller(void) { long rc = lcallee(); return (ilp_tp){.l=rc}; }
	ilp_tp p_caller(void) { void *rc= pcallee(); return (ilp_tp){.p=rc}; }
	ilp_tp lp_caller(void) { lp_tp rc = lpcallee(); return (ilp_tp){.lp = rc}; }
	ilp_tp lp_caller2(void) { lp_tp rc = lpcallee(); return (ilp_tp){.p = rc.p}; }

	struct foo* p2p_caller(void) { return pcallee(); } //optimized on both
	uintptr_t p2up_caller(void) { return (uintptr_t)pcallee(); } //optimized on both

	//not optimized by either
	ilp_tp i_caller(void) { int rc = icallee(); ilp_tp r; r.i=rc; return r; }

clang (x86_64) is able to turn all of these calls except the last one into tail calls but gcc tailcall-optimizes only the pointer-to-pointer conversions.

https://gcc.godbolt.org/z/Lw9-D2


---


### compiler : `gcc`
### title : `Missed reassociation with constants and not of that constant with IORs`
### open_at : `2020-01-30T00:28:49Z`
### last_modified_date : `2021-12-25T23:03:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93504
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:
#define N 0x202
#define OP |

unsigned f(unsigned a, unsigned b)
{
  unsigned t = a OP b;
  unsigned t1 = t&N;
  unsigned t2 = a&~N;
  return t1 | t2;
}


unsigned f1(unsigned a, unsigned b)
{
  return b&N OP a;
}
---- CUT ---
Both of these functions are the same.  On x86_64, we get the same assembly code in the end; one most RISC targets we don't because the constants force out to be too many instructions for combine to handle.

If N was a non-constant, we would get the same code at the tree level even.

I found this while working on bit-field lowering, when I was making sure gcc.dg/store_merging_14.c (f7) is optimized to the best it could be.


---


### compiler : `gcc`
### title : `Introduce rotate_truncation_mask`
### open_at : `2020-01-30T18:09:26Z`
### last_modified_date : `2020-02-06T21:25:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93512
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `enhancement`
### contents :
<jakub> i386.md has ;; Avoid useless masking of count operand.
<jakub> (define_insn_and_split "*<rotate_insn><mode>3_mask"
<jakub> and (define_insn_and_split "*<rotate_insn><mode>3_mask_1"
<jakub> for this purpose
<segher> yeah...  but that catches not all cases at all.  but the common ones, sure
<jakub> the other option is to say that in RTL all rotate counts are well defined and so effectively have implicit ROTATE_COUNT_TRUNCATED
<jakub> or introduce rotate_truncation_mask with a default that will truncate and let weirdo targets opt out
<segher> yes, i think that would help all targets
<jakub> in any case, that looks like GCC 11 material to me
<jakub> because one would need to check all code that works with rotates, do the masking, ensure simplify-rtx.c simplifies it etc.
<jakub> would need to be a property of ROTATE/ROTATERT, because e.g. for library implementation if it is done using shifts we don't want to introduce UB there
<jakub> so with the combine.c change you've acked perhaps we could defer the rest for GCC11 (and at that point change even that hunk to do % prec


---


### compiler : `gcc`
### title : `Left shift and arithmetic shift could be futher simplified in simplify-rtx.c`
### open_at : `2020-01-31T14:46:51Z`
### last_modified_date : `2021-09-05T05:37:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93525
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
simplify-rtx currently will fold and combine Arithmetic and Logical right shifts with a constant shift count and fold consecutive shifts into a single one.

If for the logical case if the if the shift count goes out of range it truncates it to 0 but doesn't handle the case for the arithmetic shift.

Also neither operation is performed for left shifts.


---


### compiler : `gcc`
### title : `Object copy not optimized out for most sizes in strict aliasing memcpy pattern`
### open_at : `2020-01-31T19:08:36Z`
### last_modified_date : `2022-10-14T15:47:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93528
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
Consider the following code (a common pattern to avoid strict aliasing violations):

    struct X
    {
        char arr[10];
    };

    void foo(void *p)
    {
        X x;

        memcpy(&x, p, sizeof(x));
        x.arr[0] = 42;
        memcpy(p, &x, sizeof(x));
    }

When the array is 1, 2, 4, 8 or 16 elements long, memcpy() is optimized out with -O2. Output for x86-64 Linux:

    _Z3fooPv:
      movb $42, (%rdi)
      ret

For other struct sizes, copies are performed nonetheless. Depending on the array size either an inline implementation or a call to memcpy() is inserted. For the above case, on x86-64 Linux GCC generates

    _Z3fooPv:
      movq (%rdi), %rax
      movq %rax, -10(%rsp)
      movb $42, -10(%rsp)
      movq -10(%rsp), %rax
      movq %rax, (%rdi)
      ret

It seems that the field types and the layout of the struct don't matter, and the optimization is conditional on object size only. For example,

    struct Y
    {
        int a, b, c;
    };

is affected as well.

As a variant, the problem also appears with a plain array instead of a struct:

    void bar(void *p)
    {
        int arr[3];

        memcpy(arr, p, sizeof(arr));
        arr[0] = 42;
        memcpy(p, arr, sizeof(arr));
    }

gives

    _Z3barPv:
      movq (%rdi), %rax
      movq %rax, -12(%rsp)
      movl $42, -12(%rsp)
      movq -12(%rsp), %rax
      movq %rax, (%rdi)
      ret


---


### compiler : `gcc`
### title : `[9 Regression] equality of address of first member to address to enclosing object not folded`
### open_at : `2020-02-01T23:49:43Z`
### last_modified_date : `2022-05-27T08:41:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93538
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
The inequality below is trivially false but since GCC 4.6 the middle-end fails to fold it into a constant (it is ultimately folded in RTL).  In GCC 4.5 it was folded by the ealias pass.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
struct A { int a[1]; };

void f (struct A *p)
{
  void *q = p->a;
  if (p != q)
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=0)

f (struct A * p)
{
  void * q;

  <bb 2> [local count: 1073741824]:
  q_2 = &p_1(D)->a;
  if (p_1(D) != q_2)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `memmove over self with result of string function not eliminated`
### open_at : `2020-02-02T00:58:55Z`
### last_modified_date : `2021-10-20T07:35:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93539
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
While experimenting with the test case from pr3538 I noticed that GCC doesn't take full advantage of the fact that functions like strcpy return their first argument.  In the test case below only the memmove call in the first function is eliminated.  The one in the second one is not (and never has been).  Clang eliminates both as expected (and always has).

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout -o/dev/stdout a.c
struct A { char a[10]; };

void f (struct A *p)
{
  __builtin_strcpy (p->a, "abcd");
  char *q = p->a;
  __builtin_memmove (q, p->a, sizeof p->a);   // eliminated (good)
}

void g (struct A *p)
{
  char *q = __builtin_strcpy (p->a, "abcd");
  __builtin_memmove (q, p->a, sizeof p->a);   // not eliminated
}

	.file	"a.c"
	.text

;; Function f (f, funcdef_no=0, decl_uid=1932, cgraph_uid=1, symbol_order=0)

f (struct A * p)
{
  char[10] * _1;

  <bb 2> [local count: 1073741824]:
  _1 = &p_2(D)->a;
  __builtin_memcpy (_1, "abcd", 5); [tail call]
  return;

}


	.p2align 4
	.globl	f
	.type	f, @function
f:
.LFB0:
	.cfi_startproc
	movl	$1684234849, (%rdi)
	movb	$0, 4(%rdi)
	ret
	.cfi_endproc
.LFE0:
	.size	f, .-f

;; Function g (g, funcdef_no=1, decl_uid=1936, cgraph_uid=2, symbol_order=1)

g (struct A * p)
{
  char * q;
  char[10] * _1;

  <bb 2> [local count: 1073741824]:
  _1 = &p_2(D)->a;
  q_5 = __builtin_memcpy (_1, "abcd", 5);
  __builtin_memmove (q_5, _1, 10); [tail call]
  return;

}


	.p2align 4
	.globl	g
	.type	g, @function
g:
.LFB1:
	.cfi_startproc
	movl	$1684234849, (%rdi)
	movl	$10, %edx
	movq	%rdi, %rsi
	movb	$0, 4(%rdi)
	jmp	memmove
	.cfi_endproc
.LFE1:
	.size	g, .-g
	.ident	"GCC: (GNU) 10.0.1 20200131 (experimental)"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `Attributes pure and const not working with aggregate return types, even trivial ones`
### open_at : `2020-02-02T06:17:56Z`
### last_modified_date : `2021-12-20T20:28:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93540
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Example:

#define SIMPLE 0
#include <stdlib.h>
#if SIMPLE
    typedef int TYPE;
#else
    typedef struct TYPE { int a; } TYPE;
#endif

//__attribute((pure))
__attribute((const))
TYPE get(void);

void TEST(void)
{
    #if !SIMPLE  // :( generates repeated calls
    if(get().a==0) abort();
    if(get().a==0) abort();
    if(get().a==0) abort();
    if(get().a==0) abort();
    #else  //OK, 1 call
    if(get()==0) abort();
    if(get()==0) abort();
    if(get()==0) abort();
    if(get()==0) abort();
    #endif
}
///////////////////////////////////

https://gcc.godbolt.org/z/N79MCx


---


### compiler : `gcc`
### title : `lower mempcpy to memcpy when result is unused`
### open_at : `2020-02-03T18:49:12Z`
### last_modified_date : `2021-09-05T05:32:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93556
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
GCC lowers calls to strpcpy whose result isn't used to strcpy, presumably because the latter tends to be more efficient.  With pr90263 resolved, GCC also expands some calls to mempcpy to memcpy on most targets, again because the latter is almost always more efficient.  But when the result of the mempcpy call is not used, the call could be transformed to memcpy unconditionally, on all targets, including those like x86 where targetm.libc_has_fast_function (BUILT_IN_MEMPCPY) returns true.

$ cat a.c && gcc -S -Wall -fdump-tree-lower=/dev/stdout a.c
void f0 (char *d, const char *s)
{
  __builtin_stpcpy (d, s);   // lowered to strcpy
}

void f1 (void *d, const void *s, __SIZE_TYPE__ n)
{
  __builtin_mempcpy (d, s, n);   // should be lowered to memcpy
}

;; Function f0 (f0, funcdef_no=0, decl_uid=1931, cgraph_uid=1, symbol_order=0)

f0 (char * d, const char * s)
{
  __builtin_strcpy (d, s);
  return;
}



;; Function f1 (f1, funcdef_no=1, decl_uid=1936, cgraph_uid=2, symbol_order=1)

f1 (void * d, const void * s, long unsigned int n)
{
  __builtin_mempcpy (d, s, n);
  return;
}


---


### compiler : `gcc`
### title : `missing mempcpy folding defeats strlen optimization`
### open_at : `2020-02-03T20:00:15Z`
### last_modified_date : `2020-02-03T20:01:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93558
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
The two functions below create the same string but f1() is micro-optimized to avoid copying the terminating nul in "1234" before immediately appending "5678" (whether or not such micro-optimization ever makes sense is a separate issue).  Yet GCC ultimately optimizes f0() better because in f1() it doesn't exploit the basic property of mempcpy(d, ..., 4): that it returns d + 4.

It seems that if it's profitable to (as far as I can see) unconditionally transform stpcpy(D, S) to strcpy(D, S)/memcpy(D, S, N) + N (when N is the known length of S), it should likewise be profitable to transform mempcpy to memcpy + N.  Either way, GCC should emit equivalently efficient code for both functions below.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
void f0 (char *d, const char *s)
{
  char *t = __builtin_stpcpy (d, "1234");
  __builtin_strcpy (t, "5678");
  if (__builtin_strlen (d) != 8)   // folded to false
    __builtin_abort ();
}

void f1 (char *d, const char *s)
{
  char *t = __builtin_mempcpy (d, "1234", 4);
  __builtin_strcpy (t, "5678");
  if (__builtin_strlen (d) != 8)   // not folded
    __builtin_abort ();
}

;; Function f0 (f0, funcdef_no=0, decl_uid=3479, cgraph_uid=1, symbol_order=0)

f0 (char * d, const char * s)
{
  char * t;

  <bb 2> [local count: 1073741824]:
  __builtin_memcpy (d_3(D), "1234", 4);
  t_5 = d_3(D) + 4;
  __builtin_memcpy (t_5, "5678", 5); [tail call]
  return;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=3484, cgraph_uid=2, symbol_order=1)

f1 (char * d, const char * s)
{
  char * t;
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  t_5 = __builtin_mempcpy (d_3(D), "1234", 4);
  __builtin_memcpy (t_5, "5678", 5);
  _1 = __builtin_strlen (d_3(D));
  if (_1 != 8)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `strstr(s, s) not folded to s`
### open_at : `2020-02-04T02:37:28Z`
### last_modified_date : `2021-09-05T05:09:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93560
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
GCC should fold strstr(a, b) to a when a and b point to the same string, the same way Clang does.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
void f (const char *s)
{
  if (__builtin_strstr (s, s) != s)   // trivially false
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1930, cgraph_uid=1, symbol_order=0)

f (const char * s)
{
  char * _1;

  <bb 2> [local count: 1073741824]:
  _1 = __builtin_strstr (s_3(D), s_3(D));
  if (_1 != s_3(D))
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] Combine duplicates instructions`
### open_at : `2020-02-04T14:07:52Z`
### last_modified_date : `2023-07-07T10:36:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93565
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Created attachment 47777
ctz_duplication

The attached example causes Combine to duplicate count trailing zero instructions on targets which have CTZ_DEFINED_VALUE = 2:

f:
	cbz	x0, .L2
	rbit	x2, x0
	rbit	x0, x0
	clz	x2, x2
	clz	x0, x0
	ldr	w2, [x1, x2, lsl 2]
	orr	w0, w2, w0
	str	w0, [x1]
.L2:
	mov	x0, 0
	ret

The cause is Combine deciding to merge CTZ into a sign-extend:

(insn 10 9 12 3 (set (reg:DI 100)
        (ctz:DI (reg/v:DI 98 [ x ]))) "ctz2.c":17:15 689 {ctzdi2}
     (expr_list:REG_DEAD (reg/v:DI 98 [ x ])
        (nil)))
(insn 12 10 14 3 (set (reg:DI 101 [ _9 ])
        (sign_extend:DI (subreg:SI (reg:DI 100) 0))) "ctz2.c":25:15 104 {*extendsidi2_aarch64}
     (nil))

allowing combination of insns 10 and 12
original costs 4 + 4 = 8
replacement costs 4 + 4 = 8
modifying insn i2    10: r100:DI=ctz(r98:DI)
deferring rescan insn with uid = 10.
modifying insn i3    12: r101:DI=ctz(r98:DI)
      REG_DEAD r98:DI

(insn 10 9 12 3 (set (reg:DI 100)
        (ctz:DI (reg/v:DI 98 [ x ]))) "ctz2.c":17:15 689 {ctzdi2}
     (nil))
(insn 12 10 14 3 (set (reg:DI 101 [ _9 ])
        (ctz:DI (reg/v:DI 98 [ x ]))) "ctz2.c":25:15 689 {ctzdi2}
     (expr_list:REG_DEAD (reg/v:DI 98 [ x ])
        (nil)))

Later passes then seem unable to CSE the two identical CTZ instructions...

This doesn't seem right - if Combine can optimize the sign-extend away then there is no point in duplicating the CTZ.


---


### compiler : `gcc`
### title : `PPC: fmr gets used instead of faster xxlor`
### open_at : `2020-02-04T17:15:21Z`
### last_modified_date : `2022-03-08T16:20:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93571
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
fmr is a 6 cycle instruction on Power8. Why is gcc not using the 2 cycle xxlor instruction )

Input:
double setflm(double x)
{
   double r = __builtin_mffs();
   __builtin_mtfsf(0xFF, x);
    return r;
}

Command line:
gcc -maix64 -O2 -save-temps flm.C -mcpu=power8

Output:
._Z6setflmd:
LFB..0:
        mffs 0
        mtfsf 255,1
        fmr 1,0
        blr


---


### compiler : `gcc`
### title : `[10 Regression] -Warray-bounds gives error: array subscript 0 is outside array bounds of struct E[1]`
### open_at : `2020-02-04T18:24:58Z`
### last_modified_date : `2020-03-05T07:02:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93582
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `normal`
### contents :
Reduced from libXt:

struct E {
} e;

struct A {
  unsigned long *l;
};

struct C {
  unsigned long *l;
} c;

void
bar (struct E *ep)
{
  c.l = ((struct A*) ((ep) + 1))->l;
}

void
f ()
{
  bar (&e);
}

$ ./cc1 -quiet -O3 x.c -Warray-bounds
x.c: In function ‘f’:
x.c:15:33: warning: array subscript 0 is outside array bounds of ‘struct E[1]’ [-Warray-bounds]
   15 |   c.l = ((struct A*) ((ep) + 1))->l;
      |         ~~~~~~~~~~~~~~~~~~~~~~~~^~~
x.c:2:3: note: while referencing ‘e’
    2 | } e;
      |   ^


---


### compiler : `gcc`
### title : `std::string::find_first_not_of is about 9X slower than strspn`
### open_at : `2020-02-05T00:41:42Z`
### last_modified_date : `2020-03-24T17:32:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93584
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.2.1`
### severity : `normal`
### contents :
Created attachment 47779
testcase

In the attached test program the benchmark function takes 0.26837 seconds if calling std::string::find_first_not_of and 0.0291547 seconds if calling strspn.


---


### compiler : `gcc`
### title : `[RX] bclr,bnot,bset on byte memory with bit 7 not fused`
### open_at : `2020-02-05T06:24:38Z`
### last_modified_date : `2020-02-05T06:25:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93587
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
The bit manipulation of bit 7 in a byte variable doesn't get fused into a single instruction.

void bleh_0 (uint8_t* x, uint8_t* y, uint8_t* z)
{
  *x ^= 1 << 7;
  *y |= 1 << 7;
  *z &= ~(1 << 7);
}

compiles to:
	mov.B	[r1], r14
	xor	#-128, r14
	mov.B	r14, [r1]
	mov.B	[r2], r14
	or	#-128, r14
	mov.B	r14, [r2]
	mov.B	[r3], r14
	and	#0x7f, r14
	mov.B	r14, [r3]
	rts


while other bit positions like 

void bleh_1 (uint8_t* x, uint8_t* y, uint8_t* z)
{
  *x ^= 1 << 6;
  *y |= 1 << 6;
  *z &= ~(1 << 6);
}

compile to:
	bnot	#6, [r1].B
	bset	#6, [r2].B
	bclr	#6, [r3].B
	rts


---


### compiler : `gcc`
### title : `Missed optimization with _mm256_set/setr_m128i intrinsics`
### open_at : `2020-02-05T13:04:45Z`
### last_modified_date : `2021-09-13T00:25:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93594
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
When _mm256_set_m128i/_mm256_setr_m128i intrinsics are used to zero the upper half of the resulting register, gcc generates unnecessary vinserti128 instruction, where a single vmovdqa would be enough. The compiler is able to recognize "_mm256_insertf128_si256(_mm256_setzero_si256(), low, 0)" pattern but not "_mm256_insertf128_si256(_mm256_castsi128_si256(low), _mm_setzero_si128(), 1)".

You can see code generated for the different pieces of code here: https://gcc.godbolt.org/z/ZMwtPq

Note that clang is able to recognize all versions and generates optimal code in all cases.

For convenience, here is the test code:

#include <immintrin.h>

__m256i cvt_setr(__m128i low)
{
    return _mm256_setr_m128i(low, _mm_setzero_si128());
}

__m256i cvt_set(__m128i low)
{
    return _mm256_set_m128i(_mm_setzero_si128(), low);
}

__m256i cvt_insert(__m128i low)
{
    return _mm256_insertf128_si256(_mm256_setzero_si256(), low, 0);
}

__m256i cvt_insert_v2(__m128i low)
{
    return _mm256_insertf128_si256(_mm256_castsi128_si256(low), _mm_setzero_si128(), 1);
}

$ g++ -O3 -mavx2


---


### compiler : `gcc`
### title : `GCC suboptimal tail call optimization in trivial function forwarding with __attribute__((noinline))`
### open_at : `2020-02-05T21:08:16Z`
### last_modified_date : `2021-03-04T21:04:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93605
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
In a trivial function-forwarder where `__attribute__((noinline))` is specified on the forwardee, an extra `movzx` instruction is generated (on x86-64) prior to the tail call. This does not occur on Clang.

Observe (https://godbolt.org/z/kFGCpW):

```
namespace impl {
    __attribute__((noinline))
    static int func (bool v, int a, int b) {
        return v ? a/b : b/a;
    }
}

int func(bool v, int a, int b) {
    return impl::func(v, a, b);
}
```

On all tested versions (trunk (10) to GCC 4), this produces the following assembly for `func`:

```
func(bool, int, int):
  movzx edi, dil
  jmp impl::func(bool, int, int)
```

On Clang trunk (10) until Clang 5.0.0, this produces the following assembly for `func`:

```
func(bool, int, int): # @func(bool, int, int)
  jmp impl::func(bool, int, int) # TAILCALL
```

Clang 5.0.0 and below produce identical assembly to GCC:

```
func(bool, int, int): # @func(bool, int, int)
  movzx edi, dil
  jmp impl::func(bool, int, int) # TAILCALL
```


---


### compiler : `gcc`
### title : `Missed optimization with _mm256_permute2x128_si256 intrinsic`
### open_at : `2020-02-06T14:39:29Z`
### last_modified_date : `2020-02-07T07:40:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93613
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.1`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #93594 +++

#include <x86intrin.h>

__m256i
foo (__m128i x)
{
  return _mm256_permute2x128_si256 (_mm256_castsi128_si256 (x), _mm256_castsi128_si256 (x), 0x80);
}

__m256i
bar (__m128i x)
{
  return _mm256_permute2x128_si256 (_mm256_setzero_si256 (), _mm256_castsi128_si256 (x), 0x02);
}

__m256i
baz (__m128i x)
{
  return _mm256_permute2x128_si256 (_mm256_castsi128_si256 (x), _mm256_setzero_si256 (), 0x20);
}

__m256i
qux (__m128i x)
{
  return _mm256_permute2x128_si256 (_mm256_set_epi64x (1, 2, 3, 4), _mm256_set_epi64x (5, 6, 7, 8), 0x80);
}

__m256i
corge (__m128i x)
{
  return _mm256_permute2x128_si256 (_mm256_set_epi64x (1, 2, 3, 4), _mm256_set_epi64x (5, 6, 7, 8), 0x02);
}

__m256i
quux (__m128i x)
{
  return _mm256_permute2x128_si256 (_mm256_set_epi64x (1, 2, 3, 4), _mm256_set_epi64x (5, 6, 7, 8), 0x20);
}

The _mm256_permute2x128_si256 issues are similar, but really unrelated and IMHO should be tracked in a separate PR.  The problem there is that the pattern we use doesn't really describe what the instruction does, uses an UNSPEC_VPERMTI, which obviously can't be simplified by the generic code.  The reason is mainly that the instruction isn't just a two source permutation, but essentially 3 source permutation, with the third source of 0.


---


### compiler : `gcc`
### title : `Missed chance to use alias checks to vectorise invariant indirection`
### open_at : `2020-02-06T16:07:50Z`
### last_modified_date : `2020-02-07T07:48:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93616
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
For:

void
foo (char **dest_ptr, char *src)
{
  for (int i = 0; i < 100; ++i)
    (*dest_ptr)[i] = src[i];
}

we should be able to emit a runtime alias check that [src, src+99]
doesn't overlap *dest_ptr and then vectorise the loop normally.


---


### compiler : `gcc`
### title : `Improving modular calculations (e.g. divisibility tests).`
### open_at : `2020-02-08T15:43:05Z`
### last_modified_date : `2023-01-06T21:21:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93634
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Consider:

bool f(unsigned n) { return n % 6 == 4; }

at -O3 the code generated for x86_64 is

mov    %edi,%eax
mov    $0xaaaaaaab,%edx
imul   %rdx,%rax
shr    $0x22,%rax
lea    (%rax,%rax,2),%eax
add    %eax,%eax
sub    %eax,%edi
cmp    $0x4,%edi
sete   %al
retq   

whereas it could be

sub    $0x4,%edi
imul   $0xaaaaaaab,%edi,%edi
ror    %edi
cmp    $0x2aaaaaa9,%edi
setbe  %al
retq   

Notice the later is quite similar to what gcc generates for n % 6 == 3:

imul   $0xaaaaaaab,%edi,%edi
sub    $0x1,%edi
ror    %edi
cmp    $0x2aaaaaaa,%edi
setbe  %al
retq   

It's true that there's a small mathematical difference for the cases r <= 3 and r >= 4 but not enough to throw away the faster algorithm. I reckon this is not obvious and I refer to https://accu.org/var/uploads/journals/Overload154.pdf#page=13 which presents the overall idea and some benchmarks. In addition, it makes some comments on gcc's generated code for other cases of n % d == r. References therein provide mathematical proofs and extra benchmarks.

FWIW:

1) This relates to bug 82853 and bug 12849 and to a lesser extend bug 89845.

2) Specifically, it confirms the idea (for unsigned integers) described by Orr Shalom Dvory in https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82853#c33


---


### compiler : `gcc`
### title : `Dependence of optimization on use of constexpr seems inappropriate`
### open_at : `2020-02-10T23:23:50Z`
### last_modified_date : `2020-02-11T06:57:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93666
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `unknown`
### severity : `enhancement`
### contents :
In this example, the optimization behavior changes simply by adding the constexpr keyword:  https://gcc.godbolt.org/z/3zWWrH

My understanding is that constexpr should not be used as a "register-esque" hint to decide whether or not to optimize.  I think, for a constructor, it means, if the right conditions are met, non-locals constructed with it should be statically initialized, and are usable in compile-time const expressions.

Clang compiles this example as I would expect:  https://gcc.godbolt.org/z/hk6Zv5


---


### compiler : `gcc`
### title : `[10/11 Regression] vector creation from two parts of two vectors produces TBL rather than ins`
### open_at : `2020-02-13T01:03:27Z`
### last_modified_date : `2023-02-17T21:05:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93720
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Take:
#define vector __attribute__((vector_size(2*sizeof(double) )))

vector double
test_vpasted2 (vector double low, vector double high)
{
          return (vector double){low[0], high[1]};
}

--- CUT ---
This produces on the trunk:
        adrp    x0, .LC0
        ldr     q2, [x0, #:lo12:.LC0]
        tbl     v0.16b, {v0.16b - v1.16b}, v2.16b
        ret
---- CUT ----
Where LC0 is {0..7,24..31}
When really this should produce just ins.

GCC 8.2 produces:
        dup     v0.2d, v0.d[0]
        ins     v0.d[1], v1.d[1]
But GCC 9 produces the correct thing of just an ins (though I don't have a compiler to prove that).

The problem is GCC 10 converts the above code to:
  _4 = VEC_PERM_EXPR <low_1(D), high_2(D), { 0, 3 }>;

Which the back-end does not optimize to do an ins.


---


### compiler : `gcc`
### title : `swapping adjacent scalars could be more efficient`
### open_at : `2020-02-13T01:19:49Z`
### last_modified_date : `2023-08-04T22:31:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93721
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
For an implementation of a swap function like this:

  template <class T>
  void swap (std::pair<T, T> &x)
  {
    T t = x.first;
    x.first = x.second;
    x.second = t;
  }

GCC for x86 emits the ROL instruction for T=char:

  _Z4swapIcEvRSt4pairIT_S1_E:
  .LFB97:
	.cfi_startproc
	rolw	$8, (%rdi)
	ret
	.cfi_endproc

but a series of MOV instructions for T=short and T=int:

_Z4swapIiEvRSt4pairIT_S1_E:
.LFB97:
	.cfi_startproc
	movl	(%rdi), %eax
	movl	4(%rdi), %edx
	movl	%eax, 4(%rdi)
	movl	%edx, (%rdi)
	ret
	.cfi_endproc

A hand-coded (but convoluted) implementation of the function like below lets GCC for x86_64 emit the ROL instruction for both int and short:

  void swap (std::pair<int, int> &x)
  {
    int y[2], t;
    static_assert (sizeof x == sizeof y);
    __builtin_memcpy (y, &x, sizeof x);
    t = y[0]; y[0] = y[1]; y[1] = t;
    __builtin_memcpy (&x, y, sizeof x);
  }

  _ZL4swapRSt4pairIiiE:
  .LFB94:
	.cfi_startproc
	rolq	$32, (%rdi)
	ret
	.cfi_endproc

Benchmarking it shows that the ROL form is measurably faster (at least on my machine) than the MOV form.


---


### compiler : `gcc`
### title : `rorq is not produced for rotate on some cases`
### open_at : `2020-02-13T01:43:19Z`
### last_modified_date : `2020-02-13T12:37:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93722
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:

void f0(unsigned long *a)
{
  __uint128_t t0 = ((__uint128_t *)a)[0];
  __uint128_t t1 = t0>>sizeof(unsigned long)*8;
  __uint128_t  t2 = t0<<sizeof(unsigned long)*8;
  ((__uint128_t*)a)[0] = t1 | t2;
}

--- CUT ---
We should just produce:
	rolq	$32, (%rdi)
	ret

--- CUT ---
But we produce:
        movq    8(%rdi), %rdx
        movq    (%rdi), %rax
        movq    %rdx, (%rdi)
        movq    %rax, 8(%rdi)
        ret

Note it gets worse when using rotate to half-way


---


### compiler : `gcc`
### title : `inline memmove for insertion into small arrays`
### open_at : `2020-02-13T17:28:12Z`
### last_modified_date : `2021-09-05T05:06:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93737
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
GCC emits what's likely more efficient code for the insertion of elements into the middle of small arrays by copying parts of the array into a temporary buffer than it does for straight calls to memmove that achieve the same result without the temporary buffer.

The example below shows the difference.  Clang emits the same, presumably optimally efficient, code for both functions as GCC does for f0.

$ cat t.c && gcc -DN=32 -O2 -S -Wall -o/dev/stdout t.c
int a[N];

void f0 (int x)
{
  int b[N];
  __builtin_memcpy (b, a + 1, sizeof a - sizeof *a);
  __builtin_memcpy (a + 2, b, sizeof a - 2 * sizeof *a);
  a[1] = x;
}

void f2 (int x)
{
  __builtin_memmove (a + 2, a + 1, sizeof a - 2 * sizeof *a);
  a[1] = x;
}


	.file	"t.c"
	.text
	.p2align 4
	.globl	f0
	.type	f0, @function
f0:
.LFB0:
	.cfi_startproc
	subq	$16, %rsp
	.cfi_def_cfa_offset 24
	movdqu	a+20(%rip), %xmm5
	movdqu	a+36(%rip), %xmm4
	movdqu	a+52(%rip), %xmm3
	movdqu	a+68(%rip), %xmm2
	movdqu	a+84(%rip), %xmm1
	movdqu	a+100(%rip), %xmm0
	movups	%xmm5, a+24(%rip)
	movq	a+116(%rip), %rax
	movdqu	a+4(%rip), %xmm6
	movups	%xmm4, a+40(%rip)
	movl	%edi, a+4(%rip)
	movq	%rax, a+120(%rip)
	movups	%xmm6, a+8(%rip)
	movups	%xmm3, a+56(%rip)
	movups	%xmm2, a+72(%rip)
	movups	%xmm1, a+88(%rip)
	movups	%xmm0, a+104(%rip)
	addq	$16, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:
	.size	f0, .-f0
	.p2align 4
	.globl	f2
	.type	f2, @function
f2:
.LFB1:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	$120, %edx
	movl	%edi, %ebx
	movl	$a+4, %esi
	movl	$a+8, %edi
	call	memmove
	movl	%ebx, a+4(%rip)
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1:
	.size	f2, .-f2
	.globl	a
	.bss
	.align 32
	.type	a, @object
	.size	a, 128
a:
	.zero	128
	.ident	"GCC: (GNU) 10.0.1 20200212 (experimental)"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] test case gcc.target/powerpc/20050603-3.c fails`
### open_at : `2020-02-13T19:45:48Z`
### last_modified_date : `2023-07-13T05:11:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93738
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
git g:8d2d39587d941a40f25ea0144cceb677df115040, r9-3594

ok, this is an old issue that slipped past unnoticed somehow.  It was introduced in gcc 8 or maybe 9 but also then effects 10.  I am not sure which order the changes were done back in 8/9.  There is an extra instruction, rldicl, generated after the change in this test case.


Executing on host: /home/seurer/gcc/git/build/gcc-9-test-2/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-9-test-2/gcc/ /home/seurer/gcc/git/gcc-9-test-2/gcc/testsuite/gcc.target/powerpc/20050603-3.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -O2 -ffat-lto-objects -fno-ident -S -o 20050603-3.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/git/build/gcc-9-test-2/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-9-test-2/gcc/ /home/seurer/gcc/git/gcc-9-test-2/gcc/testsuite/gcc.target/powerpc/20050603-3.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -O2 -ffat-lto-objects -fno-ident -S -o 20050603-3.s
PASS: gcc.target/powerpc/20050603-3.c (test for excess errors)
PASS: gcc.target/powerpc/20050603-3.c scan-assembler-not \\mrlwinm
FAIL: gcc.target/powerpc/20050603-3.c scan-assembler-not \\mrldic
PASS: gcc.target/powerpc/20050603-3.c scan-assembler-not \\mrot[lr]
PASS: gcc.target/powerpc/20050603-3.c scan-assembler-not \\ms[lr][wd]
PASS: gcc.target/powerpc/20050603-3.c scan-assembler-times \\mrl[wd]imi 1
Executing on host: /home/seurer/gcc/git/build/gcc-9-test-2/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-9-test-2/gcc/ vmx_hw_available76502.c    -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never  -mno-vsx  -lm  -o vmx_hw_available76502.exe    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/git/build/gcc-9-test-2/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-9-test-2/gcc/ vmx_hw_available76502.c -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fdiagnostics-color=never -mno-vsx -lm -o vmx_hw_available76502.exe
Setting LD_LIBRARY_PATH to :/home/seurer/gcc/git/build/gcc-9-test-2/gcc::/home/seurer/gcc/git/build/gcc-9-test-2/gcc:/home/seurer/gcc/git/build/gcc-9-test-2/./gmp/.libs:/home/seurer/gcc/git/build/gcc-9-test-2/./prev-gmp/.libs:/home/seurer/gcc/git/build/gcc-9-test-2/./mpfr/src/.libs:/home/seurer/gcc/git/build/gcc-9-test-2/./prev-mpfr/src/.libs:/home/seurer/gcc/git/build/gcc-9-test-2/./mpc/src/.libs:/home/seurer/gcc/git/build/gcc-9-test-2/./prev-mpc/src/.libs:/home/seurer/gcc/git/build/gcc-9-test-2/./isl/.libs:/home/seurer/gcc/git/build/gcc-9-test-2/./prev-isl/.libs:/home/seurer/gcc/install/gcc-7.4.0/lib64
Execution timeout is: 300
spawn [open ...]
testcase /home/seurer/gcc/git/gcc-9-test-2/gcc/testsuite/gcc.target/powerpc/powerpc.exp completed in 1 seconds

		=== gcc Summary ===

# of expected passes		5
# of unexpected failures	1


/* This should generate a single rl[wd]imi. */
void rotins (unsigned int x)
{
  b.y = (x<<12) | (x>>20);
}



Before the change:

	addis 10,2,.LC0@toc@ha
	ld 10,.LC0@toc@l(10)
	lwz 9,0(10)
	rlwimi 9,3,20,20,23
	stw 9,0(10)
	blr


After the change:

	addis 10,2,.LC0@toc@ha
	ld 10,.LC0@toc@l(10)
***	rldicl 9,3,52,32
	lwz 3,0(10)
	rlwimi 3,9,0,3840
	stw 3,0(10)
	blr


---


### compiler : `gcc`
### title : `Optimization request: pattern-match typical addition overflow checks`
### open_at : `2020-02-14T16:10:29Z`
### last_modified_date : `2023-06-09T15:09:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93742
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
It would be nice if GCC could pattern-match the typical pattern for overflow-safe addition and optimize it to just branch on the overflow flag.  Right now, this:

_Bool add_overflow(int a, int b, int *res) {
    if ((a > 0 && b > INT_MAX - a) || (a < 0 && b < INT_MIN - a)) {
        return 1;
    } else {
        *res = a + b;
        return 0;
    }
}

compiles to this:

        testl   %edi, %edi
        jle     .L2
        movl    $2147483647, %ecx
        movl    $1, %eax
        subl    %edi, %ecx
        cmpl    %esi, %ecx
        jl      .L12
.L4:
        addl    %esi, %edi
        xorl    %eax, %eax
        movl    %edi, (%rdx)
        ret
.L12:
        ret
.L2:
        je      .L4
        movl    $-2147483648, %ecx
        movl    $1, %eax
        subl    %edi, %ecx
        cmpl    %esi, %ecx
        jle     .L4
        ret

#48580 is similar but about multiplication, for which the overflow-safe pattern is a lot more complicated.  But the addition one above is a lot simpler and pretty widespread I think.  For example, it's what CERT recommends: https://wiki.sei.cmu.edu/confluence/display/c/INT32-C.+Ensure+that+operations+on+signed+integers+do+not+result+in+overflow


---


### compiler : `gcc`
### title : `Redundant store not eliminated with intermediate instruction`
### open_at : `2020-02-14T20:33:59Z`
### last_modified_date : `2021-10-18T07:57:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93745
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
double d;
void f(long*p){
  long i=*p;
  d=3.;
  *p=i;
}

I would like *p=i to be eliminated. Without the unrelated intermediate line, FRE1 does remove it. If I insert another load of *p after d=3., FRE1 replaces it with i, so it knows that at that point *p and i are still the same.


---


### compiler : `gcc`
### title : `Use vpternlog for composite logical operations`
### open_at : `2020-02-16T19:38:48Z`
### last_modified_date : `2023-07-05T07:41:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93768
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
We should pattern-match multiple logical operations
into the ternary logical operator.  While there are
lots of obscure combinations available, probably the
most useful are

  Two-input inverted logicals:
  0x11  ~(B|C)
  0x77  ~(B&C)
  0x99  ~(B^C)
  0xbb  C|~B
  0xdd  B|~C

  Three-input simple logicals:
  0x80  A&B&C
  0x96  A^B^C
  0xfe  A|B|C

  Multiple alternatives of the ?: operation, which allows
  the memory-capable operand, C, in various positions, and
  allows the input-output operand, A, in various positions:
  0xe2  B?A:C
  0xe4  C?A:B
  0xb8  B?C:A
  0xd8  C?B:A
  0xca  A?B:C
  0xac  A?C:B


---


### compiler : `gcc`
### title : `std::tuple::operator< sometimes does needless extra work`
### open_at : `2020-02-16T21:19:24Z`
### last_modified_date : `2021-04-19T10:40:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93770
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `9.2.1`
### severity : `normal`
### contents :
Hello,

In the simple program below, tuple<Foo>::operator< calls the underlying Foo::operator< twice (10 < 9, then 9 < 10), even though only the first call is needed to get the correct answer. The same thing can happen with tuples of larger dimension: if the prefix (all but the final element) compares equal, but the final element compares greater-or-equal, we will do two calls to operator< on the final element even though only the first one is needed to get the right answer.

I have an example, and a suggested fix.

My example is a tuple of dimension 1:

$ g++ -O2 qwe.cc && ./a.out
Calculating 10 < 9
Calculating 9 < 10
!(t1 < t2)

Source:
===============================
#include <iostream>
#include <tuple>

struct Foo {
  explicit Foo(int data) : data_(data) {}
  int data_;
};

bool operator<(const Foo &lhs, const Foo &rhs) {
  std::cout << "Calculating " << lhs.data_ << " < " << rhs.data_ << std::endl;
  return lhs.data_ < rhs.data_;
}

int main() {
  auto t1 = std::make_tuple(Foo(10));
  auto t2 = std::make_tuple(Foo(9));

  if (t1 < t2) {
    std::cout << "t1 < t2" << std::endl;
  } else {
    std::cout << "!(t1 < t2)" << std::endl;
  }
}

========================
The culprit is this code in /usr/include/c++/9/tuple:L1388

  template<typename _Tp, typename _Up, size_t __i, size_t __size>
    struct __tuple_compare
    {
...
      static constexpr bool
      __less(const _Tp& __t, const _Up& __u)
      {
	return bool(std::get<__i>(__t) < std::get<__i>(__u))
	  || (!bool(std::get<__i>(__u) < std::get<__i>(__t))
	      && __tuple_compare<_Tp, _Up, __i + 1, __size>::__less(__t, __u));
      }
    };

The extra work happens at the final step of the recursion, when (__i + 1 == __size).  In this case, due to template specialization, the right side of the && (namely __tuple_compare<_Tp, _Up, __i + 1, __size>::__less(__t, __u)) evaluates to the constant false regardless of __t and __u. Therefore the && is always false, and the left side of the && (namely !bool(std::get<__i>(__u) < std::get<__i>(__t)) is wasted effort.

One easy fix would be to test for the base case explicitly, as in

      static constexpr bool
      __less(const _Tp& __t, const _Up& __u)
      { 
        return bool(std::get<__i>(__t) < std::get<__i>(__u))
          || (__i + 1 != __size   // *** THIS LINE IS NEW ***
              && !bool(std::get<__i>(__u) < std::get<__i>(__t))
              && __tuple_compare<_Tp, _Up, __i + 1, __size>::__less(__t, __u));
      }


$ g++ -v
Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none:hsa
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.2.1-9ubuntu2' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 9.2.1 20191008 (Ubuntu 9.2.1-9ubuntu2)


---


### compiler : `gcc`
### title : `SLP produces VEC_PERM when should have used vector generation`
### open_at : `2020-02-16T21:31:49Z`
### last_modified_date : `2023-05-12T06:01:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93771
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:
void f(double *a, double *t, double *d)
{
  double t1 = t[0] + d[0];
  double t2 = t[3] + d[1];
  a[0] = t1;
  a[1] = t2;
}

---- CUT ---
This produces:
  vect__1.13_14 = MEM <vector(2) double> [(double *)t_6(D)];
  vect__1.14_16 = MEM <vector(2) double> [(double *)t_6(D) + 16B];
  vect__1.15_17 = VEC_PERM_EXPR <vect__1.13_14, vect__1.14_16, { 0, 3 }>;
  vect__2.18_19 = MEM <vector(2) double> [(double *)d_7(D)];
  vect_t1_8.19_20 = vect__1.15_17 + vect__2.18_19;
  MEM <vector(2) double> [(double *)a_10(D)] = vect_t1_8.19_20;

BUT that VEC_PERM_EXPR is not so good here. we only need to load t[0] and t[1] and create a vector from those two.


---


### compiler : `gcc`
### title : `Optimizer produces suboptimal code related to -ftree-vrp`
### open_at : `2020-02-17T09:41:54Z`
### last_modified_date : `2021-07-13T13:45:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93781
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
For the following code, we can known the value C000003FE is always less then 5, so the return value should be true.
test base on the x86-64 gcc 9.2 on https://gcc.godbolt.org/, so get complicated assemble.

unsigned int foo (unsigned int arg)
{
  int C00000400 = arg - 3;
  unsigned int C000003FE = 4;
  int C000003FF = 0x1 << arg;

  if (C00000400 < 0)
     C000003FE = C000003FF;

  return C000003FE < 5;
}


---


### compiler : `gcc`
### title : `GCC adds unwanted nops to align loops on powerpc 8xx since r9-1623`
### open_at : `2020-02-18T10:29:17Z`
### last_modified_date : `2020-03-12T12:37:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93800
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
GCC 9.2 add nops in front of loops. GCC 8.1 didn't when compiled for powerpc 8xx. On the 8xx, a nop is 1 cycle and alignment of loops provide no benefit, so this is a waste of cycles.

Reproducer:

volatile int g;
int f(int a, int b)
{
	int i;

	for (i = 0; i < b; i++)
		a += g;
	return a;
}

Built with -m32 -mcpu=860 -O2

00000000 <f>:
   0:	2c 04 00 00 	cmpwi   r4,0
   4:	4c 81 00 20 	blelr   
   8:	3d 40 00 00 	lis     r10,0
			a: R_PPC_ADDR16_HA	g
   c:	7c 89 03 a6 	mtctr   r4
  10:	39 4a 00 00 	addi    r10,r10,0
			12: R_PPC_ADDR16_LO	g
  14:	60 00 00 00 	nop
  18:	60 00 00 00 	nop
  1c:	60 00 00 00 	nop
  20:	81 2a 00 00 	lwz     r9,0(r10)
  24:	7c 63 4a 14 	add     r3,r3,r9
  28:	42 00 ff f8 	bdnz    20 <f+0x20>
  2c:	4e 80 00 20 	blr



The same with GCC 8.1:

00000000 <f>:
   0:	2c 04 00 00 	cmpwi   r4,0
   4:	4c 81 00 20 	blelr   
   8:	3d 40 00 00 	lis     r10,0
			a: R_PPC_ADDR16_HA	g
   c:	7c 89 03 a6 	mtctr   r4
  10:	39 4a 00 00 	addi    r10,r10,0
			12: R_PPC_ADDR16_LO	g
  14:	81 2a 00 00 	lwz     r9,0(r10)
  18:	7c 63 4a 14 	add     r3,r3,r9
  1c:	42 00 ff f8 	bdnz    14 <f+0x14>
  20:	4e 80 00 20 	blr


---


### compiler : `gcc`
### title : `gcc generates a rlwinm/or pair instead of a single rlwimi (powerpc)`
### open_at : `2020-02-18T11:52:44Z`
### last_modified_date : `2020-04-30T21:36:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93802
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
unsigned long f(unsigned short x)
{
	return (x << 16) | x;
}



Results in:

00000000 <f>:
   0:	54 69 80 1e 	rlwinm  r9,r3,16,0,15
   4:	7d 23 1b 78 	or      r3,r9,r3
   8:	4e 80 00 20 	blr



Should instead be:

rlwimi r3, r3, 16, 0, 15
blr

Problem seen with at least GCC 9.2 and GCC 8.1 and GCC 5.5


---


### compiler : `gcc`
### title : `gcc or lto-wrapper does not consider individual bitfield values on static analysis and instead tests the whole value of all bitfield bits combined`
### open_at : `2020-02-21T18:53:19Z`
### last_modified_date : `2021-02-14T23:18:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93873
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `6.3.0`
### severity : `normal`
### contents :
On gcc version 6.3.0 20170516 (Debian 6.3.0-18+deb9u1)

Given these two files (and stdtypes.h being available from https://emil.fi/d/stdtypes.h but it's basically just include stdint.h and typedef u?int\d+_t to the correct [su]\d+

$cat ltobuglib.h
#include <stdtypes.h>
#include <stdlib.h>
#include <unistd.h>

struct thing
{
	u64 v;
	struct
	{
		u64 dox:1;
		u64 freeme:1;
	} flags;
};

struct thing *makeit(struct thing *t)
{
	u8 dynamic=!t;
	if(dynamic)
	{
		t=calloc(1, sizeof(*t));
		if(!t)
		{
			return(NULL);
		}
	}
	t->v=0;
	t->flags.dox=1;
	t->flags.freeme=dynamic;
	return(t);
}

void freeit(struct thing *t)
{
	if(t->flags.freeme)
	{
		free(t);
	}
}



$cat ltobug.c
#include <stdtypes.h>
#include <stdio.h>
#include "ltobuglib.h"

s32 main(void)
{
	struct thing t={0};
	if(!makeit(&t))
	{
		return(-1);
	}
	printf("%lu %u %u\n", t.v, t.flags.dox, t.flags.freeme);
	freeit(&t);
	return(0);
}



$gcc -Wall -Werror -Wextra -O3 -flto -o ltobug ltobug.c



produces



In function ‘freeit’,
    inlined from ‘main’ at ltobug.c:13:2:
ltobuglib.h:36:3: error: attempt to free a non-heap object ‘t’ [-Werror=free-nonheap-object]
   free(t);
   ^
lto1: all warnings being treated as errors
lto-wrapper: fatal error: gcc returned 1 exit status
compilation terminated.
/usr/bin/ld: error: lto-wrapper failed
collect2: error: ld returned 1 exit status



but changing t->flags.dox=1 to 0 compiles cleanly.
Without -flto (and dox set to 1) the result is



In file included from ltobug.c:3:0:
In function ‘freeit’,
    inlined from ‘main’ at ltobug.c:13:2:
ltobuglib.h:36:3: error: attempt to free a non-heap object ‘t’ [-Werror=free-nonheap-object]
   free(t);
   ^~~~~~~
cc1: all warnings being treated as errors



On a different project of mine, the code cleanly compiles without -flto (but fails with -flto as above) but here gcc seems to do the same issue without -flto, too.


---


### compiler : `gcc`
### title : `Spurious instruction kshiftlw issued`
### open_at : `2020-02-22T18:17:21Z`
### last_modified_date : `2021-09-04T21:46:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93885
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `enhancement`
### contents :
[gcc version 10.0.1 20200216]

The following code ( https://www.godbolt.org/z/JynEy6 )
#include <immintrin.h>
__m512 f(__m512 x, __m512 y, __m512 z)
{
    const __mmask16 masky = 0b0010001000100010;
    const __m512 xy = _mm512_mask_blend_ps(masky, x, y);
    const __mmask16 maskz = _kshiftli_mask16(masky, 1);
    return _mm512_mask_blend_ps(maskz, xy, z);
}

computes 'maskz' by shifting 'masky'. The generated asm:

f(float __vector(16), float __vector(16), float __vector(16)):
        mov     eax, 8738
        kmovw   k1, eax
        mov     eax, 17476
        vmovaps zmm0{k1}, zmm1
        kmovw   k2, eax
        kshiftlw        k1, k1, 1
        vmovaps zmm0{k2}, zmm2
        ret

both loads the value (17476) directly, *and* performs the left-shift 'kshiftlw'. 
Unless I'm missing something, it would seem one of them (move+kmovw, or kshiftlw) isn't needed. 

Thanks.


---


### compiler : `gcc`
### title : `CSE where clobber writes the same value`
### open_at : `2020-02-23T01:09:38Z`
### last_modified_date : `2021-10-31T10:26:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93891
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
void f(int**p,int**q){
  ++**p;
  *q=*p;
  --**p;
}

produces

  _1 = *p_8(D);
  _2 = *_1;
  _3 = _2 + 1;
  *_1 = _3;
  *q_10(D) = _1;
  _4 = *p_8(D);

etc

where we do not CSE _4 -> _1. We do have code in vn_reference_lookup_3 to handle this kind of thing, look past a potential clobber, and ignore it if we find the same value. But for this test, vn_reference_lookup_2 returns 0 and we never look further than *q=1 when we valueize *p for _4. If we did, I expect we would skip *_1=3 using TBAA and eventually find _1=*p, which matches the potential clobber.

(this comes from the same code as PR 93745 but should not have the same issue)


---


### compiler : `gcc`
### title : `MIPS32r2: GCC is unable to figure out that it can use a single INS instruction instead of SLL+OR`
### open_at : `2020-02-23T10:02:24Z`
### last_modified_date : `2022-12-24T02:24:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93893
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `normal`
### contents :
Created attachment 47891
Testcase for a single INS instruction vs. SLL+OR

$ mipsel-unknown-linux-gnu-gcc -c -Os -march=mips32r2 testcase.c 
$ mipsel-unknown-linux-gnu-objdump -d testcase.o

testcase.o:     file format elf32-tradlittlemips


Disassembly of section .text:

00000000 <transpose_c>:
   0:	8c820000 	lw	v0,0(a0)
   4:	94a30000 	lhu	v1,0(a1)
   8:	00021400 	sll	v0,v0,0x10
   c:	00431025 	or	v0,v0,v1
  10:	03e00008 	jr	ra
  14:	acc20000 	sw	v0,0(a2)

00000018 <transpose_inline_asm>:
  18:	8ca20000 	lw	v0,0(a1)
  1c:	8c830000 	lw	v1,0(a0)
  20:	7c62fc04 	ins	v0,v1,0x10,0x10
  24:	03e00008 	jr	ra
  28:	acc20000 	sw	v0,0(a2)
  2c:	00000000 	nop


The C implementation uses an extra instruction compared to the inline assembly variant of the same function.


---


### compiler : `gcc`
### title : `Store merging could merge 0 constructors into {}`
### open_at : `2020-02-23T20:08:09Z`
### last_modified_date : `2022-06-30T22:17:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93896
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.2.1`
### severity : `enhancement`
### contents :
struct M {
    constexpr M() :p{},sz{},cz{}{}
public:
    char* p;
    unsigned sz;
    unsigned cap;
};

struct A { M a,b,c; A(); };
A::A() :a{},b{},c{}{}

gcc 9.2.1 with -march=native -Os on Haswell generates:

_ZN1AC2Ev:
        movq    $0, (%rdi)
        movq    $0, 8(%rdi)
        movq    $0, 16(%rdi)
        movq    $0, 24(%rdi)
        movq    $0, 32(%rdi)
        movq    $0, 40(%rdi)
        ret

Store merging is obviously working here, but does not use SSE movups. If the constructor is removed or defaulted the output is:

_ZN1AC2Ev:
        vpxor   %xmm0, %xmm0, %xmm0
        vmovups %xmm0, (%rdi)
        vmovups %xmm0, 16(%rdi)
        vmovups %xmm0, 32(%rdi)
        ret

Whether the type is trivial should not matter by the time store merging occurs, but for some reason it does.


---


### compiler : `gcc`
### title : `Poor trivial structure initialization code with -O3`
### open_at : `2020-02-23T20:55:12Z`
### last_modified_date : `2023-08-26T23:37:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93897
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
The following code:

    #include <cstdint>

    struct B {
        std::int64_t x;
        std::int32_t y;
        std::int32_t z;
    };
    
    B f(std::int64_t x, std::int32_t y, std::int32_t z) { 
        return {x, y, z}; 
    }

Compiled with `gcc -O3 -std=gnu++17 -march=skylake` generates the following assembly:

    f(long, int, int):
            mov     QWORD PTR [rsp-16], 0
            mov     QWORD PTR [rsp-24], rdi
            vmovdqa xmm1, XMMWORD PTR [rsp-24]
            vpinsrd xmm0, xmm1, esi, 2
            vpinsrd xmm2, xmm0, edx, 3
            vmovdqa XMMWORD PTR [rsp-24], xmm2
            mov     rax, QWORD PTR [rsp-24]
            mov     rdx, QWORD PTR [rsp-16]
            ret

Which looks a bit excessive.

Whereas when compiled with `clang-9.0 -O3 -std=gnu++17 -march=skylake` it produces the expected:

    f(long, int, int):
            mov     rax, rdi
            shl     rdx, 32
            mov     ecx, esi
            or      rdx, rcx
            ret

https://gcc.godbolt.org/z/udsiyF


---


### compiler : `gcc`
### title : `conversion from 64-bit long or unsigned long to double prevents simple optimization`
### open_at : `2020-02-24T12:38:50Z`
### last_modified_date : `2021-12-22T13:18:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93902
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Optimizations that are done with conversions from 32-bit unsigned int to double are no longer done with conversions from 64-bit unsigned long to double. For the case 64-bit long to double, this depends.

Example:

void bar (void);

void foo1 (unsigned int a, unsigned int b)
{
  if (a == b)
    {
      if ((double) a != (double) b)
        bar ();
    }
}

void foo2 (long a, long b)
{
  if (a == b)
    {
      if ((double) a != (double) b)
        bar ();
    }
}

void foo3 (unsigned long a, unsigned long b)
{
  if (a == b)
    {
      if ((double) a != (double) b)
        bar ();
    }
}

Tests done on x86_64 with: gcc-10 (Debian 10-20200222-1) 10.0.1 20200222 (experimental) [master revision 01af7e0a0c2:487fe13f218:e99b18cf7101f205bfdd9f0f29ed51caaec52779]

First, using only -O3 gives:

* For foo1, just a "ret", i.e. everything has been optimized.

* For foo2:

        .cfi_startproc
        cmpq    %rsi, %rdi
        je      .L7
.L3:
        ret
        .p2align 4,,10
        .p2align 3
.L7:
        pxor    %xmm0, %xmm0
        cvtsi2sdq       %rdi, %xmm0
        ucomisd %xmm0, %xmm0
        jnp     .L3
        jmp     bar@PLT
        .cfi_endproc

I assume that this might be different from foo1 because the conversion can yield a rounding error (since 64 is larger than 53). However, both roundings are done in the same way. The only thing that could prevent optimization is the side effect introduced by the inexact operation, which raises the inexact flag. But GCC ignores it by default (it assumes that the STDC FENV_ACCESS pragma is off). And GCC knows how to optimize this case (see the other test below).

* For foo3, this is even much more complicated, even though the C code seems simpler (because the integers can take only non-negative values):

        .cfi_startproc
        cmpq    %rsi, %rdi
        je      .L16
.L8:
        ret
        .p2align 4,,10
        .p2align 3
.L16:
        testq   %rsi, %rsi
        js      .L10
        pxor    %xmm1, %xmm1
        cvtsi2sdq       %rsi, %xmm1
.L11:
        testq   %rsi, %rsi
        js      .L12
        pxor    %xmm0, %xmm0
        cvtsi2sdq       %rsi, %xmm0
.L13:
        ucomisd %xmm0, %xmm1
        jp      .L15
        comisd  %xmm0, %xmm1
        je      .L8
.L15:
        jmp     bar@PLT
        .p2align 4,,10
        .p2align 3
.L12:
        movq    %rsi, %rax
        andl    $1, %esi
        pxor    %xmm0, %xmm0
        shrq    %rax
        orq     %rsi, %rax
        cvtsi2sdq       %rax, %xmm0
        addsd   %xmm0, %xmm0
        jmp     .L13
        .p2align 4,,10
        .p2align 3
.L10:
        movq    %rsi, %rax
        movq    %rsi, %rdx
        pxor    %xmm1, %xmm1
        shrq    %rax
        andl    $1, %edx
        orq     %rdx, %rax
        cvtsi2sdq       %rax, %xmm1
        addsd   %xmm1, %xmm1
        jmp     .L11
        .cfi_endproc

Now, let's add the -ffinite-math-only option, i.e.: "-O3 -ffinite-math-only". Since integers cannot be NaN or Inf, and overflow in the conversion to double is not possible, this should not change anything.

* foo1 is still optimized. Good.

* foo2 is now optimized like foo1, i.e. to just a "ret". This is good, but surprising compared to the foo2 case without -ffinite-math-only, and to foo3 below.

* foo3 is still not optimized and is almost as complicated, with just 2 instructions removed, probably due to the -ffinite-math-only (in case GCC thought that special values were possible).


---


### compiler : `gcc`
### title : `VRP forgets range of value read from memory`
### open_at : `2020-02-24T23:31:13Z`
### last_modified_date : `2023-09-19T15:18:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93917
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
First a case that works:

void f(int n){
  if(n<0)__builtin_unreachable();
}

EVRP assigns a range to n, and VRP1 folds the comparison to false.

Let's add an indirection:

void f(int*n){
  if(*n<0)__builtin_unreachable();
}

EVRP still assigns a range to *n, but now VRP1 seems to forget that range, does not fold the comparison, and reassigns a range to *n. And again in VRP2. __builtin_unreachable survives all the way to FAB, which in my original code prevents any loop optimization. I know of plans to fold __builtin_unreachable earlier always, which would likely solve the issue, but independently of that, it seems that VRP is dropping some interesting information present in old ranges, possibly because a memory read is not considered an interesting statement.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Unnecessary broadcast instructions for AVX512`
### open_at : `2020-02-25T17:59:20Z`
### last_modified_date : `2023-07-07T10:37:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93930
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
Created attachment 47908
Test case

The code below generates unnecessary broadcast instructions for AVX512, compiled with "-Ofast -march=skylake-avx512". This occurs for gcc trunk and 9.2/8.3 but not 7.5.

Most constants are read from memory via vbroadcastss except two, which are read as scalars and then broadcast within the loop. For gcc 7.5 all constants are read via vbroadcastss.

The problem seems to be more frequent for larger functions.

 ---

Compiler output for gcc 9.2:

        ...
.L3:
        vmovaps zmm0, ZMMWORD PTR [rdi]
        add     rdi, 64
        vmovaps zmm3, zmm0
        vmovaps zmm1, zmm0
        vmulps  zmm2, zmm0, zmm0
        vfmadd132ps     zmm3, zmm11, zmm12
        vfmadd132ps     zmm1, zmm13, zmm14
        vmovaps zmm4, zmm0
        vfmadd132ps     zmm4, zmm7, zmm8
        sub     rsi, -128
        vfmadd132ps     zmm1, zmm3, zmm2
        vmovaps zmm3, zmm0
        vfmadd132ps     zmm3, zmm9, zmm10
        vfmadd132ps     zmm3, zmm4, zmm2
        vbroadcastss    zmm4, xmm15         <--- Broadcast within loop
        vmulps  zmm3, zmm3, zmm1
        vmovaps ZMMWORD PTR [rsi-128], zmm3
        vbroadcastss    zmm3, xmm16         <--- Broadcast within loop
        vfmadd132ps     zmm3, zmm4, zmm0
        vfmadd132ps     zmm0, zmm5, zmm6
        vfmadd132ps     zmm0, zmm3, zmm2
        vmulps  zmm1, zmm1, zmm0
        vmovaps ZMMWORD PTR [rsi-64], zmm1
        cmp     rdi, rax
        jne     .L3
        ...

 ---

#include <immintrin.h>

static __m512 f(__m512 x)
{
    __m512 a = _mm512_set1_ps(11);
    __m512 b = _mm512_set1_ps(12);
    __m512 c = _mm512_set1_ps(13);
    __m512 d = _mm512_set1_ps(14);

    __m512 y = _mm512_mul_ps(x, x);

    return _mm512_fmadd_ps(y, _mm512_fmadd_ps(x, a, b), _mm512_fmadd_ps(x, c, d));
}

static __m512 g(__m512 x)
{
    __m512 a = _mm512_set1_ps(21);
    __m512 b = _mm512_set1_ps(22);
    __m512 c = _mm512_set1_ps(23);
    __m512 d = _mm512_set1_ps(24);

    __m512 y = _mm512_mul_ps(x, x);

    return _mm512_fmadd_ps(y, _mm512_fmadd_ps(x, a, b), _mm512_fmadd_ps(x, c, d));
}

static __m512 h(__m512 x)
{
    __m512 a = _mm512_set1_ps(31);
    __m512 b = _mm512_set1_ps(32);
    __m512 c = _mm512_set1_ps(33);
    __m512 d = _mm512_set1_ps(34);

    __m512 y = _mm512_mul_ps(x, x);

    return _mm512_fmadd_ps(y, _mm512_fmadd_ps(x, a, b), _mm512_fmadd_ps(x, c, d));
}

void test(__m512 *x, __m512 *y, int n)
{
    for (int i = 0; i < n; i++) {
        __m512 u = *x++;
        __m512 v = h(u);

        *y++ = _mm512_mul_ps(f(u), v);
        *y++ = _mm512_mul_ps(g(u), v);
    }
}


---


### compiler : `gcc`
### title : `PowerPC vec_extract with variable element number has code regressions for V2DI/V2DF vectors`
### open_at : `2020-02-25T19:43:21Z`
### last_modified_date : `2020-04-16T18:05:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93932
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
I've been looking at vec_extract recently, both in terms of support for the -mcpu=future and to look at supporting PR target/93230 in GCC 11.

In some cases, if you have a vec_extract built-in function where the vector is in a register, and the element is variable, the compiler decides store this vector to memory, and then do the variable extract using a scalar load.  Unfortunately, this lead to a STORE-HIT-LOAD slowdown, as the scalar load will likely have to wait for the vector store to finish.

The test cases are the fold-vect-extract-<type>.p{7,8,9}.c} files in the gcc.target/powerpc directory, where <type> is 'char', 'short', 'int', 'longlong', 'float' and 'double', and the p7/p8/p9 indicates whether the test is for -mcpu=power7, -mcpu=power8, or -mcpu=power9.

For -mcpu=power8, the regressions are:
fold-vect-extract-double.p8.c: GCC 9.x and current trunk
fold-vect-extract-longlong.p8.c: GCC 9.x and current trunk

For -mcpu=power9, the regressions are:
fold-vect-extract-double.p9.c: GCC 9.x (current trunk is ok)
fold-vect-extract-longlong.p9.c: GCC 9.x (current trunk is ok)


---


### compiler : `gcc`
### title : `missing optimization for floating-point expression converted to integer whose result is known at compile time`
### open_at : `2020-02-26T02:53:28Z`
### last_modified_date : `2021-12-22T10:05:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93939
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the following example.

#include <stdio.h>

typedef double T;

int main (void)
{
  volatile T a = 8;
  T b = a;
  int i;

  i = 3 * b;
  printf ("%d\n", (int) i);

  if (b == 8)
    {
      i = 3 * b;
      printf ("%d\n", i == 24);
    }

  return 0;
}

Even if one compiles it with -O3, a comparison instruction for i == 24 is generated, while its result 1 is known at compile time.

If I change T to int, or change the type of i to double, or comment out the first printf (so that the first i is not used), then the i == 24 is optimized as expected.

Tested under Debian/unstable / x86_64 with:

gcc-10 (Debian 10-20200222-1) 10.0.1 20200222 (experimental) [master revision 01af7e0a0c2:487fe13f218:e99b18cf7101f205bfdd9f0f29ed51caaec52779]


---


### compiler : `gcc`
### title : `IRA/LRA happily rematerialize (un-CSEs) loads without register pressure`
### open_at : `2020-02-26T10:53:00Z`
### last_modified_date : `2020-10-12T11:05:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93943
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
long a[1024], b[512], c[512];

void foo ()
{
  for (int i = 0; i < 256; ++i)
    {
      b[2*i] = a[4*i];
      b[2*i+1] = a[4*i+2];
      c[2*i] = a[4*i+1];
      c[2*i+1] = a[4*i+3];
    }
}

at -O3 is vectorized with SSE2 V2DImode vectors doing two vector loads,
two shuffles and two vector stores.  But we then emit

.L2:
        movdqa  a(%rax,%rax), %xmm0
        movdqa  %xmm0, %xmm1
        punpckhqdq      a+16(%rax,%rax), %xmm0
        punpcklqdq      a+16(%rax,%rax), %xmm1
        addq    $16, %rax
        movaps  %xmm1, b-16(%rax)
        movaps  %xmm0, c-16(%rax)
        cmpq    $4096, %rax
        jne     .L2

so took advantage of the memory op variant of the punpck instructions
enlarging the code and using more load uops.


---


### compiler : `gcc`
### title : `load via restricted pointer not eliminated after a store via a restricted pointer of incompatible type`
### open_at : `2020-02-28T17:05:46Z`
### last_modified_date : `2020-03-01T21:36:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93970
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
In the test case below the test in f() is folded to false as expected, but the similar test in g() is not folded, even though in both functions the pointers are restricted and in g() they additionally point to incompatible types.

$ cat t.c && gcc -O2 -S -Wall -Wextra  -fdump-tree-optimized=/dev/stdout t.c
void f (int *__restrict p, int *__restrict q)
{
  int t = *q;
  *p = 0;
  if (t != *q)            // folded to false
    __builtin_abort ();
}

void g (int *__restrict p, double *__restrict q)
{ 
  double t = *q;
  *p = 0;
  if (t != *q)            // not folded
    __builtin_abort ();
}



;; Function f (f, funcdef_no=0, decl_uid=1931, cgraph_uid=1, symbol_order=0)

f (int * restrict p, int * restrict q)
{
  <bb 2> [local count: 1073741824]:
  *p_3(D) = 0;
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1936, cgraph_uid=2, symbol_order=1)

g (int * restrict p, double * restrict q)
{
  double t;

  <bb 2> [local count: 1073741824]:
  t_3 = *q_2(D);
  *p_4(D) = 0;
  if (t_3 != t_3)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `std::string considered to alias declared objects of incompatible types`
### open_at : `2020-02-28T17:07:17Z`
### last_modified_date : `2021-12-12T05:29:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93971
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Inspired by the discussion in pr93745 I tried the following to see if GCC was able to eliminate the test: it isn't, not even after declaring std::vector::_M_impl._M_start restrict.

In bug 49367 Jason points out a similar limitation except involving pointers of the same type pointing to (potentially) allocated storage.  Here it's clear that _M_impl._M_start cannot point to x because x is accessed (i.e., first read and then written) by an incompatible type.  Another similar test case is in bug 49761.

Even declaring v __restrict doesn't help.

$ cat t.C && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout t.C

  #include <vector>

  double x;

  void f (std::vector<int> &v)
  {
    double t = x;
    v[0] = 0;
    if (t != x)
      __builtin_abort ();
  }

  ;; Function f (_Z1fRSt6vectorIiSaIiEE, funcdef_no=880, decl_uid=16170, cgraph_uid=169, symbol_order=170)

  f (struct vector & v)
  {
    double t;
    int * _4;

    <bb 2> [local count: 1073741824]:
    t_2 = x;
    _4 = v_3(D)->D.17243._M_impl.D.16553._M_start;
    MEM[(value_type &)_4] = 0;
    if (t_2 != t_2)
      goto <bb 3>; [0.00%]
    else
      goto <bb 4>; [100.00%]

    <bb 3> [count: 0]:
    __builtin_abort ();

    <bb 4> [local count: 1073741824]:
    return;

  }

The roughly equivalent C test case is:

  struct V { int *restrict p; };

  double x;

  void f (struct V * /* restrict */ p)   // restrict here makes no difference
  {
    double t = x;
    p->p[0] = 0;
    if (t != x)
      __builtin_abort ();
  }


---


### compiler : `gcc`
### title : `[x86] Silly code generation for _addcarry_u32/_addcarry_u64`
### open_at : `2020-03-01T23:00:01Z`
### last_modified_date : `2023-06-03T20:02:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93990
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `enhancement`
### contents :
Bug 67317 has regressed, starting in GCC 6

Same test case as 67317:

#include <x86intrin.h>
#include <stdint.h>

	unsigned long long testcarry(unsigned long long a, unsigned long long b, unsigned long long c, unsigned long long d)
	{
		unsigned long long result0, result1;
		_addcarry_u64(_addcarry_u64(0, a, c, &result0), b, d, &result1);
		return result0 ^ result1;
	}

GCC 5.4.1 (only) produces the expected output

	.cfi_startproc
	addq	%rdi, %rdx	# a, tmp99
	adcq	%rsi, %rcx	# b, tmp107
	movq	%rdx, %rax	# tmp99, D.28638
	xorq	%rcx, %rax	# tmp107, D.28638
	ret
	.cfi_endproc

GCC 6.4.1, 7.4.1, 8.3.0 and 9.2.0 all produce variations on this [GCC 9.2.0 output here]:

	.cfi_startproc
	subq	$40, %rsp	#,
	.cfi_def_cfa_offset 48
# adx.cpp:5: 	{
	movq	%fs:40, %rax	# MEM[(<address-space-1> long unsigned int *)40B], tmp107
	movq	%rax, 24(%rsp)	# tmp107, D.33144
	xorl	%eax, %eax	# tmp107
# /usr/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/adxintrin.h:69:   return __builtin_ia32_addcarryx_u64 (__CF, __X, __Y, __P);
	addq	%rdx, %rdi	# tmp105, tmp93
	movq	%rsi, %rax	# tmp104, tmp104
	setc	%r8b	#, _12
	movq	%rdi, 8(%rsp)	# tmp93,
	addb	$-1, %r8b	#, _12
	adcq	%rcx, %rax	# tmp106, tmp104
	movq	%rax, 16(%rsp)	# tmp97,
# adx.cpp:8: 		return result0 ^ result1;
	xorq	%rdi, %rax	# tmp93, <retval>
# adx.cpp:9: 	}
	movq	24(%rsp), %rdx	# D.33144, tmp109
	xorq	%fs:40, %rdx	# MEM[(<address-space-1> long unsigned int *)40B], tmp109
	jne	.L5	#,
	addq	$40, %rsp	#,
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret	

All were compiled with -O3. I checked 9.2.0 output with -O and -O2 as well, with no significant change.


---


### compiler : `gcc`
### title : `Poor codegen for cond ? lval1 : lval2`
### open_at : `2020-03-02T22:48:44Z`
### last_modified_date : `2023-09-21T14:01:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94006
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
I noticed that this (compiled with -std=c++2a and any optimization level except -O0):

#include <compare>

std::strong_ordering f1(int a, int b)
{
  return a == b ? std::strong_ordering::equal
   : std::strong_ordering::less;
}

compiles to:

_Z2f1ii:
        cmp     edi, esi
        jne     .L3
        mov     al, BYTE PTR _ZNSt15strong_ordering5equalE[rip]
        ret
.L3:
        mov     al, BYTE PTR _ZNSt15strong_ordering4lessE[rip]
        ret
_ZNSt15strong_ordering5equalE:
        .zero   1
_ZNSt15strong_ordering4lessE:
        .byte   -1


Whereas this less readable form:

#include <compare>

std::strong_ordering f2(int a, int b)
{
  return (a == b) <=> true;
}

produces much smaller code:

_Z2f2ii:
        cmp     edi, esi
        setne   al
        neg     eax
        ret


Jason observed that it seems to be because the result of <=> is a prvalue, whereas the ?: version is returning from lvalues.

We get similar codegen (but with a less extreme difference in size) for this:

struct A { int i; };
constexpr A a1 { 1 };
constexpr A a2 { 0 };

A f1 (int a, int b)
{
  return a == b ? a1 : a2;
}

A f2 (int a, int b)
{
  return A{a == b ? a1.i : a2.i};
}

The first function is obviously easier to read, but the second produces smaller code because the two global variables are not emitted.


---


### compiler : `gcc`
### title : `[missed optimization] constant automatic string variable not elided`
### open_at : `2020-03-03T04:34:22Z`
### last_modified_date : `2020-03-04T07:37:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94010
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c`
### version : `9.2.1`
### severity : `normal`
### contents :
-fmerge-all-constants misses a very simple optimization scenario involving an array:

    extern void g(const char* c);

    void f() {
        const char c[] = "12345";
        g(c);
    }

produces a local array (with or without the flag).

If `const char c[]` is replaced by `const char* c`, the above code produces a global array as expected.

This seems to be the case in every GCC version I have tested, starting from 4.9.


---


### compiler : `gcc`
### title : `combine missed opportunity to simplify comparisons with zero`
### open_at : `2020-03-04T08:02:45Z`
### last_modified_date : `2022-08-09T17:57:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94026
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Created attachment 47966
proposed patch to fix this issue

Simple test case:
int
foo (int c, int d)
{
  int a = (c >> d) & 7;

  if (a >= 2) {
    return 1;
  }

  return 0;
}

Compile option: gcc -S -O2 test.c


On aarch64, GCC trunk emits 4 instrunctions:
        asr     w0, w0, 8
        tst     w0, 6
        cset    w0, ne
        ret

which can be further simplified into:
        tst     x0, 1536
        cset    w0, ne
        ret

We see the same issue on other targets such as i386 and x86-64.

Attached please find proposed patch for this issue.


---


### compiler : `gcc`
### title : `missing floating-point optimization of x + 0 when x is not zero`
### open_at : `2020-03-04T10:44:21Z`
### last_modified_date : `2020-03-04T11:27:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94031
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
In floating point, when x is not 0 (and not sNaN when supported), x + 0 can be optimized to x. GCC misses this optimization:

void bar (double);

void foo1 (double x)
{
  x *= 2.0;  /* cannot be sNaN */
  if (x != 0)
    bar (x - 0);
}

void foo2 (double x)
{
  x *= 2.0;  /* cannot be sNaN */
  if (x != 0)
    bar (x + 0);
}

In foo1, x - 0 can be optimized to x in rounding to nearest (even when x is ±0), and GCC optimizes as expected:

.L4:
        jmp     bar@PLT

In foo2, in order to be able to optimize, one needs x != 0, guaranteed by the "if" condition, but GCC misses the optimization:

.L10:
        addsd   %xmm1, %xmm0
        jmp     bar@PLT

Tested under Debian x86_64 with gcc-10 (Debian 10-20200222-1) 10.0.1 20200222 (experimental) [master revision 01af7e0a0c2:487fe13f218:e99b18cf7101f205bfdd9f0f29ed51caaec52779]


---


### compiler : `gcc`
### title : `Runtime varies 2x just by order of two int assignments`
### open_at : `2020-03-04T20:26:22Z`
### last_modified_date : `2020-03-06T05:50:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94037
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.2.0`
### severity : `normal`
### contents :
(This report re-uses some code from https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93165 but identifies an entirely different problem.)

Given:

```
bool swap_if(bool c, int& a, int& b) {
  int v[2] = { a, b };
#ifdef FAST  /* 4.6s */
  b = v[1-c], a = v[c];
#else /* SLOW, 9.8s */
  a = v[c], b = v[!c];
#endif
  return c;
}

int* partition(int* begin, int* end) {
  int pivot = end[-1];
  int* left = begin;
  for (int* right = begin; right < end - 1; ++right) {
    left += swap_if(*right <= pivot, *left, *right);
  }
  int tmp = *left; *left = end[-1], end[-1] = tmp;
  return left;
}

void quicksort(int* begin, int* end) {
  while (end - begin > 1) {
    int* mid = partition(begin, end);
    quicksort(begin, mid);
    begin = mid + 1;
} }

static const int size = 100'000'000;

#include <sys/mman.h>
#include <unistd.h>
#include <fcntl.h>

int main(int, char**) {
  int fd = ::open("1e8ints", O_RDONLY);
  int perms = PROT_READ|PROT_WRITE;
  int flags = MAP_PRIVATE|MAP_POPULATE|MAP_NORESERVE;
  auto* a = (int*) ::mmap(nullptr, size * sizeof(int), perms, flags, fd, 0);

  quicksort(a, a + size);

  return a[0] == a[size - 1];
}
```
after
```
  $ dd if=/dev/urandom of=1e8ints bs=1000000 count=400
```

The run time of the the program above, built "-O3 -march=skylake"
vs. "-DFAST -O3 -march=skylake", varies by 2x on Skylake, similarly
on Haswell. Both cases are almost equally fast on Clang, matching 
G++'s fast version. The difference between "!c" and "1-c" in the 
array index exacerbates the disparity.

Godbolt `<https://godbolt.org/z/w-buUF>` says, slow:
```
        movl    (%rax), %edx
        movl    (%rbx), %esi
        movl    %esi, 8(%rsp)
        movl    %edx, 12(%rsp)
        cmpl    %edx, %ecx
        setge   %sil
        movzbl  %sil, %esi
        movl    8(%rsp,%rsi,4), %esi
        movl    %esi, (%rbx)
        setl    %sil
        movzbl  %sil, %esi
        movl    8(%rsp,%rsi,4), %esi
        movl    %esi, (%rax)
```
and 2x as fast:
```
        movl    (%rax), %ecx
        cmpl    %ecx, %r8d
        setge   %dl
        movzbl  %dl, %edx
        movl    (%rbx), %esi
        movl    %esi, 8(%rsp)
        movl    %ecx, 12(%rsp)
        movl    %r9d, %esi
        subl    %edx, %esi
        movslq  %esi, %rsi
        movl    8(%rsp,%rsi,4), %esi
        movl    %esi, (%rax)
        movslq  %edx, %rdx
        movl    8(%rsp,%rdx,4), %edx
        movl    %edx, (%rbx)
        cmpl    %ecx, %r8d
```
Clang 9.0.0, -DFAST, for reference:
```
        movl    (%rcx), %r11d
        xorl    %edx, %edx
        xorl    %esi, %esi
        cmpl    %r8d, %r11d
        setle   %dl
        setg    %sil
        movl    (%rbx), %eax
        movl    %eax, (%rsp)
        movl    %r11d, 4(%rsp)
        movl    (%rsp,%rsi,4), %eax
        movl    %eax, (%rcx)
        movl    (%rsp,%rdx,4), %eax
        movl    %eax, (%rbx)
```


---


### compiler : `gcc`
### title : `Missed optimization with endian and alignment independent memory access`
### open_at : `2020-03-06T13:04:26Z`
### last_modified_date : `2023-09-21T17:11:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94071
### status : `NEW`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
When copying consecutive bytes from memory into a local variable, this is done byte wise, even on platforms that support unaligned access.

Code Example (Including more examples): https://godbolt.org/z/kFh6nc
Related: Bug 54733 (Same issue, but with a local variable)

Code

    #include <stdint.h>
    uint8_t data[1024];

    uint16_t getU16(int addr) {
        return 
          (uint16_t) data[addr    ] 
        | (uint16_t) data[addr + 1] << 8;
    }

Expected: (e.g. LLVM)
        movsxd  rax, edi
        movzx   eax, word ptr [rax + data]
        ret

Actual: (g++ (Compiler-Explorer-Build) 10.0.1 20200305 (experimental))
        lea     eax, [rdi+1]
        movsx   rdi, edi
        cdqe
        movzx   eax, BYTE PTR data[rax]
        sal     eax, 8
        mov     edx, eax
        movzx   eax, BYTE PTR data[rdi]
        or      eax, edx
        ret


I found this on ARM, where this probably hurts more than on x86.


---


### compiler : `gcc`
### title : `__builtin_memcpy in constexpr context should compile`
### open_at : `2020-03-07T00:03:56Z`
### last_modified_date : `2021-08-28T21:57:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94082
### status : `UNCONFIRMED`
### tags : `missed-optimization, rejects-valid`
### component : `c++`
### version : `unknown`
### severity : `normal`
### contents :
test


---


### compiler : `gcc`
### title : `inefficient soft-float x!=Inf code`
### open_at : `2020-03-07T03:04:01Z`
### last_modified_date : `2021-08-16T21:40:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94083
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
The original bug report was apparently lost in the sourceware/gcc migration back in the spring and I didn't notice until now.

This testcase

int foo(void) {
  volatile float f, g;
  int            n;
  f = __builtin_huge_valf();
  g = __builtin_huge_valf();
  n += 1 - (f != __builtin_huge_valf());
  return n;
}

compiled for soft-float with -O2, and looking at the original tree dump I see

  f =  Inf;
  g =  Inf;
  SAVE_EXPR <!(f u<= 3.4028234663852885981170418348451692544e+38)>;, n = SAVE_EX
PR <!(f u<= 3.4028234663852885981170418348451692544e+38)> + n;;

So the C front end converted the f != Inf compare to a f u<= <max-representable-float> compare, but the problem here is that the != operation is a single libcall, but u<= is two libcalls.  So code that should have a single soft-float libcall ends up with two.  First a call to __unordsf2, then a compare and branch, and then a call to __lesf2.  This is a de-optimization.

Perhaps we can convert the f u<= <max-representable-float> back to f != Inf in the optimization to get a single libcall.  Or maybe we can add unordered soft-float libcalls like ulesf2.


---


### compiler : `gcc`
### title : `Missed optimization when converting a bitfield to an integer on x86-64`
### open_at : `2020-03-07T19:33:43Z`
### last_modified_date : `2023-07-19T04:08:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94086
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `9.2.0`
### severity : `enhancement`
### contents :
When converting a structure containing bitfields to an unsigned 16-bit integer, inefficient code is generated.

Minimal testcase:

    typedef struct half
    {
        unsigned short mantissa:10;
        unsigned short exponent:5;
        unsigned short sign:1;
    } half;
    
    unsigned short from_half(half h)
    {
        return h.mantissa | h.exponent << 10 | h.sign << 15;
    }

To compile and place output on `stdout`:

    gcc -O3 -S test.c -o -

Assembly output:

	movl	%edi, %edx
	movl	%edi, %eax
	andw	$1023, %di
	shrw	$15, %dx
	andl	$31744, %eax
	movzbl	%dl, %edx
	sall	$15, %edx
	orl	%edx, %eax
	orl	%edi, %eax
	ret

This could be optimized into:

	movl	%edi, %eax
	retq

This behavior does not occur when `unsigned short` is being converted into `half` in a similar manner.

`gcc -v`:

    Using built-in specs.
    COLLECT_GCC=gcc
    COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-pc-linux-gnu/9.2.0/lto-wrapper
    Target: x86_64-pc-linux-gnu
    Configured with: /var/tmp/portage/sys-devel/gcc-9.2.0-r3/work/gcc-9.2.0/configure --host=x86_64-pc-linux-gnu --build=x86_64-pc-linux-gnu --prefix=/usr --bindir=/usr/x86_64-pc-linux-gnu/gcc-bin/9.2.0 --includedir=/usr/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include --datadir=/usr/share/gcc-data/x86_64-pc-linux-gnu/9.2.0 --mandir=/usr/share/gcc-data/x86_64-pc-linux-gnu/9.2.0/man --infodir=/usr/share/gcc-data/x86_64-pc-linux-gnu/9.2.0/info --with-gxx-include-dir=/usr/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/g++-v9 --with-python-dir=/share/gcc-data/x86_64-pc-linux-gnu/9.2.0/python --enable-objc-gc --enable-languages=c,c++,d,go,objc,obj-c++,fortran --enable-obsolete --enable-secureplt --disable-werror --with-system-zlib --enable-nls --without-included-gettext --enable-checking=release --with-bugurl=https://bugs.gentoo.org/ --with-pkgversion='Gentoo 9.2.0-r3 p4' --disable-esp --enable-libstdcxx-time --with-build-config=bootstrap-lto --enable-shared --enable-threads=posix --enable-__cxa_atexit --enable-clocale=gnu --enable-multilib --with-multilib-list=m32,m64 --disable-altivec --disable-fixed-point --enable-targets=all --enable-libgomp --disable-libmudflap --disable-libssp --disable-systemtap --enable-vtable-verify --enable-lto --with-isl --disable-isl-version-check --enable-default-pie --enable-default-ssp
    Thread model: posix
    gcc version 9.2.0 (Gentoo 9.2.0-r3 p4)

`-Wall -Wextra` reports nothing.


---


### compiler : `gcc`
### title : `Code size and performance degradations after -ftree-loop-distribute-patterns was enabled at -O[2s]+`
### open_at : `2020-03-09T06:06:25Z`
### last_modified_date : `2022-08-05T22:45:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=94092
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Since https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88440, loop-distribute-patterns was enabled at -O[2s]+. We understand that this patch is helpful in some cases, but not for all.
In the original bugzilla, the loop count is known, and the performance and code size are all improved finally. However, if loop count is unknown, the results may become worse.

void init_array(int N, int *array) {
  int i;
  for (i = 0; i < N; i++)
    array[i] = 0;
}

init_array:
        mv      a2,a0
        mv      a0,a1
        ble     a2,zero,.L1
        slli    a2,a2,2
        li      a1,0
        tail    memset
.L1:
        ret

For coremark, this is not only harmful to performance, but also code size.
Therefore, I suggest that we have to discuss how to resolve the case of unknown loop count.


---
