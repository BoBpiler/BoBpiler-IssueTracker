### Total Bugs Detected: 4649
### Current Chunk: 19 of 30
### Bugs in this Chunk: 160 (From bug 2881 to 3040)
---


### compiler : `gcc`
### title : `suboptimal code generated for condition expression`
### open_at : `2020-08-12T09:53:40Z`
### last_modified_date : `2023-05-05T07:09:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96586
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
For the following case, we can easy known the while loop will execute once, but with newest gcc 10.2, it still generated suboptimal code with condition expression.

void Proc_7 (int Int_Par_Ref);
void Proc_2 (int *Int_Par_Ref);

int main ()
{
    int       Int_1_Loc;
    int       Int_2_Loc;
    int       Int_3_Loc;

  /* Initializations */
    Int_1_Loc = 2;
    Int_2_Loc = 3;

    while (Int_1_Loc < Int_2_Loc)
    {
      Proc_7 (0);

      Int_1_Loc += 1;
    } /* while */
 
    Int_1_Loc = 1;
    Proc_2 (&Int_1_Loc);
 
  return 0;
}

============== the key assemble of the while loop ===========
.L2:
        .loc 1 18 7 view .LVU10
        .loc 1 20 7 view .LVU11
        .loc 1 20 14 is_stmt 0 view .LVU12
        mov     edi, 5
        call    Proc_7(int)
.LVL1:
        .loc 1 22 7 is_stmt 1 view .LVU13
        .loc 1 22 17 is_stmt 0 view .LVU14
        mov     eax, DWORD PTR [rsp+12]
        add     eax, 1
        mov     DWORD PTR [rsp+12], eax
        .loc 1 16 5 is_stmt 1 view .LVU15
        .loc 1 16 22 view .LVU16
        cmp     eax, 2
        jle     .L2


---


### compiler : `gcc`
### title : `Failure to optimize self-stpcpy to strlen`
### open_at : `2020-08-13T08:10:17Z`
### last_modified_date : `2023-09-03T12:05:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96599
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
char *f(char *a)
{
    return stpcpy(a, a);
}

This can be optimized to `strlen(a) + a`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize equality comparison involving strstr to strlen+strncmp`
### open_at : `2020-08-13T09:45:23Z`
### last_modified_date : `2023-09-03T11:58:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96601
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(const char *a, const char *b)
{
    return strstr(a, b) == a;
}

This can be optimized to `strncmp(a, b, strlen(b))`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize memcmp out for more than one byte`
### open_at : `2020-08-13T15:20:41Z`
### last_modified_date : `2023-09-03T11:53:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96603
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(const void *s1, const void *s2)
{
    return memcmp(s1, s2, 2);
}

This can be optimized to `loadbe16(s1) - loadbe16(s2)` (where loadbe16 is a function that read a 16-bit big endian number). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize out loop that eventually ends but has no side effects involving decrease of loop counter using an unsigned operation and the loop being done through recursivity`
### open_at : `2020-08-14T09:55:13Z`
### last_modified_date : `2023-09-03T11:53:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96615
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
void f(int bytes)
{
    bytes = (int)((unsigned int)bytes - 64);
    if (bytes > 0)
        f(bytes);
    return;
}

This can be optimized to doing nothing. LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `fold strlen relational expressions after nul stores`
### open_at : `2020-08-14T23:56:20Z`
### last_modified_date : `2020-08-14T23:56:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96621
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
strlen(s) <= N can be folded to true after nul has been stored into s[N].

$ cat t.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout t.c
void f0 (char *s)
{
  s[0] = 0;
  if (__builtin_strlen (s) > 0)   // folded to false
    __builtin_abort ();
}

void f1 (char *s)
{ 
  s[1] = 0;
  if (__builtin_strlen (s) > 1)   // not folded but can be
    __builtin_abort ();
}


;; Function f0 (f0, funcdef_no=0, decl_uid=1931, cgraph_uid=1, symbol_order=0)

f0 (char * s)
{
  <bb 2> [local count: 1073741824]:
  *s_2(D) = 0;
  return;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=1934, cgraph_uid=2, symbol_order=1)

f1 (char * s)
{
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  MEM[(char *)s_2(D) + 1B] = 0;
  _1 = __builtin_strlen (s_2(D));
  if (_1 > 1)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `Unnecessarily large assembly generated when a bit-offsetted higher-end end of a uint64_t-backed bitfield is shifted toward the high end (left) by its bit-offset`
### open_at : `2020-08-15T09:46:13Z`
### last_modified_date : `2020-08-25T10:19:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96625
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `enhancement`
### contents :
(Bitfields backed by 32-bit unsigneds are handled well.)

My example (https://gcc.godbolt.org/z/Yac38T):

#include <stdint.h>
#define FRONTSZ 3
#define UTYPE uint64_t
struct s{ union {
    UTYPE whole;
    struct {
        UTYPE front:FRONTSZ,
                      tail:8*sizeof(UTYPE)-FRONTSZ; };
};};

UTYPE hiShifted_tail(struct s X) { return X.tail<<FRONTSZ; }
//better codegen:
UTYPE hiShifted_tail2(struct s X) { return X.whole>>FRONTSZ<<FRONTSZ; }
UTYPE hiShifted_tail3(struct s X) { return X.whole & (0xffffffffffffffff<<FRONTSZ); }



x86-64 assembly generated for hiShifted_tail{,2,3}:


0000000000000000 <hiShifted_tail> (14 bytes):
   0:	48 b8 f8 ff ff ff ff ff ff 1f 	movabs rax,0x1ffffffffffffff8
   a:	48 21 f8             	and    rax,rdi
   d:	c3                   	ret    


0000000000000000 <hiShifted_tail{2,3}> (8 bytes):
   0:	48 89 f8             	mov    rax,rdi
   3:	48 83 e0 f8          	and    rax,0xfffffffffffffff8
   7:	c3                   	ret    

The codegen follows the same pattern for other front-sizes.
hiShifted_tail() on clang (regardless of whether uint64_t or uint32_t is used as the backing type) and on gcc with uint32_t rather than uin64_t used as the bitfield-backing-type follows the smaller codegen patter of hiShifted_tail{2,3}.


---


### compiler : `gcc`
### title : `missed-optimization with conditionally unsetting bits in memory.`
### open_at : `2020-08-16T20:36:42Z`
### last_modified_date : `2021-08-14T09:47:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96632
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
https://stackoverflow.com/questions/63432157/why-does-gcc-use-btq-in-conjunction-with-btcq-when-conditionally-set-a-bit-in-a

Basically:

void __attribute__((noinline))
cond_unset_bit(uint64_t * v, uint32_t b) {
    if(__builtin_expect(!!(*v & ((1UL) << b)), 1)) {
        *v ^= ((1UL) << b);
    }
}

Compiles to:

cond_unset_bit(unsigned long*, unsigned int):
        movq    (%rdi), %rax
        btq     %rsi, %rax
        jnc     .L6
        btcq    %rsi, %rax
        movq    %rax, (%rdi)
.L6:
        ret

The btq instruction is unnecessary.

cond_unset_bit(unsigned long*, unsigned int):
        movq    (%rdi), %rax
        btcq    %rsi, %rax
        jnc     .L6
        movq    %rax, (%rdi)
.L6:
        ret


Accomplishes the same thing without the btq instruction.


---


### compiler : `gcc`
### title : `missed optimization?`
### open_at : `2020-08-16T21:25:44Z`
### last_modified_date : `2023-10-25T19:02:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96633
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.0`
### severity : `enhancement`
### contents :
Matt Godbolt's https://queue.acm.org/detail.cfm?id=3372264
has an example of optimizing on amd64:

bool isWhitespace(char c)
 {
     return c == ' '
       || c == '\r'
       || c == '\n'
       || c == '\t';
 }

GCC generates: 
	xorl	%eax, %eax
	cmpb	$32, %dil
	ja	.L1
	movabsq	$4294977024, %rax
	movl	%edi, %ecx
	shrq	%cl, %rax
	andl	$1, %eax
.L1:
	ret

following an amazing comment on the ML, I've determined the following is abot 12% faster (and shorter too):

	movabsq $4294977024, %rax
	movl   %edi, %ecx
	shrq	%cl, %rax
	shr    $6, %ecx
	andl    $1, %eax
	shrq	%cl, %rax
	ret

We're dealing with chars in the range [-128,128), and x86's shift operator only considers the bottom 6 bits.


---


### compiler : `gcc`
### title : `Failure to optimize vectorized conversion to `int` with AVX`
### open_at : `2020-08-17T11:57:33Z`
### last_modified_date : `2023-09-03T11:53:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96654
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
void f(double *src, int *dst)
{
    for (int i = 0; i < 4; i ++)
        dst[i] = (int)src[i];
}

With -O3 -mavx, LLVM outputs this :

f(double*, int*):
  vcvttpd2dq xmm0, ymmword ptr [rdi]
  vmovupd xmmword ptr [rsi], xmm0
  ret

GCC outputs this :

f(double*, int*):
  push rbp
  vmovupd xmm1, XMMWORD PTR [rdi]
  vinsertf128 ymm0, ymm1, XMMWORD PTR [rdi+16], 0x1
  mov rbp, rsp
  vcvttpd2dq xmm0, ymm0
  vmovdqu XMMWORD PTR [rsi], xmm0
  vzeroupper
  pop rbp
  ret


---


### compiler : `gcc`
### title : `[11 regression] memcmp of a constant string not folded`
### open_at : `2020-08-17T22:45:34Z`
### last_modified_date : `2020-08-18T19:00:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96665
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
g:866626efd749ed3e2b7014e88e4340b5a4c73560, r11-2709


Executing on host: /home/seurer/gcc/git/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-test/gcc/ /home/seurer/gcc/git/gcc-test/gcc/testsuite/gcc.dg/strlenopt-55.c    -fdiagnostics-plain-output   -O1 -Wall -fdump-tree-gimple -fdump-tree-optimized -S -o strlenopt-55.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/git/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-test/gcc/ /home/seurer/gcc/git/gcc-test/gcc/testsuite/gcc.dg/strlenopt-55.c -fdiagnostics-plain-output -O1 -Wall -fdump-tree-gimple -fdump-tree-optimized -S -o strlenopt-55.s
/home/seurer/gcc/git/gcc-test/gcc/testsuite/gcc.dg/strlenopt-55.c:128:3: warning: unsigned conversion from 'int' to 'short unsigned int' changes value from '65536' to '0' [-Woverflow]
PASS: gcc.dg/strlenopt-55.c  (test for warnings, line 128)
Executing on host: /home/seurer/gcc/git/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-test/gcc/ exceptions_enabled90025.cc    -fdiagnostics-plain-output  -S -o exceptions_enabled90025.s    (timeout = 300)
spawn -ignore SIGHUP /home/seurer/gcc/git/build/gcc-test/gcc/xgcc -B/home/seurer/gcc/git/build/gcc-test/gcc/ exceptions_enabled90025.cc -fdiagnostics-plain-output -S -o exceptions_enabled90025.s
PASS: gcc.dg/strlenopt-55.c (test for excess errors)
PASS: gcc.dg/strlenopt-55.c scan-tree-dump-times gimple "strlen1" 0
gcc.dg/strlenopt-55.c: pattern found 3 times
FAIL: gcc.dg/strlenopt-55.c scan-tree-dump-times gimple "memcmp" 0
PASS: gcc.dg/strlenopt-55.c scan-tree-dump-times gimple "strcmp" 0
gcc.dg/strlenopt-55.c: pattern found 3 times
FAIL: gcc.dg/strlenopt-55.c scan-tree-dump-times optimized "call_in_true_branch_not_eliminated" 0
testcase /home/seurer/gcc/git/gcc-test/gcc/testsuite/gcc.dg/dg.exp completed in 0 seconds

		=== gcc Summary ===

# of expected passes		4
# of unexpected failures	2


---


### compiler : `gcc`
### title : `Failure to optimize shift by variable+and by 1 to test for 0`
### open_at : `2020-08-18T07:32:04Z`
### last_modified_date : `2023-09-03T11:52:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96669
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x)
{
    return (1 << x) & 1;
}

This can be converted to `return x == 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize a 3 xor+and pattern to xor+andnot`
### open_at : `2020-08-18T07:44:28Z`
### last_modified_date : `2023-09-03T18:59:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96671
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `Failure to optimize combination of comparisons to dec+compare`
### open_at : `2020-08-18T10:30:46Z`
### last_modified_date : `2023-09-03T18:59:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96674
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(unsigned a, unsigned b)
{
    return (b == 0) | (a < b);
}

This can be optimized to `return a <= (b - 1);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize or+and+or pattern to and+or`
### open_at : `2020-08-18T13:38:15Z`
### last_modified_date : `2023-09-03T18:56:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96679
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int a, int b, int c)
{
    return ((b | c) & a) | b;
}

This can be optimized to `return (a & c) | b;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize xor of comparisons with specific constants to comparison of xor-ed of compared variables`
### open_at : `2020-08-18T16:36:53Z`
### last_modified_date : `2023-09-03T18:55:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96681
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return (x < 0) ^ (y < 0);
}

This can be optimized to `return (x ^ y) < 0;`. LLVM does this transformation, but GCC does not. This transformation can also be done with `return (x > -1) ^ (y > -1);`. There is also a similar transformation with `return (x > -1) ^ (y < 0);`, which can be optimized to `return ~(x ^ y) < 0;`.


---


### compiler : `gcc`
### title : `Failure to optimize not+sub to add+not`
### open_at : `2020-08-18T17:15:34Z`
### last_modified_date : `2023-09-03T18:55:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96685
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x, int y)
{
    return ~(x - y);
}

This can be optimized to `~x + y`. While this isn't necessarily faster on most platforms, it is at least equivalent and on x86 the addition can be optimized to `lea` whereas the subtraction can't. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize shift-right/add+bitwise not of constant to avoid bitwise not`
### open_at : `2020-08-18T19:21:21Z`
### last_modified_date : `2023-09-03T18:55:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96688
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x)
{
    return ~(123 >> x);
}

This return in this code can be optimized to `return ~123 >> x;`, and the same optimization is possible where 123 is replaced by essentially any constant. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize bitwise not+or+xor with constants to and+xor with bitwise not-ed constants`
### open_at : `2020-08-18T21:16:40Z`
### last_modified_date : `2023-09-03T18:55:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96691
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x)
{
    return (~x | 123) ^ 321;
}

This can be optimized to `return (x & ~123) ^ ~321;` (and the same transformation can be applied with any constants replacing 123 and 321). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize xor+or+xor to andnot+xor`
### open_at : `2020-08-18T21:37:56Z`
### last_modified_date : `2023-09-03T18:55:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96692
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int a, int b, int c)
{
    return (a ^ b) ^ (a | c);
}

This can be optimized to `return (c & ~a) ^ b;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize min/max pattern using two binary nots to min/max pattern using one binary not`
### open_at : `2020-08-18T23:44:31Z`
### last_modified_date : `2023-09-05T20:59:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96694
### status : `RESOLVED`
### tags : `easyhack, missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x, int y)
{
    return ~((~x > y) ? ~x : y);
}

This can be optimized to `return ((~y <= x) ? ~y : x);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize combination of pointer comparison to nullptr and another pointer`
### open_at : `2020-08-18T23:53:02Z`
### last_modified_date : `2023-08-07T07:30:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96695
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(char *x, char *y)
{
    return (x == 0) && (x > y);
}

This can be optimized to `return false;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize mod+div to 0`
### open_at : `2020-08-19T00:47:10Z`
### last_modified_date : `2023-08-24T21:27:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96697
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x, int y)
{
    return (x % y) / y;
}

This can be optimized to `return 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize self right-shift to 0`
### open_at : `2020-08-19T09:59:07Z`
### last_modified_date : `2023-08-24T21:27:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96701
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x)
{
    return x >> x;
}

This can be optimized to `return 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize comparisons involving result of subtraction`
### open_at : `2020-08-19T10:07:25Z`
### last_modified_date : `2023-09-05T04:34:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96702
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int a, int b)
{
    int c = (a - b);
    return c >= a && c != 0;
}

This can be optimized to `return (b <= 0) && (a != b);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize combined comparison of variables and of variable with 0 to two comparisons with 0`
### open_at : `2020-08-19T10:15:55Z`
### last_modified_date : `2023-09-04T04:31:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96703
### status : `NEW`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int x, int y)
{
    return x > y && y == 0;
}

This can be optimized to `return (y == 0) && (x > 0);` (This transformation doesn't by itself make the code faster, but it probably helps with pipelined CPUs (avoids dependency on both variables for the first comparison) and looks like it would most likely make other optimizations easier). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize right shift+unsigned compare of two variables optimally`
### open_at : `2020-08-19T13:03:08Z`
### last_modified_date : `2023-08-24T21:26:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96707
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(unsigned x, unsigned y)
{
    return (x >> y) <= x;
}

This can be optimized to `return true;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize max pattern with comparison when using a temporary variable`
### open_at : `2020-08-19T13:26:38Z`
### last_modified_date : `2023-09-11T02:21:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96708
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int a, int b)
{
    int tmp = (a < b) ? b : a;
    return tmp >= a;
}

This can be optimized to `return true;`. This transformation is done by LLVM, but not by GCC.

PS: This is probably related to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95926.


---


### compiler : `gcc`
### title : `cmov and vectorization`
### open_at : `2020-08-19T14:46:46Z`
### last_modified_date : `2020-08-28T07:23:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96709
### status : `WAITING`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Hello gcc team,
I noticed 2 problems:
1) the compiler does not generate cmov commands
2) the auto-vectorization is very unreliable

I would like to clarify this using the example of a stable shift-left, see
https://godbolt.org/z/GKnj17
I have implemented several variants for this.

to 1)
Only silent::conditional_move generates a cmov, all other cases do not.

to 2)
- The auto-vectorization only works if the smaller of the two arrays (val and bit) is at least as large as an sse register, although the values ​​could be adjusted.
- If vectorization is used at all, often only 128-bit code is generated (_mm_XXX) instead of 256-bit (avx _mm256_XXX) or larger.
- The 16-bit shift commands from AVX512 (_mmXXX_sllv_epi16) are not used if a suitable architecture is selected.

The complex shl_attempt_vectorize function works a little better, but not 100% either. Play around with the array size, the value/shift-types and the functions!

Best regards
Gero


---


### compiler : `gcc`
### title : `Failure to optimize sqrt*sqrt of same variable with ffast-math`
### open_at : `2020-08-19T20:23:07Z`
### last_modified_date : `2023-08-24T21:26:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96714
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
float f(float x)
{
    return __builtin_sqrtf(x) * __builtin_sqrtf(x);
}

This can be optimized to `return x;` (with fast math enabled). This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize copysign of variable by negated variable`
### open_at : `2020-08-19T20:31:37Z`
### last_modified_date : `2023-08-24T21:26:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96715
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
float f(float x, float y)
{
    return __builtin_copysignf(x, -x);
}

This can be optimized to `return -x;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `std::clamp for floats and doubles produces worse code than a combo of std::min / std::max`
### open_at : `2020-08-21T10:04:58Z`
### last_modified_date : `2023-08-12T22:56:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96733
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.2.0`
### severity : `enhancement`
### contents :
C++/17 introduced std::clamp in the standard library.

Very useful, but the code gcc generates for it is not optimal for doubles and floats. Source code:

// This version is good in gcc, not so on vc++
double clamp_minmax( double x, double i, double ax )
{
    return std::min( std::max( x, i ), ax );
}

// This version should be identical but is not.
// gcc compiles to scalar comparisons
// vc++ code is outright horrible, compiles into RAM access.
double clamp_builtin( double x, double i, double ax )
{
    return std::clamp( x, i, ax );
} 

Demo there: https://godbolt.org/z/Gnvc1s


---


### compiler : `gcc`
### title : `GCC generates worse assembly than clang and It fails to vectorized code compared to clang`
### open_at : `2020-08-21T18:34:27Z`
### last_modified_date : `2020-08-25T11:36:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96738
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
https://godbolt.org/z/9K3369

#include<array>
#include<cstdint>

struct number
{
	std::array<std::uint64_t,5> num;


	inline constexpr std::uint64_t& operator[](std::size_t position) noexcept
	{
		return num[position];
	}
	inline constexpr std::uint64_t const& operator[](std::size_t position) const noexcept
	{
		return num[position];
	}
};


number add_reduce(number const& a,number const& b) noexcept
{
    constexpr auto reduce_mask_51{(static_cast<std::uint64_t>(1) << 51) - 1};
    number out;
	std::uint64_t c{(a[0] + b[0])>>51};
	out[1] = a[1] + b[1] + c; c = (out[1] >> 51); out[1] &= reduce_mask_51;
	out[2] = a[2] + b[2] + c; c = (out[2] >> 51); out[2] &= reduce_mask_51;
	out[3] = a[3] + b[3] + c; c = (out[3] >> 51); out[3] &= reduce_mask_51;
	out[4] = a[4] + b[4] + c; c = (out[4] >> 51); out[4] &= reduce_mask_51;
	out[0] = c * 19;
	return out;
}


gcc:

add_reduce(number const&, number const&):
        movq    (%rdx), %rax
        addq    (%rsi), %rax
        movq    %rdi, %r8
        movq    %rdx, %rdi
        shrq    $51, %rax
        movq    8(%rdx), %rdx
        addq    8(%rsi), %rdx
        movq    %rsi, %rcx
        movabsq $2251799813685247, %rsi
        addq    %rdx, %rax
        movq    %rax, %rdx
        shrq    $51, %rax
        andq    %rsi, %rdx
        movq    %rdx, 8(%r8)
        movq    16(%rdi), %rdx
        addq    16(%rcx), %rdx
        addq    %rdx, %rax
        movq    %rax, %rdx
        shrq    $51, %rax
        andq    %rsi, %rdx
        movq    %rdx, 16(%r8)
        movq    24(%rdi), %rdx
        addq    24(%rcx), %rdx
        addq    %rax, %rdx
        movq    %rdx, %rax
        shrq    $51, %rdx
        andq    %rsi, %rax
        movq    %rax, 24(%r8)
        movq    32(%rdi), %rax
        addq    32(%rcx), %rax
        addq    %rdx, %rax
        andq    %rax, %rsi
        shrq    $51, %rax
        leaq    (%rax,%rax,8), %rdx
        movq    %rsi, 32(%r8)
        leaq    (%rax,%rdx,2), %rax
        movq    %rax, (%r8)
        movq    %r8, %rax
        ret

clang:
add_reduce(number const&, number const&):             # @add_reduce(number const&, number const&)
        movq    %rdi, %rax
        movq    (%rdx), %rcx
        movq    8(%rdx), %rdi
        addq    (%rsi), %rcx
        shrq    $51, %rcx
        addq    8(%rsi), %rdi
        addq    %rcx, %rdi
        movq    %rdi, %rcx
        shrq    $51, %rcx
        movabsq $2251799813685247, %r8          # imm = 0x7FFFFFFFFFFFF
        andq    %r8, %rdi
        movq    %rdi, 8(%rax)
        movq    16(%rdx), %rdi
        addq    16(%rsi), %rdi
        addq    %rcx, %rdi
        movq    %rdi, %rcx
        shrq    $51, %rcx
        andq    %r8, %rdi
        movq    %rdi, 16(%rax)
        movq    24(%rdx), %rdi
        addq    24(%rsi), %rdi
        addq    %rcx, %rdi
        movq    %rdi, %rcx
        andq    %r8, %rdi
        movq    %rdi, 24(%rax)
        movq    32(%rdx), %rdx
        addq    32(%rsi), %rdx
        shrq    $51, %rcx
        addq    %rcx, %rdx
        movq    %rdx, %rcx
        shrq    $51, %rcx
        andq    %r8, %rdx
        movq    %rdx, 32(%rax)
        leaq    (%rcx,%rcx,8), %rdx
        leaq    (%rcx,%rdx,2), %rcx
        movq    %rcx, (%rax)
        retq

clang with -march=native

.LCPI0_0:
        .quad   2251799813685247
add_reduce(number const&, number const&):             # @add_reduce(number const&, number const&)
        movq    %rdi, %rax
        movq    (%rdx), %rcx
        movq    8(%rdx), %rdi
        addq    (%rsi), %rcx
        shrq    $51, %rcx
        addq    8(%rsi), %rdi
        addq    %rcx, %rdi
        vmovq   %rdi, %xmm0
        shrq    $51, %rdi
        movq    16(%rdx), %rcx
        addq    16(%rsi), %rcx
        addq    %rdi, %rcx
        vmovq   %rcx, %xmm1
        shrq    $51, %rcx
        movq    24(%rdx), %rdi
        addq    24(%rsi), %rdi
        addq    %rcx, %rdi
        vmovq   %rdi, %xmm2
        shrq    $51, %rdi
        movq    32(%rdx), %rcx
        addq    32(%rsi), %rcx
        addq    %rdi, %rcx
        vpunpcklqdq     %xmm1, %xmm0, %xmm0     # xmm0 = xmm0[0],xmm1[0]
        vmovq   %rcx, %xmm1
        vpunpcklqdq     %xmm1, %xmm2, %xmm1     # xmm1 = xmm2[0],xmm1[0]
        vinserti128     $1, %xmm1, %ymm0, %ymm0
        vpandq  .LCPI0_0(%rip){1to4}, %ymm0, %ymm0
        shrq    $51, %rcx
        vmovdqu %ymm0, 8(%rax)
        leaq    (%rcx,%rcx,8), %rdx
        leaq    (%rcx,%rdx,2), %rcx
        movq    %rcx, (%rax)
        vzeroupper
        retq


---


### compiler : `gcc`
### title : `frexp, modf, and remquo missing attribute nonnull`
### open_at : `2020-08-21T20:36:40Z`
### last_modified_date : `2020-08-25T11:38:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96740
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `c`
### version : `11.0`
### severity : `normal`
### contents :
The math built-in functions frexp, modf, and remquo take pointers to objects that they are specified to unconditionally store a component of the result in.  Their internal declarations should make use of attribute nonnull to trigger -Wnonnull warnings when they're passed a null pointer.

$ cat z.c && gcc -O2 -S -Wall z.c
double f0 (double x)
{
  return __builtin_frexp (x, (int*)0);       // missing warning
}

double f1 (double x)
{ 
  return __builtin_modf (x, (double*)0);     // missing warning
}

double f2 (void)
{ 
  return __builtin_nan ((char*)0);           // -Wnonnull (good)
}

double f3 (double x, double y)
{ 
  return __builtin_remquo (x, y, (int*)0);   // missing warning
}

z.c: In function ‘f2’:
z.c:13:10: warning: argument 1 null where non-null expected [-Wnonnull]
   13 |   return __builtin_nan ((char*)0);           // -Wnonnull (good)
      |          ^~~~~~~~~~~~~
<built-in>: note: in a call to function ‘__builtin_nan’ declared ‘nonnull’


---


### compiler : `gcc`
### title : `Missed optimization on modulo when left side value is known to be greater than right side value`
### open_at : `2020-08-23T22:00:36Z`
### last_modified_date : `2020-08-24T06:27:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96753
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
uint64_t
do_mod(uint64_t x, uint64_t y) {
    if(x >= y) {
        __builtin_unreachable();
    }
    return x % y;
}

With -O3 compiles to

do_mod(unsigned long, unsigned long):
        movq    %rdi, %rax
        xorl    %edx, %edx
        divq    %rsi
        movq    %rdx, %rax
        ret

given that x is known to be less than y this could be return x;

Godbolt link: https://gcc.godbolt.org/z/sfhr96


---


### compiler : `gcc`
### title : `Failure to optimize strcpy+strlen to memcpy when strlen is done after strcpy`
### open_at : `2020-08-23T22:54:48Z`
### last_modified_date : `2023-08-24T21:26:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96754
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
size_t f1(char *a, const char *b)
{
    strcpy(a, b);
    return strlen(b);
}

size_t f2(char *a, const char *b)
{
    size_t sz = strlen(b);
    memcpy(a, b, sz + 1);
    return sz;
}

f1 can be optimized to f2. Using this intermediary form :

size_t f(char *a, const char *b)
{
    size_t tmp = strlen(b);
    strcpy(a, b);
    return tmp;
}

Results in the optimization being done properly, but it would be nice if it was doable even if the `strlen` wasn't done prior to the `strcpy`.


---


### compiler : `gcc`
### title : `Failure to optimize comparison of negative version of self`
### open_at : `2020-08-25T12:03:58Z`
### last_modified_date : `2023-08-24T21:26:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96779
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(int a)
{
    return -a == a;
}

This can be optimized to `return !a;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize comparison with bitwise not-ed with self`
### open_at : `2020-08-25T12:12:57Z`
### last_modified_date : `2023-08-24T21:26:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96782
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(int a)
{
    return a == ~a;
}

This can be optimized to `return false;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `x264: sub4x4_dct() improves when vectorization is disabled`
### open_at : `2020-08-26T03:19:26Z`
### last_modified_date : `2020-11-05T02:25:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96789
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
One of my workmates found that if we disable vectorization for SPEC2017 525.x264_r function sub4x4_dct in source file x264_src/common/dct.c with explicit function attribute __attribute__((optimize("no-tree-vectorize"))), it can speed up by 4%.

The option used is: -O3 -mcpu=power9 -fcommon -fno-strict-aliasing -fgnu89-inline

I confirmed this finding and it can further narrow down to SLP vectorization with attribute __attribute__((optimize("no-tree-slp-vectorize"))).

I also checked with r11-0 commit for this particular file, the performance keep unchanged, with/without vectorization attribute. So I think it's a trunk regression, probably exposes one SLP flaw or one cost modeling issue.


---


### compiler : `gcc`
### title : `[x86] Prefer xor/test/setcc over test/setcc/movzx sequence`
### open_at : `2020-08-29T17:47:59Z`
### last_modified_date : `2020-08-31T06:35:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96846
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
This has been reported already in bug 86352, but that bug also describes a few other issues, so I decoded to create a separate bug focused on this one particular issue.

When performing boolean operations, gcc often prefers the following pattern:

1. Test instruction (test/cmp).
2. "setcc %al" (where al can be any 8-bit register).
3. If the bool needs to be further processed, "movzx %al, %eax" (where al and eax are 8 and 32-bit versions of the register picked in #2).

The example code can be seen here:

https://gcc.godbolt.org/z/z3WGbq

For convenience I'm posting the code below:

bool narrow_boolean(int a) { return a!=5; }

unsigned int countmatch(unsigned int arr[], unsigned int key)
{
    unsigned count = 0;
    for (int i=0; i<1024 ; i++) {
        count += ((arr[i] & key) != 5);
    }
    return count;
}

narrow_boolean(int):
        cmp     edi, 5
        setne   al
        ret

countmatch(unsigned int*, unsigned int):
        lea     rcx, [rdi+4096]
        xor     eax, eax
.L4:
        mov     edx, DWORD PTR [rdi]
        and     edx, esi
        cmp     edx, 5
        setne   dl
        movzx   edx, dl
        add     rdi, 4
        add     eax, edx
        cmp     rcx, rdi
        jne     .L4
        ret

Command line parameters: -Wall -O3 -mtune=skylake -fno-tree-vectorize

The problem with the generated code is as follows:

- The setcc instruction only modifies the lower 8 bits of the full architectural register. The upper bits remain unmodified and potentially "dirty" meaning that the following instructions taking this register as input may require merging the full register value, with a performance penalty.
- Since setcc preserves upper bits, the following instructions consuming the output register become dependent on the previous instructions that write that register. This results in an unnecessary dependency. This is especially a problem in the case of narrow_boolean above.
- On Haswell and later, "movzx %al, %eax" is not eliminated at register rename and consumes ALU and has non-zero latency. "movzx %al, %ebx" (i.e. when the output register is different from input) is eliminated at rename stage, but requires an additional register. But gcc seems to prefer the former form, unless -frename-registers is specified.

See this excellent StackOverflow question and answers for details:

https://stackoverflow.com/questions/45660139/how-exactly-do-partial-registers-on-haswell-skylake-perform-writing-al-seems-to

(BTW, the code in this bug originated from that question.)

A better instruction pattern would be this:

1. Zero the target flag register beforehand with "xor %eax, %eax".
2. Perform the test.
3. "setcc %al" to set the flag.

In case if this pattern is in a loop body, the initial xor can be hoisted out of the loop. The important part is that the xor eliminates the dependency and doesn't cost ALU uop. Arguably, it is also smaller code since xor is only 2 bytes vs. 3 bytes for movxz.

The initial zeroing requires an additional register, meaning that it cannot reuse the register involved in the test in #2. However, that is often not a problem, like in the examples above. I guess, the compiler could estimate if there are spare registers and use xor/test/setcc sequence if there are and test/setcc/movzx if not.


---


### compiler : `gcc`
### title : `Code size increase +42% depending on memory size allocated on stack for ARM Cortex-M3`
### open_at : `2020-08-30T00:18:16Z`
### last_modified_date : `2020-08-31T13:42:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96847
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 49156
Example showing +42% increase depending on stack mem array sizes

When comping with GCC-10.x.0 I get a code size increase depending on the size of memory for arrays on stack.

On older GCC-9.x.0 does not get this size increase.

On a slightly constructed test-case from CSiBE bzip2 I get more than +42% size increase.

Target: arm-none-eabi Cortex-M3

See example attached, if I chose a 2 bytes less size for stack mem array I get a totally different result? How can stack memory arrays sizes make this difference, and why is this new with GCC-10.x?


---


### compiler : `gcc`
### title : `Integer min/max optimization failed under -march=skylake-avx512`
### open_at : `2020-08-31T09:33:45Z`
### last_modified_date : `2020-09-21T11:36:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96861
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
cat minmax-10.c

---
#define max(a,b) (((a) > (b))) ? (a) : (b)
#define min(a,b) (((a) < (b))) ? (a) : (b)

int smax1(int x)
{
    return max(x,1);
}
---

gcc -O2 -march=skylake-avx512 -S

---
smax1(int):
        vmovdqa xmm1, XMMWORD PTR .LC0[rip]
        vmovd   xmm0, edi
        vpmaxsd xmm0, xmm0, xmm1
        vmovd   eax, xmm0
        ret
---

gcc -O2 -march=x86-64 -S
---
smax1(int):
        test    edi, edi
        mov     eax, 1
        cmovg   eax, edi
        ret
---

It seems cost model need to be adjusted for skylake_cost.


---


### compiler : `gcc`
### title : `loop not vectorized due AVX512 condition handling`
### open_at : `2020-08-31T10:15:21Z`
### last_modified_date : `2022-02-15T06:46:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96864
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c

---
double d1[1024], d2[1024];
char c1[1024];
int test4(int n)
{
    int i;
    for (int i = 0; i < 1024; i++)
      c1[i] = d1[i] > d2[i] ? c1[i] : 1;
}
---

loop is not vectorized with under -mtune=skylake-avx512 but success with -mtune=generic

Refer to https://godbolt.org/z/xPGhTh

output of -fopt-info-all

---
Unit growth for small function inlining: 16->16 (0%)

Inlined 0 calls, eliminated 0 functions

test.c:6:5: missed: couldn't vectorize loop
test.c:7:17: missed: not vectorized: no vectype for stmt: _1 = d1[i_13];
 scalar_type: double
test.c:6:5: note: ***** Analysis failed with vector mode V4DF
test.c:6:5: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DF
test.c:3:5: note: vectorized 0 loops in function.
test.c:3:5: note: ***** Analysis failed with vector mode VOID
test.c:7:13: note: ***** Analysis failed with vector mode V4DF
test.c:7:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DF
test.c:7:13: note: ***** Analysis failed with vector mode V32QI
test.c:7:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V32QI
test.c:6:5: note: ***** Analysis failed with vector mode V32QI
test.c:6:5: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V32QI
test.c:6:5: note: ***** Analysis failed with vector mode VOID
test.c:8:1: note: ***** Analysis failed with vector mode VOID
---

Seems problem of cost model.


---


### compiler : `gcc`
### title : `Missing vectorization opportunity depending on integer type`
### open_at : `2020-09-02T00:39:04Z`
### last_modified_date : `2020-09-02T08:11:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96888
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `normal`
### contents :
The loop in the following test case isn't vectorized:

#include <cstdlib>
#include <cstdint>

// Add x to each v[i] if bit 'i' is set in LSB-encoded bits.
void Test(int8_t *__restrict v, int8_t x, const uint64_t *bits, unsigned n) {
    for (int i = 0, num_words=(n+64-1)/64; i , n; i++) {
        const uint64_t word = bits[i];
        for (int j = 0; j < 64; j++) {
            v[i*64+j] += x * (bool)(word & (uint64_t(1)<<j));
        }
    }
}

<source>:7:9: missed: couldn't vectorize loop
<source>:7:9: missed: not vectorized: control flow in loop.
<source>:8:27: missed: couldn't vectorize loop
<source>:9:30: missed: not vectorized: relevant stmt not supported: _10 = word_24 >> j_34;

However, changing one line (the one constructing the mask) from an explicit uint64_t(1) to a plan 1U (which is not correct), we get auto-vectorization:

#include <cstdlib>
#include <cstdint>

// Add x to each v[i] if bit 'i' is set in LSB-encoded bits.
void Test(int8_t *__restrict v, int8_t x, const uint64_t *bits, unsigned n) {
    for (int i = 0, num_words=(n+64-1)/64; i , n; i++) {
        const uint64_t word = bits[i];
        for (int j = 0; j < 64; j++) {
            v[i*64+j] += x * (bool)(word & (1<<j)); // CHANGE HERE
        }
    }
}

Is this a known issue? Is there a reason why the former code can't be vectorized, or do I need to restructure the code to trip the compiler?


---


### compiler : `gcc`
### title : `[AVX512] For vector compare to dest vector, avx512 vector compare could be lowered to avx version`
### open_at : `2020-09-02T03:10:42Z`
### last_modified_date : `2021-01-22T04:37:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96891
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
cat test.cpp

---
typedef float v8sf __attribute__ ((vector_size (32)));
v8sf test4(v8sf a, v8sf b)
{
    return a >= b;
}
---

gcc -O2 -mavx512f -mavx512vl -S 

---
test4:
        vcmpps  k1, ymm1, ymm0, 2
        vpternlogd      ymm0{k1}{z}, ymm0, ymm0, 0x81
        ret
---

gcc -O2 -mavx

---
test4:
        vcmpleps        ymm0, ymm1, ymm0
        ret
---

Add peephole 2 handle this?


---


### compiler : `gcc`
### title : `Failure to optimize sub+not involving constant to add`
### open_at : `2020-09-02T11:17:54Z`
### last_modified_date : `2023-08-24T21:26:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96897
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int x)
{
    return ~(~x & (~x - 1));
}

This can be optimized to `return (x + 1) | x;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] bogus -Warray-bounds on strlen with valid pointer obtained from just-past-the-end`
### open_at : `2020-09-02T16:50:41Z`
### last_modified_date : `2023-07-07T10:38:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96900
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
When a valid pointer into an array that has been derived from a past-the-end pointer to a member array of an initialized constant struct is used in a call to a string built-in like strlen GCC issues a bogus -Warray-bounds warning indicating that the offset into the array is out of its bounds.

$ cat q.c && gcc -S -Wall q.c
struct S { char n, a[3]; };

const char a[3] = { 2, 1, 0 };
const struct S s = { 3, { 2, 1, 0 } };

int f (void)
{
  const char *p = &a[sizeof a];
  return __builtin_strlen (p - sizeof a);      // no warning (good)
}

int g (void)
{
  const char *p = &s.a[sizeof s.a];
  return __builtin_strlen (p - sizeof s.a);    // bogus -Warray-bounds
}

q.c: In function ‘g’:
q.c:15:10: warning: offset ‘1’ outside bounds of constant string [-Warray-bounds]
   15 |   return __builtin_strlen (p - sizeof s.a);    // bogus -Warray-bounds
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
q.c:4:16: note: ‘s’ declared here
    4 | const struct S s = { 3, { 2, 1, 0 } };
      |                ^


---


### compiler : `gcc`
### title : `Failure to optimize __builtin_ia32_psubusw128 compared to 0 to __builtin_ia32_pminuw128 compared to operand`
### open_at : `2020-09-02T20:22:35Z`
### last_modified_date : `2023-08-24T21:26:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96906
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef int16_t v8i16 __attribute__((vector_size(16)));

v8i16 cmple_epu16(v8i16 x, v8i16 y)
{
	return __builtin_ia32_psubusw128(x, y) == 0;
}

With -msse4.1, this can be optimized to `return __builtin_ia32_pminuw128(x, y) == x;`. This transformation is done by LLVM, but not by GCC. 

PS: I'm not 100% sure this is faster but it logically should be, since the `pminuw` version doesn't have to handle zeroing an SSE register.


---


### compiler : `gcc`
### title : `Failure to optimize pblendvb pattern`
### open_at : `2020-09-03T07:31:30Z`
### last_modified_date : `2023-08-24T21:26:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96912
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef char v16i8 __attribute__((vector_size(16)));
typedef int64_t v2i64 __attribute__((vector_size(16)));

v2i64 blend_epi8(v2i64 x, v2i64 y, v16i8 mask)
{
    v2i64 tmp = (mask < 0);
    return (~tmp & x) | (tmp & y);
}

This can be optimized to `return (v2i64)__builtin_ia32_pblendvb128((v16i8)x, (v16i8)y, mask);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize vector shift left+shift right+or to pshuf`
### open_at : `2020-09-03T12:58:48Z`
### last_modified_date : `2023-08-24T21:25:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96918
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef __INT16_TYPE__ v8i16 __attribute__((vector_size(16)));
typedef char v16i8 __attribute__((vector_size(16)));

v8i16 bswap_epi16(v8i16 x)
{
	return __builtin_ia32_psllwi128(x, 8) | __builtin_ia32_psrlwi128(x, 8);
}

This can be optimized to `return (v8i16)__builtin_ia32_pshufb128((v16i8)x, v16i8{1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14});`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize combined boolean not patterns`
### open_at : `2020-09-03T13:40:15Z`
### last_modified_date : `2023-08-24T21:25:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96921
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
_Bool f(_Bool a, _Bool b)
{
    return 1 - ((1 - a) & (1 - b));
}

This can be optimized to `return a | b;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize a select-related bool pattern to or+not`
### open_at : `2020-09-03T14:06:08Z`
### last_modified_date : `2023-08-02T14:01:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96923
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
_Bool f2(_Bool a, _Bool b)
{
    return !a ? !b : 0;
}

This can be optimized to `return !(a | b);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize one's complement abs pattern`
### open_at : `2020-09-03T18:25:54Z`
### last_modified_date : `2021-05-16T22:10:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96928
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int a)
{
    return (a < 0) ? ~a : a;
}

This can be optimized to `return (a >> 31) ^ a;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize right shift of -1 to -1`
### open_at : `2020-09-03T20:44:58Z`
### last_modified_date : `2023-08-21T23:20:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96929
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int b)
{
    return -1 >> b;
}

This can be optimized to `return -1;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize out arithmetic with bigger size when it can't matter with division transformed into right shift`
### open_at : `2020-09-03T20:47:58Z`
### last_modified_date : `2023-09-21T11:13:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96930
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
unsigned f(unsigned a, unsigned b)
{
    return a / (unsigned long long)(1U << b);
}

This can be optimized to `return a >> b;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize bit-setting pattern when not using temporary`
### open_at : `2020-09-04T14:10:16Z`
### last_modified_date : `2023-09-21T11:05:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96938
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
void g(char *f, int offset, char value)
{
	*f = (int)(*f & ~(1 << (offset & 0x1F))) | (value << (offset & 0x1F));
}

This has much worse code generation than this:

void g(char *f, int offset, char value)
{
	int tmp = *f & ~(1 << (offset & 0x1F));
	*f = tmp | (value << (offset & 0x1F));
}

Which should be equivalent to the first example.

Example of the worse code generation, on x86 the first example compiles to this:

g(char*, int, char):
  movzx ecx, BYTE PTR [rdi]
  mov eax, 1
  movsx edx, dl
  shlx eax, eax, esi
  shlx edx, edx, esi
  andn eax, eax, ecx
  or eax, edx
  mov BYTE PTR [rdi], al
  ret

Whereas the second example compiles to this:

g(char*, int, char):
  movsx eax, BYTE PTR [rdi]
  movsx edx, dl
  shlx edx, edx, esi
  btr eax, esi
  or eax, edx
  mov BYTE PTR [rdi], al
  ret


---


### compiler : `gcc`
### title : `unused std::vector is not always optimized away`
### open_at : `2020-09-05T15:29:35Z`
### last_modified_date : `2023-07-26T16:42:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96945
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
While toying with a piece of code, I've noticed that the code did not get optimized as expected.

All snippets where compiled with -O3.

A)
----
#include <vector>
struct c {
};
void foo(){
    std::vector<c> vi = {c(),c(),c()};
}
----

gets compiled to: https://godbolt.org/z/s7YaEf

----
foo():
        sub     rsp, 24
        mov     edi, 3
        call    operator new(unsigned long)
        mov     esi, 3
        mov     rdi, rax
        movzx   eax, WORD PTR [rsp+13]
        mov     WORD PTR [rdi], ax
        movzx   eax, BYTE PTR [rsp+15]
        mov     BYTE PTR [rdi+2], al
        add     rsp, 24
        jmp     operator delete(void*, unsigned long)
----

Adding and defaulting the constructors produces even more optimized code (the whole vector is optimized out(!): https://godbolt.org/z/E4GT9x

B)
----
#include <vector>
struct c {
    c() = default;
    c(const c&) =default;
    c(c&&) = default;
};
void foo(){
    std::vector<c> vi = {c(),c(),c()};
}
----

----
foo():
        ret
----


Adding and defaulting the constructors, except the move constructor produces the same code as A): https://godbolt.org/z/ch71fb

B)
----
#include <vector>
struct c {
    c() = default;
    c(const c&) =default;
    c(c&&) = default;
};
void foo(){
    std::vector<c> vi = {c(),c(),c()};
}
----


If the copy or default constructor is implemented and not defaulted, then the code is optimized as B): https://godbolt.org/z/v8E37b, https://godbolt.org/z/v3EY69, #include <vector>
struct c {
    c() {};
};
void foo(){
    std::vector<c> vi = {c(),c(),c()};
}

C)
----
#include <vector>
struct c {
    c() = default;
    c(const c&) {};
};
void foo(){
    std::vector<c> vi = {c(),c(),c()};
}
----

D)
----
#include <vector>
struct c {
    c() = default;
    c(const c&) {};
    c(c&&) = default;
};
void foo(){
    std::vector<c> vi = {c(),c(),c()};
}
----

E)

----
#include <vector>
struct c {
    c() {}
};
void foo(){
    std::vector<c> vi = {c(),c(),c()};
}
----



While ideally the code for those cases is equivalent (as c has no state and all snippets are functionally equivalent), I would have expected the class with compiler-defined operators have the best codegen, followed by the class with defaulted operators, and last the class with a non-defaulted implementation.

Strangely all constructor calls of `c` are always optimized away, but depending how the class is defined g++ does or does not optimize the whole vector away.


---


### compiler : `gcc`
### title : `Long Double in Hash Table policy forces soft-float calculations`
### open_at : `2020-09-07T16:12:07Z`
### last_modified_date : `2020-10-31T13:00:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96958
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `11.0`
### severity : `normal`
### contents :
It was pointed out that some forks of GCC ( https://github.com/FEX-Emu/gcc/commit/8a2b7389f50a50a4e26ec98101d47fb1fc1c1bcd ) reduce the hashtable policy implementation from a long double to a double. Doing this reduces it from a soft-float calculation to hardware floating-point.

Reading the discussion on libstdc++ from when this code was introduced the intention was to provide massive amounts of forwards compatibility for Very Big hash tables. We're taking quite an efficiency hit for that future proofing.


---


### compiler : `gcc`
### title : `combine RMW and flags`
### open_at : `2020-09-07T22:09:16Z`
### last_modified_date : `2021-05-04T12:31:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96965
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Consider:

typedef unsigned char T;
T i[2];
int f() {
  T *p = &i[0], *q = &i[1];
  T c = __builtin_add_overflow(*p, 1, p);
  *q += c;
}

The desired code sequence on x86_64 is:

  addb $1, i(%rip)
  adcb $0, i+1(%rip)

What we get instead of the desired addb are separate load, addb, and store instructions.  There are two reasons why we don't combine them to form the addb:

- when we try_combine the 3 of them, the flag-store insn is still present, between M (add) and W (store), thus can_combine_p fails.  after we combine the flag-store into adcb, we do not retry

- if I manually force the retry, we break up the M parallel insn into a naked add in i2, and a flag-setting non-canonical compare in i0.  we substitute R and M into W, for an add without flag-setting.  finally, we deal with added_sets, building a new parallel to hold the RMW add and appending the flag-setter as the second item, after the combined add.  alas, recog won't match them in this order.  *add<mode>3_cc_overflow_1 requires the flag-setter before the reg-setter.

Here's discussion and combine dumbs from a slightly different testcase that triggers the same combine behavior:

https://gcc.gnu.org/pipermail/gcc-patches/2020-September/553242.html


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] redundant memcpy not eliminated after pointer subtraction`
### open_at : `2020-09-07T23:40:23Z`
### last_modified_date : `2023-07-07T10:38:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96966
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
The second call to memcpy and mempcpy, respectively, is redundant in each of the functions below and can be eliminated.  GCC doesn't notice the redundancy and emits both pairs of copies.  Clang emits just one call to memcpy but it too fails to eliminate the redundant call to mempcpy.

$ cat z.c && gcc -O2 -S -o /dev/stdout z.c
extern char a[32];

void f (const void *s)
{
  char *p = (char*)__builtin_memcpy (a, s, 16) + 16;
  __builtin_memcpy (p - 16, s, 16);
}


void g (const void *s)
{
  char *p = (char*)__builtin_mempcpy (a, s, 16);
  __builtin_mempcpy (p - 16, s, 16);
}
	.file	"z.c"
	.text
	.p2align 4
	.globl	f
	.type	f, @function
f:
.LFB0:
	.cfi_startproc
	movdqu	(%rdi), %xmm0
	movups	%xmm0, a(%rip)
	movdqu	(%rdi), %xmm1
	movups	%xmm1, a(%rip)
	ret
	.cfi_endproc
.LFE0:
	.size	f, .-f
	.p2align 4
	.globl	g
	.type	g, @function
g:
.LFB3:
	.cfi_startproc
	movdqu	(%rdi), %xmm0
	movups	%xmm0, a(%rip)
	movdqu	(%rdi), %xmm1
	movups	%xmm1, a(%rip)
	ret
	.cfi_endproc
.LFE3:
	.size	g, .-g
	.ident	"GCC: (GNU) 11.0.0 20200902 (experimental)"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `Missed constant propagation for forward declared constexpr in composite class`
### open_at : `2020-09-08T10:43:40Z`
### last_modified_date : `2020-09-08T22:12:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96972
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 49196
Preprocessed source code to reproduce the bug

The attached source code contains class A that contains a single instance of class B. The value of the only field of class B is initialized with a value calculated by a constexpr function. The whole code could be optimized away and replaced by constant value.

However when compiling for x64 using `g++ -O3 -o main main.cpp` gcc will instead emit a loop that calculates that value at runtime.

This bug happens only in this specific constellation of classes. Doing one of the following will cause gdb to correctly replace that code with a constant:
- Implementing buildnumber in the class body instead of forward declaring it and implementing it outside the class body
- Removing the explicitly declared constructor of class B, using the default constructor generated by the compiler
- Directly instantiating A in the main function instead of instantiating B.


---


### compiler : `gcc`
### title : `Missed optimzation for constant members of non-constant objects`
### open_at : `2020-09-09T12:12:33Z`
### last_modified_date : `2020-09-14T15:18:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96996
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
When a global class instance is const and initialized using constant arguments to a constexpr constructor, any member references are optimized away (using the constant value rather than a lookup). However, when the object is *not* const, but does have const members, such optimization does not happen.

$ cat test2.cpp; gcc -O3 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout test2.cpp
constexpr int v = 1;

struct Test {
  constexpr Test(int v, const int *p) : v(v), p(p) { }
  int const v;
  const int * const p;
};

const Test constant_test(v, &v);
Test non_constant_test(v, &v);

int constant_ref() {
    return constant_test.v + *constant_test.p;
}

int non_constant_ref() {
    return non_constant_test.v + *non_constant_test.p;
}

;; Function constant_ref (_Z12constant_refv, funcdef_no=3, decl_uid=2360, cgraph_uid=4, symbol_order=6)

constant_ref ()
{
  <bb 2> [local count: 1073741824]:
  return 2;

}



;; Function non_constant_ref (_Z16non_constant_refv, funcdef_no=4, decl_uid=2362, cgraph_uid=5, symbol_order=7)

non_constant_ref ()
{
  int _1;
  const int * _2;
  int _3;
  int _5;

  <bb 2> [local count: 1073741824]:
  _1 = non_constant_test.v;
  _2 = non_constant_test.p;
  _3 = *_2;
  _5 = _1 + _3;
  return _5;

}


In the constant_f() case, the values are completely optimized and the return value is determined at compiletime. In the non_constant_f() case, the values are retrieved at runtime.

However, AFAICS there should be no way that these values can be modified at runtime, even when the object itself is not const, since the members are const. So AFAICS, it shoul be possible to evaluation non_constant_f() at compiletime as well.

Looking at the C++ spec (I'm quoting from the C++14 draft here), this would seem to be possible as well.

[basic.type.qualifier] says "A const object is an object of type const T or a non-mutable subobject of such an object."

If I read [intro.object] correctly, subobjects (such as non-static member variables) are also objects, so a non-static member variable declared const would be a "const object".

[dcl.type.cv] says "Except that any class member declared mutable (7.1.1) can be modified, any attempt to modify a const object during its lifetime (3.8) results in undefined behavior."

So, one can assume that the const member variable is not modified, because if it was, that would be undefined behavior.

There is still the caveat of "during its lifetime", IOW, what if you would destroy the object and create a new one it is place. However, see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80794#c5 for a discussion of this case. In short, replacing non_constant_test with a new object is possible, but only when it "does not contain any non-static data member whose type is const-qualified or a reference type", which does not hold for this object. This sounds like this provision was made for pretty much this case, even.

I suspect that reason that it works for the const object now, is because of the rules for constant expressions. [expr.const] defines rules for constant exprssions and seems to only allow using(through lvalue-to-rvalue conversion) objects of non-integral types when they are constexpr. I can imagine that gcc derives that constant_test might be effectively constexpr, making any expressions that use it also effectively constant expressions. This same derivation probably does not happen for subobjects (I guess "constexpr" is not actually a concept that applies to subobjects at all). However, I think this does not mean this optimization would be invalid, just that it would happen on different grounds than the current optimization.


This issue is also related to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80794, but AFAICS that is more about partial compilation and making assumptions about what an external function can or cannot do, while this issue is primarily about link-time (though maybe they are more similar internally, I don't exactly know).

I believe that this optimization would be quite significant to make, since it allows better abstraction and separation of concerns (i.e. it allows writing a class to be generic, using constructor-supplied parameters, but if you pass constants for these parameters and have just a single instance of such a class, or when methods are inlined or constprop'd, there could be zero runtime overhead for this extra abstraction). Currently, I believe that you either have to accept runtime overhead, or resort to using template argument for such parameters (which significantly complicates everything, and takes away freedom from the compiler to decide when to resolve things at compiletime or runtime).


---


### compiler : `gcc`
### title : `rs6000:redundant rldicr fed to lvx/stvx`
### open_at : `2020-09-11T10:29:20Z`
### last_modified_date : `2020-09-16T05:28:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97019
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
When we do the early expansion for altivec built-in function vec_ld/vec_st, we can probably leave some redundant rldicr x,y,0,59 which aims to AND (-16) for the vector access address, since the lvx/stvx will do the aligned and with -16 themselves, they are useless.

===== test case ====

extern int a, b, c;
extern vector unsigned long long ev5, ev6, ev7, ev8;

int test(unsigned char *pe) {
  vector unsigned long long v1, v2, v3, v4, v9;
  vector unsigned long long v5 = ev5;
  vector unsigned long long v6 = ev6;
  vector unsigned long long v7 = ev7;
  vector unsigned long long v8 = ev8;

  unsigned char *e = pe;

  do {
    if (a) {
      asm("memory");
      v1 = __builtin_vec_ld(16, (unsigned long long *)e);
      v2 = __builtin_vec_ld(32, (unsigned long long *)e);
      v3 = __builtin_vec_ld(48, (unsigned long long *)e);
      e = e + 8;
      for (int i = 0; i < a; i++) {
        v4 = v5;
        v5 = __builtin_crypto_vpmsumd(v1, v6);
        v6 = __builtin_crypto_vpmsumd(v2, v7);
        v7 = __builtin_crypto_vpmsumd(v3, v8);
        e = e + 8;
      }
    }
    v5 = __builtin_vec_ld(16, (unsigned long long *)e);
    v6 = __builtin_vec_ld(32, (unsigned long long *)e);
    v7 = __builtin_vec_ld(48, (unsigned long long *)e);
    if (c)
      b = 1;
  } while (b);

  v9 = v4;

  int p = __builtin_unpack_vector_int128((vector __int128_t)v9, 0);

  return p;
}

==== command ====
  -m64 -O2 -mcpu=power8

Currently the function find_alignment_op in RTL swaps pass cares the case where have one single AND operation definition, we can extend it to check all definitions are AND operations and aligned with -16B.


---


### compiler : `gcc`
### title : `missing warning on buffer overflow storing a larger scalar into a smaller array`
### open_at : `2020-09-11T21:10:42Z`
### last_modified_date : `2021-07-15T19:22:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97027
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
None of the obviously out-of-bounds stores in the functions below is diagnosed.  They all should and easily could be.

typedef __INT64_TYPE__ int64_t;
typedef __attribute__ ((__vector_size__ (8))) char V8;
typedef __attribute__ ((__vector_size__ (64))) char V64;

void f0 (int i)
{
  char a[1];
  void *p = a;
  *(int64_t*)p = i;   // storing 8 bytes into a one-byte array
  __builtin_puts (a);
}

void f1 (int i)
{
  char a[1];
  if (i < 1 || 2 < i) i = 1;
  void *p = a + i;
  *(int64_t*)p = i;   // storing 8 bytes at offset 1 into a one-byte array
  __builtin_puts (a);
}

void g0 (int i)
{
  char a[1];
  void *p = a;
  *(V8*)p = (V8){ i };   // storing 8 bytes into a one-byte array
  __builtin_puts (a);
}

void g1 (int i)
{
  char a[1];
  if (i < 1 || 2 < i) i = 1;
  void *p = a + i;
  *(V64*)p = (V64){ i };   // storing 64 bytes at offset 1 into a one-byte array
  __builtin_puts (a);
}


---


### compiler : `gcc`
### title : `powerpc64 UINT_MAX constant`
### open_at : `2020-09-14T04:10:49Z`
### last_modified_date : `2022-03-08T16:20:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97042
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
/* -O2 -S */
long foo (long x) { return ~0u - x; }

for gcc-8 to current master
	lis 9,0xffff
	ori 9,9,0xffff
	rldicl 9,9,0,32
	subf 3,3,9
	blr

a regression from gcc-7
	li 9,-1
	rldicl 9,9,0,32
	subf 3,3,9
	blr

Both sequences give the same result, this is just a code quality regression.

I haven't properly debugged this but I suspect commit 5d3ae76af13


---


### compiler : `gcc`
### title : `BB vectorization behaves sub-optimal`
### open_at : `2020-09-16T06:29:39Z`
### last_modified_date : `2022-01-11T12:15:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97064
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
The testcase g++.dg/vect/slp-pr87105.cc ends in

  _64 = MIN_EXPR <_32, _87>;
  bBox_6(D)->x0 = _64;
  _67 = MIN_EXPR <_33, _86>;
  bBox_6(D)->y0 = _67;
  _70 = MAX_EXPR <_36, _87>;
  bBox_6(D)->x1 = _70;
  _73 = MAX_EXPR <_39, _86>;
  bBox_6(D)->y1 = _73;

thus feeding a 4 element store with a non-uniform SLP opportunity
starting with { MIN, MIN, MAX, MAX }.  With 2-element vector type
vectorization this eventually gets vectorized by splitting the group
which is prioritized over just building the { MIN..., MAX } vector
from scalars but with 4-element vector type vectorization no splitting
is considered and we end up successfully vectorizing just the store
with never considering the smaller vector size.

So at the moment the testcase PASSes with SSE but fails with AVX.


---


### compiler : `gcc`
### title : `Fails to CSE / inherit constant pool load`
### open_at : `2020-09-16T13:45:34Z`
### last_modified_date : `2022-01-11T13:14:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97071
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
double foo (double x)
{
  return x * -3. + 3.;
}

compiles to

0:      addis 2,12,.TOC.-.LCF0@ha
        addi 2,2,.TOC.-.LCF0@l
        .localentry     foo,.-foo
        addis 9,2,.LC0@toc@ha
        lfd 12,.LC0@toc@l(9)
        addis 9,2,.LC2@toc@ha
        lfd 0,.LC2@toc@l(9)
        fmadd 1,1,12,0
        blr

...

.LC0:   
        .long   0
        .long   -1073217536
        .align 3
.LC2:   
        .long   0
        .long   1074266112

but CSE or reload inheritance could have replaced the add of + 3. with
subtraction of the available -3. constant.  Might be more difficult to
pull off on x86 where the add/mul has memory operands.


---


### compiler : `gcc`
### title : `Missed loop unrolling with range for over initializer list`
### open_at : `2020-09-16T21:01:53Z`
### last_modified_date : `2021-12-27T04:11:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97077
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
In the following code:

#include <initializer_list>

int foo(int);

int main() {
    for(int i=0; i<5; ++i) {
        foo(i);
    }
    for(int i : {0,1,2,3,4}) {
        foo(i);
    }
}

The 2 loops should probably produce identical output. However, the output I see here https://godbolt.org/z/z1We36 (using the trunk from 2020-09-15) shows that the first loop (the traditional one) is indeed unrolled as expected, but the second one is not.


---


### compiler : `gcc`
### title : `__builtin_lround and _builtin_llround not replaced with fcvtas on aarch64`
### open_at : `2020-09-17T09:37:06Z`
### last_modified_date : `2020-09-17T16:35:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97083
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `enhancement`
### contents :
On aarch64 calling __builtin_round and casting the result to int or long long uses a single fcvtas instruction, but using __builtin_lround or __builtin_llround instead will do function call.

Seems like they are missing the same optimization.


---


### compiler : `gcc`
### title : `FMA3 code transformation leads to slowdown on Skylake`
### open_at : `2020-09-20T20:36:06Z`
### last_modified_date : `2020-09-30T12:09:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97127
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
The following clever gcc transformation leads to generation of slower code than non-transformed original:
 a = *mem;
 a = a + b * c;
where both b and c are reused further down is transformed to:
 a = b
 a = *mem + a * c;

Or, expressing the same in asm terms
 vmovuxx      (mem), %ymmA
 vfnmadd231xx %ymmB, %ymmC, %ymmA
transformed to
 vmovaxx      %ymmB, %ymmA
 vfnmadd213xx (mem), %ymmC, %ymmA

You may ask "Why transformed variant is slower?" and I can try my best to answer (my guess is that performance bottleneck is in rename stage rather than in the execution stage and transformed code occupies 3 rename slots vs 2 rename slots by original) but it would be mostly pointless. What's matters that on Skylake the transformed variant is slower and I can prove it with benchmark. BTW, on Haswell too.

You can see comparison of two variants at https://github.com/already5chosen/others/tree/master/cholesky_solver/gcc-badopt-fma3
The interesting spot is starting at line 367 in file chol.cpp.
Or starting two lines below .L21: in asm generated by gcc 10.2.0 (chol_a.s).
Run 's_chol_a 100' vs 's_chol_b 100' and see the difference in favor of the second (de-transformed) variant.
The difference, in this particular case, is small, order of 2-4 percents, but very consistent.
In more tight loops I would expect a bigger difference.


---


### compiler : `gcc`
### title : `Missed add-with-carry after FP compare`
### open_at : `2020-09-21T09:10:55Z`
### last_modified_date : `2021-07-25T01:19:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97137
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Extracted from 508.namd_r

void foo (double *r2i, double r2_0, double cutoff2_delta, int jout)
{
  bool test0 = (r2_0 < cutoff2_delta);
  jout += test0;
  r2i[jout] = r2_0;
}

produces

        xorl    %eax, %eax
        comisd  %xmm0, %xmm1
        seta    %al
        addl    %esi, %eax
        movsd   %xmm0, (%rdi,%rax,8)

while llvm optimizes this to

        ucomisd %xmm1, %xmm0
        adcl    $0, %esi
        movslq  %esi, %rax
        movsd   %xmm0, (%rdi,%rax,8)

possibly saving one register.  Note that clang needs -ffinite-math-only for this.
I'm not sure how r2_0 < cutoff2_delta behaves with NaNs, but with
-ffinite-math-only we'd be sure there are none and thus even comisd set
of CF should be enough to optimize this.


---


### compiler : `gcc`
### title : `cross-BB vectorization opportunity`
### open_at : `2020-09-21T10:21:12Z`
### last_modified_date : `2020-12-08T09:52:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97138
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
double a[1024], b[1024];
void bar();
void foo(double x)
{
  double a0 = a[0];
  double a1 = a[1];
  for (int i = 0; i < 511;)
    {
      b[2*i] = a0 + x;
      b[2*i + 1] = a1 + x;
      i++;
      bar ();
      a0 = a[2*i];
      a1 = a[2*i+1];
    }
}

can be BB vectorized but that needs handling of PHI nodes and multiple BBs
for an optimal result.


---


### compiler : `gcc`
### title : `__builtin_fmod not optimized on POWER`
### open_at : `2020-09-21T10:57:19Z`
### last_modified_date : `2021-09-14T05:36:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97142
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
I ran some Fortran benchmarks (the "Polyhedron" set) on POWER9, and found one of them has pathologically bad performance compared with xlf.  Profiling shows that's due to spending most of its time in fmod via a random-number function.  fmod isn't called when compiled with xlf -O5 or when compiling the same on x86_64.  Although it's Fortran, this doesn't appear to be Fortran-specific as the DMOD intrinsic is turned into __builtin_fmod.

The following is with gcc 10.2, comparing the two targets.

On RHEL7 POWER9 (and the same with -mcpu=native):

$ cat ggl.f90
      REAL FUNCTION GGL(Ds)
      DOUBLE PRECISION Ds , d2
      DATA d2/2147483647.D0/
      Ds = DMOD(16807.D0*Ds,d2)
      GGL = Ds/d2
      END
$ gfortran -O3 -fopt-info-all -c ggl.f90
ggl.f90:4:0: missed:   not inlinable: ggl/0 -> __builtin_fmod/2, function body not available
Unit growth for small function inlining: 12->12 (0%)

Inlined 0 calls, eliminated 0 functions

ggl.f90:6:0: note: ***** Analysis failed with vector mode V2DF
$ nm ggl.o
                 U fmod
0000000000000000 T ggl_
                 U .TOC.

On Debian 10 SKX with the same source:

$ gfortran-10 -Ofast -fopt-info-all -c ggl.f90
ggl.f90:4:0: missed:   not inlinable: ggl/0 -> __builtin_fmod/2, function body not available
Unit growth for small function inlining: 12->12 (0%)

Inlined 0 calls, eliminated 0 functions

ggl.f90:6:0: note: ***** Analysis failed with vector mode V2DF
ggl.f90:6:0: note: ***** Skipping vector mode V16QI, which would repeat the analysis for V2DF
$ nm ggl.o
0000000000000000 r .LC0
0000000000000008 r .LC1
0000000000000010 r .LC2
0000000000000000 T ggl_


---


### compiler : `gcc`
### title : `GCC uses vhaddpd which is bad for latency`
### open_at : `2020-09-21T12:56:43Z`
### last_modified_date : `2022-01-11T12:07:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97147
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef double v2df __attribute__((vector_size(16)));
double foo (v2df x, double y)
{
  return x[0] + x[1] + y;
}
double bar (v2df x, double y)
{
  return y + x[0] + x[1];
}

with -O2 -mavx2 -mtune=znver2 ends up generating

foo:
.LFB0:
        .cfi_startproc
        vhaddpd %xmm0, %xmm0, %xmm0
        vaddsd  %xmm1, %xmm0, %xmm0
        ret

bar:
.LFB1:
        .cfi_startproc
        vmovapd %xmm0, %xmm2
        vaddsd  %xmm1, %xmm0, %xmm0
        vunpckhpd       %xmm2, %xmm2, %xmm2
        vaddsd  %xmm2, %xmm0, %xmm0
        ret

where bar should be a _lot_ better according to Agner which says
that vhaddpd has a 4 uops, a latency of 7 cycles and a throughput of only
one per two cycles while both vunpckhpd and vaddsd fare a lot better here.
Coffee-lake isn't much better here.

Maybe we want to disable the V2DF instructions for most tunings somehow?


---


### compiler : `gcc`
### title : `GCC fails to optimize away uselessly allocated arrays (C++)`
### open_at : `2020-09-21T15:13:17Z`
### last_modified_date : `2020-09-23T08:14:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97151
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.0`
### severity : `normal`
### contents :
A test program:

int main()
{
    int *a = new int[10]();
    delete[] a;

    return 0;
}


Tested against: gcc 10.2.0 and gcc trunk as of 20200920 on codebolt.org with -O2, -O3 and -Ofast

Expected: new/delete are optimized away under -O2 and higher because "a" is never used for anything useful (like, side effects or whatever).

Actual: new/delete are present in assembly:

main:
        sub     rsp, 8
        mov     edi, 40
        call    operator new[](unsigned long)
        pxor    xmm0, xmm0
        mov     QWORD PTR [rax+32], 0
        mov     rdi, rax
        movups  XMMWORD PTR [rax], xmm0
        movups  XMMWORD PTR [rax+16], xmm0
        call    operator delete[](void*)
        xor     eax, eax
        add     rsp, 8
        ret

Interesting observations:

1. if "delete[]" is commented out, whole program is optimized away
2. if "new int[10]()" is replaced with "new int[10]", whole program is optimized away
3. if new/delete is replaced with matching malloc/free, whole program is optimized away

If someone wants to play with this example online, here it is: https://godbolt.org/z/YoGzxo


---


### compiler : `gcc`
### title : `Missed optimization [x86-64] tzcnt unnecessarily zeros out destination register`
### open_at : `2020-09-22T03:07:24Z`
### last_modified_date : `2020-09-22T20:32:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97156
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
See: https://gcc.godbolt.org/z/Y591MW

void __attribute__((noinline))
tzncnt_not_just_return(uint64_t v) {
    for (; v; ) {
        uint64_t i;
        i = _tzcnt_u64(v);
        v &= (v - 1);
        bench_do_not_optimize_out(i);
    }
    bench_flush_all_pending();
} 

compiles to

tzncnt_not_just_return(unsigned long):
        jmp     .L11
.L6:
        xor     eax, eax
        tzcnt   rax, rdi
        blsr    rdi, rdi
.L11:
        test    rdi, rdi
        jne     .L6
        ret

the "xor     eax, eax" isnt needed.


---


### compiler : `gcc`
### title : `Regression from GCC 8 optimizing to sincos on ppc64le`
### open_at : `2020-09-22T09:37:53Z`
### last_modified_date : `2020-09-25T08:58:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97160
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 49254
Fortran benchmark

I looked at the other case in a Fortran benchmark set where GCC does badly on ppc64le with GCC 10.  The problem seems to be in the middle end, though, not Fortran.

This case is heavy on trig functions, and I found that gfortran 10 and 9 miss the optimization of converting sin+cos to sincos which is done by 8 on ppc64le and also by 10 on x86_64.  I tried a simple example rather than the whole thing, but couldn't quickly reproduce the effect out of context, so it may be rather specific to this case.  I'll attach the original 1400-line single routine (!) but I could take quick suggestions to narrow down the issue.

With gfortran 10.2 (and similarly with 9) on RHEL7 POWER9 the top of the perf profile resulting from "gfortran -Ofast" is

    43.60%  mp_prop_design  libm-2.17.so          [.] __cos
    28.85%  mp_prop_design  mp_prop_design        [.] MAIN__
    24.40%  mp_prop_design  libm-2.17.so          [.] __sin
     1.04%  mp_prop_design  libm-2.17.so          [.] __sincos

With gfortran 8 (from "advanced toolkit 12") it's

    78.49%  a.out    mp_prop_design        [.] MAIN__
    20.29%  a.out    libm-2.28.so          [.] __sincos
     1.14%  a.out    libm-2.28.so          [.] __tan

(Those look as if they're from different systems because AT 12 picks up its own glibc when you set up the environment.)


---


### compiler : `gcc`
### title : `warning: ‘__builtin___strncat_chk’ on strncat when it is not necessary`
### open_at : `2020-09-23T13:45:58Z`
### last_modified_date : `2020-09-23T16:13:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97180
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
On gcc 9 and gcc 10 this code : 

  #define size 50
  char_table[size]

  strcpy(char_table, "");
  strncat(char_table, char_table2, size - 1);


will give a warning : 



In function ‘strncat’,
    inlined from ‘XXX’ at src/YYY.c:56:3:
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:136:10: warning: ‘__builtin___strncat_chk’ output may be truncated copying AAA bytes from a string of length BBB [-Wstringop-truncation]

I understand the warning but i see no reason to that in this case, it seems to be a bug.


---


### compiler : `gcc`
### title : `inconsistent builtin elimination for impossible range`
### open_at : `2020-09-23T20:18:52Z`
### last_modified_date : `2023-08-08T02:35:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97185
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
In the test case below GCC eliminates the memcpy and memmove calls because the only valid bound they are called with is zero.  But the same optimization isn't done for the any of the remaining calls, including memset, resulting in warnings noting that the range is (most likely) invalid.  GCC should be consistent and either eliminate all the calls (preferably), or none of them.

$ cat z.c && gcc -O2 -S -fdump-tree-optimized=/dev/stdout z.c
void f0 (void *p, const void *q, int n)
{ 
  if (n > 0) return;
  __builtin_memcpy (p, q, n);
}

void f1 (void *p, const void *q, int n)
{
  if (n > 0) return;
  __builtin_memmove (p, q, n);
}

void f2 (char *p, const char *q, int n)
{
  if (n > 0) return;
  __builtin_strncpy (p, q, n);
}

void f3 (void *p, int n)
{ 
  if (n > 0) return;
  __builtin_memset (p, 0, n);
}

void* f4 (const void *p, int n)
{
  if (n > 0) return 0;
  return __builtin_memchr (p, 0, n);
}

int f5 (const void *p, const void *q, int n)
{
  if (n > 0) return 0;
  return __builtin_memcmp (p, q, n);
}


;; Function f0 (f0, funcdef_no=0, decl_uid=1933, cgraph_uid=1, symbol_order=0)

f0 (void * p, const void * q, int n)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function f1 (f1, funcdef_no=1, decl_uid=1938, cgraph_uid=2, symbol_order=1)

f1 (void * p, const void * q, int n)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function f2 (f2, funcdef_no=2, decl_uid=1943, cgraph_uid=3, symbol_order=2)

Removing basic block 5
f2 (char * p, const char * q, int n)
{
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  if (n_3(D) > 0)
    goto <bb 4>; [60.08%]
  else
    goto <bb 3>; [39.92%]

  <bb 3> [local count: 428637736]:
  _1 = (long unsigned int) n_3(D);
  __builtin_strncpy (p_5(D), q_6(D), _1); [tail call]

  <bb 4> [local count: 1073741824]:
  return;

}


z.c: In function ‘f2’:
z.c:16:3: warning: ‘__builtin_strncpy’ specified size between 18446744071562067968 and 0 exceeds maximum object size 9223372036854775807 [-Wstringop-overflow=]
   16 |   __builtin_strncpy (p, q, n);
      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~

;; Function f3 (f3, funcdef_no=3, decl_uid=1947, cgraph_uid=4, symbol_order=3)

Removing basic block 5
f3 (void * p, int n)
{
  long unsigned int _1;

  <bb 2> [local count: 1073741824]:
  if (n_3(D) > 0)
    goto <bb 4>; [60.08%]
  else
    goto <bb 3>; [39.92%]

  <bb 3> [local count: 428637736]:
  _1 = (long unsigned int) n_3(D);
  __builtin_memset (p_5(D), 0, _1); [tail call]

  <bb 4> [local count: 1073741824]:
  return;

}


z.c: In function ‘f3’:
z.c:22:3: warning: ‘__builtin_memset’ specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [-Wstringop-overflow=]
   22 |   __builtin_memset (p, 0, n);
      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~

;; Function f4 (f4, funcdef_no=4, decl_uid=1951, cgraph_uid=5, symbol_order=4)

Removing basic block 5
f4 (const void * p, int n)
{
  long unsigned int _1;
  void * _2;
  void * _6;

  <bb 2> [local count: 1073741824]:
  if (n_3(D) > 0)
    goto <bb 4>; [23.24%]
  else
    goto <bb 3>; [76.76%]

  <bb 3> [local count: 824204225]:
  _1 = (long unsigned int) n_3(D);
  _6 = __builtin_memchr (p_5(D), 0, _1); [tail call]

  <bb 4> [local count: 1073741824]:
  # _2 = PHI <_6(3), 0B(2)>
  return _2;

}


z.c: In function ‘f4’:
z.c:28:10: warning: ‘__builtin_memchr’ specified bound [18446744071562067968, 0] exceeds maximum object size 9223372036854775807 [-Wstringop-overread]
   28 |   return __builtin_memchr (p, 0, n);
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~

;; Function f5 (f5, funcdef_no=5, decl_uid=1956, cgraph_uid=6, symbol_order=5)

Removing basic block 5
f5 (const void * p, const void * q, int n)
{
  long unsigned int _1;
  int _2;
  int _7;

  <bb 2> [local count: 1073741824]:
  if (n_3(D) > 0)
    goto <bb 4>; [42.57%]
  else
    goto <bb 3>; [57.43%]

  <bb 3> [local count: 616649929]:
  _1 = (long unsigned int) n_3(D);
  _7 = __builtin_memcmp (p_5(D), q_6(D), _1); [tail call]

  <bb 4> [local count: 1073741824]:
  # _2 = PHI <_7(3), 0(2)>
  return _2;

}


z.c: In function ‘f5’:
z.c:34:10: warning: ‘__builtin_memcmp’ specified bound [18446744071562067968, 0] exceeds maximum object size 9223372036854775807 [-Wstringop-overread]
   34 |   return __builtin_memcmp (p, q, n);
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~


---


### compiler : `gcc`
### title : `optimize vector element set/extract at variable position`
### open_at : `2020-09-24T14:26:43Z`
### last_modified_date : `2022-01-11T12:05:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97194
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
#define N 32
typedef int T;
typedef T V __attribute__((vector_size(N)));
V set (V v, int idx, T val)
{
  v[idx] = val;
  return v;
}
T get (V v, int idx)
{
  return v[idx];
}

generates with -mavx2

set:
.LFB0:
        .cfi_startproc
        pushq   %rbp
        .cfi_def_cfa_offset 16
        .cfi_offset 6, -16
        movslq  %edi, %rdi
        movq    %rsp, %rbp
        .cfi_def_cfa_register 6
        andq    $-32, %rsp
        vmovdqa %ymm0, -32(%rsp)
        movl    %esi, -32(%rsp,%rdi,4)
        vmovdqa -32(%rsp), %ymm0
^^^ store forwarding fail
        leave
        .cfi_def_cfa 7, 8
        ret

get:
.LFB1:
        .cfi_startproc
        pushq   %rbp
        .cfi_def_cfa_offset 16
        .cfi_offset 6, -16
        movslq  %edi, %rdi
        movq    %rsp, %rbp
        .cfi_def_cfa_register 6
        andq    $-32, %rsp
        vmovdqa %ymm0, -32(%rsp)
        movl    -32(%rsp,%rdi,4), %eax
        leave
        .cfi_def_cfa 7, 8
        ret

maybe not too bad.

Vary N and T to cover all types and vector sizes.

It should be possible to do the 'get' case via variable permutes
and the 'set' case via a splat of the value and a blend using
a mask generated from 'idx'.


---


### compiler : `gcc`
### title : `Failure to optimize comparison of char arithmetic to single comparison`
### open_at : `2020-09-28T07:14:56Z`
### last_modified_date : `2023-09-21T11:02:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97223
### status : `RESOLVED`
### tags : `easyhack, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool f(char x)
{
    return x < (char)(x + some_constant);
}

This (where `some-constant` is replaced by a constant expression) can be optimized to `return x <= (CHAR_MAX - some_constant);`. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize out conditional addition of zero`
### open_at : `2020-09-28T10:22:22Z`
### last_modified_date : `2023-08-04T22:28:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97225
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
For the following code:

#include <stddef.h>

struct vector {
    int *data;
    size_t size;
};

int *vector_end(struct vector *vec)
{
    return vec->data + vec->size;
}

GCC 10.2.0 on x86-64 generates the following code (same on -O2, -O3, and -Os):

vector_end:
        movq    8(%rdi), %rdx
        movq    (%rdi), %rax
        leaq    (%rax,%rdx,4), %rax
        ret

However, vector_end() needs to handle empty vectors represented as { NULL, 0 }. Pointer arithmetic on a null pointer is undefined behavior (even NULL + 0, as far as I can tell from the C standard), so the correct code is:

int *vector_end(struct vector *vec)
{
    if (vec->size == 0)
        return vec->data;
    return vec->data + vec->size;
}

I'd expect this to generate the same code, but GCC 10.2.0 generates a conditional move with -O2 and -O3:

vector_end:
        movq    8(%rdi), %rdx
        movq    (%rdi), %rax
        testq   %rdx, %rdx
        leaq    (%rax,%rdx,4), %rcx
        cmovne  %rcx, %rax
        ret

And a branch with -Os:

vector_end:
        movq    8(%rdi), %rdx
        movq    (%rdi), %rax
        testq   %rdx, %rdx
        je      .L1
        leaq    (%rax,%rdx,4), %rax
.L1:
        ret

Clang 10.0.1, on the other hand, generates the same code with and without the size check (oddly enough, it also falls back to a conditional move if the size member is an int or unsigned int instead of size_t/unsigned long):

vector_end:                             # @vector_end
        movq    8(%rdi), %rax
        shlq    $2, %rax
        addq    (%rdi), %rax
        retq

Can GCC avoid the conditional move/branch here?


---


### compiler : `gcc`
### title : `Missing vec_select and subreg optimization`
### open_at : `2020-09-30T06:30:22Z`
### last_modified_date : `2021-08-21T18:28:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97249
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Cat test.c
---
void
foo (unsigned char* p1, unsigned char* p2, short* __restrict p3)
{
    for (int i = 0 ; i != 8; i++)
     p3[i] = p1[i] + p2[i];
     return;
}
---

gcc11 -Ofast -mavx2 test.c  got

---
foo:
.LFB0:
        .cfi_startproc
        vmovq   (%rdi), %xmm0
        vmovq   (%rsi), %xmm1
        vpmovzxbw       %xmm0, %xmm0
        vpmovzxbw       %xmm1, %xmm1
        vpaddw  %xmm1, %xmm0, %xmm0
        vmovdqu %xmm0, (%rdx)
        ret
        .cfi_endproc
---

memory operand doesn't propagate into *vpmovzxbw* because rtl didn't simplify
---
(insn 9 8 10 2 (set (reg:V8HI 92 [ vect__33.6 ])
        (zero_extend:V8HI (vec_select:V8QI (subreg:V16QI (reg:V8QI 91 [ vect__40.5 ]) 0)
                (parallel [
                        (const_int 0 [0])
                        (const_int 1 [0x1])
                        (const_int 2 [0x2])
                        (const_int 3 [0x3])
                        (const_int 4 [0x4])
                        (const_int 5 [0x5])
                        (const_int 6 [0x6])
                        (const_int 7 [0x7])
                    ])))) "test.c":5:16 4638 {sse4_1_zero_extendv8qiv8hi2}
     (expr_list:REG_DEAD (reg:V8QI 91 [ vect__40.5 ])
        (nil)))
--- 

to 

---
(insn 9 8 10 2 (set (reg:V8HI 92 [ vect__33.6 ])
        (zero_extend:V8HI (reg:V8QI 91 [ vect__40.5 ])))) "test.c":5:16 4638 {sse4_1_zero_extendv8qiv8hi2}
     (expr_list:REG_DEAD (reg:V8QI 91 [ vect__40.5 ])
        (nil)))
---

Similar for other vector modes.


---


### compiler : `gcc`
### title : `[10 regression] memcmp of constant string and local constant array not folded`
### open_at : `2020-09-30T23:27:48Z`
### last_modified_date : `2023-07-07T09:06:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97260
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
GCC fails to fold the memcmp call below into a constant.  Clang folds it successfully.

$ cat x.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout -o/dev/stdout x.c
int f (void)
{
  const char a[] = "1234";
  return __builtin_memcmp (a, "1234", 4);
}
	.file	"x.c"
	.text

;; Function f (f, funcdef_no=0, decl_uid=1931, cgraph_uid=1, symbol_order=0)

f ()
{
  const char a[5];
  int _3;

  <bb 2> [local count: 1073741824]:
  a = "1234";
  _3 = __builtin_memcmp (&a, "1234", 4);
  a ={v} {CLOBBER};
  return _3;

}


	.section	.rodata.str1.1,"aMS",@progbits,1
.LC0:
	.string	"1234"
	.text
	.p2align 4
	.globl	f
	.type	f, @function
f:
.LFB0:
	.cfi_startproc
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movl	$4, %edx
	movl	$.LC0, %esi
	leaq	11(%rsp), %rdi
	movl	$875770417, 11(%rsp)
	movb	$0, 15(%rsp)
	call	memcmp
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:
	.size	f, .-f
	.ident	"GCC: (GNU) 11.0.0 20200930 (experimental)"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `division done twice for modulo and divsion for 128-bit integers`
### open_at : `2020-10-03T16:37:01Z`
### last_modified_date : `2021-08-15T11:23:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97282
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Currently, gcc calls the (long and slow) division routines for 128-bit
integers twice when both the residual and the value is needed.  For
the other integer types, this is optimized.

Take this test case:

$ cat digsum.c
#include <stdio.h>
#include <x86intrin.h>

typedef __uint128_t myint;

unsigned long digsum1 (myint x)
{
  unsigned long ret;

  if (x == 0)
    return 0; 

  ret = 0;
  while (x > 0)
    {
      ret = ret + x % 10;
      x = x / 10;
    }
  return ret;
}

unsigned long digsum2 (myint x)
{
  unsigned long ret;
  myint tmp;

  if (x == 0)
    return 0; 

  ret = 0;
  while (x > 0)
    {
      tmp = x / 10;
      ret = ret + (x - tmp * 10);
      x = tmp;
    }
  return ret;
}

#define NUM 1000000
int main()
{
  myint x;
  unsigned long sum;
  long int t1, t2;
  __uint128_t from, to;

  from = 1;
  from = (from << 93) - NUM/2;
  to = from + NUM;
  sum = 0;

  t1 = __rdtsc();
  for (x=from; x<to; x++)
    sum = sum + digsum1(x);

  t2 = __rdtsc();
  printf ("digsum1:\nsum = %lu\n", sum);
  printf ("cycles per sum = %.2f\n\n", (double) (t2-t1)/NUM);

  sum = 0;
  t1 = __rdtsc();
  for (x=from; x<to; x++)
    sum = sum + digsum2(x);

  t2 = __rdtsc();
  printf ("digsum2:\nsum = %lu\n", sum);
  printf ("cycles per sum = %.2f\n", (double) (t2-t1)/NUM);

  return 0;
}

"As is", this gives on my machine

$ gcc -O3 digsum.c
$ ./a.out
digsum1:
sum = 113493792
cycles per sum = 2021.68

digsum2:
sum = 113493792
cycles per sum = 1025.47

(similar timings if a signed type is used).

This also affects Fortran I/O.


---


### compiler : `gcc`
### title : `GCC sometimes uses an extra xmm register for the destination of _mm_blend_ps`
### open_at : `2020-10-04T16:43:48Z`
### last_modified_date : `2023-10-16T17:20:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97286
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.2.0`
### severity : `enhancement`
### contents :
GCC sometimes uses an extra xmm register for the destination of _mm_blend_ps, instead of doing the blend in-place.

Example program:

// gcc -Wall -O3 -march=znver1 -S
#include <stdint.h>
#include <stddef.h>
#include <immintrin.h>
void foo(const __m128i *in, __m128i *out, size_t count, __m128i a) {
    while (count--) {
        __m128 b = (__m128) _mm_loadu_si128(in++);
        a = (__m128i)_mm_blend_ps((__m128)a, b, 0x5);
        _mm_storeu_si128(out++, a);
    }
}

GCC output for the loop:

.L3:
	vblendps	$5, (%rdi,%rax), %xmm1, %xmm0
	decq	%rdx
	vmovdqu	%xmm0, (%rsi,%rax)
	vmovdqa	%xmm0, %xmm1
	addq	$16, %rax
	cmpq	$-1, %rdx
	jne	.L3

Note the destination of vblendps is xmm0 instead of xmm1, increasing register pressure and requiring an extra mov (vmovdqa	%xmm0, %xmm1).

clang does the vblendps in-place:

.LBB0_2:                                # =>This Inner Loop Header: Depth=1
	vblendps	$5, (%rdi,%rax), %xmm0, %xmm0 # xmm0 = mem[0],xmm0[1],mem[2],xmm0[3]
	decq	%rdx
	vmovups	%xmm0, (%rsi,%rax)
	leaq	16(%rax), %rax
	jne	.LBB0_2

This missed optimization causes register spills in a more complex loop I have, though I didn't measure significant performance difference for my use case.


---


### compiler : `gcc`
### title : `Optimization for pure vs. const function`
### open_at : `2020-10-06T16:46:00Z`
### last_modified_date : `2021-12-06T05:46:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97307
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `normal`
### contents :
This bug report is based on this stackoverflow post:
https://stackoverflow.com/q/64034889/916672

The source code I use in this post, is also available here: https://gcc.godbolt.org/z/dGvxnv

Given this C source code:

> int pure_f(int a, int b) __attribute__((pure));
> 
> int const_f(int a, int b) __attribute__((const));
> 
> int my_f(int a, int b) {
>     int x = pure_f(a, b);
>     if (a > 0) {
>         return x;
>     }
>     return a;
> }

If this is compiled with gcc with -O3, I would expect that the evaluation of pure_f(a, b) is moved into the if. But it is not done:

> my_f(int, int):
>         push    r12
>         mov     r12d, edi
>         call    pure_f(int, int)
>         test    r12d, r12d
>         cmovg   r12d, eax
>         mov     eax, r12d
>         pop     r12
>         ret

On the other side, if const_f is called instead of pure_f, it is moved into the if:


> my_f(int, int):
>         test    edi, edi
>         jg      .L4
>         mov     eax, edi
>         ret
> .L4:
>         jmp     const_f(int, int)


Why isn't this optimization applied for a pure function? From my understanding, this should also be possible and it seems to be beneficial.


---


### compiler : `gcc`
### title : `inefficient vectorization of gcc.dg/vect/bb-slp-pr65935.c`
### open_at : `2020-10-08T09:47:47Z`
### last_modified_date : `2020-10-09T11:15:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97334
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For

void rephase (void)
{
  int i,j,k,dir;
  struct site *s;
  for(i=0,s=lattice;i<sites_on_node;i++,s++)
    for(dir=0;dir<32;dir++)
      for(j=0;j<3;j++)for(k=0;k<3;k++)
        {
          s->link[dir].e[j][k].real *= s->phase[dir];
          s->link[dir].e[j][k].imag *= s->phase[dir];
        }

where SLP faces unrolled j and k loops we fail to vectorize the
loads of { s->link[dir].e[j][k].real, s->link[dir].e[j][k].imag }
because SLP discovery rejects the s->phase[dir] load:

/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:   Build SLP for _59 = s_8->phase[dir_80];
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: missed:   Build SLP failed: unvectorizable statement _59 = s_8->phase[dir_80];
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:   Building vector operands from scalars
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:   Build SLP for _59 = s_8->phase[dir_80];
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: missed:   Build SLP failed: unvectorizable statement _59 = s_8->phase[dir_80];
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:   Building vector operands from scalars

and then ends up building the multiplication operands from scalars:

/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:   node 0x4a36cb0 (max_nunits=2, refcnt=2)
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:        stmt 0 _60 = _58 * _59;
..
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:        children 0x4931310 0x4a6e690
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:   node (external) 0x4931310 (max_nunits=1, refcnt=1)
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:        { _58, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59, _59 }
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:   node (external) 0x4a6e690 (max_nunits=1, refcnt=1)
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr65935.c:28:30: note:        { _59, _62, _65, _68, _71, _74, _82, _86, _89, _92, _95, _98, _106, _110, _113, _116, _119, _122 }


---


### compiler : `gcc`
### title : `AVX2 vectorizer generates extremely strange and slow code for AoSoA complex dot product`
### open_at : `2020-10-08T23:09:36Z`
### last_modified_date : `2020-10-12T13:11:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97343
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Let's continue our complex dot product series started here
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96854

This time I have no code generation bugs for your pleasure, just "interesting" optimization issues.
All examples below, unless stated otherwise were compiled with gcc.10.2 for x86-64 with following sets 
of flags: 
set1: -Wall -mavx2 -mfma -march=skylake -O3 -ffast-math -fno-associative-math
set2: -Wall -mavx2 -mfma -march=skylake -O3 -ffast-math

The kernel in question is an example of complex dot product in so-called "hybrid AoS" layout a.k.a. AoSoA.
https://en.wikipedia.org/wiki/AoS_and_SoA#Array_of_Structures_of_Arrays
In my experience it's quite rare when in dense complex linear algebra and similar computational fields AoSoA is *not* the optimal internal form.
So, practically, I consider these kernels as more important than AoS kernel presented in bug 96854.

More specifically, the layout can be described as struct { double re[4], im[4]; };
But for purpose of simplicity I omitted the type definition fro code examples and coded it directly over flat arrays of doubles.

Part 1.
void cdot(double* res, const double* a, const double* b, int N)
{
  double acc_re = 0;
  double acc_im = 0;
  for (int c = 0; c < N; ++c) {
    for (int k = 0; k < 4; ++k) {
      acc_re = acc_re + a[c*8+k+0]*b[c*8+k+0] + a[c*8+k+4]*b[c*8+k+4];
      acc_im = acc_im - a[c*8+k+0]*b[c*8+k+4] + a[c*8+k+4]*b[c*8+k+0];
    }
  }
  res[0] = acc_re;
  res[4] = acc_im;
}

That's how we want to code it in the ideal world and let to compiles to care about dirty details.
In less ideal world, gcc is not the only compiler that can't cope with it.
MSVC (-W4 -O2 -fp:fast -arch:AVX2) also can't vectorize it. Even mighty icc generates code that it's not quite bad, but somewhat suboptimal.
So, let's it pass. I don't want to blame gcc for not being smart enough. It's just normal.
Except that when we use set2 the code generated by gcc becomes not just non-smart, but quite crazy.
I am ignoring it in the hope that it would be magically fixed by the change made by Richard Biener 2020-08-31

Part 2.
void cdot(double* res, const double* a, const double* b, int N)
{
  double acc_rere = 0;
  double acc_imim = 0;
  double acc_reim = 0;
  double acc_imre = 0;
  for (int c = 0; c < N; ++c) {
    for (int k = 0; k < 4; ++k) {
      acc_rere += a[c*8+k+0]*b[c*8+k+0];
      acc_imim += a[c*8+k+4]*b[c*8+k+4];
      acc_reim += a[c*8+k+0]*b[c*8+k+4];
      acc_imre += a[c*8+k+4]*b[c*8+k+0];
    }
  }
  res[0] = acc_rere+acc_imim;
  res[4] = acc_imre-acc_reim;
}

We are explaining it to compiler slowly.
For icc and MSVC it's enough. They understood.
icc generates near-perfect code. I can write it nicer, but do not expect my variant to be any faster.
MSVC generates near-perfect inner loop and epilogue that is not great, but not really much slower.
gcc still didn't get it. It still tries to implement 4 accumulators literally, as if -ffast-math were not here.
But, sad as it is, it's still a case of not being smart enough. So, I am not complaining.

Part 3.
static inline double sum4(double x[]) {
  return x[0]+x[1]+x[2]+x[3];
}
void cdot(double* res, const double* a, const double* b, int N)
{
  double acc_re[4] = {0};
  double acc_im[4] = {0};
  for (int c = 0; c < N; ++c) {
    for (int k = 0; k < 4; ++k) {
      acc_re[k] = acc_re[k] + a[c*8+k+0]*b[c*8+k+0] + a[c*8+k+4]*b[c*8+k+4];
      acc_im[k] = acc_im[k] - a[c*8+k+0]*b[c*8+k+4] + a[c*8+k+4]*b[c*8+k+0];
    }
  }
  res[0] = sum4(acc_re);
  res[4] = sum4(acc_im);
}

Attempt to feed compiler by teaspoon.
That's not a way I want to write code in HLL.
icc copes, producing about the same code as in Part 1
MSVC doesn't understand a Kunststück (I am sympathetic) and generates literal scalar code with local arrays on stack.
gcc with set1 is a little better than MSVC - the code is fully scalar, but at least accumulators kept in registers.
gcc with set2 is the most interesting. It vectorizes, but how?
Here is an inner loop:

.L3:
	vpermpd	$27, (%r8,%rax), %ymm2
	vpermpd	$27, 32(%rdx,%rax), %ymm3
	vpermpd	$27, (%rdx,%rax), %ymm1
	vpermpd	$27, 32(%r8,%rax), %ymm0
	vmulpd	%ymm2, %ymm1, %ymm6
	vmulpd	%ymm2, %ymm3, %ymm2
	addq	$64, %rax
	vfnmadd132pd	%ymm0, %ymm2, %ymm1
	vfmadd132pd	%ymm3, %ymm6, %ymm0
	vaddpd	%ymm1, %ymm5, %ymm5
	vaddpd	%ymm0, %ymm4, %ymm4
	cmpq	%rcx, %rax
	jne	.L3

What all this vpermpd business about? Shuffling SIMD lanes around just because it's funny?
That the first thing I do want to complain about. Not "not smart enough", but too smart for its own good.

And finally 
Part 4
static inline double sum4(double x[]) {
  return x[0]+x[1]+x[2]+x[3];
}
void cdot(double* res, const double* a, const double* b, int N)
{
  double acc_rere[4] = {0};
  double acc_imim[4] = {0};
  double acc_reim[4] = {0};
  double acc_imre[4] = {0};
  for (int c = 0; c < N; ++c) {
    for (int k = 0; k < 4; ++k) {
      acc_rere[k] += a[c*8+k+0]*b[c*8+k+0];
      acc_imim[k] += a[c*8+k+4]*b[c*8+k+4];
      acc_reim[k] += a[c*8+k+0]*b[c*8+k+4];
      acc_imre[k] += a[c*8+k+4]*b[c*8+k+0];
    }
  }
  double acc_re[4];
  double acc_im[4];
  for (int k = 0; k < 4; ++k) {
    acc_re[k] = acc_rere[k]+acc_imim[k];
    acc_im[k] = acc_imre[k]-acc_reim[k];
  }
  res[0] = sum4(acc_re);
  res[4] = sum4(acc_im);
}

Not just fed by teaspoon, but compiler's mouth held open manually, so to speak.
icc, of course, understands and generates pretty much the same good code as in Part 2.
MSVC, of course, does not understand and generates arrays on stack.

gcc with set2, of course, continues to enjoy a juggling. Doubling or tripling up vs last time's performance.
Inner loop:
.L3:
	vmovupd	(%rdx,%rax), %ymm1
	vmovupd	32(%rdx,%rax), %ymm0
	vmovupd	32(%r8,%rax), %ymm5
	vperm2f128	$49, %ymm1, %ymm0, %ymm3
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vpermpd	$221, %ymm0, %ymm10
	vpermpd	$136, %ymm0, %ymm1
	vmovupd	(%r8,%rax), %ymm0
	vpermpd	$136, %ymm3, %ymm9
	vperm2f128	$49, %ymm5, %ymm0, %ymm2
	vinsertf128	$1, %xmm5, %ymm0, %ymm0
	vpermpd	$40, %ymm2, %ymm11
	vpermpd	$125, %ymm0, %ymm5
	vpermpd	$221, %ymm3, %ymm3
	vpermpd	$40, %ymm0, %ymm0
	vpermpd	$125, %ymm2, %ymm2
	addq	$64, %rax
	vfmadd231pd	%ymm2, %ymm3, %ymm8
	vfmadd231pd	%ymm11, %ymm9, %ymm6
	vfmadd231pd	%ymm5, %ymm10, %ymm7
	vfmadd231pd	%ymm0, %ymm1, %ymm4
	cmpq	%rax, %rcx
	jne	.L3

But this time gcc with set1 was a real star of the show. My only reaction is "What?"
.L4:
	vmovupd	0(%r13), %ymm5
	vmovupd	64(%r13), %ymm7
	vmovupd	192(%r13), %ymm4
	vmovupd	128(%r13), %ymm6
	vunpcklpd	32(%r13), %ymm5, %ymm13
	vunpckhpd	32(%r13), %ymm5, %ymm12
	vunpckhpd	96(%r13), %ymm7, %ymm1
	vunpcklpd	96(%r13), %ymm7, %ymm5
	vmovupd	128(%r13), %ymm7
	vunpcklpd	224(%r13), %ymm4, %ymm2
	vunpcklpd	160(%r13), %ymm6, %ymm6
	vunpckhpd	160(%r13), %ymm7, %ymm11
	vunpckhpd	224(%r13), %ymm4, %ymm0
	vpermpd	$216, %ymm13, %ymm13
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm5, %ymm5
	vunpcklpd	%ymm2, %ymm6, %ymm4
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm11, %ymm11
	vunpcklpd	%ymm5, %ymm13, %ymm9
	vpermpd	$216, %ymm12, %ymm12
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm4, %ymm3
	vpermpd	$216, %ymm9, %ymm9
	vunpckhpd	%ymm2, %ymm6, %ymm4
	vunpckhpd	%ymm5, %ymm13, %ymm5
	vunpcklpd	%ymm1, %ymm12, %ymm6
	vunpcklpd	%ymm0, %ymm11, %ymm2
	vunpckhpd	%ymm1, %ymm12, %ymm12
	vunpckhpd	%ymm0, %ymm11, %ymm0
	vpermpd	$216, %ymm12, %ymm1
	vunpcklpd	%ymm3, %ymm9, %ymm11
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm0, %ymm0
	vmovupd	64(%r12), %ymm15
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm11, %ymm8
	vpermpd	$216, %ymm2, %ymm2
	vunpcklpd	%ymm4, %ymm5, %ymm11
	vunpckhpd	%ymm3, %ymm9, %ymm9
	vunpckhpd	%ymm4, %ymm5, %ymm4
	vunpcklpd	%ymm0, %ymm1, %ymm5
	vpermpd	$216, %ymm9, %ymm3
	vunpcklpd	%ymm2, %ymm6, %ymm9
	vunpckhpd	%ymm2, %ymm6, %ymm2
	vpermpd	$216, %ymm5, %ymm6
	vunpcklpd	96(%r12), %ymm15, %ymm12
	vunpckhpd	%ymm0, %ymm1, %ymm0
	vmovupd	%ymm6, 64(%rsp)
	vunpckhpd	96(%r12), %ymm15, %ymm6
	vmovupd	128(%r12), %ymm15
	vpermpd	$216, %ymm0, %ymm5
	vpermpd	$216, %ymm9, %ymm7
	vmovupd	(%r12), %ymm0
	vunpckhpd	160(%r12), %ymm15, %ymm9
	vmovupd	%ymm5, 96(%rsp)
	vunpcklpd	160(%r12), %ymm15, %ymm5
	vmovupd	192(%r12), %ymm15
	vunpcklpd	32(%r12), %ymm0, %ymm1
	vpermpd	$216, %ymm9, %ymm14
	vunpcklpd	224(%r12), %ymm15, %ymm9
	vunpckhpd	224(%r12), %ymm15, %ymm13
	vunpckhpd	32(%r12), %ymm0, %ymm0
	vpermpd	$216, %ymm12, %ymm12
	vpermpd	$216, %ymm9, %ymm9
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm6, %ymm6
	vunpcklpd	%ymm12, %ymm1, %ymm10
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm13, %ymm13
	vunpckhpd	%ymm12, %ymm1, %ymm1
	vunpcklpd	%ymm9, %ymm5, %ymm12
	vpermpd	$216, %ymm12, %ymm12
	vpermpd	$216, %ymm10, %ymm10
	vunpckhpd	%ymm9, %ymm5, %ymm5
	vunpcklpd	%ymm6, %ymm0, %ymm9
	vunpckhpd	%ymm6, %ymm0, %ymm0
	vunpcklpd	%ymm13, %ymm14, %ymm6
	vunpckhpd	%ymm13, %ymm14, %ymm13
	vpermpd	$216, %ymm13, %ymm14
	vunpcklpd	%ymm12, %ymm10, %ymm13
	vpermpd	$216, %ymm13, %ymm13
	vmulpd	%ymm13, %ymm8, %ymm15
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm9, %ymm9
	vpermpd	$216, %ymm0, %ymm0
	vunpckhpd	%ymm12, %ymm10, %ymm10
	vunpcklpd	%ymm6, %ymm9, %ymm12
	vunpckhpd	%ymm6, %ymm9, %ymm9
	vunpcklpd	%ymm5, %ymm1, %ymm6
	vunpckhpd	%ymm5, %ymm1, %ymm1
	vunpcklpd	%ymm14, %ymm0, %ymm5
	vunpckhpd	%ymm14, %ymm0, %ymm0
	vpermpd	$216, %ymm0, %ymm0
	vmovupd	%ymm0, 160(%rsp)
	vmovq	%r9, %xmm0
	vaddsd	%xmm15, %xmm0, %xmm0
	vunpckhpd	%xmm15, %xmm15, %xmm14
	vpermpd	$216, %ymm10, %ymm10
	vaddsd	%xmm14, %xmm0, %xmm0
	vextractf128	$0x1, %ymm15, %xmm14
	vmulpd	%ymm10, %ymm8, %ymm8
	vaddsd	%xmm14, %xmm0, %xmm15
	vunpckhpd	%xmm14, %xmm14, %xmm14
	vpermpd	$216, %ymm12, %ymm12
	vaddsd	%xmm14, %xmm15, %xmm0
	vmulpd	%ymm10, %ymm3, %ymm15
	vunpckhpd	%xmm8, %xmm8, %xmm10
	vmovq	%xmm0, %r9
	vmovq	%rcx, %xmm0
	vmulpd	%ymm13, %ymm3, %ymm3
	vaddsd	%xmm15, %xmm0, %xmm0
	vunpckhpd	%xmm15, %xmm15, %xmm14
	vextractf128	$0x1, %ymm15, %xmm15
	vaddsd	%xmm14, %xmm0, %xmm14
	vpermpd	$216, %ymm1, %ymm1
	vmovupd	%ymm1, 128(%rsp)
	vaddsd	%xmm15, %xmm14, %xmm14
	vunpckhpd	%xmm15, %xmm15, %xmm15
	vpermpd	$216, %ymm2, %ymm2
	vaddsd	%xmm15, %xmm14, %xmm0
	vmovsd	56(%rsp), %xmm14
	vpermpd	$216, %ymm9, %ymm9
	vaddsd	%xmm8, %xmm14, %xmm14
	vextractf128	$0x1, %ymm8, %xmm8
	vmovq	%xmm0, %rcx
	vaddsd	%xmm10, %xmm14, %xmm10
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm11, %ymm11
	vaddsd	%xmm8, %xmm10, %xmm10
	vunpckhpd	%xmm8, %xmm8, %xmm8
	vpermpd	$216, %ymm4, %ymm4
	vaddsd	%xmm8, %xmm10, %xmm0
	vmovsd	48(%rsp), %xmm10
	vunpckhpd	%xmm3, %xmm3, %xmm8
	vaddsd	%xmm3, %xmm10, %xmm10
	vextractf128	$0x1, %ymm3, %xmm3
	vmovsd	%xmm0, 56(%rsp)
	vaddsd	%xmm8, %xmm10, %xmm8
	vmulpd	%ymm12, %ymm7, %ymm10
	vmulpd	%ymm9, %ymm7, %ymm7
	vaddsd	%xmm3, %xmm8, %xmm8
	vunpckhpd	%xmm3, %xmm3, %xmm3
	vpermpd	$216, %ymm5, %ymm5
	vaddsd	%xmm3, %xmm8, %xmm0
	vunpckhpd	%xmm10, %xmm10, %xmm3
	addq	$256, %r12
	vmovsd	%xmm0, 48(%rsp)
	vmovq	%rdi, %xmm0
	vaddsd	%xmm10, %xmm0, %xmm8
	vextractf128	$0x1, %ymm10, %xmm10
	vmovq	%rbx, %xmm0
	vaddsd	%xmm3, %xmm8, %xmm3
	vmulpd	%ymm9, %ymm2, %ymm8
	vmulpd	%ymm12, %ymm2, %ymm2
	vaddsd	%xmm10, %xmm3, %xmm3
	vunpckhpd	%xmm10, %xmm10, %xmm10
	addq	$256, %r13
	vaddsd	%xmm10, %xmm3, %xmm1
	vaddsd	%xmm8, %xmm0, %xmm10
	vunpckhpd	%xmm8, %xmm8, %xmm3
	vextractf128	$0x1, %ymm8, %xmm8
	vaddsd	%xmm3, %xmm10, %xmm3
	vmovq	%xmm1, %rdi
	vmovq	%r11, %xmm1
	vaddsd	%xmm8, %xmm3, %xmm3
	vunpckhpd	%xmm8, %xmm8, %xmm8
	vmovq	%r10, %xmm0
	vaddsd	%xmm8, %xmm3, %xmm3
	vmovsd	40(%rsp), %xmm8
	vaddsd	%xmm7, %xmm8, %xmm8
	vmovq	%xmm3, %rbx
	vunpckhpd	%xmm7, %xmm7, %xmm3
	vaddsd	%xmm3, %xmm8, %xmm3
	vextractf128	$0x1, %ymm7, %xmm7
	vaddsd	%xmm7, %xmm3, %xmm3
	vunpckhpd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm3, %xmm3
	vmovsd	32(%rsp), %xmm7
	vaddsd	%xmm2, %xmm7, %xmm7
	vmovsd	%xmm3, 40(%rsp)
	vunpckhpd	%xmm2, %xmm2, %xmm3
	vaddsd	%xmm3, %xmm7, %xmm3
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm6, %ymm11, %ymm7
	vaddsd	%xmm2, %xmm3, %xmm3
	vunpckhpd	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vaddsd	%xmm7, %xmm1, %xmm3
	vmovupd	128(%rsp), %ymm1
	vmovsd	%xmm2, 32(%rsp)
	vunpckhpd	%xmm7, %xmm7, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm1, %ymm4, %ymm3
	vaddsd	%xmm7, %xmm2, %xmm2
	vunpckhpd	%xmm7, %xmm7, %xmm7
	vmulpd	%ymm1, %ymm11, %ymm1
	vaddsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm0, %xmm7
	vmulpd	%ymm6, %ymm4, %ymm4
	vmovq	%xmm2, %r11
	vunpckhpd	%xmm3, %xmm3, %xmm2
	vaddsd	%xmm2, %xmm7, %xmm2
	vextractf128	$0x1, %ymm3, %xmm3
	vmovupd	64(%rsp), %ymm6
	vaddsd	%xmm3, %xmm2, %xmm2
	vunpckhpd	%xmm3, %xmm3, %xmm3
	vmovupd	96(%rsp), %ymm7
	vaddsd	%xmm3, %xmm2, %xmm2
	vmovsd	24(%rsp), %xmm3
	vmovupd	160(%rsp), %ymm0
	vaddsd	%xmm1, %xmm3, %xmm3
	vmovq	%xmm2, %r10
	vunpckhpd	%xmm1, %xmm1, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vextractf128	$0x1, %ymm1, %xmm1
	vmovq	%rbp, %xmm3
	vaddsd	%xmm1, %xmm2, %xmm2
	vunpckhpd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vunpckhpd	%xmm4, %xmm4, %xmm1
	vmovsd	%xmm2, 24(%rsp)
	vmovsd	16(%rsp), %xmm2
	vaddsd	%xmm4, %xmm2, %xmm2
	vextractf128	$0x1, %ymm4, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vunpckhpd	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm1, %xmm4
	vmovsd	%xmm4, 16(%rsp)
	vmulpd	%ymm6, %ymm5, %ymm4
	vmulpd	%ymm7, %ymm5, %ymm5
	vaddsd	%xmm4, %xmm3, %xmm1
	vunpckhpd	%xmm4, %xmm4, %xmm2
	vmovq	%rsi, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm2
	vextractf128	$0x1, %ymm4, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vunpckhpd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm4
	vmovq	%xmm4, %rbp
	vmulpd	%ymm0, %ymm7, %ymm4
	vmulpd	%ymm0, %ymm6, %ymm0
	vaddsd	%xmm4, %xmm3, %xmm1
	vunpckhpd	%xmm4, %xmm4, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm2
	vextractf128	$0x1, %ymm4, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vunpckhpd	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm4
	vmovsd	8(%rsp), %xmm2
	vunpckhpd	%xmm0, %xmm0, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vextractf128	$0x1, %ymm0, %xmm0
	vmovq	%xmm4, %rsi
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm1
	vunpckhpd	%xmm0, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm6
	vmovsd	(%rsp), %xmm1
	vunpckhpd	%xmm5, %xmm5, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm1
	vextractf128	$0x1, %ymm5, %xmm5
	vmovsd	%xmm6, 8(%rsp)
	vaddsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm5, %xmm0, %xmm0
	vunpckhpd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm5
	vmovsd	%xmm5, (%rsp)
	cmpq	%rax, %r12
	jne	.L4
	movl	%r15d, %r12d
	andl	$-4, %r12d
	movl	%r12d, %edx
	cmpl	%r12d, %r15d
	je	.L5
.L3:
	movl	%r15d, %eax
	subl	%r12d, %eax
	cmpl	$1, %eax
	je	.L6
	salq	$6, %r12
	leaq	(%r14,%r12), %r13
	vmovupd	16(%r13), %xmm3
	vmovupd	48(%r13), %xmm0
	vmovupd	64(%r13), %xmm8
	vmovupd	112(%r13), %xmm10
	vmovupd	0(%r13), %xmm4
	vmovupd	32(%r13), %xmm2
	vmovupd	80(%r13), %xmm6
	vmovupd	96(%r13), %xmm1
	vunpcklpd	%xmm3, %xmm4, %xmm5
	vunpckhpd	%xmm3, %xmm4, %xmm4
	vunpcklpd	%xmm0, %xmm2, %xmm3
	vunpckhpd	%xmm0, %xmm2, %xmm2
	vunpcklpd	%xmm6, %xmm8, %xmm0
	vunpckhpd	%xmm6, %xmm8, %xmm6
	vunpcklpd	%xmm10, %xmm1, %xmm8
	vunpckhpd	%xmm10, %xmm1, %xmm1
	vunpcklpd	%xmm3, %xmm5, %xmm11
	vunpcklpd	%xmm2, %xmm4, %xmm10
	vunpckhpd	%xmm3, %xmm5, %xmm3
	vunpckhpd	%xmm2, %xmm4, %xmm2
	vunpcklpd	%xmm8, %xmm0, %xmm5
	vunpcklpd	%xmm1, %xmm6, %xmm4
	vunpckhpd	%xmm8, %xmm0, %xmm0
	vunpckhpd	%xmm1, %xmm6, %xmm1
	addq	%r8, %r12
	vunpcklpd	%xmm5, %xmm11, %xmm8
	vunpckhpd	%xmm0, %xmm3, %xmm7
	vunpckhpd	%xmm5, %xmm11, %xmm11
	vunpckhpd	%xmm1, %xmm2, %xmm5
	vmovupd	64(%r12), %xmm12
	vunpcklpd	%xmm1, %xmm2, %xmm6
	vmovupd	80(%r12), %xmm9
	vmovupd	48(%r12), %xmm1
	vmovupd	96(%r12), %xmm2
	vunpcklpd	%xmm4, %xmm10, %xmm14
	vunpcklpd	%xmm0, %xmm3, %xmm13
	vunpckhpd	%xmm4, %xmm10, %xmm10
	vmovupd	32(%r12), %xmm3
	vmovupd	16(%r12), %xmm4
	vmovapd	%xmm7, 64(%rsp)
	vmovapd	%xmm5, 96(%rsp)
	vmovupd	112(%r12), %xmm7
	vmovupd	(%r12), %xmm5
	movl	%eax, %r12d
	vunpcklpd	%xmm4, %xmm5, %xmm15
	vunpckhpd	%xmm4, %xmm5, %xmm5
	vunpcklpd	%xmm1, %xmm3, %xmm4
	vunpckhpd	%xmm1, %xmm3, %xmm3
	vunpcklpd	%xmm9, %xmm12, %xmm1
	vunpckhpd	%xmm9, %xmm12, %xmm9
	vunpcklpd	%xmm7, %xmm2, %xmm12
	vunpckhpd	%xmm7, %xmm2, %xmm2
	vunpcklpd	%xmm4, %xmm15, %xmm7
	vunpckhpd	%xmm4, %xmm15, %xmm15
	vunpcklpd	%xmm12, %xmm1, %xmm4
	vunpckhpd	%xmm12, %xmm1, %xmm1
	vunpcklpd	%xmm3, %xmm5, %xmm12
	vunpckhpd	%xmm3, %xmm5, %xmm5
	vunpcklpd	%xmm2, %xmm9, %xmm3
	vunpckhpd	%xmm2, %xmm9, %xmm2
	vunpcklpd	%xmm4, %xmm7, %xmm9
	vunpckhpd	%xmm1, %xmm15, %xmm0
	vunpckhpd	%xmm4, %xmm7, %xmm4
	vunpcklpd	%xmm3, %xmm12, %xmm7
	vunpckhpd	%xmm3, %xmm12, %xmm3
	vunpcklpd	%xmm1, %xmm15, %xmm12
	vunpcklpd	%xmm2, %xmm5, %xmm15
	vunpckhpd	%xmm2, %xmm5, %xmm2
	vmulpd	%xmm9, %xmm8, %xmm5
	vmovapd	%xmm0, 128(%rsp)
	vmovq	%r9, %xmm0
	andl	$-2, %r12d
	addl	%r12d, %edx
	vaddsd	%xmm5, %xmm0, %xmm0
	vunpckhpd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm1
	vmulpd	%xmm4, %xmm11, %xmm5
	vmulpd	%xmm4, %xmm8, %xmm4
	vmovq	%xmm1, %r9
	vmovq	%rcx, %xmm1
	vmulpd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm5, %xmm1, %xmm1
	vunpckhpd	%xmm5, %xmm5, %xmm5
	vmulpd	%xmm7, %xmm14, %xmm9
	vaddsd	%xmm5, %xmm1, %xmm1
	vmovsd	56(%rsp), %xmm5
	vmulpd	%xmm3, %xmm10, %xmm8
	vaddsd	%xmm4, %xmm5, %xmm5
	vunpckhpd	%xmm4, %xmm4, %xmm4
	vmovq	%xmm1, %rcx
	vaddsd	%xmm4, %xmm5, %xmm4
	vmovq	%rdi, %xmm1
	vmulpd	%xmm3, %xmm14, %xmm14
	vmovsd	%xmm4, 56(%rsp)
	vmovsd	48(%rsp), %xmm4
	vmovq	%rbx, %xmm0
	vaddsd	%xmm11, %xmm4, %xmm4
	vunpckhpd	%xmm11, %xmm11, %xmm11
	vmovsd	40(%rsp), %xmm3
	vaddsd	%xmm11, %xmm4, %xmm4
	vmulpd	%xmm7, %xmm10, %xmm10
	vaddsd	%xmm14, %xmm3, %xmm3
	vmovsd	%xmm4, 48(%rsp)
	vaddsd	%xmm9, %xmm1, %xmm4
	vunpckhpd	%xmm9, %xmm9, %xmm9
	vunpckhpd	%xmm14, %xmm14, %xmm14
	vaddsd	%xmm9, %xmm4, %xmm4
	vmovapd	128(%rsp), %xmm5
	vmovapd	64(%rsp), %xmm11
	vmovq	%xmm4, %rdi
	vaddsd	%xmm8, %xmm0, %xmm4
	vunpckhpd	%xmm8, %xmm8, %xmm8
	vmovsd	24(%rsp), %xmm1
	vaddsd	%xmm8, %xmm4, %xmm4
	vmovsd	16(%rsp), %xmm0
	vmovq	%xmm4, %rbx
	vaddsd	%xmm14, %xmm3, %xmm4
	vmovsd	32(%rsp), %xmm3
	vaddsd	%xmm10, %xmm3, %xmm3
	vunpckhpd	%xmm10, %xmm10, %xmm10
	vmovsd	%xmm4, 40(%rsp)
	vaddsd	%xmm10, %xmm3, %xmm7
	vmulpd	%xmm12, %xmm13, %xmm3
	vmulpd	%xmm5, %xmm13, %xmm13
	vmovsd	%xmm7, 32(%rsp)
	vmovq	%r11, %xmm7
	vmulpd	%xmm11, %xmm12, %xmm12
	vaddsd	%xmm3, %xmm7, %xmm4
	vunpckhpd	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm13, %xmm1, %xmm1
	vaddsd	%xmm3, %xmm4, %xmm7
	vmulpd	%xmm5, %xmm11, %xmm3
	vunpckhpd	%xmm13, %xmm13, %xmm13
	vmovq	%xmm7, %r11
	vmovq	%r10, %xmm7
	vaddsd	%xmm12, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm7, %xmm4
	vunpckhpd	%xmm3, %xmm3, %xmm3
	vunpckhpd	%xmm12, %xmm12, %xmm12
	vaddsd	%xmm3, %xmm4, %xmm7
	vaddsd	%xmm13, %xmm1, %xmm4
	vmovq	%xmm7, %r10
	vmovsd	%xmm4, 24(%rsp)
	vaddsd	%xmm12, %xmm0, %xmm4
	vmulpd	%xmm15, %xmm6, %xmm0
	vmovq	%rbp, %xmm7
	vmovsd	%xmm4, 16(%rsp)
	vmovapd	96(%rsp), %xmm5
	vaddsd	%xmm0, %xmm7, %xmm1
	vunpckhpd	%xmm0, %xmm0, %xmm0
	vmovq	%rsi, %xmm7
	vaddsd	%xmm0, %xmm1, %xmm4
	vmulpd	%xmm5, %xmm2, %xmm0
	vmulpd	%xmm2, %xmm6, %xmm2
	vmovq	%xmm4, %rbp
	vmulpd	%xmm5, %xmm15, %xmm15
	vaddsd	%xmm0, %xmm7, %xmm1
	vunpckhpd	%xmm0, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm4
	vmovsd	8(%rsp), %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vunpckhpd	%xmm2, %xmm2, %xmm2
	vmovq	%xmm4, %rsi
	vaddsd	%xmm2, %xmm0, %xmm6
	vmovsd	(%rsp), %xmm0
	vaddsd	%xmm15, %xmm0, %xmm0
	vunpckhpd	%xmm15, %xmm15, %xmm15
	vmovsd	%xmm6, 8(%rsp)
	vaddsd	%xmm15, %xmm0, %xmm5
	vmovsd	%xmm5, (%rsp)
	cmpl	%r12d, %eax
	je	.L5


---


### compiler : `gcc`
### title : `gcc.dg/vect/bb-slp-subgroups-3.c bad vectorization with AVX`
### open_at : `2020-10-09T10:54:44Z`
### last_modified_date : `2021-09-27T08:25:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97351
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int __attribute__((__aligned__(8))) a[8];
int __attribute__((__aligned__(8))) b[8];

void
test ()
{
    a[0] = b[0] + 1;
    a[1] = b[1] + 2;
    a[2] = b[2] + 3;
    a[3] = b[3] + 4;
    a[4] = b[0] * 3;
    a[5] = b[2] * 4;
    a[6] = b[4] * 5;
    a[7] = b[6] * 7;
}

should be vectorized using V4SI vectors in two SLP groups so we can
vectorize not only the store but also the loads and the add.  When
using -mavx2 we instead get only the store vectorized (even with
cost modeling enabled) because we try vectorizing that first.

It might be possible to guide SLP splitting during the SLP build
in a similar way how we try commutating operands. So when we figure

/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-subgroups-3.c:12:10: note:   Build SLP for _9 = _1 * 3;
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-subgroups-3.c:12:10: note:   get vectype for scalar type (group size 8): int
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-subgroups-3.c:12:10: note:   vectype: vector(8) int
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-subgroups-3.c:12:10: note:   nunits = 8
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-subgroups-3.c:12:10: missed:   Build SLP failed: different operation in stmt _9 = _1 * 3;
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-subgroups-3.c:12:10: missed:   original stmt _2 = _1 + 1;

and see the parent op (the store in this case) cannot be commutated we
can see whether matches[] divides the vector with some constraints
and whether the other lanes with matches[] == false form a valid SLP
operand (we know the == true ones likely would).  The results would then
be concatenated via a permute node.

This should eventually also replace the splitting done in SLP instance
analysis (though splitting stores might still be necessary there).

The other option is to somehow tackle this with vector size iteration,
doing multiple analyses and comparing costs/benefit though it's hard
to not compare apples & oranges since the amount of code vectorized will
usually differ (as compared to loop vectorization)


---


### compiler : `gcc`
### title : `gcc.dg/vect/bb-slp-pr78205.c fails to vectorize all opportunities with AVX`
### open_at : `2020-10-09T11:05:34Z`
### last_modified_date : `2021-09-27T08:25:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97352
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
When using AVX vectors

double x[2], a[4], b[4], c[5];

void foo ()
{
  a[0] = c[0];
  a[1] = c[1];
  a[2] = c[0];
  a[3] = c[1];
  b[0] = c[2];
  b[1] = c[3];
  b[2] = c[2];
  b[3] = c[3];
  x[0] = c[4];
  x[1] = c[4];
}

only vectorizes the x[] stores since the overall SLP analysis succeeds with
V4DFmode but only parts of the opportunities are finally vectorized and thus
SLP vectorization with V2DFmode isn't even tried.

This is the issue that vector mode iteration works on wrong granularity.

/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr78205.c:17:8: note:   === vect_slp_analyze_instance_dependence ===
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr78205.c:9:8: note:   === vect_slp_analyze_instance_alignment ===
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr78205.c:9:8: note:  removing SLP instance operations starting from: a[0] = _1;
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr78205.c:13:8: note:   === vect_slp_analyze_instance_alignment ===
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr78205.c:13:8: note:  removing SLP instance operations starting from: b[0] = _3;
/home/rguenther/src/gcc3/gcc/testsuite/gcc.dg/vect/bb-slp-pr78205.c:13:8: note:   === vect_slp_analyze_operations ===

and the failure is because of the now late performed

/* Analyze alignment of DRs of stmts in NODE.  */

static bool
vect_slp_analyze_node_alignment (vec_info *vinfo, slp_tree node)
{
...
  /* We need to commit to a vector type for the group now.  */
  if (is_a <bb_vec_info> (vinfo)
      && !vect_update_shared_vectype (first_stmt_info, SLP_TREE_VECTYPE (node)))
    return false;

because the other SLP instance (with x[] stores) set the vector type to V2DF
while this one wants V4DF.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Redundant load with SSE/AVX vector intrinsics`
### open_at : `2020-10-11T06:41:19Z`
### last_modified_date : `2023-07-07T10:38:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97366
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
When you use the same _mm_load_si128 or _mm256_load_si256 result twice, sometimes GCC loads it *and* uses it as a memory source operand.

I'm not certain this is specific to x86 back-ends, please check bug tags if it happens elsewhere.  (But it probably doesn't on 3-operand load/store RISC machines; it looks like one operation chooses to load and then operate, the other chooses to use the original source as a memory operand.)

#include <immintrin.h>
void gcc_double_load_128(int8_t *__restrict out, const int8_t *__restrict input)
{
    for (unsigned i=0 ; i<1024 ; i+=16){
        __m128i in = _mm_load_si128((__m128i*)&input[i]);
        __m128i high = _mm_srli_epi32(in, 4);
        _mm_store_si128((__m128i*)&out[i], _mm_or_si128(in,high));
    }
}

gcc 8 and later -O3 -mavx2, including 11.0.0 20200920, with 

gcc_double_load_128(signed char*, signed char const*):
        xorl    %eax, %eax
.L6:
        vmovdqa (%rsi,%rax), %xmm1         # load
        vpsrld  $4, %xmm1, %xmm0
        vpor    (%rsi,%rax), %xmm0, %xmm0  # reload as a memory operand
        vmovdqa %xmm0, (%rdi,%rax)
        addq    $16, %rax
        cmpq    $1024, %rax
        jne     .L6
        ret

GCC7.5 and earlier use  vpor %xmm1, %xmm0, %xmm0 to use the copy of the original that was already loaded.

`-march=haswell` happens to fix this for GCC trunk, for this 128-bit version but not for a __m256i version.

restrict doesn't make a difference, and there's no overlapping anyway.  The two redundant loads both happen between any other stores.

Using a memory source operand for vpsrld wasn't an option: the form with a memory source takes the *count* from  memory, not the data.  https://www.felixcloutier.com/x86/psllw:pslld:psllq

----

Note that *without* AVX, the redundant load is a possible win, for code running on Haswell and later Intel (and AMD) CPUs.  Possibly some heuristic is saving instructions for the legacy-SSE case (in a way that's probably worse overall) and hurting the AVX case.

GCC 7.5, -O3  without any -m options
gcc_double_load_128(signed char*, signed char const*):
        xorl    %eax, %eax
.L2:
        movdqa  (%rsi,%rax), %xmm0
        movdqa  %xmm0, %xmm1         # this instruction avoided
        psrld   $4, %xmm1
        por     %xmm1, %xmm0         # with a memory source reload, in GCC8 and later
        movaps  %xmm0, (%rdi,%rax)
        addq    $16, %rax
        cmpq    $1024, %rax
        jne     .L2
        rep ret


Using a memory-source POR saves 1 front-end uop by avoiding a register-copy, as long as the indexed addressing mode can stay micro-fused on Intel.  (Requires Haswell or later for that to happen, or any AMD.)  But in practice it's probably worse.  Load-port pressure, and space in the out-of-order scheduler, as well as code-size, is a problem for using an extra memory-source operand in the SSE version, with the upside being saving 1 uop for the front-end.  (And thus in the ROB.)  mov-elimination on modern CPUs means the movdqa register copy costs no back-end resources (ivybridge and bdver1).

I don't know if GCC trunk is using por  (%rsi,%rax), %xmm0  on purpose for that reason, of if it's just a coincidence.
I don't think it's a good idea on most CPUs, even if alignment is guaranteed.

This is of course 100% a loss with AVX; we have to `vmovdqa/u` load for the shift, and it can leave the original value in a register so we're not saving a vmovdqua.  And it's a bigger loss because indexed memory-source operands unlaminate from 3-operand instructions even on Haswell/Skylake: https://stackoverflow.com/questions/26046634/micro-fusion-and-addressing-modes/31027695#31027695 so it hurts the front-end as well as wasting cycles on load ports, and taking up space in the RS (scheduler).

The fact that -mtune=haswell fixes this for 128-bit vectors is interesting, but it's clearly still a loss in the AVX version for all AVX CPUs.  2 memory ops / cycle on Zen could become a bottleneck, and it's larger code size.  And -mtune=haswell *doesn't* fix it for the -mavx2 _m256i version.

There is a possible real advantage in the SSE case, but it's very minor and outweighed by disadvantages.  Especially for older CPUs like Nehalem that can only do 1 load / 1 store per clock.  (Although this has so many uops in the loop that it barely bottlenecks on that.)


---


### compiler : `gcc`
### title : `we are near 2021, add carry intrinsic still does the wrong thing and generates silly code.`
### open_at : `2020-10-12T15:33:56Z`
### last_modified_date : `2023-06-04T00:16:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97387
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
#include <stdint.h>
#include <x86intrin.h>

void add256(uint64_t a[4], uint64_t b[4]){
  uint8_t carry = 0;
  for (int i = 0; i < 4; ++i)
    carry = _addcarry_u64(carry, a[i], b[i], &a[i]);
}

People have reported the issue for many many times but the carry is still buggy.

https://gcc.godbolt.org/z/TnM8cv


---


### compiler : `gcc`
### title : `RISC-V Unnecessary andi instruction when loading volatile bool`
### open_at : `2020-10-14T11:56:11Z`
### last_modified_date : `2021-02-13T20:48:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97417
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
When loading a volatile bool value I see that the compiler adds unnecessary andi instruction. Here is reproducer:

#include <stdbool.h>

extern volatile bool active;

int foo(void)
{
  if (!active) {
    return 42;
  } else {
    return -42;
  }
}

And the code generated looks like this:

foo:
        lui     a5,%hi(active)
        lbu     a5,%lo(active)(a5)
        li      a0,42
        andi    a5,a5,0xff
        beq     a5,zero,.L1
        li      a0,-42
.L1:
        ret

The andi instruction seems to be unnecessary since lbu is zero extending the byte.

ref. https://github.com/riscv/riscv-gnu-toolchain/issues/737


---


### compiler : `gcc`
### title : `-O3 is great for basic AoSoA packing of complex arrays, but horrible one step above the basic`
### open_at : `2020-10-14T19:24:59Z`
### last_modified_date : `2020-10-16T12:38:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97428
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
That my next example of bad handling of AoSoA layout by gcc optimizer/vectorizer.
For discussion of AoSoA see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97343

The issue in hand is transformation (packing) of complex AoS numbers into AoSoA format.
Compiler used: gcc 10.2
Target: AVX2 (Skylake)

Part 1.
typedef struct { double re, im; } dcmlx_t;
typedef struct { double re[4], im[4]; } dcmlx4_t;

void foo(dcmlx4_t dst[], const dcmlx_t src[], int n)
{
  for (int i = 0; i < n; ++i) {
    dcmlx_t s00 = src[i*4+0];
    dcmlx_t s01 = src[i*4+1];
    dcmlx_t s02 = src[i*4+2];
    dcmlx_t s03 = src[i*4+3];

    dcmlx_t s10 = src[i*4+0+n];
    dcmlx_t s11 = src[i*4+1+n];
    dcmlx_t s12 = src[i*4+2+n];
    dcmlx_t s13 = src[i*4+3+n];

    dst[i*2+0].re[0] = s00.re;
    dst[i*2+0].re[1] = s01.re;
    dst[i*2+0].re[2] = s02.re;
    dst[i*2+0].re[3] = s03.re;
    dst[i*2+0].im[0] = s00.im;
    dst[i*2+0].im[1] = s01.im;
    dst[i*2+0].im[2] = s02.im;
    dst[i*2+0].im[3] = s03.im;

    dst[i*2+1].re[0] = s10.re;
    dst[i*2+1].re[1] = s11.re;
    dst[i*2+1].re[2] = s12.re;
    dst[i*2+1].re[3] = s13.re;
    dst[i*2+1].im[0] = s10.im;
    dst[i*2+1].im[1] = s11.im;
    dst[i*2+1].im[2] = s12.im;
    dst[i*2+1].im[3] = s13.im;
  }
}

-march=skylake -O2 produces following inner loop:
.L3:
	vmovsd	(%rdx), %xmm7
	vmovsd	8(%rdx), %xmm3
	vmovsd	16(%rdx), %xmm6
	vmovsd	24(%rdx), %xmm2
	vmovsd	32(%rdx), %xmm5
	vmovsd	40(%rdx), %xmm1
	vmovsd	48(%rdx), %xmm4
	vmovsd	56(%rdx), %xmm0
	addq	$64, %rdx
	vmovsd	%xmm7, (%rcx)
	vmovsd	%xmm6, 8(%rcx)
	vmovsd	%xmm5, 16(%rcx)
	vmovsd	%xmm4, 24(%rcx)
	vmovsd	%xmm3, 32(%rcx)
	vmovsd	%xmm2, 40(%rcx)
	vmovsd	%xmm1, 48(%rcx)
	vmovsd	%xmm0, 56(%rcx)
	addq	$64, %rcx
	cmpq	%rax, %rdx
	jne	.L3


Quite reasonable for non-vectorizing  optimization level. It's possible to save
one instruction by using indexed addressing, but in majority of situations it wouldn't be faster.

-march=skylake -O3 inner loop:
.L3:
	vmovupd	(%rdx,%rax), %ymm0
	vmovupd	32(%rdx,%rax), %ymm2
	vunpcklpd	%ymm2, %ymm0, %ymm1
	vunpckhpd	%ymm2, %ymm0, %ymm0
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm0, %ymm0
	vmovupd	%ymm1, (%rcx,%rax)
	vmovupd	%ymm0, 32(%rcx,%rax)
	addq	$64, %rax
	cmpq	%r8, %rax
	jne	.L3

That's excellent. It's not only looks better. According to my measurement, for
source array in external memory and destination in L1/L2 cache it's actually
~1.5x faster than -O2, which is not a small fit.

Part 2.
A little more involved case. Now we want to interleave 2 lines of source matrix.
Sometimes it's desirable to have interleaved layout, because it improves locality of access for the rest of processing, and also can reduce pressure on GPRs that are used as pointers or indices.

typedef struct { double re, im; } dcmlx_t;
typedef struct { double re[4], im[4]; } dcmlx4_t;

void foo_i2(dcmlx4_t dst[], const dcmlx_t src[], int n)
{
  for (int i = 0; i < n; ++i) {
    dcmlx_t s00 = src[i*4+0];
    dcmlx_t s01 = src[i*4+1];
    dcmlx_t s02 = src[i*4+2];
    dcmlx_t s03 = src[i*4+3];

    dcmlx_t s10 = src[i*4+0+n];
    dcmlx_t s11 = src[i*4+1+n];
    dcmlx_t s12 = src[i*4+2+n];
    dcmlx_t s13 = src[i*4+3+n];

    dst[i*2+0].re[0] = s00.re;
    dst[i*2+0].re[1] = s01.re;
    dst[i*2+0].re[2] = s02.re;
    dst[i*2+0].re[3] = s03.re;
    dst[i*2+0].im[0] = s00.im;
    dst[i*2+0].im[1] = s01.im;
    dst[i*2+0].im[2] = s02.im;
    dst[i*2+0].im[3] = s03.im;

    dst[i*2+1].re[0] = s10.re;
    dst[i*2+1].re[1] = s11.re;
    dst[i*2+1].re[2] = s12.re;
    dst[i*2+1].re[3] = s13.re;
    dst[i*2+1].im[0] = s10.im;
    dst[i*2+1].im[1] = s11.im;
    dst[i*2+1].im[2] = s12.im;
    dst[i*2+1].im[3] = s13.im;
  }
}

-march=skylake -O2 produces following inner loop:
.L3:
	vmovsd	(%rdx), %xmm15
	vmovsd	8(%rdx), %xmm11
	vmovsd	16(%rdx), %xmm14
	vmovsd	24(%rdx), %xmm10
	vmovsd	32(%rdx), %xmm13
	vmovsd	40(%rdx), %xmm9
	vmovsd	48(%rdx), %xmm12
	vmovsd	56(%rdx), %xmm8
	vmovsd	(%rax), %xmm7
	vmovsd	8(%rax), %xmm3
	vmovsd	16(%rax), %xmm6
	vmovsd	24(%rax), %xmm2
	vmovsd	32(%rax), %xmm5
	vmovsd	40(%rax), %xmm1
	vmovsd	48(%rax), %xmm4
	vmovsd	56(%rax), %xmm0
	subq	$-128, %rcx
	vmovsd	%xmm15, -128(%rcx)
	vmovsd	%xmm14, -120(%rcx)
	vmovsd	%xmm13, -112(%rcx)
	vmovsd	%xmm12, -104(%rcx)
	vmovsd	%xmm11, -96(%rcx)
	vmovsd	%xmm10, -88(%rcx)
	vmovsd	%xmm9, -80(%rcx)
	vmovsd	%xmm8, -72(%rcx)
	vmovsd	%xmm7, -64(%rcx)
	vmovsd	%xmm6, -56(%rcx)
	vmovsd	%xmm5, -48(%rcx)
	vmovsd	%xmm4, -40(%rcx)
	vmovsd	%xmm3, -32(%rcx)
	vmovsd	%xmm2, -24(%rcx)
	vmovsd	%xmm1, -16(%rcx)
	vmovsd	%xmm0, -8(%rcx)
	addq	$64, %rdx
	addq	$64, %rax
	cmpq	%rcx, %r8
	jne	.L3

Once again, in absence of vectorizer it's very reasonable.
But may be, vectorizer can do better, as it did in the Part 1?

-march=skylake -O3 inner loop:
.L4:
	vmovupd	(%rcx), %ymm5
	vmovupd	64(%rcx), %ymm4
	vunpcklpd	32(%rcx), %ymm5, %ymm3
	vunpckhpd	32(%rcx), %ymm5, %ymm1
	vmovupd	128(%rcx), %ymm5
	vmovupd	192(%rcx), %ymm7
	vunpcklpd	160(%rcx), %ymm5, %ymm0
	vunpckhpd	160(%rcx), %ymm5, %ymm2
	vmovupd	192(%rcx), %ymm5
	vunpcklpd	96(%rcx), %ymm4, %ymm6
	vunpcklpd	224(%rcx), %ymm5, %ymm5
	vunpckhpd	96(%rcx), %ymm4, %ymm4
	vunpckhpd	224(%rcx), %ymm7, %ymm7
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm3, %ymm3
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	%ymm5, %ymm0, %ymm8
	vpermpd	$216, %ymm1, %ymm1
	vunpckhpd	%ymm5, %ymm0, %ymm0
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm2, %ymm2
	vunpcklpd	%ymm6, %ymm3, %ymm15
	vunpcklpd	%ymm4, %ymm1, %ymm5
	vunpckhpd	%ymm6, %ymm3, %ymm6
	vunpckhpd	%ymm4, %ymm1, %ymm1
	vpermpd	$216, %ymm0, %ymm3
	vunpcklpd	%ymm7, %ymm2, %ymm0
	vunpckhpd	%ymm7, %ymm2, %ymm2
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm0, %ymm5, %ymm13
	vunpcklpd	%ymm2, %ymm1, %ymm12
	vunpckhpd	%ymm0, %ymm5, %ymm5
	vunpckhpd	%ymm2, %ymm1, %ymm1
	vunpcklpd	%ymm3, %ymm6, %ymm0
	vmovupd	(%rdx), %ymm2
	vunpckhpd	%ymm3, %ymm6, %ymm6
	vmovupd	64(%rdx), %ymm3
	vunpcklpd	32(%rdx), %ymm2, %ymm2
	vpermpd	$216, %ymm1, %ymm4
	vunpcklpd	96(%rdx), %ymm3, %ymm1
	vmovupd	128(%rdx), %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm2, %ymm2
	vunpcklpd	%ymm1, %ymm2, %ymm2
	vunpcklpd	160(%rdx), %ymm3, %ymm1
	vmovupd	192(%rdx), %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	224(%rdx), %ymm3, %ymm3
	vmovupd	64(%rdx), %ymm7
	vpermpd	$216, %ymm3, %ymm3
	vunpcklpd	%ymm3, %ymm1, %ymm1
	vmovupd	(%rdx), %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm2, %ymm2
	vmovupd	%ymm4, (%rsp)
	vunpcklpd	%ymm1, %ymm2, %ymm2
	vunpckhpd	32(%rdx), %ymm3, %ymm4
	vunpckhpd	96(%rdx), %ymm7, %ymm1
	vmovupd	128(%rdx), %ymm3
	vmovupd	192(%rdx), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm4, %ymm4
	vunpcklpd	%ymm1, %ymm4, %ymm4
	vunpckhpd	160(%rdx), %ymm3, %ymm1
	vunpckhpd	224(%rdx), %ymm7, %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm3, %ymm3
	vunpcklpd	%ymm3, %ymm1, %ymm1
	vmovupd	(%r11), %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	32(%r11), %ymm3, %ymm9
	vmovupd	64(%r11), %ymm7
	vpermpd	$216, %ymm4, %ymm4
	vunpcklpd	%ymm1, %ymm4, %ymm4
	vunpcklpd	96(%r11), %ymm7, %ymm1
	vmovupd	128(%r11), %ymm3
	vmovupd	192(%r11), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm9, %ymm9
	vunpcklpd	%ymm1, %ymm9, %ymm9
	vunpcklpd	160(%r11), %ymm3, %ymm1
	vunpcklpd	224(%r11), %ymm7, %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm3, %ymm3
	vmovupd	64(%r11), %ymm7
	vunpcklpd	%ymm3, %ymm1, %ymm1
	vmovupd	(%r11), %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm9, %ymm9
	vunpckhpd	32(%r11), %ymm3, %ymm3
	vunpcklpd	%ymm1, %ymm9, %ymm9
	vunpckhpd	96(%r11), %ymm7, %ymm1
	vmovupd	128(%r11), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm3, %ymm3
	vunpcklpd	%ymm1, %ymm3, %ymm3
	vunpckhpd	160(%r11), %ymm7, %ymm1
	vmovupd	192(%r11), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vunpckhpd	224(%r11), %ymm7, %ymm7
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	%ymm7, %ymm1, %ymm1
	vmovupd	(%r10), %ymm7
	vpermpd	$216, %ymm15, %ymm15
	vunpcklpd	%ymm8, %ymm15, %ymm10
	vunpckhpd	%ymm8, %ymm15, %ymm15
	vunpcklpd	32(%r10), %ymm7, %ymm8
	vmovupd	64(%r10), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm3, %ymm3
	vunpcklpd	%ymm1, %ymm3, %ymm3
	vunpcklpd	96(%r10), %ymm7, %ymm1
	vmovupd	128(%r10), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm8, %ymm8
	vunpcklpd	%ymm1, %ymm8, %ymm8
	vunpcklpd	160(%r10), %ymm7, %ymm1
	vmovupd	192(%r10), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	224(%r10), %ymm7, %ymm7
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	%ymm7, %ymm1, %ymm1
	vmovupd	(%r10), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm1, %ymm8, %ymm8
	vunpckhpd	32(%r10), %ymm7, %ymm1
	vmovupd	64(%r10), %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vunpckhpd	96(%r10), %ymm7, %ymm7
	vmovupd	192(%r10), %ymm11
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	%ymm7, %ymm1, %ymm1
	vmovupd	128(%r10), %ymm7
	vunpckhpd	224(%r10), %ymm11, %ymm11
	vunpckhpd	160(%r10), %ymm7, %ymm7
	vpermpd	$216, %ymm11, %ymm11
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	%ymm11, %ymm7, %ymm7
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	%ymm7, %ymm1, %ymm7
	vmovupd	(%r9), %ymm1
	vpermpd	$216, %ymm7, %ymm7
	vmovupd	%ymm7, 32(%rsp)
	vunpcklpd	32(%r9), %ymm1, %ymm7
	vmovupd	64(%r9), %ymm1
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	96(%r9), %ymm1, %ymm1
	vmovupd	192(%r9), %ymm14
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm1, %ymm7, %ymm7
	vmovupd	128(%r9), %ymm1
	vunpcklpd	224(%r9), %ymm14, %ymm11
	vunpcklpd	160(%r9), %ymm1, %ymm1
	vpermpd	$216, %ymm11, %ymm11
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm11, %ymm1, %ymm1
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm7, %ymm7
	vmovupd	64(%r9), %ymm11
	vunpcklpd	%ymm1, %ymm7, %ymm7
	vmovupd	(%r9), %ymm1
	vunpckhpd	96(%r9), %ymm11, %ymm11
	vunpckhpd	32(%r9), %ymm1, %ymm1
	vmovupd	128(%r9), %ymm14
	vpermpd	$216, %ymm11, %ymm11
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm11, %ymm1, %ymm1
	vunpckhpd	160(%r9), %ymm14, %ymm11
	vmovupd	192(%r9), %ymm14
	vpermpd	$216, %ymm11, %ymm11
	vunpckhpd	224(%r9), %ymm14, %ymm14
	vpermpd	$216, %ymm10, %ymm10
	vpermpd	$216, %ymm14, %ymm14
	vunpcklpd	%ymm14, %ymm11, %ymm11
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm11, %ymm11
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$68, %ymm10, %ymm14
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm9, %ymm9
	vunpcklpd	%ymm11, %ymm1, %ymm1
	vpermpd	$238, %ymm10, %ymm10
	vpermpd	$68, %ymm2, %ymm11
	vpermpd	$238, %ymm2, %ymm2
	vshufpd	$12, %ymm11, %ymm14, %ymm11
	vshufpd	$12, %ymm2, %ymm10, %ymm2
	vpermpd	$216, %ymm15, %ymm15
	vpermpd	$68, %ymm0, %ymm10
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$68, %ymm9, %ymm14
	vpermpd	$238, %ymm0, %ymm0
	vpermpd	$238, %ymm9, %ymm9
	vshufpd	$12, %ymm9, %ymm0, %ymm0
	vpermpd	$216, %ymm6, %ymm6
	vmovupd	%ymm0, 64(%rsp)
	vpermpd	$68, %ymm15, %ymm9
	vpermpd	$216, %ymm7, %ymm7
	vpermpd	$68, %ymm8, %ymm0
	vshufpd	$12, %ymm14, %ymm10, %ymm14
	vshufpd	$12, %ymm0, %ymm9, %ymm0
	vpermpd	$216, %ymm13, %ymm13
	vpermpd	$68, %ymm7, %ymm9
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$238, %ymm8, %ymm8
	vpermpd	$68, %ymm6, %ymm10
	vpermpd	$238, %ymm7, %ymm7
	vpermpd	$238, %ymm6, %ymm6
	vpermpd	$238, %ymm15, %ymm15
	vshufpd	$12, %ymm8, %ymm15, %ymm15
	vshufpd	$12, %ymm9, %ymm10, %ymm10
	vshufpd	$12, %ymm7, %ymm6, %ymm8
	vmovupd	%ymm10, 128(%rsp)
	vpermpd	$68, %ymm4, %ymm6
	vpermpd	$68, %ymm13, %ymm10
	vshufpd	$12, %ymm6, %ymm10, %ymm10
	vpermpd	$216, %ymm12, %ymm12
	vmovupd	32(%rsp), %ymm6
	vpermpd	$216, %ymm3, %ymm3
	vpermpd	$238, %ymm13, %ymm13
	vpermpd	$238, %ymm4, %ymm4
	vmovupd	%ymm15, 96(%rsp)
	vmovupd	%ymm8, 160(%rsp)
	vshufpd	$12, %ymm4, %ymm13, %ymm15
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$68, %ymm3, %ymm4
	vpermpd	$68, %ymm12, %ymm8
	vpermpd	$238, %ymm3, %ymm3
	vpermpd	$238, %ymm12, %ymm12
	vshufpd	$12, %ymm4, %ymm8, %ymm8
	vshufpd	$12, %ymm3, %ymm12, %ymm12
	vpermpd	$68, %ymm5, %ymm4
	vpermpd	$68, %ymm6, %ymm3
	vpermpd	$238, %ymm5, %ymm5
	vpermpd	$238, %ymm6, %ymm6
	vshufpd	$12, %ymm6, %ymm5, %ymm6
	vmovupd	(%rsp), %ymm5
	vpermpd	$216, %ymm1, %ymm1
	vshufpd	$12, %ymm3, %ymm4, %ymm3
	vpermpd	$68, %ymm5, %ymm7
	vpermpd	$68, %ymm1, %ymm4
	vpermpd	$238, %ymm5, %ymm5
	vpermpd	$238, %ymm1, %ymm1
	vshufpd	$12, %ymm1, %ymm5, %ymm5
	vshufpd	$12, %ymm4, %ymm7, %ymm7
	vpermpd	$68, %ymm10, %ymm1
	vpermpd	$68, %ymm11, %ymm4
	vpermpd	$68, %ymm2, %ymm9
	vshufpd	$12, %ymm1, %ymm4, %ymm4
	vpermpd	$238, %ymm10, %ymm10
	vpermpd	$68, %ymm15, %ymm1
	vpermpd	$238, %ymm2, %ymm2
	vpermpd	$238, %ymm15, %ymm15
	vpermpd	$238, %ymm11, %ymm11
	vshufpd	$12, %ymm10, %ymm11, %ymm11
	vshufpd	$12, %ymm1, %ymm9, %ymm1
	vshufpd	$12, %ymm15, %ymm2, %ymm10
	vpermpd	$68, %ymm14, %ymm9
	vpermpd	$68, %ymm8, %ymm2
	vpermpd	$238, %ymm14, %ymm14
	vpermpd	$238, %ymm8, %ymm8
	vshufpd	$12, %ymm8, %ymm14, %ymm8
	vmovupd	64(%rsp), %ymm14
	vshufpd	$12, %ymm2, %ymm9, %ymm2
	vpermpd	$68, %ymm14, %ymm9
	vmovupd	%ymm2, 32(%rsp)
	vpermpd	$68, %ymm12, %ymm2
	vshufpd	$12, %ymm2, %ymm9, %ymm13
	vpermpd	$238, %ymm12, %ymm12
	vpermpd	$238, %ymm14, %ymm9
	vmovupd	96(%rsp), %ymm15
	vshufpd	$12, %ymm12, %ymm9, %ymm14
	vpermpd	$68, %ymm3, %ymm2
	vpermpd	$68, %ymm0, %ymm9
	vpermpd	$238, %ymm3, %ymm3
	vpermpd	$238, %ymm0, %ymm0
	vshufpd	$12, %ymm3, %ymm0, %ymm12
	vmovupd	%ymm13, 64(%rsp)
	vpermpd	$68, %ymm6, %ymm0
	vpermpd	$238, %ymm6, %ymm13
	vmovupd	128(%rsp), %ymm6
	vpermpd	$68, %ymm15, %ymm3
	vpermpd	$238, %ymm15, %ymm15
	vshufpd	$12, %ymm2, %ymm9, %ymm2
	vshufpd	$12, %ymm13, %ymm15, %ymm13
	vpermpd	$238, %ymm6, %ymm9
	vpermpd	$68, %ymm6, %ymm15
	vmovupd	160(%rsp), %ymm6
	vshufpd	$12, %ymm0, %ymm3, %ymm3
	vpermpd	$68, %ymm7, %ymm0
	vpermpd	$238, %ymm7, %ymm7
	vshufpd	$12, %ymm7, %ymm9, %ymm9
	vshufpd	$12, %ymm0, %ymm15, %ymm15
	vpermpd	$68, %ymm6, %ymm7
	vpermpd	$68, %ymm5, %ymm0
	vshufpd	$12, %ymm0, %ymm7, %ymm7
	vmovupd	%ymm14, 192(%rsp)
	vpermpd	$68, %ymm2, %ymm0
	vpermpd	$238, %ymm5, %ymm14
	vpermpd	$238, %ymm2, %ymm2
	vpermpd	$68, %ymm4, %ymm5
	vpermpd	$238, %ymm4, %ymm4
	vshufpd	$12, %ymm0, %ymm5, %ymm5
	vshufpd	$12, %ymm2, %ymm4, %ymm4
	vpermpd	$68, %ymm12, %ymm0
	vpermpd	$68, %ymm3, %ymm2
	vmovupd	%ymm5, (%rsp)
	vmovupd	%ymm4, 96(%rsp)
	vpermpd	$68, %ymm11, %ymm5
	vpermpd	$68, %ymm1, %ymm4
	vpermpd	$238, %ymm3, %ymm3
	vpermpd	$238, %ymm1, %ymm1
	vshufpd	$12, %ymm0, %ymm5, %ymm5
	vshufpd	$12, %ymm2, %ymm4, %ymm4
	vshufpd	$12, %ymm3, %ymm1, %ymm1
	vpermpd	$68, %ymm13, %ymm2
	vpermpd	$238, %ymm12, %ymm0
	vpermpd	$68, %ymm10, %ymm3
	vmovupd	32(%rsp), %ymm12
	vshufpd	$12, %ymm2, %ymm3, %ymm3
	vpermpd	$238, %ymm6, %ymm6
	vpermpd	$238, %ymm10, %ymm2
	vpermpd	$238, %ymm13, %ymm13
	vshufpd	$12, %ymm14, %ymm6, %ymm14
	vshufpd	$12, %ymm13, %ymm2, %ymm2
	vpermpd	$68, %ymm15, %ymm6
	vpermpd	$68, %ymm12, %ymm13
	vpermpd	$238, %ymm15, %ymm15
	vpermpd	$238, %ymm12, %ymm12
	vshufpd	$12, %ymm15, %ymm12, %ymm12
	vpermpd	$238, %ymm11, %ymm11
	vmovupd	64(%rsp), %ymm15
	vshufpd	$12, %ymm0, %ymm11, %ymm0
	vpermpd	$238, %ymm9, %ymm10
	vpermpd	$68, %ymm8, %ymm11
	vpermpd	$238, %ymm8, %ymm8
	vshufpd	$12, %ymm6, %ymm13, %ymm13
	vshufpd	$12, %ymm10, %ymm8, %ymm10
	vpermpd	$68, %ymm9, %ymm6
	vpermpd	$238, %ymm15, %ymm8
	vpermpd	$68, %ymm15, %ymm9
	vmovupd	192(%rsp), %ymm15
	vshufpd	$12, %ymm6, %ymm11, %ymm11
	vpermpd	$68, %ymm7, %ymm6
	vpermpd	$238, %ymm7, %ymm7
	vshufpd	$12, %ymm6, %ymm9, %ymm9
	vshufpd	$12, %ymm7, %ymm8, %ymm8
	vpermpd	$68, %ymm14, %ymm6
	vpermpd	$68, %ymm15, %ymm7
	vshufpd	$12, %ymm6, %ymm7, %ymm7
	vpermpd	$238, %ymm14, %ymm14
	vpermpd	$238, %ymm15, %ymm6
	vshufpd	$12, %ymm14, %ymm6, %ymm6
	vpermpd	$68, (%rsp), %ymm14
	vpermpd	$68, %ymm13, %ymm15
	vshufpd	$12, %ymm15, %ymm14, %ymm14
	vmovupd	96(%rsp), %ymm15
	vmovupd	%ymm14, (%rax)
	vpermpd	$238, (%rsp), %ymm14
	vpermpd	$238, %ymm13, %ymm13
	vshufpd	$12, %ymm13, %ymm14, %ymm13
	vpermpd	$68, %ymm12, %ymm14
	vmovupd	%ymm13, 32(%rax)
	vpermpd	$68, %ymm15, %ymm13
	vshufpd	$12, %ymm14, %ymm13, %ymm13
	vpermpd	$238, %ymm12, %ymm12
	vmovupd	%ymm13, 64(%rax)
	vpermpd	$238, %ymm15, %ymm13
	vshufpd	$12, %ymm12, %ymm13, %ymm12
	vpermpd	$68, %ymm11, %ymm13
	vmovupd	%ymm12, 96(%rax)
	vpermpd	$238, %ymm11, %ymm11
	vpermpd	$68, %ymm5, %ymm12
	vpermpd	$238, %ymm5, %ymm5
	vshufpd	$12, %ymm11, %ymm5, %ymm5
	vpermpd	$68, %ymm10, %ymm11
	vmovupd	%ymm5, 160(%rax)
	vpermpd	$238, %ymm10, %ymm10
	vpermpd	$68, %ymm0, %ymm5
	vpermpd	$238, %ymm0, %ymm0
	vshufpd	$12, %ymm11, %ymm5, %ymm5
	vshufpd	$12, %ymm10, %ymm0, %ymm0
	vmovupd	%ymm5, 192(%rax)
	vmovupd	%ymm0, 224(%rax)
	vpermpd	$68, %ymm9, %ymm5
	vpermpd	$68, %ymm4, %ymm0
	vpermpd	$238, %ymm9, %ymm9
	vpermpd	$238, %ymm4, %ymm4
	vshufpd	$12, %ymm5, %ymm0, %ymm0
	vshufpd	$12, %ymm9, %ymm4, %ymm4
	vmovupd	%ymm0, 256(%rax)
	vmovupd	%ymm4, 288(%rax)
	vpermpd	$68, %ymm1, %ymm0
	vpermpd	$68, %ymm8, %ymm4
	vpermpd	$238, %ymm1, %ymm1
	vpermpd	$238, %ymm8, %ymm8
	vshufpd	$12, %ymm4, %ymm0, %ymm0
	vshufpd	$12, %ymm8, %ymm1, %ymm1
	vmovupd	%ymm0, 320(%rax)
	vmovupd	%ymm1, 352(%rax)
	vpermpd	$68, %ymm3, %ymm0
	vpermpd	$68, %ymm7, %ymm1
	vshufpd	$12, %ymm1, %ymm0, %ymm0
	vpermpd	$238, %ymm3, %ymm3
	vmovupd	%ymm0, 384(%rax)
	vpermpd	$68, %ymm6, %ymm1
	vpermpd	$68, %ymm2, %ymm0
	vpermpd	$238, %ymm7, %ymm7
	vpermpd	$238, %ymm2, %ymm2
	vpermpd	$238, %ymm6, %ymm6
	addq	$256, %rcx
	vshufpd	$12, %ymm13, %ymm12, %ymm12
	vshufpd	$12, %ymm7, %ymm3, %ymm3
	vmovupd	%ymm12, 128(%rax)
	vmovupd	%ymm3, 416(%rax)
	vshufpd	$12, %ymm1, %ymm0, %ymm0
	vshufpd	$12, %ymm6, %ymm2, %ymm2
	vmovupd	%ymm0, 448(%rax)
	vmovupd	%ymm2, 480(%rax)
	addq	$256, %rdx
	addq	$256, %r11
	addq	$256, %r10
	addq	$256, %r9
	addq	$512, %rax
	cmpq	%rcx, %rbp
	jne	.L4

I am not kidding.
gcc 10.2 -O3 really generates code that is approximately 3 times slower than scalar output of -O2 and, may be, 4-4.5 times slower than good SIMD code similar to what was generated in Part1.

My guess is that it's once again, as in nearly all my complains of recent months
it a case of earlier phase of optimization producing a mess that totally confuses a later stage. I just can't guess what is the name of stage in fault this time.
You have so many.


---


### compiler : `gcc`
### title : `Missed dead code optimization from data flow analysis`
### open_at : `2020-10-15T01:38:10Z`
### last_modified_date : `2023-06-26T05:00:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97434
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.3.0`
### severity : `enhancement`
### contents :
I found a pretty simple case where GCC cannot optimize out a redundant check. I've reduced it to the following minimal test case:


unsigned int random_number(void);                                                                
void eliminate_me(void);                                                                         
                                                                                                 
void main(void)                                                                                  
{                                                                                                
        unsigned int a = random_number();                                                        
        unsigned int b = random_number();                                                        
                                                                                                 
        if (b > a)                                                                               
                return;                                                                          
                                                                                                 
        int x = b - 8;                                                                           
                                                                                                 
        if (x > 0 && x > a)                                                                               
                eliminate_me();                                                                  
}


I think it should be really easy to prove that eliminate_me() cannot be called, because x can never be greater than a (otherwise b would have also been greater than a and the function would have terminated earlier). I don't know anything about how compilers do data flow analysis in detail, but GCC can usually figure out so much that I'm surprised it cannot figure out this one.


---


### compiler : `gcc`
### title : `builtins subcarry and addcarry still not generate the right code. Not get optimized to immediate value`
### open_at : `2020-10-15T08:28:08Z`
### last_modified_date : `2023-06-03T20:02:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97437
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
#include<cstdint>
#include<array>
#if defined(_MSC_VER)
#include<intrin.h>
#elif defined(__x86_64__) || defined(__i386__)
#include<immintrin.h>
#endif

struct field_number
{
	using value_type = std::conditional_t<sizeof(std::size_t)>=8,std::uint64_t,std::uint32_t>;
	value_type content[32/sizeof(value_type)];
	inline constexpr value_type const& operator[](std::size_t pos) const noexcept
	{
		return content[pos];
	}
	inline constexpr value_type& operator[](std::size_t pos) noexcept
	{
		return content[pos];
	}
};

namespace intrinsics
{
template<typename T>
#if __cpp_lib_concepts >= 202002L
requires (std::unsigned_integral<T>)
#endif
inline constexpr bool sub_borrow(bool borrow,T a,T b,T& out) noexcept
{
#if defined(_MSC_VER) || defined(__x86_64__) || defined(__i386__)
#if __cpp_lib_is_constant_evaluated >= 201811L
	if(std::is_constant_evaluated())
		return (out=a-b-borrow)>=a;
	else
#endif
	{
		if constexpr(sizeof(T)==8)
#if defined(__x86_64__)
			return _subborrow_u64(borrow,a,b,
#if !defined(__INTEL_COMPILER ) &&(defined(__GNUC__) || defined(__clang__))
			reinterpret_cast<unsigned long long*>(&out));
#else
			&out);
#endif
#else
			return (out=a-b-borrow)>=a;
#endif

		if constexpr(sizeof(T)==4)
			return _subborrow_u32(borrow,a,b,reinterpret_cast<std::uint32_t*>(&out));
		else if constexpr(sizeof(T)==2)
			return _subborrow_u16(borrow,a,b,reinterpret_cast<std::uint16_t*>(&out));
		else if constexpr(sizeof(T)==1)
			return _subborrow_u8(borrow,a,b,reinterpret_cast<std::uint8_t*>(&out));
	}
#else
	return (out=a-b-borrow)>=a;
#endif
}

}


field_number operator-(field_number const& x,field_number const& y) noexcept
{
	using namespace intrinsics;
	using unsigned_type = field_number::value_type;
	constexpr unsigned_type zero{};
	field_number f;
	bool borrow{sub_borrow(false,x[0],y[0],f[0])};
	borrow=sub_borrow(borrow,x[1],y[1],f[1]);
	borrow=sub_borrow(borrow,x[2],y[2],f[2]);
	borrow=sub_borrow(borrow,x[3],y[3],f[3]);
	unsigned_type v{};
	sub_borrow(borrow,v,v,v);
	v&=static_cast<unsigned_type>(38);
	borrow=sub_borrow(false,f[0],v,f[0]);
	borrow=sub_borrow(borrow,f[1],zero,f[1]);
	borrow=sub_borrow(borrow,f[2],zero,f[2]);
	borrow=sub_borrow(borrow,f[3],zero,f[3]);
	sub_borrow(borrow,v,v,v);
	v&=static_cast<unsigned_type>(38);
	borrow=sub_borrow(false,f[0],v,f[0]);
	borrow=sub_borrow(borrow,f[1],zero,f[1]);
	borrow=sub_borrow(borrow,f[2],zero,f[2]);
	borrow=sub_borrow(borrow,f[2],zero,f[3]);
	return f;
}

https://godbolt.org/z/xM8xef

operator-(field_number const&, field_number const&):
        movq    (%rsi), %r9
        subq    (%rdx), %r9
        movq    %rdi, %r8
        movq    %rdx, %rax
        movq    %r9, (%rdi)
        movq    8(%rsi), %rdi
        sbbq    8(%rdx), %rdi
        movq    %rdi, 8(%r8)
        movq    16(%rsi), %rdx
        sbbq    16(%rax), %rdx
        movq    %rdx, 16(%r8)
        movq    24(%rax), %rax
        movq    24(%rsi), %rsi
        sbbq    %rax, %rsi

//Here is an output dependency. No need movl 0 to %eax.
        movl    $0, %eax
        movq    %rax, %rcx
        sbbq    %rax, %rcx
        andl    $38, %ecx
        subq    %rcx, %r9
        sbbq    %rax, %rdi// why sbbq %rax,%rdi instead of sbbq 0 %rdi ????
//The %rax register should not get allocated or used in GCC
        sbbq    %rax, %rdx
        sbbq    %rax, %rsi
        sbbq    %rcx, %rcx
        andl    $38, %ecx
        subq    %rcx, %r9
        sbbq    %rax, %rdi
        movq    %r9, (%r8)
        sbbq    %rax, %rdx
        movq    %rdi, 8(%r8)
        movq    %rdx, 16(%r8)
        sbbq    %rax, %rdx
        movq    %r8, %rax
        movq    %rdx, 24(%r8)
        ret


The assembly GCC generated is still worse than clang. although clang does not generate the optimal one either.

The subborrow instruction in GCC does not get optimized as immediate value

The "correct" assembly it generates should be like what clang generates (you can use different registers no problem) minus that xorl    %ecx, %ecx clean up instruction.


operator-(field_number const&, field_number const&):                # @operator-(field_number const&, field_number const&)
        movq    %rdi, %rax
        movq    (%rsi), %r8
        subq    (%rdx), %r8
        movq    8(%rsi), %r9
        sbbq    8(%rdx), %r9
        movq    16(%rsi), %rdi
        sbbq    16(%rdx), %rdi
        movq    24(%rsi), %rsi
        sbbq    24(%rdx), %rsi
        sbbq    %rcx, %rcx
        andl    $38, %ecx
        subq    %rcx, %r8
        sbbq    $0, %r9
        sbbq    $0, %rdi
        sbbq    $0, %rsi
        sbbq    %rcx, %rcx
        andl    $38, %ecx
        subq    %rcx, %r8
        sbbq    $0, %r9
        movq    %r8, (%rax)
        movq    %r9, 8(%rax)
        sbbq    $0, %rdi
        movq    %rdi, 16(%rax)
        sbbq    $0, %rdi
        movq    %rdi, 24(%rax)
        retq


---


### compiler : `gcc`
### title : `Unneccessary stack frame when using stack protector`
### open_at : `2020-10-15T15:21:52Z`
### last_modified_date : `2021-09-19T05:28:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97448
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.0`
### severity : `normal`
### contents :
While building Linux kernel for 32 bits powerpc with GCC 10.1 with stack frame protection, functions that don't use the stack at all get a pointless stack frame, while they don't when stackframe protection is not used:

c0016af4 <get_order>:
c0016af4:       38 63 ff ff     addi    r3,r3,-1
c0016af8:       94 21 ff f0     stwu    r1,-16(r1)
c0016afc:       54 63 a3 3e     rlwinm  r3,r3,20,12,31
c0016b00:       7c 63 00 34     cntlzw  r3,r3
c0016b04:       20 63 00 20     subfic  r3,r3,32
c0016b08:       38 21 00 10     addi    r1,r1,16
c0016b0c:       4e 80 00 20     blr



c000d1b8 <raw_copy_from_user>:
c000d1b8:       94 21 ff f0     stwu    r1,-16(r1)
c000d1bc:       38 21 00 10     addi    r1,r1,16
c000d1c0:       48 00 c5 44     b       c0019704 <__copy_tofrom_user>


Used GCC version:

GCC version with the BUG:

Using built-in specs.
COLLECT_GCC=/opt/gcc-10.1.0-nolibc/powerpc64-linux/bin/powerpc64-linux-gcc
COLLECT_LTO_WRAPPER=/home/opt/gcc-10.1.0-nolibc/powerpc64-linux/bin/../libexec/gcc/powerpc64-linux/10.1.0/lto-wrapper
Target: powerpc64-linux
Configured with: /home/arnd/git/gcc/configure --target=powerpc64-linux --enable-targets=all --prefix=/home/arnd/cross/x86_64/gcc-10.1.0-nolibc/powerpc64-linux --enable-languages=c --without-headers --disable-bootstrap --disable-nls --disable-threads --disable-shared --disable-libmudflap --disable-libssp --disable-libgomp --disable-decimal-float --disable-libquadmath --disable-libatomic --disable-libcc1 --disable-libmpx --enable-checking=release
Thread model: single
Supported LTO compression algorithms: zlib
gcc version 10.1.0 (GCC)


Build command:

powerpc64-linux-gcc -Wp,-MMD,arch/powerpc/kernel/.setup-common.o.d  -nostdinc -isystem /home/opt/gcc-10.1.0-nolibc/powerpc64-linux/bin/../lib/gcc/powerpc64-linux/10.1.0/include -I./arch/powerpc/include -I./arch/powerpc/include/generated  -I./include -I./arch/powerpc/include/uapi -I./arch/powerpc/include/generated/uapi -I./include/uapi -I./include/generated/uapi -include ./include/linux/kconfig.h -include ./include/linux/compiler_types.h -D__KERNEL__ -I ./arch/powerpc -Wall -Wundef -Werror=strict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -fshort-wchar -fno-PIE -Werror=implicit-function-declaration -Werror=implicit-int -Wno-format-security -std=gnu89 -mcpu=powerpc -mcpu=powerpc -m32 -msoft-float -pipe -ffixed-r2 -mmultiple -mno-readonly-in-sdata -mcpu=powerpc64 -mno-altivec -mno-vsx -fno-asynchronous-unwind-tables -mno-string -mbig-endian -mstack-protector-guard=tls -mstack-protector-guard-reg=r2 -fno-delete-null-pointer-checks -Wno-frame-address -Wno-format-truncation -Wno-format-overflow -Wno-address-of-packed-member -O2 -fno-allow-store-data-races -Wframe-larger-than=1024 -fstack-protector -Wno-unused-but-set-variable -Wimplicit-fallthrough -Wno-unused-const-variable -fomit-frame-pointer -fno-var-tracking-assignments -Wdeclaration-after-statement -Wvla -Wno-pointer-sign -Wno-stringop-truncation -Wno-zero-length-bounds -Wno-array-bounds -Wno-stringop-overflow -Wno-restrict -Wno-maybe-uninitialized -fno-strict-overflow -fno-merge-all-constants -fmerge-constants -fno-stack-check -fconserve-stack -Werror=date-time -Werror=incompatible-pointer-types -Werror=designated-init -fmacro-prefix-map=./= -Wno-packed-not-aligned -mstack-protector-guard-offset=552 -Werror    -DKBUILD_MODFILE='"arch/powerpc/kernel/setup-common"' -DKBUILD_BASENAME='"setup_common"' -DKBUILD_MODNAME='"setup_common"' -c -o arch/powerpc/kernel/setup-common.o arch/powerpc/kernel/setup-common.c


---


### compiler : `gcc`
### title : `__uint128_t remainder for division by 3`
### open_at : `2020-10-16T13:33:45Z`
### last_modified_date : `2021-08-15T11:24:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97459
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
The following two functions are equivalent:

unsigned r3_128u_v1 (__uint128_t n)
{
  unsigned long a;
  a = (n >> 64) + (n & 0xffffffffffffffff);
  return a % 3;
}

unsigned r3_128u_v2 (__uint128_t n)
{
  return (unsigned) (n%3);
}

and the first one is definitely faster.

(The approach is due to Hacker's Delight, 2nd edition, "Remainder by
Summing Digits". There are also other interesting approaches there.)


---


### compiler : `gcc`
### title : `Missed redundant store optimization opportunity`
### open_at : `2020-10-16T17:31:29Z`
### last_modified_date : `2021-08-15T00:18:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97464
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
The code

    void f( int& x, float& y )
    {
        ++x;
        y = 1;
        --x;
    }

compiles to

    f(int&, float&):
        mov     eax, DWORD PTR [rdi]
        mov     DWORD PTR [rsi], 0x3f800000
        mov     DWORD PTR [rdi], eax
        ret

(https://godbolt.org/z/so4h3v)

but the load from, and the store to, [rdi] are redundant. It's obvious that TBAA is active, but it for some reason doesn't go far enough.

This is a simplified example from "realer" code where x is a reference count whose unnecessary manipulations could have been optimized out entirely.


---


### compiler : `gcc`
### title : `arm cortex-m0+ or constant value can use adds`
### open_at : `2020-10-19T13:03:18Z`
### last_modified_date : `2020-10-19T14:17:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97492
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
Given the following C code the compiler generates code that can be further optimized.

void write_byte(void)
{
  unsigned *regs = (unsigned *) 0x4210000;
  *regs = (*regs & ~0xff) | (0x9 & 0xff);
}

In this code the first byte is cleared and a constant 9 is or'ed into the word. This is compiled down to these instructions for cortex-m0+.

write_byte:
        movs    r3, #255
        ldr     r1, .L2
        ldr     r2, [r1]
        bics    r2, r3
        subs    r3, r3, #246
        orrs    r3, r2
        str     r3, [r1]
        bx      lr
.L3:
        .align  2
.L2:
        .word   69271552

In this case where a constant less than 255 is or'ed into the last byte of a word then the subs + orrs instruction (or sometimes movs + orrs) sequence can be replaced by a single adds instruction to save 2 bytes.


---


### compiler : `gcc`
### title : `Suboptimal use of cntlzw and cntlzd`
### open_at : `2020-10-20T14:44:54Z`
### last_modified_date : `2020-10-21T18:53:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97503
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.1.1`
### severity : `normal`
### contents :
int f(int x)
{
	return x ? __builtin_clz(x) : 32;
}

Is built as

00000000 <f>:
   0:	2c 03 00 00 	cmpwi   r3,0
   4:	40 82 00 0c 	bne     10 <f+0x10>
   8:	38 60 00 20 	li      r3,32
   c:	4e 80 00 20 	blr
  10:	7c 63 00 34 	cntlzw  r3,r3
  14:	4e 80 00 20 	blr


I would expect

00000000 <f>:
   0:	7c 63 00 34 	cntlzw  r3,r3
   4:	4e 80 00 20 	blr

Because cntlzw (Count Leading Zeros Word) is documentated in powerpc instruction set as returning 0 to 32 inclusive

The same applies to the 64 bits version:

long f(long x)
{
	return x ? __builtin_clzll(x) : 64;
}

0000000000000000 <.f>:
   0:	2c 23 00 00 	cmpdi   r3,0
   4:	41 82 00 0c 	beq     10 <.f+0x10>
   8:	7c 63 00 74 	cntlzd  r3,r3
   c:	4e 80 00 20 	blr
  10:	38 60 00 40 	li      r3,64
  14:	4e 80 00 20 	blr


---


### compiler : `gcc`
### title : `builtin_constant_p (x + cst) should be optimized to builtin_constant_p (x)`
### open_at : `2020-10-21T15:14:08Z`
### last_modified_date : `2020-11-03T10:09:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97519
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
As discussed in PR97445 we should optimize builtins_constant_p (var+cst) and similar cases.


---


### compiler : `gcc`
### title : `(len / N) * N <= len is not optimized to 1`
### open_at : `2020-10-22T14:51:09Z`
### last_modified_date : `2023-02-10T22:30:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97529
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
From https://github.com/rust-lang/rust/issues/74938

GCC compiles:

#include <stddef.h>

const size_t N = 3;

int foo(size_t len) {
	size_t newlen = (len / N) * N;
	return newlen <= len;
}

to:

foo:
        movabs  rdx, -6148914691236517205
        mov     rax, rdi
        mul     rdx
        mov     rax, rdx
        and     rdx, -2
        shr     rax
        add     rdx, rax
        xor     eax, eax
        cmp     rdi, rdx
        setnb   al
        ret
N:
        .quad   3

clang:

foo:                                    # @foo
        mov     eax, 1
        ret
N:
        .quad   3


---


### compiler : `gcc`
### title : `[missed optimization] constexprness not noticed when UBsan enabled`
### open_at : `2020-10-23T16:27:52Z`
### last_modified_date : `2023-04-26T13:18:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97553
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.0`
### severity : `normal`
### contents :
(GodBolt example: https://godbolt.org/z/Kvan5c)

Consider the following code:

  #include <string_view>
  
  constexpr std::string_view f() { return "hello"; }
  
  static constexpr std::string_view g() {
      auto x { f() };
      return x.substr(1, 3);
  } 
  
  int foo() { return g().length(); }

if you compile it with flags `--std=c++17 -O3`, it results in a pleasant:

  foo():
          mov     eax, 3
          ret

but if you also enabled undefined-behavior sanitization, i.e. `--std=c++17 -fsanitize=undefined -O3`, then you get a much longer program with UB-related instrumentation - which is never used.

I'm not sure if it's because some optimizations are disabled with UBsan, in which case this might be a "misfeature", or whether they're enabled but the optimization is just missed.


---


### compiler : `gcc`
### title : `NRVO is very fragile (adding an extra scope breaks the optimization)`
### open_at : `2020-10-24T16:37:46Z`
### last_modified_date : `2020-10-26T16:15:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97562
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.0`
### severity : `normal`
### contents :
Consider the following (where T is just some type that is trivially copyable but large enough):

struct T {
    char _[24];
};

bool good_enough(T const&);
T get();

T as_loop() {
    for (;;) {
        T obj = get();
        if (good_enough(obj)) {
            return obj;
        }
    }
}

T as_goto() {
    loop:
        T obj = get();
        if (good_enough(obj)) {
            return obj;
        }
        goto loop;
}

The two functions, as_loop and as_goto, are semantically equivalent. Yet they emit rather different code. With gcc 10.2 -O3 (https://godbolt.org/z/5dx3od), as_loop is:

as_loop():
        push    r12
        mov     r12, rdi
        sub     rsp, 32
.L3:
        mov     rdi, rsp
        call    get()
        mov     rdi, rsp
        call    good_enough(T const&)
        test    al, al
        je      .L3
        mov     rax, QWORD PTR [rsp+16]
        movdqu  xmm0, XMMWORD PTR [rsp]
        mov     QWORD PTR [r12+16], rax
        mov     rax, r12
        movups  XMMWORD PTR [r12], xmm0
        add     rsp, 32
        pop     r12
        ret

while as_goto is:

as_goto():
        push    r12
        mov     r12, rdi
.L8:
        mov     rdi, r12
        call    get()
        mov     rdi, r12
        call    good_enough(T const&)
        test    al, al
        je      .L8
        mov     rax, r12
        pop     r12
        ret

That is, the goto implementation gives us NRVO, but the loop implementation does not. This has something to do with the extra scope. Adding an extra set of braces, as in:

T as_goto() {
    {
    loop:
        T obj = get();
        if (good_enough(obj)) {
            return obj;
        }
        goto loop;
    }
}

emits the same code as the loop example.


---


### compiler : `gcc`
### title : `Overzealous SRA of boolean bitfields`
### open_at : `2020-10-27T08:43:55Z`
### last_modified_date : `2023-07-19T04:07:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97588
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `normal`
### contents :
For the nonsense code (reduced from real code):

--------------------------------------------------
struct s
{
  unsigned int foo : 11;
  unsigned int flag1 : 1;
  unsigned int bar : 11;
  unsigned int flag2 : 1;
};

void
f (int n, int *x, struct s *ptr, struct s flags)
{
  for (int i = 0; i < n; ++i)
    if (x[i] == 1)
      flags.flag1 = 1;
    else if (x[i] == 2)
      flags.flag2 = 1;
    else if (x[i] == 3)
      {
	if (flags.flag1)
	  *ptr++ = flags;
      }
    else if (x[i] == 4)
      {
	if (flags.flag2)
	  *ptr++ = flags;
      }
    else
      *ptr++ = flags;
}
--------------------------------------------------

SRA significantly pessimises the output.  At the machine level,
each update to flags is usually a simple register OR, bit-test,
or move, but SRA instead decides to split flags up into 4
pieces and reassemble it for "*ptr++ = flags" (which in the
original code is the hot statement).


---


### compiler : `gcc`
### title : `Failure to optimize out compare into reuse of subtraction result`
### open_at : `2020-10-27T22:01:48Z`
### last_modified_date : `2023-09-21T10:57:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97603
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int g();

int f(int a, int b)
{
    if (a != b)
        return a - b;
    return g();
}

This can be optimized to using the result of `a - b` to check for `a != b`. This is done by LLVM, but not by GCC. For example, on x86, LLVM generates this :

f(int, int):
  sub edi, esi
  jne .LBB0_1
  jmp g()
.LBB0_1:
  mov eax, edi
  ret

GCC generates this :

f(int, int):
  cmp edi, esi
  je .L7
  mov eax, edi
  sub eax, esi
  ret
.L7:
  jmp g()


---


### compiler : `gcc`
### title : `unused conditionally freed allocation not eliminated`
### open_at : `2020-10-27T22:25:42Z`
### last_modified_date : `2020-11-03T10:18:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97605
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
In the test program below GCC eliminates the unnecessary pair of malloc/free in f() but it doesn't perform the likely much more impactful optimization in g() whose body could be transformed into:

  void* g (int i)
  {
    if (i)
      return 0;
    return malloc (1);
  }


$ cat q.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout q.c
typedef __SIZE_TYPE__   size_t;

extern void free (void*);
extern void* malloc (size_t);

void f (int i)
{ 
  void *p = malloc (1);   // eliminated
  free (p);               // ditto
}

void* g (int i)
{
  void *p = malloc (1);
  if (i)
    {
      free (p);
      return 0;
    }

  return p; 
}

;; Function f (f, funcdef_no=0, decl_uid=1936, cgraph_uid=1, symbol_order=0)

f (int i)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1940, cgraph_uid=2, symbol_order=1)

Removing basic block 5
g (int i)
{
  void * p;
  void * _1;

  <bb 2> [local count: 1073741824]:
  p_5 = malloc (1);
  if (i_6(D) != 0)
    goto <bb 3>; [9.39%]
  else
    goto <bb 4>; [90.61%]

  <bb 3> [local count: 100824360]:
  free (p_5);

  <bb 4> [local count: 1073741824]:
  # _1 = PHI <0B(3), p_5(2)>
  return _1;

}


---


### compiler : `gcc`
### title : `Spurious sign extension`
### open_at : `2020-10-27T23:54:55Z`
### last_modified_date : `2021-03-06T09:53:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97607
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
The following code

extern void fun(char);                                                                                                               
void wrapper(char x) { fun(x); }

Should compile 'wrapper' to a single jump, but instead it does this (on amd64):

wrapper:
	movsx	edi, dil
	jmp	fun

Presumably 'x' is getting promoted to int in wrapper, and the promotion never gets removed.


---


### compiler : `gcc`
### title : `GCC doesn't optimize-out outside-affecting lambdas within y-combinator while clang does.`
### open_at : `2020-10-30T14:29:54Z`
### last_modified_date : `2022-11-02T00:15:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97640
### status : `UNCONFIRMED`
### tags : `c++-lambda, missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
Suppose we have a y-combinator, so that we can write lambdas in recursive fashion.

    template <class F>
    struct y_combinator {
        F f;
        template <class... Ts>
        constexpr decltype(auto) operator()(Ts&&... ts) const { return f(std::ref(*this), std::forward<Ts>(ts)...); }
      
        template <class... Ts>
        constexpr decltype(auto) operator()(Ts&&... ts) { return f(std::ref(*this), std::forward<Ts>(ts)...); }
    };

    
Let's have a constexpr-qualified lambda

    auto sum = y_combinator{[&](auto self,int a,int b,int res = 0) mutable {
        if (a>b) {
          return res;
        }
        res += a;
        return self(a+1,b,res);
    }};

This lambda is constexpr-qualified.
So both gcc and clang do optimize-out all the things in the name of constexpr expansion.
https://compiler-explorer.com/z/qx4MGE

==============================================================================

Let's try an outside-affecting lambda

    auto sum = y_combinator{[&,res=0](auto self,int a,int b) mutable {
        if (a>b) {
          return res;
        }
        res += a;
        return self(a+1,b);
    }};

This non-constexpr-qualified lambda is optimized-out in clang but in gcc it is not.
https://compiler-explorer.com/z/hPGWod


---


### compiler : `gcc`
### title : `Specify that there is no address arithmetic on a pointer`
### open_at : `2020-10-31T16:40:45Z`
### last_modified_date : `2021-09-26T07:55:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97656
### status : `UNCONFIRMED`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
This involves Fortran, but possibly also other languages.

Consider

$ cat alias.f90
program main
  interface
     subroutine foo(a)
       integer, intent(inout) :: a
     end subroutine foo
  end interface
  integer, dimension(2) :: x
  x(1) = 42
  x(2) = 42
  call foo(x(1))
  if (x(2) /= 42) stop "Error!"
end program main

This program specifies that foo has a scalar argument, passed by reference.
It is forbidden by Fortran's rules foo could access the element x(2),
therefore the call to stop could be optimized away, but it isn't:

$ gfortran -O3 -S alias.f90 
$ grep _gfortran_stop alias.s 
        call    _gfortran_stop_string
$ 

It would be good if there was a TREE_NO_POINTER_ARITHMETC (or simlar) flag
that could be set by the Fortran front end.


---


### compiler : `gcc`
### title : `(cond ? 2 : 0) is not optimized to int(cond) << 1`
### open_at : `2020-11-03T10:28:49Z`
### last_modified_date : `2021-06-01T20:50:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97690
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
enum E { init=0, active=1, done=2 };

int f(bool d)
{
    return d ? done : init;
}

int g(bool d)
{
    return int(d) << 1;
}

The first function is more readable and less fragile (in case the order of bits is changed) but GCC produces larger code for it.

For x86_64 with -O3:

f(bool):
        xor     eax, eax
        test    dil, dil
        setne   al
        add     eax, eax
        ret
g(bool):
        movzx   eax, dil
        add     eax, eax
        ret

And for x86_64 -Os:

f(bool):
        neg     dil
        sbb     eax, eax
        and     eax, 2
        ret
g(bool):
        movzx   eax, dil
        add     eax, eax
        ret


Clang produces the same code for both functions at all optimization levels:

f(bool):                                  # @f(bool)
        lea     eax, [rdi + rdi]
        ret
g(bool):                                  # @g(bool)
        lea     eax, [rdi + rdi]
        ret


---


### compiler : `gcc`
### title : `Failure to optimise "x & 1 ? x - 1 : x" to "x & -2"`
### open_at : `2020-11-04T00:50:15Z`
### last_modified_date : `2023-06-09T14:21:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97711
### status : `ASSIGNED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
GCC doesn't optimise the expression:

  int f (int x) { return x & 1 ? x - 1 : x; }

to x & -2 (but clang does).  The original motivation was actually:

  char *g (char *x) { return (__UINTPTR_TYPE__) x & 1 ? x - 1 : x; }

which I guess might require a different pattern.

(And the reason for writing g that way was to do all pointer
arithmetic on pointers rather than converting to a uintptr_t,
doing arithmetic, and converting back.)


---


### compiler : `gcc`
### title : `GCC using branches when a conditional move would be better`
### open_at : `2020-11-05T17:18:53Z`
### last_modified_date : `2021-12-15T01:07:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97734
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Created attachment 49511
graph

Playing with the code in https://github.com/patmorin/arraylayout I noticed that I could not reproduce the results for the eytzinger layout. It turns out the issue was with current gcc selecting moves instead of conditional moves for that particular code.

A reduced testcase is

#include <stdint.h>
uint64_t branchfree_search(uint64_t x, uint64_t n, uint32_t *a) {
    uint64_t i = 0;
    while (i < n) {
        i = (x <= a[i]) ? (2*i + 1) : (2*i + 2);
    }
    uint64_t j = (i+1) >> __builtin_ffsl(~(i+1));
    return (j == 0) ? n : j-1;
}

I have placed it in

https://gcc.godbolt.org/z/Krqrz7

Results
* ICC: conditional move
* Clang: branches
* GCC 6.4: conditional move
* Newer GCCs with -O2:  branches
* GCC with -Os: conditional move

The attached graph shows how the conditional move is better for "small" array sizes.


---


### compiler : `gcc`
### title : `[9/10/11 Regression] switch codegen`
### open_at : `2020-11-06T03:34:33Z`
### last_modified_date : `2021-12-07T08:18:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97736
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.1.0`
### severity : `normal`
### contents :
In Gcc 8 and previous, the following code 

  bool is_vowel(char c) {
    switch (c)
    case'a':case'e':case'i':case'o':case'u':
      return true;
    return false;
  }

compiled with -O2 or better, for numerous x86-64 targets,
resolves to a bitwise flag check, e.g.

        lea     ecx, [rdi-97]
        xor     eax, eax
        cmp     cl, 20
        ja      .L1
        mov     eax, 1
        sal     rax, cl
        test    eax, 1065233
        setne   al
  .L1:
        ret

Starting in gcc-9, this optimization is not performed 
anymore at -O2 for many common targets (e.g. -march=skylake),
and we get

        sub     edi, 97
        cmp     dil, 20
        ja      .L2
        movzx   edi, dil
        jmp     [QWORD PTR .L4[0+rdi*8]]
  .L4:
        .quad   .L5
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L5
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L5
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L5
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L2
        .quad   .L5
  .L2:
        mov     eax, 0
        ret
  .L5:
        mov     eax, 1
        ret

same as with -O0 or -O1.


---


### compiler : `gcc`
### title : `Optimizing division by value & - value for HAKMEM 175`
### open_at : `2020-11-06T08:23:11Z`
### last_modified_date : `2021-09-26T08:24:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97738
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
A straightforward implementation of HAKMEM 175 (returning
the next number with the same number of bits) is

unsigned int
next_same_bit (unsigned int value)
{
  unsigned int lowest_bit;
  unsigned int left_bits;
  unsigned int changed_bits;
  unsigned int right_bits;

  lowest_bit = value & - value;
  left_bits = value + lowest_bit;
  changed_bits = value ^ left_bits;
  right_bits = (changed_bits / lowest_bit) >> 2;
  return left_bits | right_bits;
}

In two's complement, this can be replaced by

unsigned int
next_s_bit (unsigned int value)
{
  unsigned int lowest_bit;
  unsigned int ctz;
  unsigned int left_bits;
  unsigned int changed_bits;
  unsigned int right_bits;

  ctz = __builtin_ctz (value);
  lowest_bit = 1u << ctz;
  left_bits = value + lowest_bit;
  changed_bits = value ^ left_bits;
  right_bits = changed_bits >> (ctz + 2);
  return left_bits | right_bits;
}

to replace the expensive division by what is known to be a
power of two by a shift.

That transformation is counter-productive (and might be done
the other way) if there is no division by lowest_bit.


---


### compiler : `gcc`
### title : `Failure to optimize boolean multiplication to select`
### open_at : `2020-11-06T10:58:55Z`
### last_modified_date : `2023-09-21T10:56:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97743
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
int f(bool b)
{
    return b * 743;
}

This can be optimized to `return b ? 743 : 0;`. This optimization is done by LLVM, but not GCC.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] missed combine opt with logical ops after zero extended load`
### open_at : `2020-11-06T21:07:21Z`
### last_modified_date : `2023-07-27T09:22:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97747
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `normal`
### contents :
Consider this testcase
struct
{
  unsigned int a : 1;
  unsigned int b : 1;
  unsigned int c : 1;
  unsigned int d : 1;
  unsigned int pad1 : 28;
} s;

void
sub (void)
{
  s.a = 1;
  s.c = 1;
}

Compiling with -O2 -S for ARM I get
sub:
	@ args = 0, pretend = 0, frame = 0
	@ frame_needed = 0, uses_anonymous_args = 0
	@ link register save eliminated.
	movw	r2, #:lower16:.LANCHOR0
	movt	r2, #:upper16:.LANCHOR0
	ldrb	r3, [r2]	@ zero_extendqisi2
	bic	r3, r3, #5
	orr	r3, r3, #5
	strb	r3, [r2]
	bx	lr
The bic bit-clear instruction is obviously unnecessary.

In the combine dump file I see that we have
(insn 9 7 11 2 (set (reg:SI 120)
        (and:SI (reg:SI 119 [ MEM <unsigned char> [(struct  *)&sD.5619] ])
            (const_int -6 [0xfffffffffffffffa]))) "tmp.c":13:7 90 {*arm_andsi3_insn}
     (expr_list:REG_DEAD (reg:SI 119 [ MEM <unsigned char> [(struct  *)&sD.5619] ])
        (nil)))
(insn 11 9 13 2 (set (reg:SI 122)
        (ior:SI (reg:SI 120)
            (const_int 5 [0x5]))) "tmp.c":13:7 106 {*iorsi3_insn}
     (expr_list:REG_DEAD (reg:SI 120)
        (nil)))

And the combiner does:
Trying 9 -> 11:
    9: r120:SI=r119:SI&0xfffffffffffffffa
      REG_DEAD r119:SI
   11: r122:SI=r120:SI|0x5
      REG_DEAD r120:SI
Failed to match this instruction:
(set (reg:SI 122)
    (ior:SI (and:SI (reg:SI 119 [ MEM <unsigned char> [(struct  *)&sD.5619] ])
            (const_int 250 [0xfa]))
        (const_int 5 [0x5])))

The problem here is that the ARM port generated a zero_extend for the load byte, so combine knows that r120 has only 8 nonzero bits, it modified the -6 to 250 and then fails to notice that the and operation can be folded away because in SImode the operation is no longer redundant with the modified constant.

On targets that do not generate the zero_extend, the and -6 operation gets optimized away in combine.  For instance, with the current RISC-V port I get
sub:
	lui	a4,%hi(s)
	lbu	a5,%lo(s)(a4)
	ori	a5,a5,5
	sb	a5,%lo(s)(a4)
	ret

This likely fails on any target where movqi generates a zero extended load.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Inefficient handling of 128-bit arguments`
### open_at : `2020-11-08T19:53:14Z`
### last_modified_date : `2023-07-16T11:48:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97756
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
This is an offshoot from PR 97459.

The code

#define ONE ((__uint128_t) 1)
#define TWO_64 (ONE << 64)
#define MASK60 ((1ul << 60) - 1)

typedef __uint128_t mytype;

void
div_rem_13_v2 (mytype n, mytype *div, unsigned int *rem)
{
  const mytype magic = TWO_64 * 14189803133622732012u + 5675921253449092805u * ONE;
  unsigned long a, b, c;
  unsigned int r;

  a = n & MASK60;
  b = (n >> 60);
  b = b & MASK60;
  c = (n >> 120);
  r = (a+b+c) % 13;
  n = n - r;
  *div = n * magic;
  *rem = r;
}

when compiled on x86_64 on Zen with -O3 -march=native has quite
some register shuffling at the beginning:

   0:   49 89 f0                mov    %rsi,%r8
   3:   48 89 fe                mov    %rdi,%rsi
   6:   49 89 d1                mov    %rdx,%r9
   9:   48 ba ff ff ff ff ff    movabs $0xfffffffffffffff,%rdx
  10:   ff ff 0f 
  13:   4c 89 c7                mov    %r8,%rdi
  16:   48 89 f0                mov    %rsi,%rax
  19:   49 89 c8                mov    %rcx,%r8
  1c:   48 89 f1                mov    %rsi,%rcx
  1f:   49 89 fa                mov    %rdi,%r10
  22:   48 0f ac f8 3c          shrd   $0x3c,%rdi,%rax
  27:   48 21 d1                and    %rdx,%rcx
  2a:   41 56                   push   %r14
  2c:   49 c1 ea 38             shr    $0x38,%r10
  30:   48 21 d0                and    %rdx,%rax
  33:   53                      push   %rbx
  34:   48 bb c5 4e ec c4 4e    movabs $0x4ec4ec4ec4ec4ec5,%rbx
  3b:   ec c4 4e 
  3e:   4c 01 d1                add    %r10,%rcx
  41:   45 31 db                xor    %r11d,%r11d
  44:   48 01 c1                add    %rax,%rcx
  47:   48 89 c8                mov    %rcx,%rax
  4a:   48 f7 e3                mul    %rbx
  4d:   48 c1 ea 02             shr    $0x2,%rdx
  51:   48 8d 04 52             lea    (%rdx,%rdx,2),%rax
  55:   48 8d 04 82             lea    (%rdx,%rax,4),%rax
  59:   48 89 ca                mov    %rcx,%rdx
  5c:   48 b9 ec c4 4e ec c4    movabs $0xc4ec4ec4ec4ec4ec,%rcx
  63:   4e ec c4 
  66:   48 29 c2                sub    %rax,%rdx
  69:   48 29 d6                sub    %rdx,%rsi
  6c:   49 89 d6                mov    %rdx,%r14
  6f:   4c 19 df                sbb    %r11,%rdi
  72:   48 0f af ce             imul   %rsi,%rcx
  76:   48 89 f2                mov    %rsi,%rdx
  79:   48 89 f8                mov    %rdi,%rax
  7c:   c4 e2 cb f6 fb          mulx   %rbx,%rsi,%rdi
  81:   48 0f af c3             imul   %rbx,%rax
  85:   49 89 31                mov    %rsi,(%r9)
  88:   48 01 c8                add    %rcx,%rax
  8b:   48 01 c7                add    %rax,%rdi
  8e:   49 89 79 08             mov    %rdi,0x8(%r9)
  92:   45 89 30                mov    %r14d,(%r8)
  95:   5b                      pop    %rbx
  96:   41 5e                   pop    %r14
  98:   c3                      retq


---


### compiler : `gcc`
### title : `Could std::has_single_bit be faster?`
### open_at : `2020-11-09T00:01:25Z`
### last_modified_date : `2023-09-21T13:56:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97759
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `10.2.1`
### severity : `enhancement`
### contents :
Hello gcc-team,

we are thrilled that C++20 offers some efficient bit implementation and that we could exchange some of our own implementation with the standardized ones, making the code more accessible.

I replaced our implementation and noticed that `std::has_single_bit` was slower than what we had before by around 30%. (The other functions matched our timings.)

Additionally, we have a (micro-)benchmark that compares the standard arithmetic bit trick (https://graphics.stanford.edu/~seander/bithacks.html#DetermineIfPowerOf2) with the implementation where popcount == 1. We decided to use the arithmetic version, because we measured that it was faster than popcount on our machines (mostly intel processors).

Interestingly, it seems that the popcount benchmark matches the std::has_single_bit time-wise, so I guess that std::has_single_bit is implemented via popcount.

Those timings could be reproduced at an unknown location https://quick-bench.com/q/Y28keu_mSh25WwhO05T4SKrbHpk

I don't know how to fix this, but I would expect that the optimizer would recognize popcount=1 and knows that there is a more efficient version. Or change the implementation to arithmetic, where again the optimizer could decide to replace that by a popcount if that is more efficient on some architecture?

Thank you!


---


### compiler : `gcc`
### title : `[ICELAKE]suboptimal vectorization for vpopcntw/b/q`
### open_at : `2020-11-10T02:12:22Z`
### last_modified_date : `2023-09-21T14:02:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97770
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c
---
void
foo(int* __restrict dest, int* src, int n)
{
  for (int i = 0; i != 8; i++)
    dest[i] = __builtin_popcount (src[i]);
}
---

gcc -O3 -march=icelake-server -S -fopt-info-all

Inlined 0 calls, eliminated 0 functions

test.c:4:3: missed: couldn't vectorize loop
test.c:5:15: missed: not vectorized: relevant stmt not supported: _7 = __builtin_popcount (_5);
test.c:2:1: note: vectorized 0 loops in function.
test.c:4:3: note: ***** Analysis failed with vector mode VOID
test.c:4:3: note: ***** Analysis failed with vector mode V8SI
test.c:4:3: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
test.c:6:1: note: ***** Analysis failed with vector mode VOID


This loop could be vectorized by ICC and Clang:

foo(int*, int*, int):
        vpopcntd  ymm0, YMMWORD PTR [rsi]                       #5.15
        vmovdqu   YMMWORD PTR [rdi], ymm0                       #5.5
        vzeroupper                                              #6.1
        ret


---


### compiler : `gcc`
### title : `Expressions evaluated as long chain instead of as tree or the like`
### open_at : `2020-11-10T16:51:48Z`
### last_modified_date : `2023-06-07T03:15:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97784
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
When compiling something like

#define O +
long x4(long x, long a, long b, long c, long d) { return x O a O b O c O d; }

we end up with machine code like

        add 3,3,4        # 10   [c=4 l=4]  *adddi3/0
        add 3,3,5        # 11   [c=4 l=4]  *adddi3/0
        add 3,3,6        # 12   [c=4 l=4]  *adddi3/0
        add 3,3,7        # 18   [c=4 l=4]  *adddi3/0
        blr              # 30   [c=4 l=4]  simple_return

Every of those "add" insns depends on the result of the previous one,
making this slower than necessary: it has the latency of 4 add insns in
series, while some can be done in parallel.


This problem is there on gimple level already:

  _1 = x_4(D) + a_5(D);
  _2 = _1 + b_6(D);
  _3 = _2 + c_7(D);
  _9 = _3 + d_8(D);
  return _9;


A very similar problem also happens as a result of RTL unrolling.


---


### compiler : `gcc`
### title : `rs6000 isinf etc. are pretty horrible`
### open_at : `2020-11-10T18:44:35Z`
### last_modified_date : `2020-11-11T07:35:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97786
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int isfinite(double x) { return __builtin_isfinite (x); }
int isinf(double x) { return __builtin_isinf (x); }
int isinf_sign(double x) { return __builtin_isinf_sign (x); }
int isnan(double x) { return __builtin_isnan (x); }
int isnormal(double x) { return __builtin_isnormal (x); }
int fpclassify(double x) { return __builtin_fpclassify (5, 6, 7, 8, 9, x); }

We can generate much better code for all these than the generic code
we use now.


---


### compiler : `gcc`
### title : `AoSoA complex caxpy-like loops: AVX2+FMA -Ofast 7 times slower than -O3`
### open_at : `2020-11-14T20:44:21Z`
### last_modified_date : `2022-11-28T07:24:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97832
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
I am reporting under 'target' because AVX2+FMA is the only 256-bit SIMD platform I have to play with. If it's really tree-optomization, please change.

void foo(double* restrict y, const double* restrict x0, const double* restrict x1, int clen)
{
  int xi = clen & 2;
  double f00_re = x0[0+xi+0];
  double f10_re = x1[0+xi+0];
  double f01_re = x0[0+xi+1];
  double f11_re = x1[0+xi+1];
  double f00_im = x0[4+xi+0];
  double f10_im = x1[4+xi+0];
  double f01_im = x0[4+xi+1];
  double f11_im = x1[4+xi+1];
  int clen2 = (clen+xi) * 2;
  double* y0 = &y[0];
  double* y1 = &y[clen2];
  #pragma GCC unroll 0
  for (int c = 0; c < clen2; c += 8) {
    // y0[c] = y0[c] - x0[c]*conj(f00) - x1[c]*conj(f10);
    // y1[c] = y1[c] - x0[c]*conj(f01) - x1[c]*conj(f11);
    #pragma GCC unroll 4
    for (int k = 0; k < 4; ++k) {
      double x0_re = x0[c+0+k];
      double x0_im = x0[c+4+k];
      double y0_re = y0[c+0+k];
      double y0_im = y0[c+4+k];
      double y1_re = y1[c+0+k];
      double y1_im = y1[c+4+k];
      y0_re = y0_re - x0_re * f00_re - x0_im * f00_im;
      y0_im = y0_im + x0_re * f00_im - x0_im * f00_re;
      y1_re = y1_re - x0_re * f01_re - x0_im * f01_im;
      y1_im = y1_im + x0_re * f01_im - x0_im * f01_re;
      double x1_re = x1[c+0+k];
      double x1_im = x1[c+4+k];
      y0_re = y0_re - x1_re * f10_re - x1_im * f10_im;
      y0_im = y0_im + x1_re * f10_im - x1_im * f10_re;
      y1_re = y1_re - x1_re * f11_re - x1_im * f11_im;
      y1_im = y1_im + x1_re * f11_im - x1_im * f11_re;
      y0[c+0+k] = y0_re;
      y0[c+4+k] = y0_im;
      y1[c+0+k] = y1_re;
      y1[c+4+k] = y1_im;
    }
  }
}

When compiled with 'gcc.10.2. -march=skylake -O3' it produces pretty decent code. The only problem is over-aggressive load+op combining similar to what we already discussed in 97127. It seems, this problem can't be solved without major overhaul of gcc optimizer architecture, but luckily an impact is quite minor.
But when we compile with 'gcc.10.2. -march=skylake -Ofast' the fun begins:

.L5:
	vmovupd	(%r9), %ymm7
	vmovupd	64(%r9), %ymm6
	vunpcklpd	32(%r9), %ymm7, %ymm2
	vunpckhpd	32(%r9), %ymm7, %ymm0
	vmovupd	64(%r9), %ymm7
	vmovupd	192(%r9), %ymm4
	vunpckhpd	96(%r9), %ymm7, %ymm5
	vmovupd	128(%r9), %ymm7
	vunpcklpd	96(%r9), %ymm6, %ymm6
	vunpcklpd	160(%r9), %ymm7, %ymm3
	vunpckhpd	160(%r9), %ymm7, %ymm1
	vmovupd	192(%r9), %ymm7
	vunpcklpd	224(%r9), %ymm4, %ymm4
	vunpckhpd	224(%r9), %ymm7, %ymm8
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm3, %ymm3
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm6, %ymm2, %ymm7
	vunpckhpd	%ymm6, %ymm2, %ymm2
	vunpcklpd	%ymm4, %ymm3, %ymm6
	vunpckhpd	%ymm4, %ymm3, %ymm3
	vunpcklpd	%ymm5, %ymm0, %ymm4
	vunpckhpd	%ymm5, %ymm0, %ymm0
	vunpcklpd	%ymm8, %ymm1, %ymm5
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm3, %ymm3
	vunpcklpd	%ymm5, %ymm4, %ymm11
	vpermpd	$216, %ymm2, %ymm2
	vunpckhpd	%ymm5, %ymm4, %ymm4
	vunpckhpd	%ymm8, %ymm1, %ymm1
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm4, %ymm8
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm3, %ymm2, %ymm4
	vunpckhpd	%ymm3, %ymm2, %ymm2
	vpermpd	$216, %ymm2, %ymm5
	vunpcklpd	%ymm1, %ymm0, %ymm2
	vpermpd	$216, %ymm4, %ymm10
	vpermpd	$216, %ymm2, %ymm4
	vmovupd	64(%rax), %ymm2
	vmovupd	(%rax), %ymm3
	vmovupd	%ymm4, 448(%rsp)
	vunpckhpd	96(%rax), %ymm2, %ymm4
	vmovupd	128(%rax), %ymm2
	vpermpd	$216, %ymm6, %ymm6
	vunpckhpd	%ymm1, %ymm0, %ymm1
	vpermpd	$216, %ymm7, %ymm7
	vunpcklpd	32(%rax), %ymm3, %ymm9
	vunpckhpd	32(%rax), %ymm3, %ymm14
	vunpckhpd	160(%rax), %ymm2, %ymm0
	vmovupd	64(%rax), %ymm3
	vunpcklpd	%ymm6, %ymm7, %ymm12
	vunpckhpd	%ymm6, %ymm7, %ymm7
	vpermpd	$216, %ymm1, %ymm6
	vunpcklpd	160(%rax), %ymm2, %ymm1
	vmovupd	192(%rax), %ymm2
	vunpcklpd	96(%rax), %ymm3, %ymm3
	vmovupd	%ymm5, 416(%rsp)
	vunpcklpd	224(%rax), %ymm2, %ymm5
	vunpckhpd	224(%rax), %ymm2, %ymm2
	vpermpd	$216, %ymm3, %ymm3
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm9, %ymm9
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm0, %ymm0
	vmovupd	%ymm10, 384(%rsp)
	vpermpd	$216, %ymm14, %ymm14
	vunpcklpd	%ymm3, %ymm9, %ymm10
	vpermpd	$216, %ymm2, %ymm2
	vunpckhpd	%ymm3, %ymm9, %ymm9
	vunpcklpd	%ymm5, %ymm1, %ymm3
	vpermpd	$216, %ymm3, %ymm3
	vmovupd	%ymm8, 288(%rsp)
	vpermpd	$216, %ymm10, %ymm10
	vunpcklpd	%ymm4, %ymm14, %ymm8
	vunpckhpd	%ymm4, %ymm14, %ymm14
	vunpcklpd	%ymm2, %ymm0, %ymm4
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm8, %ymm8
	vunpckhpd	%ymm2, %ymm0, %ymm2
	vunpcklpd	%ymm3, %ymm10, %ymm0
	vpermpd	$216, %ymm0, %ymm13
	vunpcklpd	%ymm4, %ymm8, %ymm0
	vunpckhpd	%ymm4, %ymm8, %ymm8
	vpermpd	$216, %ymm2, %ymm2
	vunpckhpd	%ymm3, %ymm10, %ymm10
	vpermpd	$216, %ymm14, %ymm14
	vpermpd	$216, %ymm0, %ymm3
	vpermpd	$216, %ymm8, %ymm0
	vmovupd	%ymm6, 480(%rsp)
	vunpckhpd	%ymm5, %ymm1, %ymm1
	vmovupd	%ymm3, 512(%rsp)
	vmovupd	(%rsi), %ymm3
	vmovupd	%ymm0, 544(%rsp)
	vunpcklpd	%ymm2, %ymm14, %ymm0
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm0, %ymm4
	vpermpd	$216, %ymm9, %ymm9
	vunpcklpd	%ymm1, %ymm9, %ymm6
	vmovupd	%ymm4, 640(%rsp)
	vunpckhpd	%ymm1, %ymm9, %ymm9
	vunpcklpd	32(%rsi), %ymm3, %ymm4
	vunpckhpd	32(%rsi), %ymm3, %ymm1
	vmovupd	64(%rsi), %ymm3
	vunpckhpd	%ymm2, %ymm14, %ymm14
	vunpcklpd	96(%rsi), %ymm3, %ymm8
	vunpckhpd	96(%rsi), %ymm3, %ymm5
	vmovupd	128(%rsi), %ymm3
	vpermpd	$216, %ymm14, %ymm2
	vunpckhpd	160(%rsi), %ymm3, %ymm0
	vmovupd	%ymm2, 672(%rsp)
	vunpcklpd	160(%rsi), %ymm3, %ymm2
	vmovupd	192(%rsi), %ymm3
	vmovupd	192(%rsi), %ymm14
	vunpcklpd	224(%rsi), %ymm3, %ymm3
	vpermpd	$216, %ymm9, %ymm9
	vmovupd	%ymm9, 608(%rsp)
	vunpckhpd	224(%rsi), %ymm14, %ymm9
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$216, %ymm3, %ymm3
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm9, %ymm9
	vmovupd	%ymm6, 576(%rsp)
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm8, %ymm4, %ymm6
	vpermpd	$216, %ymm0, %ymm0
	vunpckhpd	%ymm8, %ymm4, %ymm4
	vunpcklpd	%ymm3, %ymm2, %ymm8
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$216, %ymm6, %ymm6
	vunpckhpd	%ymm3, %ymm2, %ymm2
	vunpcklpd	%ymm5, %ymm1, %ymm3
	vunpckhpd	%ymm5, %ymm1, %ymm1
	vunpcklpd	%ymm9, %ymm0, %ymm5
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm5, %ymm5
	vunpcklpd	%ymm8, %ymm6, %ymm14
	vpermpd	$216, %ymm4, %ymm4
	vunpckhpd	%ymm8, %ymm6, %ymm6
	vpermpd	$216, %ymm3, %ymm3
	vunpckhpd	%ymm9, %ymm0, %ymm0
	vpermpd	$216, %ymm6, %ymm9
	vunpcklpd	%ymm5, %ymm3, %ymm6
	vunpckhpd	%ymm5, %ymm3, %ymm3
	vunpcklpd	%ymm2, %ymm4, %ymm5
	vunpckhpd	%ymm2, %ymm4, %ymm4
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm4, %ymm2
	vpermpd	$216, %ymm1, %ymm1
	vmovupd	%ymm2, 832(%rsp)
	vunpcklpd	%ymm0, %ymm1, %ymm2
	vunpckhpd	%ymm0, %ymm1, %ymm1
	vpermpd	$216, %ymm1, %ymm0
	vmovupd	%ymm0, 896(%rsp)
	vmovupd	(%rbx), %ymm0
	vpermpd	$216, %ymm2, %ymm4
	vunpckhpd	32(%rbx), %ymm0, %ymm1
	vunpcklpd	32(%rbx), %ymm0, %ymm2
	vmovupd	64(%rbx), %ymm0
	vpermpd	$216, %ymm5, %ymm5
	vmovupd	%ymm5, 800(%rsp)
	vmovupd	%ymm4, 864(%rsp)
	vunpcklpd	96(%rbx), %ymm0, %ymm5
	vunpckhpd	96(%rbx), %ymm0, %ymm4
	vmovupd	128(%rbx), %ymm0
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm3, %ymm3
	vmovupd	%ymm9, 704(%rsp)
	vmovupd	%ymm6, 736(%rsp)
	vmovupd	%ymm3, 768(%rsp)
	vunpcklpd	160(%rbx), %ymm0, %ymm3
	vmovupd	192(%rbx), %ymm8
	vunpckhpd	160(%rbx), %ymm0, %ymm0
	vunpcklpd	224(%rbx), %ymm8, %ymm6
	vunpckhpd	224(%rbx), %ymm8, %ymm9
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm9, %ymm9
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm1, %ymm1
	vpermpd	$216, %ymm3, %ymm3
	vpermpd	$216, %ymm0, %ymm0
	vunpcklpd	%ymm5, %ymm2, %ymm8
	vunpckhpd	%ymm5, %ymm2, %ymm2
	vunpcklpd	%ymm6, %ymm3, %ymm5
	vunpckhpd	%ymm6, %ymm3, %ymm3
	vunpcklpd	%ymm4, %ymm1, %ymm6
	vunpckhpd	%ymm4, %ymm1, %ymm1
	vunpcklpd	%ymm9, %ymm0, %ymm4
	vunpckhpd	%ymm9, %ymm0, %ymm0
	vpermpd	$216, %ymm5, %ymm5
	vpermpd	$216, %ymm3, %ymm3
	vpermpd	$216, %ymm4, %ymm4
	vpermpd	$216, %ymm0, %ymm0
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$216, %ymm2, %ymm2
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm1, %ymm1
	vunpcklpd	%ymm5, %ymm8, %ymm9
	vunpckhpd	%ymm5, %ymm8, %ymm8
	vunpcklpd	%ymm4, %ymm6, %ymm5
	vunpckhpd	%ymm4, %ymm6, %ymm6
	vunpcklpd	%ymm3, %ymm2, %ymm4
	vunpckhpd	%ymm3, %ymm2, %ymm2
	vunpcklpd	%ymm0, %ymm1, %ymm3
	vunpckhpd	%ymm0, %ymm1, %ymm1
	vpermpd	$216, %ymm9, %ymm9
	vpermpd	$216, %ymm8, %ymm8
	vpermpd	$216, %ymm1, %ymm0
	vpermpd	$216, %ymm10, %ymm15
	vmovupd	%ymm0, 240(%rsp)
	vmulpd	320(%rsp), %ymm9, %ymm10
	vmulpd	64(%rsp), %ymm8, %ymm0
	vmovupd	(%rsp), %ymm1
	vpermpd	$216, %ymm12, %ymm12
	vpermpd	$216, %ymm7, %ymm7
	vfmadd231pd	176(%rsp), %ymm12, %ymm10
	vfmadd231pd	%ymm1, %ymm7, %ymm0
	vpermpd	$216, %ymm14, %ymm14
	vpermpd	$216, %ymm11, %ymm11
	vpermpd	$216, %ymm6, %ymm6
	vpermpd	$216, %ymm5, %ymm5
	vaddpd	%ymm10, %ymm0, %ymm0
	vmulpd	64(%rsp), %ymm9, %ymm10
	vpermpd	$216, %ymm2, %ymm2
	vsubpd	%ymm0, %ymm13, %ymm0
	vmulpd	320(%rsp), %ymm8, %ymm13
	vpermpd	$216, %ymm4, %ymm4
	vfmadd231pd	%ymm1, %ymm12, %ymm10
	vmovupd	%ymm0, 352(%rsp)
	vmovupd	208(%rsp), %ymm0
	vfmadd231pd	176(%rsp), %ymm7, %ymm13
	vpermpd	$216, %ymm3, %ymm3
	addq	$256, %r9
	addq	$256, %rax
	addq	$256, %rsi
	vsubpd	%ymm13, %ymm10, %ymm10
	vmulpd	%ymm0, %ymm9, %ymm13
	vmulpd	96(%rsp), %ymm9, %ymm9
	vaddpd	%ymm15, %ymm10, %ymm10
	vmulpd	96(%rsp), %ymm8, %ymm15
	vmulpd	%ymm0, %ymm8, %ymm8
	vmovupd	%ymm10, 928(%rsp)
	vmovupd	128(%rsp), %ymm10
	vfmadd231pd	32(%rsp), %ymm12, %ymm13
	vfmadd231pd	%ymm10, %ymm12, %ymm9
	vmovupd	32(%rsp), %ymm12
	vfmadd231pd	%ymm10, %ymm7, %ymm15
	vfmadd231pd	%ymm12, %ymm7, %ymm8
	vmovupd	(%rsp), %ymm7
	addq	$256, %rbx
	addq	$256, %r11
	vaddpd	%ymm15, %ymm13, %ymm13
	vsubpd	%ymm8, %ymm9, %ymm9
	vmovapd	%ymm10, %ymm15
	vmovupd	288(%rsp), %ymm10
	vsubpd	%ymm13, %ymm14, %ymm1
	vaddpd	704(%rsp), %ymm9, %ymm13
	vmulpd	%ymm15, %ymm10, %ymm9
	vmulpd	%ymm10, %ymm7, %ymm7
	vmovupd	176(%rsp), %ymm14
	vmovupd	320(%rsp), %ymm15
	vmovupd	%ymm1, 960(%rsp)
	vfmadd231pd	%ymm12, %ymm11, %ymm9
	vmovupd	64(%rsp), %ymm12
	vfmadd231pd	%ymm14, %ymm11, %ymm7
	vmulpd	%ymm12, %ymm6, %ymm8
	vmovupd	512(%rsp), %ymm1
	vfmadd231pd	%ymm15, %ymm5, %ymm8
	vaddpd	%ymm8, %ymm7, %ymm7
	vmulpd	%ymm12, %ymm5, %ymm8
	vmulpd	%ymm15, %ymm6, %ymm12
	vsubpd	%ymm7, %ymm1, %ymm7
	vfmadd231pd	(%rsp), %ymm11, %ymm8
	vfmadd231pd	%ymm14, %ymm10, %ymm12
	vsubpd	%ymm12, %ymm8, %ymm8
	vmulpd	96(%rsp), %ymm6, %ymm12
	vmulpd	%ymm0, %ymm6, %ymm6
	vaddpd	544(%rsp), %ymm8, %ymm8
	vmovupd	736(%rsp), %ymm1
	vmovupd	288(%rsp), %ymm10
	vfmadd231pd	%ymm0, %ymm5, %ymm12
	vmulpd	96(%rsp), %ymm5, %ymm5
	vmovupd	416(%rsp), %ymm0
	vaddpd	%ymm12, %ymm9, %ymm9
	vmovupd	32(%rsp), %ymm12
	vsubpd	%ymm9, %ymm1, %ymm9
	vmovupd	128(%rsp), %ymm1
	vfmadd231pd	%ymm12, %ymm10, %ymm6
	vfmadd231pd	%ymm1, %ymm11, %ymm5
	vmovupd	384(%rsp), %ymm10
	vmovupd	%ymm9, 512(%rsp)
	vsubpd	%ymm6, %ymm5, %ymm11
	vmulpd	%ymm1, %ymm0, %ymm5
	vmovupd	(%rsp), %ymm6
	vmovupd	576(%rsp), %ymm1
	vmulpd	%ymm0, %ymm6, %ymm9
	vaddpd	768(%rsp), %ymm11, %ymm11
	vfmadd231pd	%ymm12, %ymm10, %ymm5
	vmovupd	64(%rsp), %ymm12
	vmulpd	%ymm12, %ymm2, %ymm6
	vfmadd231pd	%ymm10, %ymm14, %ymm9
	vfmadd231pd	%ymm15, %ymm4, %ymm6
	vaddpd	%ymm9, %ymm6, %ymm6
	vmulpd	%ymm12, %ymm4, %ymm9
	vmulpd	%ymm15, %ymm2, %ymm12
	vsubpd	%ymm6, %ymm1, %ymm6
	vmovupd	800(%rsp), %ymm1
	vfmadd231pd	(%rsp), %ymm10, %ymm9
	vfmadd231pd	%ymm14, %ymm0, %ymm12
	vsubpd	%ymm12, %ymm9, %ymm9
	vmulpd	96(%rsp), %ymm2, %ymm12
	vmulpd	208(%rsp), %ymm2, %ymm2
	vaddpd	608(%rsp), %ymm9, %ymm9
	vfmadd231pd	208(%rsp), %ymm4, %ymm12
	vmulpd	96(%rsp), %ymm4, %ymm4
	vaddpd	%ymm12, %ymm5, %ymm5
	vfmadd231pd	128(%rsp), %ymm10, %ymm4
	vmovupd	480(%rsp), %ymm10
	vsubpd	%ymm5, %ymm1, %ymm5
	vmovapd	%ymm0, %ymm1
	vmovupd	32(%rsp), %ymm0
	vfmadd231pd	%ymm0, %ymm1, %ymm2
	vmovupd	448(%rsp), %ymm1
	vsubpd	%ymm2, %ymm4, %ymm4
	vmovupd	(%rsp), %ymm2
	vmulpd	%ymm10, %ymm2, %ymm12
	vmulpd	128(%rsp), %ymm10, %ymm2
	vaddpd	832(%rsp), %ymm4, %ymm4
	vfmadd231pd	%ymm1, %ymm14, %ymm12
	vfmadd231pd	%ymm0, %ymm1, %ymm2
	vmovupd	240(%rsp), %ymm0
	vmulpd	64(%rsp), %ymm0, %ymm14
	vfmadd231pd	%ymm15, %ymm3, %ymm14
	vmulpd	%ymm0, %ymm15, %ymm15
	vaddpd	%ymm14, %ymm12, %ymm12
	vmovupd	640(%rsp), %ymm14
	vfmadd231pd	176(%rsp), %ymm10, %ymm15
	vsubpd	%ymm12, %ymm14, %ymm12
	vmulpd	64(%rsp), %ymm3, %ymm14
	vfmadd231pd	(%rsp), %ymm1, %ymm14
	vsubpd	%ymm15, %ymm14, %ymm14
	vaddpd	672(%rsp), %ymm14, %ymm14
	vmulpd	96(%rsp), %ymm0, %ymm15
	vmovupd	208(%rsp), %ymm0
	vfmadd231pd	%ymm0, %ymm3, %ymm15
	vmulpd	96(%rsp), %ymm3, %ymm3
	vaddpd	%ymm15, %ymm2, %ymm2
	vmovupd	864(%rsp), %ymm15
	vfmadd231pd	128(%rsp), %ymm1, %ymm3
	vsubpd	%ymm2, %ymm15, %ymm2
	vmovupd	240(%rsp), %ymm15
	vmulpd	%ymm0, %ymm15, %ymm1
	vpermpd	$68, 352(%rsp), %ymm15
	vpermpd	$238, 352(%rsp), %ymm0
	vfmadd231pd	32(%rsp), %ymm10, %ymm1
	vmovupd	928(%rsp), %ymm10
	vsubpd	%ymm1, %ymm3, %ymm1
	vpermpd	$68, %ymm10, %ymm3
	vpermpd	$238, %ymm10, %ymm10
	vshufpd	$12, %ymm3, %ymm15, %ymm3
	vshufpd	$12, %ymm10, %ymm0, %ymm10
	vpermpd	$68, %ymm7, %ymm15
	vpermpd	$68, %ymm8, %ymm0
	vpermpd	$238, %ymm7, %ymm7
	vpermpd	$238, %ymm8, %ymm8
	vshufpd	$12, %ymm0, %ymm15, %ymm15
	vshufpd	$12, %ymm8, %ymm7, %ymm7
	vpermpd	$68, %ymm9, %ymm0
	vpermpd	$68, %ymm6, %ymm8
	vshufpd	$12, %ymm0, %ymm8, %ymm8
	vpermpd	$238, %ymm6, %ymm6
	vpermpd	$238, %ymm9, %ymm0
	vshufpd	$12, %ymm0, %ymm6, %ymm0
	vpermpd	$68, %ymm12, %ymm9
	vpermpd	$68, %ymm14, %ymm6
	vpermpd	$238, %ymm12, %ymm12
	vpermpd	$238, %ymm14, %ymm14
	vshufpd	$12, %ymm6, %ymm9, %ymm6
	vshufpd	$12, %ymm14, %ymm12, %ymm12
	vpermpd	$68, %ymm8, %ymm9
	vpermpd	$68, %ymm3, %ymm14
	vpermpd	$238, %ymm8, %ymm8
	vpermpd	$238, %ymm3, %ymm3
	vshufpd	$12, %ymm9, %ymm14, %ymm9
	vshufpd	$12, %ymm8, %ymm3, %ymm8
	vpermpd	$68, %ymm10, %ymm14
	vpermpd	$68, %ymm0, %ymm3
	vpermpd	$238, %ymm10, %ymm10
	vpermpd	$238, %ymm0, %ymm0
	vshufpd	$12, %ymm3, %ymm14, %ymm3
	vshufpd	$12, %ymm0, %ymm10, %ymm0
	vpermpd	$68, %ymm15, %ymm14
	vpermpd	$68, %ymm6, %ymm10
	vpermpd	$238, %ymm15, %ymm15
	vpermpd	$238, %ymm6, %ymm6
	vshufpd	$12, %ymm10, %ymm14, %ymm10
	vshufpd	$12, %ymm6, %ymm15, %ymm15
	vpermpd	$68, %ymm7, %ymm14
	vpermpd	$68, %ymm12, %ymm6
	vpermpd	$238, %ymm7, %ymm7
	vpermpd	$238, %ymm12, %ymm12
	vshufpd	$12, %ymm6, %ymm14, %ymm6
	vshufpd	$12, %ymm12, %ymm7, %ymm7
	vpermpd	$68, %ymm10, %ymm14
	vpermpd	$68, %ymm9, %ymm12
	vpermpd	$238, %ymm10, %ymm10
	vpermpd	$238, %ymm9, %ymm9
	vshufpd	$12, %ymm10, %ymm9, %ymm9
	vpermpd	$68, %ymm15, %ymm10
	vmovupd	%ymm9, -224(%rax)
	vpermpd	$238, %ymm15, %ymm15
	vpermpd	$68, %ymm8, %ymm9
	vpermpd	$238, %ymm8, %ymm8
	vshufpd	$12, %ymm10, %ymm9, %ymm9
	vshufpd	$12, %ymm15, %ymm8, %ymm8
	vmovupd	%ymm9, -192(%rax)
	vmovupd	%ymm8, -160(%rax)
	vpermpd	$68, %ymm6, %ymm9
	vpermpd	$68, %ymm3, %ymm8
	vpermpd	$238, %ymm6, %ymm6
	vpermpd	$238, %ymm3, %ymm3
	vshufpd	$12, %ymm6, %ymm3, %ymm3
	vpermpd	$68, %ymm7, %ymm6
	vmovupd	%ymm3, -96(%rax)
	vpermpd	$238, %ymm7, %ymm7
	vpermpd	$68, %ymm0, %ymm3
	vpermpd	$238, %ymm0, %ymm0
	vshufpd	$12, %ymm7, %ymm0, %ymm0
	vmovupd	960(%rsp), %ymm7
	vshufpd	$12, %ymm6, %ymm3, %ymm3
	vshufpd	$12, %ymm14, %ymm12, %ymm12
	vmovupd	%ymm3, -64(%rax)
	vpermpd	$238, %ymm7, %ymm14
	vpermpd	$68, %ymm7, %ymm3
	vmovupd	512(%rsp), %ymm7
	vmovupd	%ymm0, -32(%rax)
	vaddpd	896(%rsp), %ymm1, %ymm1
	vpermpd	$68, %ymm13, %ymm0
	vshufpd	$12, %ymm0, %ymm3, %ymm3
	vpermpd	$68, %ymm7, %ymm6
	vpermpd	$68, %ymm11, %ymm0
	vshufpd	$12, %ymm9, %ymm8, %ymm8
	vshufpd	$12, %ymm0, %ymm6, %ymm6
	vmovupd	%ymm8, -128(%rax)
	vpermpd	$238, %ymm7, %ymm9
	vpermpd	$68, %ymm4, %ymm0
	vpermpd	$68, %ymm5, %ymm8
	vpermpd	$238, %ymm11, %ymm11
	vshufpd	$12, %ymm11, %ymm9, %ymm11
	vshufpd	$12, %ymm0, %ymm8, %ymm8
	vpermpd	$68, %ymm2, %ymm9
	vpermpd	$68, %ymm1, %ymm0
	vshufpd	$12, %ymm0, %ymm9, %ymm9
	vpermpd	$238, %ymm5, %ymm5
	vpermpd	$68, %ymm8, %ymm0
	vpermpd	$68, %ymm3, %ymm7
	vpermpd	$238, %ymm13, %ymm13
	vpermpd	$238, %ymm4, %ymm4
	vshufpd	$12, %ymm4, %ymm5, %ymm4
	vshufpd	$12, %ymm0, %ymm7, %ymm7
	vpermpd	$238, %ymm2, %ymm2
	vpermpd	$68, %ymm4, %ymm0
	vshufpd	$12, %ymm13, %ymm14, %ymm13
	vpermpd	$238, %ymm4, %ymm4
	vpermpd	$68, %ymm13, %ymm10
	vpermpd	$238, %ymm1, %ymm1
	vpermpd	$238, %ymm13, %ymm13
	vshufpd	$12, %ymm1, %ymm2, %ymm1
	vshufpd	$12, %ymm0, %ymm10, %ymm10
	vpermpd	$68, %ymm9, %ymm2
	vshufpd	$12, %ymm4, %ymm13, %ymm0
	vpermpd	$68, %ymm6, %ymm4
	vshufpd	$12, %ymm2, %ymm4, %ymm4
	vpermpd	$238, %ymm8, %ymm8
	vpermpd	$68, %ymm1, %ymm2
	vpermpd	$68, %ymm11, %ymm5
	vpermpd	$238, %ymm3, %ymm3
	vshufpd	$12, %ymm8, %ymm3, %ymm3
	vshufpd	$12, %ymm2, %ymm5, %ymm5
	vpermpd	$68, %ymm4, %ymm8
	vpermpd	$68, %ymm7, %ymm2
	vpermpd	$238, %ymm4, %ymm4
	vpermpd	$238, %ymm6, %ymm6
	vpermpd	$238, %ymm9, %ymm9
	vpermpd	$238, %ymm7, %ymm7
	vmovupd	%ymm12, -256(%rax)
	vshufpd	$12, %ymm9, %ymm6, %ymm6
	vshufpd	$12, %ymm8, %ymm2, %ymm2
	vshufpd	$12, %ymm4, %ymm7, %ymm7
	vmovupd	%ymm2, -256(%r11)
	vpermpd	$68, %ymm6, %ymm4
	vpermpd	$68, %ymm3, %ymm2
	vpermpd	$238, %ymm6, %ymm6
	vpermpd	$238, %ymm3, %ymm3
	vshufpd	$12, %ymm4, %ymm2, %ymm2
	vshufpd	$12, %ymm6, %ymm3, %ymm3
	vmovupd	%ymm2, -192(%r11)
	vmovupd	%ymm3, -160(%r11)
	vpermpd	$68, %ymm10, %ymm2
	vpermpd	$68, %ymm5, %ymm3
	vpermpd	$238, %ymm11, %ymm11
	vpermpd	$238, %ymm1, %ymm1
	vshufpd	$12, %ymm3, %ymm2, %ymm2
	vshufpd	$12, %ymm1, %ymm11, %ymm1
	vmovupd	%ymm2, -128(%r11)
	vpermpd	$68, %ymm1, %ymm3
	vpermpd	$68, %ymm0, %ymm2
	vpermpd	$238, %ymm10, %ymm10
	vpermpd	$238, %ymm5, %ymm5
	vpermpd	$238, %ymm0, %ymm0
	vpermpd	$238, %ymm1, %ymm1
	vshufpd	$12, %ymm5, %ymm10, %ymm5
	vshufpd	$12, %ymm3, %ymm2, %ymm2
	vshufpd	$12, %ymm1, %ymm0, %ymm1
	vmovupd	%ymm7, -224(%r11)
	vmovupd	%ymm5, -96(%r11)
	vmovupd	%ymm2, -64(%r11)
	vmovupd	%ymm1, -32(%r11)
	cmpq	%r9, %rdi
	jne	.L5

That's almost 7 times slower than -O3, 2.4 times slower than scalar code, generated by -O2 and twice slower than clang -Ofast.
Being twice slower than clang is not a small fit.

I knew about this bug several weeks ago, but somehow didn't realize that 11.0 is so near, so was lazy to report at time.
Now I am sorry.

Sources and compilation scripts for bigger, more real-world testbench here:
https://github.com/already5chosen/others/tree/master/cholesky_solver/gcc-badopt-aosoa-caxpy2x2


---


### compiler : `gcc`
### title : `[missed optimization] tls init function check emitted for consinit thread_local variables (C++20)`
### open_at : `2020-11-16T07:46:30Z`
### last_modified_date : `2021-11-29T15:32:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97848
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
The code

=== begin code ===


extern thread_local constinit int x;

int foo_good() {
    return x;
}

void set_foo(int y) {
    x = y;
}

=== end code ===

knows that x will be statically initialized, but still emits the code to check for the TLS init function and maybe call it:

=== begin objdump ===

foo_good():
        movl    $TLS init function for x, %eax
        testq   %rax, %rax
        je      .L7
        subq    $8, %rsp
        call    TLS init function for x
        movq    x@gottpoff(%rip), %rax
        movl    %fs:(%rax), %eax
        addq    $8, %rsp
        ret
.L7:
        movq    x@gottpoff(%rip), %rax
        movl    %fs:(%rax), %eax
        ret
set_foo(int):
        movl    $TLS init function for x, %eax
        pushq   %rbx
        movl    %edi, %ebx
        testq   %rax, %rax
        je      .L12
        call    TLS init function for x
.L12:
        movq    x@gottpoff(%rip), %rax
        movl    %ebx, %fs:(%rax)
        popq    %rbx
        ret

=== end objdump ===

Instead, we should see the same output as for __thread_local.


---


### compiler : `gcc`
### title : `Missed optimization: repeated call`
### open_at : `2020-11-16T17:40:55Z`
### last_modified_date : `2021-09-01T04:47:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97856
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
For code such as:

    void f(int);
    void g(int v) {
        switch (v) {
            case 1: f(1);
            case 2: f(2);
            case 3: f(3);
            default: f(4);
        }
    }

GCC generates for e.g. x86_64:

    g(int):
            subq    $8, %rsp
            cmpl    $2, %edi
            je      .L2
            cmpl    $3, %edi
            je      .L3
            cmpl    $1, %edi
            je      .L13
            movl    $4, %edi
            addq    $8, %rsp
            jmp     f(int)
    .L13:
            call    f(int)
    .L2:
            movl    $2, %edi
            call    f(int)
    .L3:
            movl    $3, %edi
            call    f(int)
            movl    $4, %edi
            addq    $8, %rsp
            jmp     f(int)

Repeating the `f(4);` call can be avoided by reordering the cases, reducing code size.


---


### compiler : `gcc`
### title : `[ARM NEON] Missed optimization for less-than comparison on vectors`
### open_at : `2020-11-17T07:49:49Z`
### last_modified_date : `2021-05-04T12:31:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97872
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Hi,
For the following test-case:

#include <arm_neon.h>

uint8x8_t f1(int8x8_t a, int8x8_t b) {
  return a < b;
}

uint8x8_t f2(int8x8_t a, int8x8_t b) {
  return vclt_s8 (a, b);
}

Code-gen for f2 uses vcgt insn
f2:
        vcgt.s8 d0, d1, d0
        bx      lr

However code-gen for f1 results in:
f1:
        vmov.i32 d16, #0xffffffff  @ v8qi
        vmov.i32 d17, #0  @ v8qi
        vcgt.s8 d0, d1, d0
        vbsl    d0, d16, d17
        bx      lr

which IIUC is redundant, since vcgt will set all-ones, or all-zeros in d0 depending on the comparison.

The reason this happens is because vclt_s8 uses __builtin_neon_vcgtv8qi that emits vcgt.s8, while f1 is lowered to using VCOND in optimized dump:

f1 (int8x8_t a, int8x8_t b)
{
  vector(8) signed char _2;
  uint8x8_t _5;

  <bb 2> [local count: 1073741824]:
  _2 = .VCOND (a_3(D), b_4(D), { -1, -1, -1, -1, -1, -1, -1, -1 }, { 0, 0, 0, 0, 0, 0, 0, 0 }, 107);
  _5 = VIEW_CONVERT_EXPR<uint8x8_t>(_2);
  return _5;

}

and correspondingly expanded to:
;; _2 = .VCOND (a_3(D), b_4(D), { -1, -1, -1, -1, -1, -1, -1, -1 }, { 0, 0, 0, 0, 0, 0, 0, 0 }, 107);

(insn 7 6 8 (set (reg:V8QI 117)
        (const_vector:V8QI [
                (const_int -1 [0xffffffffffffffff]) repeated x8
            ])) "foo.c":4:12 -1
     (nil))

(insn 8 7 9 (set (reg:V8QI 118)
        (const_vector:V8QI [
                (const_int 0 [0]) repeated x8
            ])) "foo.c":4:12 -1
     (nil))

(insn 9 8 10 (set (reg:V8QI 119)
        (neg:V8QI (gt:V8QI (reg/v:V8QI 116 [ b ])
                (reg/v:V8QI 115 [ a ])))) "foo.c":4:12 -1
     (nil))

(insn 10 9 0 (set (reg:V8QI 113 [ _2 ])
        (unspec:V8QI [
                (reg:V8QI 119)
                (reg:V8QI 117)
                (reg:V8QI 118)
            ] UNSPEC_VBSL)) "foo.c":4:12 -1
     (nil))

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `Failure to optimize abs optimally (at least one completely useless instruction on x86)`
### open_at : `2020-11-17T10:05:11Z`
### last_modified_date : `2023-09-21T10:55:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97873
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int abs(int x)
{
        return (x < 0) ? -x : x;
}

For x86 -O3, LLVM outputs this:

abs:
  mov eax, edi
  neg eax
  cmovl eax, edi
  ret

GCC outputs this:

abs:
  mov eax, edi
  neg eax
  cmp eax, edi
  cmovl eax, edi
  ret


---


### compiler : `gcc`
### title : `suboptimal loop vectorization`
### open_at : `2020-11-17T13:11:41Z`
### last_modified_date : `2021-01-12T16:52:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97875
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Looking at the code generated for gcc.target/arm/simd/mve-vsub_1.c:
#include <stdint.h>

void test_vsub_i32 (int32_t * dest, int32_t * a, int32_t * b) {
  int i;
  for (i=0; i<4; i++) {
    dest[i] = a[i] - b[i];
  }
}

Compiled with -mfloat-abi=hard -mfpu=auto -march=armv8.1-m.main+mve -mthumb -O3, we get:
test_vsub_i32:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        add     ip, r1, #4
        adds    r3, r2, #4
        sub     ip, r0, ip
        subs    r3, r0, r3
        cmp     ip, #8
        it      hi
        cmphi   r3, #8
        bls     .L2
        orr     r3, r2, r0
        orrs    r3, r3, r1
        lsls    r3, r3, #28
        bne     .L2
        vldrw.32        q3, [r1]
        vldrw.32        q2, [r2]
        vsub.i32        q3, q3, q2
        vstrw.32        q3, [r0]
        bx      lr
.L2:
        ldr     r3, [r1]
        push    {r4}
        ldr     r4, [r2]
        subs    r3, r3, r4
        str     r3, [r0]
        ldr     r4, [r2, #4]
        ldr     r3, [r1, #4]
        subs    r3, r3, r4
        str     r3, [r0, #4]
        ldr     r4, [r2, #8]
        ldr     r3, [r1, #8]
        subs    r3, r3, r4
        str     r3, [r0, #8]
        ldr     r3, [r1, #12]
        ldr     r2, [r2, #12]
        ldr     r4, [sp], #4
        subs    r3, r3, r2
        str     r3, [r0, #12]
        bx      lr


but only the short vectorized part is necessary:
        vldrw.32        q3, [r1]
        vldrw.32        q2, [r2]
        vsub.i32        q3, q3, q2
        vstrw.32        q3, [r0]
        bx      lr

Since the loop trip count is constant (=4), why isn't this better optimized?


If I declare 'dest' as __restrict__, I get something better, but still not perfect:
test_vsub_i32:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        orr     r3, r2, r0
        orrs    r3, r3, r1
        lsls    r3, r3, #28
        bne     .L2
        vldrw.32        q3, [r1]
        vldrw.32        q2, [r2]
        vsub.i32        q3, q3, q2
        vstrw.32        q3, [r0]
        bx      lr
.L2:
        push    {r4, r5}
        ldr     r3, [r1]
        ldr     r4, [r2]
        subs    r4, r3, r4
        str     r4, [r0]
        ldr     r3, [r1, #4]
        ldr     r4, [r2, #4]
        subs    r5, r3, r4
        str     r5, [r0, #4]
        ldrd    r4, r3, [r1, #8]
        ldrd    r5, r1, [r2, #8]
        subs    r4, r4, r5
        subs    r3, r3, r1
        strd    r4, r3, [r0, #8]
        pop     {r4, r5}
        bx      lr



Compiling for cortex-a9 and Neon:
-mfloat-abi=hard -mcpu=cortex-a9 -mfpu=neon -O3
test_vsub_i32:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        add     ip, r2, #4
        adds    r3, r1, #4
        sub     ip, r0, ip
        subs    r3, r0, r3
        cmp     ip, #8
        it      hi
        cmphi   r3, #8
        bls     .L2
        vld1.32 {q8}, [r1]
        vld1.32 {q9}, [r2]
        vsub.i32        q8, q8, q9
        vst1.32 {q8}, [r0]
        bx      lr
.L2:
        ldr     r3, [r1]
        push    {r4}
        ldr     r4, [r2]
        subs    r3, r3, r4
        str     r3, [r0]
        ldr     r4, [r2, #4]
        ldr     r3, [r1, #4]
        subs    r3, r3, r4
        str     r3, [r0, #4]
        ldr     r4, [r2, #8]
        ldr     r3, [r1, #8]
        subs    r3, r3, r4
        ldr     r4, [sp], #4
        str     r3, [r0, #8]
        ldr     r3, [r1, #12]
        ldr     r2, [r2, #12]
        subs    r3, r3, r2
        str     r3, [r0, #12]
        bx      lr


But in this case adding __restrict__ works well:
test_vsub_i32:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        vld1.32 {q8}, [r1]
        vld1.32 {q9}, [r2]
        vsub.i32        q8, q8, q9
        vst1.32 {q8}, [r0]
        bx      lr


---


### compiler : `gcc`
### title : `[10/11 Regression] Failure to optimize neg plus div to avoid using x87 floating point stack`
### open_at : `2020-11-18T09:35:52Z`
### last_modified_date : `2023-09-21T10:54:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97887
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
float f(float a)
{
    return -a / a;
}

On x86 -O3, LLVM outputs this:

.LCPI0_0:
  .long 0x80000000 # float -0
  .long 0x80000000 # float -0
  .long 0x80000000 # float -0
  .long 0x80000000 # float -0
f(float):
  movaps xmm1, xmmword ptr [rip + .LCPI0_0] # xmm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
  xorps xmm1, xmm0
  divss xmm1, xmm0
  movaps xmm0, xmm1
  ret

GCC outputs this:

f(float):
  movss DWORD PTR [rsp-4], xmm0
  fld DWORD PTR [rsp-4]
  movaps xmm1, xmm0
  fchs
  fstp DWORD PTR [rsp-4]
  movss xmm0, DWORD PTR [rsp-4]
  divss xmm0, xmm1
  ret

I'm *pretty sure* that loading the value into the x87 stack (especially mixed with SSE instructions) is much slower than using SSE instructions for this.


---


### compiler : `gcc`
### title : `[x86] Consider using registers on large initializations`
### open_at : `2020-11-18T12:08:17Z`
### last_modified_date : `2020-12-24T02:34:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97891
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Consider the following example code:

struct A
{
    long a;
    short b;
    int c;
    char d;
    long x;
    bool y;
    int z;
    char* p;

    A() :
        a(0), b(0), c(0), d(0), x(0), y(false), z(0), p(0)
    {}
};

void test(A* p, unsigned int count)
{
    for (unsigned int i = 0; i < count; ++i)
    {
        p[i] = A();
    }
}

When compiled with "-O3 -march=nehalem" the generated code is:

test(A*, unsigned int):
        testl   %esi, %esi
        je      .L1
        leal    -1(%rsi), %eax
        leaq    (%rax,%rax,2), %rax
        salq    $4, %rax
        leaq    48(%rdi,%rax), %rax
.L3:
        xorl    %edx, %edx
        movq    $0, (%rdi)
        addq    $48, %rdi
        movw    %dx, -40(%rdi)
        movl    $0, -36(%rdi)
        movb    $0, -32(%rdi)
        movq    $0, -24(%rdi)
        movb    $0, -16(%rdi)
        movl    $0, -12(%rdi)
        movq    $0, -8(%rdi)
        cmpq    %rax, %rdi
        jne     .L3
.L1:
        ret

https://gcc.godbolt.org/z/TrfWYr

Here, the main loop body between .L3 and .L1 is 60 bytes large, with a significant amount of space wasted on the $0 constants encoded in mov instructions. It would be more efficient to use a single zero register in all member initializations, especially given that %edx is already used like that.

A loop rewritten like this:

    for (unsigned int i = 0; i < count; ++i)
    {
        __asm__
        (
            "movq    %q1, (%0)\n\t"
            "movw    %w1, 8(%0)\n\t"
            "movl    %1, 12(%0)\n\t"
            "movb    %b1, 16(%0)\n\t"
            "movq    %q1, 24(%0)\n\t"
            "movb    %b1, 32(%0)\n\t"
            "movl    %1, 36(%0)\n\t"
            "movq    %q1, 40(%0)\n\t"
            : : "r" (p + i), "q" (0)
        );
    }

compiles to:

test(A*, unsigned int):
        testl   %esi, %esi
        je      .L1
        leal    -1(%rsi), %eax
        leaq    (%rax,%rax,2), %rax
        salq    $4, %rax
        leaq    48(%rdi,%rax), %rdx
        xorl    %eax, %eax
.L3:
        movq    %rax, (%rdi)
        movw    %ax, 8(%rdi)
        movl    %eax, 12(%rdi)
        movb    %al, 16(%rdi)
        movq    %rax, 24(%rdi)
        movb    %al, 32(%rdi)
        movl    %eax, 36(%rdi)
        movq    %rax, 40(%rdi)

        addq    $48, %rdi
        cmpq    %rdx, %rdi
        jne     .L3
.L1:
        ret

Here, the loop between .L3 and .L1 only takes 34 bytes, which is nearly half the original size.

Constant (for example, zero) initialization is a frequently used pattern to initialize structures, so the sequences like the above are quite wide spread. Converting cases like this to the use of registers could save some code size and reduce cache pressure.


---


### compiler : `gcc`
### title : `[ARM NEON] Missed optimization in lowering test operation`
### open_at : `2020-11-19T09:44:46Z`
### last_modified_date : `2021-05-05T15:46:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97903
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Hi,
For the following test-case:

#include <arm_neon.h>

uint8x8_t f1(int8x8_t a, int8x8_t b) {
  return (uint8x8_t) ((a & b) != 0);
}

uint8x8_t f2(int8x8_t a, int8x8_t b) {
  return vtst_s8 (a, b);
}

Code-gen:

f2:
        vtst.8  d0, d0, d1
        bx      lr


f1:
        vmov.i32        d16, #0  @ v8qi
        vand    d1, d0, d1
        vmov.i32        d17, #0xffffffff  @ v8qi
        vceq.i8 d1, d1, d16
        vbsl    d1, d16, d17
        vmov    d0, d1  @ v8qi
        bx      lr

The optimized dump for f1 shows:
  _1 = a_4(D) & b_5(D);
  _3 = .VCOND (_1, { 0, 0, 0, 0, 0, 0, 0, 0 }, { -1, -1, -1, -1, -1, -1, -1, -1 }, { 0, 0, 0, 0, 0, 0, 0, 0 }, 113);
  _6 = VIEW_CONVERT_EXPR<uint8x8_t>(_3);

I think we miss opportunity to combine AND followed by VCOND into a vector test instruction. Should we add a .VTEST internal function that expands to vtst ? Or alternatively, add a peephole pattern in backend ?

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `[ARM NEON] Missed optimization in lowering to vcage`
### open_at : `2020-11-19T11:32:57Z`
### last_modified_date : `2021-06-21T09:12:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97906
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Hi,
Similar to PR97872 and PR97903, for following test-case:

#include <arm_neon.h>

uint32x2_t f1(float32x2_t a, float32x2_t b)
{
  return vabs_f32 (a) >= vabs_f32 (b);
}

uint32x2_t f2(float32x2_t a, float32x2_t b)
{
  return (uint32x2_t) __builtin_neon_vcagev2sf (a, b);
}

Code-gen:

f2:
        vacge.f32  d0, d0, d1
        bx         lr

f1:
	vabs.f32	d0, d0
	vabs.f32	d1, d1
	sub	sp, sp, #8
	vmov.32	r3, d0[0]
	vmov	s13, r3
	vmov.32	r3, d1[0]
	vmov	s12, r3
	vmov.32	r3, d1[1]
	vcmpe.f32	s12, s13
	vmov	s14, r3
	vmov.32	r3, d0[1]
	vmrs	APSR_nzcv, FPSCR
	vmov	s15, r3
	ite	ls
	movls	r3, #-1
	movhi	r3, #0
	vcmpe.f32	s14, s15
	str	r3, [sp]
	vmrs	APSR_nzcv, FPSCR
	ite	ls
	movls	r3, #-1
	movhi	r3, #0
	str	r3, [sp, #4]
	vldr	d0, [sp]
	add	sp, sp, #8
	@ sp needed
	bx	lr

For f1, it is initially lowered to:

f1 (float32x2_t a, float32x2_t b)
{
  vector(2) <signed-boolean:32> _1;
  vector(2) int _2;
  uint32x2_t _6;
  __simd64_float32_t _7;
  __simd64_float32_t _8;

  <bb 2> [local count: 1073741824]:
  _8 = __builtin_neon_vabsv2sf (a_4(D));
  _7 = __builtin_neon_vabsv2sf (b_5(D));
  _1 = _7 <= _8;
  _2 = VEC_COND_EXPR <_1, { -1, -1 }, { 0, 0 }>;
  _6 = VIEW_CONVERT_EXPR<uint32x2_t>(_2);
  return _6;
}

and veclower seems to "scalarize" the cond_expr op:

f1 (float32x2_t a, float32x2_t b)
{
  vector(2) int _2;
  uint32x2_t _6;
  __simd64_float32_t _7;
  __simd64_float32_t _8;
  float _11;
  float _12;
  int _13;
  float _14;
  float _15;
  int _16;

  <bb 2> [local count: 1073741824]:
  _8 = __builtin_neon_vabsv2sf (a_4(D));
  _7 = __builtin_neon_vabsv2sf (b_5(D));
  _11 = BIT_FIELD_REF <_7, 32, 0>;
  _12 = BIT_FIELD_REF <_8, 32, 0>;
  _13 = _11 <= _12 ? -1 : 0;
  _14 = BIT_FIELD_REF <_7, 32, 32>;
  _15 = BIT_FIELD_REF <_8, 32, 32>;
  _16 = _14 <= _15 ? -1 : 0;
  _2 = {_13, _16};
  _6 = VIEW_CONVERT_EXPR<uint32x2_t>(_2);
  return _6;

}

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `Unoptimal code generation with __builtin_*_overflow{,_p} for short`
### open_at : `2020-11-23T13:49:19Z`
### last_modified_date : `2020-11-24T09:51:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97950
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
For the following code, the generation is unoptimal on x86-64.

For most of the functions with `short` a jump is generated.
For the functions with `__int128` all but `mul_overflow_{,un}signed___int128` seems to have extra `mov` produced.

The same problems apply to all the `__builtin_*_overflow_p`
-------------
#define TEST_OVERFLOW(type) \
bool mul_overflow_signed_##type(signed type a, signed type b, signed type c) {return __builtin_mul_overflow(a,b,&c);} \
bool add_overflow_signed_##type(signed type a, signed type b, signed type c) {return __builtin_add_overflow(a,b,&c);} \
bool sub_overflow_signed_##type(signed type a, signed type b, signed type c) {return __builtin_sub_overflow(a,b,&c);} \
bool mul_overflow_unsigned_##type(unsigned type a, unsigned type b, unsigned type c) {return __builtin_mul_overflow(a,b,&c);} \
bool add_overflow_unsigned_##type(unsigned type a, unsigned type b, unsigned type c) {return __builtin_add_overflow(a,b,&c);} \
bool sub_overflow_unsigned_##type(unsigned type a, unsigned type b, unsigned type c) {return __builtin_sub_overflow(a,b,&c);} \

TEST_OVERFLOW(short)
TEST_OVERFLOW(__int128)
-------------


---


### compiler : `gcc`
### title : `unnecessary moves with __builtin_{add,sub}_overflow_p and __int128`
### open_at : `2020-11-23T17:58:46Z`
### last_modified_date : `2023-01-19T23:23:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97961
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
In #97950 Jackub told me to open a new bug for that.

The snippet bellow has the following problems
- f1 and f2 generate 4 unnecessary moves
        mov     r9, rdi
        mov     r8, rsi
        mov     rsi, r9
        mov     rdi, r8

- f4 has "only" 2 unnecessary moves
        mov     r9, rdi
        mov     rdi, rsi

- f3 should be identical to f4 except for the flag checking.


------------
bool f1(unsigned __int128 a,unsigned __int128 b) {
    return __builtin_add_overflow_p(a, b, (unsigned __int128)0);
}

bool f2(__int128 a,__int128 b) {
    return __builtin_add_overflow_p(a, b, (__int128)0);
}


bool f3(unsigned __int128 a,unsigned __int128 b) {
    return __builtin_sub_overflow_p(a, b, (unsigned __int128)0);
}

bool f4(__int128 a,__int128 b) {
    return __builtin_sub_overflow_p(a, b, (__int128)0);
}
------------

asm generated
------------
f1(unsigned __int128, unsigned __int128):
        mov     r9, rdi
        mov     r8, rsi
        mov     rsi, r9
        mov     rdi, r8
        add     rsi, rdx
        adc     rdi, rcx
        setc    al
        ret
f2(__int128, __int128):
        mov     r9, rdi
        mov     r8, rsi
        mov     rsi, r9
        mov     rdi, r8
        add     rsi, rdx
        adc     rdi, rcx
        seto    al
        ret
f3(unsigned __int128, unsigned __int128):
        mov     r9, rdi
        mov     r8, rsi
        mov     rdi, r8
        mov     rax, r9
        mov     r8, rdx
        sub     rax, r8
        mov     rdx, rdi
        sbb     rdx, rcx
        cmp     r9, rax
        mov     rcx, rdi
        sbb     rcx, rdx
        setc    al
        ret
f4(__int128, __int128):
        mov     r9, rdi
        mov     rdi, rsi
        cmp     r9, rdx
        sbb     rdi, rcx
        seto    al
        ret
-------------------


---


### compiler : `gcc`
### title : `Missed optimization opportunity for VRP`
### open_at : `2020-11-24T07:47:48Z`
### last_modified_date : `2021-08-14T22:29:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97964
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
We compiled two programs (A1.c, A2.c) by gcc-10.2.0 with -O3 option.
The two programs are equivalent, but resulted in different assembly codes.
Although varialbe a is volatile, the value of variable t can be 
determined at compile time via VRP optimization.  

Note that gcc-7.5.0 or older versions compiled the both code into the
same minimum assebmly codes.  For more precise: 

  gcc-10.2.0  diff
  gcc-9.3.0   diff
  gcc-8.0.1   diff
  gcc-7.5.0   OK
  gcc-6.5.0   OK
  gcc-5.5.0   OK


+---------------------------------+---------------------------------+
|                A1.c             |              A2.c               |
+---------------------------------+---------------------------------+
|int main (void)                  |int main (void)                  |
|{                                |{                                |
|  volatile int a = -1;           |  volatile int a = 1;            |
|  long b = -2;                   |                                 |
|                                 |                                 |
|  int c = a>0;                   |                                 |
|  int d = b*c;                   |                                 |
|  int e = 1-d;                   |  a;                             |
|  int t = (-1/(int)e)==1;        |  int t = 0;                     |
|                                 |                                 |
|  if (t != 0) __builtin_abort(); |  if (t != 0) __builtin_abort(); |
|                                 |                                 |
|  return 0;                      |  return 0;                      |
|}                                |}                                |
+---------------------------------+---------------------------------+

gcc-10.2.0
+-------------------------------+------------------------------+
| A1.s (gcc-10.2.0 A1.c -O3 -S) | A2.s (gcc-10.2.0 A2.c -O3 -S)|
+-------------------------------+------------------------------+
|main:                          |main:                         |
|.LFB0:                         |.LFB0:                        |
|   .cfi_startproc              |   .cfi_startproc             |
|   subq    $24, %rsp           |   movl    $1, -4(%rsp)       |
|   .cfi_def_cfa_offset 32      |   movl    -4(%rsp), %eax     |
|   movl    $1, 12(%ecx)        |                              |
|   movl    $-1, 12(%rsp)       |                              |
|   movl    12(%rsp), %eax      |                              |
|   testl   %eax, %eax          |                              |
|   setle   %al                 |                              |
|   movzbl  %al, %eax           |                              |
|   leal    -2(%rax,%rax), %eax |                              |
|   subl    %eax, %ecx          |                              |
|   movl    $-1, %eax           |                              |
|   cltd                        |                              |
|   idiv    %ecx                |                              |
|   cmpl    $1, %eax            |                              |
|   je      .L3                 |                              |
|   xorl    %eax, %eax          |   xorl    %eax, %eax         |
|   addq    $24, %rsp           |                              |
|   .cfi_def_cfa_offset 8       |                              |
|   ret                         |   ret                        |
|   .cfi_endproc                |   .cfi_endproc               |
|   .section   .text.unlikely   |                              |
|   .cfi_startproc              |                              |
|   .type   main.cold, @function|                              |
|main.cold:                     |                              |
|.LFSB0:                        |                              |
|.L3:                           |                              |
|    .cfi_def_cfa_offset 32     |                              |
|    call    abort              |                              |
|    .cfi_endproc               |                              |
|.LFE0:                         |.LFE0:                        |
|    .section  text.startup     |                              |
|    .size   main, .-main       |   .size   main, .-main       |
|    .section  .text.unlikely   |                              |
|    .size   main.cold, .-mai...|                              |
|.LCOLDE0:                      |                              |
|    .section  .text.startup    |                              |
|.LHOTE0:                       |                              |
|    .ident  "GCC:(GNU) 10.2.0" |   .ident  "GCC:(GNU) 10.2.0" |
|    .section  .note.GNU-stac...|   .section  .note.GNU-stac...|
+--------------------------------------------------------------+

gcc-7.5.0
+-------------------------------+------------------------------+
| A1.s (gcc-7.5.0 A1.c -O3 -S)  | A2.s (gcc-7.5.0 A2.c -O3 -S) |
+-------------------------------+------------------------------+
|main:                          |main:                         |
|.LFB0:                         |.LFB0:                        |
|    .cfi_startproc             |   .cfi_startproc             |
|    movl    $1, -4(%rsp)       |   movl    $1, -4(%rsp)       |
|    movl    -4(%rsp), %eax     |   movl    -4(%rsp), %eax     |
|    xorl    %eax, %eax         |   xorl    %eax, %eax         |
|    ret                        |   ret                        |
|    .cfi_endproc               |   .cfi_endproc               |
|.LFE0:                         |.LFE0:                        |
|    .size   main, .-main       |   .size   main, .-main       |
|    .ident  "GCC:(Ubuntu 7.5...|   .ident  "GCC:(Ubuntu 7.5...|
|    .section  .note.GNU-stac...|   .section  .note.GNU-stac...|
+--------------------------------------------------------------+

$ gcc -v
using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/10.2.0/lto-wrapper
target: x86_64-pc-linux-gnu
configure woth: ../configure --enable-languages=c,c++ --prefix=/usr/local
--disable-bootstrap --disable-multilib
thred model: posix
Supported LTO compression algorithms: zlib
gcc version 10.2.0 (GCC) 

$ gcc-7 -v
Using built-in specs.
COLLECT_GCC=gcc-7
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 7.5.0-3ubuntu1~18.04'
--with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs
--enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr
--with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu-
--enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext
--enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap
--enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes
--with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify
--enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib
--with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror
--with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib
--with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver
--enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)


---


### compiler : `gcc`
### title : `Missed optimization opportunity for VRP`
### open_at : `2020-11-24T12:32:13Z`
### last_modified_date : `2021-07-28T20:54:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97967
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
We have another VRP related test case for GCC-10.2.0. 

We expect A1.c and A2.c should compile to the same codes, but they did
not.

+---------------------------------+---------------------------------+
|                A1.c             |              A2.c               |
+---------------------------------+---------------------------------+
|int main (void)                  |int main (void)                  |
|{                                |{                                |
|  volatile int a = 1;            |  volatile int a = 1;            |
|                                 |                                 |
|  int b = a%2;                   |  a;                             |
|  int t = 200<(short)(-50*b);    |  int t = 0;                     |
|                                 |                                 |
|  if (t != 0) __builtin_abort(); |  if (t != 0) __builtin_abort(); |
|                                 |                                 |
|  return 0;                      |  return 0;                      |
|}                                |}                                |
+---------------------------------+---------------------------------+

+-------------------------------+------------------------------+
| A1.s (gcc-10.2.0 A1.c -O3 -S) | A2.s (gcc-10.2.0 A2.c -O3 -S)|
+-------------------------------+------------------------------+
|main:                          |main:                         |
|.LFB0:                         |.LFB0:                        |
|   .cfi_startproc              |   .cfi_startproc             |
|   subq    $24, %rsp           |   movl    $1, -4(%rsp)       |
|   .cfi_def_cfa_offset 32      |   movl    -4(%rsp), %eax     |
|   movl    $1, 12(%rsp)        |                              |
|   movl    12(%rsp), %eax      |                              |
|   movl    %eax, %edx          |                              |
|   shrl    $31, %edx           |                              |
|   addl    %edx, %eax          |                              |
|   andl    $1, %eax            |                              |
|   subl    %edx, %eax          |                              |
|   imull   $-50, %eax, %eax    |                              |
|   cmpw    $200, %ax           |                              |
|   jg      .L3                 |                              |
|   xorl    %eax, %eax          |   xorl    %eax, %eax         |
|   addq    $24, %rsp           |                              |
|   .cfi_def_cfa_offset 8       |                              |
|   ret                         |   ret                        |
|   .cfi_endproc                |   .cfi_endproc               |
|   .section   .text.unlikely   |                              |
|   .cfi_startproc              |                              |
|   .type   main.cold, @function|                              |
|main.cold:                     |                              |
|.LFSB0:                        |                              |
|.L3:                           |                              |
|    .cfi_def_cfa_offset 32     |                              |
|    call    abort              |                              |
|    .cfi_endproc               |                              |
|.LFE0:                         |.LFE0:                        |
|    .section  text.startup     |                              |
|    .size   main, .-main       |   .size   main, .-main       |
|    .section  .text.unlikely   |                              |
|    .size   main.cold, .-mai...|                              |
|.LCOLDE0:                      |                              |
|    .section  .text.startup    |                              |
|.LHOTE0:                       |                              |
|    .ident  "GCC: (GNU) 10.2.0"|   .ident  "GCC: (GNU) 10.2.0"|
|    .section  .note.GNU-stac...|   .section  .note.GNU-stac...|
+--------------------------------------------------------------+

$ gcc -v
using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/10.2.0/lto-wrapper
target: x86_64-pc-linux-gnu
configure woth: ../configure --enable-languages=c,c++ --prefix=/usr/local
--disable-bootstrap --disable-multilib
thred model: posix
Supported LTO compression algorithms: zlib
gcc version 10.2.0 (GCC)


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Unnecessary mov instruction with comparison and cmov`
### open_at : `2020-11-24T15:19:09Z`
### last_modified_date : `2023-08-08T01:26:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97968
### status : `NEW`
### tags : `missed-optimization, needs-bisection, ra`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
The same problem applies with all comparison operators but '==' for 'int' and 'long'  on x86-64.

(Returning a negative value instead of 0 makes the compiler generate a jump instead of a cmov. I don't know if it's worth a bug)

-------------------
int f(int n, int j)
{
    return n > j ? n : 0;
}
-------------------


with O2 produces


-------------------
f(int, int):
        mov     eax, edi
        cmp     edi, esi
        mov     edx, 0
        cmovle  eax, edx
        ret
-------------------

Ideally it should produce something like that. (the first mov can be deleted with some little changes later)

-----------------------
f(int, int):
        mov     eax, 0
        cmp     edi, esi
        cmovg   eax, edi
        ret
-----------------------


---


### compiler : `gcc`
### title : `[11 Regression] Worse code for -O3 than -O2 on aarch64 vector multiply-add`
### open_at : `2020-11-25T11:49:05Z`
### last_modified_date : `2023-07-07T10:38:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97984
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
The code:
void x (long * __restrict a, long * __restrict b)
{
  a[0] *= b[0];
  a[1] *= b[1];
  a[0] += b[0];
  a[1] += b[1];
}

at -O2 generates:
x:
        ldp     x4, x3, [x0]
        ldp     x2, x1, [x1]
        madd    x2, x2, x4, x2
        madd    x1, x1, x3, x1
        stp     x2, x1, [x0]
        ret

whereas at -O3 it does:
x:
        ldp     x2, x3, [x0]
        ldr     x4, [x1]
        ldr     q1, [x1]
        mul     x2, x2, x4
        ldr     x4, [x1, 8]
        fmov    d0, x2
        ins     v0.d[1], x3
        mul     x1, x3, x4
        ins     v0.d[1], x1
        add     v0.2d, v0.2d, v1.2d
        str     q0, [x0]
        ret

which is clearly inferior.
GCC 9 used to generate the good code for both -O2 and -O3


---


### compiler : `gcc`
### title : `Missed optimization: Multiply of extended integer cannot overflow`
### open_at : `2020-11-25T19:41:52Z`
### last_modified_date : `2021-12-15T23:56:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97997
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
When an integer is extended and then multiplied by another integer of the original size, the resulting multiplication can never overflow. However, gcc does not seem to realize this. Consider:

uint16_t calc_u(uint16_t x ) {
        return (uint32_t)x * 10 / 10;
}

If gcc would realize that x * 10 cannot overflow, it can optimize away the * 10 / 10. However, it does not:

$ gcc-10 -Os -Wall -Wextra -pedantic foo.c && objdump -S --disassemble=calc_u a.out
00000000000011a0 <calc_u>:
    11a0:       f3 0f 1e fa             endbr64 
    11a4:       0f b7 c7                movzwl %di,%eax
    11a7:       b9 0a 00 00 00          mov    $0xa,%ecx
    11ac:       31 d2                   xor    %edx,%edx
    11ae:       6b c0 0a                imul   $0xa,%eax,%eax
    11b1:       f7 f1                   div    %ecx
    11b3:       c3                      retq   

When doing the multiplication signed, this optimization *does* happen:

uint16_t calc_s(uint16_t x ) {
        return (int32_t)x * 10 / 10;
}

$ gcc-10 -Os -Wall -Wextra -pedantic foo.c  && objdump -S --disassemble=calc_s a.out

0000000000001199 <calc_s>:
    1199:       f3 0f 1e fa             endbr64 
    119d:       89 f8                   mov    %edi,%eax
    119f:       c3                      retq   

Since signed overflow is undefined, gcc presumably assumes that the multiplication does not overflow and optimizes this. This shows that the machinery for this optimization exists and works and suggests that the only thing missing in the unsigned case is realizing that the overflow cannot happen.

The above uses 16/32bit numbers, but the same happens on 32/64bit (just not on 8/16 bit, because then things are integer-promoted and multiplication is always signed). When using -O2 or -O3, the code generated for unsigned is different, but still not fully optimized.

Maybe I'm missing some corner case of the C language that would make this optimization incorrect, but I think it should be allowed.

The original code that triggered this report is:

#define ticks2us(t)   (uint32_t)((uint64_t)(t)*1000000 / TICKS_PER_SEC)

Which could be optimized to a single multiply or even bitshift rather than a multiply and division for particular values of TICKS_PER_SEC, while staying generally applicable (but slower) for other values.

I took a guess at the component, please correct that if needed.

$ gcc-10 --version
gcc-10 (Ubuntu 10.2.0-5ubuntu1~20.04) 10.2.0
Copyright (C) 2020 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


---


### compiler : `gcc`
### title : `PPC: mfvsrwz+extsw not merged to mtvsrwa`
### open_at : `2020-11-26T19:12:52Z`
### last_modified_date : `2020-12-08T23:22:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98020
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
int extract(vector signed int v)
{
   return v[2];
}

Command line:
gcc -mcpu=power8 -maltivec -m64 -O3 -save-temps extract.C

Output:
_Z7extractDv4_i:
.LFB0:
        .cfi_startproc
        mfvsrwz 3,34
        extsw 3,3
        blr


---


### compiler : `gcc`
### title : `optimization dependant on condition order`
### open_at : `2020-11-27T10:04:35Z`
### last_modified_date : `2022-01-28T23:18:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98026
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
In all the following functions gcc should recognize that j can't be greater than 100, and link_error should not appear in assembly. Currently only f3 is optimized.


-------------------
void link_error();

void f1(int i, int j) {
  if (j > i || i > 100) return;

  if (j > 100) link_error();
}

void f2(int i, int j) {
  if (i > 100 || j > i) return;

  if (j > 100) link_error();
}

void f3(int i, int j) {
  if (i > 100) return;
  if (j > i) return;

  if (j > 100) link_error();
}

void f4(signed int i,unsigned int j) {
  if (i > 100) return;
  if (j > i) return;

  if (j > 100) link_error();
}
------------------


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] __builtin_sub_overflow_p not folded to const when some constraints are known`
### open_at : `2020-11-27T10:50:50Z`
### last_modified_date : `2023-07-13T21:14:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98028
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
According to godbolt, f1 used to be optimised with gcc 7.

The same problem can be seen with signed types (and maybe more conditions?).
All the following functions should be only one instruction plus ret with O2

-----------------
unsigned f1(unsigned i, unsigned j) {
  if (j != i) __builtin_unreachable();
  return __builtin_sub_overflow_p(i, j, (unsigned)0);
}

unsigned f2(unsigned i, unsigned j) {
  if (j > i) __builtin_unreachable();
  return __builtin_sub_overflow_p(i, j, (unsigned)0);
}

unsigned f3(unsigned i, unsigned j) {
  if (j >= i) __builtin_unreachable();
  return __builtin_sub_overflow_p(i, j, (unsigned)0);
}

unsigned f4(unsigned i, unsigned j) {
  if (j < i) __builtin_unreachable();
  return __builtin_sub_overflow_p(i, j, (unsigned)0);
}
-----------------


---


### compiler : `gcc`
### title : `ABA problem in atomic wait`
### open_at : `2020-11-27T13:46:16Z`
### last_modified_date : `2023-05-28T21:53:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98033
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `11.0`
### severity : `enhancement`
### contents :
In __atomic_wait in include/bits/atomic_wait.h we have:

      while (!__pred())
	{
	  if constexpr (__platform_wait_uses_type<_Tp>)
	    {
	      __platform_wait(__addr, __old);
	    }

If the predicate is initially false (i.e. *addr == old) we enter the loop. If the value changes before calling __platform_wait then that will return immediately. If it then changes back to the old value the predicate will be false again and we will re-enter the loop. This time __platform_wait will block waiting for the value to change.

The standard says we're allowed to miss such transient values, and so the code above is conforming. But I think we can make it work on linux for the std::atomic_signed_lock_free type at least (which isn't defined, but should be atomic<int>).

If __platform_wait returned a boolean indicating whether the not-equal condition was met (either because it was already true, the EAGAIN case, or because the futex wait got woken up) then the caller could check that and not re-check the predicate.


---


### compiler : `gcc`
### title : `Failure to optimize cmp+setnb+add to cmp+sbb`
### open_at : `2020-11-29T23:07:30Z`
### last_modified_date : `2023-09-21T10:45:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98060
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
int r(unsigned v0, unsigned v1, int v2)
{
    return (v0 >= v1) + v2;
}

This code, on x86, can be implemented with `cmp` followed by `sbb`. This optimization is done by LLVM, but not by GCC.

-O3 x86 output on LLVM :

r:
  mov eax, edx
  cmp edi, esi
  sbb eax, -1
  ret

On GCC :

r:
  xor eax, eax
  cmp edi, esi
  setnb al
  add eax, edx
  ret


---


### compiler : `gcc`
### title : `Optimize __builtin_unordered (...) || __builtin_is{less,greater}{,equal}`
### open_at : `2020-12-02T11:45:32Z`
### last_modified_date : `2021-01-18T10:17:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98095
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
These two functions are equivalent:

int t3 (float a, float b)
{
  return  __builtin_isless (a, b) || __builtin_isunordered (a, b);
}

int t4 (float a, float b)
{
  return  !__builtin_isgreaterequal (a, b);
}

gcc -O2:

t3:
        ucomiss %xmm0, %xmm1
        seta    %al
        ucomiss %xmm1, %xmm0
        setp    %dl
        orl     %edx, %eax
        movzbl  %al, %eax
        ret

t4:
        xorl    %eax, %eax
        ucomiss %xmm1, %xmm0
        setb    %al
        ret

Proof:

for ordered operands:
the result of t3 is (a < b); the result of t4 is !(a >= b) = (a < b)

for unordered operands:
the result of the t3 is true; the result of t4 is !false = true.

q.e.d.

The above equivalence is true for all relations. I didn't check __builtin_islessgreater; although gcc provides UNEQ, it doesn't provide __builtin_isequal builtin function.


---


### compiler : `gcc`
### title : `[11 Regression] popcnt is not vectorized on s390 since f5e18dd9c7da`
### open_at : `2020-12-03T01:28:53Z`
### last_modified_date : `2020-12-07T11:54:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98113
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
s390's vxe/popcount-1.c began to fail after PR96789 fix.

The reason is that for the following source code

uv4si __attribute__((noinline))
vpopctf (uv4si a)
{
  uv4si r;
  int i;

  for (i = 0; i < 4; i++)
    r[i] = __builtin_popcount (a[i]);

  return r;
}

FRE turned

  _4 = BIT_FIELD_REF <aD.2283, 32, 0>;
  _11 = __builtin_popcountD.1211 (_4);
  _18 = (unsigned intD.9) _11;
  BIT_FIELD_REF <rD.2286, 32, 0> = _18;
  i_20 = 1;
  ivtmp_21 = 3;
  _25 = VIEW_CONVERT_EXPR<unsigned intD.9[4]>(aD.2283)[i_20];
  _26 = __builtin_popcountD.1211 (_25);
  _27 = (unsigned intD.9) _26;
  VIEW_CONVERT_EXPR<unsigned intD.9[4]>(rD.2286)[i_20] = _27;
  i_29 = i_20 + 1;
  ivtmp_30 = ivtmp_21 + 4294967295;
  _34 = VIEW_CONVERT_EXPR<unsigned intD.9[4]>(aD.2283)[i_29];
  _35 = __builtin_popcountD.1211 (_34);
  _36 = (unsigned intD.9) _35;
  VIEW_CONVERT_EXPR<unsigned intD.9[4]>(rD.2286)[i_29] = _36;
  i_38 = i_29 + 1;
  ivtmp_39 = ivtmp_30 + 4294967295;
  _1 = VIEW_CONVERT_EXPR<unsigned intD.9[4]>(aD.2283)[i_38];
  _2 = __builtin_popcountD.1211 (_1);
  _3 = (unsigned intD.9) _2;
  VIEW_CONVERT_EXPR<unsigned intD.9[4]>(rD.2286)[i_38] = _3;
  i_10 = i_38 + 1;
  ivtmp_16 = ivtmp_39 + 4294967295;
  _7 = rD.2286;
  rD.2286 ={v} {CLOBBER};
  return _7;

into

  _4 = BIT_FIELD_REF <a_17(D), 32, 0>;
  _11 = __builtin_popcountD.1211 (_4);
  _18 = (unsigned intD.9) _11;
  r_14 = BIT_INSERT_EXPR <r_15(D), _18, 0 (32 bits)>;
  _25 = BIT_FIELD_REF <a_17(D), 32, 32>;
  _26 = __builtin_popcountD.1211 (_25);
  _27 = (unsigned intD.9) _26;
  r_33 = BIT_INSERT_EXPR <r_14, _27, 32 (32 bits)>;
  _34 = BIT_FIELD_REF <a_17(D), 32, 64>;
  _35 = __builtin_popcountD.1211 (_34);
  _36 = (unsigned intD.9) _35;
  r_32 = BIT_INSERT_EXPR <r_33, _36, 64 (32 bits)>;
  _1 = BIT_FIELD_REF <a_17(D), 32, 96>;
  _2 = __builtin_popcountD.1211 (_1);
  _3 = (unsigned intD.9) _2;
  r_31 = BIT_INSERT_EXPR <r_32, _3, 96 (32 bits)>;
  _7 = r_31;
  return _7;

that is, replaced a sequence of stores with a sequence of
BIT_INSERT_EXPRs.

slp1 now says: "missed:  not vectorized: no grouped stores in basic
block", presumably because it doesn't understand BIT_INSERT_EXPRs.


---


### compiler : `gcc`
### title : `Z: Load and test LTDBR instruction gets not used for comparison against 0.0`
### open_at : `2020-12-03T15:47:05Z`
### last_modified_date : `2020-12-04T07:28:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98124
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.3.0`
### severity : `normal`
### contents :
#include <math.h>
double sign(double in)
{
   return in == 0.0 ? 0.0 : copysign(1.0, in);
}

Command line:
gcc m64 -O2 -save-temps copysign.C

Output:
_Z4signd:
.LFB234:
        .cfi_startproc
        larl    %r5,.L8
        lzdr    %f2
        cdbr    %f0,%f2
        je      .L6
        ld      %f2,.L9-.L8(%r5)
        cpsdr   %f0,%f0,%f2
        br      %r14

Use of LTDBR expected instead of  lzdr    %f2 + cdbr    %f0,%f2


---


### compiler : `gcc`
### title : `BB vect fail to SLP one case`
### open_at : `2020-12-04T10:46:45Z`
### last_modified_date : `2023-10-09T07:38:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98138
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Test case:

  extern void test(unsigned int t[4][4]);

  void foo(unsigned char *p1, int i1, unsigned char *p2, int i2)
  {
    unsigned int tmp[4][4];
    unsigned int a0, a1, a2, a3;

    for (int i = 0; i < 4; i++, p1 += i1, p2 += i2) {
      a0 = (p1[0] - p2[0]) + ((p1[4] - p2[4]) << 16);
      a1 = (p1[1] - p2[1]) + ((p1[5] - p2[5]) << 16);
      a2 = (p1[2] - p2[2]) + ((p1[6] - p2[6]) << 16);
      a3 = (p1[3] - p2[3]) + ((p1[7] - p2[7]) << 16);

      int t0 = a0 + a1;
      int t1 = a0 - a1;
      int t2 = a2 + a3;
      int t3 = a2 - a3;

      tmp[i][0] = t0 + t2;
      tmp[i][2] = t0 - t2;
      tmp[i][1] = t1 + t3;
      tmp[i][3] = t1 - t3;
    }
    test(tmp);
  }

The expected code on ppc64le can look like:

  // p1 byte 0 to byte 7 
  d1_0_7 = load_dword(p1)
  // p1+i1 b0 to b7, rename it as 8 to 15       
  d1_8_15 = load_dword(p1 + i1)
  d1_16_23 = load_dword(p1 + 2*i1) 
  d1_24_31 = load_dword(p1 + 3*i1)

  V_d1_0_15 = construct_vec(d1_0_7,d1_8_15) // vector char
  V_d1_16_31 = construct_vec(d1_16_23,d1_24_31)
  V_d1_0_3_all = vperm(V_d1_0_15, V_d1_0_15, 
                      {0 8 16 24 1 9 17 25 2 10 18 26 3 11 19 27})
  V_d1_4_7_all = vperm(V_d1_0_15, V_d1_0_15, 
                      {4 12 20 28 5 13 21 29 6 14 22 30 7 15 23 31})

  // Do the similar for p2 with i2, get V_d2_0_3_all, V_d2_4_7_all

  // Do the subtraction together (all 4x4 bytes)
  V_sub1 = V_d1_0_3_all - V_d2_0_3_all
  V_sub2 = V_d1_4_7_all - V_d2_4_7_all
  
  // Do some unpack and get the promoted vector int
  V_a0_tmp = vec_promote(V_sub2, {0 1 2 3}) // vector int {b4 b12 b20 b28}
  V_a0_1 = V_a0_tmp << 16
  V_a0_0 = vec_promote(V_sub1, {0 1 2 3}).  // vector int {b0 b8 b16 b24}
  // vector int {a0_iter0, a0_iter1, a0_iter2, a0_iter3}
  V_a0 = V_a0_0 + V_a0_1
  
  // Get the similar for V_a1, V_a2, V_a3

  // Compute t0/t1/t2/t3
  // vector int {t0_iter0, t0_iter1, t0_iter2, t0_iter3}
  V_t0 = V_a0 + V_a1  
  V_t1 = V_a0 - V_a1
  V_t2 = V_a2 + V_a3
  V_t3 = V_a2 - V_a3

  // Compute tmps
  // vector int {tmp[0][0], tmp[1][0], tmp[2][0], tmp[3][0]}
  V_tmp0 = V_t0 + V_t2
  V_tmp2 = V_t0 - V_t2
  V_tmp1 = V_t1 + V_t3
  V_tmp3 = V_t1 - V_t3
    
  // Final construct the {tmp[0][0], tmp[0][1], tmp[0][2], tmp[0][3]} ...
  // with six further permutation on V_tmp0/V_tmp1/V_tmp2/V_tmp3


---


### compiler : `gcc`
### title : `arm: missed vectorization with MVE compared to Neon`
### open_at : `2020-12-04T14:22:40Z`
### last_modified_date : `2021-04-21T15:31:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98143
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
While working on enabling auto-vectorization for MVE, I noticed a missed optimization compared to Neon:

#include <stdint.h>
uint16_t *dest;
void func()
{
  int i;
  for (i=0;i<16;i++)
    dest[i]=3;
}

Compiled with -O3 -S -dp -mfloat-abi=hard -mfpu=auto -mcpu=cortex-a9 -mthumb:
func:
        movw    r3, #:lower16:.LANCHOR0 @ 15    [c=4 l=4]  *thumb2_movsi_vfp/4
        vmov.i16        q8, #3  @ v8hi  @ 7     [c=4 l=4]  *neon_movv8hi/2
        movt    r3, #:upper16:.LANCHOR0 @ 16    [c=4 l=4]  *arm_movt/0
        ldr     r3, [r3]        @ 14    [c=12 l=4]  *thumb2_movsi_vfp/5
        vst1.16 {q8}, [r3]!     @ 8     [c=8 l=4]  *movmisalignv8hi_neon_store
        vst1.16 {q8}, [r3]      @ 11    [c=8 l=4]  *movmisalignv8hi_neon_store
        bx      lr      @ 44    [c=8 l=4]  *thumb2_return

Compiled with -O3 -S -dp -mfloat-abi=hard -mfpu=auto -march=armv8.1-m.main+mve -mthumb:
func:
        movs    r2, #3  @ 7     [c=4 l=2]  *thumb2_movsi_shortim
        ldr     r3, .L3 @ 5     [c=12 l=4]  *thumb2_movsi_vfp/5
        ldr     r3, [r3]        @ 6     [c=12 l=4]  *thumb2_movsi_vfp/5
        strh    r2, [r3]        @ movhi @ 9     [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #2]    @ movhi @ 12    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #4]    @ movhi @ 15    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #6]    @ movhi @ 18    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #8]    @ movhi @ 21    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #10]   @ movhi @ 24    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #12]   @ movhi @ 27    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #14]   @ movhi @ 30    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #16]   @ movhi @ 33    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #18]   @ movhi @ 36    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #20]   @ movhi @ 39    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #22]   @ movhi @ 42    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #24]   @ movhi @ 45    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #26]   @ movhi @ 48    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #28]   @ movhi @ 51    [c=4 l=4]  *thumb2_movhi_vfp/4
        strh    r2, [r3, #30]   @ movhi @ 54    [c=4 l=4]  *thumb2_movhi_vfp/4
        bx      lr      @ 84    [c=8 l=4]  *thumb2_return



This PR is about building the const, as the problems with stores are probably part of PR97875.

In summry, with Neon we build the constant vector with:
        vmov.i16        q8, #3  @ v8hi  @ 7     [c=4 l=4]  *neon_movv8hi/2
but with MVE:
        movs    r2, #3  @ 7     [c=4 l=2]  *thumb2_movsi_shortim
and then store it as 16-bits value as many times as needed.

I haven't managed to understand why we can't make use of mve.md's mve_mov<mode> where there is an alternative with "Dm", which should work?


---


### compiler : `gcc`
### title : `[x86] Failure to optimize operation on indentically shuffled operands into a shuffle of the result of the operation`
### open_at : `2020-12-07T02:09:57Z`
### last_modified_date : `2023-09-21T10:43:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98167
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
__m128 f(__m128 a, __m128 b) {
    return _mm_mul_ps(_mm_shuffle_ps(a, a, 0), _mm_shuffle_ps(b, b, 0));
}

This can be optimized to:

__m128 f(__m128 a, __m128 b) {
    __m128 tmp = _mm_mul_ss(a, b);
    return _mm_shuffle_ps(tmp, tmp, 0);
}

This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `isnan pattern not folded`
### open_at : `2020-12-07T08:19:13Z`
### last_modified_date : `2020-12-10T18:29:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98169
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
The two following functions should produce the same assembly, at least on x86, but for f1 gcc does not recognize the pattern for f1 and produce not optimal code.

The problem is the same for all floating types.

bool f1(float a) {
    return a == a;
}

bool f2(float a) {
    return !__builtin_isnan(a);
}


Furthermore, for __float128, instead of calling __unordtf2 the following code call __netf2

bool f3(__float128 a) {
    return a != a;
}


---


### compiler : `gcc`
### title : `-fno-tree-pta improves tfft2 benchmark by 50% on zen and -march=natie.`
### open_at : `2020-12-07T13:14:41Z`
### last_modified_date : `2021-09-18T22:18:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98173
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
I got curious about overall effect of PTA.
This compares -fno-tree-pta to -ftree-pta.  
There is significant regression on tfft2 (and some with tramp3d) with -march=native on zen.

https://lnt.opensuse.org/db_default/v4/CPP/latest_runs_report?younger_in_days=14&older_in_days=0&min_percentage_change=0.02&revisions=03f48fb0d37c769661d30aa0f08d91bb7174cd98%2C68ee6d12fe9882223f47f81f93616577ab4e36da&include_user_branches=on

https://lnt.opensuse.org/db_default/v4/SPEC/latest_runs_report?younger_in_days=14&older_in_days=0&min_percentage_change=0.02&revisions=03f48fb0d37c769661d30aa0f08d91bb7174cd98%2C68ee6d12fe9882223f47f81f93616577ab4e36da&include_user_branches=on


---


### compiler : `gcc`
### title : `Loop invariant memory could not be hoisted when nonpure_call in loop body`
### open_at : `2020-12-07T13:57:35Z`
### last_modified_date : `2023-02-21T21:46:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98176
### status : `NEW`
### tags : `missed-optimization, openacc`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For testcase

#include <cmath>

void foo(float *x, float tx, float *ret, int n)
{

#pragma omp simd
    for (int i = 0; i < n; i++)
    {
        float s,c;

        sincosf(x[i] * tx, &s, &c);

        *ret += s * c;   
    }
}

It could not be vectorized with -Ofast -fopenmp-simd -std=c++11

https://gcc.godbolt.org/z/ba77az

By manually hoist it could be vectorized with simd clone

void foo(float *x, float tx, float *ret, int n)
{
    float tmp = 0.0f;

#pragma omp simd
    for (int i = 0; i < n; i++)
    {
        float s,c;

        sincosf(x[i] * tx, &s, &c);

        tmp += s*c;      
    }
    *ret += tmp;
}

https://gcc.godbolt.org/z/bea17x

Is it possible for lim to perform store motion on case like this?


---


### compiler : `gcc`
### title : `[10 Regression] X86 unoptimal code for float equallity comparison followed by jump`
### open_at : `2020-12-09T10:07:55Z`
### last_modified_date : `2023-07-07T09:19:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98212
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For f1 code an unnecessary `comiss` instruction is used, the parity flag is still set after the `jp` instruction.
For f2, I'm not sure if it's the optimal way to do that.

The same problems appear for `float`, `double` and `long double`

------
void f();

void f1(float a, float b) {
    if (a != b)
        f();
}

void f2(float a, float b) {
    if (a == b)
        f();
}

------
f1(float, float):
        ucomiss xmm0, xmm1
        jp      .L4
        comiss  xmm0, xmm1
        jne     .L4
        ret
.L4:
        jmp     f()

f2(float, float):
        ucomiss xmm0, xmm1
        jnp     .L11
.L7:
        ret
.L11:
        jne     .L7
        jmp     f()
------


---


### compiler : `gcc`
### title : `[TARGET_MMX_WITH_SSE] Implement 64bit vector compares (AVX512 masked compares missing)`
### open_at : `2020-12-10T01:56:09Z`
### last_modified_date : `2023-05-08T12:21:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98218
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Refer to https://godbolt.org/z/sYE88f

cat test.c

typedef char v8qi __attribute__ ((vector_size(8)));
v8qi f1(v8qi a, v8qi b) {
  return a == b;
}

gcc -O2 -msse4.1 -S

f1(char __vector(8), char __vector(8)):
        pextrb  edx, xmm0, 0
        pextrb  eax, xmm1, 0
        pextrb  ecx, xmm0, 1
        cmp     dl, al
        pextrb  eax, xmm1, 1
        pextrb  esi, xmm0, 2
        setne   dl
        pextrb  edi, xmm0, 3
        pextrb  r8d, xmm0, 4
        sub     edx, 1
        cmp     cl, al
        pextrb  eax, xmm1, 2
        setne   cl
        pextrb  r9d, xmm0, 5
        movzx   edx, dl
        sub     ecx, 1
        cmp     sil, al
        pextrb  eax, xmm1, 3
        setne   sil
        pextrb  r10d, xmm0, 6
        pextrb  r11d, xmm0, 7
        movzx   ecx, cl
        sub     esi, 1
        cmp     dil, al
        pextrb  eax, xmm1, 4
        setne   dil
        movzx   esi, sil
        sub     edi, 1
        cmp     r8b, al
        pextrb  eax, xmm1, 5
        setne   r8b
        movzx   edi, dil
        sub     r8d, 1
        cmp     r9b, al
        pextrb  eax, xmm1, 6
        setne   r9b
        movzx   r8d, r8b
        sub     r9d, 1
        cmp     r10b, al
        pextrb  eax, xmm1, 7
        setne   r10b
        movzx   r9d, r9b
        sub     r10d, 1
        cmp     r11b, al
        setne   al
        movzx   r10d, r10b
        sub     eax, 1
        movzx   eax, al
        sal     rax, 8
        or      rax, r10
        sal     rax, 8
        or      rax, r9
        sal     rax, 8
        or      rax, r8
        sal     rax, 8
        or      rax, rdi
        sal     rax, 8
        or      rax, rsi
        sal     rax, 8
        or      rax, rcx
        sal     rax, 8
        or      rax, rdx
        movq    xmm0, rax
        ret

It should be better with

f1(char __vector(8), char __vector(8)):                           # @f1(char __vector(8), char __vector(8))
        pcmpeqb xmm0, xmm1
        ret


---


### compiler : `gcc`
### title : `x plus/minus y cmp 0 produces unoptimal code`
### open_at : `2020-12-11T10:18:53Z`
### last_modified_date : `2023-06-13T05:56:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98236
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
Created attachment 49733
input file

Compiling test.c on x86 with -O2 leads to unoptimal code generation.

f1 to f4 could be optimized to `add + set(g|ge|l|le)`
f5 to f8 could be optimized to `sud|cmp + set(g|ge|l|le)`


For f2, f4, f6 and f8 no pattern is recognized.


For f1, f3, f5 and f7, the optimizers are not aware that, at least on x86, `add` and `sub` can set flags.

This can also be seen with the following function. gcc will produce `cmp + sub` although only `sub` could be used

-----------
void foo();

int bar(int x, int y) {
    if (x - y)
        foo();
    return x - y;
}
-----------
produces
-----------
bar(int, int):
        push    rbp
        mov     ebp, esi
        push    rbx
        mov     ebx, edi
        sub     rsp, 8
        cmp     edi, esi
        je      .L2
        call    foo()
.L2:
        mov     eax, ebx
        add     rsp, 8
        sub     eax, ebp
        pop     rbx
        pop     rbp
        ret
-----------

expected

-----------
bar(int, int):
        push    rbx
        mov     ebx, edi
        sub     ebx, esi
        je      .L2
        call    foo()
.L2:
        mov     eax, ebx
        pop     rbx
        ret
-----------


---


### compiler : `gcc`
### title : `Failure to optimize simple pattern for __builtin_convertvector`
### open_at : `2020-12-12T17:42:41Z`
### last_modified_date : `2023-09-21T10:41:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98254
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef int32_t __attribute__((vector_size(16))) v4i32;
typedef int16_t __attribute__((vector_size(8))) v4i16;

v4i32 f(short *a)
{
    return (v4i32){a[0], a[1], a[2], a[3]};
}

This can be optimized to `return __builtin_convertvector(*(v4i16 *)a, v4i32);` (or at least, something very close to that, if aliasing is to be taken into account). LLVM does this transformation, but GCC does not.


---


### compiler : `gcc`
### title : `switch optimisation and -fstrict-enums`
### open_at : `2020-12-14T14:48:46Z`
### last_modified_date : `2020-12-14T16:36:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98278
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 49761
input file

The attached file, compiled with -O2 -fstrict-enums, only for `foobar` all branches are transformed to a jump table. 
If I did not made any mistake all the function should be able to be transformed should produce the same assembly.

Changing the definition of the enum to
enum e { A = 1, B, C, D}; will produce worse code. Only one jumtable is used/created.
enum e { A = 2, B = 4, C = 6, D = 8 }; (or any other sequence) no jump table is created.


---
