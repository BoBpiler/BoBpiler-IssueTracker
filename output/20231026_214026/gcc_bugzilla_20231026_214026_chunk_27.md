### Total Bugs Detected: 4649
### Current Chunk: 27 of 30
### Bugs in this Chunk: 160 (From bug 4161 to 4320)
---


### compiler : `gcc`
### title : `We do not simplify (a - (N*M)) / N + M -> a / N`
### open_at : `2023-02-10T22:26:54Z`
### last_modified_date : `2023-09-04T03:26:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108757
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
The Eigen project code has a missed optimization that basically simplifies down to the following test case:

linux$ cat bug.c 
#define N 32
#define M 2

unsigned long int
foo (unsigned long int a)
{
  return (a - (N*M)) / N + M;
}

linux$ gcc -O2 -S bug.c 

linux$ cat bug.s 
foo:
	addi 3,3,-64
	srdi 3,3,5
	addi 3,3,2
	blr

We should be able to simplify this down to just 'a / N', which for power-of-2 N, results in just the srdi...although, I don't think N is required to be a power-of-2 to fold this down to just 'a / N'.  Can we also simplify this for non-constant N & M?  Maybe under -fast-math or something similar?


---


### compiler : `gcc`
### title : `unaligned byteswapped 16bit load is just bad`
### open_at : `2023-02-12T22:08:36Z`
### last_modified_date : `2023-02-12T22:08:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108766
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Take:
```
short f(unsigned char *a)
{
  return a[0] << 8 | a[1];
}
```
`-O2 -march=rv64iadc_zba_zbb_zbc_zbs_zicsr` produces:
```
        lbu     a5,1(a0)
        lbu     a4,0(a0)
        slli    a0,a5,8
        or      a0,a0,a4
        slliw   a5,a0,8
        srli    a0,a0,8
        or      a0,a0,a5
        sext.h  a0,a0
        ret
```

That is just horrible.
It should just be:
```
        lbu     a5,1(a0)
        lbu     a4,0(a0)
        slli    a0,a4,8
        or      a0,a0,a5
        sext.h  a0,a0
        ret
```

It is trying to do an unaligned short load and then a byteswap.


---


### compiler : `gcc`
### title : `Missing optimization with direct register access instead of structure mapping`
### open_at : `2023-02-13T19:54:15Z`
### last_modified_date : `2023-02-16T06:18:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108778
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.2.1`
### severity : `normal`
### contents :
The following example uses two functional identical ways to save the SREG of a AVR ÂµC, in this case an avr128da32. 

To access the SREG two different ways are used: one directly via the SREG macro and one via a structure mapping (the Cpu_t structure is actually not included in the avr headers, 
so a definition of the Cpu_t structure is given here).

If the SREG macro way is used the optimization regarding the variable g is missed (see assembler code below).

Using the structure mapping the load / store of g is correctly optimized out of the loop.
In the SREG macro case unfortunately not!

#include <stdint.h>
#include <util/atomic.h>
#include <avr/interrupt.h>
#include <avr/cpufunc.h>

#define ACCESS_ONCE(x) (*(volatile typeof(x)*)&(x))

typedef struct Cpu { // this is missing in avr headers
    volatile uint8_t r0;
    volatile uint8_t r1;
    volatile uint8_t r2;
    volatile uint8_t r3;
    volatile uint8_t ccp;
    volatile uint8_t r5;
    volatile uint8_t r6;
    volatile uint8_t r7;
    volatile uint8_t r8;
    volatile uint8_t r9;
    volatile uint8_t ra;
    volatile uint8_t rampz;
    volatile uint8_t rc;
    volatile uint16_t sp;
    volatile uint8_t sreg;
} Cpu_t;

#define CPU (*(Cpu_t *) 0x0030) 

static uint8_t  flag;
static uint16_t counter;
static uint16_t g; 

static inline uint16_t count() {
    const uint8_t save = CPU.sreg;
//    const uint8_t save = SREG; // suppresses optimization
    asm volatile("cli" : : :);
    const uint16_t t = ACCESS_ONCE(counter);
//    SREG = save; // suppresses optimization
    CPU.sreg = save;
    return t;
}
static void func(void) {
    for(uint8_t i = 0; i < 20; i++) {
        g += count();
        if (ACCESS_ONCE(flag)) {
            ACCESS_ONCE(flag) = 1;
        }
    }
}

ISR(USART0_RXC_vect) {
    _MemoryBarrier();
    counter += 1;
    if (counter >= 100) {
        flag = 1;
    }
}

int main() {
    func();
}

the generated assembly should be:

main:
lds r24,g        ;  g_lsm.16, g
lds r25,g+1      ;  g_lsm.16, g
ldi r18,lo8(20)  ;  ivtmp_7,
ldi r19,lo8(1)   ;  tmp56,
.L5:
in r22,__SREG__  ;  save, MEM[(struct Cpu_t *)48B].sreg
cli
lds r20,counter  ;  t, MEM[(volatile uint16_t *)&counter]
lds r21,counter+1        ;  t, MEM[(volatile uint16_t *)&counter]
out __SREG__,r22         ;  MEM[(struct Cpu_t *)48B].sreg, save
add r24,r20      ;  g_lsm.16, t
adc r25,r21      ;  g_lsm.16, t
lds r20,flag     ;  _6, MEM[(volatile uint8_t *)&flag]
cpse r20,__zero_reg__    ;  _6
sts flag,r19     ;  MEM[(volatile uint8_t *)&flag], tmp56
.L4:
subi r18,lo8(-(-1))      ;  ivtmp_7,
cpse r18,__zero_reg__    ;  ivtmp_7,
rjmp .L5         ;
sts g,r24        ;  g, g_lsm.16
sts g+1,r25      ;  g, g_lsm.16
ldi r24,0                ;
ldi r25,0                ;
ret

but using SREG it gets:

main:
ldi r24,lo8(20)  ;  ivtmp_12,
ldi r25,lo8(1)   ;  tmp59,
.L5:
in r18,__SREG__  ;  save, MEM[(volatile uint8_t *)63B]
cli
lds r20,counter  ;  t, MEM[(volatile uint16_t *)&counter]
lds r21,counter+1        ;  t, MEM[(volatile uint16_t *)&counter]
out __SREG__,r18         ;  MEM[(struct Cpu_t *)48B].sreg, save
lds r18,g        ;  g, g
lds r19,g+1      ;  g, g
add r18,r20      ;  tmp53, t
adc r19,r21      ; , t
sts g,r18        ;  g, tmp53
sts g+1,r19      ;  g, tmp53
lds r18,flag     ;  _6, MEM[(volatile uint8_t *)&flag]
cpse r18,__zero_reg__    ;  _6
sts flag,r25     ;  MEM[(volatile uint8_t *)&flag], tmp59
.L4:
subi r24,lo8(-(-1))      ;  ivtmp_12,
cpse r24,__zero_reg__    ;  ivtmp_12,
rjmp .L5         ;
ldi r24,0                ;
ldi r25,0                ;
ret


---


### compiler : `gcc`
### title : `nested cycle vectorization too restrictive`
### open_at : `2023-02-14T10:10:05Z`
### last_modified_date : `2023-02-14T10:10:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108785
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For the testcase in PR108782 we're rejecting to vectorize the nested cycle
because

      if (!flow_bb_inside_loop_p (loop, gimple_bb (use_stmt)))
        {
          if (dump_enabled_p ())
            dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,
                             "intermediate value used outside loop.\n");

          return NULL;
        }

but really all inner PHIs should be trivially vectorizable without
further restrictions.  The only thing we need to do is detect double
reductions, that is, when there's an outer reduction feeding the inner
loop PHI.


---


### compiler : `gcc`
### title : `missed vectorization in presence of conversion from uint64_t to float`
### open_at : `2023-02-15T14:16:14Z`
### last_modified_date : `2023-05-30T23:18:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108804
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.1`
### severity : `enhancement`
### contents :
in the following code [1] foo does not vectorize, bar doos
compiled with -march=haswell -Ofast --no-math-errno -Wall
see
https://godbolt.org/z/E6xzfavxc

clang seems do do better

[1]
#include<cstdint>


 
uint64_t d[512];
//uint32_t f[1024];
float f[1024];

void foo() {
    for (int i=0; i<512; ++i) {
        uint64_t k = d[i];
        auto x  = (k & 0x007FFFFF) |  0x3F800000;
        k = k >> 23;
        auto y  = (k & 0x007FFFFF) |  0x3F800000;
        f[i]=x; f[128+i] = y;

    }    
}

void bar() {
    for (int i=0; i<512; ++i) {
        uint64_t k = d[i];
        uint32_t x  = (k & 0x007FFFFF);
        x |= 0x3F800000;
        uint32_t y  = k >> 23;
        y  = (y & 0x007FFFFF) |  0x3F800000;
        f[i]=x; f[128+i] = y;

    }  
}


---


### compiler : `gcc`
### title : `[aarch64] use a extra mov instruction compare to llvm`
### open_at : `2023-02-16T10:30:44Z`
### last_modified_date : `2023-02-17T07:59:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108818
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
* test:https://gcc.godbolt.org/z/res6aTYqP
```
unsigned sel(unsigned X) {
  return X == 6 ? 6 : 8;
}
```

* gcc:
```
sel:
        cmp     w0, 6
        mov     w1, 8
        mov     w0, 6
        csel    w0, w1, w0, ne
        ret
```

* llvm:
```
sel:                                    // @sel
        mov     w8, #8
        cmp     w0, #6
        csel    w0, w0, w8, eq
        ret

```


---


### compiler : `gcc`
### title : `Inefficient address generation on POWER and RISC-V`
### open_at : `2023-02-16T17:53:31Z`
### last_modified_date : `2023-09-29T00:07:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108826
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
For the code (reduced from embench)

struct {
  unsigned int table[4][100];
} * _nettle_aes_decrypt_T;
unsigned int _nettle_aes_decrypt_w1;
void _nettle_aes_decrypt() {
  _nettle_aes_decrypt_T->table[2][0] =
      _nettle_aes_decrypt_T->table[2][_nettle_aes_decrypt_w1 >> 6 & 5];
}

current trunk generates

0:      addis 2,12,.TOC.-.LCF0@ha
        addi 2,2,.TOC.-.LCF0@l
        .localentry     _nettle_aes_decrypt,.-_nettle_aes_decrypt
        addis 9,2,.LANCHOR0+8@toc@ha
        lwz 9,.LANCHOR0+8@toc@l(9)
        addis 10,2,.LANCHOR0@toc@ha
        ld 10,.LANCHOR0@toc@l(10)
        srwi 9,9,6
        andi. 9,9,0x5
        addi 9,9,200
        sldi 9,9,2
        lwzx 9,10,9
        stw 9,800(10)
        blr

After the TOC loading, this shifts the value once, does the and, adds 200
and then shifts back the value. These two shifts are not necessary.

A better alternative would be something like (please excuse any errors)

        srwi 9,9,4
        andi 9,9,20
        add  9,9,2
        lwz  9,800(9)
        stw  9,800(9)

saving an instruction.

RISC-V does something similar.  According to godbolt:

        lui     a5,%hi(_nettle_aes_decrypt_w1)
        lw      a5,%lo(_nettle_aes_decrypt_w1)(a5)
        lui     a4,%hi(_nettle_aes_decrypt_T)
        ld      a4,%lo(_nettle_aes_decrypt_T)(a4)
        srliw   a5,a5,6
        andi    a5,a5,5
        addi    a5,a5,200
        slli    a5,a5,2
        add     a5,a4,a5
        lw      a5,0(a5)
        sw      a5,800(a4)
        ret


(which is why I think this is a general RTL optimization issue).
x86 is much better:

        movl    _nettle_aes_decrypt_w1(%rip), %eax
        movq    _nettle_aes_decrypt_T(%rip), %rdx
        shrl    $6, %eax
        andl    $5, %eax
        movl    800(%rdx,%rax,4), %eax
        movl    %eax, 800(%rdx)
        ret

but it can use the complex addressing modes on x86.


---


### compiler : `gcc`
### title : `QImode binary ops with one zero-extracted argument can be optimized`
### open_at : `2023-02-17T09:26:38Z`
### last_modified_date : `2023-02-17T16:52:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108831
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Following testcase:

--cut here--
struct S
{
  unsigned char pad1;
  unsigned char val;
  unsigned short pad2;
};

unsigned char
test_add (unsigned char a, struct S b)
{
  a += b.val;

  return a;
}
--cut here--

should be compiled to something like:

        addb %dh, %al

but is currently compiled to:

        movzbl  %dh, %edx
        addl    %edx, %eax


---


### compiler : `gcc`
### title : `Option for rerolling loops`
### open_at : `2023-02-17T17:16:12Z`
### last_modified_date : `2023-02-20T08:16:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108839
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Code sometimes contains manual unrolling.  For example, the BLAS
reference implementation, subroutine DSCAL, has

      IF (INCX.EQ.1) THEN
*
*        code for increment equal to 1
*
*
*        clean-up loop
*
         M = MOD(N,5)
         IF (M.NE.0) THEN
            DO I = 1,M
               DX(I) = DA*DX(I)
            END DO
            IF (N.LT.5) RETURN
         END IF
         MP1 = M + 1
         DO I = MP1,N,5
            DX(I) = DA*DX(I)
            DX(I+1) = DA*DX(I+1)
            DX(I+2) = DA*DX(I+2)
            DX(I+3) = DA*DX(I+3)
            DX(I+4) = DA*DX(I+4)
         END DO
      ELSE

While such code may have been beneficial on old architectures, by
now this disturbs the compiler's own unrolling and vectorization,
and it increases code size.

It could be beneficial to have a -freroll-loops option, which
undid the manual unrolling of the code above. This could be
stand-alone, or included in options such as -Os.


---


### compiler : `gcc`
### title : `Aarch64 doesn't optimize away shift counter masking`
### open_at : `2023-02-17T17:59:00Z`
### last_modified_date : `2023-04-19T08:37:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108840
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
As mentioned in 
https://gcc.gnu.org/pipermail/gcc-patches/2023-February/612214.html
aarch64 doesn't optimize away and instructions masking shift count if there is more than one shift with the same count.  Consider -O2 -fno-tree-vectorize:
int
foo (int x, int y)
{
  return x << (y & 31);
}

void
bar (int x[3], int y)
{
  x[0] <<= (y & 31);
  x[1] <<= (y & 31);
  x[2] <<= (y & 31);
}

void
baz (int x[3], int y)
{
  y &= 31;
  x[0] <<= y;
  x[1] <<= y;
  x[2] <<= y;
}

void corge (int, int, int);

void
qux (int x, int y, int z, int n)
{
  n &= 31;
  corge (x << n, y << n, z >> n);
}

foo is optimized correctly, combine matches the shift with masking, but in the rest of cases due to costs the desirable combination is rejected.  Shift with embedded masking of the count should have rtx_cost the same as normal shift when it is actually under the hood the shift itself.


---


### compiler : `gcc`
### title : `sometimes a < b && c < b is not optimized to MAX<a, c> < b`
### open_at : `2023-02-18T04:10:12Z`
### last_modified_date : `2023-02-20T08:20:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108841
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
```
int f1(int a0, int a1, int b, int c0, int c1)
{
        int a = a0 < a1 ? a1 : a0;
        if (a < b) {
          int c = c0 < c1 ? c1 : c0;
          if (c < b)
            return 0;
        }
        return 1;
}
int f2(int a0, int a1, int b, int c0, int c1)
{
        int a = a0 < a1 ? a1 : a0;
        int c = c0 < c1 ? c1 : c0;
        if (a < b) {
          if (c < b)
            return 0;
        }
        return 1;
}
```
These 2 functions should produce the same code, the only difference is the calculation of c is not condtional.


---


### compiler : `gcc`
### title : `sincos opportunity missed`
### open_at : `2023-02-18T09:38:56Z`
### last_modified_date : `2023-02-20T08:28:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108844
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Two related test cases (which do the same, but are handled differently).

This is code for calculating a Jacobian, a frequent task in solving
non-linear systems of equations.  (I am using C instead of Fortran because
Fortran does not support fallthrough).

$ cat a.c
#include <math.h>

void f1 (double x, double y, double f[2], double fjac[2][2], int flag)
{
  switch (flag)
    {
    case 1:
      f[0] = x * y;
      f[1] = sin(x)*y*y;
      break;
    case 2:
      fjac[0][0] = y;
      fjac[1][0] = cos(x)*y*y;
      fjac[0][1] = x;
      fjac[1][1] = 2*sin(x)*y;
      break;
    case 3:
      f[0] = x * y;
      f[1] = sin(x)*y*y;
      fjac[0][0] = y;
      fjac[1][0] = cos(x)*y*y;
      fjac[0][1] = x;
      fjac[1][1] = 2*sin(x)*y;
      break;
     default:
      __builtin_unreachable();
    }
}
$ cat b.c
#include <math.h>

void f1 (double x, double y, double f[2], double fjac[2][2], int flag)
{
  switch (flag)
    {
    case 1:
    case 3:
      f[0] = x * y;
      f[1] = sin(x)*y*y;
      if (flag != 3)
        break;
      /* Fallthrough */
    case 2:
      fjac[0][0] = y;
      fjac[1][0] = cos(x)*y*y;
      fjac[0][1] = x;
      fjac[1][1] = 2*sin(x)*y;
      break;
    default:
      __builtin_unreachable();
    }
}
$ gcc -O3 -S a.c b.c

a.s looks good for flag=3:
        leaq    64(%rsp), %rsi
        leaq    72(%rsp), %rdi
        movaps  %xmm3, 48(%rsp)
        movsd   %xmm1, 32(%rsp)
        movsd   %xmm0, 24(%rsp)
        call    sincos

but the code for flag=2 looks like

        cmpl    $2, %edx
        je      .L2
[...]
.L2:
        .cfi_restore_state
        movaps  %xmm3, 32(%rsp)
        movsd   %xmm1, 24(%rsp)
        movsd   %xmm0, (%rsp)
        call    cos
        movsd   (%rsp), %xmm2
        movq    %xmm0, %rbx
        movapd  %xmm2, %xmm0
        call    sin

b.s generates no call to sincos:
$ egrep  '(sin|cos)' b.s
        call    sin
        call    sin
        call    cos


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] unnecessary bitwise AND on boolean types and shifting of the "sign" bit`
### open_at : `2023-02-19T13:37:50Z`
### last_modified_date : `2023-09-17T01:49:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108847
### status : `ASSIGNED`
### tags : `deferred, missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `trivial`
### contents :
Godbolt: https://gcc.godbolt.org/z/fsavMzMo7

```
void
set_bool(bool& fl, __UINT32_TYPE__ value)
  {
    fl |= value >> 31;
  }
```

This code shifts a `uint32` to the right by 31 bits, so the result will only be 0 or 1.

Clang outputs:

```
set_bool(bool&, unsigned int):                         # @set_bool(bool&, unsigned int)
        shr     esi, 31
        or      byte ptr [rdi], sil
        ret
```

but GCC emits an additional unnecessary bitwise AND operation on the destination operand:

```
set_bool(bool&, unsigned int):
        shr     esi, 31
        or      BYTE PTR [rdi], sil
        and     BYTE PTR [rdi], 1
        ret
```


---


### compiler : `gcc`
### title : `gcc -pie generates unwanted PE export table`
### open_at : `2023-02-19T22:34:35Z`
### last_modified_date : `2023-09-30T14:18:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108851
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
When gcc is invoked with -pie option then for PE executables it automatically generates export table, even when executable does not export anything.

Test case:

$ cat test-pie.c
int func(void) {
        return 42;
}

int main() {
        return func();
}

$ x86_64-w64-mingw32-gcc -pie test-pie.c -o test-pie.exe

$ x86_64-w64-mingw32-objdump -p test-pie.exe | grep -A 20 'There is an export table'
There is an export table in .edata at 0x140008000

The Export Tables (interpreted .edata section contents)

Export Flags                    0
Time/Date stamp                 63f2a29f
Major/Minor                     0/0
Name                            0000000000008028 test-pie.exe
Ordinal Base                    1
Number in:
        Export Address Table            00000000
        [Name Pointer/Ordinal] Table    00000000
Table Addresses
        Export Address Table            0000000000008028
        Name Pointer Table              0000000000008028
        Ordinal Table                   0000000000008028

Export Address Table -- Ordinal Base 1

[Ordinal/Name Pointer] Table

Without gcc's -pie option, executable does not have export table.

Note that similar issue was reported also to LD https://sourceware.org/bugzilla/show_bug.cgi?id=30004 and proposed LD patch does not change behavior in this issue.


---


### compiler : `gcc`
### title : `Increment and decrement on std::experimental::where_expression should optimize better`
### open_at : `2023-02-20T10:23:05Z`
### last_modified_date : `2023-05-25T07:07:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108856
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.0`
### severity : `normal`
### contents :
#include <experimental/simd>

namespace stdx = std::experimental;

auto f(stdx::native_simd<int> a, stdx::native_simd_mask<int> k)
{
  ++where(k, a);
  return a;
}

With AVX512 this should compile to a bitmask to vectormask conversion with subsequent subtraction:
	kmovw	k0, edi
	vpbroadcastmw2d	zmm1, k0
	vpsubd	zmm0, zmm0, zmm1

Instead we get:
  vmovdqa32 zmm1, zmm0
  mov eax, 1
  kmovw k1, edi
  vpbroadcastd zmm0, eax
  vmovdqa32 zmm2, zmm1
  vpaddd zmm2{k1}, zmm1, zmm0
  vmovdqa32 zmm0, zmm2

Without AVX512 this should compile to a single subtraction:
	vpsubd	ymm0, ymm0, ymm1

Instead we get:
  mov eax, 1
  vmovd xmm2, eax
  vpbroadcastd ymm2, xmm2
  vpaddd ymm2, ymm0, ymm2
  vpblendvb ymm0, ymm0, ymm2, ymm1


---


### compiler : `gcc`
### title : `Unrolling could use range information`
### open_at : `2023-02-20T20:26:51Z`
### last_modified_date : `2023-03-08T16:36:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108863
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
Created attachment 54497
Assembly code generated by test case

Looking a bit more at the code generated for the test code of PR108839.

For the test
$ cat u2.c
void foo(double *const restrict dx, double *dy, double da, long int n)
{
      long int m = n % 4;
      for (unsigned long i = 0; i < m; i++ )
        dy[i] = dy[i] + da * dx[i];
}

a recently-ish trunk gives, with

$ gcc -S -O3  -funroll-all-loops -fno-tree-vectorize u2.c

far too much unrolling for a loop which can only be executed, at
most, four times (see attachment).

The range information about m does not appear to be propagated to
the unroll passes.


---


### compiler : `gcc`
### title : `Using the C++ front-end causes no DCE from happening`
### open_at : `2023-02-21T16:27:02Z`
### last_modified_date : `2023-02-21T23:28:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108873
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `13.0`
### severity : `enhancement`
### contents :
+++ This bug was initially created as a clone of Bug #105833 +++
Testcase from #105833
```
void foo();

static int b;
static int **c;
static int ***d = &c;
static int ****e = &d;

static char(a)(char g, int h) { return h >= 2 ? g : g << h; }

int main() {
  **e == 0;
  b = b ? 4 : 0;
  if (!a(*e != 0, b))
    foo();
}
```
For some reason (I Have not done any anlysis of why), using the C++ front-end with the above does not get optimized by does when using the C front-end.
This does not seem like a regression either.


---


### compiler : `gcc`
### title : `[10/11/12/13 Regression] Missing bswap detection`
### open_at : `2023-02-21T16:30:04Z`
### last_modified_date : `2023-03-10T17:54:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108874
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
If we look at the arm testcases in gcc.target/arm/rev16.c
typedef unsigned int __u32;

__u32
__rev16_32_alt (__u32 x)
{
  return (((__u32)(x) & (__u32)0xff00ff00UL) >> 8)
         | (((__u32)(x) & (__u32)0x00ff00ffUL) << 8);
}

__u32
__rev16_32 (__u32 x)
{
  return (((__u32)(x) & (__u32)0x00ff00ffUL) << 8)
         | (((__u32)(x) & (__u32)0xff00ff00UL) >> 8);
}

we should be able to generate rev16 instructions for aarch64 (and arm) i.e. recognise a __builtin_bswap16 essentially.
GCC fails to do so and generates:
__rev16_32_alt:
        lsr     w1, w0, 8
        lsl     w0, w0, 8
        and     w1, w1, 16711935
        and     w0, w0, -16711936
        orr     w0, w1, w0
        ret
__rev16_32:
        lsl     w1, w0, 8
        lsr     w0, w0, 8
        and     w1, w1, -16711936
        and     w0, w0, 16711935
        orr     w0, w1, w0
        ret

whereas clang manages to recognise it all into:
__rev16_32_alt:                         // @__rev16_32_alt
        rev16   w0, w0
        ret
__rev16_32:                             // @__rev16_32
        rev16   w0, w0
        ret

does the bswap pass need some tweaking perhaps?

Looks like this worked fine with GCC 5 but broke in the GCC 6 timeframe so marking as a regression


---


### compiler : `gcc`
### title : `Mis-optimization with splitting floating point into a significand and exponent.`
### open_at : `2023-02-21T21:42:40Z`
### last_modified_date : `2023-02-22T18:21:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108878
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.0`
### severity : `enhancement`
### contents :
Fortran does not have an equivalent to C's frexp(), which splits a floating point value into its significand and exponent.  Fortran has exponent() and fraction() to grab the exponent and significand, respectively.  Consider,

% cat a.f90
subroutine foo(x, f, m)
   real, intent(in) :: x
   real, intent(out) :: f
   integer, intent(out) :: m
   m = exponent(x)
   f = fraction(x)
end subroutine

% gfortran -Ofast -fdump-tree-optimized -S a.f90
% more a.f90.254t.optimized
...
  <bb 2> [local count: 1073741824]:
  _5 = *x_4(D);
  __builtin_frexpf (_5, &D.4261);
  _1 = D.4261;
  *m_7(D) = _1;
  D.4261 ={v} {CLOBBER(eol)};
  _2 = __builtin_frexpf (_5, &D.4263);
  *f_11(D) = _2;
  D.4263 ={v} {CLOBBER(eol)};
  return;

As can be seen, __builtin_frexpf() is called twice.  The generated assembly also shows two calls to frexpf().


% more a.s
...
        .cfi_offset 3, -32
        movq    %rsi, %rbx
        subq    $32, %rsp
        movss   (%rdi), %xmm1
        leaq    -20(%rbp), %rdi
        movaps  %xmm1, %xmm0
        movss   %xmm1, -36(%rbp)
        call    frexpf
        movl    -20(%rbp), %eax
        movss   -36(%rbp), %xmm1
        leaq    -20(%rbp), %rdi
        movl    %eax, (%r12)
        movaps  %xmm1, %xmm0
        call    frexpf
        movss   %xmm0, (%rbx)
        addq    $32, %rsp
        popq    %rbx
        popq    %r12
        popq    %rbp
        .cfi_def_cfa 7, 8
...


---


### compiler : `gcc`
### title : `Conversions std::float16_t<->float with FP16C are not vectorized`
### open_at : `2023-02-23T11:02:39Z`
### last_modified_date : `2023-02-24T07:16:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108902
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Please see
https://godbolt.org/z/dGn4qhPef

thx
Gero


---


### compiler : `gcc`
### title : `Miss vectorization for masked gather w/o restrict qualifier.`
### open_at : `2023-02-24T05:08:41Z`
### last_modified_date : `2023-02-24T12:59:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108916
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
void
foo1 (double* __restrict a, long long* b, double* c, long long* d)
{
    for (long long i = 0; i != 10000; i++)
      {
        if (b[i])
          a[i] = 0;
        else
          a[i] = c[d[i]];
      }
}


void
foo2 (double* __restrict a, long long* b, double* __restrict c, long long* d)
{
    for (long long i = 0; i != 10000; i++)
      {
        if (b[i])
          a[i] = 0;
        else
          a[i] = c[d[i]];
      }
}

w/ -Ofast -march=skylake-avx512, foo1 can't be vectorized, but foo2 can, the only difference between foo1 and foo2 is restrict qualifier for double* c.
But there's already a restrict qualifier for double* __restrict a in both cases,   why double* __restrict c matters.


---


### compiler : `gcc`
### title : `fmod() 13x slowdown in gcc4.9 dropping "fprem" and calling fmod()`
### open_at : `2023-02-24T12:22:46Z`
### last_modified_date : `2023-03-09T23:26:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108922
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.1`
### severity : `normal`
### contents :
Created attachment 54528
bench.cpp

This performance regression is since:

[PATCH, i386]: Enable reminder{sd,df,xf} and fmod{sf,df,xf} only for flag_finite_math_only.
https://gcc.gnu.org/pipermail/gcc-patches/2014-September/400104.html

https://gcc.gnu.org/git/?p=gcc.git;a=commitdiff;h=93ba85fdd253b4b9cf2b9e54e8e5969b1a3db098

Reproducible with attached "bench.cpp":
g++ (GCC) 4.8.3 20140517 (prerelease)
real	0m0.329s
g++ (GCC) 4.9.3 20150207 (prerelease)
real	0m4.396s

The committer claims "do not return NaN for infinities, but generate invalid-arithmetic-operand exception.". But my attached testcase tests that all the corner cases do have both the same result value and the same exceptions generated.

The committer also claims "fixes ieee_2.f90 testsuite failure" but I have no idea where to find this testsuite.

g++ (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18)
/home/azul/t/zuc1182/fmod.C:7
  4005f8:       dd 44 24 30             fldl   0x30(%rsp)
  4005fc:       dd 44 24 38             fldl   0x38(%rsp)
  400600:       d9 c1                   fld    %st(1)
  400602:       d9 c1                   fld    %st(1)
  400604:       d9 f8                   fprem
  400606:       df e0                   fnstsw %ax
  400608:       f6 c4 04                test   $0x4,%ah
  40060b:       75 f7                   jne    400604 <main+0x34>
  40060d:       dd d9                   fstp   %st(1)
  40060f:       dd 5c 24 18             fstpl  0x18(%rsp)
  400613:       f2 0f 10 44 24 18       movsd  0x18(%rsp),%xmm0
  400619:       66 0f 2e c0             ucomisd %xmm0,%xmm0
^^^
Here it tests the result is finite;
if it is not it will fallback to calling fmod().
But I do not find even that needed, one could just use the "fprem" result.
  40061d:       7a 06                   jp     400625 <main+0x55>
  40061f:       74 2f                   je     400650 <main+0x80>
  400621:       d9 c9                   fxch   %st(1)
  400623:       eb 0b                   jmp    400630 <main+0x60>
  400625:       d9 c9                   fxch   %st(1)
  400627:       66 0f 1f 84 00 00 00    nopw   0x0(%rax,%rax,1)
  40062e:       00 00
  400630:       dd 5c 24 08             fstpl  0x8(%rsp)
  400634:       f2 0f 10 4c 24 08       movsd  0x8(%rsp),%xmm1
  40063a:       dd 5c 24 08             fstpl  0x8(%rsp)
  40063e:       f2 0f 10 44 24 08       movsd  0x8(%rsp),%xmm0
  400644:       e8 6f fe ff ff          callq  4004b8 <fmod@plt>
  400649:       eb 09                   jmp    400654 <main+0x84>
  40064b:       0f 1f 44 00 00          nopl   0x0(%rax,%rax,1)
  400650:       dd d8                   fstp   %st(0)
  400652:       dd d8                   fstp   %st(0)
  400654:       83 c3 01                add    $0x1,%ebx
  400657:       f2 0f 11 44 24 28       movsd  %xmm0,0x28(%rsp)
/home/azul/t/zuc1182/fmod.C:6
  40065d:       81 fb 00 e1 f5 05       cmp    $0x5f5e100,%ebx
  400663:       75 93                   jne    4005f8 <main+0x28>

Similar issue may be with drem() (=remainder()) vs. "fprem1" instruction.

I expect the same issue also affects fmodf(), dremf() and remainderf().

Another topic is why the glibc fmod() implementation just does not use "fprem" on i686/x86_64 arch.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Missing rev16 detection`
### open_at : `2023-02-25T19:50:10Z`
### last_modified_date : `2023-07-07T10:44:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108933
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
This is the arm version of this bug.


+++ This bug was initially created as a clone of Bug #108874 +++


If we look at the arm testcases in gcc.target/arm/rev16.c
typedef unsigned int __u32;

__u32
__rev16_32_alt (__u32 x)
{
  return (((__u32)(x) & (__u32)0xff00ff00UL) >> 8)
         | (((__u32)(x) & (__u32)0x00ff00ffUL) << 8);
}

__u32
__rev16_32 (__u32 x)
{
  return (((__u32)(x) & (__u32)0x00ff00ffUL) << 8)
         | (((__u32)(x) & (__u32)0xff00ff00UL) >> 8);
}

we should be able to generate rev16 instructions for aarch64 (and arm) i.e. recognise a __builtin_bswap16 essentially.
GCC fails to do so and generates:
__rev16_32_alt:
        lsr     w1, w0, 8
        lsl     w0, w0, 8
        and     w1, w1, 16711935
        and     w0, w0, -16711936
        orr     w0, w1, w0
        ret
__rev16_32:
        lsl     w1, w0, 8
        lsr     w0, w0, 8
        and     w1, w1, -16711936
        and     w0, w0, 16711935
        orr     w0, w1, w0
        ret

whereas clang manages to recognise it all into:
__rev16_32_alt:                         // @__rev16_32_alt
        rev16   w0, w0
        ret
__rev16_32:                             // @__rev16_32
        rev16   w0, w0
        ret

does the bswap pass need some tweaking perhaps?

Looks like this worked fine with GCC 5 but broke in the GCC 6 timeframe so marking as a regression


---


### compiler : `gcc`
### title : `Optimize shift counts`
### open_at : `2023-02-27T12:36:00Z`
### last_modified_date : `2023-09-21T14:02:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108949
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
From https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108941#c13 :
Because various backends support shift count truncation or have patterns that recognize it in certain cases, I wonder if middle-end couldn't canonicalize shift count (N + x)
where N is multiple of shift first operand's bitsize B to x & (B - 1) where the latter
is often optimized away while the former is not.
For similar N - x it is more questionable because N - x is a single GIMPLE statement while -y & (B - 1) are two; perhaps it could be done at expansion time though.
In generic code at least for SHIFT_COUNT_TRUNCATED targets, otherwise maybe if one can easily detect negation optab and subtraction instruction not accepting immediate for the minuend.  Or handle all this in each of the backends?

int
foo (int x, int y)
{
  return x << (y & 31);
}

int
bar (int x, int y)
{
  return x << (32 + y);
}

int
baz (int x, int y)
{
  return x << (-y & 31);
}

int
qux (int x, int y)
{
  return x << (32 - y);
}


---


### compiler : `gcc`
### title : `inefficient codegen for trivial equality (defaulted operator==)`
### open_at : `2023-02-27T16:29:01Z`
### last_modified_date : `2023-07-13T20:31:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108953
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `12.0`
### severity : `enhancement`
### contents :
Consider this example:

#include <cstdint>
#include <cstddef>
#include <string.h>

struct C
{
   uint8_t a;
   uint8_t b;
   uint8_t c;
   uint8_t d;
   uint16_t e;
   uint16_t f;
   int32_t g;

   bool operator==(C const&) const = default;
};

bool check(C const& lhs, C const& rhs) {
    #ifdef MEMCMP
    return memcmp(&lhs, &rhs, sizeof(lhs)) == 0;
    #else
    return lhs == rhs;
    #endif
}

There are two implementations of check here, but lead to suboptimal code.

When using MEMCMP, gcc trunk -O3 emits:

check(C const&, C const&):
        mov     rax, QWORD PTR [rsi]
        cmp     QWORD PTR [rdi], rax
        je      .L5
.L2:
        mov     eax, 1
        test    eax, eax
        sete    al
        ret
.L5:
        mov     eax, DWORD PTR [rsi+8]
        cmp     DWORD PTR [rdi+8], eax
        jne     .L2
        xor     eax, eax
        test    eax, eax
        sete    al
        ret

There's a few extra instructions here (mov eax, 1; test eax, eax; sete al;... do we need all three of those to return 0?)

When using defaulted comparisons, gcc trunk -O3 doesn't collapse any of the comparisons, and instead emits 7 distinct checks:

check(C const&, C const&):
        movzx   ecx, BYTE PTR [rsi]
        xor     eax, eax
        cmp     BYTE PTR [rdi], cl
        jne     .L1
        movzx   edx, BYTE PTR [rsi+1]
        cmp     BYTE PTR [rdi+1], dl
        jne     .L1
        movzx   edx, BYTE PTR [rsi+2]
        cmp     BYTE PTR [rdi+2], dl
        jne     .L1
        movzx   edx, BYTE PTR [rsi+3]
        cmp     BYTE PTR [rdi+3], dl
        jne     .L1
        movzx   edx, WORD PTR [rsi+4]
        cmp     WORD PTR [rdi+4], dx
        jne     .L1
        movzx   eax, WORD PTR [rsi+6]
        cmp     WORD PTR [rdi+6], ax
        mov     edx, DWORD PTR [rsi+8]
        sete    al
        cmp     DWORD PTR [rdi+8], edx
        sete    dl
        and     eax, edx
.L1:
        ret

Compare this to clang, which for both the memcmp and the default equality versions emits this:

check(C const&, C const&):                        # @check(C const&, C const&)
        mov     rax, qword ptr [rdi]
        xor     rax, qword ptr [rsi]
        mov     ecx, dword ptr [rdi + 8]
        xor     ecx, dword ptr [rsi + 8]
        or      rcx, rax
        sete    al
        ret

Looks like there are two missing optimizations here for gcc: (1) the memcmp does get optimized into an 8-byte and 4-byte comparison, but then the result of that optimization doesn't get optimized further and (2) multiple trivial comparisons don't get coalesced together.


---


### compiler : `gcc`
### title : `Powerpcle could generate mtvsrdd for zero extend DI to TI mode, when the TImode is in a vector register`
### open_at : `2023-02-27T22:28:49Z`
### last_modified_date : `2023-02-28T10:46:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108958
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
If you have a DImode variable (i.e. long) in a GPR, and you want to zero extend it to TImode (i.e. `__int128', and the result is needed in a vector register, you could just do a single `mtvsrdd' instruction, instead of separate zero a GPR register, and then `mtvsrd' and `mtvsrdd' instructions.


---


### compiler : `gcc`
### title : `[13 Regression] RISC-V: shiftadd cost model bug needlessly preferring syth_multiply`
### open_at : `2023-03-01T19:34:20Z`
### last_modified_date : `2023-03-06T21:50:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108987
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
gcc trunk is preferring synthetic multiply using shift+add even when they are costlier than multiply.

unsigned long long f5(unsigned long long i)
{
  return i * 0x0202020202020202ULL;
}

riscv64-unknown-linux-gnu-gcc -c -O2 -march=rv64gc_zba

f5:
	slli	a5,a0,8
	add	a0,a5,a0
	slli	a5,a0,16
	add	a0,a0,a5
	slli	a5,a0,32
	add	a0,a0,a5
	slli	a0,a0,1
	ret

With gcc 12.2 this used to be

f5:
	lui	a5,%hi(.LC0)
	ld	a5,%lo(.LC0)(a5)
	mul	a0,a0,a5
	ret

This is a regression introduced by commit f90cb39235c4 ("RISC-V: costs: support shift-and-add in strength-reduction"). It introduced the cost for shift[1-3]+add (to favor SH*ADD) but due to a coding bug ended up doing this for all shift values, affecting synth multiply among others.

This showed up as dynamic icount regression in SPEC 531.deepsjeng.


---


### compiler : `gcc`
### title : `Too restrictive precision check in fold and simplify pattern for PR70920`
### open_at : `2023-03-02T13:49:44Z`
### last_modified_date : `2023-03-03T08:21:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108990
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
While experimenting with a new gimple pass we noticed that pr70920.c is sensitive on order of substitutions made.  If 0 is propagated first into if stmt, match and simplify fails to simplify the conditional since it compares pointer type 0 with integer converted to pointer type.
TYPE_PRECISION of int is 32.

(for cmp (ne eq)
 (simplify
  (cmp (convert @0) INTEGER_CST@1)
  (if (((POINTER_TYPE_P (TREE_TYPE (@0))
         && !FUNC_OR_METHOD_TYPE_P (TREE_TYPE (TREE_TYPE (@0)))
         && INTEGRAL_TYPE_P (TREE_TYPE (@1))
         /* Don't perform this optimization in GENERIC if @0 has reference
            type when sanitizing.  See PR101210.  */
         && !(GENERIC
              && TREE_CODE (TREE_TYPE (@0)) == REFERENCE_TYPE
              && (flag_sanitize & (SANITIZE_NULL | SANITIZE_ALIGNMENT))))
        || (INTEGRAL_TYPE_P (TREE_TYPE (@0))
            && POINTER_TYPE_P (TREE_TYPE (@1))
            && !FUNC_OR_METHOD_TYPE_P (TREE_TYPE (TREE_TYPE (@1)))))
       && TYPE_PRECISION (TREE_TYPE (@0)) == TYPE_PRECISION (TREE_TYPE (@1)))
   (cmp @0 (convert @1)))))

So last conditional is false (32 and 64)

In this adjusted testcase:
#include <stdint.h>

void f1();
void f2();

void
foo (int a)
{
  int *b = (int *)a;
  if (b == (void *)0)
    {
      f1 ();
    }
}

the unnecessary cast survives to forwprop while it chould be caught by cpp1


---


### compiler : `gcc`
### title : `Regression: Branch direction canonicalization leads to pointless tail duplication / CSE/sinking by inverting branch`
### open_at : `2023-03-02T17:12:50Z`
### last_modified_date : `2023-03-02T23:39:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108992
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `6.0`
### severity : `normal`
### contents :
There are two regressions, in GCC 7 and in GCC 8. If you invert branch manually (replace 'cond' with '!cond') - the results were the same previously.

// Regressed since GCC 7: https://godbolt.org/z/h4brz7zG9
void use(int *);
void use2(int *);

void foo(bool cond, int * p)
{
    if (cond) {
        use(p);
    }
    use2(p);
}

// GCC 6
foo(bool, int*):
        test    dil, dil
        push    rbx
        mov     rbx, rsi
        je      .L2
        mov     rdi, rsi
        call    use(int*)
.L2:
        mov     rdi, rbx
        pop     rbx
        jmp     use2(int*)

// GCC 7
foo(bool, int*):
        test    dil, dil
        jne     .L8
        mov     rdi, rsi
        jmp     use2(int*)
.L8:
        sub     rsp, 24
        mov     rdi, rsi
        mov     QWORD PTR [rsp+8], rsi
        call    use2(int*)
        mov     rsi, QWORD PTR [rsp+8]
        add     rsp, 24
        mov     rdi, rsi
        jmp     use2(int*)


// Regressed since GCC 8: https://godbolt.org/z/MjxqTnbKa
void use(int *);
void use2(int *);

void foo(int * p, bool cond)
{
    if (cond) {
        use(p);
    }
    use2(p);
}

// GCC 7
foo(int*, bool):
        test    sil, sil
        push    rbx
        mov     rbx, rdi
        je      .L2
        call    use(int*)
.L2:
        mov     rdi, rbx
        pop     rbx
        jmp     use2(int*)

// GCC 8
foo(int*, bool):
        push    rbx
        mov     rbx, rdi
        test    sil, sil
        jne     .L5
        mov     rdi, rbx
        pop     rbx
        jmp     use2(int*)
.L5:
        call    use(int*)
        mov     rdi, rbx
        pop     rbx
        jmp     use2(int*)


---


### compiler : `gcc`
### title : `GCC prediction on bool comparisons seems wrong`
### open_at : `2023-03-02T22:51:37Z`
### last_modified_date : `2023-03-07T14:49:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108997
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
I noticed this while looking into 108992.
I suspect we look into this but I am not 100% sure this is a bug.
Testcase:
void use(int *);
void use2(int *);

void foo(bool cond, int * p)
{
    if (cond) {
        use(p);
    }
    use2(p);
}

GCC predicts the branch to be taken 66% of the time, but I would have assume it should be 50% as it is a bool and there is no other information. I suspect this is due to prediction when comparing against 0.


---


### compiler : `gcc`
### title : `Maybe LRA produce inaccurate hardware register occupancy information for subreg operand`
### open_at : `2023-03-03T01:31:38Z`
### last_modified_date : `2023-03-30T08:17:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=108999
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
The problem code on the compiler explorer is here: https://godbolt.org/z/GaGWEahPY

The problem is that the line `mov z1.d, z4.d` of the assembly code[1] is unnecessary. I find the reason is the LRA pass[2] thinks the hard registers `(subreg:VNx2DI (reg/v:VNx4DI 103 [ result ]) [16, 16])` occupied is in conflict with `(reg/v:VNx2DI 98 [ v19 ])`[3]. That is not true because r103 occupied 32 and 33, and r98 occupied 34 according to the dump information of IRA[4]. This is because the function process_alt_operands in lra-constraints.cc source file[5]. When it checks whether the operand 0 of insn 39 is in conflict with other operands of insn 39, it set operand 0 occupied 33 and 34 according to the mode(`biggest_mode[i]`) and the start hard regno 33(`clobbered_hard_regno`). The mode it used is VNx4DI, I think it should use Vnx2DI which is the proper mode for the entire operand 0. So for getting the occupied hard register of the normal subreg operand, it is maybe too wider if use the inner reg's mode.

References:

[1] assembly code
```
subreg_coalesce5:
        mov     p1.b, p0.b
        ld2d    {z0.d - z1.d}, p0/z, [x0]
        cmp     w1, 0
        ble     .L2
        sxtw    x1, w1
        mov     x0, 0
.L3:
        ld1d    z3.d, p1/z, [x2, x0, lsl 3]
        ld1d    z2.d, p1/z, [x3, x0, lsl 3]
        add     x0, x0, 1
        movprfx z4.d, p0/z, z1.d
        mla     z4.d, p0/m, z3.d, z2.d
        movprfx z0.d, p0/z, z0.d
        mla     z0.d, p0/m, z3.d, z2.d
        mov     z1.d, z4.d
        cmp     x1, x0
        bne     .L3
.L2:
        st2d    {z0.d - z1.d}, p0, [x4]
        ret
```

[2] partial content of LRA dump info
```
...
            0 Early clobber: reject++
            0 Conflict early clobber reload: reject--
          alt=0,overall=6,losers=1,rld_nregs=0
            0 Early clobber: reject++
          alt=1,overall=1,losers=0,rld_nregs=0
	 Choosing alt 1 in insn 36:  (0) &w  (1) Upl  (2) w  (3) w  (4) 0  (5) Dz {*cond_fmavnx2di_any}
            0 Early clobber: reject++
            0 Matched conflict early clobber reloads: reject--
          alt=0,overall=6,losers=1,rld_nregs=0
            0 Early clobber: reject++
            0 Conflict early clobber reload: reject--
          alt=1,overall=6,losers=1,rld_nregs=0
            0 Early clobber: reject++
            2 Matching earlyclobber alt: reject--
          alt=2,overall=6,losers=1,rld_nregs=1
            0 Early clobber: reject++
            3 Matching earlyclobber alt: reject--
          alt=3,overall=6,losers=1,rld_nregs=1
            0 Early clobber: reject++
            5 Matching earlyclobber alt: reject--
            5 Non-pseudo reload: reject+=2
            5 Non input pseudo reload: reject++
            alt=4,overall=9,losers=1 -- refuse
            Staticly defined alt reject+=6
            0 Early clobber: reject++
            5 Non-pseudo reload: reject+=2
            5 Non input pseudo reload: reject++
            alt=5,overall=16,losers=1 -- refuse
	 Choosing alt 0 in insn 39:  (0) =&w  (1) Upl  (2) w  (3) w  (4) w  (5) Dz {*cond_fmavnx2di_any}
      Creating newreg=117, assigning class FP_REGS to r117
   39: r117:VNx2DI=unspec[r104:VNx16BI#0,r97:VNx2DI*r98:VNx2DI+r103:VNx4DI#[16,16],const_vector] 284
      REG_DEAD r98:VNx2DI
      REG_DEAD r97:VNx2DI
    Inserting insn reload after:
   76: r103:VNx4DI#[16,16]=r117:VNx2DI
...
```

[3] partial rtl of IRA pass
```lisp
(insn 36 43 37 4 (set (subreg:VNx2DI (reg/v:VNx4DI 103 [ result ]) 0)
        (unspec:VNx2DI [
                (subreg:VNx2BI (reg/v:VNx16BI 104 [ pg ]) 0)
                (plus:VNx2DI (mult:VNx2DI (reg/v:VNx2DI 97 [ v18 ])
                        (reg/v:VNx2DI 98 [ v19 ]))
                    (subreg:VNx2DI (reg/v:VNx4DI 103 [ result ]) 0))
                (const_vector:VNx2DI repeat [
                        (const_int 0 [0])
                    ])
            ] UNSPEC_SEL)) "/app/example.c":13:25 discrim 1 7465 {*cond_fmavnx2di_any}
     (nil))
(insn 39 37 40 4 (set (subreg:VNx2DI (reg/v:VNx4DI 103 [ result ]) [16, 16])
        (unspec:VNx2DI [
                (subreg:VNx2BI (reg/v:VNx16BI 104 [ pg ]) 0)
                (plus:VNx2DI (mult:VNx2DI (reg/v:VNx2DI 97 [ v18 ])
                        (reg/v:VNx2DI 98 [ v19 ]))
                    (subreg:VNx2DI (reg/v:VNx4DI 103 [ result ]) [16, 16]))
                (const_vector:VNx2DI repeat [
                        (const_int 0 [0])
                    ])
            ] UNSPEC_SEL)) "/app/example.c":14:25 discrim 1 7465 {*cond_fmavnx2di_any}
     (expr_list:REG_DEAD (reg/v:VNx2DI 98 [ v19 ])
        (expr_list:REG_DEAD (reg/v:VNx2DI 97 [ v18 ])
            (nil))))
```

[4] partial content of IRA dump info
```
Disposition:
    6:r96  l0    69   24:r97  l0    35   23:r98  l0    34    4:r99  l0     1
    3:r102 l0     0    2:r103 l0    32    1:r104 l0    68    5:r106 l0     1
    7:r107 l0     2    8:r108 l0     3    0:r109 l0     4   12:r110 l0    68
    9:r111 l0     0   14:r112 l0     1   13:r113 l0     2   11:r114 l0     3
   10:r115 l0     4
```

[5] partial source code of  process_alt_operands
```c++
/* lra-constraints.cc */
static bool
process_alt_operands (int only_alternative)
{
  for (nop = 0; nop < n_operands; nop++)
    {
      ...
      biggest_mode[nop] = GET_MODE (op);
      if (GET_CODE (op) == SUBREG)
        {
            /* !!! Here use reg instead of subreg's mode */
            biggest_mode[nop] = wider_subreg_mode (op);
            operand_reg[nop] = reg = SUBREG_REG (op);
        }
    }
  ...
  for (nalt = 0; nalt < n_alternatives; nalt++)
    {
      ...
      for (nop = 0; nop < early_clobbered_regs_num; nop++)
        {
          ...
          /* !!! Here set operand0 occupied 33 and 34, where:
                   biggest_mode[i] is VNx4DI
                   clobbered_hard_regno is 33 */
          add_to_hard_reg_set (&temp_set, biggest_mode[i], clobbered_hard_regno);
          ...
        }
    }
  ...
}
```


---


### compiler : `gcc`
### title : `Shrink Wrap missed opportunity`
### open_at : `2023-03-03T14:04:38Z`
### last_modified_date : `2023-06-27T13:19:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109009
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `unknown`
### severity : `normal`
### contents :
The following test case does not get shrink wrapped:

void bar (void);

long
foo (long i, long cond)
{
  if (cond)
    bar ();
  return i+1;
}

However, if we change the return statement from
   return i+1;
to
   return i;

then shrink wrapping kicks in.


---


### compiler : `gcc`
### title : `missed optimization in presence of __builtin_ctz`
### open_at : `2023-03-03T14:56:19Z`
### last_modified_date : `2023-04-24T01:35:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109011
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
in the following code foo does not vectorize, bar does.
clang vectorize foo using a pattern that invokes vplzcntd

(code made a bit complex to make vectorization "relevant") 

see https://godbolt.org/z/5fa1zbPeG

#include <cstdint>
uint32_t x[256];
uint32_t y[256];
uint32_t w[256];
uint32_t z[256];



void foo() {
  for (int i=0; i<256;i++) {
    auto p = x[i] ?  __builtin_ctz(x[i]) : y[i];
   z[i] = w[i]*p;
 }  
}


void bar() {
  for (int j=0; j<256;j+=8)
  for (int i=j; i<j+8;i++) {
   // auto p = x[i] ?  x[i] : y[i];
   auto p = x[i] ?  __builtin_ctz(x[i]) : y[i];
   z[i] = w[i]*p;
 }  
}


---


### compiler : `gcc`
### title : `Failure to optimize b + c -1`
### open_at : `2023-03-03T23:49:51Z`
### last_modified_date : `2023-03-04T00:01:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109019
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
Looks like a general RTL issue, I see this on POWER, RV64 and ARM64 (the
latter two on godbolt).


[tkoenig@gcc135 ~]$ cat c.c
long foo (long b, long c)
{
  return b + c - 1;
}
[tkoenig@gcc135 ~]$ gcc -O3 -S c.c
[tkoenig@gcc135 ~]$ cat c.s
        .file   "c.c"
        .machine power8
        .abiversion 2
        .section        ".text"
        .align 2
        .p2align 4,,15
        .globl foo
        .type   foo, @function
foo:
.LFB0:
        .cfi_startproc
        add 3,3,4
        addi 3,3,-1
        blr
        .long 0
        .byte 0,0,0,0,0,0,0,0
        .cfi_endproc
.LFE0:
        .size   foo,.-foo
        .ident  "GCC: (GNU) 13.0.1 20230215 (experimental)"
        .section        .note.GNU-stack,"",@progbits

This should be

        addi    3,4,-1
        ret


---


### compiler : `gcc`
### title : `Low performance of std::map constructor for already sorted data of std::pair<none-const key, value>`
### open_at : `2023-03-04T13:28:34Z`
### last_modified_date : `2023-03-06T17:38:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109022
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.2.0`
### severity : `normal`
### contents :
Performance of constructor accepting iterators to sorted data of std::pair<none-const key, value> is much lower then inserting same data for std::pair<const key, value> (matches value_type of std::map).

Here is benchmark showing the issue:
```
#include <sstream>
#include <map>
#include <iomanip>
#include <algorithm>
#include <random>

constexpr size_t DataSizeStart = 8 << 0;
constexpr size_t DataSizeStop = 8 << 3;

using TestData = std::vector<std::pair<std::string, int>>;
using TestDataConst = std::vector<std::pair<const std::string, int>>;

std::string makeStringFor(size_t x)
{
  std::ostringstream out;
  out << std::setfill('0') << std::setw(6) << x;
  return out.str();
}

TestData sortedData(size_t n)
{
  TestData r;
  r.reserve(n);
  size_t i = 0;
  std::generate_n(std::back_inserter(r), n, [&i]() {
    return std::pair{makeStringFor(++i), i};
  });
  return r;
}

TestData shuffledData(size_t n)
{
  auto data = sortedData(n);
  std::random_device rd;
  std::mt19937 g(rd());

  std::shuffle(data.begin(), data.end(), g);
  return data;
}

TestDataConst sortedConstData(size_t n)
{
  auto r = sortedData(n);
  return {r.begin(), r.end()};
}

TestDataConst shuffledConstData(size_t n)
{
  auto r = shuffledData(n);
  return {r.begin(), r.end()};
}

template<auto Data>
static void CreateMapInsert(benchmark::State& state) {
  auto n = state.range(0);
  auto data = Data(n);
  for (auto _ : state) {
    benchmark::DoNotOptimize(data);
    std::map<std::string, int> m;

    for (auto& p : data) {
      m.insert(m.end(), p);
    }
    benchmark::DoNotOptimize(m);
  }
}
BENCHMARK(CreateMapInsert<sortedData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);
// BENCHMARK(CreateMapInsert<shuffledData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);
BENCHMARK(CreateMapInsert<sortedConstData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);
// BENCHMARK(CreateMapInsert<shuffledConstData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);

template<auto Data>
static void CreateMapDirectly(benchmark::State& state) {
  auto n = state.range(0);
  auto data = Data(n);
  for (auto _ : state) {
    benchmark::DoNotOptimize(data);
    std::map<std::string, int> m{data.begin(), data.end()};
    benchmark::DoNotOptimize(m);
  }
}
BENCHMARK(CreateMapDirectly<sortedData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);
// BENCHMARK(CreateMapDirectly<shuffledData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);
BENCHMARK(CreateMapDirectly<sortedConstData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);
// BENCHMARK(CreateMapDirectly<shuffledConstData>)->RangeMultiplier(2)->Range(DataSizeStart, DataSizeStop);
```

Live demo gcc 12.2: https://quick-bench.com/q/H8lqugNgMG-sWQ6cKBsdB7EBRpU
Note that CreateMapInsert<sortedConstData> performs best since it feeds to constructos to iterators to std::pair<const std::string, in> while CreateMapInsert<sortedData> performs badly (looks like time complexity is worse) adn only diffrence is that it provides iterators to std::<std::string, int>.

Same issue is observed for clang 14.0 and libstdc++: https://quick-bench.com/q/6VA3vRdKmqPEYrvTnFFla4gxwXk
but it is not a problem on clang 14.0 and libc++: https://quick-bench.com/q/-S-lX3N8Y-8vL9CCl-5bW5jolWA

here is result for sing size input data (easier to read): https://quick-bench.com/q/kbDOPA8PNZFfrzUH_-70cJgApeE

Issue was observed when filing answer on https://stackoverflow.com/a/75535056/1387438


---


### compiler : `gcc`
### title : `fcmov will not be generated`
### open_at : `2023-03-05T11:11:01Z`
### last_modified_date : `2023-03-06T08:38:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109028
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
Hello,
very rarely fcmov instructions are generated (https://godbolt.org/z/qE6f76Gda)

thx
Gero

#include <cmath>
#include <array>
#include <numbers>

static constexpr size_t Size = 1024;

using float80_t = long double;
using float64_t = double;
using float32_t = float;

template <typename Type>
inline constexpr Type   foo(const Type x)   noexcept
{
    return (x > 42) ? std::numbers::pi_v<Type> : std::numbers::e_v<Type>;
}

template <typename Type>
inline constexpr Type   bar(const Type x)   noexcept
{
   return std::signbit(x) ? std::numbers::pi_v<Type> : 0;
}

template <typename Type>
inline constexpr Type   baz(const Type x)   noexcept
{
    return std::copysign(std::numbers::pi_v<Type>, x);
}

template <typename Container, typename Function>
inline constexpr void for_all(Container& cnt, Function&& f)	noexcept
{
    for (auto& val : cnt)
    {
        val = f(val);
    }
}

float80_t foo80(const float80_t x)   noexcept { return foo(x); }
float80_t bar80(const float80_t x)   noexcept { return bar(x); }
float80_t baz80(const float80_t x)   noexcept { return baz(x); }

void foos80(std::array<float80_t, Size>& cnt)   noexcept { for_all(cnt, foo<float80_t>); }
void bars80(std::array<float80_t, Size>& cnt)   noexcept { for_all(cnt, bar<float80_t>); }
void bazs80(std::array<float80_t, Size>& cnt)   noexcept { for_all(cnt, baz<float80_t>); }


---


### compiler : `gcc`
### title : `__builtin_signbit for 64bit fp does not vectorize`
### open_at : `2023-03-05T12:08:25Z`
### last_modified_date : `2023-03-31T07:19:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109029
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Hallo,
std::signbit(double) generiert sehr ineffizienten code und kann somit nicht vektorisiert werden (https://godbolt.org/z/se6Ea8bo9).

thx
Gero

-std=c++20 -march=x86-64-v3 -O3 -mno-vzeroupper

#include <cmath>
#include <array>
#include <numbers>
#include <algorithm>

static constexpr size_t Size = 1024;

using float80_t = long double;
using float64_t = double;
using float32_t = float;

template <typename Type>
inline constexpr bool   foo(const Type x)   noexcept
{
    return std::signbit(x);
}

template <typename Type>
inline constexpr Type   bar(const Type x)   noexcept
{
   return std::signbit(x) ? std::numbers::pi_v<Type> : 0;
}

template <typename Container, typename Function>
inline constexpr void for_all(Container& cnt, Function&& f)	noexcept
{
	std::transform(cnt.begin(), cnt.end(), cnt.begin(), f);
}

template <typename ContainerRes, typename ContainerArg, typename Function>
inline constexpr void for_all(ContainerRes& res, const ContainerArg& arg, Function&& f)	noexcept
{
	std::transform(arg.begin(), arg.end(), res.begin(), f);
}

float64_t foo64(const float64_t x)   noexcept { return foo(x); }
float32_t foo32(const float32_t x)   noexcept { return foo(x); }

float64_t bar64(const float64_t x)   noexcept { return bar(x); }
float32_t bar32(const float32_t x)   noexcept { return bar(x); }

void foos64(std::array<bool, Size>& res, const std::array<float64_t, Size>& arg)   noexcept { for_all(res, arg, foo<float64_t>); }
void foos32(std::array<bool, Size>& res, const std::array<float32_t, Size>& arg)   noexcept { for_all(res, arg, foo<float32_t>); }

void bars64(std::array<float64_t, Size>& cnt)   noexcept { for_all(cnt, bar<float64_t>); }
void bars32(std::array<float32_t, Size>& cnt)   noexcept { for_all(cnt, bar<float32_t>); }


---


### compiler : `gcc`
### title : `meaningless memory store on RISC-V and LoongArch`
### open_at : `2023-03-06T07:51:03Z`
### last_modified_date : `2023-10-07T20:32:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109035
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
On LoongArch, without -fPIE, the compiler stores a register with no reason:

$ cat t.c
int test(int x)
{
	char buf[128 << 10];
	return buf[x];
}
$ ./gcc/cc1 t.c -nostdinc  -O2 -fdump-rtl-all -o- 2>/dev/null | grep test: -A20
test:
.LFB0 = .
	lu12i.w	$r13,-135168>>12			# 0xfffffffffffdf000
	ori	$r13,$r13,4080
	add.d	$r3,$r3,$r13
.LCFI0 = .
	lu12i.w	$r12,-131072>>12			# 0xfffffffffffe0000
	lu12i.w	$r13,131072>>12			# 0x20000
	add.d	$r13,$r13,$r12
	addi.d	$r12,$r3,16
	add.d	$r12,$r13,$r12
	lu12i.w	$r13,131072>>12			# 0x20000
	st.d	$r12,$r3,8
	ori	$r13,$r13,16
	ldx.b	$r4,$r12,$r4
	add.d	$r3,$r3,$r13
.LCFI1 = .
	jr	$r1
.LFE0:
	.size	test, .-test
	.section	.eh_frame,"aw",@progbits

Note the "st.d	$r12,$r3,8" instruction is completely meaningless.

The t.c.300r.ira dump contains some "interesting" thing:

Pass 0 for finding pseudo/allocno costs

    a0 (r87,l0) best GR_REGS, allocno GR_REGS
    a1 (r84,l0) best NO_REGS, allocno NO_REGS
    a2 (r83,l0) best GR_REGS, allocno GR_REGS

  a0(r87,l0) costs: SIBCALL_REGS:2000,2000 JIRL_REGS:2000,2000 CSR_REGS:2000,2000 GR_REGS:2000,2000 FP_REGS:8000,8000 ALL_REGS:32000,32000 MEM:8000,8000
  a1(r84,l0) costs: SIBCALL_REGS:1000000,1000000 JIRL_REGS:1000000,1000000 CSR_REGS:1000000,1000000 GR_REGS:1000000,1000000 FP_REGS:1004000,1004000 ALL_REGS:1016000,1016000 MEM:1004000,1004000
  a2(r83,l0) costs: SIBCALL_REGS:1000000,1000000 JIRL_REGS:1000000,1000000 CSR_REGS:1000000,1000000 GR_REGS:1000000,1000000 FP_REGS:1004000,1004000 ALL_REGS:1008000,1008000 MEM:1004000,1004000

Here r84 is the pseudo register for ($frame - 131072).  Any idea why the
compiler selects "NO_REGS" here?

On RISC-V there is also a meaningless "sd      a5,8(sp)" instruction:
https://godbolt.org/z/aPorqj73b


---


### compiler : `gcc`
### title : `Miss optimization to simplify bit_and + rotate to shift`
### open_at : `2023-03-06T09:55:49Z`
### last_modified_date : `2023-05-19T23:29:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109038
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
unsigned
foo (unsigned int a)
{
  unsigned int b = a & 0x00FFFFFF;
  unsigned int c = ((b & 0x000000FF) << 8
		    | (b & 0x0000FF00) << 8
		    | (b & 0x00FF0000) << 8
		    | (b & 0xFF000000) >> 24);
  return c;
}

gcc generates

foo:
        mov     eax, edi
        and     eax, 16777215
        rol     eax, 8
        ret

But it can be optimized as below

foo:                                    # @foo
        mov     eax, edi
        shl     eax, 8
        ret


---


### compiler : `gcc`
### title : `Missed fold for (n - 1) / 2 when n is odd`
### open_at : `2023-03-06T18:18:59Z`
### last_modified_date : `2023-03-07T09:10:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109044
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `enhancement`
### contents :
int foo(unsigned n) {
    if(n % 2 == 0) __builtin_unreachable();
    return (n - 1) / 2;
}

int bar(unsigned n) {
    return n >> 1;
}



foo(unsigned int):
        lea     eax, [rdi-1]
        shr     eax
        ret
bar(unsigned int):
        mov     eax, edi
        shr     eax
        ret


https://godbolt.org/z/3G7enYdnM

https://alive2.llvm.org/ce/z/m3qbdN


---


### compiler : `gcc`
### title : `assume attribute does not always optimize std::optional cases`
### open_at : `2023-03-06T19:32:14Z`
### last_modified_date : `2023-03-13T18:58:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109045
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
The assume attribute is meant to help expressing more complex assumptions which involve function calls.  Given that interfaces should use std::optional when the semantics matches this should mean code like this should be optimized:

#include <optional>

std::optional<long> g(long);

long f(long a)
{
  auto r = g(a);
  [[assume(!r || *r > 0)]];
  return r.value_or(0) / 2;
}


The generated code should use the unsigned divide by two method but it does not.  With today's gcc trunk version:

0000000000000000 <_Z1fl>:
   0:	48 83 ec 18          	sub    $0x18,%rsp
   4:	e8 00 00 00 00       	call   9 <_Z1fl+0x9>
   9:	48 89 04 24          	mov    %rax,(%rsp)
   d:	48 89 54 24 08       	mov    %rdx,0x8(%rsp)
  12:	31 c0                	xor    %eax,%eax
  14:	80 7c 24 08 00       	cmpb   $0x0,0x8(%rsp)
  19:	74 11                	je     2c <_Z1fl+0x2c>
  1b:	48 8b 14 24          	mov    (%rsp),%rdx
  1f:	48 89 d0             	mov    %rdx,%rax
  22:	48 c1 e8 3f          	shr    $0x3f,%rax
  26:	48 01 d0             	add    %rdx,%rax
  29:	48 d1 f8             	sar    %rax
  2c:	48 83 c4 18          	add    $0x18,%rsp
  30:	c3                   	ret    

The instructions from 1f to 28 including are not needed (and the initial load at 1b adjusted).


---


### compiler : `gcc`
### title : `[13 regression] redundant mask compare generated by vectorizer.`
### open_at : `2023-03-07T06:01:35Z`
### last_modified_date : `2023-04-13T08:14:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109048
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
#include <math.h>

void tmp2 (float *af, int type, int type2, float *res)
{
    const int Etype = (type == 1 || type2 == 2);
    const float f1 = (type == 3 || type2 == 4) ? 4.f : 2.f;
    const float f2 = (type == 3 || type2 == 4) ? 0.25f : 0.5f;

    for (int i = 0; i < 256; i++)
    {
        float x = af[i];
        int z = (x < 0.f);
        float t1 = (z ? 1.f : f2) + (x < f1 ? 1.f : 0.f);
        float neg_t1 = -fabsf(t1);
        float t2 = Etype ? neg_t1 : t1;
        res[i] += t2 + x;
    }
}

gcc trunk now generates


<bb 58> [local count: 5368707]:
  vect_cst__110 = {iftmp.0_34, iftmp.0_34, iftmp.0_34, iftmp.0_34, iftmp.0_34, iftmp.0_34, iftmp.0_34, iftmp.0_34};
  vect_cst__119 = {prephitmp_41, prephitmp_41, prephitmp_41, prephitmp_41, prephitmp_41, prephitmp_41, prephitmp_41, prephitmp_41};
  vect_cst__123 = {iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16};

  <bb 17> [local count: 53687070]:
  # i_18 = PHI <i_47(26), 0(58)>
  # ivtmp_15 = PHI <ivtmp_43(26), 256(58)>
  # vectp_af.11_105 = PHI <vectp_af.11_106(26), af_24(D)(58)>
  # vectp_res.23_125 = PHI <vectp_res.23_126(26), res_28(D)(58)>
  # vectp_res.28_130 = PHI <vectp_res.28_131(26), res_28(D)(58)>
  # ivtmp_133 = PHI <ivtmp_134(26), 0(58)>
  # DEBUG i => NULL
  # DEBUG BEGIN_STMT
  _38 = (long unsigned int) i_18;
  _37 = _38 * 4;
  _36 = af_24(D) + _37;
  vect_x_20.13_107 = MEM <vector(8) float> [(float *)vectp_af.11_105];
  x_20 = *_36;
  # DEBUG x => NULL
  # DEBUG BEGIN_STMT
  # DEBUG D#1 => NULL
  # DEBUG z => NULL
  # DEBUG BEGIN_STMT
  mask__50.14_109 = vect_x_20.13_107 >= { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
  _50 = x_20 >= 0.0;
  mask__52.15_111 = vect_x_20.13_107 < vect_cst__110;
  _52 = x_20 < iftmp.0_34;
  mask__53.16_112 = mask__50.14_109 & mask__52.15_111;
  _53 = _50 & _52;
  mask__55.17_114 = vect_x_20.13_107 >= vect_cst__110;
  _55 = x_20 >= iftmp.0_34;
  mask__56.18_115 = mask__50.14_109 & mask__55.17_114;
  _56 = _50 & _55;
  mask__74.19_117 = vect_x_20.13_107 < { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
  _74 = x_20 < 0.0;
  vect__ifc__73.20_120 = VEC_COND_EXPR <mask__74.19_117, { 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0 }, vect_cst__119>;
  _ifc__73 = _74 ? 2.0e+0 : prephitmp_41;
  _174 = ~mask__53.16_112;
  _175 = mask__74.19_117 & _174;
  vect__ifc__75.21_122 = VEC_COND_EXPR <_175, { 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0, 2.0e+0 }, vect_cst__119>;
  _ifc__75 = _53 ? prephitmp_41 : _ifc__73;
  vect_prephitmp_17.22_124 = VEC_COND_EXPR <mask__56.18_115, vect_cst__123, vect__ifc__75.21_122>;
  prephitmp_17 = _56 ? iftmp.1_16 : _ifc__75;
  # DEBUG t1 => D#2
  # DEBUG BEGIN_STMT
  # DEBUG neg_t1 => -D#2
  # DEBUG BEGIN_STMT
  # DEBUG t2 => prephitmp_17
  # DEBUG BEGIN_STMT
  _12 = res_28(D) + _37;
  vect__26.25_127 = MEM <vector(8) float> [(float *)vectp_res.23_125];
  _26 = *_12;
  vect__27.26_128 = vect__26.25_127 + vect_x_20.13_107;
  _27 = _26 + x_20;
  vect__45.27_129 = vect_prephitmp_17.22_124 + vect__27.26_128;
  _45 = prephitmp_17 + _27;
  MEM <vector(8) float> [(float *)vectp_res.28_130] = vect__45.27_129;
  # DEBUG BEGIN_STMT
  i_47 = i_18 + 1;
  # DEBUG i => i_47
  # DEBUG BEGIN_STMT
  ivtmp_43 = ivtmp_15 - 1;
  vectp_af.11_106 = vectp_af.11_105 + 32;
  vectp_res.23_126 = vectp_res.23_125 + 32;
  vectp_res.28_131 = vectp_res.28_130 + 32;
  ivtmp_134 = ivtmp_133 + 1;
  if (ivtmp_134 < 32)
    goto <bb 26>; [90.00%]
  else
    goto <bb 56>; [10.00%]

vs gcc12.2 


<bb 57> [local count: 5368707]:
  vect_cst__128 = {iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16, iftmp.1_16};
  vect_cst__134 = {iftmp.0_33, iftmp.0_33, iftmp.0_33, iftmp.0_33, iftmp.0_33, iftmp.0_33, iftmp.0_33, iftmp.0_33};

  <bb 5> [local count: 53687070]:
  # i_15 = PHI <i_30(13), 0(57)>
  # ivtmp_20 = PHI <ivtmp_49(13), 256(57)>
  # vectp_af.24_124 = PHI <vectp_af.24_125(13), af_24(D)(57)>
  # vectp_res.31_138 = PHI <vectp_res.31_139(13), res_28(D)(57)>
  # vectp_res.36_143 = PHI <vectp_res.36_144(13), res_28(D)(57)>
  # ivtmp_146 = PHI <ivtmp_147(13), 0(57)>
  # DEBUG i => NULL
  # DEBUG BEGIN_STMT
  _7 = (long unsigned int) i_15;
  _8 = _7 * 4;
  _9 = af_24(D) + _8;
  vect_x_25.26_126 = MEM <vector(8) float> [(float *)vectp_af.24_124];
  x_25 = *_9;
  # DEBUG x => NULL
  # DEBUG BEGIN_STMT
  # DEBUG D#1 => NULL
  # DEBUG z => NULL
  # DEBUG BEGIN_STMT
  _130 = vect_x_25.26_126 >= { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
  vect_iftmp.27_131 = VEC_COND_EXPR <_130, vect_cst__128, { 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0 }>;
  iftmp.2_17 = x_25 >= 0.0 ? iftmp.1_16 : 1.0e+0;
  vect__41.28_133 = vect_iftmp.27_131 + { 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0, 1.0e+0 };
  _41 = iftmp.2_17 + 1.0e+0;
  _135 = vect_x_25.26_126 >= vect_cst__134;
  vect_prephitmp_42.29_136 = VEC_COND_EXPR <_135, vect_iftmp.27_131, vect__41.28_133>;
  prephitmp_42 = x_25 >= iftmp.0_33 ? iftmp.2_17 : _41;
  # DEBUG t1 => NULL
  # DEBUG BEGIN_STMT
  # DEBUG neg_t1 => -prephitmp_42
  # DEBUG BEGIN_STMT
  vect_neg_t1_27.30_137 = -vect_prephitmp_42.29_136;
  neg_t1_27 = -prephitmp_42;
  # DEBUG t2 => neg_t1_27
  # DEBUG BEGIN_STMT
  _10 = res_28(D) + _8;
  vect__11.33_140 = MEM <vector(8) float> [(float *)vectp_res.31_138];
  _11 = *_10;
  vect__35.34_141 = vect__11.33_140 + vect_x_25.26_126;
  _35 = _11 + x_25;
  vect__13.35_142 = vect_neg_t1_27.30_137 + vect__35.34_141;
  _13 = neg_t1_27 + _35;
  MEM <vector(8) float> [(float *)vectp_res.36_143] = vect__13.35_142;
  # DEBUG BEGIN_STMT
  i_30 = i_15 + 1;
  # DEBUG i => i_30
  # DEBUG BEGIN_STMT
  ivtmp_49 = ivtmp_20 - 1;
  vectp_af.24_125 = vectp_af.24_124 + 32;
  vectp_res.31_139 = vectp_res.31_138 + 32;
  vectp_res.36_144 = vectp_res.36_143 + 32;
  ivtmp_147 = ivtmp_146 + 1;
  if (ivtmp_147 < 32)


---


### compiler : `gcc`
### title : `[missed optimization] value-range tracking fails in simple case with __builtin_unreachable`
### open_at : `2023-03-07T14:14:33Z`
### last_modified_date : `2023-03-07T22:14:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109053
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
I'm trying to use __builtin_unreachable() to inject assumptions about values of variables into this code. In this case, the fact that a reference count must be one or greater.

Consider the code:



struct refcounted {
    int* p;
    refcounted() : p(new int(1)) {}
    ~refcounted() {
        assume_stuff();
        if (!--*p) {
            delete p;
        }
    }
    refcounted(const refcounted& x) : p(x.p) {
        assume_stuff();
        ++*p;
        assume_stuff();
    }
    refcounted& operator=(const refcounted& x) {
        assume_stuff();
        x.assume_stuff();
        if (this != &x) {
            ++*x.p;
            if (!--*p) {
                delete p;
            }
            p = x.p;
        }
        assume_stuff();
        x.assume_stuff();
        return *this;
    }

    void assume_stuff() const {
        if (*p <= 0) {
            __builtin_unreachable();
        }
    }
};

refcounted assign(refcounted& a, refcounted& b) {
    auto x = a;
    a = b;
    return x;
}


In the assign() function, although we assign to `a`, we also return it (as `x`), so there's never a reason to call operator delete. Yet the code does.

assign(refcounted&, refcounted&):
 mov    %rdi,%rax
 mov    (%rsi),%rdi
 mov    %rdi,(%rax)
 addl   $0x1,(%rdi)

; gcc now knows that (%rdi) is 2 or greater

 cmp    %rdx,%rsi
 je     68 <assign(refcounted&, refcounted&)+0x68>
 push   %rbp
 mov    %rdx,%rbp
 push   %rbx
 mov    %rsi,%rbx
 sub    $0x18,%rsp
 mov    (%rdx),%rcx
 addl   $0x1,(%rcx)
 mov    (%rdi),%edx
 sub    $0x1,%edx

; gcc now knows that (%rdi) is 1 or greater

 je     40 <assign(refcounted&, refcounted&)+0x40>


; so how can it be zero? 
; if gcc tracked the ranges correctly, it would have eliminated the branch and made assign() a leaf function

 mov    %edx,(%rdi)
 mov    %rcx,(%rbx)
 add    $0x18,%rsp
 pop    %rbx
 pop    %rbp
 ret    
 cs nopw 0x0(%rax,%rax,1)
 mov    $0x4,%esi
 mov    %rax,0x8(%rsp)
 call   4f <assign(refcounted&, refcounted&)+0x4f>
    R_X86_64_PLT32 operator delete(void*, unsigned long)-0x4
 mov    0x0(%rbp),%rcx
 mov    0x8(%rsp),%rax
 mov    %rcx,(%rbx)
 add    $0x18,%rsp
 pop    %rbx
 pop    %rbp
 ret    
 nopw   0x0(%rax,%rax,1)
 ret    


Also on: https://godbolt.org/z/Tnehj86hc


---


### compiler : `gcc`
### title : `[12 Regression] SLP costs for vec duplicate too high since g:4963079769c99c4073adfd799885410ad484cbbe`
### open_at : `2023-03-08T22:32:22Z`
### last_modified_date : `2023-04-03T09:03:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109072
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.0`
### severity : `normal`
### contents :
The following example

---

#include <arm_neon.h>

float32x4_t f (float32x4_t v, float res)
{
  float data[4];
  data[0] = res;
  data[1] = res;
  data[2] = res;
  data[3] = res;
  return vld1q_f32 (&data[0]);
}

---

compiled with -Ofast fails to SLP starting with GCC 12.

This used to generate:

f:
        dup     v0.4s, v1.s[0]
        ret

and now generates:

f:
        fmov    w5, s1
        fmov    w1, s1
        fmov    w4, s1
        fmov    w0, s1
        mov     x2, 0
        mov     x3, 0
        bfi     x2, x5, 0, 32
        bfi     x3, x1, 0, 32
        bfi     x2, x4, 32, 32
        bfi     x3, x0, 32, 32
        fmov    d0, x2
        ins     v0.d[1], x3
        ret

The SLP costs went from:

  Vector cost: 2
  Scalar cost: 4

to:

  Vector cost: 12
  Scalar cost: 4

it looks like it's no longer costing it as a duplicate but instead 4 vec inserts.

bisected to:

commit g:4963079769c99c4073adfd799885410ad484cbbe
Author: Richard Sandiford <richard.sandiford@arm.com>
Date:   Tue Feb 15 18:09:33 2022 +0000

    vect+aarch64: Fix ldp_stp_* regressions

    ldp_stp_1.c, ldp_stp_4.c and ldp_stp_5.c have been failing since
    vectorisation was enabled at -O2.  In all three cases SLP is
    generating vector code when scalar code would be better.

....


---


### compiler : `gcc`
### title : `Missing optimization on aarch64 for types like `float32x4x2_t``
### open_at : `2023-03-09T10:21:09Z`
### last_modified_date : `2023-04-02T17:04:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109078
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.2.0`
### severity : `enhancement`
### contents :
Here is a simple code: https://godbolt.org/z/3qMTTfcfx

#include <arm_neon.h>
#include <stddef.h>
#include <stdbool.h>

void simple_gemm(
  float* restrict out,
  float const* restrict a,
  float const* restrict b,
  size_t k, bool zero_out
) {
  register float32x4x2_t o0;
  o0.val[0] = vdupq_n_f32(0.0f);
  o0.val[1] = vdupq_n_f32(0.0f);

  // begin dot
  {
    register float32x4_t a0;
    register float32x4x2_t b0;

    while (k >= 1) {
      b0 = vld1q_f32_x2(b);
      a0 = vdupq_n_f32(a[0]);

      o0.val[0] = vfmaq_f32(o0.val[0], a0, b0.val[0]);
      o0.val[1] = vfmaq_f32(o0.val[1], a0, b0.val[1]);

      b += 8;
      a += 1;
      k -= 1;
    }
  } // end dot

  // begin writeback
  {
    if (!zero_out) {
      register float32x4x2_t t0;
      t0 = vld1q_f32_x2(out);
      
      o0.val[0] = vaddq_f32(o0.val[0], t0.val[0]);
      o0.val[1] = vaddq_f32(o0.val[1], t0.val[1]);
    }

    // TODO: both clang and gcc generates redundant mov because of bad register allocation.
    vst1q_f32_x2(out, o0);
  } // end writeback
}


The assembly generated:

simple_gemm:
        movi    v3.4s, 0
        and     w4, w4, 255
        mov     v4.16b, v3.16b
        cbz     x3, .L2
.L3:
        ld1     {v0.4s - v1.4s}, [x2], 32
        subs    x3, x3, #1
        ld1r    {v2.4s}, [x1], 4
        fmla    v3.4s, v2.4s, v0.4s
        fmla    v4.4s, v2.4s, v1.4s
        bne     .L3
.L2:
        cbnz    w4, .L4
        ld1     {v0.4s - v1.4s}, [x0]
        fadd    v3.4s, v3.4s, v0.4s
        fadd    v4.4s, v4.4s, v1.4s
.L4:
        mov     v0.16b, v3.16b
        mov     v1.16b, v4.16b
        st1     {v0.4s - v1.4s}, [x0]
        ret

The two values of float32x4x2_t o0 are assigned to v3 and v4. They should be able to be used directly as operands of st1, so the mov at L4 is redundant.  

I also found that in some code, the register pair may not be neighboring, which results in some redundant mov instructions.


---


### compiler : `gcc`
### title : `Missing optimization for x86 avx intrinsic _mm256_zeroall().`
### open_at : `2023-03-09T10:26:20Z`
### last_modified_date : `2023-03-10T01:42:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109079
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.2.0`
### severity : `enhancement`
### contents :
Here is a simple code: https://godbolt.org/z/q9o5rf4dM

#include <immintrin.h>

void fn(float *out) {
    _mm256_zeroall();
    register __m256 r0;
    r0 = _mm256_setzero_ps();
    _mm256_storeu_ps(out, r0);
}

which is compiled into

fn:
        vzeroall
        vxorps  xmm0, xmm0, xmm0
        vmovups YMMWORD PTR [rdi], ymm0
        vzeroupper
        ret

There are both vzeroall and vxorps instructions in the code, but only one is needed.
In my specific use case (matrix product), I want to initialize multiple registers using vzeroall with _mm256_zeroall() to reduce code size and prevent uninitialized variable warnings by setting all register variables as _mm256_setzero_ps().
The missing optimization makes the leading _mm256_zeroall() instruction useless.


---


### compiler : `gcc`
### title : `GCC does not always vectorize conditional reduction`
### open_at : `2023-03-10T09:24:32Z`
### last_modified_date : `2023-10-06T15:53:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109088
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Such case, LLVM succeed in auto-vectorization but GCC fail.

https://godbolt.org/z/1x68jo36d


---


### compiler : `gcc`
### title : `[[assume(...)]] is not taken into account for structs`
### open_at : `2023-03-13T18:49:22Z`
### last_modified_date : `2023-07-14T12:48:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109112
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Everything is built with flags: -std=c++23 -O3

[[assume(...)]] doesn't seem to work quite as well when dealing with member variables.

First version of `fn`:

void do_something();
void fn(bool x){
    [[assume(!x)]];
    if (x) do_something();
}

`fn` compiles into nothing:
fn(bool):
  ret

Second version of `fn`, wrapping the `bool` argument in a simple struct:

struct S { bool x; };
void do_something();
void fn(S s){
    [[assume(!s.x)]];
    if (s.x) do_something();
}

This no longer compiles into just `ret`:
fn(S):
        test    dil, dil
        jne     .L5
        ret
.L5:
        jmp     do_something()

Expected behaviour was for `fn(S)` to reduce to just `ret`

Godbolt link with the examples: https://godbolt.org/z/nreM4Y6dW


---


### compiler : `gcc`
### title : `vector_pair register allocation bug`
### open_at : `2023-03-13T21:42:25Z`
### last_modified_date : `2023-03-15T16:25:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109116
### status : `ASSIGNED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
There seems to be a bug in the register allocator when using a __vector_pair.  GCC didn't choose a register for the load that served the later instruction.

With this testcase

```
#include <altivec.h>

#if !__has_builtin(__builtin_vsx_disassemble_pair)
#define __builtin_vsx_disassemble_pair __builtin_mma_disassemble_pair
#endif

int main() {
  float A[8] = { float(1), float(2), float(3), float(4),
                 float(5), float(6), float(7), float(8) };
  __vector_pair P;
  __vector_quad Q;
  vector float B, C[2], D[4];

  __builtin_mma_xxsetaccz(&Q);
  P = *reinterpret_cast<__vector_pair *>(A);
  B = *reinterpret_cast<vector float *>(A);
  __builtin_vsx_disassemble_pair((void*)(C), &P);
  __builtin_mma_xvf32gerpp(&Q, reinterpret_cast<__vector unsigned char>(C[0]), reinterpret_cast<__vector unsigned char>(B));
  __builtin_mma_xvf32gerpp(&Q, reinterpret_cast<__vector unsigned char>(C[1]), reinterpret_cast<__vector unsigned char>(B));
  __builtin_mma_disassemble_acc((void *)D, &Q);

  return int(D[0][0]);
}
```

It produces an output with extra (unneeded) register moves.

```
    plxvp 12,.LANCHOR0@pcrel
    xxsetaccz 0
    plxv 33,.LC1@pcrel
    xxlor 45,13,13
    xxlor 32,12,12
    xvf32gerpp 0,45,33
    xvf32gerpp 0,32,33
    xxmfacc 0
```


---


### compiler : `gcc`
### title : `missing loading fre with branch comparing two pointers`
### open_at : `2023-03-14T04:58:30Z`
### last_modified_date : `2023-03-20T16:49:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109119
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
```
void fun (unsigned *x, unsigned *b) {
   asm volatile(""::"r"(*x));
  unsigned *a = b;//malloc (*x);
  if (a == 0)
    return;
  if (a != x)            // (A)
    *a = *x;
  *x = *a;
}
```
GCC currently produces:
  <bb 2> [local count: 1073741824]:
  _1 = *x_6(D);
  __asm__ __volatile__("" :  : "r" _1);
  if (b_7(D) == 0B)
    goto <bb 7>; [18.09%]
  else
    goto <bb 3>; [81.91%]

  <bb 3> [local count: 879501929]:
  if (x_6(D) != b_7(D))
    goto <bb 5>; [70.00%]
  else
    goto <bb 4>; [30.00%]

  <bb 4> [local count: 263850576]:
  pretmp_10 = *b_7(D);
  goto <bb 6>; [100.00%]

  <bb 5> [local count: 615651353]:
  *b_7(D) = _1;

  <bb 6> [local count: 879501929]:
  # prephitmp_11 = PHI <pretmp_10(4), _1(5)>
  *x_6(D) = prephitmp_11;

But pretmp_10 should be the same as _1 as at that point b_7 == x_6.

Note I noticed this while looking into PR 96564.


---


### compiler : `gcc`
### title : `[optimization] More advanced constexpr value compile time evaluation`
### open_at : `2023-03-14T11:10:50Z`
### last_modified_date : `2023-03-14T14:45:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109127
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Hello,

I'd like to report the idea which could improve the application performance.

The idea is related to `constexpr` math, which can be performed at compile time. At some degree C++ compiler manages to perform the optimization. But in my more real example for some reason it does not perform that kind of optimization.

Let's start with the simple example which explains the idea and which works. Following function serializes the `constexpr` unsigned into the string. It does not work right, as an output is reversed, but we will get into it later.

```cpp
// The expected output is "543\0"
void foo1(char* ptr)
{
    constexpr unsigned Tag = 345;

    auto v = Tag;

    do
    {
        *ptr++ = (v % 10) + '0';
        v /= 10;
    }
    while(v);

    *ptr = 0;
}
```


The produced assembly is as following:


```asm
foo1(char*):
        mov     eax, DWORD PTR .LC0[rip]
        mov     DWORD PTR [rdi], eax
        ret

.LC0:
        .byte   53
        .byte   52
        .byte   51
        .byte   0
```

It is good enough. I would replace the reading from the memory `.LC0` with the hardcoded unsigned integer though, so CPU does not have to access other memory locations:

```
        mov     eax, 0x35343300
        ; instead of
        mov     eax, DWORD PTR .LC0[rip]
```

Now, I change the code a bit to use 16-base math. That is an intermediate step before we go to the real code:

```cpp
void foo2(char* ptr)
{
    constexpr unsigned Tag = 0xF345;

    auto v = Tag;

    while(v != 0xF)
    {
        *ptr++ = (v % 16) + '0';
        v /= 16;
    }

    *ptr = 0;
}
```

The assembly is the same as above, which is good.

The thing which does not work is if I reverse the output bytes, then compiler does not perform the `constexpr` math in the compile time:


```cpp
void foo3(char* ptr)
{
    constexpr unsigned Tag = 0x345;

    // Convert 0x345 -> 0xF543
    auto v = Tag;
    auto reversed = 0xFu; // 0xF is a stop value
    while(v)
    {
        reversed <<= 4;
        reversed |= v & 0xFu;
        v >>= 4;
    }

    // Now serialize 0xF543 into "345\0"
    while(reversed != 0xF)
    {
        *ptr++ = (reversed % 16) + '0';
        reversed /= 16;
    }

    *ptr = 0;
}

```

The assembly output is following:

```asm
foo3(char*):
        mov     eax, 62277
.L2:
        mov     edx, eax
        add     rdi, 1
        shr     eax, 4
        and     edx, 15
        add     edx, 48
        mov     BYTE PTR [rdi-1], dl
        cmp     eax, 15
        jne     .L2
        mov     BYTE PTR [rdi], 0
        ret
```

In the assembly above there is a `.L2` loop, which could be calculated during the compilation.

The workaround is to force compiler to calculate the reversed unsigned and store it as constexpr:

```cpp
constexpr unsigned reverse(unsigned v)
{
    auto reversed = 0xFu;
    while(v)
    {
        reversed <<= 4;
        reversed |= v & 0xFu;
        v >>= 4;
    }

    return reversed;
}

void foo3(char* ptr)
{
    constexpr unsigned Tag = 0x543;
    constexpr unsigned ReversedTag = reverse(Tag);

    auto reversed = ReversedTag;
    while(reversed != 0xF)
    {
        *ptr++ = (reversed % 16) + '0';
        reversed /= 16;
    }

    *ptr = 0;
}

```

The assembly is back to normal:

```cpp
foo3(char*):
        mov     eax, DWORD PTR .LC0[rip]
        mov     DWORD PTR [rdi], eax
        ret
.LC0:
        .byte   53
        .byte   52
        .byte   51
        .byte   0
```


---


### compiler : `gcc`
### title : `[13/14 Regression] 464.h264ref regressed by 6.5% on a Neoverse-N1 CPU with PGO, LTO, -Ofast and -march=native`
### open_at : `2023-03-14T17:03:42Z`
### last_modified_date : `2023-07-27T09:25:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109130
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
LNT shows what looks like a clear recent 6.5% regression on the
464.h264ref benchmark from SPEC 2006 CPU suite when compiled with
PGO, LTO, -Ofast and -march=native on a Neoverse-N1 CPU:

  https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=586.220.0

In the same time period, I cannot (immediately) see any similar
regressions on any other CPU (all others that we monitor are x86_64)
or option combination.

Unfortunately I don't have the time and resources to further
investigate, but the commit range bad177e848787258..2fc55f51f9953b45
includes 8e26ac4749c (AArch64: Fix codegen regressions around tbz),
among other things.


---


### compiler : `gcc`
### title : `missed vector constructor optimizations`
### open_at : `2023-03-16T09:47:00Z`
### last_modified_date : `2023-03-16T10:24:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109153
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
The following example:

#include <arm_neon.h>
#include <string.h>

uint32x2_t foo(const uint8_t *buf, int stride) {
  uint32_t a0, a1;
  memcpy(&a0, buf, 4);
  memcpy(&a1, buf + stride, 4);
  uint32x2_t a_u32 = vdup_n_u32(a0);
  return vset_lane_u32(a1, a_u32, 1);
}

generates

foo:
        add     x1, x0, w1, sxtw
        ld1r    {v0.2s}, [x0]
        ld1     {v0.s}[1], [x1]
        ret

where the initial value is replicated to then be overwritten.

At the gimple level we have:

  _9 = {_4, _4};
  __vec_10 = BIT_INSERT_EXPR <_9, _7, 32 (32 bits)>;

which should have been optimized to:

  _9 = {_4, _7 }

but this cannot be blindly done as the resulting vector needs to be
cheaper.

For instance if it was

  _9 = {_4, _4, _4, _4};
  __vec_10 = BIT_INSERT_EXPR <_9, _7, 32 (32 bits)>;

this wouldn't have been cheaper.

Similarly another testcase gives

  <bb 2> [local count: 1073741824]:
  _4 = VEC_PERM_EXPR <a_2(D), b_3(D), { 0, 8, 1, 9, 2, 10, 3, 11 }>;
  _5 = VEC_PERM_EXPR <a_2(D), b_3(D), { 4, 12, 5, 13, 6, 14, 7, 15 }>;
  _6 = {_4, _5};
  return _6;

which should have been just a singe VEC_PERM_EXPR.


---


### compiler : `gcc`
### title : `[13/14 regression] jump threading de-optimizes nested floating point comparisons`
### open_at : `2023-03-16T11:57:47Z`
### last_modified_date : `2023-10-18T08:55:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109154
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 54681
Reduced microbude test case

We're observing a significant performance drop (~30%) in an application when comparing gcc trunk against gcc 12, observed with -mcpu=neoverse-v1 on an aarch64 Neoverse-V1. With OMP_NUM_THREADS=1 we see a regression of nearly 60% between gcc12 and gcc13. The test case attached is reduced from a test shared here https://github.com/UoB-HPC/microBUDE and has been made more suitable for a gcc bug report.

$ install-gcc-12/bin/g++ --version
g++ (GCC) 12.2.1 20221222 [master r13-4850-g74544bdadc4]
Copyright (C) 2022 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 
$ install-gcc-trunk/bin/g++ --version
g++ (GCC) 13.0.1 20230315 (experimental)
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

Command line used : 
 
$ install-gcc-12/bin/g++ -std=c++17 -Wall -Wno-sign-compare -Wno-unused-variable -Ofast -mcpu=neoverse-v1 -fopenmp -g3 reduced_microbude.cpp -o microbude-12-neoverse-v1
$ ./microbude-12-neoverse-v1

$ install-gcc-trunk/bin/g++ -std=c++17 -Wall -Wno-sign-compare -Wno-unused-variable -Ofast -mcpu=neoverse-v1  -fopenmp -g3 reduced_microbude.cpp -o microbude-trunk-neoverse-v1
 
Bisecting suggests that commit https://gcc.gnu.org/git/gitweb.cgi?p=gcc.git;h=4fbe3e6a could be a possible candidate. Thank you Tom Lin for the help here with the bisection. 
 
Thanks


---


### compiler : `gcc`
### title : `Support Absolute Difference detection in GCC`
### open_at : `2023-03-16T13:09:33Z`
### last_modified_date : `2023-07-14T11:02:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109156
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Today we support Sum of Absolute differences

#include <stdlib.h>
 
#define TYPE_IN signed char
#define TYPE_OUT signed int

TYPE_OUT SABD_example (int n, TYPE_IN *restrict a, TYPE_IN *restrict b)
{
  TYPE_OUT out = 0;
  for (int i = 0; i < n; i++)
    out += abs(b[i] - a[i]);
  return out;
}

which is implemented through the SAD_EXPR tree code.

The goal is to support absolute difference ABD and widening absolute difference (no reduction).

#include <stdlib.h>
 
#define TYPE_IN signed int
#define TYPE_OUT signed int
 
void ABD_example (int n, TYPE_IN *restrict a, TYPE_IN *restrict b, TYPE_OUT *restrict out)
{
  for (int i = 0; i < n; i++)
    out[i] = abs(b[i] - a[i]);
}


This code shares 90% of the work with the vect_recog_sad_pattern expression with one difference, the SAD expression starts at a reduction, the ADB expressions start at an optional cast but otherwise from the abs.

There are two ways we're thinking of implementing ABD, (ABDL we can't do at the moment because we can't do widening in IFNs).

1. refactor the SAD detection code such that the body of the code that detects ABD is refactored out and can be used by a new pattern.  This has the down side of us having to do duplicate work in patterns that may match this.  In general this doesn't happen often because SAD stops at the + and should be OK if ADB matches after SAD. though cast_forwardprop may make this hard to do.

2. It looks like all targets that implement SAD do so with an instruction that does ABD and then perform a reduction.  So it looks like no target has the semantics for SAD.

So this brings up the question of why the detection wasn't done based on ABD instead and leaving the reduction explicit in the vectorizer.

So question is, should we create a completely new standalone pattern for ABD or should be make ABD the thing being detected and change SAD_EXPR to recognize ADB + reduction.

Removing SAD completely in favor of ABD + reduction means that hand optimized versions in targets need updating so I'm in favor of still emitting SAD.


---


### compiler : `gcc`
### title : `vector.resize( v.size() + 100 ) does unnecessary comparison`
### open_at : `2023-03-20T08:26:47Z`
### last_modified_date : `2023-06-01T10:56:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109205
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.2.0`
### severity : `enhancement`
### contents :
This function:

#include <vector>

void testResize( std::vector<char> & v )
{
  v.resize( v.size() + 100 );
}


Will compile (-O2) to something like this:

        mov     rcx, QWORD PTR [rdi+8]
        mov     rax, QWORD PTR [rdi]
        mov     rdx, rcx
        sub     rdx, rax
        lea     rsi, [rdx+100]
        cmp     rdx, rsi
        jb      .L39
        add     rax, rsi
        cmp     rcx, rax
        je      .L36
        mov     QWORD PTR [rdi+8], rax
.L36:
        ret
.L39:
        mov     esi, 100
        jmp     std::vector<char, std::allocator<char> >::_M_default_append(unsigned long)

The call to _M_default_append is guarded by a cmp.  This seems unnecessary, as the argument passed to resize is always bigger than size.  

Doing the addition with a signed type does not change the result.

See: https://godbolt.org/z/xY77fsz4z


---


### compiler : `gcc`
### title : `[13/14 Regression] -Os generates significantly more code since r13-723`
### open_at : `2023-03-20T11:53:49Z`
### last_modified_date : `2023-07-27T09:25:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109213
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Input C-code:

static char g, d[5] = {0};
static int f = 0, *e;
static void b(int) {};

static int i() {
    for (int j =0; j < 1; j +=1)
        f = 0;
    return 0;
}

static void m() {
    int n[10];
    int o[56] = {};
    for (int h = 0; h < 3; h++)
        if (d[h])
            for (; f; f++)
                if (n[0])
                    break;
                    
    int *r[] = {&o[0]};
}


int main() {
    int p[10] = {};
    m();
    if (!i()) {
        int *q = &p[0];
        for (; g;)
            b(e == q);
    }
}

-----------------------------------------------------
12.2 -Os:

main:
        xorl    %eax, %eax
        movl    %eax, f(%rip)
        xorl    %eax, %eax
        ret

-----------------------------------------------------
trunk -Os: 

m:
        movl    f(%rip), %eax
        xorl    %edx, %edx
        xorl    %ecx, %ecx
.L5:
        cmpb    $0, d(%rdx)
        je      .L2
        movl    -40(%rsp), %esi
.L3:
        testl   %eax, %eax
        je      .L2
        testl   %esi, %esi
        jne     .L2
        incl    %eax
        movb    $1, %cl
        jmp     .L3
.L2:
        incq    %rdx
        cmpq    $3, %rdx
        jne     .L5
        testb   %cl, %cl
        je      .L1
        movl    %eax, f(%rip)
.L1:
        ret
main:
        xorl    %eax, %eax
        call    m
        xorl    %eax, %eax
        movl    %eax, f(%rip)
        xorl    %eax, %eax
        ret
d:
        .zero   5


---


### compiler : `gcc`
### title : `Missed fneg/fsub optimization`
### open_at : `2023-03-21T19:25:49Z`
### last_modified_date : `2023-04-18T09:03:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109240
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
+++ This bug was initially created as a clone of Bug #109230 +++

On aarch64 we optimize at -O2 only half of the following routines:
typedef float V __attribute__((vector_size (4 * sizeof (float))));
typedef int VI __attribute__((vector_size (4 * sizeof (float))));

__attribute__((noipa)) V
foo (V x, V y)
{
  V a = x - y;
  V b = y + x;
  return __builtin_shuffle (b, a, (VI) { 0, 5, 2, 7 });
}

__attribute__((noipa)) V
bar (V x, V y)
{
  V a = x - y;
  V b = y + x;
  return __builtin_shuffle (a, b, (VI) { 4, 1, 6, 3 });
}

__attribute__((noipa)) V
baz (V x, V y)
{
  V a = x - y;
  V b = y + x;
  return __builtin_shuffle (b, a, (VI) { 4, 1, 6, 3 });
}

__attribute__((noipa)) V
qux (V x, V y)
{
  V a = x - y;
  V b = y + x;
  return __builtin_shuffle (a, b, (VI) { 0, 5, 2, 7 });
}

__attribute__((noipa)) V
boo (V x, V y)
{
  V a = x + y;
  V b = y - x;
  return __builtin_shuffle (b, a, (VI) { 0, 5, 2, 7 });
}

__attribute__((noipa)) V
corge (V x, V y)
{
  V a = x + y;
  V b = y - x;
  return __builtin_shuffle (a, b, (VI) { 4, 1, 6, 3 });
}

__attribute__((noipa)) V
fred (V x, V y)
{
  V a = x + y;
  V b = y - x;
  return __builtin_shuffle (b, a, (VI) { 4, 1, 6, 3 });
}

__attribute__((noipa)) V
garply (V x, V y)
{
  V a = x + y;
  V b = y - x;
  return __builtin_shuffle (a, b, (VI) { 0, 5, 2, 7 });
}

starting with r13-4024-gb2bb611d90d01f64a24 (plus r13-4122-g1bc7efa948f751 bugfix).
The other half could be handled similarly, just with fneg+fsub rather than fneg+fadd.

Unfortunately, match.pd canonicalizes those, we still have 0, 5, 2, 7 permutations for all of them, but the two operations swapped.  Unfortunately match.pd doesn't allow :c
on vec_perm, and if we use (for op (plus minus)
                                otherop (minus plus)
then we couldn't add :c to the plus one.  So, copy and paste the whole large simplification, swap (plus:c @0 @1) and (minus @0 @1) and replace (plus at the end with (minus?  Or handle the commutativity manually?
(for op (plus minus)
     otherop (minus plus)
 (simplify
  (vec_perm (op @0 @1) (otherop @2 @3) VECTOR_CST@4)
and use operand_equal_p manually to allow all forms we want?


---


### compiler : `gcc`
### title : `Missed optimization for 2-dimensional array with equal values accessed through Enums`
### open_at : `2023-03-22T11:36:22Z`
### last_modified_date : `2023-03-22T13:13:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109246
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Given the following code:

enum class E {
    A = 0,
    B = 1,
};

constexpr int t[2][2]{{1, 1}, {1, 1}};

int f1(E a) { return t[static_cast<int>(a)][static_cast<int>(a)]; }

int f2(E a, E b) { return t[static_cast<int>(a)][static_cast<int>(b)]; }

and compiling with '-O3 -fstrict-enums' gives:
- https://godbolt.org/z/s8jssnqq5

f1(E):
        movsx   rdi, edi
        lea     rax, [rdi+rdi*2]
        mov     eax, DWORD PTR t[0+rax*4]
        ret
f2(E, E):
        movsx   rsi, esi
        movsx   rdi, edi
        lea     rax, [rsi+rdi*2]
        mov     eax, DWORD PTR t[0+rax*4]
        ret
t:
        .long   1
        .long   1
        .long   1
        .long   1

gcc should be able to determine that in all cases both 'f1' and 'f2' can only ever return 1.

The same is happening for
3x3 - https://godbolt.org/z/7v7hfq357
4x4 - https://godbolt.org/z/fTejK8YKb
5x5 - https://godbolt.org/z/31GcKPG6c
arrays.

Also explicitly telling the compiler about the valid values for 'a' and 'b' using '__builtin_unreachable()' like this:

enum class E {
    A = 0,
    B = 1,
};

constexpr int t[2][2]{{1, 1}, {1, 1}};

int f1(E a) {
    if (a != E::A && a != E::B) {
        __builtin_unreachable();
    }

    return t[static_cast<int>(a)][static_cast<int>(a)];
}

int f2(E a, E b) {
    if (a != E::A && a != E::B) {
        __builtin_unreachable();
    }
    if (b != E::A && b != E::B) {
        __builtin_unreachable();
    }

    return t[static_cast<int>(a)][static_cast<int>(b)];
}

does not help - https://godbolt.org/z/6x7xKo1xj


---


### compiler : `gcc`
### title : `Missed optimization for table lookups`
### open_at : `2023-03-22T12:38:37Z`
### last_modified_date : `2023-03-22T12:55:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109249
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Example #1 - https://godbolt.org/z/95GjPo9bs

gcc:
GetTypeDamageMultiplier(Type, Type):
        sub     rsp, 1184
        movsx   rax, edi
        movsx   rdx, esi
        mov     ecx, 162
        lea     rdi, [rsp-120]
        mov     esi, OFFSET FLAT:.LC0
        lea     rax, [rax+rax*8]
        rep     movsq
        lea     rax, [rdx+rax*2]
        movss   xmm0, DWORD PTR [rsp-120+rax*4]
        add     rsp, 1184
        ret

clang:
GetTypeDamageMultiplier(Type, Type):     # @GetTypeDamageMultiplier(Type, Type)
        movsxd  rax, edi
        lea     rax, [rax + 8*rax]
        movsxd  rcx, esi
        lea     rax, [rcx + 2*rax]
        lea     rcx, [rip + .L__const.GetTypeDamageMultiplier(Type, Type).table]
        movss   xmm0, dword ptr [rcx + 4*rax]   # xmm0 = mem[0],zero,zero,zero
        ret

Example #2 - https://godbolt.org/z/sqocKjYco

gcc:
GetTypeDamageMultiplier(Type, Type):
        sub     rsp, 144
        movsx   rdi, edi
        movsx   rsi, esi
        movaps  xmm1, XMMWORD PTR .LC2[rip]
        movss   xmm0, DWORD PTR .LC1[rip]
        movaps  xmm2, XMMWORD PTR .LC6[rip]
        lea     rax, [rsi+rdi*8]
        movaps  XMMWORD PTR [rsp-72], xmm1
        movaps  xmm1, XMMWORD PTR .LC3[rip]
        shufps  xmm0, xmm0, 0
        movaps  XMMWORD PTR [rsp-120], xmm0
        movaps  XMMWORD PTR [rsp-104], xmm0
        movaps  XMMWORD PTR [rsp-88], xmm0
        movaps  XMMWORD PTR [rsp+8], xmm0
        movaps  xmm0, XMMWORD PTR .LC7[rip]
        movaps  XMMWORD PTR [rsp-56], xmm1
        movaps  xmm1, XMMWORD PTR .LC4[rip]
        movaps  XMMWORD PTR [rsp+24], xmm0
        movaps  xmm0, XMMWORD PTR .LC8[rip]
        movaps  XMMWORD PTR [rsp-40], xmm1
        movaps  xmm1, XMMWORD PTR .LC5[rip]
        movaps  XMMWORD PTR [rsp+40], xmm0
        movaps  xmm0, XMMWORD PTR .LC9[rip]
        movaps  XMMWORD PTR [rsp-24], xmm1
        movaps  XMMWORD PTR [rsp+56], xmm1
        movaps  xmm1, XMMWORD PTR .LC10[rip]
        movaps  XMMWORD PTR [rsp+72], xmm0
        movaps  XMMWORD PTR [rsp+104], xmm0
        movaps  xmm0, XMMWORD PTR .LC11[rip]
        movaps  XMMWORD PTR [rsp-8], xmm2
        movaps  XMMWORD PTR [rsp+88], xmm1
        movaps  XMMWORD PTR [rsp+120], xmm0
        movss   xmm0, DWORD PTR [rsp-120+rax*4]
        add     rsp, 144
        ret

clang:
GetTypeDamageMultiplier(Type, Type):     # @GetTypeDamageMultiplier(Type, Type)
        movsxd  rax, edi
        movsxd  rcx, esi
        lea     rax, [rcx + 8*rax]
        lea     rcx, [rip + .L__const.GetTypeDamageMultiplier(Type, Type).table]
        movss   xmm0, dword ptr [rcx + 4*rax]   # xmm0 = mem[0],zero,zero,zero
        ret


---


### compiler : `gcc`
### title : `rs6000:pass_analyze_swaps should preserve some rtx notes`
### open_at : `2023-03-23T06:23:11Z`
### last_modified_date : `2023-03-23T09:02:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109259
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
When I was constructing test case for PR109069, I found that rs6000 specific pass pass_analyze_swaps doesn't try to preserve REG_EQUAL notes for vector constant. One typical case is:

#include <altivec.h>

vector signed int test2(vector signed int v1){
  vector signed int v = {-128, -1280, 1280, 128};
  return v+v1;
}

On LE, with -O2 -mcpu=power8, in 261r.swaps dump:

(insn 6 3 17 2 (set (reg/f:DI 121)
        (unspec:DI [
                (symbol_ref/u:DI ("*.LC0") [flags 0x82])
                (reg:DI 2 2)
            ] UNSPEC_TOCREL)) "tt.c":7:12 764 {*tocrefdi}
     (expr_list:REG_EQUAL (symbol_ref/u:DI ("*.LC0") [flags 0x82])
        (nil)))
(insn 17 6 18 2 (set (reg:V4SI 122)
        (mem/u/c:V4SI (and:DI (reg/f:DI 121)
                (const_int -16 [0xfffffffffffffff0])) [0  S16 A128])) "tt.c":7:12 -1
     (nil))

while in 260r.dfinit dump, it has:

(note 3 2 6 2 NOTE_INSN_FUNCTION_BEG)
(insn 6 3 7 2 (set (reg/f:DI 121)
        (unspec:DI [
                (symbol_ref/u:DI ("*.LC0") [flags 0x82])
                (reg:DI 2 2)
            ] UNSPEC_TOCREL)) "tt.c":7:12 764 {*tocrefdi}
     (expr_list:REG_EQUAL (symbol_ref/u:DI ("*.LC0") [flags 0x82])
        (nil)))
(insn 7 6 8 2 (set (reg:V4SI 122)
        (vec_select:V4SI (mem/u/c:V4SI (reg/f:DI 121) [0  S16 A128])
            (parallel [
                    (const_int 2 [0x2])
                    (const_int 3 [0x3])
                    (const_int 0 [0])
                    (const_int 1 [0x1])
                ]))) "tt.c":7:12 1369 {*vsx_lxvd2x4_le_v4si}
     (nil))
(insn 8 7 9 2 (set (reg:V4SI 120)
        (vec_select:V4SI (reg:V4SI 122)
            (parallel [
                    (const_int 2 [0x2])
                    (const_int 3 [0x3])
                    (const_int 0 [0])
                    (const_int 1 [0x1])
                ]))) "tt.c":7:12 1358 {xxswapd_v4si}
     (expr_list:REG_EQUAL (const_vector:V4SI [
                (const_int -128 [0xffffffffffffff80])
                (const_int -1280 [0xfffffffffffffb00])
                (const_int 1280 [0x500])
                (const_int 128 [0x80])
            ])
        (nil)))

====

with -O2 -mcpu=power9, in 262r.cse1 rtl dump:

(insn 6 3 7 2 (set (reg/f:DI 121)
        (unspec:DI [
                (symbol_ref/u:DI ("*.LC0") [flags 0x82])
                (reg:DI 2 2)
            ] UNSPEC_TOCREL)) "tt.c":7:12 764 {*tocrefdi}
     (expr_list:REG_EQUAL (symbol_ref/u:DI ("*.LC0") [flags 0x82])
        (nil)))
(insn 7 6 8 2 (set (reg:V4SI 120)
        (mem/u/c:V4SI (reg/f:DI 121) [0  S16 A128])) "tt.c":7:12 1182 {vsx_movv4si_64bit}
     (expr_list:REG_DEAD (reg/f:DI 121)
        (expr_list:REG_EQUAL (const_vector:V4SI [
                    (const_int -128 [0xffffffffffffff80])
                    (const_int -1280 [0xfffffffffffffb00])
                    (const_int 1280 [0x500])
                    (const_int 128 [0x80])
                ])
            (nil))))


---


### compiler : `gcc`
### title : `Guard variable still provided for static constinit variable with an empty destructor`
### open_at : `2023-03-24T00:48:54Z`
### last_modified_date : `2023-03-25T00:47:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109268
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `13.0`
### severity : `enhancement`
### contents :
This program: 

struct X {
    constexpr X() { }
    constexpr ~X() { }
};

int main()
{
    static constinit X data;
}

compiled on gcc trunk with -std=c++2b -O3 emits:

X::~X() [base object destructor]:
        ret
main:
        movzx   eax, BYTE PTR guard variable for main::data[rip]
        test    al, al
        je      .L14
        xor     eax, eax
        ret
.L14:
        push    rcx
        mov     edi, OFFSET FLAT:guard variable for main::data
        call    __cxa_guard_acquire
        test    eax, eax
        jne     .L15
.L5:
        xor     eax, eax
        pop     rdx
        ret
.L15:
        mov     edx, OFFSET FLAT:__dso_handle
        mov     esi, OFFSET FLAT:_ZZ4mainE4data
        mov     edi, OFFSET FLAT:_ZN1XD1Ev
        call    __cxa_atexit
        mov     edi, OFFSET FLAT:guard variable for main::data
        call    __cxa_guard_release
        jmp     .L5

But data is constant-initialized (enforced by constinit), so there shouldn't be a need for a guard variable in this context? clang does not generate one in this case.


---


### compiler : `gcc`
### title : `Unnecessary vectorization alias versioning check`
### open_at : `2023-03-24T07:27:26Z`
### last_modified_date : `2023-03-27T10:24:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109271
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Split out from PR80198

void __attribute__((noipa)) generic(int * a, int * b, int c)
{
  int i;
  a = __builtin_assume_aligned (a, 16);
  b = __builtin_assume_aligned (b, 16);
  for (i=0; i < 256; i++) {
      a[i] = b[i] | c;
  }
}

is versioned for aliasing by the vectorizer, not realizing the alignment
guarantees allow vectorizing with 16byte vectors without.  Actually
the CCP pass after the vectorizer is able to elide the check, so this
but is merely to track the cost side of this (also when mixed with
a more complex required versioning check it might fail to optimize later).


---


### compiler : `gcc`
### title : `RISC-V: complex constants synthesized should be improved`
### open_at : `2023-03-24T22:04:11Z`
### last_modified_date : `2023-10-06T19:38:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109279
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
This is code bloat regression since gcc 12.1, seen yet again in SPEC2017 deepsjeng. After 2e886eef7f2b5a ("RISC-V: Produce better code with complex constants [PR95632] [PR106602]").

unsigned long long FileAttacks(unsigned long long occ, const unsigned int sq) {
    unsigned int o;
    unsigned int f = sq & 7;

    occ   =   0x0101010101010101ULL & (occ   >> f);
    o     = ( 0x0080402010080400ULL *  occ ) >> 58;
    return  ( aFileAttacks[o][sq>>3]    ) << f;
}

cc1 -O2 -march=rv64gc_zba_zbb_zbc_zbs -mabi=lp64d   # stage1 is enough

Before above commit
-------------------
        lui     a4,%hi(.LC0)
        ld      a4,%lo(.LC0)(a4)
        andi    a3,a1,7
        srl     a5,a0,a3
        and     a5,a5,a4
        lui     a4,%hi(.LC1)
        ld      a4,%lo(.LC1)(a4)
        srliw   a1,a1,3
        mul     a5,a5,a4
        lui     a4,%hi(aFileAttacks)
        addi    a4,a4,%lo(aFileAttacks)
        srli    a5,a5,58
        sh3add  a5,a5,a1
        sh3add  a5,a5,a4
        ld      a0,0(a5)
        sll     a0,a0,a3
        ret

        .section        .srodata.cst8,"aM",@progbits,8
        .align  3
.LC0:
        .dword  0x0101010101010101
        .align  3
.LC1:
        .dword  0x0080402010080400


With commit
-----------

       li      a5,16842752
        addi    a5,a5,257
        slli    a5,a5,16
        addi    a5,a5,257
        andi    a3,a1,7
        slli    a5,a5,16
        srl     a4,a0,a3
        addi    a5,a5,257
        and     a4,a4,a5
        slli    a5,a4,9
        add     a5,a5,a4
        slli    a5,a5,9
        add     a5,a5,a4
        slli    a4,a5,27
        add     a5,a5,a4
        srli    a5,a5,45
        srliw   a1,a1,3
        andi    a5,a5,504
        lui     a4,%hi(aFileAttacks)
        add     a5,a5,a1
        addi    a4,a4,%lo(aFileAttacks)
        sh3add  a5,a5,a4
        ld      a0,0(a5)
        sll     a0,a0,a3
        ret


---


### compiler : `gcc`
### title : `Very significant increase in code size (gcc-avr)`
### open_at : `2023-03-24T22:35:51Z`
### last_modified_date : `2023-04-16T20:43:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109280
### status : `WAITING`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `use std::optional results in suboptimal code`
### open_at : `2023-03-25T11:55:21Z`
### last_modified_date : `2023-07-12T21:35:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109281
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `enhancement`
### contents :
In the following (almost real) code gcc emits suboptimal code if std::optional is used w/r/t home made one and clang

see https://godbolt.org/z/Pba51Ye7Y


-----

code


#include <optional>

// #define USE_OPTIONAL

#ifdef USE_OPTIONAL
struct SubRingCrossings {
  SubRingCrossings(int ci, int ni, float nd) : closestIndex(ci), nextIndex(ni), nextDistance(nd) {}

  int closestIndex;
  int nextIndex;
  float nextDistance;
};
#else
struct SubRingCrossings {
  SubRingCrossings() : valid(false) {}
  SubRingCrossings(int ci, int ni, float nd) : valid(true), closestIndex(ci), nextIndex(ni), nextDistance(nd) {}

  bool valid;
  int closestIndex;
  int nextIndex;
  float nextDistance;
};
#endif

bool condition();

#ifdef USE_OPTIONAL
std::optional<SubRingCrossings> foo() {
    if (condition()) {
        return std::nullopt;
    }
    return SubRingCrossings(1, 2, 3.14);
}
#else
SubRingCrossings foo() {
    if (condition()) {
        return SubRingCrossings();
    }
    return SubRingCrossings(1, 2, 3.14);
}
#endif

int bar() {
    auto tmp = foo();
#ifdef USE_OPTIONAL
    if (tmp) {
        return tmp->closestIndex;
#else
    if (tmp.valid) {
        return tmp.closestIndex;
#endif
    } else {
        return 0;
    }
}


---


### compiler : `gcc`
### title : `Optimizing sal shr pairs when inlining function`
### open_at : `2023-03-26T14:43:53Z`
### last_modified_date : `2023-05-20T01:30:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109287
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `enhancement`
### contents :
I was trying to construct a span type to be used for working with a tile-based image

```
#include <cstdint>
#include <type_traits>
#include <cstddef>

template<class T, size_t TileSize>
class span_2d_tiled
{
public:
    using IndexType = size_t;

    static constexpr size_t tile_size()
    {
        return TileSize;
    }

    constexpr explicit span_2d_tiled(): span_2d_tiled{0u, 0u, nullptr} {}

    constexpr explicit span_2d_tiled(IndexType w, IndexType h, T* ptr):
        m_tilecount_x{1 + (w - 1)/TileSize},
        m_tilecount_y{1 + (h - 1)/TileSize},
        m_ptr{ptr}
    {}

    constexpr auto tilecount_x() const { return m_tilecount_x; }

    constexpr auto tilecount_y() const { return m_tilecount_y; }

    constexpr T& operator()(IndexType x, IndexType y) const
    {
        auto const x_tile = x/TileSize;
        auto const y_tile = y/TileSize;
        auto const x_offset = x%TileSize;
        auto const y_offset = y%TileSize;
        auto const tile_start = y_tile*m_tilecount_x + x_tile;

        return *(m_ptr + tile_start + y_offset*TileSize + x_offset);
    }

private:
    IndexType m_tilecount_x;
    IndexType m_tilecount_y;
    T* m_ptr;
};

template<size_t TileSize, class Func>
void visit_tiles(size_t x_count, size_t y_count, Func&& f)
{
    for(size_t k = 0; k != y_count; ++k)
    {
        for(size_t l = 0; l != x_count; ++l)
        {
            for(size_t y = 0; y != TileSize; ++y)
            {
                for(size_t x = 0; x != TileSize; ++x)
                {
                    f(l*TileSize + x, k*TileSize + y);
                }
            }
        }
    }
}

void do_stuff(float);

void call_do_stuff(span_2d_tiled<float, 16> foo)
{
    visit_tiles<decltype(foo)::tile_size()>(foo.tilecount_x(), foo.tilecount_y(), [foo](size_t x, size_t y){
        do_stuff(foo(x, y));
    });
}
```

Here, the user of this API wants to access individual pixels. Thus, the coordinates are transformed before calling f. To do so, we multiply by TileSize and adds the appropriate offset. In the callback, the pixel value is looked up. But now we must find out what tile it is, and the offset within that tile, which means that the inverse transformation must be applied. As can be seen in the Godbolt link, GCC does not fully understand what is going on here. However, latest clang appears to do a much better job with the same settings. It also unrolls the inner loop, much better than if I used

```
#pragma GCC unroll 16
```


---


### compiler : `gcc`
### title : `-Os generates bigger code than -O2 on 32-bit ARM`
### open_at : `2023-03-28T16:27:11Z`
### last_modified_date : `2023-04-22T20:57:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109317
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
Simple loops like the one in the example below are better optimized for size on 32-bit ARM with -O2 option than -Os option.


$ cat test-arm.c
char *test1(char *ptr) {
  while (*ptr != '\0' || *(ptr+1) != '\0') ptr++;
  ptr++;
  return ptr;
}



$ arm-linux-gnueabi-gcc -O2 -c test-arm.c && arm-linux-gnueabi-objdump -d test-arm.o

test-arm.o:     file format elf32-littlearm


Disassembly of section .text:

00000000 <test1>:
   0:   e5d02000        ldrb    r2, [r0]
   4:   e2800001        add     r0, r0, #1
   8:   e3520000        cmp     r2, #0
   c:   1afffffb        bne     0 <test1>
  10:   e5d02000        ldrb    r2, [r0]
  14:   e3520000        cmp     r2, #0
  18:   1afffff8        bne     0 <test1>
  1c:   e12fff1e        bx      lr



$ arm-linux-gnueabi-gcc -Os -c test-arm.c && arm-linux-gnueabi-objdump -d test-arm.o

test-arm.o:     file format elf32-littlearm


Disassembly of section .text:

00000000 <test1>:
   0:   e1a03000        mov     r3, r0
   4:   e5d32000        ldrb    r2, [r3]
   8:   e2800001        add     r0, r0, #1
   c:   e3520000        cmp     r2, #0
  10:   1afffffa        bne     0 <test1>
  14:   e5d02000        ldrb    r2, [r0]
  18:   e3520000        cmp     r2, #0
  1c:   1afffff7        bne     0 <test1>
  20:   e12fff1e        bx      lr



$ arm-linux-gnueabi-gcc -v
Using built-in specs.
COLLECT_GCC=arm-linux-gnueabi-gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc-cross/arm-linux-gnueabi/12/lto-wrapper
Target: arm-linux-gnueabi
Configured with: ../src/configure -v --with-pkgversion='Debian 12.2.0-14' --with-bugurl=file:///usr/share/doc/gcc-12/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-12 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libitm --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --without-target-system-zlib --enable-multiarch --disable-sjlj-exceptions --with-arch=armv5te --with-float=soft --disable-werror --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=arm-linux-gnueabi --program-prefix=arm-linux-gnueabi- --includedir=/usr/arm-linux-gnueabi/include
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.2.0 (Debian 12.2.0-14)


---


### compiler : `gcc`
### title : `Sub-optimal assembler code generation for valid C on x86-64`
### open_at : `2023-03-29T01:04:18Z`
### last_modified_date : `2023-03-30T02:32:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109326
### status : `WAITING`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.4.0`
### severity : `normal`
### contents :
Created attachment 54782
compiler output

I have a bit of code here that is compiling without warnings and producing what appear to be gross errors in the assembler output for some functions.  Pertinent info:

$ gcc10.4 -v
Using built-in specs.
COLLECT_GCC=gcc10.4
COLLECT_LTO_WRAPPER=/home/stevet/libexec/gcc/x86_64-pc-linux-gnu/10.4.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: ../gcc-10.4.0/configure --prefix=/home/stevet --program-suffix=10.4 --enable-shared --enable-linker-build-id --without-included-gettext --enable-threads=posix --enable-nls --enable-bootstrap --enable-clocale=gnu --with-tune=generic --enable-languages=c --disable-multilib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.4.0 (GCC) 

uname -a
Linux mx 5.18.0-4mx-amd64 #1 SMP PREEMPT_DYNAMIC Debian 5.18.16-1~mx21+1 (2022-08-22) x86_64 GNU/Linux

  

Unit compilation command:
gcc10.4 -c  -D_POSIX_C_SOURCE=200112L -DOLOCK_192 -DARCH_64 -DLINUX -I./ -I./ -pthread -m64 -std=c99 -Wall -Wextra -Wno-implicit-fallthrough -Werror -falign-functions=16 -falign-loops=1 -falign-jumps=1 -fno-inline-small-functions -fdiagnostics-color=never -fverbose-asm  --save-temps  -O3 -ggdb  -o olock.o olock.c


it should be noted that the bad code generation seems lessened, but not eliminated at -O2.  Similarly, the problems were slightly different between
gcc-10.2.1 and the most recent 10.x release.


First thing to note is the assembler generated for the relatively simple olock_reset_op() function.  Near as I can tell, the asm bears exactly zero relation to the C code of that function.  The mystery constant $0xa06 seems notable and also appears in init_olock_op_element_struct().

init_olock_op_struct() begins with an access to %fs:0x0, which is then clobbered by an add $0x0, %rax shortly thereafter.  Perhaps this is normal.

olock_fsm_event() doesn't look good either.  There are three callq *%reg instances where there should be at most one.

I'm not sure about olock_op_allocator().  olock_opcode_acqs() looks suspicious, but I'm not that well versed in x86 so I could be wrong.

If I knew that the dynamic linker would fixup the %fs:0x0 references to something normal I'd have more confidence about the rest of the code, but it looks like about half the functions aren't correct at this point.

I've not yet tested any of this code yet; it is still subject to revision while I clean it up.  With this type of algorithm it is unfortunately necessary to have mostly correct code before even thinking about testing it.  This version is close to that point.

As I note I can only attach one file, I'll include the assembler output for the troublesome olock_reset_op() function for reference.

    216 0000000000000290 <olock_reset_op>:
    217      290:       0f b7 57 10             movzwl 0x10(%rdi),%edx
    218      294:       66 85 d2                test   %dx,%dx
    219      297:       0f 84 f4 04 00 00       je     791 <olock_reset_op+0x501>
    220      29d:       8d 42 ff                lea    -0x1(%rdx),%eax
    221      2a0:       66 83 f8 0e             cmp    $0xe,%ax
    222      2a4:       0f 86 e8 04 00 00       jbe    792 <olock_reset_op+0x502>
    223      2aa:       89 d1                   mov    %edx,%ecx
    224      2ac:       48 8d 47 2c             lea    0x2c(%rdi),%rax
    225      2b0:       66 c1 e9 04             shr    $0x4,%cx
    226      2b4:       83 e9 01                sub    $0x1,%ecx
    227      2b7:       0f b7 c9                movzwl %cx,%ecx
    228      2ba:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    229      2be:       48 c1 e1 07             shl    $0x7,%rcx
    230      2c2:       48 8d 8c 0f ac 01 00    lea    0x1ac(%rdi,%rcx,1),%rcx
    231      2c9:       00 
    232      2ca:       41 b9 06 0a 00 00       mov    $0xa06,%r9d
    233      2d0:       c7 40 f4 00 00 00 00    movl   $0x0,-0xc(%rax)
    234      2d7:       41 ba 06 0a 00 00       mov    $0xa06,%r10d
    235      2dd:       41 bb 06 0a 00 00       mov    $0xa06,%r11d
    236      2e3:       c7 40 0c 00 00 00 00    movl   $0x0,0xc(%rax)
    237      2ea:       be 06 0a 00 00          mov    $0xa06,%esi
    238      2ef:       41 b8 06 0a 00 00       mov    $0xa06,%r8d
    239      2f5:       48 05 80 01 00 00       add    $0x180,%rax
    240      2fb:       c7 80 a4 fe ff ff 00    movl   $0x0,-0x15c(%rax)
    241      302:       00 00 00 
    242      305:       c7 80 bc fe ff ff 00    movl   $0x0,-0x144(%rax)
    243      30c:       00 00 00 
    244      30f:       c7 80 d4 fe ff ff 00    movl   $0x0,-0x12c(%rax)
    245      316:       00 00 00 
    246      319:       c7 80 ec fe ff ff 00    movl   $0x0,-0x114(%rax)
    247      320:       00 00 00 
    248      323:       c7 80 04 ff ff ff 00    movl   $0x0,-0xfc(%rax)
    249      32a:       00 00 00 
    250      32d:       c7 80 1c ff ff ff 00    movl   $0x0,-0xe4(%rax)
    251      334:       00 00 00 
    252      337:       c7 80 34 ff ff ff 00    movl   $0x0,-0xcc(%rax)
    253      33e:       00 00 00 
    254      341:       c7 80 4c ff ff ff 00    movl   $0x0,-0xb4(%rax)
    255      348:       00 00 00 
    256      34b:       c7 80 64 ff ff ff 00    movl   $0x0,-0x9c(%rax)
    257      352:       00 00 00 
    258      355:       c7 80 7c ff ff ff 00    movl   $0x0,-0x84(%rax)
    259      35c:       00 00 00 
    260      35f:       c7 40 94 00 00 00 00    movl   $0x0,-0x6c(%rax)
    261      366:       c7 40 ac 00 00 00 00    movl   $0x0,-0x54(%rax)
    262      36d:       c7 40 c4 00 00 00 00    movl   $0x0,-0x3c(%rax)
    263      374:       c7 40 dc 00 00 00 00    movl   $0x0,-0x24(%rax)
    264      37b:       c6 80 7c fe ff ff 00    movb   $0x0,-0x184(%rax)
    265      382:       c6 80 94 fe ff ff 00    movb   $0x0,-0x16c(%rax)
    266      389:       c6 80 ac fe ff ff 00    movb   $0x0,-0x154(%rax)
    267      390:       c6 80 c4 fe ff ff 00    movb   $0x0,-0x13c(%rax)
    268      397:       c6 80 dc fe ff ff 00    movb   $0x0,-0x124(%rax)
    269      39e:       c6 80 f4 fe ff ff 00    movb   $0x0,-0x10c(%rax)
    270      3a5:       c6 80 0c ff ff ff 00    movb   $0x0,-0xf4(%rax)
    271      3ac:       c6 80 24 ff ff ff 00    movb   $0x0,-0xdc(%rax)
    272      3b3:       c6 80 3c ff ff ff 00    movb   $0x0,-0xc4(%rax)
    273      3ba:       c6 80 54 ff ff ff 00    movb   $0x0,-0xac(%rax)
    274      3c1:       c6 80 6c ff ff ff 00    movb   $0x0,-0x94(%rax)
    275      3c8:       c6 40 84 00             movb   $0x0,-0x7c(%rax)
    276      3cc:       c6 40 9c 00             movb   $0x0,-0x64(%rax)
    277      3d0:       c6 40 b4 00             movb   $0x0,-0x4c(%rax)
    278      3d4:       c6 40 cc 00             movb   $0x0,-0x34(%rax)
    279      3d8:       c6 40 e4 00             movb   $0x0,-0x1c(%rax)
    280      3dc:       66 44 89 88 80 fe ff    mov    %r9w,-0x180(%rax)
    281      3e3:       ff 
    282      3e4:       41 b9 06 0a 00 00       mov    $0xa06,%r9d
    283      3ea:       66 44 89 90 98 fe ff    mov    %r10w,-0x168(%rax)
    284      3f1:       ff 
    285      3f2:       41 ba 06 0a 00 00       mov    $0xa06,%r10d
    286      3f8:       66 44 89 98 b0 fe ff    mov    %r11w,-0x150(%rax)
    287      3ff:       ff 
    288      400:       41 bb 06 0a 00 00       mov    $0xa06,%r11d
    289      406:       66 89 b0 c8 fe ff ff    mov    %si,-0x138(%rax)
    290      40d:       be 06 0a 00 00          mov    $0xa06,%esi
    291      412:       66 44 89 80 e0 fe ff    mov    %r8w,-0x120(%rax)
    292      419:       ff 
    293      41a:       41 b8 06 0a 00 00       mov    $0xa06,%r8d
    294      420:       66 44 89 88 f8 fe ff    mov    %r9w,-0x108(%rax)
    295      427:       ff 
    296      428:       41 b9 06 0a 00 00       mov    $0xa06,%r9d
    297      42e:       66 44 89 90 10 ff ff    mov    %r10w,-0xf0(%rax)
    298      435:       ff 
    299      436:       41 ba 06 0a 00 00       mov    $0xa06,%r10d
    300      43c:       66 44 89 98 28 ff ff    mov    %r11w,-0xd8(%rax)
    301      443:       ff 
    302      444:       41 bb 06 0a 00 00       mov    $0xa06,%r11d
    303      44a:       66 89 b0 40 ff ff ff    mov    %si,-0xc0(%rax)
    304      451:       be 06 0a 00 00          mov    $0xa06,%esi
    305      456:       66 44 89 80 58 ff ff    mov    %r8w,-0xa8(%rax)
    306      45d:       ff 
    307      45e:       41 b8 06 0a 00 00       mov    $0xa06,%r8d
    308      464:       66 44 89 88 70 ff ff    mov    %r9w,-0x90(%rax)
    309      46b:       ff 
    310      46c:       41 b9 06 0a 00 00       mov    $0xa06,%r9d
    311      472:       66 44 89 50 88          mov    %r10w,-0x78(%rax)
    312      477:       66 44 89 58 a0          mov    %r11w,-0x60(%rax)
    313      47c:       66 89 70 b8             mov    %si,-0x48(%rax)
    314      480:       66 44 89 40 d0          mov    %r8w,-0x30(%rax)
    315      485:       66 44 89 48 e8          mov    %r9w,-0x18(%rax)
    316      48a:       48 39 c8                cmp    %rcx,%rax
    317      48d:       0f 85 37 fe ff ff       jne    2ca <olock_reset_op+0x3a>
    318      493:       89 d0                   mov    %edx,%eax
    319      495:       83 e0 f0                and    $0xfffffff0,%eax
    320      498:       f6 c2 0f                test   $0xf,%dl
    321      49b:       0f 84 f8 02 00 00       je     799 <olock_reset_op+0x509>
    322      4a1:       0f b7 f0                movzwl %ax,%esi
    323      4a4:       8d 48 01                lea    0x1(%rax),%ecx
    324      4a7:       48 8d 34 76             lea    (%rsi,%rsi,2),%rsi
    325      4ab:       48 c1 e6 03             shl    $0x3,%rsi
    326      4af:       4c 8d 04 37             lea    (%rdi,%rsi,1),%r8
    327      4b3:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    328      4ba:       00 
    329      4bb:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    330      4c0:       41 b8 06 0a 00 00       mov    $0xa06,%r8d
    331      4c6:       66 44 89 44 37 2c       mov    %r8w,0x2c(%rdi,%rsi,1)
    332      4cc:       66 39 d1                cmp    %dx,%cx
    333      4cf:       0f 83 bc 02 00 00       jae    791 <olock_reset_op+0x501>
    334      4d5:       0f b7 c9                movzwl %cx,%ecx
    335      4d8:       41 bb 06 0a 00 00       mov    $0xa06,%r11d
    336      4de:       8d 70 02                lea    0x2(%rax),%esi
    337      4e1:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    338      4e5:       48 c1 e1 03             shl    $0x3,%rcx
    339      4e9:       4c 8d 04 0f             lea    (%rdi,%rcx,1),%r8
    340      4ed:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    341      4f4:       00 
    342      4f5:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    343      4fa:       66 44 89 5c 0f 2c       mov    %r11w,0x2c(%rdi,%rcx,1)
    344      500:       66 39 d6                cmp    %dx,%si
    345      503:       0f 83 88 02 00 00       jae    791 <olock_reset_op+0x501>
    346      509:       0f b7 f6                movzwl %si,%esi
    347      50c:       41 ba 06 0a 00 00       mov    $0xa06,%r10d
    348      512:       8d 48 03                lea    0x3(%rax),%ecx
    349      515:       48 8d 34 76             lea    (%rsi,%rsi,2),%rsi
    350      519:       48 c1 e6 03             shl    $0x3,%rsi
    351      51d:       4c 8d 04 37             lea    (%rdi,%rsi,1),%r8
    352      521:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    353      528:       00 
    354      529:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    355      52e:       66 44 89 54 37 2c       mov    %r10w,0x2c(%rdi,%rsi,1)
    356      534:       66 39 ca                cmp    %cx,%dx
    357      537:       0f 86 54 02 00 00       jbe    791 <olock_reset_op+0x501>
    358      53d:       0f b7 c9                movzwl %cx,%ecx
    359      540:       41 b9 06 0a 00 00       mov    $0xa06,%r9d
    360      546:       8d 70 04                lea    0x4(%rax),%esi
    361      549:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    362      54d:       48 c1 e1 03             shl    $0x3,%rcx
    363      551:       4c 8d 04 0f             lea    (%rdi,%rcx,1),%r8
    364      555:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    365      55c:       00 
    366      55d:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    367      562:       66 44 89 4c 0f 2c       mov    %r9w,0x2c(%rdi,%rcx,1)
    368      568:       66 39 f2                cmp    %si,%dx
    369      56b:       0f 86 20 02 00 00       jbe    791 <olock_reset_op+0x501>
    370      571:       0f b7 f6                movzwl %si,%esi
    371      574:       8d 48 05                lea    0x5(%rax),%ecx
    372      577:       48 8d 34 76             lea    (%rsi,%rsi,2),%rsi
    373      57b:       48 c1 e6 03             shl    $0x3,%rsi
    374      57f:       4c 8d 04 37             lea    (%rdi,%rsi,1),%r8
    375      583:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    376      58a:       00 
    377      58b:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    378      590:       41 b8 06 0a 00 00       mov    $0xa06,%r8d
    379      596:       66 44 89 44 37 2c       mov    %r8w,0x2c(%rdi,%rsi,1)
    380      59c:       66 39 ca                cmp    %cx,%dx
    381      59f:       0f 86 ec 01 00 00       jbe    791 <olock_reset_op+0x501>
    382      5a5:       0f b7 c9                movzwl %cx,%ecx
    383      5a8:       41 bb 06 0a 00 00       mov    $0xa06,%r11d
    384      5ae:       8d 70 06                lea    0x6(%rax),%esi
    385      5b1:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    386      5b5:       48 c1 e1 03             shl    $0x3,%rcx
    387      5b9:       4c 8d 04 0f             lea    (%rdi,%rcx,1),%r8
    388      5bd:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    389      5c4:       00 
    390      5c5:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    391      5ca:       66 44 89 5c 0f 2c       mov    %r11w,0x2c(%rdi,%rcx,1)
    392      5d0:       66 39 f2                cmp    %si,%dx
    393      5d3:       0f 86 b8 01 00 00       jbe    791 <olock_reset_op+0x501>
    394      5d9:       0f b7 f6                movzwl %si,%esi
    395      5dc:       41 ba 06 0a 00 00       mov    $0xa06,%r10d
    396      5e2:       8d 48 07                lea    0x7(%rax),%ecx
    397      5e5:       48 8d 34 76             lea    (%rsi,%rsi,2),%rsi
    398      5e9:       48 c1 e6 03             shl    $0x3,%rsi
    399      5ed:       4c 8d 04 37             lea    (%rdi,%rsi,1),%r8
    400      5f1:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    401      5f8:       00 
    402      5f9:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    403      5fe:       66 44 89 54 37 2c       mov    %r10w,0x2c(%rdi,%rsi,1)
    404      604:       66 39 ca                cmp    %cx,%dx
    405      607:       0f 86 84 01 00 00       jbe    791 <olock_reset_op+0x501>
    406      60d:       0f b7 c9                movzwl %cx,%ecx
    407      610:       41 b9 06 0a 00 00       mov    $0xa06,%r9d
    408      616:       8d 70 08                lea    0x8(%rax),%esi
    409      619:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    410      61d:       48 c1 e1 03             shl    $0x3,%rcx
    411      621:       4c 8d 04 0f             lea    (%rdi,%rcx,1),%r8
    412      625:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    413      62c:       00 
    414      62d:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    415      632:       66 44 89 4c 0f 2c       mov    %r9w,0x2c(%rdi,%rcx,1)
    416      638:       66 39 f2                cmp    %si,%dx
    417      63b:       0f 86 50 01 00 00       jbe    791 <olock_reset_op+0x501>
    418      641:       0f b7 f6                movzwl %si,%esi
    419      644:       8d 48 09                lea    0x9(%rax),%ecx
    420      647:       48 8d 34 76             lea    (%rsi,%rsi,2),%rsi
    421      64b:       48 c1 e6 03             shl    $0x3,%rsi
    422      64f:       4c 8d 04 37             lea    (%rdi,%rsi,1),%r8
    423      653:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    424      65a:       00 
    425      65b:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    426      660:       41 b8 06 0a 00 00       mov    $0xa06,%r8d
    427      666:       66 44 89 44 37 2c       mov    %r8w,0x2c(%rdi,%rsi,1)
    428      66c:       66 39 ca                cmp    %cx,%dx
    429      66f:       0f 86 1c 01 00 00       jbe    791 <olock_reset_op+0x501>
    430      675:       0f b7 c9                movzwl %cx,%ecx
    431      678:       41 bb 06 0a 00 00       mov    $0xa06,%r11d
    432      67e:       8d 70 0a                lea    0xa(%rax),%esi
    433      681:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    434      685:       48 c1 e1 03             shl    $0x3,%rcx
    435      689:       4c 8d 04 0f             lea    (%rdi,%rcx,1),%r8
    436      68d:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    437      694:       00 
    438      695:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    439      69a:       66 44 89 5c 0f 2c       mov    %r11w,0x2c(%rdi,%rcx,1)
    440      6a0:       66 39 f2                cmp    %si,%dx
    441      6a3:       0f 86 e8 00 00 00       jbe    791 <olock_reset_op+0x501>
    442      6a9:       0f b7 f6                movzwl %si,%esi
    443      6ac:       41 ba 06 0a 00 00       mov    $0xa06,%r10d
    444      6b2:       8d 48 0b                lea    0xb(%rax),%ecx
    445      6b5:       48 8d 34 76             lea    (%rsi,%rsi,2),%rsi
    446      6b9:       48 c1 e6 03             shl    $0x3,%rsi
    447      6bd:       4c 8d 04 37             lea    (%rdi,%rsi,1),%r8
    448      6c1:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    449      6c8:       00 
    450      6c9:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    451      6ce:       66 44 89 54 37 2c       mov    %r10w,0x2c(%rdi,%rsi,1)
    452      6d4:       66 39 ca                cmp    %cx,%dx
    453      6d7:       0f 86 b4 00 00 00       jbe    791 <olock_reset_op+0x501>
    454      6dd:       0f b7 c9                movzwl %cx,%ecx
    455      6e0:       41 b9 06 0a 00 00       mov    $0xa06,%r9d
    456      6e6:       8d 70 0c                lea    0xc(%rax),%esi
    457      6e9:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    458      6ed:       48 c1 e1 03             shl    $0x3,%rcx
    459      6f1:       4c 8d 04 0f             lea    (%rdi,%rcx,1),%r8
    460      6f5:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    461      6fc:       00 
    462      6fd:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    463      702:       66 44 89 4c 0f 2c       mov    %r9w,0x2c(%rdi,%rcx,1)
    464      708:       66 39 f2                cmp    %si,%dx
    465      70b:       0f 86 80 00 00 00       jbe    791 <olock_reset_op+0x501>
    466      711:       0f b7 f6                movzwl %si,%esi
    467      714:       8d 48 0d                lea    0xd(%rax),%ecx
    468      717:       48 8d 34 76             lea    (%rsi,%rsi,2),%rsi
    469      71b:       48 c1 e6 03             shl    $0x3,%rsi
    470      71f:       4c 8d 04 37             lea    (%rdi,%rsi,1),%r8
    471      723:       41 c7 40 20 00 00 00    movl   $0x0,0x20(%r8)
    472      72a:       00 
    473      72b:       41 c6 40 28 00          movb   $0x0,0x28(%r8)
    474      730:       41 b8 06 0a 00 00       mov    $0xa06,%r8d
    475      736:       66 44 89 44 37 2c       mov    %r8w,0x2c(%rdi,%rsi,1)
    476      73c:       66 39 ca                cmp    %cx,%dx
    477      73f:       76 50                   jbe    791 <olock_reset_op+0x501>
    478      741:       0f b7 c9                movzwl %cx,%ecx
    479      744:       83 c0 0e                add    $0xe,%eax
    480      747:       48 8d 0c 49             lea    (%rcx,%rcx,2),%rcx
    481      74b:       48 c1 e1 03             shl    $0x3,%rcx
    482      74f:       48 8d 34 0f             lea    (%rdi,%rcx,1),%rsi
    483      753:       c7 46 20 00 00 00 00    movl   $0x0,0x20(%rsi)
    484      75a:       c6 46 28 00             movb   $0x0,0x28(%rsi)
    485      75e:       be 06 0a 00 00          mov    $0xa06,%esi
    486      763:       66 89 74 0f 2c          mov    %si,0x2c(%rdi,%rcx,1)
    487      768:       66 39 c2                cmp    %ax,%dx
    488      76b:       76 24                   jbe    791 <olock_reset_op+0x501>
    489      76d:       0f b7 c0                movzwl %ax,%eax
    490      770:       48 8d 04 40             lea    (%rax,%rax,2),%rax
    491      774:       48 c1 e0 03             shl    $0x3,%rax
    492      778:       48 8d 14 07             lea    (%rdi,%rax,1),%rdx
    493      77c:       c7 42 20 00 00 00 00    movl   $0x0,0x20(%rdx)
    494      783:       c6 42 28 00             movb   $0x0,0x28(%rdx)
    495      787:       ba 06 0a 00 00          mov    $0xa06,%edx
    496      78c:       66 89 54 07 2c          mov    %dx,0x2c(%rdi,%rax,1)
    497      791:       c3                      retq   
    498      792:       31 c0                   xor    %eax,%eax
    499      794:       e9 08 fd ff ff          jmpq   4a1 <olock_reset_op+0x211>
    500      799:       c3                      retq   
    501      79a:       66 0f 1f 44 00 00       nopw   0x0(%rax,%rax,1)


There seems to be some structure in the above, but in comparison to the source it doesn't seem the slightest bit relevant.


---


### compiler : `gcc`
### title : `Use std::move in std::accumulate etc. for std::string for C++11/14/17`
### open_at : `2023-03-29T09:32:33Z`
### last_modified_date : `2023-03-29T20:50:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109333
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.0`
### severity : `enhancement`
### contents :
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0616r0.pdf changed some <numeric> algos to use std::move for the accumulator, which is an observable semantic change. For that reason, we only implement the change for C++20 and up, to retain the behaviour specified in earlier standards.

If the accumulator is std::string then we know it's not observable, and so we can improve performance by always moving (except in C++98 mode).


---


### compiler : `gcc`
### title : `FAIL: 23_containers/vector/bool/allocator/copy.cc (test for excess errors)`
### open_at : `2023-03-31T07:28:40Z`
### last_modified_date : `2023-10-04T14:27:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109353
### status : `NEW`
### tags : `diagnostic, missed-optimization, testsuite-fail`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
After the last ranger changes this test now FAILs with

In file included from /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/vector:62,
                 from /home/rguenther/src/trunk/libstdc++-v3/testsuite/23_containers/vector/bool/allocator/copy.cc:20:
In static member function 'static _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = long unsigned int; _Up = long unsigned int; bool _IsMove = false]',
    inlined from '_OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_algobase.h:506,
    inlined from '_OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_algobase.h:533,
    inlined from '_OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_algobase.h:540,
    inlined from '_OI std::copy(_II, _II, _OI) [with _II = long unsigned int*; _OI = long unsigned int*]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_algobase.h:633,
    inlined from 'std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::_M_copy_aligned(const_iterator, const_iterator, iterator) [with _Alloc = __gnu_test::propagating_allocator<bool, false>]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_bvector.h:1303,
    inlined from 'void std::vector<bool, _Alloc>::_M_insert_aux(iterator, bool) [with _Alloc = __gnu_test::propagating_allocator<bool, false>]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/vector.tcc:945,
    inlined from 'void std::vector<bool, _Alloc>::push_back(bool) [with _Alloc = __gnu_test::propagating_allocator<bool, false>]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_bvector.h:1121,
    inlined from 'void test01()' at /home/rguenther/src/trunk/libstdc++-v3/testsuite/23_containers/vector/bool/allocator/copy.cc:33:
/home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_algobase.h:437: warning: 'void* __builtin_memmove(void*, const void*, long unsigned int)' writing between 9 and 9223372036854775807 bytes into a region of size 8 overflows the destination [-Wstringop-overflow=]
In file included from /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/x86_64-pc-linux-gnu/bits/c++allocator.h:33,
                 from /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/allocator.h:46,
                 from /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/vector:63:
In member function '_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = long unsigned int]',
    inlined from 'static _Tp* std::allocator_traits<std::allocator<_Tp1> >::allocate(allocator_type&, size_type) [with _Tp = long unsigned int]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/alloc_traits.h:482,
    inlined from '__gnu_test::uneq_allocator<Tp, Alloc>::pointer __gnu_test::uneq_allocator<Tp, Alloc>::allocate(size_type, const void*) [with Tp = long unsigned int; Alloc = std::allocator<long unsigned int>]' at /home/rguenther/src/trunk/libstdc++-v3/testsuite/util/testsuite_allocator.h:360,
    inlined from 'static std::allocator_traits< <template-parameter-1-1> >::pointer std::allocator_traits< <template-parameter-1-1> >::allocate(_Alloc&, size_type) [with _Alloc = __gnu_test::propagating_allocator<long unsigned int, false, std::allocator<long unsigned int> >]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/alloc_traits.h:333,
    inlined from 'std::_Bvector_base<_Alloc>::_Bit_pointer std::_Bvector_base<_Alloc>::_M_allocate(std::size_t) [with _Alloc = __gnu_test::propagating_allocator<bool, false>]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_bvector.h:643,
    inlined from 'void std::vector<bool, _Alloc>::_M_insert_aux(iterator, bool) [with _Alloc = __gnu_test::propagating_allocator<bool, false>]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/vector.tcc:943,
    inlined from 'void std::vector<bool, _Alloc>::push_back(bool) [with _Alloc = __gnu_test::propagating_allocator<bool, false>]' at /home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/stl_bvector.h:1121,
    inlined from 'void test01()' at /home/rguenther/src/trunk/libstdc++-v3/testsuite/23_containers/vector/bool/allocator/copy.cc:33:
/home/rguenther/obj-trunk-g/x86_64-pc-linux-gnu/libstdc++-v3/include/bits/new_allocator.h:147: note: destination object of size 8 allocated by 'operator new'


---


### compiler : `gcc`
### title : `Missed optimization for std::optional branchless unwrapping`
### open_at : `2023-04-01T16:52:47Z`
### last_modified_date : `2023-04-11T11:59:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109370
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.2.1`
### severity : `enhancement`
### contents :
Given the following source code godbolt: https://godbolt.org/z/vW6ebqafK

#include <optional>

int f(std::optional<int>&& o) {
    if (!o) return -1;

    return *o;
}

gcc with '-O3' generates:
f(std::optional<int>&&):
        cmp     BYTE PTR [rdi+4], 0
        je      .L3
        mov     eax, DWORD PTR [rdi]
        ret
.L3:
        mov     eax, -1
        ret

while clang generates:
f(std::optional<int>&&):                    # @f(std::optional<int>&&)
        xor     eax, eax
        cmp     byte ptr [rdi + 4], 1
        sbb     eax, eax
        or      eax, dword ptr [rdi]
        ret


---


### compiler : `gcc`
### title : `new builtin like __builtin_sqrt but does not set errno`
### open_at : `2023-04-02T20:59:44Z`
### last_modified_date : `2023-04-03T00:39:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109378
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `unknown`
### severity : `enhancement`
### contents :
Hello gcc team,
https://godbolt.org/z/Wa1rfxrPo
when I write a function that contains std::sqrt, it always contains the nan(?) tests for the argument. E.g. sqrtf64.
If I use my_sqrt the tests are done inside sqrt and not in the calling function - clear (because noinline).

Wouldn't it be better to rewrite __builtin_sqrt so that these tests are done inside __builtin_sqrt and not already in the calling context?
This would have the advantage that std::sqrt would not "contaminate" the calling function with conditional jumps and thus inflate it.
I can make this clear with foo vs. bar.
And of course __builtin_sqrt must be able to be vectorized automatically and must be inline for certain contexts (e.g. __FAST_MATH__).

regards
Gero


---


### compiler : `gcc`
### title : `improve __builtin_fmal`
### open_at : `2023-04-02T21:12:01Z`
### last_modified_date : `2023-04-02T21:22:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109379
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
Hello gcc team,
__builtin_fmal generates quite a lot of overhead. Can you please optimize this or make it an inline function?

thx
Gero


---


### compiler : `gcc`
### title : `[QoI] std::type_index::operator<=> should not call __builtin_strcmp twice`
### open_at : `2023-04-03T03:12:49Z`
### last_modified_date : `2023-04-11T12:03:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109383
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.0`
### severity : `enhancement`
### contents :
The current the current implementation of type_index::operator<=> in libstdc++ is the following:
```
    strong_ordering
    operator<=>(const type_index& __rhs) const noexcept
    {
      if (*_M_target == *__rhs._M_target)
	return strong_ordering::equal;
      if (_M_target->before(*__rhs._M_target))
	return strong_ordering::less;
      return strong_ordering::greater;
    }
```

When the two referenced type_info are not equal, the current implementation may needed to call __builtin_strcmp twice (each in type_info::operator== and type_info::before).

IIUC whenever it's necessary to call __builtin_strcmp from operator<=>, we should just call it once and return __builtin_strcmp(...) <=> 0. Perhaps it would be better to add a member function to type_info for convenience.


It's a bit strange to me that while all implementations I investigated (libc++, libstdc++, and msvc stl) need to use strcmp/__builtin_strcmp for type_info comparison in some cases, currently none of them avoids calling it twice from type_index::operator<=>, although strcmp/__builtin_strcmp is already performing three-way comparison.


---


### compiler : `gcc`
### title : `Inefficient codegen on AArch64 when structure types are returned`
### open_at : `2023-04-03T12:38:36Z`
### last_modified_date : `2023-04-26T13:03:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109391
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
This example https://godbolt.org/z/Pe3f3ozGf

---

#include <arm_neon.h>

int16x8x3_t bsl(const uint16x8x3_t *check, const int16x8x3_t *in1,
                              const int16x8x3_t *in2) {
  int16x8x3_t out;
  for (uint32_t j = 0; j < 3; j++) {
    out.val[j] = vbslq_s16(check->val[j], in1->val[j], in2->val[j]);
  }
  return out;
}


---

Generates:

bsl:
        ldp     q6, q16, [x1]
        ldp     q0, q4, [x2]
        ldp     q5, q7, [x0]
        bsl     v5.16b, v6.16b, v0.16b
        ldr     q0, [x2, 32]
        bsl     v7.16b, v16.16b, v4.16b
        ldr     q6, [x1, 32]
        mov     v1.16b, v5.16b
        ldr     q5, [x0, 32]
        bsl     v5.16b, v6.16b, v0.16b
        mov     v0.16b, v1.16b
        mov     v1.16b, v7.16b
        mov     v2.16b, v5.16b
        ret

with 3 superfluous moves.  It looks like reload is having trouble dealing
with the new compound types as return arguments.

So in RTL We have:

(insn 17 20 22 2 (set (subreg:V8HI (reg/v:V3x8HI 105 [ out ]) 16)
        (xor:V8HI (and:V8HI (xor:V8HI (reg:V8HI 115 [ in2_11(D)->val[1] ])
                    (reg:V8HI 114 [ in1_10(D)->val[1] ]))
                (reg:V8HI 113 [ check_9(D)->val[1] ]))
            (reg:V8HI 115 [ in2_11(D)->val[1] ]))) "/app/example.c":7:16 discrim 1 2558 {aarch64_simd_bslv8hi_internal}
     (expr_list:REG_DEAD (reg:V8HI 115 [ in2_11(D)->val[1] ])
        (expr_list:REG_DEAD (reg:V8HI 114 [ in1_10(D)->val[1] ])
            (expr_list:REG_DEAD (reg:V8HI 113 [ check_9(D)->val[1] ])
                (nil)))))
(insn 22 17 29 2 (set (subreg:V8HI (reg/v:V3x8HI 105 [ out ]) 32)
        (xor:V8HI (and:V8HI (xor:V8HI (reg:V8HI 118 [ in2_11(D)->val[2] ])
                    (reg:V8HI 117 [ in1_10(D)->val[2] ]))
                (reg:V8HI 116 [ check_9(D)->val[2] ]))
            (reg:V8HI 118 [ in2_11(D)->val[2] ]))) "/app/example.c":7:16 discrim 1 2558 {aarch64_simd_bslv8hi_internal}
     (expr_list:REG_DEAD (reg:V8HI 118 [ in2_11(D)->val[2] ])
        (expr_list:REG_DEAD (reg:V8HI 117 [ in1_10(D)->val[2] ])
            (expr_list:REG_DEAD (reg:V8HI 116 [ check_9(D)->val[2] ])
                (nil)))))
(insn 29 22 30 2 (set (reg/i:V3x8HI 32 v0)
        (reg/v:V3x8HI 105 [ out ])) "/app/example.c":10:1 3964 {*aarch64_movv3x8hi}
     (expr_list:REG_DEAD (reg/v:V3x8HI 105 [ out ])
        (nil)))
(insn 30 29 37 2 (use (reg/i:V3x8HI 32 v0)) "/app/example.c":10:1 -1
     (nil))

Reload then decides to insert a bunch of reloads:

	 Choosing alt 0 in insn 17:  (0) =w  (1) 0  (2) w  (3) w {aarch64_simd_bslv8hi_internal}
      Creating newreg=126 from oldreg=113, assigning class FP_REGS to r126
   17: r126:V8HI=r115:V8HI^r114:V8HI&r126:V8HI^r115:V8HI
      REG_DEAD r115:V8HI
      REG_DEAD r114:V8HI
      REG_DEAD r113:V8HI
    Inserting insn reload before:
   43: r126:V8HI=r113:V8HI
    Inserting insn reload after:
   44: r105:V3x8HI#16=r126:V8HI

which introduces these moves.  The problem existed with the previous structure types as well (OImode etc) so it's not new but costs us lots of perf.

I don't think I can fix this with the same pass as https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106106 can I? It looks like in this case the RTL looks fine.


---


### compiler : `gcc`
### title : `Very trivial address calculation does not fold`
### open_at : `2023-04-03T17:57:46Z`
### last_modified_date : `2023-09-08T13:14:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109393
### status : `NEW`
### tags : `missed-optimization`
### component : `c`
### version : `13.0`
### severity : `normal`
### contents :
The following function

int func(int *a, int j) {
  int k = j - 1;
  return a[j - 1] == a[k];
}

surprisingly does not fold to `return 1;` at -O2 or higher (with any GCC version). It can also be seen here: https://godbolt.org/z/cqr43q7fq

There are a lot of variants for this behaviour but this is the most apparent. As can be seen in the godbolt link, the issue seems to be a combination of:

  1) The -1 in a[j - 1] is turned into GIMPLE equivalent with *((a + (ulong) j) + (ulong) -1) but a[k] is turned into *(a + (ulong) (j - 1)).
  2) The -1 is never propagated outside of the (long unsigned int) casts even if it's completely legal/possible.

I feel that I'm missing something here about pointer rules / historical context of these choices and I would appreciate if someone more knowlegable could explain this combination to me.

There are a lot of cases where this can lead to inefficient codegen but most prominently this is the reason for a additional redundant load in a hot loop of SPEC2017's nab in the function downheap_pairs and similar missed optimizations in omnetpp's shiftup function.

Hence this issue can both cause very unexpected missed optimization (as in the example) and also decreases the performance of important benchmarks.

Note: The testcase is not optimized even with -fno-wrapv or -fstrict-overflow, but does optimize with -fwrapv which is the reverse of what I would expect since -fno-wrapv should be more permissive?


---


### compiler : `gcc`
### title : `Missing 'advance' optimizations for std::istreambuf_iterator`
### open_at : `2023-04-04T10:24:21Z`
### last_modified_date : `2023-04-04T15:27:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109400
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.0`
### severity : `enhancement`
### contents :
We have a custom overload of std::advance for std::istreambuf_iterator which operates directly on the streambuf's get area, instead of incrementing character by character.

But it doesn't get used when calling std::next, because that calls std::advance without ADL, and the custom overload hasn't been declared yet. We either need to declare istreambuf_iterator and its std::advance, or add a custom overload of std::next.

Additionally, std::ranges::advance is not specialized for std::istreambuf_iterator so just increments character by character. The ranges::advance(i, n) form should use std::advance. The ranges::advance(i, sentinel) form will already be optimal when the sentinel is the same type as the iterator, but will be slow when using default_sentinel_t. The ranges::advance(i, n, sentinel) form could be done efficiently, and has the advantage that it's not undefined if EOF is reached before advancing n times.


---


### compiler : `gcc`
### title : `Optimise max (a, b) + min (a, b) into a + b`
### open_at : `2023-04-04T11:10:24Z`
### last_modified_date : `2023-04-04T11:31:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109401
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
The testcase

#include <algorithm>
#include <stdint.h>

uint32_t
foo (uint32_t a, uint32_t b)
{
  return std::max (a, b) + std::min (a, b);
}

uint32_t
foom (uint32_t a, uint32_t b)
{
  return std::max (a, b) * std::min (a, b);
}

could optimise foo into a + b and foom into a * b.
Should be a matter of some match.pd patterns?


---


### compiler : `gcc`
### title : `Missing use of aarch64 SVE2 unpredicated integer multiply`
### open_at : `2023-04-04T14:36:10Z`
### last_modified_date : `2023-04-24T09:29:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109406
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
For the testcase
#define N 1024

long long res[N];
long long in1[N];
long long in2[N];

void
mult (void)
{
  for (int i = 0; i < N; i++)
    res[i] = in1[i] * in2[i];
}

With -O3 -march=armv8.5-a+sve2 we generate the loop:
        ptrue   p1.b, all
        whilelo p0.d, wzr, w2
.L2:
        ld1d    z0.d, p0/z, [x4, x0, lsl 3]
        ld1d    z1.d, p0/z, [x3, x0, lsl 3]
        mul     z0.d, p1/m, z0.d, z1.d
        st1d    z0.d, p0, [x1, x0, lsl 3]
        incd    x0
        whilelo p0.d, w0, w2
        b.any   .L2
        ret

SVE2 supports the MUL (vectors, unpredicated) instruction that would allow us to  eliminate the use of p1. Clang manages to do this (though it has other inefficiencies) in https://godbolt.org/z/7xj6xEchx


---


### compiler : `gcc`
### title : `RISC-V: unnecessary sext.w in rv64`
### open_at : `2023-04-05T05:20:31Z`
### last_modified_date : `2023-10-07T20:39:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109414
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
I recently encountered several suboptimal cases with unnecessary sext.w in not/and/xor/or in rv64i.

e.g.
```
int xor_2 (int x, int n) {
	return (x + 1) ^ n;
}

int not_2 (int x, int n) {
	return ~(x + n);
}
```


gcc:
```
xor_2:
	addiw	a0,a0,1
	xor	a0,a1,a0
	sext.w	a0,a0
	ret
not_2:
	addw	a0,a0,a1
	not	a0,a0
	sext.w	a0,a0
        ret
```

clang:
```
xor_2:
        addw    a0, a0, a1
        xor     a0, a0, a1
        ret
not_2:
        addw    a0, a0, a1
        not     a0, a0
        ret
```

This case looks a bit similar to https://gcc.gnu.org/bugzilla//show_bug.cgi?id=106585 , where the X iterator is used in insn pattern.


---


### compiler : `gcc`
### title : `Missed constant propagation cases after reload`
### open_at : `2023-04-05T06:15:37Z`
### last_modified_date : `2023-04-08T23:04:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109416
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
gcc splits a movdi_32bit pattern into two insns after reload in rv32, which brings constant propagation opportunities.

e.g.
```
long long int Data;

void init() {
    Data = 0x0;
}

void init2() {
    Data = 0xf00000000;
}

void init3() {
    Data = 0xf0000000f;
}

```


asm output
```
init:
	lui	a5,%hi(Data)
	li	a3,0
	li	a4,0
	sw	a3,%lo(Data)(a5)
	sw	a4,%lo(Data+4)(a5)
	ret
init2:
	lui	a5,%hi(Data)
	li	a2,0
	li	a3,15
	sw	a2,%lo(Data)(a5)
	sw	a3,%lo(Data+4)(a5)
	ret
init3:
	lui	a5,%hi(Data)
	li	a2,15
	li	a3,15
	sw	a2,%lo(Data)(a5)
	sw	a3,%lo(Data+4)(a5)
	ret
```

could be optimized into
```
init:
	lui	a5,%hi(Data)
	sw	zero,%lo(Data)(a5)
	sw	zero,%lo(Data+4)(a5)
	ret
init2:
	lui	a5,%hi(Data)
	li	a2,15
	sw	zero,%lo(Data)(a5)
	sw	a2,%lo(Data+4)(a5)
	ret
init3:
	lui	a5,%hi(Data)
	li	a2,15
	sw	a2,%lo(Data)(a5)
	sw	a2,%lo(Data+4)(a5)
	ret
```



A similar case in AArch64
```
__int128 Data;

void init() {
    Data = 0xfffff;
}
```

output
```
init:
        adrp    x0, .LANCHOR0
        add     x0, x0, :lo12:.LANCHOR0
        mov     x2, 1048575
        mov     x3, 0
        stp     x2, x3, [x0]
        ret
```
could be optimized into

```
init:
        adrp    x0, .LANCHOR0
        add     x0, x0, :lo12:.LANCHOR0
        mov     x2, 1048575
        stp     x2, xzr, [x0]
        ret
```


---


### compiler : `gcc`
### title : `~((x > y) ? x : y) produces two not instructions`
### open_at : `2023-04-05T19:47:15Z`
### last_modified_date : `2023-05-16T03:50:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109424
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
```
int f(int x, int y)
{
    int t = ((x > y) ? x : y);
    return ~t;
}
int f1(int x, int y)
{
    return ~((x > y) ? x : y);
}
```
You would assume GCC produce the same code for both, but nope, the first f is worse.

The reason why is GCC decides to move the ~ into the ?: operator making the code worse.

fold does this "optimization" but nothing undones it, phi-opt undones it for casts in some but not all cases.


---


### compiler : `gcc`
### title : `ivopts: Compute complexity for unsupported addressing modes`
### open_at : `2023-04-06T07:33:41Z`
### last_modified_date : `2023-04-11T12:39:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109429
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Greetings everyone,
I have pinged the mailing list about this issue about a month ago (I am not the original poster), and heard no response since.

For some addressing modes, it seems that the complexity is calculated in sub-optimal manner. More information and a patch are contained in the link below.

https://gcc.gnu.org/pipermail/gcc-patches/2022-December/608895.html


---


### compiler : `gcc`
### title : `AArch64: suboptimal codegen in 128 bit constant stores`
### open_at : `2023-04-06T16:05:54Z`
### last_modified_date : `2023-04-11T13:13:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109436
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
When splitting a 128-bit constant, there may be cases where the part that is cut out is a constant 0. And we could use a zero register to avoid the "mov reg, 0" instruction.

e.g.
```
__int128 Data;

void init() {
    Data = 0xfffff;
}
```

gcc
```
init:
        adrp    x0, .LANCHOR0
        add     x0, x0, :lo12:.LANCHOR0
        mov     x2, 1048575
        mov     x3, 0
        stp     x2, x3, [x0]
        ret
```


clang
```
init:
        mov     w8, #1048575
        adrp    x9, Data
        add     x9, x9, :lo12:Data
        stp     x8, xzr, [x9]
        ret
```


---


### compiler : `gcc`
### title : `Missed optimization of vector::at when a function is called inside the loop`
### open_at : `2023-04-06T18:15:27Z`
### last_modified_date : `2023-04-11T13:15:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109440
### status : `UNCONFIRMED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
#include<vector>
#include<string>
using namespace std;

bool bar();

using T = int;

T vat(std::vector<T> v) {
    T s;
    for (auto i = 0; i < v.size(); ++i) {
        if (bar())
            s += v.at(i);
    }

    return s;
}


$ gcc -O2 -fexceptions -fno-unroll-loops


.LC0:
        .string "vector::_M_range_check: __n (which is %zu) >= this->size() (which is %zu)"
vat(std::vector<int, std::allocator<int> >):
        mov     rax, QWORD PTR [rdi]
        cmp     QWORD PTR [rdi+8], rax
        je      .L9
        push    r12
        push    rbp
        mov     rbp, rdi
        push    rbx
        xor     ebx, ebx
        jmp     .L6
.L14:
        mov     rax, QWORD PTR [rbp+8]
        sub     rax, QWORD PTR [rbp+0]
        add     rbx, 1
        sar     rax, 2
        cmp     rbx, rax
        jnb     .L13
.L6:
        call    bar()
        test    al, al
        je      .L14
        mov     rcx, QWORD PTR [rbp+0]
        mov     rdx, QWORD PTR [rbp+8]
        sub     rdx, rcx
        sar     rdx, 2
        mov     rax, rdx
        cmp     rbx, rdx
        jnb     .L15
        add     r12d, DWORD PTR [rcx+rbx*4]
        add     rbx, 1
        cmp     rbx, rax
        jb      .L6
.L13:
        mov     eax, r12d
        pop     rbx
        pop     rbp
        pop     r12
        ret
.L9:
        mov     eax, r12d
        ret
.L15:
        mov     rsi, rbx
        mov     edi, OFFSET FLAT:.LC0
        xor     eax, eax
        call    std::__throw_out_of_range_fmt(char const*, ...)


---


### compiler : `gcc`
### title : `missed optimization when all elements of vector are known`
### open_at : `2023-04-06T18:31:37Z`
### last_modified_date : `2023-05-18T05:34:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109441
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Reference: https://godbolt.org/z/af4x6zhz9

When all elements of vector are 0, then the compiler should be able to remove the loop and just return 0.

Testcase:

#include<vector>
using namespace std;

using T = int;


T v() {
    T s;
    std::vector<T> v;
    v.resize(1000, 0);
    for (auto i = 0; i < v.size(); ++i) {
        s += v[i];
    }

    return s;
}



$ g++ -O3 -std=c++17

.LC0:
  .string "vector::_M_fill_insert"
v():
  push rbx
  pxor xmm0, xmm0
  mov edx, 1000
  xor esi, esi
  sub rsp, 48
  lea rcx, [rsp+12]
  lea rdi, [rsp+16]
  mov QWORD PTR [rsp+32], 0
  mov DWORD PTR [rsp+12], 0
  movaps XMMWORD PTR [rsp+16], xmm0
  call std::vector<int, std::allocator<int> >::_M_fill_insert(__gnu_cxx::__normal_iterator<int*, std::vector<int, std::allocator<int> > >, unsigned long, int const&)
  mov rdx, QWORD PTR [rsp+24]
  mov rdi, QWORD PTR [rsp+16]
  mov rax, rdx
  sub rax, rdi
  mov rsi, rax
  sar rsi, 2
  cmp rdx, rdi
  je .L99
  test rax, rax
  mov ecx, 1
  cmovne rcx, rsi
  cmp rax, 12
  jbe .L107
  mov rdx, rcx
  pxor xmm0, xmm0
  mov rax, rdi
  shr rdx, 2
  sal rdx, 4
  add rdx, rdi
.L101:
  movdqu xmm2, XMMWORD PTR [rax]
  add rax, 16
  paddd xmm0, xmm2
  cmp rdx, rax
  jne .L101
  movdqa xmm1, xmm0
  psrldq xmm1, 8
  paddd xmm0, xmm1
  movdqa xmm1, xmm0
  psrldq xmm1, 4
  paddd xmm0, xmm1
  movd ebx, xmm0
  test cl, 3
  je .L99
  and rcx, -4
  mov eax, ecx
.L100:
  lea edx, [rax+1]
  add ebx, DWORD PTR [rdi+rcx*4]
  movsx rdx, edx
  cmp rdx, rsi
  jnb .L99
  add eax, 2
  lea rcx, [0+rdx*4]
  add ebx, DWORD PTR [rdi+rdx*4]
  cdqe
  cmp rax, rsi
  jnb .L99
  add ebx, DWORD PTR [rdi+4+rcx]
.L99:
  test rdi, rdi
  je .L98
  mov rsi, QWORD PTR [rsp+32]
  sub rsi, rdi
  call operator delete(void*, unsigned long)
.L98:
  add rsp, 48
  mov eax, ebx
  pop rbx
  ret
.L107:
  xor eax, eax
  xor ecx, ecx
  jmp .L100
  mov rbx, rax
  jmp .L105
v() [clone .cold]:


---


### compiler : `gcc`
### title : `Dead local copy of std::vector not removed from function`
### open_at : `2023-04-06T19:00:54Z`
### last_modified_date : `2023-06-15T18:38:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109442
### status : `REOPENED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.0`
### severity : `normal`
### contents :
T vat1(std::vector<T> v1) {
    auto v = v1;
    return 10;
}

g++ -O3 -std=c++20 -fno-exceptions

vat1(std::vector<int, std::allocator<int> >):
        mov     rax, QWORD PTR [rdi+8]
        sub     rax, QWORD PTR [rdi]
        je      .L11
        push    rbp
        mov     rbp, rax
        movabs  rax, 9223372036854775804
        push    rbx
        sub     rsp, 8
        cmp     rax, rbp
        jb      .L15
        mov     rbx, rdi
        mov     rdi, rbp
        call    operator new(unsigned long)
        mov     rsi, QWORD PTR [rbx]
        mov     rdx, QWORD PTR [rbx+8]
        mov     rdi, rax
        sub     rdx, rsi
        cmp     rdx, 4
        jle     .L16
        call    memmove
        mov     rdi, rax
.L6:
        mov     rsi, rbp
        call    operator delete(void*, unsigned long)
        add     rsp, 8
        mov     eax, 10
        pop     rbx
        pop     rbp
        ret
.L11:
        mov     eax, 10
        ret
.L15:
        call    std::__throw_bad_array_new_length()
.L16:
        jne     .L6
        mov     eax, DWORD PTR [rsi]
        mov     DWORD PTR [rdi], eax
        jmp     .L6


---


### compiler : `gcc`
### title : `missed optimization of std::vector access (Related to issue 35269)`
### open_at : `2023-04-06T19:31:49Z`
### last_modified_date : `2023-06-15T18:04:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109443
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `c++`
### version : `13.0`
### severity : `normal`
### contents :
here is slightly modified code example from issue #35269. Both accesses are similar bug different code is generated. the function `h` has better codegen than `g` for some reason.


$ g++ -O3 -std=c++20 -fno-exceptions

void f(int);

void g(std::vector<int> v)
{
    for (std::vector<int>::size_type i = 0; i < v.size(); i++)
        f( v[ i ] );
}

void h(std::vector<int> v)
{
    for (std::vector<int>::const_iterator i = v.begin(); i != v.end(); ++i)
        f( *i );
}


g(std::vector<int, std::allocator<int> >):
        mov     rdx, QWORD PTR [rdi]
        cmp     QWORD PTR [rdi+8], rdx
        je      .L6
        push    rbp
        mov     rbp, rdi
        push    rbx
        xor     ebx, ebx
        sub     rsp, 8
.L3:
        mov     edi, DWORD PTR [rdx+rbx*4]
        add     rbx, 1
        call    f(int)
        mov     rdx, QWORD PTR [rbp+0]
        mov     rax, QWORD PTR [rbp+8]
        sub     rax, rdx
        sar     rax, 2
        cmp     rbx, rax
        jb      .L3
        add     rsp, 8
        pop     rbx
        pop     rbp
        ret
.L6:
        ret



h(std::vector<int, std::allocator<int> >):
        push    rbp
        push    rbx
        sub     rsp, 8
        mov     rbx, QWORD PTR [rdi]
        cmp     rbx, QWORD PTR [rdi+8]
        je      .L10
        mov     rbp, rdi
.L12:
        mov     edi, DWORD PTR [rbx]
        add     rbx, 4
        call    f(int)
        cmp     QWORD PTR [rbp+8], rbx
        jne     .L12
.L10:
        add     rsp, 8
        pop     rbx
        pop     rbp
        ret


---


### compiler : `gcc`
### title : `r13-6372-g822a11a1e642e0 regression due to noline with -Ofast -march=sapphirerapids -funroll-loops -flto, 541.leela_r performance decrease by 2-3%`
### open_at : `2023-04-07T05:30:19Z`
### last_modified_date : `2023-04-20T09:31:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109445
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
r13-6372-g822a11a1e642e0 regression due to noline with -Ofast -march=sapphirerapids -funroll-loops -flto, 541.leela_r performance decrease by 2-3%

Follow is the inline dump, left dump is before the commit, right dump is after the commit.

  <bb 104> [local count: 210861628]:               <bb 104> [local count: 210861628]:
  # DEBUG BEGIN_STMT                               # DEBUG BEGIN_STMT
  _466 = s_rng;                                    _466 = s_rng;
----------------------------------------------------------------------------------------------
  _607 = _466;                                <>   _561 = _466;
  _118 = _607;                                     _118 = _561;
----------------------------------------------------------------------------------------------
  _35 = this_72(D)->board.D.5191.m_empty_cnt; =    _35 = this_72(D)->board.D.5191.m_empty_cnt;
  _5 = _35 & 65535;                                _5 = _35 & 65535;
  # DEBUG this => _118                             # DEBUG this => _118
  max_458 = (const uint16) _5;                     max_458 = (const uint16) _5;
  # DEBUG max => max_458                           # DEBUG max => max_458
  # DEBUG BEGIN_STMT                               # DEBUG BEGIN_STMT
----------------------------------------------------------------------------------------------
  # DEBUG this => _118                        <>
  # DEBUG BEGIN_STMT
  # DEBUG mask => 4294967295
  # DEBUG BEGIN_STMT
  # DEBUG BEGIN_STMT
  _467 = _118->s1;
  _468 = _467 << 13;
  _469 = _467 ^ _468;
  b_470 = _469 >> 19;
  # DEBUG b => b_470
  # DEBUG BEGIN_STMT
  _471 = _467 << 12;
  _472 = _471 & 4294959104;
  _473 = b_470 ^ _472;
  _118->s1 = _473;
  # DEBUG BEGIN_STMT
  _474 = _118->s2;
  _475 = _474 << 2;
  _476 = _474 ^ _475;
  b_477 = _476 >> 25;
  # DEBUG b => b_477
  # DEBUG BEGIN_STMT
  _478 = _474 << 4;
  _479 = _478 & 4294967168;
  _480 = b_477 ^ _479;
  _118->s2 = _480;
  # DEBUG BEGIN_STMT
  _481 = _118->s3;
  _482 = _481 << 3;
  _483 = _481 ^ _482;
  b_484 = _483 >> 11;
  # DEBUG b => b_484
  # DEBUG BEGIN_STMT
  _485 = _481 << 17;
  _486 = _485 & 4292870144;
  _487 = b_484 ^ _486;
  _118->s3 = _487;
  # DEBUG BEGIN_STMT
  _488 = _473 ^ _480;
  _489 = _487 ^ _488;
  _611 = _489;                                     _459 = random (_118);
  # DEBUG this => NULL
  # DEBUG b => NULL
  _459 = _611;
----------------------------------------------------------------------------------------------
  _460 = _459 >> 16;                          =    _460 = _459 >> 16;
  _461 = (unsigned int) max_458;                   _461 = (unsigned int) max_458;
  _462 = _460 * _461;                              _462 = _460 * _461;
  _463 = _462 >> 16;                               _463 = _462 >> 16;
----------------------------------------------------------------------------------------------
  _612 = _463;                                <>   _563 = _463;
----------------------------------------------------------------------------------------------
  # DEBUG this => NULL                        =    # DEBUG this => NULL
  # DEBUG max => NULL                              # DEBUG max => NULL
----------------------------------------------------------------------------------------------
  _120 = _612;                                <>   _120 = _563;
----------------------------------------------------------------------------------------------
  vidx_121 = (int) _120;                      =    vidx_121 = (int) _120;
  # DEBUG vidx => vidx_121                         # DEBUG vidx => vidx_121
  # DEBUG BEGIN_STMT                               # DEBUG BEGIN_STMT
  _37 = this_72(D)->board.D.5191.m_tomove;         _37 = this_72(D)->board.D.5191.m_tomove;
----------------------------------------------------------------------------------------------
  # DEBUG D#1845 => 1                         <>   # DEBUG D#1824 => 1
----------------------------------------------------------------------------------------------
  # DEBUG this => this_72(D)                  =    # DEBUG this => this_72(D)
  # DEBUG color => _37                             # DEBUG color => _37
  # DEBUG vidx => vidx_121                         # DEBUG vidx => vidx_121
  # DEBUG allow_sa => 1                            # DEBUG allow_sa => 1
  # DEBUG BEGIN_STMT                               # DEBUG BEGIN_STMT
----------------------------------------------------------------------------------------------
  _495 = s_rng;                               <>   _472 = s_rng;
  if (_495 == 0B)                                  if (_472 == 0B)
----------------------------------------------------------------------------------------------
    goto <bb 105>; [17.43%]                   =      goto <bb 105>; [17.43%]
  else                                             else
    goto <bb 106>; [82.57%]                          goto <bb 106>; [82.57%]


  <bb 106> [local count: 210861628]:               <bb 106> [local count: 210861628]:
  # DEBUG BEGIN_STMT                               # DEBUG BEGIN_STMT
----------------------------------------------------------------------------------------------
  _497 = s_rng;                               <>   _474 = s_rng;
  _619 = _497;                                     _570 = _474;
  _420 = _619;                                     _420 = _570;
----------------------------------------------------------------------------------------------
  # DEBUG this => _420                        =    # DEBUG this => _420
  # DEBUG max => 2                                 # DEBUG max => 2
  # DEBUG BEGIN_STMT                               # DEBUG BEGIN_STMT
----------------------------------------------------------------------------------------------
  # DEBUG this => _420                        <>
  # DEBUG BEGIN_STMT
  # DEBUG mask => 4294967295
  # DEBUG BEGIN_STMT
  # DEBUG BEGIN_STMT
  _498 = _420->s1;
  _499 = _498 << 13;
  _500 = _498 ^ _499;
  b_501 = _500 >> 19;
  # DEBUG b => b_501
  # DEBUG BEGIN_STMT
  _502 = _498 << 12;
  _503 = _502 & 4294959104;
  _504 = b_501 ^ _503;
  _420->s1 = _504;
  # DEBUG BEGIN_STMT
  _505 = _420->s2;
  _506 = _505 << 2;
  _507 = _505 ^ _506;
  b_508 = _507 >> 25;
  # DEBUG b => b_508
  # DEBUG BEGIN_STMT
  _509 = _505 << 4;
  _510 = _509 & 4294967168;
  _511 = b_508 ^ _510;
  _420->s2 = _511;
  # DEBUG BEGIN_STMT
  _512 = _420->s3;
  _513 = _512 << 3;
  _514 = _512 ^ _513;
  b_515 = _514 >> 11;
  # DEBUG b => b_515
  # DEBUG BEGIN_STMT
  _516 = _512 << 17;
  _517 = _516 & 4292870144;
  _518 = b_515 ^ _517;
  _420->s3 = _518;
  # DEBUG BEGIN_STMT
  _519 = _504 ^ _511;
  _520 = _518 ^ _519;
  _623 = _520;
  # DEBUG this => NULL
  # DEBUG b => NULL
  _490 = _623;                                     _467 = random (_420);
  _491 = _490 >> 16;                               _468 = _467 >> 16;
  _492 = 2;                                        _469 = 2;
  _493 = _491 * _492;                              _470 = _468 * _469;
  _494 = _493 >> 16;                               _471 = _470 >> 16;
  _624 = _494;                                     _572 = _471;
----------------------------------------------------------------------------------------------
  # DEBUG this => NULL                        =    # DEBUG this => NULL
  # DEBUG max => NULL                              # DEBUG max => NULL
----------------------------------------------------------------------------------------------
  _421 = _624;                                <>   _421 = _572;
----------------------------------------------------------------------------------------------
  # DEBUG dir => (int) _421                   =    # DEBUG dir => (int) _421
  if (_421 == 0)                                   if (_421 == 0)
    goto <bb 110>; [50.00%]                          goto <bb 110>; [50.00%]
  else                                             else
    goto <bb 118>; [50.00%]                          goto <bb 118>; [50.00%]
----------------------------------------------------------------------------------------------


---


### compiler : `gcc`
### title : `[12/13/14 Regression] wrong uninitialized warning with std::variant of an empty class and std::exception_ptr`
### open_at : `2023-04-08T05:10:41Z`
### last_modified_date : `2023-07-13T20:38:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109448
### status : `NEW`
### tags : `diagnostic, missed-optimization, needs-reduction`
### component : `tree-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
```cpp
// test.cpp
#include <variant>
#include <vector>
#include <cassert>
#include <cstdint>
#include <exception>
#include <atomic>

template <typename T>
class Try {
public:
    Try() = default;
    ~Try() = default;

    Try(Try<T>&& other) = default;

    Try& operator=(Try<T>&& other) = default;
    Try& operator=(std::exception_ptr error) {
        if (std::holds_alternative<std::exception_ptr>(_value) &&
            std::get<std::exception_ptr>(_value) == error) {
            return *this;
        }

        _value.template emplace<std::exception_ptr>(error);
        return *this;
    }

    template <class... U>
    Try(U&&... value)
        requires std::is_constructible_v<T, U...>
        : _value(std::in_place_type<T>, std::forward<U>(value)...) {}

    Try(std::exception_ptr error) : _value(error) {}

private:
    Try(const Try&) = delete;
    Try& operator=(const Try&) = delete;

public:
    constexpr bool available() const noexcept {
        return !std::holds_alternative<std::monostate>(_value);
    }
    constexpr bool hasError() const noexcept {
        return std::holds_alternative<std::exception_ptr>(_value);
    }
    const T& value() const& {
        checkHasTry();
        return std::get<T>(_value);
    }
    T& value() & {
        checkHasTry();
        return std::get<T>(_value);
    }
    T&& value() && {
        checkHasTry();
        return std::move(std::get<T>(_value));
    }
    const T&& value() const&& {
        checkHasTry();
        return std::move(std::get<T>(_value));
    }

    template <class... Args>
    T& emplace(Args&&... args) {
        return _value.template emplace<T>(std::forward<Args>(args)...);
    }

    void setException(std::exception_ptr error) {
        if (std::holds_alternative<std::exception_ptr>(_value) &&
            std::get<std::exception_ptr>(_value) == error) {
            return;
        }
        _value.template emplace<std::exception_ptr>(error);
    }
    std::exception_ptr getException() const {
        logicAssert(std::holds_alternative<std::exception_ptr>(_value),
                    "Try object do not has on error");
        return std::get<std::exception_ptr>(_value);
    }
private:
    inline void checkHasTry() const {
        if (std::holds_alternative<T>(_value))
           { return; }
        else if (std::holds_alternative<std::exception_ptr>(_value)) {
            std::rethrow_exception(std::get<std::exception_ptr>(_value));
        } else if (std::holds_alternative<std::monostate>(_value)) {
            throw std::logic_error("Try object is empty");
        } else {
            assert(false);
        }
    }

private:
    std::variant<std::monostate, T, std::exception_ptr> _value;
};


namespace detail {

enum class State : uint8_t {
    START = 0,
    ONLY_RESULT = 1 << 0,
    ONLY_CONTINUATION = 1 << 1,
    DONE = 1 << 5,
};

constexpr State operator|(State lhs, State rhs) {
    return State((uint8_t)lhs | (uint8_t)rhs);
}

constexpr State operator&(State lhs, State rhs) {
    return State((uint8_t)lhs & (uint8_t)rhs);
}

}  // namespace detail

template <class T>
struct TestPromise {
public:
    TestPromise() : _state(detail::State::START) {}
    ~TestPromise() {}

public:
    bool hasResult() const noexcept {
        constexpr auto allow = detail::State::DONE | detail::State::ONLY_RESULT;
        auto state = _state.load(std::memory_order_acquire);
        return (state & allow) != detail::State();
    }

    void setResult(Try<T>&& value) {
        assert(!hasResult());
        _try_value = std::move(value);
    }

    std::atomic<detail::State> _state;
    Try<T> _try_value;
};

struct Empty {};

int main() {
    int n = 10;
    std::vector<TestPromise<Empty>> promise(n);
    for (int i = 0; i < n; ++i) {
        promise[i].setResult(Empty());
    }
}
```

# no problem
$ g++-11 -std=c++20 -Wall -Werror -O3 test.cpp

# no problem
$ g++ -std=c++20 -Wall -Werror -O0 test.cpp

# Errorï¼â*(std::__exception_ptr::exception_ptr*)((char*)&<unnamed> + offsetof(Try<Empty>,Try<Empty>::_value.std::variant<std::monostate, Empty, std::__exception_ptr::exception_ptr>::<unnamed>.std::__detail::__variant::_Variant_base<std::monostate, Empty, std::__exception_ptr::exception_ptr>::<unnamed>.std::__detail::__variant::_Move_assign_base<false, std::monostate, Empty, std::__exception_ptr::exception_ptr>::<unnamed>.std::__detail::__variant::_Copy_assign_base<false, std::monostate, Empty, std::__exception_ptr::exception_ptr>::<unnamed>.std::__detail::__variant::_Move_ctor_base<false, std::monostate, Empty, std::__exception_ptr::exception_ptr>::<unnamed>.std::__detail::__variant::_Copy_ctor_base<false, std::monostate, Empty, std::__exception_ptr::exception_ptr>::<unnamed>.std::__detail::__variant::_Variant_storage<false, std::monostate, Empty, std::__exception_ptr::exception_ptr>::_M_u)).std::__exception_ptr::exception_ptr::_M_exception_objectâ may be used uninitialized [-Werror=maybe-uninitialized]
$ g++ -std=c++20 -Wall -Werror -O3 test.cpp


---


### compiler : `gcc`
### title : `suboptimal sequence for converting 64-bit unsigned int to float`
### open_at : `2023-04-10T10:45:07Z`
### last_modified_date : `2023-04-11T13:38:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109463
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `enhancement`
### contents :
double f(uint64_t x) { return x; } gives:

test   rdi,rdi
js     10 <f+0x10>
pxor   xmm0,xmm0
cvtsi2sd xmm0,rdi
ret
nop
10:
mov    rax,rdi
and    edi,0x1
pxor   xmm0,xmm0
shr    rax,1
or     rax,rdi
cvtsi2sd xmm0,rax
addsd  xmm0,xmm0
ret

In particular, the sequence:

mov    rax,rdi
and    edi,0x1
shr    rax,1
or     rax,rdi
cvtsi2sd xmm0,rax

Can be replaced with:

movzx  eax,dil
shr    rdi,1
or     rdi,rax
cvtsi2sd xmm0,rdi

Since all 9 low bits of rdi are below the sticky bit, oring them together in any order suffices to round correctly.

Alternatively, in order to avoid clobbering rdi, use the following sequence:

mov    rax,rdi
shr    rax,1
or     al,dil
cvtsi2sd xmm0,rax

(The penalty for partial register access appears to be very cheap or nonexistent on recent uarchs.)


---


### compiler : `gcc`
### title : `Missing loop unrolling for small std::array?`
### open_at : `2023-04-11T10:42:51Z`
### last_modified_date : `2023-04-11T13:52:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109471
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
Hi,
I did some benchmarks where I wanted to bench a switch-case statement against a small std::vector. In GCC 12.2 the result was as expected: The switch-case statement was faster than the linear search in std::vector. But when I switched to clang 13 or above, the std::vector implementation was much faster than the switch-case statement!

You can the results here:
https://quick-bench.com/q/9DEDS7rQm0MnIFxwZt3A2iD86G0

Please switch here between GCC and clang.

Here an assembly output:

https://godbolt.org/z/j8oenP8x9

Kind regards
Stefano


---


### compiler : `gcc`
### title : `Missing optimization for 8bit/8bit multiplication / regression`
### open_at : `2023-04-11T19:19:16Z`
### last_modified_date : `2023-05-16T23:53:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109476
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
For avr-gcc > 4.6.4 the follwing function

uint16_t mul(const uint8_t a, const uint16_t b) {
    return static_cast<uint8_t>((b >> 8) + 0) * a ;
}

produces suboptimal

mul(unsigned char, unsigned int):
        mov r18,r23
        ldi r19,0
        mov r20,r24
        mul r20,r18
        movw r24,r0
        mul r20,r19
        add r25,r0
        clr __zero_reg__
        ret

whereas avr-gcc 4.6.4 produces optimal

mul(unsigned char, unsigned int):
        mul r23,r24
        movw r24,r0
        clr r1
        ret


---


### compiler : `gcc`
### title : `Unoptimal uncprop with assembler flag output`
### open_at : `2023-04-12T08:54:33Z`
### last_modified_date : `2023-04-12T19:43:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109483
### status : `NEW`
### tags : `inline-asm, missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Following testcase (int3 mnemonic is for marker only):


--cut here--
_Bool foo (int cnt)
{
  if (cnt == -1)
    {
      _Bool success;
      asm volatile("int3" : "=@ccz" (success));

      if (!success)
	return 0;
    }

  asm volatile ("" ::: "memory");
  return 1;
}
--cut here--

compiles w/ -O2 on x86_64 to:

0000000000000000 <foo>:
   0:   83 ff ff                cmp    $0xffffffff,%edi
   3:   74 0b                   je     10 <foo+0x10>
   5:   b8 01 00 00 00          mov    $0x1,%eax
   a:   c3                      retq   
   b:   0f 1f 44 00 00          nopl   0x0(%rax,%rax,1)
  10:   cc                      int3   
  11:   0f 94 c0                sete   %al
  14:   74 ef                   je     5 <foo+0x5>
  16:   c3                      retq   

Please note setting of %al before conditional jump. The instruction could be moved after the jump, where the register could be cleared using "xor %eax, %eax", similar to what clang creates:

0000000000000000 <foo>:
   0:   83 ff ff                cmp    $0xffffffff,%edi
   3:   75 06                   jne    b <foo+0xb>
   5:   cc                      int3   
   6:   74 03                   je     b <foo+0xb>
   8:   31 c0                   xor    %eax,%eax
   a:   c3                      retq   
   b:   b0 01                   mov    $0x1,%al
   d:   c3                      retq   

Also note that for ZF=1 gcc sets %al to 1, jumps to *5 where the register is again set to 1. This is not the case in the clang code.


---


### compiler : `gcc`
### title : `Optimization regression with pseudodestructors`
### open_at : `2023-04-12T13:00:58Z`
### last_modified_date : `2023-04-12T14:05:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109486
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.2.1`
### severity : `normal`
### contents :
Created attachment 54840
Source file, the same as in the text.

GCC 11 and higher seems to produce a loop when compiling the following code with O3 (attached):

template <typename T>
class SillyVector final {
    void clear() {
        while (ptr_ > data_) {
            (--ptr_)->~T();
        }
    }
    
private:
    T *data_, *ptr_;
};

template class SillyVector<long>;

The (abridged) amd64 assembly generated by GCC11 is:

_ZN11SillyVectorIlE5clearEv:
.LFB1:
        .cfi_startproc
        movq    8(%rdi), %rax
        movq    (%rdi), %rdx
        cmpq    %rdx, %rax
        jbe     .L1
        .p2align 4,,10
        .p2align 3
.L3:
        subq    $8, %rax
        movq    %rax, 8(%rdi)
        cmpq    %rdx, %rax
        ja      .L3
.L1:
        ret

GCC10 (and older versions of gcc in general) produce this:

_ZN11SillyVectorIlE5clearEv:
.LFB1:
        .cfi_startproc
        movq    8(%rdi), %rdx
        movq    (%rdi), %rcx
        cmpq    %rcx, %rdx
        jbe     .L1
        leaq    -1(%rdx), %rax
        subq    %rcx, %rax
        notq    %rax
        andq    $-8, %rax
        addq    %rdx, %rax
        movq    %rax, 8(%rdi)
.L1:
        ret


GCC10 version seems superior for the most cases.

Compilers used are
GCC10:
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-10/root/usr --mandir=/opt/rh/devtoolset-10/root/usr/share/man --infodir=/opt/rh/devtoolset-10/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --with-default-libstdcxx-abi=gcc4-compatible --enable-plugin --enable-initfini-array --with-isl=/builddir/build/BUILD/gcc-10.2.1-20210130/obj-x86_64-redhat-linux/isl-install --disable-libmpx --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.2.1 20210130 (Red Hat 10.2.1-11) (GCC) 

GCC11:
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-11/root/usr --mandir=/opt/rh/devtoolset-11/root/usr/share/man --infodir=/opt/rh/devtoolset-11/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --with-default-libstdcxx-abi=gcc4-compatible --enable-plugin --enable-initfini-array --with-isl=/builddir/build/BUILD/gcc-11.2.1-20210728/obj-x86_64-redhat-linux/isl-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.2.1 20210728 (Red Hat 11.2.1-1) (GCC) 

Intermediate file is omitted as no preprocessor is necessary to reproduce.
arm64 versions seem to exhibit the same behavior: loop in gcc11, no loop in gcc 10
Call to pseudo destructor is necessary for the issue to arise, it doesn't happen if such call is omitted


---


### compiler : `gcc`
### title : `Stack is used (unexpectedly) for copying on-heap objects (no problem in clang)`
### open_at : `2023-04-13T11:08:31Z`
### last_modified_date : `2023-04-14T08:12:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109495
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
Could be a bug or could be a feature -- can't tell. Please help understand why GCC involves stack in certain conditions when copying on-heap objects (no problem in clang).

Problem:

In actual live project I have a large struct/class with a lot of data in it ("a lot" is defined as larger than Wframe-larger-than), I've also got many locations where that struct is copied (directly or indirectly), and got Wframe-larger-than enabled. In some cases I'm hitting the error on lines that _indirectly_ copy-construct the object from an object that's stored on heap into an object that is also stored on heap (i.e. heap-to-heap operation, if you will).

After finding minimal case that reproduces the issue (https://godbolt.org/z/xcnP9E39a), disassembling the code and looking into potential reasons why GCC might want to use the stack (and thus trigger Wframe-larger-than), I see that (subjectively for no apparent reason) GCC involves stack as an intermediate buffer, which I am looking to avoid. clang doesn't use intermediate buffer in this same scenario. Some surprising observations (which make the problem go away) might be indicative of a bug in GCC:

1) If private member "y" of class Parent is made public, Wframe-larger-than goes away (https://godbolt.org/z/G7x5EWKEf);
2) If type of member "s" of struct Child is changed from std::string to int, the problem goes away (https://godbolt.org/z/zK6Wjj8nK);
3) If class Parent is adjusted so that there is no padding at the end, for example by changing type of "z" member of class Parent to short (https://godbolt.org/z/jsbsc6jcb) or by deleting member "z" altogether (https://godbolt.org/z/3jqaoKse9), the problem goes away;
4) If inheritance is taken out of the equation, for example by aggregating class Parent below class Child (instead of inheriting; https://godbolt.org/z/451K1s7bc), the problem goes away.

Expected behavior: GCC not reporting Wframe-larger-than.

Actual behavior: GCC reports Wframe-larger-than.


// Code (attached as test.cpp):

#include <string>

class Parent
{
private:
    unsigned char y[ 1024 * 64 ];           // Beef up class' size past Wframe-larger-than

public:
    short x;                                // sizeof == 2 -> increases alignment requirement to 2
    char z;                                 // sizeof == 1 -> triggers padding (1) at the end of struct
};

struct Child : public Parent
{
    std::string s;
};

int main( int, char** )
{
    auto* ptr = new Child();
    auto* ptr2 = new Child( *ptr );         // g++ reports frame-larger-than violation; clang doesn't.
    delete ptr2;
    delete ptr;

    return 0;
}

// Environment (gcc version):

$ g++ --version
g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Copyright (C) 2021 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
// but it could be any GCC version.

// Command line (gcc; https://godbolt.org/z/xcnP9E39a):

$ g++ -Werror -Wframe-larger-than=4096 -o test test.cpp
/code/Src/E2EE/dstepanovs.cpp: In copy constructor âChild::Child(const Child&)â:
/code/Src/E2EE/dstepanovs.cpp:14:8: error: the frame size of 65568 bytes is larger than 4096 bytes [-Werror=frame-larger-than=]
   14 | struct Child : public Parent
      |        ^~~~~
cc1plus: all warnings being treated as errors

// Compiling same code with same flags with clang yields no error (clang command line; https://godbolt.org/z/PMq5Yr3Tn):
$ clang++ -Werror -Wframe-larger-than=4096 -o test test.cpp


---


### compiler : `gcc`
### title : `SVE support for ctz`
### open_at : `2023-04-13T12:21:31Z`
### last_modified_date : `2023-04-13T14:21:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109498
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
We fail to vectorise the following function with SVE:

// -march=armv8.2-a+sve -O2
void
ctz (int *__restrict x, int *__restrict y, int n)
{
  for (int i = 0; i < n; i++)
    x[i] = __builtin_ctz (y[i]);
}

It should use a combination of RBIT + CLZ.


---


### compiler : `gcc`
### title : `Unnecessary zeroing in SVE loops`
### open_at : `2023-04-13T12:35:02Z`
### last_modified_date : `2023-04-13T14:19:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109499
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
The following two loops contain unnecessary zeroing operations:

// -march=armv8.2-a+sve -O2
void
f (int *__restrict x, int *__restrict y, int n)
{
  for (int i = 0; i < n; i++)
    x[i] = x[i] ? y[i] : 0;
}

void
g (int *__restrict x, int *__restrict y, int n)
{
  for (int i = 0; i < n; i++)
    x[i] = x[i] ? y[i] & 15 : 0;
}

Output:

f(int*, int*, int):
        cmp     w2, 0
        ble     .L1
        mov     x3, 0
        cntw    x4
        whilelo p0.s, wzr, w2
        mov     z1.b, #0
.L3:
        ld1w    z0.s, p0/z, [x0, x3, lsl 2]
        cmpne   p1.s, p0/z, z0.s, #0
        ld1w    z0.s, p1/z, [x1, x3, lsl 2]   // Sets inactive lanes to zero
        sel     z0.s, p1, z0.s, z1.s          // Not needed
        st1w    z0.s, p0, [x0, x3, lsl 2]
        add     x3, x3, x4
        whilelo p0.s, w3, w2
        b.any   .L3
.L1:
        ret
g(int*, int*, int):
        cmp     w2, 0
        ble     .L6
        mov     x3, 0
        cntw    x4
        whilelo p0.s, wzr, w2
        mov     z1.s, #15
.L8:
        ld1w    z0.s, p0/z, [x0, x3, lsl 2]
        cmpne   p1.s, p0/z, z0.s, #0
        ld1w    z0.s, p1/z, [x1, x3, lsl 2]   // Sets inactive lanes to zero
        movprfx z0.s, p1/z, z0.s              // Not needed
        and     z0.s, p1/m, z0.s, z1.s        // Could be AND (immediate)
        st1w    z0.s, p0, [x0, x3, lsl 2]
        add     x3, x3, x4
        whilelo p0.s, w3, w2
        b.any   .L8
.L6:
        ret

It would be good to model somehow that IFN_MASK_LOAD has a zeroing effect on AArch64, so that this is exposed at the gimple level.  At the same time, we probably don't want the behaviour of the ifn to depend on target hooks.  Not sure what the best design is here.


---


### compiler : `gcc`
### title : `Missed Dead Code Elimination when using __builtin_unreachable`
### open_at : `2023-04-14T13:15:42Z`
### last_modified_date : `2023-04-17T07:31:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109513
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
cat input.c

void foo(void);
void bar(void);

static char a, h;
static int b, c, d, k;
static int **e;
static int ***f = &e;
static short g;
static int *j, *l = &c;

static char m(int *i) {
  if (*i)
    return 0;
  for (; c;)
    ;
  if (i == &b)
    bar(); 
  return 1;
}
int main() {
  int *n = &b;
  for (; k; ++k)
    m(n);
  h = a + 10;
  if (h) {
    int *o = &d;
    for (; g; ++g) {
      *e = l;
      j = **f;
      o = j;
    }
    if (!(o == &c || o == &d))
      foo();
  }
}

In the above piece of code the calls to foo and bar are both dead

gcc-trunk at -O3 generates the following code:
main:
        subq    $8, %rsp
        movl    k(%rip), %ecx
        testl   %ecx, %ecx
        je      .L7
.L2:
        movl    c(%rip), %edx
        testl   %edx, %edx
        je      .L5
.L6:
        jmp     .L6
.L5:
        call    bar
        addl    $1, k(%rip)
        jne     .L2
.L7:
        cmpw    $0, g(%rip)
        je      .L4
        movq    e(%rip), %rax
        movq    $c, (%rax)
        xorl    %eax, %eax
        movw    %ax, g(%rip)
.L4:
        xorl    %eax, %eax
        addq    $8, %rsp
        ret

it eliminates the call to foo but not to bar. If I try to "help" the compiler by replacing bar() with __builtin_unreachable(), it generates worse code and it also misses eliminating the dead call to foo:

void foo(void);

static char a, h;
static int b, c, d, k;
static int **e;
static int ***f = &e;
static short g;
static int *j, *l = &c;

static char m(int *i) {
  if (*i)
    return 0;
  for (; c;)
    ;
  if (i == &b)
    __builtin_unreachable(); // <- the call to bar was here
  return 1;
}

int main() {
  int *n = &b;
  for (; k; ++k)
    m(n);
  h = a + 10;
  if (h) {
    int *o = &d;
    for (; g; ++g) {
      *e = l;
      j = **f;
      o = j;
    }
    if (!(o == &c || o == &d))
      foo();
  }
}

gcc-trunk -O3 output:

main:
        movl    k(%rip), %ecx
        testl   %ecx, %ecx
        je      .L22
.L2:
        jmp     .L2
.L22:
        movq    e(%rip), %rsi
        movzwl  g(%rip), %eax
        xorl    %ecx, %ecx
        movl    $d, %edx
        jmp     .L3
.L4:
        movq    $c, (%rsi)
        addl    $1, %eax
        movl    $1, %ecx
        movl    $c, %edx
.L3:
        testw   %ax, %ax
        jne     .L4
        testb   %cl, %cl
        je      .L5
        movw    $0, g(%rip)
.L5:
        cmpq    $d, %rdx
        je      .L18
        cmpq    $c, %rdx
        je      .L18
        pushq   %rax
        call    foo
        xorl    %eax, %eax
        popq    %rdx
        ret
.L18:
        xorl    %eax, %eax
        ret

This is also an old regression. gcc-7.4 at -O3 generates for the version with __builtin_unreachable:

main:
        movl    k(%rip), %eax
        testl   %eax, %eax
        je      .L7
.L2:
        jmp     .L2
.L7:
        cmpw    $0, g(%rip)
        je      .L4
        movq    e(%rip), %rax
        movw    $0, g(%rip)
        movq    $c, (%rax)
.L4:
        xorl    %eax, %eax
        ret


---


### compiler : `gcc`
### title : `redundant register assignment`
### open_at : `2023-04-15T16:15:08Z`
### last_modified_date : `2023-04-17T07:38:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109527
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
On this function

short test(short* a)
{
    *a = 1;
    return *a;
}


latest gcc -O2 generates:

test(short*):
        mov     eax, 1
        mov     WORD PTR [rdi], ax
        mov     eax, 1
        ret

I believe the second assignment to eax is redundant and can be removed:

test(short*):
        mov     eax, 1
        mov     WORD PTR [rdi], ax
        ret


---


### compiler : `gcc`
### title : `Warning "is used uninitialized" raised for an initialized variable.`
### open_at : `2023-04-16T07:20:29Z`
### last_modified_date : `2023-04-17T08:02:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109530
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `12.2.0`
### severity : `normal`
### contents :
For the following program:

#include <stdio.h>
#include <setjmp.h>

jmp_buf buf;
jmp_buf buf1;

typedef struct ry_s {
  struct ry_s *next;
} ry_t[1];

struct ry_s *m_global;

static inline void
ry_init(ry_t state)
{
  m_global = state;
}

static inline void
ry_clear(ry_t state)
{
  m_global = state->next;
}

int func2(void)
{
  for(_Bool b = 1 ; b ; b = 0)
    for(ry_t test1 ; b ; ry_clear(test1), b = 0 )
      if (ry_init(test1), setjmp (buf) == 0)
        {
          FILE *f = fopen("tmp","wt");
          if ((setjmp (buf1) == 0) )
            {
              fprintf(f, "This is a text\n");
              fclose(f);
            }
        }
  return 0;
}


When running with "-O2  -Wall -Wextra", I get the following warning:

<source>: In function 'func2':
<source>:31:17: warning: 'f' is used uninitialized [-Wuninitialized]
   31 |           FILE *f = fopen("tmp","wt");

whereas f is obviously initialized in the statement.
I am confused by this warning.


---


### compiler : `gcc`
### title : `Improve code generation for dynamic loop unrolling`
### open_at : `2023-04-17T13:24:32Z`
### last_modified_date : `2023-04-18T08:44:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109537
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
For the current dynamic loop unrolling implementation, we will try to do the followng transform:
'''
for (i = 0; i < n; i++)
     body;

   ==>  (LOOP->LPT_DECISION.TIMES == 3)

   i = 0;
   mod = n % 4;

   switch (mod)
     {
       case 3:
         body; i++;
       case 2:
         body; i++;
       case 1:
         body; i++;
       case 0: ;
     }

   while (i < n)
     {
       body; i++;
       body; i++;
       body; i++;
       body; i++;
     }
'''
 
It would be better if we could carry out loop unrolling in the following way (R == # unrolls)

'''
i=0; if (i > n-R-1) goto remain (not needed with loop bounds as shown)
for(; i< n-R-1; i+= R)
  {
  body;
  body;
  ...
  body;  // R times
  }
remain:
if (i < n)
  for(; i < n; i++)
    body
'''

For the following sample code:
'''
void matrix_add_const(int N1, int* A, int val)
{
  int i, j;
  for (j = 0; j < N1; j++)
    A[j] += val;
}
'''

Gcc will generate more jump instructions compared to clang.

gcc's assemly
'''
$ cat unroll.gcc.s 
	.file	"unroll.c"
	.text
	.p2align 4
	.globl	matrix_add_const
	.type	matrix_add_const, @function
matrix_add_const:
.LFB0:
	.cfi_startproc
	testl	%edi, %edi
	jle	.L1
	movslq	%edi, %rdi
	leaq	-4(,%rdi,4), %rax
	leaq	(%rsi,%rdi,4), %rcx
	shrq	$2, %rax
	addq	$1, %rax
	andl	$7, %eax
	je	.L3
	cmpq	$1, %rax
	je	.L26
	cmpq	$2, %rax
	je	.L27
	cmpq	$3, %rax
	je	.L28
	cmpq	$4, %rax
	je	.L29
	cmpq	$5, %rax
	je	.L30
	cmpq	$6, %rax
	jne	.L41
.L31:
	addl	%edx, (%rsi)
	addq	$4, %rsi
.L30:
	addl	%edx, (%rsi)
	addq	$4, %rsi
.L29:
	addl	%edx, (%rsi)
	addq	$4, %rsi
.L28:
	addl	%edx, (%rsi)
	addq	$4, %rsi
.L27:
	addl	%edx, (%rsi)
	addq	$4, %rsi
.L26:
	addl	%edx, (%rsi)
	addq	$4, %rsi
	cmpq	%rcx, %rsi
	je	.L42
.L3:
	addl	%edx, (%rsi)
	addl	%edx, 4(%rsi)
	addl	%edx, 8(%rsi)
	addl	%edx, 12(%rsi)
	addl	%edx, 16(%rsi)
	addl	%edx, 20(%rsi)
	addl	%edx, 24(%rsi)
	addl	%edx, 28(%rsi)
	addq	$32, %rsi
	cmpq	%rcx, %rsi
	jne	.L3
.L1:
	ret
	.p2align 4,,10
	.p2align 3
.L41:
	addl	%edx, (%rsi)
	addq	$4, %rsi
	jmp	.L31
.L42:
	ret
	.cfi_endproc
.LFE0:
	.size	matrix_add_const, .-matrix_add_const
	.ident	"GCC: (GNU) 13.0.0 20221022 (experimental)"
	.section	.note.GNU-stack,"",@progbits
'''

clang's assembly
'''
$ cat unroll.clang.s 
	.text
	.file	"unroll.c"
	.globl	matrix_add_const        # -- Begin function matrix_add_const
	.p2align	4, 0x90
	.type	matrix_add_const,@function
matrix_add_const:                       # @matrix_add_const
	.cfi_startproc
# %bb.0:
	testl	%edi, %edi
	jle	.LBB0_11
# %bb.1:
	movl	%edi, %r9d
	cmpl	$8, %edi
	jae	.LBB0_3
# %bb.2:
	xorl	%ecx, %ecx
	jmp	.LBB0_10
.LBB0_3:
	movl	%r9d, %ecx
	andl	$-8, %ecx
	movd	%edx, %xmm0
	pshufd	$0, %xmm0, %xmm0        # xmm0 = xmm0[0,0,0,0]
	leaq	-8(%rcx), %rax
	movq	%rax, %rdi
	shrq	$3, %rdi
	addq	$1, %rdi
	movl	%edi, %r8d
	andl	$1, %r8d
	testq	%rax, %rax
	je	.LBB0_4
# %bb.5:
	movq	%r8, %rax
	subq	%rdi, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB0_6:                                # =>This Inner Loop Header: Depth=1
	movdqu	(%rsi,%rdi,4), %xmm1
	movdqu	16(%rsi,%rdi,4), %xmm2
	movdqu	32(%rsi,%rdi,4), %xmm3
	movdqu	48(%rsi,%rdi,4), %xmm4
	paddd	%xmm0, %xmm1
	paddd	%xmm0, %xmm2
	movdqu	%xmm1, (%rsi,%rdi,4)
	movdqu	%xmm2, 16(%rsi,%rdi,4)
	paddd	%xmm0, %xmm3
	paddd	%xmm0, %xmm4
	movdqu	%xmm3, 32(%rsi,%rdi,4)
	movdqu	%xmm4, 48(%rsi,%rdi,4)
	addq	$16, %rdi
	addq	$2, %rax
	jne	.LBB0_6
# %bb.7:
	testq	%r8, %r8
	je	.LBB0_9
.LBB0_8:
	movdqu	(%rsi,%rdi,4), %xmm1
	movdqu	16(%rsi,%rdi,4), %xmm2
	paddd	%xmm0, %xmm1
	paddd	%xmm0, %xmm2
	movdqu	%xmm1, (%rsi,%rdi,4)
	movdqu	%xmm2, 16(%rsi,%rdi,4)
.LBB0_9:
	cmpq	%r9, %rcx
	je	.LBB0_11
	.p2align	4, 0x90
.LBB0_10:                               # =>This Inner Loop Header: Depth=1
	addl	%edx, (%rsi,%rcx,4)
	addq	$1, %rcx
	cmpq	%rcx, %r9
	jne	.LBB0_10
.LBB0_11:
	retq
.LBB0_4:
	xorl	%edi, %edi
	testq	%r8, %r8
	jne	.LBB0_8
	jmp	.LBB0_9
.Lfunc_end0:
	.size	matrix_add_const, .Lfunc_end0-matrix_add_const
	.cfi_endproc
                                        # -- End function
	.ident	"clang version 10.0.0-4ubuntu1 "
	.section	".note.GNU-stack","",@progbits
	.addrsig

'''


---


### compiler : `gcc`
### title : `Avoid using BLKmode for unions with a non-BLKmode member when possible`
### open_at : `2023-04-18T10:57:43Z`
### last_modified_date : `2023-07-19T13:26:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109543
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Hi,

So with the following C-code:
$ cat t.c
#include <arm_neon.h>
#ifdef GOOD
typedef float64x2x2_t TYPE;
#else
typedef union
{
      float64x2x2_t v;
      double d[4];
  } TYPE;
  #endif


void foo (TYPE *a, TYPE *b, TYPE *c, unsigned  n)
{
 TYPE X = a[0];
 TYPE Y = b[0];
 TYPE Z = c[0];
 for (unsigned  i = 0; i < n; ++n)
 {
  TYPE temp = X;
  X = Y;
  Y = Z;
  Z = temp;
 }
 a[0] = X;
 b[0] = Y;
 c[0] = Z;
}

If compiled for aarch64 with -DGOOD the compiler will use vector register moves in the loop, whereas without -DGOOD it will use the stack with memmoves.

The reason for this is because when picking the mode to address a UNION with gcc will always choose BLKmode as soon as any member of a UNION is BLKmode. In such cases I think it would be safe to go with not-BLKmode of members that have the same size as the entire UNION?


---


### compiler : `gcc`
### title : `[13/14 Regression]  Missed Dead Code Elimination when using __builtin_unreachable since r13-3596-ge7310e24b1c0ca`
### open_at : `2023-04-18T15:07:29Z`
### last_modified_date : `2023-09-17T04:10:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109546
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `normal`
### contents :
cat input.c 

void foo(void);
static int a, c;
static int *b = &a;
static int **d = &b;
void __assert_fail() __attribute__((__noreturn__));
int main() {
  int *e = *d;
  if (e == &a || e == &c);
  else {
  __assert_fail();
  }
  if (e == &a || e == &c);
  else
    foo();
}

both gcc 12 and trunk at -O3 generate the following code:

main:
        movq    b(%rip), %rax
        cmpq    $c, %rax
        je      .L2
        cmpq    $a, %rax
        jne     .L7
.L2:
        xorl    %eax, %eax
        ret
.L7:
        pushq   %rax
        xorl    %eax, %eax
        call    __assert_fail
b:
        .quad   a

the call to foo is eliminated but the call to __assert_fail is missed, even though 
both if statement conditions are identical. 

If add a __builtin_unreachable before the call to __assert_fail the opposite happens:

void foo(void);
static int a, c;
static int *b = &a;
static int **d = &b;
void assert_fail() __attribute__((__noreturn__));
int main() {
  int *e = *d;
  if (e == &a || e == &c);
  else {
    __builtin_unreachable(); 
  assert_fail();
  }
  if (e == &a || e == &c);
  else
    foo();
}

gcc-trunk -O3 generates:

main:
        movq    b(%rip), %rax
        cmpq    $c, %rax
        je      .L4
        cmpq    $a, %rax
        je      .L4
        pushq   %rax
        call    foo
        xorl    %eax, %eax
        popq    %rdx
        ret
.L4:
        xorl    %eax, %eax
        ret
b:
        .quad   a

the call to __assert_fail is now properly eliminated but now the call to foo is missed. 

This is a regression as gcc-12 generates: 

main:
        xorl    %eax, %eax
        ret


---


### compiler : `gcc`
### title : `[13] RISC-V: Multiple vsetvli for load/store loop`
### open_at : `2023-04-18T22:24:15Z`
### last_modified_date : `2023-05-29T12:47:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109547
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
I was just poking around with a simple loop using the vector intrinsics and found some odd generated code.  This is on the gcc-13 branch, but that's pretty close to trunk so I'm filing it for 14.  I'm probably not going to have time to look for a bit, as it seems to just be a performance issue.

$ cat test.c
#include <riscv_vector.h>

void func(unsigned char *out, unsigned char *in, unsigned long len) {
  unsigned long i = 0;
  while (i < len) {
    unsigned long vl = __riscv_vsetvl_e8m1(len - i);
    vuint8m1_t r = __riscv_vle8_v_u8m1(in + i, vl);
    __riscv_vse8_v_u8m1(out + i, r, vl);
    i += vl;
  }
}
$ ../toolchain/install/bin/riscv64-unknown-linux-gnu-gcc test.c -O3 -c -S -o- -march=rv64gcv -fdump-rtl-all
        .file   "test.c"
        .option nopic
        .attribute arch, "rv64i2p0_m2p0_a2p0_f2p0_d2p0_c2p0_v1p0_zve32f1p0_zve32x1p0_zve64d1p0_zve64f1p0_zve64x1p0_zvl128b1p0_zvl32b1p0_zvl64b1p0"
        .attribute unaligned_access, 0
        .attribute stack_align, 16
        .text
        .align  1
        .globl  func
        .type   func, @function
func:
.LFB2:
        .cfi_startproc
        beq     a2,zero,.L1
        li      a5,0
.L3:
        sub     a4,a2,a5
        add     a6,a1,a5
        add     a3,a0,a5
        vsetvli a4,a4,e8,m1,ta,mu
        vsetvli zero,a4,e8,m1,ta,ma
        add     a5,a5,a4
        vle8.v  v24,0(a6)
        vse8.v  v24,0(a3)
        bgtu    a2,a5,.L3
.L1:
        ret
        .cfi_endproc
.LFE2:
        .size   func, .-func
        .ident  "GCC: (g85b95ea729c) 13.0.1 20230417 (prerelease)"
        .section        .note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `memcmp with string literals cause branchy code maybe could use branchless?`
### open_at : `2023-04-19T12:39:21Z`
### last_modified_date : `2023-09-25T00:38:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109555
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `enhancement`
### contents :
hello!


the example:

#include <cstring>
int main(int argc, char **argv) {
    return std::memcmp(argv[0], "MCRYF", 5) == 0;
}


this code compiled with GCC (-std=c++11 -O2) will be translated as the following assembler code (https://godbolt.org/z/zzjaGdWdT):

main:
        mov     rax, QWORD PTR [rsi]
        cmp     DWORD PTR [rax], 1498563405
        je      .L5
.L2:
        mov     eax, 1
.L3:
        xor     eax, 1
        ret
.L5:
        cmp     BYTE PTR [rax+4], 70
        jne     .L2
        xor     eax, eax
        jmp     .L3



but if the code is compiled using CLang, we get a more optimal branch-less assembler code (https://godbolt.org/z/E9o1oMzra):

main:                                   # @main
        mov     rax, qword ptr [rsi]
        mov     ecx, 1498563405
        xor     ecx, dword ptr [rax]
        movzx   edx, byte ptr [rax + 4]
        xor     edx, 70
        xor     eax, eax
        or      edx, ecx
        sete    al
        ret



maybe it's not a difficult fix, and maybe someone will see the point to improve this...


thanks!


---


### compiler : `gcc`
### title : `-Wmaybe-uninitialized false positive with std::optional`
### open_at : `2023-04-20T01:34:21Z`
### last_modified_date : `2023-04-20T07:09:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109561
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `12.2.0`
### severity : `normal`
### contents :
(code and error below)

I don't know how to put this nicely, this is a terrible message:

- Most of it consists of standard library internals.

- The actual message ("may be used uninitialized") is buried well within it and also refers to internal fields.

- It is reported no less than 5 times!?

- Most of all, AFAICS, it's bogus. The default constructor of std::optional should construct an object that does not contain a value and such objects can be copied just fine.

- Like many such bogus warnings apparently, it's rather volatile. Unrelated changes, e.g. to i or removing the defaulted constructor declaration, can make it disappear. Producing this minimal example was a lot of guesswork what can be removed in which order without disturbing the message.

- It's a regression from 10.2.1 and 11.3.

% cat test.cpp
#include <optional>
#include <vector>
#include <string>

struct O
{
  std::optional <int> i;
  std::optional <std::string> s;
  O () = default;
};

std::vector <O> a;

void f ()
{
  for (auto &i: a)
    i = { };
}
% g++ --std=c++20 -O2 -Wall -c test.cpp
In file included from /usr/include/c++/12/string:53,
                 from test.cpp:3:
In member function âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::pointer std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_data() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â,
    inlined from âconstexpr bool std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_is_local() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â at /usr/include/c++/12/bits/basic_string.h:274:23,
    inlined from âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::operator=(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â at /usr/include/c++/12/bits/basic_string.h:859:23,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_move_assign(std::_Optional_payload_base<_Tp>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:194:19,
    inlined from âconstexpr std::_Optional_payload<_Tp, true, false, false>& std::_Optional_payload<_Tp, true, false, false>::operator=(std::_Optional_payload<_Tp, true, false, false>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:420:22,
    inlined from âconstexpr std::_Optional_payload<_Tp, false, _Copy, _Move>& std::_Optional_payload<_Tp, false, _Copy, _Move>::operator=(std::_Optional_payload<_Tp, false, _Copy, _Move>&&) [with _Tp = std::__cxx11::basic_string<char>; bool _Copy = false; bool _Move = false]â at /usr/include/c++/12/optional:436:26,
    inlined from âconstexpr std::_Optional_base<_Tp, <anonymous>, <anonymous> >& std::_Optional_base<_Tp, <anonymous>, <anonymous> >::operator=(std::_Optional_base<_Tp, <anonymous>, <anonymous> >&&) [with _Tp = std::__cxx11::basic_string<char>; bool <anonymous> = false; bool <anonymous> = false]â at /usr/include/c++/12/optional:550:23,
    inlined from âconstexpr std::optional<std::__cxx11::basic_string<char> >& std::optional<std::__cxx11::basic_string<char> >::operator=(std::optional<std::__cxx11::basic_string<char> >&&)â at /usr/include/c++/12/optional:705:11,
    inlined from âconstexpr O& O::operator=(O&&)â at test.cpp:5:8,
    inlined from âvoid f()â at test.cpp:17:11:
/usr/include/c++/12/bits/basic_string.h:234:28: warning: â*(const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)((char*)&<unnamed> + offsetof(O, O::s.std::optional<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::<unnamed>.std::_Optional_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false>::<unnamed>)).std::__cxx11::basic_string<char>::_M_dataplus.std::__cxx11::basic_string<char>::_Alloc_hider::_M_pâ may be used uninitialized [-Wmaybe-uninitialized]
  234 |       { return _M_dataplus._M_p; }
      |                            ^~~~
test.cpp: In function âvoid f()â:
test.cpp:17:11: note: â<anonymous>â declared here
   17 |     i = { };
      |           ^
In member function âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â,
    inlined from âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::operator=(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â at /usr/include/c++/12/bits/basic_string.h:866:17,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_move_assign(std::_Optional_payload_base<_Tp>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:194:19,
    inlined from âconstexpr std::_Optional_payload<_Tp, true, false, false>& std::_Optional_payload<_Tp, true, false, false>::operator=(std::_Optional_payload<_Tp, true, false, false>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:420:22,
    inlined from âconstexpr std::_Optional_payload<_Tp, false, _Copy, _Move>& std::_Optional_payload<_Tp, false, _Copy, _Move>::operator=(std::_Optional_payload<_Tp, false, _Copy, _Move>&&) [with _Tp = std::__cxx11::basic_string<char>; bool _Copy = false; bool _Move = false]â at /usr/include/c++/12/optional:436:26,
    inlined from âconstexpr std::_Optional_base<_Tp, <anonymous>, <anonymous> >& std::_Optional_base<_Tp, <anonymous>, <anonymous> >::operator=(std::_Optional_base<_Tp, <anonymous>, <anonymous> >&&) [with _Tp = std::__cxx11::basic_string<char>; bool <anonymous> = false; bool <anonymous> = false]â at /usr/include/c++/12/optional:550:23,
    inlined from âconstexpr std::optional<std::__cxx11::basic_string<char> >& std::optional<std::__cxx11::basic_string<char> >::operator=(std::optional<std::__cxx11::basic_string<char> >&&)â at /usr/include/c++/12/optional:705:11,
    inlined from âconstexpr O& O::operator=(O&&)â at test.cpp:5:8,
    inlined from âvoid f()â at test.cpp:17:11:
/usr/include/c++/12/bits/basic_string.h:1064:16: warning: â*(const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)((char*)&<unnamed> + offsetof(O, O::s.std::optional<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::<unnamed>.std::_Optional_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false>::<unnamed>)).std::__cxx11::basic_string<char>::_M_string_lengthâ may be used uninitialized [-Wmaybe-uninitialized]
 1064 |       { return _M_string_length; }
      |                ^~~~~~~~~~~~~~~~
test.cpp: In function âvoid f()â:
test.cpp:17:11: note: â<anonymous>â declared here
   17 |     i = { };
      |           ^
In member function âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â,
    inlined from âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::operator=(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â at /usr/include/c++/12/bits/basic_string.h:866:17,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_move_assign(std::_Optional_payload_base<_Tp>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:194:19,
    inlined from âconstexpr std::_Optional_payload<_Tp, true, false, false>& std::_Optional_payload<_Tp, true, false, false>::operator=(std::_Optional_payload<_Tp, true, false, false>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:420:22,
    inlined from âconstexpr std::_Optional_payload<_Tp, false, _Copy, _Move>& std::_Optional_payload<_Tp, false, _Copy, _Move>::operator=(std::_Optional_payload<_Tp, false, _Copy, _Move>&&) [with _Tp = std::__cxx11::basic_string<char>; bool _Copy = false; bool _Move = false]â at /usr/include/c++/12/optional:436:26,
    inlined from âconstexpr std::_Optional_base<_Tp, <anonymous>, <anonymous> >& std::_Optional_base<_Tp, <anonymous>, <anonymous> >::operator=(std::_Optional_base<_Tp, <anonymous>, <anonymous> >&&) [with _Tp = std::__cxx11::basic_string<char>; bool <anonymous> = false; bool <anonymous> = false]â at /usr/include/c++/12/optional:550:23,
    inlined from âconstexpr std::optional<std::__cxx11::basic_string<char> >& std::optional<std::__cxx11::basic_string<char> >::operator=(std::optional<std::__cxx11::basic_string<char> >&&)â at /usr/include/c++/12/optional:705:11,
    inlined from âconstexpr O& O::operator=(O&&)â at test.cpp:5:8,
    inlined from âvoid f()â at test.cpp:17:11:
/usr/include/c++/12/bits/basic_string.h:1064:16: warning: â*(const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)((char*)&<unnamed> + offsetof(O, O::s.std::optional<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::<unnamed>.std::_Optional_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false>::<unnamed>)).std::__cxx11::basic_string<char>::_M_string_lengthâ may be used uninitialized [-Wmaybe-uninitialized]
 1064 |       { return _M_string_length; }
      |                ^~~~~~~~~~~~~~~~
test.cpp: In function âvoid f()â:
test.cpp:17:11: note: â<anonymous>â declared here
   17 |     i = { };
      |           ^
In member function âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::length() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â,
    inlined from âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::basic_string(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â at /usr/include/c++/12/bits/basic_string.h:676:22,
    inlined from âconstexpr void std::_Construct(_Tp*, _Args&& ...) [with _Tp = __cxx11::basic_string<char>; _Args = {__cxx11::basic_string<char, char_traits<char>, allocator<char> >}]â at /usr/include/c++/12/bits/stl_construct.h:119:7,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_construct(_Args&& ...) [with _Args = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >}; _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:278:19,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_move_assign(std::_Optional_payload_base<_Tp>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:198:26,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_move_assign(std::_Optional_payload_base<_Tp>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:189:7,
    inlined from âconstexpr std::_Optional_payload<_Tp, true, false, false>& std::_Optional_payload<_Tp, true, false, false>::operator=(std::_Optional_payload<_Tp, true, false, false>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:420:22,
    inlined from âconstexpr std::_Optional_payload<_Tp, false, _Copy, _Move>& std::_Optional_payload<_Tp, false, _Copy, _Move>::operator=(std::_Optional_payload<_Tp, false, _Copy, _Move>&&) [with _Tp = std::__cxx11::basic_string<char>; bool _Copy = false; bool _Move = false]â at /usr/include/c++/12/optional:436:26,
    inlined from âconstexpr std::_Optional_base<_Tp, <anonymous>, <anonymous> >& std::_Optional_base<_Tp, <anonymous>, <anonymous> >::operator=(std::_Optional_base<_Tp, <anonymous>, <anonymous> >&&) [with _Tp = std::__cxx11::basic_string<char>; bool <anonymous> = false; bool <anonymous> = false]â at /usr/include/c++/12/optional:550:23,
    inlined from âconstexpr std::optional<std::__cxx11::basic_string<char> >& std::optional<std::__cxx11::basic_string<char> >::operator=(std::optional<std::__cxx11::basic_string<char> >&&)â at /usr/include/c++/12/optional:705:11,
    inlined from âconstexpr O& O::operator=(O&&)â at test.cpp:5:8,
    inlined from âvoid f()â at test.cpp:17:11:
/usr/include/c++/12/bits/basic_string.h:1071:16: warning: â*(const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)((char*)&<unnamed> + offsetof(O, O::s.std::optional<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::<unnamed>.std::_Optional_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false>::<unnamed>)).std::__cxx11::basic_string<char>::_M_string_lengthâ may be used uninitialized [-Wmaybe-uninitialized]
 1071 |       { return _M_string_length; }
      |                ^~~~~~~~~~~~~~~~
test.cpp: In function âvoid f()â:
test.cpp:17:11: note: â<anonymous>â declared here
   17 |     i = { };
      |           ^
In member function âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::length() const [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â,
    inlined from âconstexpr std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::basic_string(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>&&) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]â at /usr/include/c++/12/bits/basic_string.h:676:22,
    inlined from âconstexpr void std::_Construct(_Tp*, _Args&& ...) [with _Tp = __cxx11::basic_string<char>; _Args = {__cxx11::basic_string<char, char_traits<char>, allocator<char> >}]â at /usr/include/c++/12/bits/stl_construct.h:119:7,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_construct(_Args&& ...) [with _Args = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >}; _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:278:19,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_move_assign(std::_Optional_payload_base<_Tp>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:198:26,
    inlined from âconstexpr void std::_Optional_payload_base<_Tp>::_M_move_assign(std::_Optional_payload_base<_Tp>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:189:7,
    inlined from âconstexpr std::_Optional_payload<_Tp, true, false, false>& std::_Optional_payload<_Tp, true, false, false>::operator=(std::_Optional_payload<_Tp, true, false, false>&&) [with _Tp = std::__cxx11::basic_string<char>]â at /usr/include/c++/12/optional:420:22,
    inlined from âconstexpr std::_Optional_payload<_Tp, false, _Copy, _Move>& std::_Optional_payload<_Tp, false, _Copy, _Move>::operator=(std::_Optional_payload<_Tp, false, _Copy, _Move>&&) [with _Tp = std::__cxx11::basic_string<char>; bool _Copy = false; bool _Move = false]â at /usr/include/c++/12/optional:436:26,
    inlined from âconstexpr std::_Optional_base<_Tp, <anonymous>, <anonymous> >& std::_Optional_base<_Tp, <anonymous>, <anonymous> >::operator=(std::_Optional_base<_Tp, <anonymous>, <anonymous> >&&) [with _Tp = std::__cxx11::basic_string<char>; bool <anonymous> = false; bool <anonymous> = false]â at /usr/include/c++/12/optional:550:23,
    inlined from âconstexpr std::optional<std::__cxx11::basic_string<char> >& std::optional<std::__cxx11::basic_string<char> >::operator=(std::optional<std::__cxx11::basic_string<char> >&&)â at /usr/include/c++/12/optional:705:11,
    inlined from âconstexpr O& O::operator=(O&&)â at test.cpp:5:8,
    inlined from âvoid f()â at test.cpp:17:11:
/usr/include/c++/12/bits/basic_string.h:1071:16: warning: â*(const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)((char*)&<unnamed> + offsetof(O, O::s.std::optional<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::<unnamed>.std::_Optional_base<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false>::<unnamed>)).std::__cxx11::basic_string<char>::_M_string_lengthâ may be used uninitialized [-Wmaybe-uninitialized]
 1071 |       { return _M_string_length; }
      |                ^~~~~~~~~~~~~~~~
test.cpp: In function âvoid f()â:
test.cpp:17:11: note: â<anonymous>â declared here
   17 |     i = { };
      |           ^


---


### compiler : `gcc`
### title : `Useless stack adjustment by 16 around calls with odd stack-argument counts on SysV x86_64`
### open_at : `2023-04-20T09:40:25Z`
### last_modified_date : `2023-04-20T21:30:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109567
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `12.2.0`
### severity : `normal`
### contents :
For function calls with odd stack argument counts, gcc generates a useless `sub $16, %rsp` at the beginning of the calling function.

Example (https://godbolt.org/z/Y4ErE8ee9):
#include <stdio.h>
int callprintf_0stk(char const*Fmt){ return printf(Fmt,0,0,0,0,0),0; }
int callprintf_1stk(char const *Fmt){ return printf(Fmt,0,0,0,0,0, 1),0; }  //useless sub $0x10,%rsp
int callprintf_2stk(char const *Fmt){ return printf(Fmt,0,0,0,0,0, 1,2),0; }
int callprintf_3stk(char const *Fmt){ return printf(Fmt,0,0,0,0,0, 1,2,3),0; } //useless sub $0x10,%rsp
int callprintf_4stk(char const *Fmt){ return printf(Fmt,0,0,0,0,0, 1,2,3,4),0; }
int callprintf_5stk(char const *Fmt){ return printf(Fmt,0,0,0,0,0, 1,2,3,4,5),0; } //useless sub $0x10,%rsp
int callprintf_6stk(char const *Fmt){ return printf(Fmt,0,0,0,0,0, 1,2,3,4,5,6),0; }
int callprintf_7stk(char const *Fmt){ return printf(Fmt,0,0,0,0,0, 1,2,3,4,5,6,7),0; } //useless sub $0x10,%rsp


---


### compiler : `gcc`
### title : `Deeply nested loop unrolling overwhelms register allocator with -O3`
### open_at : `2023-04-21T12:16:34Z`
### last_modified_date : `2023-04-24T11:56:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109587
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
On matrix multiplication routines such as 

#include <arm_neon.h>

template<int N, int M, int K>
void f(const float32_t *__restrict a, const float32_t *__restrict b, float32_t *c) {
    for (int i = 0; i < N; ++i) {
        for (int j=0; j < M; ++j) {
            for (int k=0; k < K; ++k) {
                c[i*N + j] += a[i*K + k] * b[k*M + j];
            }
        }
    }
}

template void f<16, 16, 16>(const float32_t *__restrict a, const float32_t *__restrict b, float32_t *c);

the loop unroller fully unrolls the inner loop because the iteration count 16 is below the threshold.  But especially because this results in a RMW operation we don't have enough registers to deal with it and we spill profoundly.

The loop can be split in two but this requires manual work on each GEMM kernel.

Perhaps the loop unroller can use a better heuristic here?

It also looks like adding a pragmas

template<int N, int M, int K>
void f(const float32_t *__restrict a, const float32_t *__restrict b, float32_t *c) {
    for (int i = 0; i < N; ++i) {
        for (int j=0; j < M; ++j) {
#pragma GCC unroll 8
            for (int k=0; k < K; ++k) {
                c[i*N + j] += a[i*K + k] * b[k*M + j];
            }
        }
    }
}

helps but because this blocks cunrolli and instead unrolls in RTL we loose scheduling and result in not as efficient code.

So can we do better here with early unrolling?


---


### compiler : `gcc`
### title : `Failure to recognize shifts as sign/zero extension`
### open_at : `2023-04-22T00:12:33Z`
### last_modified_date : `2023-05-30T20:41:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109592
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
This is a trivial sign extension:

int sextb32(int x)
{ return (x << 24) >> 24; }

Yet on RV64 with ZBB enabled we get:

sextb32:
        slliw   a0,a0,24        # 6     [c=4 l=4]  ashlsi3
        sraiw   a0,a0,24        # 13    [c=8 l=4]  *ashrsi3_extend
        ret             # 21    [c=0 l=4]  simple_return

We actually get a good form to optimize in simplify_binary_operation_1:

> #0  simplify_context::simplify_binary_operation (this=0x7fffffffda68, code=ASHIFTRT, mode=E_SImode,     op0=0x7fffea11eb40, op1=0x7fffea009610) at /home/jlaw/riscv-persist/ventana/gcc/gcc/simplify-rtx.cc:2558
> 2558      gcc_assert (GET_RTX_CLASS (code) != RTX_COMPARE);
> (gdb) p code
> $24 = ASHIFTRT
> (gdb) p mode
> $25 = E_SImode
> (gdb) p debug_rtx (op0)
> (ashift:SI (subreg/s/u:SI (reg/v:DI 74 [ x ]) 0)
>     (const_int 24 [0x18]))
> $26 = void
> (gdb) p debug_rtx (op1)
> (const_int 24 [0x18])
> $27 = void

So that's (ashiftrt (ashift (object) 24) 24), ie sign extension. 

I suspect if we fix simplify_binary_operation_1 then we'll see this get simplified by fwprop.  I also suspect we could construct a zero extension variant.


---


### compiler : `gcc`
### title : `Useless branch not eliminated when writing to a union`
### open_at : `2023-04-23T18:26:22Z`
### last_modified_date : `2023-04-23T18:58:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109601
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `12.2.1`
### severity : `normal`
### contents :
Created attachment 54909
Minimal testcase

On x86_64-pc-linux-gnu the attached code compiles (with -O3) to rather unsatisfying assembly with a branch that does the same thing whether taken or not:

>   10:	85 c0                	test   %eax,%eax
>   12:	74 04                	je     18
>   14:	c3                   	ret
>   15:	0f 1f 00             	nopl   (%rax)
>   18:	c3                   	ret

I've tested on my machine in GCC 12.2.1, but Compiler Explorer ( https://godbolt.org/z/cW69rfrPc ) shows the same thing happens in a lot of other versions, including trunk (6ab856aa49bef7c04efa6144a5048e129b3a058b).

Notably, this doesn't happen in GCC 4.5.3.

This seems to happen for a few other targets as well, e.g: arm, riscv32; but not on some others, e.g. aarch64, mips.


---


### compiler : `gcc`
### title : `Vectorization failure for a small loop containing a simple branch`
### open_at : `2023-04-24T08:22:33Z`
### last_modified_date : `2023-04-25T07:48:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109603
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
For the following small case,

#include <stdlib.h>
#include <stdio.h>
#include <time.h>

#define NANOSECS        1000000000L

int main(int argc, char * argv[])
{
  long long i, even, odd, c;
  char *eptr;
  struct timespec ts0, ts1;

  c = strtoll(argv[1], &eptr, 10);

  printf("c = %lld \n", c);

  even = odd = 0;

  clock_gettime(CLOCK_MONOTONIC, &ts0);

  for (i = 0; i < c; i++)
  {
    if (i % 2) 
      even++;
    else
      odd++;
  }

  clock_gettime(CLOCK_MONOTONIC, &ts1);

  printf("even = %lld odd = %lld\n", even, odd);
  printf("elapsed %ld\n", (ts1.tv_sec - ts0.tv_sec) * NANOSECS + (ts1.tv_nsec - ts0.tv_nsec));
    
  return 0;
}

Using "-mcpu=neoverse-n1" gcc fails to vectorize the loop, while using "-mcpu=neoverse-n1 -mtune=generic" or without -mcpu and -mtune, gcc can successfully vectorize it.

============

The scalar version for the loop is like,

  400660:       36000381        tbz     w1, #0, 4006d0 <main+0xd0>
  400664:       91000694        add     x20, x20, #0x1
  400668:       91000421        add     x1, x1, #0x1
  40066c:       eb01027f        cmp     x19, x1
  400670:       54ffff81        b.ne    400660 <main+0x60>  // b.any
  ...
  4006d0:       910006b5        add     x21, x21, #0x1
  4006d4:       17ffffe5        b       400668 <main+0x68>

The vectorization version is like below (factor=2), and it is much faster on neoverse-n1.

  400670:       91000421        add     x1, x1, #0x1
  400674:       4e241c20        and     v0.16b, v1.16b, v4.16b
  400678:       4ee48421        add     v1.2d, v1.2d, v4.2d
  40067c:       4ee09800        cmeq    v0.2d, v0.2d, #0
  400680:       6e631ca0        bsl     v0.16b, v5.16b, v3.16b
  400684:       4ee08442        add     v2.2d, v2.2d, v0.2d
  400688:       eb13003f        cmp     x1, x19
  40068c:       54ffff21        b.ne    400670 <main+0x70>  // b.any

============

It seems neoverse-n1 vector cost model is inaccurate and does work well for this small case.

(1) For -mcpu=neoverse-n1 version, the vectorization cost model result is

Vector inside of loop cost: 12
Scalar iteration cost: 5

12 > 5*2, so gcc doesn't think it's worth doing vectorization for factor=2.

(2) For the version without -mcpu , the vectorization cost model result is

Vector inside of loop cost: 4
Scalar iteration cost: 5

Actually, the loop body cost for vectorized version is 4, which is too small, and it looks incorrect as well, although in reality vectorized version is faster than scalar version. In contract, the 12 for -mcpu=neoverse-n1 looks more reasonable, although it blocked the vectorization.


---


### compiler : `gcc`
### title : `Redundant VSETVL after optimized code of RVV`
### open_at : `2023-04-25T01:07:22Z`
### last_modified_date : `2023-05-05T13:18:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109615
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
Assume we have a sample code as below.


#include "riscv_vector.h"

void f (int8_t * restrict in, int8_t * restrict out, int n, int m, int cond)
{
  size_t vl = 101;
  if (cond)
    vl = m * 2;
  else
    vl = m * 2 * vl;

  for (size_t i = 0; i < n; i++)
    {
      vint8mf8_t v = __riscv_vle8_v_i8mf8 (in + i, vl);
      __riscv_vse8_v_i8mf8 (out + i, v, vl);

      vbool64_t mask = __riscv_vlm_v_b64 (in + i + 100, vl);

      vint8mf8_t v2 = __riscv_vle8_v_i8mf8_tumu (mask, v, in + i + 100, vl);
      __riscv_vse8_v_i8mf8 (out + i + 100, v2, vl);
    }

  for (size_t i = 0; i < n; i++)
    {
      vint8mf8_t v = __riscv_vle8_v_i8mf8 (in + i + 300, vl);
      __riscv_vse8_v_i8mf8 (out + i + 300, v, vl);
    }
}

Currently the upstream will generate the code as below with *-march=rv64gcv -O3 -frename-registers* options. It looks like the last vsetvl of .L4 bb is redundant.

f:
        slliw   a3,a3,1
        bne     a4,zero,.L2
        li      a5,101
        mul     a3,a3,a5
.L2:
        addi    a4,a1,100
        add     t1,a0,a2
        mv      t0,a0
        beq     a2,zero,.L1
        vsetvli zero,a3,e8,mf8,tu,mu
.L4:
        addi    a6,t0,100
        addi    a7,a4,-100
        vle8.v  v1,0(t0)
        addi    t0,t0,1
        vse8.v  v1,0(a7)
        vlm.v   v0,0(a6)
        vle8.v  v1,0(a6),v0.t
        vse8.v  v1,0(a4)
        addi    a4,a4,1
        bne     t0,t1,.L4
        addi    a0,a0,300
        addi    a1,a1,300
        add     a2,a0,a2
        vsetvli zero,a3,e8,mf8,ta,ma   // <= redundant ?
.L5:
        vle8.v  v2,0(a0)
        addi    a0,a0,1
        vse8.v  v2,0(a1)
        addi    a1,a1,1
        bne     a2,a0,.L5
.L1:
        ret


---


### compiler : `gcc`
### title : `Simple std::optional types returned on stack, not registers`
### open_at : `2023-04-26T07:22:08Z`
### last_modified_date : `2023-04-26T09:48:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109631
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `unknown`
### severity : `enhancement`
### contents :
A std::optional<T> is fundamentally like a std::pair<T, bool>, but with nice access features, type safety, etc.  But for simple functions it is significantly less efficient, because in gcc it is returned on the stack even when T fits in a single register.

<https://godbolt.org/z/6hx6vxaG7>

On targets such as x86-64 and AARCH64, simple structs and pairs that fit in two registers, are returned in two registers - there is no need for the caller to reserve stack space and pass a hidden pointer to the callee function.  On clang with its standard library, this also applies to std::optional<T> for suitable types T such as "int", but on gcc and its standard C++ library, the stack is used for returning the std::optional<int>.  Since both clang and gcc follow the same calling conventions, this must (I believe) be a result of different implementations of std::optional<> in the C++ libraries.

(I note that code for std::pair<bool, T> is more efficient here than using std::pair<T, bool>.  But perhaps the details of std::optional<> require that the contained element comes first.)



#include <optional>
#include <utility>

using A = std::optional<int>;
using B = std::pair<int, bool>;
using C = std::pair<bool, int>;

A foo_A(int x) {
    return x;
}

B foo_b(int x) {
    B y { x, true };
    return y;
}

C foo_c(int x) {
    C y { true, x };
    return y;
}


---


### compiler : `gcc`
### title : `Inefficient codegen when complex numbers are emulated with structs`
### open_at : `2023-04-26T13:03:02Z`
### last_modified_date : `2023-05-23T19:16:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109632
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
The following two cases are the same

struct complx_t {
    float re;
    float im;
};

complx_t
add(const complx_t &a, const complx_t &b) {
  return {a.re + b.re, a.im + b.im};
}

_Complex float
add(const _Complex float *a, const _Complex float *b) {
  return {__real__ *a + __real__ *b, __imag__ *a + __imag__ *b};
}

But we generate much different code (looking at -O2),  For the first one we do:

        ldr     d1, [x1]
        ldr     d0, [x0]
        fadd    v0.2s, v0.2s, v1.2s
        fmov    x0, d0
        lsr     x1, x0, 32
        lsr     w0, w0, 0
        fmov    s1, w1
        fmov    s0, w0
        ret

which is bad for obvious reasons, but also also never needed to go through the genreg for such a reversal. we could have used many other NEON instructions.

For the second one we generate the good instructions:

add(float _Complex const*, float _Complex const*):
        ldp     s3, s2, [x0]
        ldp     s0, s1, [x1]
        fadd    s1, s2, s1
        fadd    s0, s3, s0
        ret

The difference being that in the second one we have decomposed the initial structure by loading the elements:

  <bb 2> [local count: 1073741824]:
  _1 = REALPART_EXPR <*a_8(D)>;
  _2 = REALPART_EXPR <*b_9(D)>;
  _3 = _1 + _2;
  _4 = IMAGPART_EXPR <*a_8(D)>;
  _5 = IMAGPART_EXPR <*b_9(D)>;
  _6 = _4 + _5;
  _10 = COMPLEX_EXPR <_3, _6>;
  return _10;

In the first one we've kept them as vectors:

  <bb 2> [local count: 1073741824]:
  vect__1.6_13 = MEM <const vector(2) float> [(float *)a_8(D)];
  vect__2.9_15 = MEM <const vector(2) float> [(float *)b_9(D)];
  vect__3.10_16 = vect__1.6_13 + vect__2.9_15;
  MEM <vector(2) float> [(float *)&D.4435] = vect__3.10_16;
  return D.4435;

This part is probably a costing issue, we SLP them even though it's not profitable because for the APCS we have to return them in separate registers.

Using -fno-tree-vectorize gets the gimple code right:

  <bb 2> [local count: 1073741824]:
  _1 = a_8(D)->re;
  _2 = b_9(D)->re;
  _3 = _1 + _2;
  D.4435.re = _3;
  _4 = a_8(D)->im;
  _5 = b_9(D)->im;
  _6 = _4 + _5;
  D.4435.im = _6;
  return D.4435;

But we generate worse code:

        ldp     s1, s0, [x0]
        mov     x2, 0
        ldp     s3, s2, [x1]
        fadd    s1, s1, s3
        fadd    s0, s0, s2
        fmov    w1, s1
        fmov    w0, s0
        bfi     x2, x1, 0, 32
        bfi     x2, x0, 32, 32
        lsr     x0, x2, 32
        lsr     w2, w2, 0
        fmov    s1, w0
        fmov    s0, w2

where we again use genreg as a very complicated way to do a no-op.

So there are two bugs here:

1. a costing, we shouldn't SLP
2. an expansion, the code out of expand is bad to begin with.


---


### compiler : `gcc`
### title : `unnecessary range check in complete switch on bitfield`
### open_at : `2023-04-26T18:17:20Z`
### last_modified_date : `2023-09-21T13:55:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109637
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.0`
### severity : `enhancement`
### contents :
This fully populated switch still produces some kind of useless range check:

struct S { unsigned x : 2; };
int f (struct S *s) {
    switch(s->x) {
        case 0: return 0;
        case 1: return 1;
        case 2: return 2;
        case 3: return 3;
    }
}

->
        movzbl  (%rdi), %eax
        andl    $3, %eax
        leal    -1(%rax), %edx
        movzbl  %al, %eax
        cmpb    $3, %dl
        movl    $0, %edx
        cmovnb  %edx, %eax
        ret

GCC apparently understands that the switch is complete at some level since anything after the switch is recognised as dead code, so the range check is a bit puzzling.

The code is fine if we explicitly mask the switch value as in `switch(s->x & 3)`:

        movzbl  (%rdi), %eax
        andl    $3, %eax
        ret


---


### compiler : `gcc`
### title : `unsigned > 1 ? 0 : n is not optimized to n == 1`
### open_at : `2023-04-26T21:00:31Z`
### last_modified_date : `2023-09-21T14:16:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109638
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Take:
```
unsigned test_cst (unsigned n)
{
  if (n > 1)
    n = 0;
  return n;
}
unsigned test_cst1 (unsigned n)
{
  return (n == 1);
}
```
For gimple at least the second one is smaller. I suspect for x86 the second version is also faster because it uses sete rather than cmov.
But these 2 functions should produce the same code.

I found this on accident while looking at gcc.dg/tree-ssa/builtin-snprintf-2.c  testcase.

This can only be done for 1 too.


---


### compiler : `gcc`
### title : `(a ? -1 : 0) | b could be optimized better for aarch64`
### open_at : `2023-04-27T22:47:15Z`
### last_modified_date : `2023-05-02T21:11:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109657
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `target`
### version : `13.0`
### severity : `enhancement`
### contents :
Take (at -O2):
```
unsigned b(unsigned a, unsigned b){
    if(b){
        return -1;
    }
    return a;
}
unsigned b1(unsigned a, unsigned b){
    unsigned t = b ? -1 : 0;
    return a | t;
}
```
Right now we produce:
```
b:
        cmp     w1, 0
        csinv   w0, w0, wzr, eq
        ret
b1:
        cmp     w1, 0
        csetm   w1, ne
        orr     w0, w1, w0
        ret
```

We can help combine here by adding a pattern for:
(set (reg/i:SI 0 x0)
    (ior:SI (neg:SI (ne:SI (reg:CC 66 cc)
                (const_int 0 [0])))
        (reg:SI 102)))

Which is the same as 
(set (reg/i:SI 0 x0)
    (if_then_else:SI (eq (reg:CC 66 cc)
            (const_int 0 [0]))
        (reg:SI 97)
        (const_int -1 [0xffffffffffffffff])))
pattern.

I suspect both are considered canonical too.


---


### compiler : `gcc`
### title : `bad SLP vectorization on zen`
### open_at : `2023-05-01T21:31:43Z`
### last_modified_date : `2023-05-10T15:37:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109690
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
model name      : AMD Ryzen 7 5800X 8-Core Processor
reproduces on my znver1 laptop too.

h@ryzen3:~/gcc-kub/build/gcc> cat tt.c
int a[100];

[[gnu::noipa]]
void loop()
{
          for (int i = 0; i < 3; i++)
                  a[i]+=a[i];
}
int
main()
{
        for (int j = 0; j < 1000000000; j++)
          loop ();
        return 0;
}


jh@ryzen3:~/gcc-kub/build/gcc> ./xgcc -B ./ -O2 -march=native tt.c ; perf stat ./a.out

 Performance counter stats for './a.out':

           2683.95 msec task-clock:u                     #    1.000 CPUs utilized             
                 0      context-switches:u               #    0.000 /sec                      
                 0      cpu-migrations:u                 #    0.000 /sec                      
                52      page-faults:u                    #   19.374 /sec                      
       13001141361      cycles:u                         #    4.844 GHz                         (83.31%)
            691180      stalled-cycles-frontend:u        #    0.01% frontend cycles idle        (83.31%)
            101980      stalled-cycles-backend:u         #    0.00% backend cycles idle         (83.31%)
       12999928665      instructions:u                   #    1.00  insn per cycle            
                                                  #    0.00  stalled cycles per insn     (83.31%)
        3000013809      branches:u                       #    1.118 G/sec                       (83.41%)
              1525      branch-misses:u                  #    0.00% of all branches             (83.36%)

       2.684376360 seconds time elapsed

       2.684369000 seconds user
       0.000000000 seconds sys


jh@ryzen3:~/gcc-kub/build/gcc> ./xgcc -B ./ -O2 -march=native tt.c -fno-tree-vectorize ; perf stat ./a.out

 Performance counter stats for './a.out':

           1238.92 msec task-clock:u                     #    1.000 CPUs utilized             
                 0      context-switches:u               #    0.000 /sec                      
                 0      cpu-migrations:u                 #    0.000 /sec                      
                52      page-faults:u                    #   41.972 /sec                      
        6000338140      cycles:u                         #    4.843 GHz                         (83.21%)
            314660      stalled-cycles-frontend:u        #    0.01% frontend cycles idle        (83.21%)
                 0      stalled-cycles-backend:u         #    0.00% backend cycles idle         (83.23%)
        7999796562      instructions:u                   #    1.33  insn per cycle            
                                                  #    0.00  stalled cycles per insn     (83.53%)
        2999887795      branches:u                       #    2.421 G/sec                       (83.53%)
               698      branch-misses:u                  #    0.00% of all branches             (83.28%)

       1.239116606 seconds time elapsed

       1.239121000 seconds user
       0.000000000 seconds sys


---


### compiler : `gcc`
### title : `Takes until forwprop2 to remove !a sometimes`
### open_at : `2023-05-02T05:06:03Z`
### last_modified_date : `2023-05-08T06:18:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109691
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
int f(_Bool c, int a, int b)
{
        _Bool d = c;
        d = !d;
        if (d != 0) return a;
        return b;
}
```

It takes until forwprop2 now to remove all of the code before the conditional.
In GCC 4.9.0, we used to do it almost all the way in forwprop1; with just one assignment added.

I noticed this while looking into PR 67628 but it is not exactly an issue there so filing it seperately.


---


### compiler : `gcc`
### title : `arm: lack of MVE instruction costing causing worse codegen on a vec_duplicate`
### open_at : `2023-05-02T12:11:52Z`
### last_modified_date : `2023-05-18T10:44:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109697
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Hi all,

In the arm backend, for MVE targets we previously had this bug on the vcmp patterns: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=107987

The fix is fine, but it resulted in some failing tests:
        * gcc.target/arm/mve/intrinsics/vcmpcsq_n_u16.c
        * gcc.target/arm/mve/intrinsics/vcmpcsq_n_u32.c
        * gcc.target/arm/mve/intrinsics/vcmpcsq_n_u8.c
        * gcc.target/arm/mve/intrinsics/vcmpeqq_n_f16.c
        * gcc.target/arm/mve/intrinsics/vcmpeqq_n_f32.c
        * gcc.target/arm/mve/intrinsics/vcmpeqq_n_u16.c
        * gcc.target/arm/mve/intrinsics/vcmpeqq_n_u32.c
        * gcc.target/arm/mve/intrinsics/vcmpeqq_n_u8.c
        * gcc.target/arm/mve/intrinsics/vcmpgeq_n_f16.c
        * gcc.target/arm/mve/intrinsics/vcmpgeq_n_f32.c
        * gcc.target/arm/mve/intrinsics/vcmpgtq_n_f16.c
        * gcc.target/arm/mve/intrinsics/vcmpgtq_n_f32.c
        * gcc.target/arm/mve/intrinsics/vcmphiq_n_u16.c
        * gcc.target/arm/mve/intrinsics/vcmphiq_n_u32.c
        * gcc.target/arm/mve/intrinsics/vcmphiq_n_u8.c
        * gcc.target/arm/mve/intrinsics/vcmpleq_n_f16.c
        * gcc.target/arm/mve/intrinsics/vcmpleq_n_f32.c
        * gcc.target/arm/mve/intrinsics/vcmpltq_n_f16.c
        * gcc.target/arm/mve/intrinsics/vcmpltq_n_f32.c
        * gcc.target/arm/mve/intrinsics/vcmpneq_n_f16.c
        * gcc.target/arm/mve/intrinsics/vcmpneq_n_f32.c
        * gcc.target/arm/mve/intrinsics/vcmpneq_n_u16.c
        * gcc.target/arm/mve/intrinsics/vcmpneq_n_u32.c
        * gcc.target/arm/mve/intrinsics/vcmpneq_n_u8.c
(after Andrea improved these tests in GCC13)

The testcases that are failing are the ones that compare against a scalar immediate (e.g. "vcmpeqq (a, 1.1)"), because the compiler prefers to do:
```
        vldr.64 d6, .L5
        vldr.64 d7, .L5+8
        vcmp.f16        eq, q0, q3
```
When previously we would much more simply:
```
        movs    r3, #1
        vcmp.u16        cs, q0, r3
```

The underlying reason for this change is a known deficiency of the MVE implementation: the lack of proper instruction costing.
The compiler falls back to calculating costs based on the operands and the new vec_duplicate in the patterns (mve_vcmp<mve_cmp_op>q_n_<mode>, etc) gets given a cost of 32 (when instead it should know that the vec duplicate is free and this is all just one instruction...), so the "literal load + vector-vector compare" wins out against the "put the immediate in a GP reg + vector-scalar compare".
For now, I plan on simply XFAIL-ing the tests.


---


### compiler : `gcc`
### title : `ABSU<a> == 0 is not optimized to just a == 0`
### open_at : `2023-05-04T00:40:58Z`
### last_modified_date : `2023-05-05T05:24:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109722
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(signed char a)
{
        int t = a;
        if (t < 0) t = -t;
        a = t;
        return a == 0;
}
```
Right now this gets optimized to just:
```
  _9 = ABSU_EXPR <a_3(D)>;
  _1 = _9 == 0;
  _7 = (int) _1;
```

If we change `signed char` to `unsigned char`, we get:
  _1 = a_2(D) == 0;
  _3 = (int) _1;

As we had expected for the original case too.

The fix is to change in match.pd:
/* Convert ABS_EXPR<x> == 0 or ABS_EXPR<x> != 0 to x == 0 or x != 0.  */
(for op (eq ne)
 (simplify
  (op (abs @0) zerop@1)
  (op @0 @1)))
to:
/* Convert ABS[U]_EXPR<x> == 0 or ABS[U]_EXPR<x> != 0 to x == 0 or x != 0.  */
(for op (abs absu)
 (for eqne (eq ne)
  (simplify
   (eqne (op @0) zerop@1)
   (eqne @0 @1)))


---


### compiler : `gcc`
### title : `RISC-V: Unnecessary VSETVLI of the RVV intrinsic in loop`
### open_at : `2023-05-05T06:17:16Z`
### last_modified_date : `2023-08-16T14:56:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109743
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55004
Test file for the unnecessary VSETVL for RVV intrinsic.

Assume we have the below example code, it looks like we can eliminate 2 VSETVL instructions.

#include "riscv_vector.h"

void
foo2 (int32_t *a, int32_t *b, int n)
{
  if (n <= 0)
      return;
  int i = n;
  size_t vl = __riscv_vsetvl_e32m1 (i);

  for (; i >= 0; i--)
  {
    vint32m1_t v = __riscv_vle32_v_i32m1 (a, vl);
    __riscv_vse32_v_i32m1 (b, v, vl);

    if (i >= vl)
      continue;

    if (i == 0)
      return;

    vl = __riscv_vsetvl_e32m1 (i);
  }
}

When compile with option '-march=rv64gc_zve64d -mabi=lp64d -O3 test.c -c -S -o -', it will generate the assembly code like below.

foo2:
.LFB2:
        .cfi_startproc
        ble     a2,zero,.L1
        mv      a4,a2
        li      a3,-1
        vsetvli a5,a2,e32,m1,ta,mu
        vsetvli zero,a5,e32,m1,ta,ma  <- can be eliminated.
.L5:
        vle32.v v1,0(a0)
        vse32.v v1,0(a1)
        bgeu    a4,a5,.L3
.L10:
        beq     a2,zero,.L1
        vsetvli a5,a4,e32,m1,ta,mu
        addi    a4,a4,-1
        vsetvli zero,a5,e32,m1,ta,ma  <- can be eliminated.
        vle32.v v1,0(a0)
        vse32.v v1,0(a1)
        addiw   a2,a2,-1
        bltu    a4,a5,.L10
.L3:
        addiw   a2,a2,-1
        addi    a4,a4,-1
        bne     a2,a3,.L5
.L1:
        ret

GCC version:
riscv64-unknown-linux-gnu-gcc (gd7cb9720ed5) 14.0.0 20230503 (experimental)
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


---


### compiler : `gcc`
### title : `Fails removing redundant comparison in for loop over multiple variables, unless members of struct`
### open_at : `2023-05-05T10:10:40Z`
### last_modified_date : `2023-05-06T12:03:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109746
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.1.1`
### severity : `normal`
### contents :
Compiled with g++ (GCC) 13.1.1 20230429 on x86-64 Linux, with -O2 or higher.

In the code below, the comparisons with i are redundant because j is increasing much faster.
GCC manages to optimize this away when i and j are members of some struct, but not when they are just integers.

struct S {
  unsigned x;
};

unsigned f() {
  unsigned N = 1e9, x = 0;
  for (unsigned i = 3, j = 1; i < N && j < N; i += 2, j += 3)
    x ^= i * j;
  return x;
}

unsigned g() {
  unsigned N = 1e9, x = 0;
  for (S i {3}, j {1}; i.x < N && j.x < N; i.x += 2, j.x += 3)
    x ^= i.x * j.x;
  return x;
}

int main() {
  return f();
}


---


### compiler : `gcc`
### title : `[12/13 Regression] SLP cost of constructors is off`
### open_at : `2023-05-05T11:56:19Z`
### last_modified_date : `2023-05-23T17:00:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109747
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
When r12-7319-g90d693bdc9d718 added the accounting for GPR->XMM moves to the
cost of SLP CTORs I failed to realize that the costing code will pass down
the very same (and full) SLP node from vect_prologue_cost_for_slp but
will generate costs for each individual actual vector that's constructed.

So for the case where the SLP node covers more than one vector the costing
will be off.  That's visible in the costing of the testcase for PR108724
for example which is

void foo(int *a, const int *__restrict b, const int *__restrict c)
{
  for (int i = 0; i < 16; i++) {
    a[i] = b[i] + c[i];
  }
}

and we end up with

_17 8 times unaligned_store (misalign -1) costs 96 in body
node 0x3fb5838 1 times vec_construct costs 100 in prologue
node 0x3fb5838 1 times vec_construct costs 100 in prologue
node 0x3fb5838 1 times vec_construct costs 100 in prologue
node 0x3fb5838 1 times vec_construct costs 100 in prologue
node 0x3fb5838 1 times vec_construct costs 100 in prologue
node 0x3fb5838 1 times vec_construct costs 100 in prologue
node 0x3fb5838 1 times vec_construct costs 100 in prologue
node 0x3fb5838 1 times vec_construct costs 100 in prologue


---


### compiler : `gcc`
### title : `V2SI multiply high is not vectorized on x86_64`
### open_at : `2023-05-07T16:21:37Z`
### last_modified_date : `2023-05-08T08:09:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109764
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
The folowing testcase:

--cut here--
#define N 2

unsigned int ur[N], ua[N], ub[N];

void mulh (void)
{
  int i;

  for (i = 0; i < N; i++)
    ur[i] = ((unsigned long) ua[i] * ub[i]) >> 32;
}

void mulh_slp (void)
{
  ur[0] = ((unsigned long) ua[0] * ub[0]) >> 32;
  ur[1] = ((unsigned long) ua[1] * ub[1]) >> 32;
}
--cut here--

should vectorize on x86_64 with the patch I'm going to attach, and with -fno-vect-cost-model. The compiler however does not even consider "<s>mulv2si3_highpart" pattern.


---


### compiler : `gcc`
### title : `Passing doubles through the stack generates a stack adjustment per each such argument at -Os/-Oz.`
### open_at : `2023-05-08T09:06:20Z`
### last_modified_date : `2023-06-16T08:47:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109766
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.1.0`
### severity : `normal`
### contents :
/*
 Passing doubles through the stack generates a stack adjustment pear each such argument at -Os/-Oz.
 These stack adjustments are only coalesced at -O1/-O2/-O3, leaving -Os/-Oz with larger code.
*/
#define $expr(...) (__extension__({__VA_ARGS__;}))
#define $regF0 $expr(register double x __asm("xmm0"); x)
#define $regF1 $expr(register double x __asm("xmm1"); x)
#define $regF2 $expr(register double x __asm("xmm2"); x)
#define $regF3 $expr(register double x __asm("xmm3"); x)
#define $regF4 $expr(register double x __asm("xmm4"); x)
#define $regF5 $expr(register double x __asm("xmm5"); x)
#define $regF6 $expr(register double x __asm("xmm6"); x)
#define $regF7 $expr(register double x __asm("xmm7"); x)

void func(char const*Fmt, ...);
void callfunc(char const*Fmt, double D0, double D1, double D2, double D3, double D4, double D5, double D6, double D7){
    func(Fmt,$regF0,$regF1,$regF2,$regF3,$regF4,$regF5,$regF6,$regF7,
            D0,D1,D2,D3,D4,D5,D6,D7);
/*
//gcc @ -Os/-Oz
0000000000000000 <callfunc>:
   0:   50                      push   %rax
   1:   b0 08                   mov    $0x8,%al
   3:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
   8:   66 0f d6 3c 24          movq   %xmm7,(%rsp)
   d:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
  12:   66 0f d6 34 24          movq   %xmm6,(%rsp)
  17:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
  1c:   66 0f d6 2c 24          movq   %xmm5,(%rsp)
  21:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
  26:   66 0f d6 24 24          movq   %xmm4,(%rsp)
  2b:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
  30:   66 0f d6 1c 24          movq   %xmm3,(%rsp)
  35:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
  3a:   66 0f d6 14 24          movq   %xmm2,(%rsp)
  3f:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
  44:   66 0f d6 0c 24          movq   %xmm1,(%rsp)
  49:   48 8d 64 24 f8          lea    -0x8(%rsp),%rsp
  4e:   66 0f d6 04 24          movq   %xmm0,(%rsp)
  53:   e8 00 00 00 00          callq  58 <callfunc+0x58>
                        54: R_X86_64_PLT32      func-0x4
  58:   48 83 c4 48             add    $0x48,%rsp
  5c:   c3                      retq
$sz(callfunc)=93

//clang @ -Os/-Oz
0000000000000000 <callfunc>:
   0:	48 83 ec 48          	sub    $0x48,%rsp
   4:	f2 0f 11 7c 24 38    	movsd  %xmm7,0x38(%rsp)
   a:	f2 0f 11 74 24 30    	movsd  %xmm6,0x30(%rsp)
  10:	f2 0f 11 6c 24 28    	movsd  %xmm5,0x28(%rsp)
  16:	f2 0f 11 64 24 20    	movsd  %xmm4,0x20(%rsp)
  1c:	f2 0f 11 5c 24 18    	movsd  %xmm3,0x18(%rsp)
  22:	f2 0f 11 54 24 10    	movsd  %xmm2,0x10(%rsp)
  28:	f2 0f 11 4c 24 08    	movsd  %xmm1,0x8(%rsp)
  2e:	f2 0f 11 04 24       	movsd  %xmm0,(%rsp)
  33:	b0 08                	mov    $0x8,%al
  35:	e8 00 00 00 00       	callq  3a <callfunc+0x3a>
			36: R_X86_64_PLT32	func-0x4
  3a:	48 83 c4 48          	add    $0x48,%rsp
  3e:	c3                   	retq   
$sz(callfunc)=63


//gcc @ -O1
0000000000000000 <callfunc>:
   0:	48 83 ec 48          	sub    $0x48,%rsp
   4:	f2 0f 11 7c 24 38    	movsd  %xmm7,0x38(%rsp)
   a:	f2 0f 11 74 24 30    	movsd  %xmm6,0x30(%rsp)
  10:	f2 0f 11 6c 24 28    	movsd  %xmm5,0x28(%rsp)
  16:	f2 0f 11 64 24 20    	movsd  %xmm4,0x20(%rsp)
  1c:	f2 0f 11 5c 24 18    	movsd  %xmm3,0x18(%rsp)
  22:	f2 0f 11 54 24 10    	movsd  %xmm2,0x10(%rsp)
  28:	f2 0f 11 4c 24 08    	movsd  %xmm1,0x8(%rsp)
  2e:	f2 0f 11 04 24       	movsd  %xmm0,(%rsp)
  33:	b8 08 00 00 00       	mov    $0x8,%eax
  38:	e8 00 00 00 00       	callq  3d <callfunc+0x3d>
			39: R_X86_64_PLT32	func-0x4
  3d:	48 83 c4 48          	add    $0x48,%rsp
  41:	c3                   	retq   
$sz(callfunc)=66
*/
}

https://godbolt.org/z/d8T3hxqWK


---


### compiler : `gcc`
### title : `Unnecessary pblendw for vectorized or`
### open_at : `2023-05-08T13:58:36Z`
### last_modified_date : `2023-05-08T16:05:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109771
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
I have an example of vectorization of 4x64-bit struct (representation of 256-bit integer). The implementation just uses for loop of count 4.

This is vectorized in isolation however when combined with some non-trivial control-flow and additional wrapping functions the final assembly contains weird pblendw instructions.

pblendw xmm1, xmm3, 240          (GCC 13, x86-64-v2)
movlpd  xmm1, QWORD PTR [rdi+16] (GCC 13, x86-64-v1)
shufpd  xmm1, xmm3, 2            (GCC 12)

I believe this is some kind of regression in GCC 13 because I have a bigger context where GCC 12 was optimizing it "correctly". However, I lost this information during test reduction.

https://godbolt.org/z/jzK44h3js

cpp:

struct u256 {
    unsigned long w[4];
};

inline u256 or_(u256 x, u256 y) {
    u256 z;
    for (int i = 0; i < 4; ++i) 
        z.w[i] = x.w[i] | y.w[i];
    return z;
}

inline void or_to(u256& z, u256 y) { z = or_(z, y); }

void op_or(u256* t) { or_to(t[1], t[0]); }

void test(u256* t) {
    void* tbl[]{&&CLOBBER, &&OR};
CLOBBER:
    goto * 0;
OR:
    op_or(t);
    goto * 0;
}


x86-64-v2 asm:

test(u256*):
        xorl    %eax, %eax
        jmp     *%rax
        movdqu  32(%rdi), %xmm3
        movdqu  (%rdi), %xmm1
        movdqu  16(%rdi), %xmm2
        movdqu  48(%rdi), %xmm0
        por     %xmm3, %xmm1
        movups  %xmm1, 32(%rdi)
        movdqa  %xmm2, %xmm1
        pblendw $240, %xmm0, %xmm1
        pblendw $240, %xmm2, %xmm0
        por     %xmm1, %xmm0
        movups  %xmm0, 48(%rdi)
        jmp     *%rax


---


### compiler : `gcc`
### title : `548.exchange2_r compiled with -O2 -flto regressed by 5% on aarch64 between r14-135-gd06e9264b0192c and r14-192-g636e2273aec555`
### open_at : `2023-05-10T14:18:10Z`
### last_modified_date : `2023-05-10T15:44:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109796
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
The slowdown can be seen clearly at

https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=581.407.0

Fortran at -O2 may not be super important and the slowdown is not huge but it looks very stable so maybe worth a look.  I do not see any similar regression on a different architecture or configuration.


---


### compiler : `gcc`
### title : `libjxl 0.7 is a lot slower in GCC 13.1 vs Clang 16`
### open_at : `2023-05-11T14:24:18Z`
### last_modified_date : `2023-06-19T16:28:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109811
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.1`
### severity : `normal`
### contents :
Created attachment 55051
Graphs

Check this:

https://www.phoronix.com/review/gcc13-clang16-raptorlake/3


---


### compiler : `gcc`
### title : `GraphicsMagick resize is a lot slower in GCC 13.1 vs Clang 16 on Intel Raptor Lake`
### open_at : `2023-05-11T14:25:48Z`
### last_modified_date : `2023-10-12T04:48:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109812
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.1`
### severity : `normal`
### contents :
Check this:

https://www.phoronix.com/review/gcc13-clang16-raptorlake/3


---


### compiler : `gcc`
### title : `Optimizing __builtin_signbit(x) ? -x : x or abs for FP`
### open_at : `2023-05-12T13:23:50Z`
### last_modified_date : `2023-05-14T23:09:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109829
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
Consider the following 2 functions:

__float128 abs1(__float128 x) { return __builtin_fabsf128(x); }
__float128 abs2(__float128 x) { return __builtin_signbit(x) ? -x : x; }

They should provide the same results, however the codegen is different:

abs1(__float128):
        pand    xmm0, XMMWORD PTR .LC0[rip]
        ret
abs2(__float128):
        movmskps        eax, xmm0
        test    al, 8
        je      .L4
        pxor    xmm0, XMMWORD PTR .LC1[rip]
.L4:
        ret


Looks like match.pd miss the __builtin_signbit(x) ? -x : x -> __builtin_fabs*(x) pattern.

Playground: https://godbolt.org/z/bsxeozGqv


---


### compiler : `gcc`
### title : `aarch64: Inefficient code for logical or of two booleans`
### open_at : `2023-05-12T15:29:42Z`
### last_modified_date : `2023-05-12T17:48:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109832
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
For the following testcase:

bool f(bool x, bool y) {
    return x || y;
}

on AArch64 at -O2 we generate the rather convoluted:

f:
        and     w1, w1, 255
        tst     w0, 255
        ccmp    w1, 0, 0, eq
        cset    w0, ne
        ret

but clang gives the more optimal:

f:                                      // @f
        orr     w8, w0, w1
        and     w0, w8, #0x1
        ret

we should probably do what clang does here.


---


### compiler : `gcc`
### title : `signbit comparisons -> copysign optimization`
### open_at : `2023-05-13T16:45:57Z`
### last_modified_date : `2023-05-13T16:45:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109843
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
float copysign1(float  x, float y)
{
        bool t = __builtin_signbit(x) == 0;
        bool t1 = __builtin_signbit(y) == 0;
        return (t == t1) ? y : -y;
}
float copysign2(float  x, float y)
{
        bool t = __builtin_signbit(x) != 0;
        bool t1 = __builtin_signbit(y) != 0;
        return (t == t1) ? y : -y;
}
float copysign3(float  x, float y)
{
        bool t = __builtin_signbit(x) != 0;
        bool t1 = __builtin_signbit(y) == 0;
        return (t != t1) ? y : -y;
}
float copysign4(float  x, float y)
{
        bool t = __builtin_signbit(x) == 0;
        bool t1 = __builtin_signbit(y) != 0;
        return (t != t1) ? y : -y;
}
float copysign5(float  x, float y)
{
        return __builtin_copysignf(x, y);
}

These all should end up being the same code.
Hopefully I didn't mess these up.


---


### compiler : `gcc`
### title : `Unnecessary basic block with single jmp instruction`
### open_at : `2023-05-13T17:43:36Z`
### last_modified_date : `2023-05-13T17:51:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109844
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
The code

void err(void);

void merge_bb(int y) {
    if (y) 
        return err();
}

is

merge_bb:
        test    edi, edi
        jne     .L4
        ret
.L4:
        jmp     err


but could be

merge_bb:
        test    edi, edi
        jne     err
        ret

https://godbolt.org/z/eafPa4o4T


---


### compiler : `gcc`
### title : `Addition overflow/carry flag unnecessarily put in a temporary register`
### open_at : `2023-05-13T18:32:00Z`
### last_modified_date : `2023-05-31T23:21:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109845
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.1.0`
### severity : `enhancement`
### contents :
When we have an addition and an overflow check and the overflow flag is combined with some other condition the codegen may generate variant when the overflow flag is temporary register.

    unsigned s = y + z;
    _Bool ov = s < y;

    if (x || ov) 
        return;

This produces

        add     esi, edx
        setc    al
        test    edi, edi
        jne     .L1
        test    eax, eax
        jne     .L1

while it could be

        add     esi, edx
        jc      .L6
        test    edi, edi
        jne     .L6


There are easy workaround to the C code which make the assembly optimal:

1. Change the order of checks 
    if (ov || x)

2. Split if into two
    if (x)
        return;
    if (ov) 
        return;

https://godbolt.org/z/rxsrnhPdc


---


### compiler : `gcc`
### title : `suboptimal code for vector walking loop`
### open_at : `2023-05-13T22:26:04Z`
### last_modified_date : `2023-06-30T14:28:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109849
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.0`
### severity : `normal`
### contents :
jan@localhost:/tmp> cat t.C
#include <vector>
typedef unsigned int uint32_t;
std::vector<std::pair<uint32_t, uint32_t>> stack;
void
test()
{
        while (!stack.empty()) {
                std::pair<uint32_t, uint32_t> cur = stack.back();
                stack.pop_back();
                if (cur.second)
                        break;
        }
}
jan@localhost:/tmp> gcc t.C -O3 -S 

yields to:

_Z4testv:
.LFB1264:
        .cfi_startproc
        movq    stack(%rip), %rcx
        movq    stack+8(%rip), %rax
        jmp     .L5
        .p2align 4,,10
        .p2align 3
.L6:
        movl    -4(%rax), %edx
        subq    $8, %rax
        movq    %rax, stack+8(%rip)
        testl   %edx, %edx
        jne     .L4
.L5:
        cmpq    %rax, %rcx
        jne     .L6
.L4:
        ret

We really should order the basic blocks putting cmpq before L6 saving a jump.
Moreover clang does

        .p2align        4, 0x90
.LBB1_1:                                # =>This Inner Loop Header: Depth=1
        cmpq    %rax, %rcx
        je      .LBB1_3
# %bb.2:                                #   in Loop: Header=BB1_1 Depth=1
        cmpl    $0, -4(%rcx)
        leaq    -8(%rcx), %rcx
        movq    %rcx, stack+8(%rip)
        je      .LBB1_1
.LBB1_3:
        retq

saving an instruction. Why we do not move stack+8 updating out of the loop?


---


### compiler : `gcc`
### title : `[14 Regression] r14-172 caused some SPEC2017 bmk to degrade on Power`
### open_at : `2023-05-15T02:06:56Z`
### last_modified_date : `2023-06-20T10:35:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109858
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
On Power9, 
  - at option -Ofast, 557.xz_r degraded -1.06%.
  - at option -Ofast, 511.povray_r degraded -1.24%.

On Power10,
  - at option -O2, 520.omnetpp_r degraded -1.84%.

I tried to run them in a few times, the gaps are stable. I've excluded those one in +-1% as noises. The full data is listed below:

P9 O2:
500.perlbench_r	0.00%
502.gcc_r	-0.20%
505.mcf_r	0.00%
520.omnetpp_r	-0.30%
523.xalancbmk_r	0.00%
525.x264_r	0.00%
531.deepsjeng_r	0.00%
541.leela_r	0.00%
548.exchange2_r	0.00%
557.xz_r	0.00%
503.bwaves_r	0.00%
507.cactuBSSN_r	0.00%
508.namd_r	-0.32%
510.parest_r	0.00%
511.povray_r	0.32%
519.lbm_r	-0.16%
521.wrf_r	0.00%
526.blender_r	0.27%
527.cam4_r	0.00%
538.imagick_r	0.00%
544.nab_r	0.00%
549.fotonik3d_r	0.00%
554.roms_r	0.00%

P9 Ofast:
500.perlbench_r	0.00%
502.gcc_r	0.00%
505.mcf_r	0.00%
520.omnetpp_r	0.00%
523.xalancbmk_r	0.00%
525.x264_r	0.17%
531.deepsjeng_r	0.00%
541.leela_r	0.00%
548.exchange2_r	2.15%
557.xz_r	-1.06%
503.bwaves_r	2.21%
507.cactuBSSN_r	-0.20%
508.namd_r	-0.33%
510.parest_r	0.00%
511.povray_r	-1.24%
519.lbm_r	0.00%
521.wrf_r	0.19%
526.blender_r	0.00%
527.cam4_r	0.28%
538.imagick_r	0.32%
544.nab_r	0.00%
549.fotonik3d_r	-0.83%
554.roms_r	1.33%

Power10 O2:
500.perlbench_r	0.00%
502.gcc_r	0.33%
505.mcf_r	0.55%
520.omnetpp_r	-1.84%
523.xalancbmk_r	0.00%
525.x264_r	0.36%
531.deepsjeng_r	0.00%
541.leela_r	0.00%
548.exchange2_r	0.00%
557.xz_r	0.31%
503.bwaves_r	0.00%
507.cactuBSSN_r	-0.31%
508.namd_r	-0.22%
510.parest_r	-0.64%
511.povray_r	-0.86%
519.lbm_r	-0.75%
521.wrf_r	0.20%
526.blender_r	0.20%
527.cam4_r	0.00%
538.imagick_r	-0.15%
544.nab_r	-0.18%
549.fotonik3d_r	0.00%
554.roms_r	0.00%

Power10 Ofast:
502.gcc_r	0.16%
505.mcf_r	0.00%
520.omnetpp_r	0.00%
523.xalancbmk_r	0.22%
525.x264_r	0.30%
531.deepsjeng_r	0.00%
541.leela_r	0.00%
548.exchange2_r	0.00%
557.xz_r	0.31%
	
503.bwaves_r	-0.18%
507.cactuBSSN_r	0.00%
508.namd_r	0.22%
510.parest_r	-0.52%
511.povray_r	0.00%
519.lbm_r	0.50%
521.wrf_r	0.00%
526.blender_r	-0.80%
527.cam4_r	0.18%
538.imagick_r	-0.11%
544.nab_r	0.17%
549.fotonik3d_r	0.63%
554.roms_r	-0.63%


---


### compiler : `gcc`
### title : `IV-OPTs could use int but still uses smaller sized for IV`
### open_at : `2023-05-15T16:24:54Z`
### last_modified_date : `2023-05-17T06:46:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109862
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(char a)
{
        unsigned char t;
        unsigned short t1 = a;
        for(t = 0; t < 8; t ++)
        {
                t1 >>=1;
                t1 += a;
        }
        return t1;
}
int f1(char a)
{
        unsigned int t;
        unsigned short t1 = a;
        for(t = 0; t < 8; t ++)
        {
                t1 >>=1;
                t1 += a;
        }
        return t1;
}

```
f1 produces better code as there is no extra zero-extend (anding) inside the loop for the IV of t.

Note this shows up in coremarks in the crc functions (if not unrolling).


---


### compiler : `gcc`
### title : `Sometimes using sub/test instead just test`
### open_at : `2023-05-15T18:44:19Z`
### last_modified_date : `2023-06-06T21:50:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109866
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int g(void); int h(void); int t(void);
int f(int a, int b)
{
  int c = a - b;
  if(c == 0)
    return g();
  if (c > 0)
    return h();
  return t();
}
```
This is reduced from bzip2 in spec 2006, though I am not so sure any more.
On x86_64 GCC produces:
```
        subl    %esi, %edi
        testl   %edi, %edi
        je      .L5
        jle     .L3
        jmp     h()
.L3:
        jmp     t()
.L5:
        jmp     g()
```
But GCC should produce (likes clang/LLVM does):
```
        cmpl    %esi, %edi
        je      .L5
        jle     .L3
        jmp     h()
.L3:
        jmp     t()
.L5:
        jmp     g()
```

Note a similar thing happens with aarch64 target too.


---


### compiler : `gcc`
### title : `comparing SCHAR_MIN and SCHAR_MAX but with widden type could be optimized better`
### open_at : `2023-05-16T02:48:53Z`
### last_modified_date : `2023-05-17T06:58:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109869
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take (for most targets):
```
bool f1 (signed char i)
{
  unsigned long long _1 = i;
  bool _5 = _1 == 127;
  bool _6 = _1 == 18446744073709551488ull;
  return _5 | _6;
}
bool f2 (signed char i)
{
  return i == -128 || i == 127;
  __SIZE_TYPE__ _1 = i;
}
```

These two functions should produce the same code.

I noticed this while looking at PR 77899 .


---


### compiler : `gcc`
### title : `[SH] GCC 13's -Os code is 50% bigger than GCC 4's`
### open_at : `2023-05-16T10:51:34Z`
### last_modified_date : `2023-07-07T06:12:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109874
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
Using the following C code snippet:

------
unsigned int CHRmask1,CHRmask2,CHRmask4,CHRmask8;

void SetupCartCHRMapping(unsigned int size)
{
#if 0
    CHRmask1 = (size >> 10) - 1;
    CHRmask2 = (size >> 11) - 1;
    CHRmask4 = (size >> 12) - 1;
    CHRmask8 = (size >> 13) - 1;
#else
    size >>= 10;

    CHRmask1 = size - 1;
    size >>= 1;
    CHRmask2 = size - 1;
    size >>= 1;
    CHRmask4 = size - 1;
    size >>= 1;
    CHRmask8 = size - 1;
#endif
}
------

Compiling with -Os, GCC 13.1 will generate the exact same code for the two cases, as it rightfully detects that they are functionally the same:

------
_SetupCartCHRMapping:
        mov.l   r12,@-r15
        mova    .L3,r0
        mov.l   .L3,r12
        mov     r4,r1
        shlr8   r1
        add     r0,r12
        mov.l   .L4,r0
        shlr2   r1
        add     #-1,r1
        mov.l   r1,@(r0,r12)
        mov     r4,r1
        shlr8   r1
        mov.l   .L5,r0
        shlr    r1
        shlr2   r1
        add     #-1,r1
        mov.l   r1,@(r0,r12)
        mov     r4,r1
        shlr8   r1
        mov.l   .L6,r0
        shlr2   r1
        shlr2   r1
        shlr8   r4
        add     #-1,r1
        shlr2   r4
        mov.l   r1,@(r0,r12)
        shlr    r4
        mov.l   .L7,r0
        shlr2   r4
        add     #-1,r4
        mov.l   r4,@(r0,r12)
        rts     
        mov.l   @r15+,r12
.L3:
        .long   _GLOBAL_OFFSET_TABLE_
.L4:
        .long   _CHRmask1@GOTOFF
.L5:
        .long   _CHRmask2@GOTOFF
.L6:
        .long   _CHRmask4@GOTOFF
.L7:
        .long   _CHRmask8@GOTOFF
_CHRmask8:
        .zero   4
_CHRmask4:
        .zero   4
_CHRmask2:
        .zero   4
_CHRmask1:
        .zero   4
------

The code part (excluding labels and data fields) is 33 instructions.

GCC 4.9.4 won't detect that the two versions of the code are equivalent, and generate different machine code for them. The second version generates the smallest code, at only 21 instructions:

------
_SetupCartCHRMapping:
        shlr8   r4
        shlr2   r4
        mov.l   .L2,r1
        mov     r4,r2
        add     #-1,r2
        mov.l   r2,@r1
        mov     r4,r1
        mov.l   .L3,r2
        shlr    r1
        add     #-1,r1
        mov.l   r1,@r2
        shlr2   r4
        mov.l   .L4,r1
        mov     r4,r2
        add     #-1,r2
        mov.l   r2,@r1
        shlr    r4
        mov.l   .L5,r1
        add     #-1,r4
        rts     
        mov.l   r4,@r1
.L2:
        .long   _CHRmask1
.L3:
        .long   _CHRmask2
.L4:
        .long   _CHRmask4
.L5:
        .long   _CHRmask8
------

So GCC 13.1 at -Os generates code that is 50% bigger than what GCC 4 would generate for a functionally equivalent algorithm.


---


### compiler : `gcc`
### title : `missed simplifications of MAX<a&CST0,a&CST1> and MIN<a&CST0,a&CST1>`
### open_at : `2023-05-16T19:56:16Z`
### last_modified_date : `2023-09-08T22:51:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109878
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int max_and(int a, int b)
{
        b = a & 3;
        a = a & 1;
        if (b > a)
          return b;
        else
          return a;
}
int min_and(int a, int b)
{
        b = a & 3;
        a = a & 1;
        if (b < a)
          return b;
        else
          return a;
}
```
max_and should just be optimized to `a&3` while min_and should be just optimized to `a&1` The general rule is:
MAX<a&CST0, a & CST1> -> a & CST0 IFF CST0 &CST1 == CST1, that is CST1 is a true subset of CST0.

I found this on accident while thinking about some generated code in insn-automata.cc on x86_64.
MIN<a&CST0, a & CST1> -> a & CST0 IFF CST0&CST1 == CST0, that is CST0 is a true subset of CST1.


---


### compiler : `gcc`
### title : `gcc does not generate movmskps and testps instructions  (clang does)`
### open_at : `2023-05-17T07:40:19Z`
### last_modified_date : `2023-05-17T19:35:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109885
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
in this simple code (on avx2)

int sum(float const * x) {
   int ret = 0;
   for (int i=0; i<8; ++i) ret +=(0==x[i]);
   return ret;
}

int one(float const * x) {
   int ret = 0;
   for (int i=0; i<8; ++i) ret |=(0==x[i]);
   return ret;
}

int all(float const * x) {
   int ret = 1;
   for (int i=0; i<8; ++i) ret &=(0==x[i]);
   return ret;
}

clang uses movmskps and testps instructions, gcc does not

see for instance

https://godbolt.org/z/r11r8xoYz


---


### compiler : `gcc`
### title : `SLP failure with explicit fma`
### open_at : `2023-05-17T14:35:36Z`
### last_modified_date : `2023-05-22T11:40:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109892
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.0`
### severity : `enhancement`
### contents :
At -O2 -mfma (x86) or -O3 (arm64) we fail to SLP-vectorize 'f', but succeed in 'g':

double f(double x[], long n)
{
    double r0 = 0, r1 = 0;
    for (; n; x += 2, n--) {
        r0 = __builtin_fma(x[0], x[0], r0);
        r1 = __builtin_fma(x[1], x[1], r1);
    }
    return r0 + r1;
}
static double muladd(double x, double y, double z)
{
    return x * y + z;
}
double g(double x[], long n)
{
    double r0 = 0, r1 = 0;
    for (; n; x += 2, n--) {
        r0 = muladd(x[0], x[0], r0);
        r1 = muladd(x[1], x[1], r1);
    }
    return r0 + r1;
}

It seems we are calling vectorizable_reduction for __builtin_fma even though it would not participate in a reduction when vectorizing for 16-byte vectors?


---


### compiler : `gcc`
### title : `[14 Regression]  Missed Dead Code Elimination when using __builtin_unreachable since  r14-160-gf828503eeb79ad1f1ada6db7deccc5abcc2f3ca3`
### open_at : `2023-05-17T15:15:58Z`
### last_modified_date : `2023-08-09T07:29:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109893
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
void foo(void);
void bar(void);
static char a;
static int b, e, f;
static int *c = &b, *g;
int main() {
    int *j = 0;
    if (a) {
        g = 0;
        if (c)
            bar();
    } else {
        j = &e;
        c = 0;
    }
    if (c == &f == b || c == &e)
        ;
    else
        __builtin_unreachable();
    if (g || e) {
        if (j == &e || j == 0)
            ;
        else
            foo();
    }
    a = 4;
}

gcc -O3: 

main:
        cmpb    $0, a(%rip)
        je      .L2
        xorl    %esi, %esi
        cmpq    $0, c(%rip)
        movq    %rsi, g(%rip)
        je      .L7
        pushq   %rdx
        call    bar
        movb    $4, a(%rip)
        xorl    %eax, %eax
        popq    %rcx
        ret
.L2:
        xorl    %eax, %eax
        movq    %rax, c(%rip)
.L7:
        movb    $4, a(%rip)
        xorl    %eax, %eax
        ret
c:
        .quad   b

gcc-trunk -O3 

main:
        subq    $8, %rsp
        cmpb    $0, a(%rip)
        je      .L2
        xorl    %edx, %edx
        cmpq    $0, c(%rip)
        movq    %rdx, g(%rip)
        je      .L6
        call    bar
        xorl    %eax, %eax
.L4:
        cmpq    $0, g(%rip)
        je      .L9
.L6:
        movb    $4, a(%rip)
        xorl    %eax, %eax
        addq    $8, %rsp
        ret
.L2:
        xorl    %eax, %eax
        movq    %rax, c(%rip)
        movl    $e, %eax
        jmp     .L4
.L9:
        cmpl    $0, e(%rip)
        je      .L6
        testq   %rax, %rax
        je      .L6
        cmpq    $e, %rax
        je      .L6
        call    foo
        jmp     .L6
c:
        .quad   b

Bisects to: https://gcc.gnu.org/git/gitweb.cgi?p=gcc.git;h=f828503eeb79ad1f1ada6db7deccc5abcc2f3ca3


---


### compiler : `gcc`
### title : `Missed optimisation: overflow detection in multiplication instructions for operator new`
### open_at : `2023-05-17T18:34:39Z`
### last_modified_date : `2023-05-18T14:08:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109896
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.1`
### severity : `normal`
### contents :
In the following code:
struct S
{
    char buf[47];       // weird size
};

void *f(unsigned long paramCount)
{
    return new S[paramCount];
}

GCC generates (see https://gcc.godbolt.org/z/o5eocj5n9):
        movabsq $196241958230952676, %rax
        cmpq    %rdi, %rax
        jb      .L2
        imulq   $47, %rdi, %rdi
        jmp     operator new[](unsigned long)
f(unsigned long) [clone .cold]:
.L2:
        pushq   %rax
        call    __cxa_throw_bad_array_new_length

That's a slight pessimisation of the typical, non-exceptional case because of the presence of the compare instructions. On modern x86, that's 3 retire slots and 2 uops, in addition to the multiplication's 3 cycles (which may be speculated and start early). But the presence of a 10-byte instruction and the fact that the jump is further than 8-bit displacement range mean those three instructions occupy 18 bytes, meaning the front-end is sub-utilised, requiring 2 cycles to decode the 5 instructions (pre-GLC [I think] CPUs decode 4 instructions in 16 bytes per cycle).

Instead, GCC should emit the multiplication and check if the overflow flag was set. I believe the optimal code for GCC would be:

        imulq   $47, %rdi, %rdi
        jo      .L2
        jmp     operator new[](unsigned long)

That's 15 bytes, so 1 cycle for the decoder to decode all 3 instructions. That's 3+1 cycles and 2 retire slots before the JMP.

In the Godbolt link above, Clang and MSVC emitted a CMOV:

        mulq    %rcx
        movq    $-1, %rdi
        cmovnoq %rax, %rdi
        jmp     operator new[](unsigned long)@PLT

This is slightly worse (19 bytes, 4 instructions, though also 3+1 cycles). For GCC's -fno-exceptions case, I recommend keeping the IMUL+JO case and only load -1 in the .text.unlikely section. But see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109895


---


### compiler : `gcc`
### title : `_mm256_abs_epi8 is not expanded on gimple level`
### open_at : `2023-05-17T23:20:40Z`
### last_modified_date : `2023-05-24T02:09:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109900
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
Take (at -O3 -march=x86-64-v3):
```
#include <immintrin.h>
__m256i
should_be_cmpeq_abs0 ()
{
  return _mm256_set1_epi8 (1);
}
__m256i
should_be_cmpeq_abs1 ()
{
  return _mm256_abs_epi8(_mm256_set1_epi8 (-1));
}
```
I would have expected these two produce the same code generation.
In the end, we still have a builtin function in the IR rather than ABS_EXPR. The RTL level uses abs.
In fact combine tries to combine the two instructions:
Trying 5 -> 6:
    5: r85:V32QI=const_vector
    6: r84:V32QI=abs(r85:V32QI)
      REG_DEAD r85:V32QI
      REG_EQUAL const_vector
Failed to match this instruction:
(set (reg:V32QI 84)
    (const_vector:V32QI [
            (const_int 1 [0x1]) repeated x32
        ]))


---


### compiler : `gcc`
### title : `Optimization opportunity: ((((a) > (b)) - ((a) < (b))) < 0) -> ((a) < (b))`
### open_at : `2023-05-18T00:43:13Z`
### last_modified_date : `2023-08-02T13:21:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109901
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
LLVM/Clang also misses this optimization opportunity, so I also filed an issue with them:

https://github.com/llvm/llvm-project/issues/62790

The following transformations can be done as an optimization:

((((a) > (b)) - ((a) < (b))) < 0) -> ((a) < (b))

((((a) > (b)) - ((a) < (b))) <= 0) -> ((a) <= (b))

((((a) > (b)) - ((a) < (b))) == -1) -> ((a) < (b))

((((a) > (b)) - ((a) < (b))) == 1) -> ((a) > (b))

((((a) > (b)) - ((a) < (b))) == 0) -> ((a) == (b))

((((a) > (b)) - ((a) < (b))) > 0) -> ((a) > (b))

((((a) > (b)) - ((a) < (b))) >= 0) -> ((a) >= (b))

((((a) >= (b)) - ((a) <= (b))) < 0) -> ((a) < (b))

((((a) >= (b)) - ((a) <= (b))) <= 0) -> ((a) <= (b))

((((a) >= (b)) - ((a) <= (b))) == -1) -> ((a) < (b))

((((a) >= (b)) - ((a) <= (b))) == 1) -> ((a) > (b))

((((a) >= (b)) - ((a) <= (b))) == 0) -> ((a) == (b))

((((a) >= (b)) - ((a) <= (b))) > 0) -> ((a) > (b))

((((a) >= (b)) - ((a) <= (b))) >= 0) -> ((a) >= (b))


Both (((a) > (b)) - ((a) < (b))) and (((a) >= (b)) - ((a) <= (b))) will generate -1, 0 or 1 when comparing two integers (signed or unsigned). When comparators using this trick are inlined into the caller, the above transformations become applicable.

I noticed that neither compiler exploits this high level optimization opportunity when I was working on using a faster binary search implementation for the OpenZFS b-tree code. It relied on a macro to achieve C++-style inlining of the comparator into the implementation by making different versions of the same function.

The following example at godbolt does not use a macro to make it easier to see which lines of assembly correspond to which lines of high level C:

https://gcc.godbolt.org/z/cdedccxae

On amd64, GCC generates 15 instructions for the loop. If you comment out line 35 and uncomment line 37, GCC will generate 11 instructions for the loop. This is the output GCC would produce if it supported that high level optimization.

Had the comparator returned 1 for less than and 0 for greater than or equal to, we would have had the 11-instruction version of the loop without any need for this optimization. Changing the semantics because our compilers lack this optimization would be painful in part because the entire code base expects the -1, 0 or 1 return value semantics and other code depends on these comparators.

It would be nice if GCC implemented this optimization.


---


### compiler : `gcc`
### title : `a rrotate (32-b) -> a lrotate b`
### open_at : `2023-05-19T08:05:09Z`
### last_modified_date : `2023-05-19T08:05:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109906
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
static inline
unsigned rotateright(unsigned x, int t)
{
  if (t >= 32) __builtin_unreachable();
  unsigned tl = x >> (t);
  unsigned th = x << (32-t);
  return tl | th;
}

unsigned rotateleft(unsigned t, int x)
{
  return rotate (t, 32-x);
}
```

I would have assumed GCC would produce a rotate left for rotateleft but does not currently.


---


### compiler : `gcc`
### title : `Missed optimization for bit extraction (uses shift instead of single bit-test)`
### open_at : `2023-05-19T10:10:10Z`
### last_modified_date : `2023-06-11T09:26:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109907
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55116
C test case.

The following missed optimization occurs with current v14 master and also with older versions of the compiler:

$ avr-gcc ext.c -dumpbase "" -save-temps -dp -mmcu=atmega128 -c -Os

Functons like

uint8_t cset_32bit31 (uint32_t x)
{
    return (x & (1ul << 31)) ? 1 : 0; // bloat
}

that extract a single bit might generate very expensive code like:

cset_32bit31:
	movw r26,r24	 ;  18	[c=4 l=1]  *movhi/0
	movw r24,r22	 ;  19	[c=4 l=1]  *movhi/0
	lsl r27	 ;  24	[c=16 l=4]  *ashrsi3_const/3
	sbc r24,r24
	mov r25,r24
	movw r26,r24
	andi r24,lo8(1)	 ;  12	[c=4 l=1]  andqi3/1
	ret		 ;  22	[c=0 l=1]  return

where the following 3 instructions would suffice.  This is smaller, faster and imposes no additioal register pressure:

	bst r25,7	 ;  16	[c=4 l=3]  *extzv/4
	clr r24
	bld r24,0

What also would work is loading 0 or 1 depending on a single bit like:

LDI  r24, 0  # R24 = 0
SBRC r25, 7  # Skip next instruction if R25.7 == 0.
LDI  r24, 1  # R24 = 1

The bloat also occurs when the complement of the bit is extracted like in

uint8_t cset_32bit30_not (uint32_t x)
{
    return (x & (1ul << 30)) ? 0 : 1; // bloat 
}

cset_32bit30_not:
	movw r26,r24	 ;  19	[c=4 l=1]  *movhi/0
	movw r24,r22	 ;  20	[c=4 l=1]  *movhi/0
	ldi r18,30	 ;  25	[c=44 l=7]  *lshrsi3_const/3
	1:	
	lsr r27
	ror r26
	ror r25
	ror r24
	dec r18	
	brne 1b	
	ldi r18,1	 ;  7	[c=32 l=2]  xorsi3/2
	eor r24,r18
	andi r24,lo8(1)	 ;  13	[c=4 l=1]  andqi3/1
	ret		 ;  23	[c=0 l=1]  return

This case is even worse because it's a loop of 30 single bit-shifts to extract the bit.  Again, skipping one instrauction depending on a bit was possible:

LDI  r24, 1  # R24 = 1
SBRC r25, 6  # Skip next instruction if R25.7 == 0.
LDI  r24, 0  # R24 = 0

or

LDI  r24, 0  # R24 = 0
SBRS r25, 6  # Skip next instruction if R25.7 == 1.
LDI  r24, 1  # R24 = 1

or extract one bit using the T-flag:

BST r25, 6     # SREG.T = R25.6
LDI r24, 0xff  # R24 = 0xff
BLD r24, 0     # R24.0 = SREG.T
COM r24        # R24 = R24 ^ 0xff

-------------------------------------------------------

Configured with: --target=avr --disable-nls --with-dwarf2 --with-gnu-as --with-gnu-ld --disable-shared --enable-languages=c,c++

gcc version 14.0.0 20230518 (experimental) (GCC)


---


### compiler : `gcc`
### title : `transform atomic exchange to unconditional store when old value is unused?`
### open_at : `2023-05-22T10:49:12Z`
### last_modified_date : `2023-06-01T01:40:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109930
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.1.1`
### severity : `enhancement`
### contents :
I'm not sure if that is a valid substitution, but...

I have a state machine that has a few transitions where I already know the old state, so I can simply do an unconditional store, but I'd also like to have an assertion on the old state in my debug version:

    std::atomic<uint32_t> x;

    /* ... */

    auto old_value = x.exchange(5);
    assert(old_value == 3);

with NDEBUG set, the assert is omitted, and no one is interested in the old value, so the load-with-reserve can be omitted and the store-conditional replaced with a regular store, and this should still be semantically equivalent.


---


### compiler : `gcc`
### title : `Knowledge on literal not used in optimization`
### open_at : `2023-05-22T12:33:29Z`
### last_modified_date : `2023-05-22T14:53:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109931
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Function for comparing a lower-cased string with runtime string of known size:


constexpr bool ICaseEqualLowercase(const char* lowercase, const char* y,
                                   unsigned size) noexcept {
    constexpr char kLowerToUpperMask = static_cast<char>(~unsigned{32});
    for (unsigned i = 0; i < size; ++i) {
        const auto lowercase_c = lowercase[i];
        if (lowercase_c != y[i]) {
            if (!('a' <= lowercase_c && lowercase_c <= 'z') ||
                (lowercase_c & kLowerToUpperMask) != y[i]) {
                return false;
            }
        }
    }

    return true;
}
bool test2(const char* y) {
    return ICaseEqualLowercase("hello", y, 5);
}


With GCC trunk and -O2 flags the GCC fails to understand that all the characters of `lowercase` are lowercase ASCII and the expression `!('a' <= lowercase_c && lowercase_c <= 'z')` is always `false`.

Because of that, additional instructions in loop are emitted:
        lea     esi, [rdx-97]
        cmp     sil, 25
        ja      .L6


Godbolt playground: https://godbolt.org/z/xrc1T4oeW


---


### compiler : `gcc`
### title : `((a^c) & b) | a is not opimized to (b & c) | a`
### open_at : `2023-05-23T07:13:57Z`
### last_modified_date : `2023-08-27T20:57:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109938
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(int a, int b, int c)
{
  return ((a^c) & b) | a;
}
int f1(int a, int b, int c)
{
  return ((a^c) | a) & (b|a);
}
int f2(int a, int b, int c)
{
  return (b & c) | a;
}
```

These 3 functions should produce the same code.
Currently only f1 and f2 produces the decent code.

I noticed this while looking into PR 107887 and it is related to PR 100864.


---


### compiler : `gcc`
### title : `[13/14 Regression] Missed Dead Code Elimination when using __builtin_unreachable since r13-6834-g41ade3399bd`
### open_at : `2023-05-23T13:30:32Z`
### last_modified_date : `2023-08-21T23:09:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109943
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
void foo(void);
static int b = 5;
static int *c = &b;
static char(a)(char d, char g) {
    ;
    return d % g;
}
static void e(unsigned char d, unsigned g) {
    char h = a(g, d);
    if (h)
        if ((d) >= 0) {
            foo();
        };
}
static void f(int d) { e(b, d); }
int main() {
    f(*c);
    *c = 0;
}

gcc-trunk -O3 generates:

main:
        movl    $0, b(%rip)
        xorl    %eax, %eax
        ret

However, including additional info via __builtin_unreachable results to worse code:

void foo(void);
static int b = 5;
static int *c = &b;
static char(a)(char d, char g) {
    if ((g) <= 0) {
        __builtin_unreachable();
    }
    return d % g;
}
static void e(unsigned char d, unsigned g) {
    char h = a(g, d);
    if (h)
        if ((d) >= 0) {
            foo();
        };
}
static void f(int d) { e(b, d); }
int main() {
    f(*c);
    *c = 0;
}

gcc-trunk (and 13) -O3 generates:

main:
        movl    b(%rip), %ecx
        movsbl  %cl, %eax
        movzbl  %cl, %ecx
        cltd
        idivl   %ecx
        testl   %edx, %edx
        jne     .L11
        movl    $0, b(%rip)
        xorl    %eax, %eax
        ret
.L11:
        pushq   %rax
        call    foo
        xorl    %edx, %edx
        xorl    %eax, %eax
        movl    %edx, b(%rip)
        popq    %rcx
        ret

https://godbolt.org/z/n34Tsr3vq

gcc-12 can generate optimized code for both cases

Started with r13-6834-g41ade3399bd


---


### compiler : `gcc`
### title : `vector CTOR with byte elements and SSE2 has STLF fail`
### open_at : `2023-05-23T13:48:26Z`
### last_modified_date : `2023-05-24T12:48:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109944
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
I've experimented with CTORs from smaller elements and byte handling with
plain SSE2 is quite bad (for word we have pinsrw).

void foo(char *a, char *m, char d, char e)
{
  char b = *m;
  char c = m[2];
  a[0] = b;
  a[1] = c;
  a[2] = d;
  a[3] = e;
  a[4] = b;
  a[5] = c;
  a[6] = b;
  a[7] = c;
  a[8] = b;
  a[9] = c;
  a[10] = b;
  a[11] = c;
  a[12] = b;
  a[13] = c;
  a[14] = b;
  a[15] = c;
}

generates

        movzbl  2(%rsi), %r8d
        movl    %edx, %r9d
        movzbl  (%rsi), %edx
        movzbl  %cl, %ecx
        movzbl  %r9b, %r9d
        movq    %r8, %rax
        salq    $8, %rax
        orq     %rdx, %rax
        salq    $8, %rax
        orq     %r8, %rax
        salq    $8, %rax
        orq     %rdx, %rax
        salq    $8, %rax
        orq     %rax, %rcx
        orq     %r8, %rax
        salq    $8, %rcx
        salq    $8, %rax
        orq     %r9, %rcx
        orq     %rdx, %rax
        salq    $8, %rcx
        salq    $8, %rax
        orq     %r8, %rcx
        orq     %r8, %rax
        salq    $8, %rcx
        salq    $8, %rax
        orq     %rdx, %rcx
        orq     %rdx, %rax
        movq    %rcx, -24(%rsp)
        movq    %rax, -16(%rsp)
        movdqa  -24(%rsp), %xmm0
        movups  %xmm0, (%rdi)

while we can handle a splat from QImode via

        movzbl  (%rsi), %eax
        movd    %eax, %xmm0
        punpcklbw       %xmm0, %xmm0
        punpcklwd       %xmm0, %xmm0
        pshufd  $0, %xmm0, %xmm0
        movups  %xmm0, (%rdi)

I think we can go and for a generic V16QImode CTOR and SSE2 create two
V8HImode vectors using pinsrw, for the first from zero-extended QImode
values of the even elements and for the second from zero-extended and
left-shifted values of the odd elements and then IOR the two vectors.

Alternatively the above needs to be pessimized better in the cost model.

Btw, for HImode elements I see we do

        movzwl  (%rsi), %eax
        movd    %eax, %xmm0
        movdqa  %xmm0, %xmm1
        movdqa  %xmm0, %xmm2
        pinsrw  $1, 4(%rsi), %xmm1
...

not sure why we don't do

        pxor %xmm1, %xmm1
        pinsrw  $0, (%rsi), %xmm1

and thus avoid the round-trip through the GPR for the initial element?


---


### compiler : `gcc`
### title : `Missing loop PHI optimization`
### open_at : `2023-05-24T17:30:32Z`
### last_modified_date : `2023-06-07T02:43:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109957
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
void foo();
int main() {
  _Bool c = 0;
  _Bool e = 1;
  int i;
  for (i = 0; i < 10000; i++)
  {
    c |= (e!=0);
    e = 0;
  }
  if (c == 0)
    foo();
  return 0;
}
```

This should be just optimized to just `return 0`.
The reason is once c is 1, it will always stay 1.
But currently we don't notice that.

Note this code is reduced from PR 108352 testcase after a phiopt improvement that provided the above form and ran into a testcase failure because of that.


---


### compiler : `gcc`
### title : ``(a > 1) ? 0 : (a == 1)` is not optimized when spelled out at -O2+`
### open_at : `2023-05-24T22:10:02Z`
### last_modified_date : `2023-09-21T14:16:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109959
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
_Bool f(unsigned a)
{
        if (a > 1)
          return 0;
        return a == 1;
}


_Bool f0(unsigned a)
{
  return (a > 1) ? 0 : (a == 1);
}
```
Both of these should just optimize to:
`return a == 1`, f0 is currently.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] missing combining of `(a&1) != 0 || (a&2)!=0` into `(a&3)!=0``
### open_at : `2023-05-25T00:40:12Z`
### last_modified_date : `2023-09-21T14:16:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=109960
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take the following C++ code (reduced from stmt_can_terminate_bb_p):
```
static inline bool f1(unsigned *a)
{
        return (*a&1);
}
static inline bool f2(unsigned *a)
{
        return (*a&2);
}

bool f(int c, unsigned *a)
{
  if (c)
    return 0;
  return f2(a) || f1(a) ;
}
```

At -O1 we can produce:
```
        movl    $0, %eax
        testl   %edi, %edi
        jne     .L1
        testb   $3, (%rsi)
        setne   %al
.L1:
        ret
```
But at -O2 we get:
        xorl    %eax, %eax
        testl   %edi, %edi
        jne     .L1
        movl    (%rsi), %edx
        movl    %edx, %eax
        andl    $1, %eax
        andl    $2, %edx
        movl    $1, %edx
        cmovne  %edx, %eax
.L1:
        ret

Which is just so much worse.
This started in GCC 9.


---
