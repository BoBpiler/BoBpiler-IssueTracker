### Total Bugs Detected: 4649
### Current Chunk: 29 of 30
### Bugs in this Chunk: 160 (From bug 4481 to 4640)
---


### compiler : `gcc`
### title : `std::basic_string::assign dramatically slower than other means of copying memory`
### open_at : `2023-08-08T10:42:26Z`
### last_modified_date : `2023-08-17T20:40:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110945
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `12.2.0`
### severity : `normal`
### contents :
See https://quick-bench.com/q/bqGjfyd180oOlJhiY_XnURMNKG8

Using the copy constructor performs best, and ends up using std::memcpy internally. Even using .resize() and std::copy is much faster than .assign(), because it is subject to more partial loop unrolling.

basic_string::assign:
https://github.com/gcc-mirror/gcc/blob/25c4b1620ebc10fceabd86a34fdbbaf8037e7e82/libstdc%2B%2B-v3/include/bits/basic_string.h#L1713C28-L1713C28

this calls the four-iterator form of .replace():
https://github.com/gcc-mirror/gcc/blob/25c4b1620ebc10fceabd86a34fdbbaf8037e7e82/libstdc%2B%2B-v3/include/bits/basic_string.h#L2378

this calls this form of _M_replace_dispatch(): (I think)
https://github.com/gcc-mirror/gcc/blob/25c4b1620ebc10fceabd86a34fdbbaf8037e7e82/libstdc%2B%2B-v3/include/bits/basic_string.tcc#L430

this calls _M_replace():
https://github.com/gcc-mirror/gcc/blob/25c4b1620ebc10fceabd86a34fdbbaf8037e7e82/libstdc%2B%2B-v3/include/bits/basic_string.tcc#L507

in this case, it should call _S_move():
https://github.com/gcc-mirror/gcc/blob/25c4b1620ebc10fceabd86a34fdbbaf8037e7e82/libstdc%2B%2B-v3/include/bits/basic_string.h#L431

this calls char_traits::move():
https://github.com/gcc-mirror/gcc/blob/25c4b1620ebc10fceabd86a34fdbbaf8037e7e82/libstdc%2B%2B-v3/include/bits/char_traits.h#L223

and that calls __builtin_memcpy()

However, I must have followed this chain of calls incorrectly, because I do not see a call to memmove in the output assembly, and most of the time is spent here:

>        nopl   (%rax)
>        movdqa 0x42d8a0(%rdx),%xmm0
> 63.27% movups %xmm0,(%rax,%rdx,1)
> 36.69% add    $0x10,%rdx
> 0.03%  cmp    $0x100000,%rdx


---


### compiler : `gcc`
### title : `((cast)cmp) - 1 should be tranformed into -(cast)cmp` where cmp` is the inverse of cmp`
### open_at : `2023-08-08T18:25:58Z`
### last_modified_date : `2023-08-24T21:27:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110949
### status : `ASSIGNED`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f1(int a, int t)
{
  auto _6 = a == 115;
  auto _7 = (signed int) _6;
  return _6 - 1;
}
```
This should be the same as:
```
int f2(int a, int t)
{
  auto _6 = a != 115;
  auto _7 = (signed int) _6;
  return -_6;
}
```


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-2946-g46c8c225455`
### open_at : `2023-08-09T19:42:55Z`
### last_modified_date : `2023-08-15T09:24:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110963
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/devdMf1qd

Given the following code:

void foo(void);
static int c = 76, f, g;
static int *h, *j, *k = &g;
static int **i = &h;
static short a;
static char(l)(char b) {
    if (!(((b) >= 77) && ((b) <= 77))) {
        __builtin_unreachable();
    }
    return 0;
}
static short(m)(short d, short e) { return d + e; }
static short n(char) {
    j = *i;
    if (j == 0)
        ;
    else
        *i = 0;
    *k = 0;
    return 0;
}
static char o() {
    l(0);
    return 0;
}
static char p(int ad) {
    a = m(!0, ad);
    l(a);
    if (f) {
        *i &&n(o());
        *i = 0;
    } else
        n(0);
    if (h == &f || h == 0)
        ;
    else
        foo();
    return 0;
}
int main() {
    p(c);
    c = 8;
}

gcc-trunk -O3 does not eliminate the call to foo:

main:
	movl	f(%rip), %edi
	testl	%edi, %edi
	je	.L11
.L7:
	movl	$8, c(%rip)
	xorl	%eax, %eax
	ret
.L11:
	cmpq	$0, h(%rip)
	je	.L4
	xorl	%esi, %esi
	movq	%rsi, h(%rip)
.L4:
	movq	h(%rip), %rax
	xorl	%ecx, %ecx
	movl	%ecx, g(%rip)
	testq	%rax, %rax
	je	.L7
	cmpq	$f, %rax
	je	.L7
	pushq	%rax
	call	foo
	xorl	%eax, %eax
	movl	$8, c(%rip)
	popq	%rdx
	ret

gcc-13.2.0 -O3 eliminates the call to foo:

main:
	movl	f(%rip), %ecx
	movq	h(%rip), %rax
	testl	%ecx, %ecx
	jne	.L3
	testq	%rax, %rax
	je	.L4
	xorl	%edx, %edx
	movq	%rdx, h(%rip)
.L4:
	xorl	%eax, %eax
	movl	%eax, g(%rip)
.L3:
	movl	$8, c(%rip)
	xorl	%eax, %eax
	ret

Bisects to r14-2946-g46c8c225455


---


### compiler : `gcc`
### title : `missing combining if ranges due to cast differences`
### open_at : `2023-08-10T06:17:28Z`
### last_modified_date : `2023-08-10T07:02:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110965
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(long long l)
{
  int t =  (int)l;
  if (t == 1 || t == 2)
    return true;
  if (t == 3)
    return true;

  return false;
}

int f1(long long l)
{
  unsigned t =  (unsigned)l;
  if (t == 1 || t == 2)
    return true;
  if (t == 3)
    return true;
  return false;
}
```

f and f1 should produce the same code but currently does not.


---


### compiler : `gcc`
### title : `9% 444.namd regression between g:c2a447d840476dbd (2023-08-03 18:47) and g:73da34a538ddc2ad (2023-08-09 20:17)`
### open_at : `2023-08-10T14:47:51Z`
### last_modified_date : `2023-08-29T14:43:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110973
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `middle-end`
### version : `13.1.0`
### severity : `normal`
### contents :
All is -Ofast -march=native
zen2
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=300.120.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=301.120.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=301.120.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=288.120.0
Icelake
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=789.120.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=790.120.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=785.120.0
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=791.120.0


---


### compiler : `gcc`
### title : `Miss-optimization for O2 fully masked loop on floating point reduction.`
### open_at : `2023-08-11T01:45:02Z`
### last_modified_date : `2023-08-11T13:32:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110979
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/YsaesW8zT

float
foo3 (float* __restrict a, int n)
{
    float sum = 0.0f;
    for (int i = 0; i != 100; i++)
      sum += a[i];
    return sum;
}

-O2 -march=znver4 --param vect-partial-vector-usage=2, we get

  <bb 3> [local count: 66437776]:
  # sum_13 = PHI <sum_10(3), 0.0(2)>
  # loop_mask_16 = PHI <_54(3), { -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 }(2)>
  # ivtmp.13_12 = PHI <ivtmp.13_15(3), ivtmp.13_1(2)>
  # ivtmp.16_2 = PHI <ivtmp.16_3(3), 84(2)>
  # DEBUG i => NULL
  # DEBUG sum => NULL
  # DEBUG BEGIN_STMT
  _4 = (void *) ivtmp.13_12;
  _11 = &MEM <vector(16) float> [(float *)_4];
  vect__4.6_17 = .MASK_LOAD (_11, 32B, loop_mask_16);
  cond_18 = .VCOND_MASK (loop_mask_16, vect__4.6_17, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
  stmp_sum_10.7_19 = BIT_FIELD_REF <cond_18, 32, 0>;
  stmp_sum_10.7_20 = sum_13 + stmp_sum_10.7_19;
  stmp_sum_10.7_21 = BIT_FIELD_REF <cond_18, 32, 32>;
  stmp_sum_10.7_22 = stmp_sum_10.7_20 + stmp_sum_10.7_21;
  stmp_sum_10.7_23 = BIT_FIELD_REF <cond_18, 32, 64>;
  stmp_sum_10.7_24 = stmp_sum_10.7_22 + stmp_sum_10.7_23;
  stmp_sum_10.7_25 = BIT_FIELD_REF <cond_18, 32, 96>;
  stmp_sum_10.7_26 = stmp_sum_10.7_24 + stmp_sum_10.7_25;
  stmp_sum_10.7_27 = BIT_FIELD_REF <cond_18, 32, 128>;
  stmp_sum_10.7_28 = stmp_sum_10.7_26 + stmp_sum_10.7_27;
  stmp_sum_10.7_29 = BIT_FIELD_REF <cond_18, 32, 160>;
  stmp_sum_10.7_30 = stmp_sum_10.7_28 + stmp_sum_10.7_29;
  stmp_sum_10.7_31 = BIT_FIELD_REF <cond_18, 32, 192>;
  stmp_sum_10.7_32 = stmp_sum_10.7_30 + stmp_sum_10.7_31;
  stmp_sum_10.7_33 = BIT_FIELD_REF <cond_18, 32, 224>;
  stmp_sum_10.7_34 = stmp_sum_10.7_32 + stmp_sum_10.7_33;
  stmp_sum_10.7_35 = BIT_FIELD_REF <cond_18, 32, 256>;
  stmp_sum_10.7_36 = stmp_sum_10.7_34 + stmp_sum_10.7_35;
  stmp_sum_10.7_37 = BIT_FIELD_REF <cond_18, 32, 288>;
  stmp_sum_10.7_38 = stmp_sum_10.7_36 + stmp_sum_10.7_37;
  stmp_sum_10.7_39 = BIT_FIELD_REF <cond_18, 32, 320>;
  stmp_sum_10.7_40 = stmp_sum_10.7_38 + stmp_sum_10.7_39;
  stmp_sum_10.7_41 = BIT_FIELD_REF <cond_18, 32, 352>;
  stmp_sum_10.7_42 = stmp_sum_10.7_40 + stmp_sum_10.7_41;
  stmp_sum_10.7_43 = BIT_FIELD_REF <cond_18, 32, 384>;
  stmp_sum_10.7_44 = stmp_sum_10.7_42 + stmp_sum_10.7_43;
  stmp_sum_10.7_45 = BIT_FIELD_REF <cond_18, 32, 416>;
  stmp_sum_10.7_46 = stmp_sum_10.7_44 + stmp_sum_10.7_45;
  stmp_sum_10.7_47 = BIT_FIELD_REF <cond_18, 32, 448>;
  stmp_sum_10.7_48 = stmp_sum_10.7_46 + stmp_sum_10.7_47;
  stmp_sum_10.7_49 = BIT_FIELD_REF <cond_18, 32, 480>;
  sum_10 = stmp_sum_10.7_48 + stmp_sum_10.7_49;
  # DEBUG sum => sum_10
  # DEBUG BEGIN_STMT
  # DEBUG i => NULL
  # DEBUG sum => sum_10
  # DEBUG BEGIN_STMT
  _53 = {ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2, ivtmp.16_2};
  _54 = _53 > { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };
  ivtmp.13_15 = ivtmp.13_12 + 64;
  ivtmp.16_3 = ivtmp.16_2 + 240;
  if (ivtmp.16_3 != 228)


Looks like an cost model issue?

For aarch64, it looks fine since they have FADDA(Floating-point add strictly-ordered reduction, accumulating in scalar).


---


### compiler : `gcc`
### title : `(unsigned)(signed_char) != (unsigned)-1 is never changed back into signed_char != -1`
### open_at : `2023-08-11T03:25:10Z`
### last_modified_date : `2023-09-25T08:52:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110982
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
While looking into a failure after improving VRP, I see we never translate this:
```
bool f(signed char t)
{
  unsigned int t1 = t;
  return t1 != (unsigned int)-1;
}
```
into:
```
bool f0(signed char t)
{
  return t != -1;
}
```


---


### compiler : `gcc`
### title : `phiopt's spaceship_replacement should support cast and !=`
### open_at : `2023-08-11T05:06:36Z`
### last_modified_date : `2023-08-11T09:16:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110984
### status : `ASSIGNED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Testcase:
```
int f5n (double i, double j)
{
        signed char c;
        if (i != j)
        {
          if (i < j)
           c = 1;
         else
           c = -1;
        }
        else
           c = 0;
        unsigned int t1 = c;
        return t1 != (unsigned int)-1;
}
```


---


### compiler : `gcc`
### title : `[14 Regression] aarch64 has support for conditional not (and vectorized conditional not ) after r14-3110-g7fb65f10285`
### open_at : `2023-08-11T08:04:14Z`
### last_modified_date : `2023-10-22T22:34:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110986
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
After commit r14-3110-g7fb65f10285 (MATCH: [PR110937/PR100798] (a ? ~b : b) should be optimized to b ^ -(a)), I have noticed regressions on aarch64:

Running gcc:gcc.target/aarch64/aarch64.exp ...
FAIL: gcc.target/aarch64/cond_op_imm_1.c scan-assembler csinv\tw[0-9]*.*
FAIL: gcc.target/aarch64/cond_op_imm_1.c scan-assembler csinv\tx[0-9]*.*

Running gcc:gcc.target/aarch64/sve/aarch64-sve.exp ...
FAIL: gcc.target/aarch64/sve/cond_unary_5.c scan-assembler-not \\tmov\\tz
FAIL: gcc.target/aarch64/sve/cond_unary_5.c scan-assembler-times \\tneg\\tz[0-9]+\\.b, p[0-7]/m, 3
FAIL: gcc.target/aarch64/sve/cond_unary_5.c scan-assembler-times \\tneg\\tz[0-9]+\\.h, p[0-7]/m, 2
FAIL: gcc.target/aarch64/sve/cond_unary_5.c scan-assembler-times \\tneg\\tz[0-9]+\\.s, p[0-7]/m, 1
FAIL: gcc.target/aarch64/sve/cond_unary_5.c scan-assembler-times \\tnot\\tz[0-9]+\\.b, p[0-7]/m, 3
FAIL: gcc.target/aarch64/sve/cond_unary_5.c scan-assembler-times \\tnot\\tz[0-9]+\\.h, p[0-7]/m, 2
FAIL: gcc.target/aarch64/sve/cond_unary_5.c scan-assembler-times \\tnot\\tz[0-9]+\\.s, p[0-7]/m, 1


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O2 since r14-1135-gc53f51005de`
### open_at : `2023-08-11T13:08:42Z`
### last_modified_date : `2023-08-15T09:09:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110991
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static unsigned char a;
static char b;
void foo(void);
int main() {
  a = 25;
  for (; a > 13; --a)
    b = a > 127 ?: a << 3;
  if (!b)
    foo();
}

gcc-3a13884b23a (trunk) -O2 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O2 can.
-----------------------------------------------------------------------
gcc-3a13884b23ae32b43d56d68a9c6bd4ce53d60017 -O2 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movd	.LC0(%rip), %xmm0
	movd	.LC1(%rip), %xmm1
	xorl	%eax, %eax
.L2:
	addl	$1, %eax
	movdqa	%xmm0, %xmm2
	paddb	%xmm1, %xmm0
	cmpl	$3, %eax
	jne	.L2
	movdqa	%xmm2, %xmm0
	movb	$13, a(%rip)
	paddb	%xmm2, %xmm0
	paddb	%xmm0, %xmm0
	movdqa	%xmm0, %xmm1
	paddb	%xmm0, %xmm1
	pxor	%xmm0, %xmm0
	pcmpgtb	%xmm2, %xmm0
	movd	.LC2(%rip), %xmm2
	pand	%xmm0, %xmm2
	pandn	%xmm1, %xmm0
	por	%xmm2, %xmm0
	movd	%xmm0, %eax
	sarl	$24, %eax
	movb	%al, b(%rip)
	testb	%al, %al
	je	.L10
	xorl	%eax, %eax
	ret
.L10:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O2 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB0:
	.cfi_startproc
	movb	$112, b(%rip)
	xorl	%eax, %eax
	movb	$13, a(%rip)
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-1135-gc53f51005de


---


### compiler : `gcc`
### title : `[13/14 Regression] missed VRP optimization due to transformation of `a & -zero_one_valued_p` into `a * zero_one_valued_p``
### open_at : `2023-08-11T13:24:08Z`
### last_modified_date : `2023-09-18T23:51:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110992
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static unsigned b;
static short c = 4;
void foo(void);
static short(a)(short d, short g) { return d * g; }
void e();
static char f() {
  b = 0;
  return 0;
}
int main() {
  int h = b;
  if ((short)(a(c && e, 65535) & h)) {
    foo();
    h || f();
  }
}

gcc-bcda361daae (trunk) -O3 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O3 can.
-----------------------------------------------------------------------
gcc-bcda361daaec8623c91d0dff3ea8e576373b5f50 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	cmpw	$0, b(%rip)
	jne	.L8
	xorl	%eax, %eax
	ret
.L8:
	pushq	%rax
	.cfi_def_cfa_offset 16
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	.cfi_def_cfa_offset 8
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB2:
	.cfi_startproc
	xorl	%eax, %eax
	ret
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-1654-g7ceed7e3e29


---


### compiler : `gcc`
### title : `Code generation for vectorized -(a[i] != 0) with number of elements change could be improved`
### open_at : `2023-08-12T09:37:21Z`
### last_modified_date : `2023-08-21T07:47:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111002
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Testcase:
```
void __attribute__ ((noipa))
f (int *__restrict r,
   int *__restrict a,
   short *__restrict pred)
{
  for (int i = 0; i < 1024; ++i)
    r[i] = pred[i] != 0 ? -1 : 0;
}
```

Kinda of patch:
```
/* Sink unary operations to branches, but only if we do fold both.  */
(for op (negate bit_not abs absu)
 (simplify
  (op (view_convert? (vec_cond:s @0 @1 @2)))
  (if (element_precision (type) == element_precision (@1))
   (vec_cond @0 (op! (view_convert @1)) (op! (view_convert @2))))))
```

That is `Sink unary operations` one needs to add support for view_convert there ...

I Noticed this while working on PR 110986 (but is not needed for that issue).


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O3 since r14-2161-g237e83e2158`
### open_at : `2023-08-12T13:53:57Z`
### last_modified_date : `2023-08-14T08:03:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111003
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static int c, d, e, f;
static short g;
static int *h = &c;
void foo(void);
short(a)();
static unsigned b(unsigned char j, int l) { return j > l ? j : j << l; }
static int *i();
static void k(int j, unsigned char l) {
  i();
  g = f;
  f = g;
  for (; g;) {
    int m;
    d = a();
    for (; d;) {
      if (l)
        if (!(j >= -639457069 && j <= -639457069))
          if (m)
            foo();
      m = (10 != (l ^ b(j, 6))) < (0 > e);
    }
  }
}
static int *i() {
  for (; e; e = a(e, 6))
    ;
  return h;
}
int main() { k(c, c); }

gcc-8441841a1b9 (trunk) -O3 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O3 can.
-----------------------------------------------------------------------
gcc-8441841a1b985d68245954af1ff023db121b0635 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB3:
	.cfi_startproc
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	movl	e(%rip), %edi
	movl	c(%rip), %ebx
	testl	%edi, %edi
	je	.L5
	.p2align 4,,10
	.p2align 3
.L2:
	movl	$6, %esi
	xorl	%eax, %eax
	call	a
	movswl	%ax, %edi
	movl	%edi, e(%rip)
	testl	%edi, %edi
	jne	.L2
.L5:
	movl	f(%rip), %eax
	movswl	%ax, %edx
	movw	%ax, g(%rip)
	movl	%edx, f(%rip)
	testw	%ax, %ax
	je	.L36
	movzbl	%bl, %ebp
	testb	%bl, %bl
	je	.L39
	movl	%ebp, %eax
	sall	$6, %eax
	xorl	%ebp, %eax
	cmpl	$10, %eax
	setne	%r12b
	.p2align 4,,10
	.p2align 3
.L18:
	xorl	%eax, %eax
	call	a
	cwtl
	movl	%eax, d(%rip)
	testl	%eax, %eax
	je	.L14
	cmpl	$-639457069, %ebx
	jne	.L17
.L19:
	jmp	.L19
	.p2align 4,,10
	.p2align 3
.L16:
	cmpl	$6, %ebp
	jg	.L20
	movl	e(%rip), %eax
	xorl	%r13d, %r13d
	shrl	$31, %eax
	cmpb	%al, %r12b
	setb	%r13b
	.p2align 4,,10
	.p2align 3
.L17:
	testl	%r13d, %r13d
	je	.L16
	call	foo
	movl	e(%rip), %eax
	shrl	$31, %eax
	cmpl	$6, %ebp
	setg	%dl
	xorl	%r13d, %r13d
	orl	%r12d, %edx
	cmpb	%al, %dl
	movl	d(%rip), %eax
	setb	%r13b
	testl	%eax, %eax
	jne	.L17
.L14:
	cmpw	$0, g(%rip)
	jne	.L18
.L36:
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 40
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
.L39:
	.cfi_restore_state
	cmpl	$6, %ebp
	jg	.L7
	.p2align 4,,10
	.p2align 3
.L10:
	xorl	%eax, %eax
	call	a
	cwtl
	movl	%eax, d(%rip)
	testl	%eax, %eax
	je	.L8
.L9:
	jmp	.L9
	.p2align 4,,10
	.p2align 3
.L12:
	cmpw	$0, g(%rip)
	je	.L36
.L7:
	xorl	%eax, %eax
	call	a
	cwtl
	movl	%eax, d(%rip)
	testl	%eax, %eax
	je	.L12
.L13:
	jmp	.L13
	.p2align 4,,10
	.p2align 3
.L8:
	cmpw	$0, g(%rip)
	jne	.L10
	jmp	.L36
	.p2align 4,,10
	.p2align 3
.L20:
	cmpl	$-639457069, %ebx
	jne	.L20
	jmp	.L19
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB3:
	.cfi_startproc
	movl	e(%rip), %edi
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movl	c(%rip), %ebx
	testl	%edi, %edi
	je	.L5
	.p2align 4,,10
	.p2align 3
.L2:
	movl	$6, %esi
	xorl	%eax, %eax
	call	a
	movswl	%ax, %edi
	movl	%edi, e(%rip)
	testl	%edi, %edi
	jne	.L2
.L5:
	movl	f(%rip), %eax
	movswl	%ax, %edx
	movw	%ax, g(%rip)
	movl	%edx, f(%rip)
	testw	%ax, %ax
	je	.L37
	movzbl	%bl, %eax
	cmpl	$6, %eax
	jg	.L6
	testb	%bl, %bl
	jne	.L7
	.p2align 4,,10
	.p2align 3
.L10:
	xorl	%eax, %eax
	call	a
	cwtl
	movl	%eax, d(%rip)
	testl	%eax, %eax
	je	.L8
.L9:
	jmp	.L9
	.p2align 4,,10
	.p2align 3
.L18:
	cmpw	$0, g(%rip)
	je	.L37
.L6:
	xorl	%eax, %eax
	call	a
	cwtl
	movl	%eax, d(%rip)
	testl	%eax, %eax
	je	.L18
	cmpl	$-639457069, %ebx
	je	.L39
.L19:
	testl	%eax, %eax
	je	.L18
	cmpl	$-639457069, %ebx
	jne	.L19
.L39:
	jmp	.L39
	.p2align 4,,10
	.p2align 3
.L8:
	cmpw	$0, g(%rip)
	jne	.L10
.L37:
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L7:
	.cfi_restore_state
	cmpl	$-639457069, %ebx
	je	.L15
	.p2align 4,,10
	.p2align 3
.L12:
	xorl	%eax, %eax
	call	a
	cwtl
	movl	%eax, d(%rip)
	testl	%eax, %eax
	je	.L16
.L17:
	jmp	.L17
	.p2align 4,,10
	.p2align 3
.L13:
	cmpw	$0, g(%rip)
	je	.L37
.L15:
	xorl	%eax, %eax
	call	a
	cwtl
	movl	%eax, d(%rip)
	testl	%eax, %eax
	je	.L13
.L14:
	jmp	.L14
	.p2align 4,,10
	.p2align 3
.L16:
	cmpw	$0, g(%rip)
	jne	.L12
	jmp	.L37
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-2161-g237e83e2158
CC to include: rguenther@suse.de


---


### compiler : `gcc`
### title : `SVE produced code for different type sizes (smaller than int) with comparison in a loop can be improved`
### open_at : `2023-08-12T20:03:46Z`
### last_modified_date : `2023-10-22T22:34:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111005
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
void __attribute__ ((noipa))
f0 (int *__restrict r,
   int *__restrict a,
   short *__restrict pred)
{
  for (int i = 0; i < 1024; ++i)
  {
    int p = pred[i]?-1:0;
    r[i] = p ;
  }
}

void __attribute__ ((noipa))
f1 (int *__restrict r,
   int *__restrict a,
   short *__restrict pred)
{
  for (int i = 0; i < 1024; ++i)
  {
    int p = pred[i];
    r[i] = p ;
  }
}
```

f1 produces:
```
.L6:
        ld1sh   z31.s, p7/z, [x2, x1, lsl 1]
        st1w    z31.s, p7, [x0, x1, lsl 2]
        incw    x1
        whilelo p7.s, w1, w3
        b.any   .L6
```

While f0 produces:
```
.L2:
        ld1h    z0.h, p0/z, [x2, x1, lsl 1]
        punpklo p2.h, p0.b
        cmpne   p3.h, p1/z, z0.h, #0
        punpkhi p0.h, p0.b
        mov     z0.h, p3/z, #1
        neg     z0.h, p1/m, z0.h
        sunpklo z1.s, z0.h
        sunpkhi z0.s, z0.h
        st1w    z1.s, p2, [x0, x1, lsl 2]
        st1w    z0.s, p0, [x4, x1, lsl 2]
        inch    x1
        whilelo p0.h, w1, w3
        b.any   .L2
```

While it should produce:
```
.L6:
        ld1sh   z31.s, p7/z, [x2, x1, lsl 1]
        cmpne   p1.s,  p7/z, z31.s, #0
        mov     z31.s, p1/z, #-1                 // =0xffffffffffffffff
        st1w    z31.s, p7, [x0, x1, lsl 2]
        incw    x1
        whilelo p7.s, w1, w3
        b.any   .L6
```

That is:
sign extend load
compare-not-equal to 0; setting p1
set z31 to -1 or 0 based on p1
store z31

But instead we push to do unpacking from VN2HI to VHI ...


---


### compiler : `gcc`
### title : `[SVE] Extra neg for storing to short from int comparison`
### open_at : `2023-08-12T20:11:26Z`
### last_modified_date : `2023-08-20T07:27:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111006
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
void __attribute__ ((noipa))
f0 (unsigned short *__restrict r,
   int *__restrict a,
   int *__restrict pred)
{
  for (int i = 0; i < 1024; ++i)
  {
    int p = pred[i]?-1:0;
    r[i] = p ;
  }
}
```
Compile with `-march=armv8.5+sve2 -O3`.

Currently we get:
```
.L2:
        ld1w    z31.s, p7/z, [x2, x1, lsl 2]
        cmpne   p15.s, p6/z, z31.s, #0
        mov     z31.s, p15/z, #1
        neg     z31.h, p6/m, z31.h
        st1h    z31.s, p7, [x0, x1, lsl 1]
        incw    x1
        whilelo p7.s, w1, w3
        b.any   .L2
```
But we should just get:
```
.L2:
        ld1w    z31.s, p7/z, [x2, x1, lsl 2]
        cmpne   p15.s, p6/z, z31.s, #0
        mov     z31.s, p15/z, #-1
        st1h    z31.s, p7, [x0, x1, lsl 1]
        incw    x1
        whilelo p7.s, w1, w3
        b.any   .L2
```


---


### compiler : `gcc`
### title : `[13/14 regression] error: unable to find a register to spill compiling GCDAProfiling.c since r13-5092-g4e0b504f26f78f`
### open_at : `2023-08-14T09:42:40Z`
### last_modified_date : `2023-08-23T14:46:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111010
### status : `RESOLVED`
### tags : `ice-on-valid-code, missed-optimization, ra`
### component : `target`
### version : `13.2.1`
### severity : `normal`
### contents :
Building current LLVM main with GCC 13.1.0 fails on Solaris/amd64, compiling
compiler-rt/lib/profile/GCDAProfiling.c.  I could reduce that file to the attached
testcase:

$ gcc -O3 -m32 -fPIC -c GCDAProfiling.i -w
GCDAProfiling.i: In function ‘c’:
GCDAProfiling.i:21:1: error: unable to find a register to spill
   21 | }
      | ^
GCDAProfiling.i:21:1: error: this is the insn:
(insn 109 333 249 9 (set (reg:DI 300)
        (ior:DI (ashift:DI (zero_extend:DI (mem:SI (plus:SI (mult:SI (reg:SI 377 [orig:229 _118 ] [229])
                                (const_int 4 [0x4]))
                            (reg/f:SI 338 [orig:83 a.0_1 ] [83])) [3 MEM[(unsigned int *)_11]+0 S4 A32]))
                (const_int 32 [0x20]))
            (zero_extend:DI (mem:SI (plus:SI (mult:SI (reg:SI 299 [233])
                            (const_int 4 [0x4]))
                        (reg/f:SI 338 [orig:83 a.0_1 ] [83])) [3 MEM[(unsigned int *)_15]+0 S4 A32])))) "GCDAProfiling.i":15:7 680 {*concatsidi3_3}
     (expr_list:REG_DEAD (reg:SI 377 [orig:229 _118 ] [229])
        (expr_list:REG_DEAD (reg/f:SI 338 [orig:83 a.0_1 ] [83])
            (expr_list:REG_DEAD (reg:SI 299 [233])
                (nil)))))

The corresponding cc1 invocation is

cc1 -fpreprocessed GCDAProfiling.i -quiet -m32 -mtune=generic -march=pentium4 -O3 -w -fPIC

The testcase compiles on the gcc-12 branch and trunk, so this is gcc-13 branch
regression only.

However, it does compile with a Linux/x86_64 gcc 13.1.0.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O3 since r14-573-g69f1a8af45d`
### open_at : `2023-08-14T12:17:50Z`
### last_modified_date : `2023-08-14T15:43:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111012
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static int b, c;
static char d;
static short e = -1L;
static int *j = &c;
void foo(void);
void bar150_(void);
void bar173_(void);
static char(a)(char k, char l) { return k + l; }
static void g(unsigned k, int l) {
  if (l)
    if (!k)
      foo();
  if (k)
    bar150_();
}
static const unsigned char h();
static char i(int k) {
  if (k)
    bar173_();
  c <= 0 >= b;
  if (k)
    return c;
  return c;
}
static void f(char k, unsigned) {
  char m = h(8 != c);
  g(m && 8, k);
}
static const unsigned char h(int k) {
  d = i(c);
  *j = a(e, d < k < k && c) ^ k;
  b = 0;
  return c;
}
int main() { f(c, b); }

gcc-9ec5d6de735 (trunk) -O3 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O3 can.
-----------------------------------------------------------------------
gcc-9ec5d6de7355c15b3811150d1581dab5bd489966 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB5:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset 3, -24
	subq	$8, %rsp
	.cfi_def_cfa_offset 32
	movl	c(%rip), %ebx
	cmpl	$8, %ebx
	setne	%bpl
	testl	%ebx, %ebx
	jne	.L9
	movl	$-2, c(%rip)
	xorl	%eax, %eax
	movl	%eax, b(%rip)
.L4:
	call	bar150_
.L6:
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 24
	xorl	%eax, %eax
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L9:
	.cfi_restore_state
	call	bar173_
	movl	c(%rip), %edx
	movzbl	%bpl, %ebp
	movl	$0, b(%rip)
	movsbl	%dl, %eax
	cmpl	%eax, %ebp
	setg	%al
	movzbl	%al, %eax
	cmpl	%eax, %ebp
	setg	%al
	testl	%edx, %edx
	setne	%dl
	andl	%edx, %eax
	subl	$1, %eax
	movsbl	%al, %eax
	xorl	%ebp, %eax
	movl	%eax, c(%rip)
	testb	%bl, %bl
	je	.L3
	testb	%al, %al
	jne	.L4
	call	foo
	jmp	.L6
.L3:
	testb	%al, %al
	je	.L6
	jmp	.L4
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O3 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB5:
	.cfi_startproc
	movl	c(%rip), %eax
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	xorl	%ebx, %ebx
	cmpl	$8, %eax
	setne	%bl
	testl	%eax, %eax
	jne	.L10
.L2:
	cmpl	%eax, %ebx
	setg	%al
	movzbl	%al, %eax
	cmpl	%eax, %ebx
	jg	.L3
	notl	%ebx
.L4:
	movl	%ebx, c(%rip)
	movl	$0, b(%rip)
	call	bar150_
	xorl	%eax, %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L3:
	.cfi_restore_state
	cmpl	$1, c(%rip)
	sbbb	%bl, %bl
	xorl	$1, %ebx
	movsbl	%bl, %ebx
	jmp	.L4
.L10:
	call	bar173_
	movsbl	c(%rip), %eax
	jmp	.L2
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-573-g69f1a8af45d


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression at -O2 since r14-338-g1dd154f6407`
### open_at : `2023-08-14T12:47:53Z`
### last_modified_date : `2023-08-25T19:07:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111013
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
static int b, c, e;
static unsigned d;
static short f;
void foo(void);
void(a)();
static char g(int h) {
  int i = 2749857453;
j:
  a();
k:
  if (b)
    goto j;
  f = 1;
  if ((unsigned)(7 ^ e | (h & d && f)) >= 2)
    i = 0;
  e = 0;
  if (c)
    goto k;
  if (!(i <= 6))
    foo();
  return h;
}
int main() {
  d--;
  b = c = e && g(1);
}

gcc-9ec5d6de735 (trunk) -O2 cannot eliminate the call to foo but gcc-releases/gcc-13.1.0 -O2 can.
-----------------------------------------------------------------------
gcc-9ec5d6de7355c15b3811150d1581dab5bd489966 -O2 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	e(%rip), %eax
	subl	$1, d(%rip)
	testl	%eax, %eax
	jne	.L3
.L2:
	movl	%eax, c(%rip)
	movl	%eax, b(%rip)
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L3:
	.cfi_restore_state
	xorl	%eax, %eax
	call	a
	movl	b(%rip), %edx
	testl	%edx, %edx
	jne	.L3
	movl	d(%rip), %ecx
	movl	e(%rip), %eax
	movl	$-1545109843, %edx
	xorl	%esi, %esi
	movl	c(%rip), %edi
	andl	$1, %ecx
	jmp	.L5
	.p2align 4,,10
	.p2align 3
.L8:
	xorl	%eax, %eax
.L5:
	xorl	$7, %eax
	orl	%ecx, %eax
	cmpl	$2, %eax
	cmovnb	%esi, %edx
	testl	%edi, %edi
	jne	.L8
	xorl	%eax, %eax
	movl	%eax, e(%rip)
	cmpl	$6, %edx
	jg	.L15
.L6:
	movl	$1, %eax
	jmp	.L2
.L15:
	call	foo
	jmp	.L6
---------- END OUTPUT ---------

-----------------------------------------------------------------------
gcc-2b98cc24d6af0432a74f6dad1c722ce21c1f7458 -O2 case.c -S -o case.s
--------- OUTPUT ---------
main:
.LFB1:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	e(%rip), %eax
	subl	$1, d(%rip)
	testl	%eax, %eax
	jne	.L3
.L2:
	movl	%eax, c(%rip)
	movl	%eax, b(%rip)
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L3:
	.cfi_restore_state
	xorl	%eax, %eax
	call	a
	movl	b(%rip), %edx
	movl	c(%rip), %eax
	testl	%edx, %edx
	jne	.L3
	testl	%eax, %eax
	je	.L4
.L5:
	jmp	.L5
.L4:
	xorl	%eax, %eax
	movl	%eax, e(%rip)
	movl	$1, %eax
	jmp	.L2
---------- END OUTPUT ---------

-----------------------------------------------------------------------
Bisects to r14-338-g1dd154f6407


---


### compiler : `gcc`
### title : `missing extendv4siv4hi (and friends)`
### open_at : `2023-08-15T07:33:52Z`
### last_modified_date : `2023-08-21T09:25:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111023
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
We could vectorize gcc.dg/vect/pr65947-7.c if we implement the
extendv4siv4hi pattern (sign-extend V4HI to V4SI).  We can already do
vec_unpacks_lo via

        pcmpgtw %xmm0, %xmm1
        movdqa  %xmm0, %xmm2
        punpcklwd       %xmm1, %xmm2

and that would trivially extend to the required pattern - just the
input is v4hi instead of v8hi.

Other related patterns are probably missing as well, where we can do
vec_unpack[s]_lo we should be able to implement [zero_]extend.


---


### compiler : `gcc`
### title : `attribute((malloc)) and posix_memalign() (and other functions that return newly allocated object address into an output parameter)`
### open_at : `2023-08-15T09:47:44Z`
### last_modified_date : `2023-08-15T16:03:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111025
### status : `NEW`
### tags : `missed-optimization`
### component : `c`
### version : `unknown`
### severity : `normal`
### contents :
Functions such as posix_memalign() don't return the pointer to the newly allocated memory as their return value, thus attribute((malloc)) cannot be used with them.

It would be useful to have some form of attribute((malloc)) that could apply to function such as posix_memalign().

This new attribute((malloc)) form could also be used on asprintf() for example.

With support for the attribute((malloc))'s deallocator specification, it could improve warnings at compile time and prevents developer mistake.


---


### compiler : `gcc`
### title : `using small types inside loops sometimes confuses the vectorizer`
### open_at : `2023-08-15T23:17:05Z`
### last_modified_date : `2023-08-22T04:35:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111032
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
void __attribute__ ((noipa))
f0 (int *__restrict r,
   int *__restrict a,
   int *__restrict pred)
{
  for (int i = 0; i < 1024; ++i)
  {
    unsigned short p = pred[i]?3:0;
    r[i] = p ;
  }
}

void __attribute__ ((noipa))
f1 (int *__restrict r,
   int *__restrict a,
   int *__restrict pred)
{
  for (int i = 0; i < 1024; ++i)
  {
    int p = pred[i]?1<<3:0;
    r[i] = p ;
  }
}
```

These 2 functions should produce the same code, selecting between 8 and 0 but instead in f0, we have a truncation and then an extension.

This happens on x86_64 at -O3 and aarch64 at -O3.

Though aarch64 with `-O3 -march=armv8.5-a+sve2` will be fixed with the patch to PR 111006 (which I will be submitting later today) because SVE uses conversions rather than VEC_PACK_TRUNC_EXPR/vec_unpack_hi_expr/vec_unpack_lo_expr here.


---


### compiler : `gcc`
### title : `std::format_to(std::back_inserter(str), "") should write directly to the string`
### open_at : `2023-08-17T20:56:30Z`
### last_modified_date : `2023-08-18T16:21:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111052
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.1.1`
### severity : `normal`
### contents :
fmt::format_to has a nice optimization where it recognizes a back_insert_iterator<string> and extracts the contained string so that it can format directly to the string, instead of writing one character at a time through the output iterator.

We could get the string, move it into a _Seq_sink<string> to format into it, then move assign (or swap) the populated string back to the original one that the iterator referred to.


---


### compiler : `gcc`
### title : `std::ranges::copy is missing important optimizations`
### open_at : `2023-08-17T21:24:36Z`
### last_modified_date : `2023-08-18T16:20:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111053
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `13.1.1`
### severity : `normal`
### contents :
std::copy is overloaded for a few special cases, like deque iterators and ostreambuf_iterators. std::ranges::copy is missing these optimizations.

    __copy_or_move(_Iter __first, _Sent __last, _Out __result)
    {
      // TODO: implement more specializations to be at least on par with
      // std::copy/std::move.

We should optimize copying a range to an ostreambuf_iterator, as we do for std::copy.

We should optimize copying to/from a deque, by working on a whole page at a time.

As mentioned in Bug 111052 comment 1 we could optimize copying a range to back_insert_iterator<C> by calling C::insert to insert the whole range at once (this might need ranges::to <https://wg21.link/P1206R7> support first, so it isn't limited to Cpp17Iterators). Similarly for insert iterators.


---


### compiler : `gcc`
### title : `5-10% regression of parest on icelake between g:d073e2d75d9ed492de9a8dc6970e5b69fae20e5a (Aug 15 2023) and g:9ade70bb86c8744f4416a48bb69cf4705f00905a (Aug 16)`
### open_at : `2023-08-18T11:48:15Z`
### last_modified_date : `2023-08-29T06:22:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111064
### status : `UNCONFIRMED`
### tags : `missed-optimization, needs-bisection`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
-Ofast -march=native
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=801.457.0
-Ofast -march=native -flto + PGO
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=792.457.0

It does not seem to show on zen or altra


---


### compiler : `gcc`
### title : `csneg is not used for (cset) * 2 - 1`
### open_at : `2023-08-19T21:13:47Z`
### last_modified_date : `2023-08-19T21:24:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111078
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
int f(int a, int b)
{
        return a == b ? 1 : -1;
}


int f0(int a, int b)
{
        int t = a == b;
        return t * 2 - 1;
}
```

These 2 should produce the same code on aarch64 but currently only the first one produces:
```
        cmp     w0, w1
        mov     w0, 1
        csneg   w0, w0, w0, eq
        ret
```


---


### compiler : `gcc`
### title : `Frame pointer is not used even when -fomit-frame-pointer is specified`
### open_at : `2023-08-22T05:25:59Z`
### last_modified_date : `2023-08-26T00:28:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111096
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
The code, by Kent Dickey posted to comp.arch

typedef unsigned int u32;
typedef unsigned long long u64;

u64 do_op(u64 out0, u64 in0, u64 in1, u32 opcode, int size);

void
calc_loop(u64 *optr, u64 *iptr0, u64 *iptr1, u32 opcode, int size, int len)
{
        u64     o0, i0, i1, val, result;
        int     num, shift, pos;
        int     i, j;

        // size is 0,1,2,3 representing 8,16,32,64 bytes
        num = 8 >> size;                // 8,4,2,1
        shift = 8 << size;              // 8,16,32,64
        for(i = 0; i < len; i++) {
                o0 = optr[i];
                i0 = iptr0[i];
                i1 = iptr1[i];
                result = 0;
                pos = 0;
                for(j = 0; j < num; j++) {
                        val = do_op(o0, i0, i1, opcode, size);
                        result = result | (val << pos);
                        pos += shift;
                        if(shift < 64) {
                                o0 = o0 >> shift;
                                i0 = i0 >> shift;
                                i1 = i1 >> shift;
                        }
                }
                optr[i] = result;
        }
}

compiled for aarch64 on godbolt with recent trunk and -O3 -fomit-frame-pointer
(see https://godbolt.org/z/5bKPeGWrK ) does not set up the frame pointer,
but it also does not use it for aoviding spill/restore pairs:

calc_loop:
        stp     x19, x20, [sp, -144]!
        mov     w6, 8
        asr     w19, w6, w4
        stp     x27, x28, [sp, 64]
        lsl     w27, w6, w4
        str     x30, [sp, 80]
        stp     x0, x1, [sp, 112]
        str     x2, [sp, 128]
        cmp     w5, 0
        ble     .L1
        sbfiz   x0, x5, 3, 32
        stp     x21, x22, [sp, 16]
        mov     w20, w4
        stp     x23, x24, [sp, 32]
        mov     w21, w3
        stp     x25, x26, [sp, 48]
        str     x0, [sp, 136]
        cmp     w27, 63
        ble     .L3
        mov     x25, 0
.L6:
        ldr     x0, [sp, 112]
        ldr     x23, [x0, x25]
        ldr     x0, [sp, 120]
        ldr     x0, [x0, x25]
        str     x0, [sp, 104]
        ldr     x0, [sp, 128]
        ldr     x24, [x0, x25]
        cbz     w19, .L10
        mov     w22, 0
        mov     w28, 0
        mov     x26, 0
.L5:
        ldr     x1, [sp, 104]
        mov     w4, w20
        mov     w3, w21
        mov     x2, x24
        mov     x0, x23
        add     w22, w22, 1
        bl      do_op
        lsl     x0, x0, x28
        add     w28, w28, w27
        orr     x26, x26, x0
        cmp     w19, w22
        bne     .L5
        ldr     x0, [sp, 112]
        str     x26, [x0, x25]
        add     x25, x25, 8
        ldr     x0, [sp, 136]
        cmp     x0, x25
        bne     .L6
.L17:
        ldp     x21, x22, [sp, 16]
        ldp     x23, x24, [sp, 32]
        ldp     x25, x26, [sp, 48]
.L1:
        ldp     x27, x28, [sp, 64]
        ldr     x30, [sp, 80]
        ldp     x19, x20, [sp], 144
        ret
.L3:
        str     xzr, [sp, 104]
        ldp     x0, x1, [sp, 104]
        ldr     x24, [x1, x0]
        ldr     x1, [sp, 120]
        ldr     x25, [x1, x0]
        ldr     x1, [sp, 128]
        ldr     x22, [x1, x0]
        cbz     w19, .L11
.L20:
        mov     w26, 0
        mov     w28, 0
        mov     x23, 0
.L8:
        mov     x2, x22
        mov     x1, x25
        mov     x0, x24
        mov     w4, w20
        mov     w3, w21
        add     w26, w26, 1
        bl      do_op
        lsr     x24, x24, x27
        lsl     x0, x0, x28
        add     w28, w28, w27
        orr     x23, x23, x0
        lsr     x25, x25, x27
        lsr     x22, x22, x27
        cmp     w19, w26
        bne     .L8
        ldp     x0, x1, [sp, 104]
        str     x23, [x1, x0]
        add     x0, x0, 8
        ldr     x1, [sp, 136]
        str     x0, [sp, 104]
        cmp     x1, x0
        beq     .L17
.L19:
        ldp     x0, x1, [sp, 104]
        ldr     x24, [x1, x0]
        ldr     x1, [sp, 120]
        ldr     x25, [x1, x0]
        ldr     x1, [sp, 128]
        ldr     x22, [x1, x0]
        cbnz    w19, .L20
.L11:
        ldp     x0, x1, [sp, 104]
        mov     x23, 0
        str     x23, [x1, x0]
        add     x0, x0, 8
        ldr     x1, [sp, 136]
        str     x0, [sp, 104]
        cmp     x1, x0
        bne     .L19
        b       .L17
.L10:
        ldr     x0, [sp, 112]
        mov     x26, 0
        str     x26, [x0, x25]
        add     x25, x25, 8
        ldr     x0, [sp, 136]
        cmp     x0, x25
        bne     .L6
        b       .L17


---


### compiler : `gcc`
### title : `(a|c) == (a|c)  should be convert into (a&~c) == (a&~c) IFF ~c simplifies`
### open_at : `2023-08-23T01:19:53Z`
### last_modified_date : `2023-08-23T07:57:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111108
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
#define C 0x1
unsigned g(unsigned X, unsigned Y)
{
        return (X | C) == (Y | C);
}
unsigned g1(unsigned X, unsigned Y)
{
        return (X & ~C) == (Y & ~C);
}
unsigned g2(unsigned X, unsigned Y)
{
        return ((X | C) & ~C) == ((Y | C) & ~C);
}
```
These all functions are the same as (X|C) == (Y|C) is saying the only bits of X and Y which you want to compare is ~C.

Note I suspect we could also do this for non-constant C but that would normally produce worse code but maybe we could see if both X & ~C and Y & ~C simplifies (due to non-zero bits ...).


---


### compiler : `gcc`
### title : `(A CMP B) != 0 and (A CMP B) == true patterns produce an extra (A CMP B) rather than reusing it`
### open_at : `2023-08-23T05:05:55Z`
### last_modified_date : `2023-08-23T23:16:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111110
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
(simplify
  (ne (cmp@2 @0 @1) integer_zerop)
  (if (types_match (type, TREE_TYPE (@2)))
   (cmp @0 @1)))
 (simplify
  (eq (cmp@2 @0 @1) integer_truep)
  (if (types_match (type, TREE_TYPE (@2)))
   (cmp @0 @1)))



Those (cmp @0 @1) results really should just be @2 ....


---


### compiler : `gcc`
### title : `RISC-V: Resulting more vsetvl instructions of vwadd + vadd  but not vwadd + vwadd`
### open_at : `2023-08-23T09:13:24Z`
### last_modified_date : `2023-08-23T09:14:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111112
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
For GCC, first res = b[i] + c[i], than res += promoted_d
For Clang, first res = promoted_d + b[i], than res += c[i]
Resulting more vsetvl instructions when using GCC's method
since vwadd and vadd need different type.

https://godbolt.org/z/9vexWjccn

C Code:
void __attribute__ ((noipa))
f (SIGNEDNESS char *restrict a, SIGNEDNESS char *restrict b,
   SIGNEDNESS char *restrict c, SIGNEDNESS char d)
{
  int promoted_d = d;
  for (int i = 0; i < 8; ++i)
    /* Deliberate use of signed >>.  */
    a[i] = ( promoted_d + b[i] + c[i]) >> 2;
}

RISC-V GCC:
f:
        vsetivli        zero,8,e16,m1,ta,ma
        vle8.v  v4,0(a1)
        vle8.v  v3,0(a2)
        vmv.v.x v2,a3
        vsetvli zero,zero,e8,mf2,ta,ma
        vwadd.vv        v1,v4,v3
        vsetvli zero,zero,e16,m1,ta,ma
        vadd.vv v1,v1,v2
        vsetvli zero,zero,e8,mf2,ta,ma
        vnsra.wi        v1,v1,2
        vse8.v  v1,0(a0)
        ret

RISC-V Clang:
f:                                      # @f
        vsetivli        zero, 8, e8, mf2, ta, ma
        vle8.v  v8, (a1)
        vle8.v  v9, (a2)
        vwadd.vx        v10, v8, a3
        vwadd.wv        v10, v10, v9
        vnsrl.wi        v8, v10, 2
        vse8.v  v8, (a0)
        ret


---


### compiler : `gcc`
### title : `RISC-V: Failed combine extend + vcond_mask when modify by vect_recog_over_widening_pattern`
### open_at : `2023-08-23T09:37:14Z`
### last_modified_date : `2023-08-24T04:50:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111114
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
The vect_recog_over_widening_pattern in vect pass will change:
  v2 = EXTEND (v1)
  v3 = VEC_COND_EXPR (mask, v2, { 1, 1, ... })
to:
  v2 = VEC_COND_EXPR (mask, v1, { 1, 1, ... })
  v3 = EXTEND (v2)
when it is safe. This change looks like no gain for riscv and makes it impossible to combine them into a MASK_EXTEND in combine pass.

https://godbolt.org/z/PWE4rzr7j


---


### compiler : `gcc`
### title : `Failure to vectorize conditional grouped store`
### open_at : `2023-08-23T09:50:37Z`
### last_modified_date : `2023-08-24T09:40:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111115
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
void foo (float * __restrict x, int *flag)
{
  for (int i = 0; i < 512; ++i)
    {
      if (flag[i])
        {
          float a = x[2*i+0] + 3.f;
          float b = x[2*i+1] + 177.f;
          x[2*i+0] = a;
          x[2*i+1] = b;
        }
    }
}

fails to vectorize on x86_64 with -march=znver4 (it needs masked stores
enabled by tuning).  This is because we do not support VMAT_CONTIGUOUS_PERMUTE
for either .MASK_LOAD nor .MASK_STORE.  Simply enabling that shows we fail
to properly handle the mask part.

The proper solution is to handle them in SLP which they are not either.


---


### compiler : `gcc`
### title : `Bogus -Wstringop-overread with -std=gnu++20 -O2 and std::vector<bool>`
### open_at : `2023-08-23T12:27:53Z`
### last_modified_date : `2023-08-23T15:49:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111118
### status : `UNCONFIRMED`
### tags : `alias, diagnostic, missed-optimization`
### component : `middle-end`
### version : `13.2.0`
### severity : `normal`
### contents :
The following C++ code compiled with "-c -O1 -std=gnu++20 -Werror=stringop-overread" emits a bogus warning/error with gcc trunk, while it works without problem when using "-O1 -std=gnu++17" or "-O0 -std=gnu++20" instead. Tested on compiler explorer on x64 Linux "https://godbolt.org/z/491jTvj65":

#include <vector>

struct S {
    std::vector<bool> p;
    void foo();
};

#ifdef INLINE
inline
#endif
void S::foo() {
    p.reserve(64);
}

=======================

The code above works fine when running on GCC 12.3.0, but GCC starts raising the warning bellow sicne GCC v13.1.

In file included from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/vector:62,
                 from <source>:1:
In static member function 'static constexpr _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = long unsigned int; _Up = long unsigned int; bool _IsMove = false]',
    inlined from 'constexpr _OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:506:30,
    inlined from 'constexpr _OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:533:42,
    inlined from 'constexpr _OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:540:31,
    inlined from 'constexpr _OI std::copy(_II, _II, _OI) [with _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:633:7,
    inlined from 'constexpr std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::_M_copy_aligned(const_iterator, const_iterator, iterator) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:1303:28,
    inlined from 'constexpr void std::vector<bool, _Alloc>::_M_reallocate(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/vector.tcc:851:40,
    inlined from 'constexpr void std::vector<bool, _Alloc>::reserve(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:1091:17,
    inlined from 'void S::foo()' at <source>:12:14:
/opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:437:30: warning: 'void* __builtin_memmove(void*, const void*, long unsigned int)' writing between 9 and 9223372036854775807 bytes into a region of size 8 overflows the destination [-Wstringop-overflow=]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/x86_64-linux-gnu/bits/c++allocator.h:33,
                 from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/allocator.h:46,
                 from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/vector:63:
In member function '_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = long unsigned int]',
    inlined from 'constexpr _Tp* std::allocator< <template-parameter-1-1> >::allocate(std::size_t) [with _Tp = long unsigned int]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/allocator.h:198:40,
    inlined from 'static constexpr _Tp* std::allocator_traits<std::allocator<_Up> >::allocate(allocator_type&, size_type) [with _Tp = long unsigned int]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/alloc_traits.h:482:28,
    inlined from 'constexpr std::_Bvector_base<_Alloc>::_Bit_pointer std::_Bvector_base<_Alloc>::_M_allocate(std::size_t) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:643:48,
    inlined from 'constexpr void std::vector<bool, _Alloc>::_M_reallocate(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/vector.tcc:849:43,
    inlined from 'constexpr void std::vector<bool, _Alloc>::reserve(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:1091:17,
    inlined from 'void S::foo()' at <source>:12:14:
/opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/new_allocator.h:147:55: note: destination object of size 8 allocated by 'operator new'
  147 |         return static_cast<_Tp*>(_GLIBCXX_OPERATOR_NEW(__n * sizeof(_Tp)));
      |                                                       ^
ASM generation compiler returned: 0
In file included from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/vector:62,
                 from <source>:1:
In static member function 'static constexpr _Up* std::__copy_move<_IsMove, true, std::random_access_iterator_tag>::__copy_m(_Tp*, _Tp*, _Up*) [with _Tp = long unsigned int; _Up = long unsigned int; bool _IsMove = false]',
    inlined from 'constexpr _OI std::__copy_move_a2(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:506:30,
    inlined from 'constexpr _OI std::__copy_move_a1(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:533:42,
    inlined from 'constexpr _OI std::__copy_move_a(_II, _II, _OI) [with bool _IsMove = false; _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:540:31,
    inlined from 'constexpr _OI std::copy(_II, _II, _OI) [with _II = long unsigned int*; _OI = long unsigned int*]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:633:7,
    inlined from 'constexpr std::vector<bool, _Alloc>::iterator std::vector<bool, _Alloc>::_M_copy_aligned(const_iterator, const_iterator, iterator) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:1303:28,
    inlined from 'constexpr void std::vector<bool, _Alloc>::_M_reallocate(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/vector.tcc:851:40,
    inlined from 'constexpr void std::vector<bool, _Alloc>::reserve(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:1091:17,
    inlined from 'void S::foo()' at <source>:12:14:
/opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_algobase.h:437:30: warning: 'void* __builtin_memmove(void*, const void*, long unsigned int)' writing between 9 and 9223372036854775807 bytes into a region of size 8 overflows the destination [-Wstringop-overflow=]
  437 |             __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
      |             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/x86_64-linux-gnu/bits/c++allocator.h:33,
                 from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/allocator.h:46,
                 from /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/vector:63:
In member function '_Tp* std::__new_allocator<_Tp>::allocate(size_type, const void*) [with _Tp = long unsigned int]',
    inlined from 'constexpr _Tp* std::allocator< <template-parameter-1-1> >::allocate(std::size_t) [with _Tp = long unsigned int]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/allocator.h:198:40,
    inlined from 'static constexpr _Tp* std::allocator_traits<std::allocator<_Up> >::allocate(allocator_type&, size_type) [with _Tp = long unsigned int]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/alloc_traits.h:482:28,
    inlined from 'constexpr std::_Bvector_base<_Alloc>::_Bit_pointer std::_Bvector_base<_Alloc>::_M_allocate(std::size_t) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:643:48,
    inlined from 'constexpr void std::vector<bool, _Alloc>::_M_reallocate(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/vector.tcc:849:43,
    inlined from 'constexpr void std::vector<bool, _Alloc>::reserve(size_type) [with _Alloc = std::allocator<bool>]' at /opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/stl_bvector.h:1091:17,
    inlined from 'void S::foo()' at <source>:12:14:
/opt/compiler-explorer/gcc-13.1.0/include/c++/13.1.0/bits/new_allocator.h:147:55: note: destination object of size 8 allocated by 'operator new'
  147 |         return static_cast<_Tp*>(_GLIBCXX_OPERATOR_NEW(__n * sizeof(_Tp)));
      |                        

=================================

This issue now is realted with "std::vector<bool>", but the issue "Bug 98465 - Bogus -Wstringop-overread with -std=gnu++20 -O2 and std::string::insert" is very similar, although it is related with "std::string::insert".

Regards,
Rogerio


---


### compiler : `gcc`
### title : `maskload and maskstore for integer modes are oddly conditional on AVX2`
### open_at : `2023-08-23T12:29:11Z`
### last_modified_date : `2023-08-29T12:12:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111119
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
We have

(define_expand "maskload<mode><sseintvecmodelower>"
  [(set (match_operand:V48_AVX2 0 "register_operand")
    (unspec:V48_AVX2
      [(match_operand:<sseintvecmode> 2 "register_operand")
       (match_operand:V48_AVX2 1 "memory_operand")]
      UNSPEC_MASKMOV))]
  "TARGET_AVX")

and

(define_mode_iterator V48_AVX2
  [V4SF V2DF
   V8SF V4DF
   (V4SI "TARGET_AVX2") (V2DI "TARGET_AVX2")
   (V8SI "TARGET_AVX2") (V4DI "TARGET_AVX2")])

so for example maskloadv4siv4si is disabled with just -mavx while the actual
instruction can operate just fine on SImode sized data by pretending its
SFmode.

check_effective_target_vect_masked_load is conditional on AVX, not AVX2.

With just AVX we can still use SSE2 vectorization for integer operations using
masked loads/stores from AVX.


---


### compiler : `gcc`
### title : `Not always using czero.eqz`
### open_at : `2023-08-24T05:30:17Z`
### last_modified_date : `2023-08-24T15:16:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111126
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
int f(bool a, int c)
{
  return a * c;
}

int f0(bool a, int c)
{
  return (-a) & c;
}

int f1(bool a, int c)
{
  return a ? c : 0;
}

int f3(int a, int b, int c)
{
  return (a == b) * c;
}
int g0(bool a, int c)
{
  return a ? 0 : c;
}
int g1(bool a, int c)
{
  a = !a;
  return a * c;
}
```

Currently for f, f0 we emit:
```
        neg     a0,a0
        and     a0,a0,a1
```

Which is good for riscv without zicond but with we should just get the same as f1.

f1 we do we:
```
        czero.eqz       a0,a1,a0
```

though without zicond we get:
```
        snez    a5,a0
        neg     a5,a5
        and     a0,a1,a5
```
Notice the extra snez (note the above is because that is ifcvt.cc doing the emitting).

This is all about what is the canonical form of bool ? a : 0.


---


### compiler : `gcc`
### title : `SLP of scatters not implemented`
### open_at : `2023-08-24T13:10:30Z`
### last_modified_date : `2023-08-24T13:10:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111133
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
There is no scatter support for (single-lane) SLP.  For example

#define N 16

void __attribute__((noipa))
f (float *restrict y, float *restrict x, int *restrict indices)
{
  for (int i = 0; i < N; ++i)
    {
      x[indices[i * 2]] = y[i * 2] + 1;
      x[indices[i * 2 + 1]] = y[i * 2 + 1] + 2;
    }
}

doesn't vectorize.  Note this is also due to

t.c:6:21: note:   === vect_analyze_data_ref_dependences ===
t.c:8:25: missed:   possible alias involving gather/scatter between *_10 and *_20
t.c:6:21: missed:  bad data dependence.

but with SLP scatters guarantee left-to-right commit so that shouldn't
be an issue there and also shows why SLP is even required for this
to vectorize.


---


### compiler : `gcc`
### title : `RISC-V: improve scalar constants cost model`
### open_at : `2023-08-24T17:44:05Z`
### last_modified_date : `2023-08-25T20:12:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111139
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
The current const cost determination in riscv_rtx_costs () and its children such as riscv_const_insns () needs improvements.

1. There's some likely inaccuracies with various heuristics.
2. Those heuristics are distributed in a bunch of places and better be consolidated.
3. We need to make const cost cpu/uarch tunable as hardware widgets like macro fusions could ammortize multi-insn const costs.


Some of the heuristics to cleanup/revisit:

1a. riscv_rtx_cost() returns 1 insn even if riscv_const_insns () returns > 1.

    case CONST:
      if ((cost = riscv_const_insns (x)) > 0)
	{
	  if (cost == 1 && outer_code == SET)
	    *total = COSTS_N_INSNS (1);
	  else if (outer_code == SET || GET_MODE (x) == VOIDmode)
	    *total = COSTS_N_INSNS (1);
	}

1b. riscv_const_insns () caps the const cost to 4 even if it higher with intent to force const pool. RV backend in general no longer favors const pools for large constants since 2e886eef7f2b5a ("RISC-V: Produce better code with complex constants [PR95632] [PR106602]"). This heuristic needs to be revisited.

1c. riscv_split_integer_cost () adds 2 to initial cost computed.


---


### compiler : `gcc`
### title : `[missed optimization] unlikely code slows down diffutils x86-64 ASCII processing`
### open_at : `2023-08-24T20:05:09Z`
### last_modified_date : `2023-08-26T16:43:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111143
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.1.1`
### severity : `normal`
### contents :
Created attachment 55788
source code illustrating the performance problem

This bug report may be related to bug 110823 (also found for diffutils) but the symptoms differ somewhat so I am reporting it separately. I observed it with GCC 13.1.1 20230614 (Red Hat 13.1.1-4) on x86-64.

While tuning GNU diffutils I noticed that its loops to process mostly-ASCII text were not compiled well by GCC on x86-64. For a stripped-down example of the problem, compile the attached program with:

gcc -O2 -S code-mcel.c

The result is in the attached file code-mcel.s. Its loop kernel assuming ASCII text (starting on line 44) looks like this:

	.L6:
		movsbq	(%rbx), %rax
		testb	%al, %al
		js	.L4
		addq	%rax, %r12
		movl	$1, %eax
	.L5:
		addq	%rax, %rbx
		cmpq	%r13, %rbx
		jb	.L6

The "movl $1, %eax" immediately followed by "addq %rax, %rbx" is poorly scheduled; the resulting dependency makes the code run quite a bit slower than it should. Replacing it with "addq $1, %rbx" and readjusting the surrounding code accordingly, as is done in the attached file code-mcel-opt.s, causes the benchmark to run 38% faster on my laptop's Intel i5-1335U.

It seems that code that GCC knows is unlikely (because of __builtin_expect) is causing the kernel, which GCC knows is likely, to be poorly optimized.


---


### compiler : `gcc`
### title : `bitwise_inverted_equal_p can be used in the `(x | y) & (~x ^ y)` pattern to catch more`
### open_at : `2023-08-25T01:37:13Z`
### last_modified_date : `2023-08-29T08:07:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111147
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int g(int x, int y)
{
  return (x | y) & (~x ^ y); // x & y
}
int g0(int x, int y)
{
  return (~x | y) & (x ^ y); // ~x & y
}
```

g is correctly optimized to `x & y` but g0 is not optimized to `~x & y` even though it could be.

If this pattern used bitwise_inverted_equal_p it could be.

Note this could also be used to catch comparisons too.

Filing this not to lose this idea.


---


### compiler : `gcc`
### title : `Missing boolean optimizations due to comparisons`
### open_at : `2023-08-25T02:06:24Z`
### last_modified_date : `2023-09-17T06:11:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111148
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
```
_Bool  f(int a, int b)
{
  _Bool X = a==1, Y = b == 2;
return (X & !Y) | (!X & Y); // X ^ Y
}

_Bool f0(int X, int Y)
{
  _Bool a = X==1, b = Y == 2;
  return (!a & b) ^ a; // a | b
}

_Bool  f1(int a, int b)
{
  _Bool x = a==1, y = b == 2;
  return (!x & y) | !(x | y); // !x
}

_Bool  f2(int a, int b)
{
  _Bool x = a==1, y = b == 2;
  return (x | y) ^ (x | !y); // !x
}

_Bool f3(int a, int b)
{
  _Bool x = a==1, y = b == 2;
  return (x | !y) & (!x | y); // (x==y) or x ^ y ^1
}

_Bool  f4(int a, int b)
{
  _Bool x = a==1, y = b == 2;
return (!x | y) ^ (x | !y); // x ^ y
}
_Bool f5(int a, int b)
{
  _Bool X = a == b;
  _Bool Y = !X;
  return X == Y; // 0
}
_Bool f6(int a, int b)
{
  _Bool X = a == b;
  _Bool Y = !X;
  return X != Y; // 1
}
```

These all patterns should move over to using bitwise_inverted_equal_p .


---


### compiler : `gcc`
### title : `bool0 != bool1 should be expanded as bool0 ^ bool1`
### open_at : `2023-08-25T02:09:39Z`
### last_modified_date : `2023-08-30T06:11:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111149
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
_Bool k(_Bool a, _Bool b)
{
  return a != b;
}
```
This should be changed into:
```
_Bool k0(_Bool a, _Bool b)
{
  return a ^ b;
}
```

Note clange handles this.

The main problem with this is with respect to having it inside GIMPLE_COND ...


---


### compiler : `gcc`
### title : `(vec CMP vec) != (vec CMP vec) should just produce (vec CMP vec) ^ (vec CMP vec)`
### open_at : `2023-08-25T06:28:43Z`
### last_modified_date : `2023-08-25T06:28:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111150
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
typedef int v4si __attribute((__vector_size__(4*sizeof(int))));
v4si f2_(v4si a, v4si b, v4si c, v4si d)
{
  v4si X = a == b;
  v4si Y = c == d;
  return (X != Y);
}

```
Currently we produce on x86_64 -O2:
```
        pcmpeqd %xmm1, %xmm0
        pcmpeqd %xmm3, %xmm2
        pxor    %xmm1, %xmm1
        pcmpeqd %xmm2, %xmm0
        pcmpeqd %xmm1, %xmm0
```
But we should produce:
```
        pcmpeqd %xmm1, %xmm0
        pcmpeqd %xmm3, %xmm2
        pxor    %xmm2, %xmm0
```

In forwprop1 we have:
```
  _1 = a_4(D) == b_5(D);
  X_6 = VEC_COND_EXPR <_1, { -1, -1, -1, -1 }, { 0, 0, 0, 0 }>;
  _2 = c_7(D) == d_8(D);
  Y_9 = VEC_COND_EXPR <_2, { -1, -1, -1, -1 }, { 0, 0, 0, 0 }>;
  _3 = X_6 != Y_9;
```
But we can push != further up since -1 != 0 is true.

basically in scalar we have:
(a ? -1 : 0) != (b ? -1 : 0)

but we know that is really `(a^b) ? -1 : 0`

Now in theory you could do the full expansion for this but I think we should just handle the case where we have -1 and 0.


---


### compiler : `gcc`
### title : `RISC-V: Incorrect Vector cost model for reduction`
### open_at : `2023-08-25T10:07:41Z`
### last_modified_date : `2023-09-18T08:25:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111153
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c`
### version : `14.0`
### severity : `normal`
### contents :
Consider this following case:

#include <stdint.h>

#define DEF_REDUC_PLUS(TYPE)			\
TYPE __attribute__ ((noinline, noclone))	\
reduc_plus_##TYPE (TYPE * __restrict a, int n)		\
{						\
  TYPE r = 0;					\
  for (int i = 0; i < n; ++i)			\
    r += a[i];					\
  return r;					\
}

#define TEST_PLUS(T)				\
  T (int32_t)					\

TEST_PLUS (DEF_REDUC_PLUS)

 -O3 --param=riscv-autovec-preference=scalable:

reduc_plus_int32_t:
        ble     a1,zero,.L8
        addiw   a5,a1,-1
        li      a4,4
        addi    sp,sp,-16
        mv      a2,a0
        sext.w  a3,a1
        bleu    a5,a4,.L9
        srliw   a4,a3,2
        slli    a4,a4,4
        mv      a5,a0
        add     a4,a4,a0
        vsetivli        zero,4,e32,m1,ta,ma
        vmv.v.i v1,0
        vse32.v v1,0(sp)
.L4:
        vle32.v v1,0(a5)
        vle32.v v2,0(sp)
        addi    a5,a5,16
        vadd.vv v1,v2,v1
        vse32.v v1,0(sp)
        bne     a4,a5,.L4
        ld      a5,0(sp)
        lw      a4,0(sp)
        andi    a1,a1,-4
        srai    a5,a5,32
        addw    a5,a4,a5
        lw      a4,8(sp)
        addw    a5,a5,a4
        ld      a4,8(sp)
        srai    a4,a4,32
        addw    a0,a5,a4
        beq     a3,a1,.L15
.L3:
        subw    a3,a3,a1
        slli    a5,a1,32
        slli    a3,a3,32
        srli    a3,a3,32
        srli    a5,a5,30
        add     a2,a2,a5
        vsetvli a5,a3,e8,mf4,tu,mu
        vsetvli a4,zero,e32,m1,ta,ma
        sub     a1,a3,a5
        vmv.v.i v1,0
        vsetvli zero,a3,e32,m1,tu,ma
        vle32.v v2,0(a2)
        vmv.v.v v1,v2
        bne     a3,a5,.L21
.L7:
        vsetvli a4,zero,e32,m1,ta,ma
        vmv.s.x v2,zero
        vredsum.vs      v1,v1,v2
        vmv.x.s a5,v1
        addw    a0,a0,a5
.L15:
        addi    sp,sp,16
        jr      ra
.L21:
        slli    a5,a5,2
        add     a2,a2,a5
        vsetvli zero,a1,e32,m1,tu,ma
        vle32.v v2,0(a2)
        vadd.vv v1,v1,v2
        j       .L7
.L8:
        li      a0,0
        ret
.L9:
        li      a1,0
        li      a0,0
        j       .L3

-O3 --param=riscv-autovec-preference=scalable -fno-vect-cost-model:
reduc_plus_int32_t:
        ble     a1,zero,.L4
        vsetvli a3,zero,e32,m1,ta,ma
        vmv.v.i v1,0
.L3:
        vsetvli a5,a1,e32,m1,tu,ma
        slli    a4,a5,2
        sub     a1,a1,a5
        vle32.v v2,0(a0)
        add     a0,a0,a4
        vadd.vv v1,v2,v1
        bne     a1,zero,.L3
        vsetvli a3,zero,e32,m1,ta,ma
        vmv.s.x v2,zero
        vredsum.vs      v1,v1,v2
        vmv.x.s a0,v1
        ret
.L4:
        li      a0,0
        ret

The current vector cost model generates inferiors codegen.


---


### compiler : `gcc`
### title : `gcc unnecessarily creates vector operations for packing 32 bit integers into struct (x86_64)`
### open_at : `2023-08-26T18:29:00Z`
### last_modified_date : `2023-08-28T12:53:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111166
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `13.2.1`
### severity : `normal`
### contents :
Created attachment 55799
preprocessed file that triggers the bug, as requested

GCC version: gcc version 13.2.1 20230801 (GCC)

Target: x86_64-pc-linux-gnu

Configured with: /build/gcc/src/gcc/configure --enable-languages=ada,c,c++,d,fortran,go,lto,objc,obj-c++ --enable-bootstrap --prefix=/usr --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --with-build-config=bootstrap-lto --with-linker-hash-style=gnu --with-system-zlib --enable-__cxa_atexit --enable-cet=auto --enable-checking=release --enable-clocale=gnu --enable-default-pie --enable-default-ssp --enable-gnu-indirect-function --enable-gnu-unique-object --enable-libstdcxx-backtrace --enable-link-serialization=1 --enable-linker-build-id --enable-lto --enable-multilib --enable-plugin --enable-shared --enable-threads=posix --disable-libssp --disable-libstdcxx-pch --disable-werror

Command used: gcc -v -save-temps weird_gcc_behaviour.c -o weird_gcc_behaviour.s -S -O3 -mtune=generic -march=x86-64
(same behaviour is observed with -O2)

Command gives no output to stdout nor stderr, and returns with exit code 0

When compiling the function `turn_into_struct`, a simple function that packs 4 32 bit unsigned integers arguments into a simple struct holding 4 such integers and passes that along to `do_smth_with_4_u32`, at -O2 or -O3 the generated assembly contains a couple vector operations (`punpckldq` and `punpcklqdq`), as well as spilling onto the stack. This does not seem like a good idea to me, performance wise

When compiled at -Os it instead uses `salq`, `movl` (to ensure the upper 32 bits are cleared) and `orq` to pack the data together, avoiding memory altogether, which (intuitively to me) seems like a significantly faster implementation as it doesn't need to touch SSE nor memory


---


### compiler : `gcc`
### title : `swapping around duplicated conditionals can produce better code`
### open_at : `2023-08-26T19:39:51Z`
### last_modified_date : `2023-08-28T19:26:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111167
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take reduced from GCC's sources for lookup_attribute:
```
void f(int);
inline void canonicalize_attr_name(const char *&s, int l) {
  if (l > 4 && s[0] == '_' && s[l - 1] == '_' ) 
    s += 2;
}
void lookup_attribute(const char *attr_ns, int list, int t) {
  if (attr_ns && attr_ns[0] != '_') {
    canonicalize_attr_name(attr_ns, 5);
  }
  if (list == 0)
  {
    short attr_ns_len = attr_ns ? t : 0;
    f(attr_ns_len);
  }
}

```

In optimized we have:
```
  if (attr_ns_25(D) != 0B)
    goto <bb 4>; [70.00%]
  else
    goto <bb 3>; [30.00%]

  <bb 3> [local count: 322122543]:
  if (list_10(D) == 0)
    goto <bb 6>; [50.00%]
  else
    goto <bb 7>; [50.00%]

  <bb 4> [local count: 751619279]:
  if (list_10(D) == 0)
    goto <bb 5>; [50.00%]
  else
    goto <bb 7>; [50.00%]

```

But we should really just have:
if (list_10(D) == 0) goto bb7;
if (attr_ns_25(D) != 0B) goto bb5; else bb6;


---


### compiler : `gcc`
### title : `a[0,1]|b[0,-1] == 0 is not reduced to a ==0 & b ==0 if a and b are defined by comparisons`
### open_at : `2023-08-27T05:50:22Z`
### last_modified_date : `2023-08-27T05:50:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111168
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
static inline int comLG_(int a, int b)
{
        int c = (a < b) * -1;
        return (a > b) | c;
}


int f(int a, int b)
{
        return comLG_(a,b) == 0;
}
```

This should be reduced down to just `(a < b) == 0 & (a > b) == 0` or rather just `a == b`.


---


### compiler : `gcc`
### title : `variant of cond-bool-2.c fails`
### open_at : `2023-08-28T19:25:47Z`
### last_modified_date : `2023-09-17T08:57:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111217
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
static inline _Bool nand(_Bool a, _Bool b)
{
  _Bool t = 0;
  if (a) { if (b) t = 1; }
  return !t;
  //  return !(a && b);
}

_Bool f(int a, int b)
{
    return nand(nand(b, nand(a, a)), nand(a, nand(b, b)));
}
```

we get at ifcombine:
  <bb 2> [local count: 1073741824]:
  if (a_3(D) != 0)
    goto <bb 3>; [50.00%]
  else
    goto <bb 4>; [50.00%]

  <bb 3> [local count: 536870912]:
  if (b_2(D) != 0)
    goto <bb 6>; [50.00%]
  else
    goto <bb 5>; [50.00%]

  <bb 4> [local count: 536870912]:
  if (b_2(D) != 0)
    goto <bb 6>; [50.00%]
  else
    goto <bb 5>; [50.00%]

  <bb 5> [local count: 536870912]:
  ...

  <bb 6> [local count: 536870912]:
  # iftmp.0_21 = PHI <1(3), 0(4)>

So we could swap these ifs around slighlty

 if (b_2(D) != 0) goto L1; else goto L2;
L1:
 if (a_3(D) != 0) goto L3; else goto L4;
L3: goto L4;
L4:
 iftmp.0_21 = PHI <1(3), 0(4)>
L1: goto bb5;

And then it will be optimized.


---


### compiler : `gcc`
### title : `RISC-V: Failed to combine vwmul + vadd into vwmacc`
### open_at : `2023-08-30T07:00:59Z`
### last_modified_date : `2023-09-06T12:49:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111232
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdlib.h>
#define N 64

#define DOT -21856

signed char X[N] __attribute__ ((__aligned__(__BIGGEST_ALIGNMENT__))) = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63};
signed char Y[N] __attribute__ ((__aligned__(__BIGGEST_ALIGNMENT__))) = {64,63,62,61,60,59,58,57,56,55,54,53,52,51,50,49,48,47,46,45,44,43,42,41,40,39,38,37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1};

/* char->short->short dot product.
   The dot-product pattern should be detected.
   Should be vectorized on vect_sdot_qi targets (targets that support 
   dot-product of signed char).  
   This test currently fails to vectorize on targets that support
   dot-product of chars into and int accumulator.
   Can also be vectorized as widening-mult + summation,
   or with type-conversion support.
 */
__attribute__ ((noinline)) short
foo(int len) {
  int i;
  short result = 0;

  for (i=0; i<len; i++) {
    result += (X[i] * Y[i]);
  }
  return result;
}

ASM:

	vsetvli	a5,a0,e8,mf2,ta,ma
	vle8.v	v4,0(a4)
	vle8.v	v3,0(a3)
	sub	a0,a0,a5
	vsetvli	a2,zero,e8,mf2,ta,ma
	add	a4,a4,a5
	vwmul.vv	v2,v4,v3
	add	a3,a3,a5
	vsetvli	zero,a5,e16,m1,tu,ma
	vadd.vv	v1,v2,v1
	bne	a0,zero,.L8
	vsetvli	a2,zero,e16,m1,ta,ma
	vmv.s.x	v2,zero
	vredsum.vs	v1,v1,v2
	vmv.x.s	a0,v1
	ret

It should be vwmacc instead of vwmul + vadd.


---


### compiler : `gcc`
### title : `LoongArch: Suboptimal code for (a & ~mask) | (b & mask) where mask is a constant with value ((1 << n) - 1) << m`
### open_at : `2023-08-31T04:31:18Z`
### last_modified_date : `2023-09-07T08:01:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111252
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
int test(int a, int b)
{
  return (a & ~0x10) | (b & 0x10);
}

compiles to:

	addi.w	$r12,$r0,-17			# 0xffffffffffffffef
	and	$r12,$r12,$r4
	andi	$r5,$r5,16
	or	$r12,$r12,$r5
	slli.w	$r4,$r12,0
	jr	$r1

It should be improved:

bstrpick.w $r4, $r4, 4, 4
bstrins.w  $r5, $r4, 4, 4
or         $r5, $r4, $r0


---


### compiler : `gcc`
### title : `RISC-V: Miss combine two vsetvl insns`
### open_at : `2023-08-31T07:05:36Z`
### last_modified_date : `2023-09-18T12:17:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111255
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Missed combine two bellow vsetvl insns:

        vsetvli a5,a4,e8,m1,tu,mu      => this two vsetvl insns 
        vsetvli zero,a5,e16,m2,ta,ma      should be combined.


C Code:

#include <stdint.h>

#define DEF_LOOP(OLD_TYPE, NEW_TYPE)                                           \
  void __attribute__ ((noipa))                                                 \
  test_##OLD_TYPE##_2_##NEW_TYPE (NEW_TYPE *__restrict r,                      \
				  OLD_TYPE *__restrict a, NEW_TYPE b,          \
				  OLD_TYPE *__restrict pred, int n)            \
  {                                                                            \
    for (int i = 0; i < n; ++i)                                                \
      {                                                                        \
	r[i] = pred[i] ? (NEW_TYPE) a[i] : b;                                  \
      }                                                                        \
  }

/* INT -> narrower-INT */
#define TEST_ALL_X2X_NARROWER(T)                                               \
  T (int16_t, int8_t)

TEST_ALL_X2X_NARROWER (DEF_LOOP)

Assembly:

test_int16_t_2_int8_t:
        ble     a4,zero,.L5
        vsetvli t1,zero,e8,m1,ta,ma
        vmv.v.x v4,a2
.L3:
        vsetvli a5,a4,e8,m1,tu,mu      => this two vsetvl insns 
        vsetvli zero,a5,e16,m2,ta,ma      should be combined.
        vle16.v v0,0(a3)
        vsetvli t1,zero,e16,m2,ta,ma
        vmsne.vi        v0,v0,0
        vsetvli zero,a5,e16,m2,ta,ma
        vle16.v v2,0(a1),v0.t
        vsetvli a6,zero,e8,m1,ta,ma
        slli    a7,a5,1
        vncvt.x.x.w     v2,v2
        sub     a4,a4,a5
        vmerge.vvm      v2,v4,v2,v0
        vsetvli zero,a5,e8,m1,ta,ma
        vse8.v  v2,0(a0)
        add     a3,a3,a7
        add     a0,a0,a5
        add     a1,a1,a7
        bne     a4,zero,.L3
.L5:
        ret

compiler explorer: https://godbolt.org/z/KPP8G1E3W


---


### compiler : `gcc`
### title : `new signed overflow after vectorizer`
### open_at : `2023-08-31T11:48:29Z`
### last_modified_date : `2023-08-31T13:22:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111257
### status : `ASSIGNED`
### tags : `missed-optimization, wrong-code`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
The vectorizer is not removing the original scalar calculations, and they may overflow after vectorization.

This can be seen with

  int a[8];

  void foo(void)
  {
    for (int i = 0; i < 8; i++)
      a[i] = a[i] + 5;
  }

The IR for the loop before vectorization looks like

  <bb 3> [local count: 954449104]:
  # i_10 = PHI <i_7(5), 0(2)>
  # ivtmp_4 = PHI <ivtmp_3(5), 8(2)>
  _1 = a[i_10];
  _2 = _1 + 5;
  a[i_10] = _2;
  i_7 = i_10 + 1;
  ivtmp_3 = ivtmp_4 - 1;
  if (ivtmp_3 != 0)
    goto <bb 5>; [87.50%]
  else
    goto <bb 4>; [12.50%]

  <bb 5> [local count: 835156385]:
  goto <bb 3>; [100.00%]

and it is vectorized to

  <bb 3> [local count: 238585440]:
  # i_10 = PHI <i_7(5), 0(2)>
  # ivtmp_4 = PHI <ivtmp_3(5), 8(2)>
  # vectp_a.4_9 = PHI <vectp_a.4_8(5), &a(2)>
  # vectp_a.8_16 = PHI <vectp_a.8_17(5), &a(2)>
  # ivtmp_19 = PHI <ivtmp_20(5), 0(2)>
  vect__1.6_13 = MEM <vector(4) int> [(int *)vectp_a.4_9];
  _1 = a[i_10];
  vect__2.7_15 = vect__1.6_13 + { 5, 5, 5, 5 };
  _2 = _1 + 5;
  MEM <vector(4) int> [(int *)vectp_a.8_16] = vect__2.7_15;
  i_7 = i_10 + 1;
  ivtmp_3 = ivtmp_4 - 1;
  vectp_a.4_8 = vectp_a.4_9 + 16;
  vectp_a.8_17 = vectp_a.8_16 + 16;
  ivtmp_20 = ivtmp_19 + 1;
  if (ivtmp_20 < 2)
    goto <bb 5>; [50.00%]
  else
    goto <bb 4>; [50.00%]

  <bb 5> [local count: 119292723]:
  goto <bb 3>; [100.00%]

This vectorized loop still read _1 from a[i_10] and adds 5 to it, so the second loop iteration will add 5 to the value of a[1]. But the first iteration has already added 5 to a[1], so we are now doing a different calculation compared to the original loop, and this can overflow even if the original did not.


---


### compiler : `gcc`
### title : `[14 Regression] Codegen regression from i386 argument passing changes`
### open_at : `2023-09-01T12:55:45Z`
### last_modified_date : `2023-10-20T22:49:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111267
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Code generation for the following testcase:

typedef struct {
  float minx, miny;
  float maxx, maxy;
} AABB;

int TestOverlap(AABB a, AABB b) {
  return a.minx <= b.maxx
      && a.miny <= b.maxy
      && a.maxx >= b.minx
      && a.maxx >= b.minx;
}

int TestOverlap2(AABB a, AABB b) {
  return a.miny <= b.maxy
      && a.maxx >= b.minx;
}

has regressed due to commit https://gcc.gnu.org/git/?p=gcc.git;a=commit;h=bdf2737cda53a83332db1a1a021653447b05a7e7

GCC13.2:

TestOverlap:
        comiss  xmm3, xmm0
        movq    rdx, xmm0
        movq    rsi, xmm1
        movq    rax, xmm3
        jb      .L10
        shr     rdx, 32
        shr     rax, 32
        movd    xmm0, eax
        movd    xmm4, edx
        comiss  xmm0, xmm4
        jb      .L10
        movd    xmm1, esi
        xor     eax, eax
        comiss  xmm1, xmm2
        setnb   al
        ret
.L10:
        xor     eax, eax
        ret
TestOverlap2:
        shufps  xmm0, xmm0, 85
        shufps  xmm3, xmm3, 85
        comiss  xmm3, xmm0
        jb      .L17
        xor     eax, eax
        comiss  xmm1, xmm2
        setnb   al
        ret
.L17:
        xor     eax, eax
        ret

GCC trunk:

TestOverlap:
        movq    rax, xmm1
        movq    rdx, xmm2
        movq    rsi, xmm0
        mov     rdi, rax
        movq    rax, xmm3
        mov     rcx, rsi
        xchg    rdx, rax
        movd    xmm1, edx
        mov     rsi, rax
        mov     rax, rdx
        comiss  xmm1, xmm0
        jb      .L10
        shr     rcx, 32
        shr     rax, 32
        movd    xmm0, eax
        movd    xmm4, ecx
        comiss  xmm0, xmm4
        jb      .L10
        movd    xmm0, esi
        movd    xmm1, edi
        xor     eax, eax
        comiss  xmm1, xmm0
        setnb   al
        ret
.L10:
        xor     eax, eax
        ret
TestOverlap2:
        movq    rdx, xmm2
        movq    rax, xmm3
        movq    rsi, xmm0
        xchg    rdx, rax
        mov     rcx, rsi
        mov     rsi, rax
        mov     rax, rdx
        shr     rcx, 32
        shr     rax, 32
        movd    xmm4, ecx
        movd    xmm0, eax
        comiss  xmm0, xmm4
        jb      .L17
        movd    xmm0, esi
        xor     eax, eax
        comiss  xmm1, xmm0
        setnb   al
        ret
.L17:
        xor     eax, eax
        ret

(Can also be seen here https://godbolt.org/z/E4xrEn6KW)


---


### compiler : `gcc`
### title : `ifcombine and reassociation does not like to produce `a ^ b` sometimes`
### open_at : `2023-09-02T21:17:43Z`
### last_modified_date : `2023-09-04T08:08:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111275
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f0(_Bool u, _Bool v)
{
  _Bool t1, t;
  t = !u & v;
  t1 = !v & u;
  if (t) goto L40;
  if (t1) goto L40;
  return 0;
L40:
  return 1;
}
int f1(_Bool u, _Bool v)
{
  _Bool t1, t;
  t = !u & v;
  if (t) goto L40;
  t1 = !v & u;
  if (t1) goto L40;
  return 0;
L40:
  return 1;
}
```

These 2 should just produce `return u ^ v;` (or `return u != v;`) but currently only f0 does.


---


### compiler : `gcc`
### title : ``a & (b ^ ~a)` (or `a & ~(a ^ b)`) not optimized to `a & b` in gimple`
### open_at : `2023-09-04T06:03:51Z`
### last_modified_date : `2023-10-11T16:47:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111282
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(int a, int b)
{
        return a & (b ^ ~a);
}
``
This should be optimized to `a & b` which it is on the RTL.


---


### compiler : `gcc`
### title : `[14 Regression] Missed Dead Code Elimination since r14-2228-g02460c0b8c9`
### open_at : `2023-09-05T14:56:54Z`
### last_modified_date : `2023-09-05T21:06:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111292
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/sxj43ojn3

Given the following code:

void foo(void);
static struct {
    int a;
    char b;
} c;
static int d, f, g;
static int *e = &d, *h, *i;
static int **j = &h;
int main() {
    {
        short k = d;
        int l;
        for (; c.a; c.a--) {
            int **m = 0;
            int *n = &l;
            if (*e)
                g = *i != 0;
            else
                __builtin_unreachable();
            f = 8;
            for (; f; f--) {
                *n = c.b || k;
                if ((k && *i) + g) {
                    int ***o = &m;
                    *o = j;
                } else
                    i = &g;
            }
            if (m == 0 || m == &h)
                ;
            else
                foo();
            ;
        }
    }
}

gcc-trunk -O3 does not eliminate the call to foo:

main:
	movl	c(%rip), %eax
	pushq	%rbx
	movzwl	d(%rip), %ebx
	testl	%eax, %eax
	je	.L37
	.p2align 4,,10
	.p2align 3
.L2:
	movq	i(%rip), %rcx
	xorl	%esi, %esi
	movl	(%rcx), %edx
	testl	%edx, %edx
	setne	%sil
	cmpb	$0, c+4(%rip)
	movl	%esi, g(%rip)
	jne	.L3
	testw	%bx, %bx
	jne	.L4
	testl	%edx, %edx
	jne	.L5
	movl	$0, f(%rip)
	movq	$g, i(%rip)
.L6:
	subl	$1, %eax
	movl	%eax, c(%rip)
	jne	.L2
.L37:
	xorl	%eax, %eax
	popq	%rbx
	ret
	.p2align 4,,10
	.p2align 3
.L3:
	testw	%bx, %bx
	je	.L45
	movl	(%rcx), %esi
	movl	%edx, %edi
	orl	%esi, %edi
	je	.L28
	xorl	%edi, %edi
	testl	%esi, %esi
	je	.L46
	.p2align 4,,10
	.p2align 3
.L23:
	movl	$0, f(%rip)
	movl	$h, %esi
	testb	%dil, %dil
	je	.L6
.L24:
	movq	%rcx, i(%rip)
.L11:
	testq	%rsi, %rsi
	je	.L6
	cmpq	$h, %rsi
	je	.L6
	call	foo
	movl	c(%rip), %eax
	subl	$1, %eax
	movl	%eax, c(%rip)
	jne	.L2
	jmp	.L37
	.p2align 4,,10
	.p2align 3
.L4:
	testl	%edx, %edx
	jne	.L5
	movl	$0, f(%rip)
	movl	(%rcx), %esi
	testl	%esi, %esi
	je	.L43
.L8:
	movl	$h, %esi
	jmp	.L11
	.p2align 4,,10
	.p2align 3
.L45:
	movl	$0, f(%rip)
	testl	%edx, %edx
	jne	.L8
.L43:
	movq	$g, i(%rip)
	jmp	.L6
	.p2align 4,,10
	.p2align 3
.L5:
	movl	$0, f(%rip)
	jmp	.L6
.L28:
	movl	$1, %edi
	movl	$g, %ecx
	xorl	%esi, %esi
.L12:
	movl	(%rcx), %r8d
	movl	%edx, %r9d
	orl	%r8d, %r9d
	jne	.L47
	movl	$1, %edi
	movl	$g, %ecx
.L14:
	movl	(%rcx), %r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	je	.L31
	movl	$h, %esi
	testl	%r8d, %r8d
	jne	.L23
.L15:
	movl	(%rcx), %r8d
	movl	%edx, %r11d
	orl	%r8d, %r11d
	je	.L32
	movl	$h, %esi
	testl	%r8d, %r8d
	jne	.L23
.L16:
	movl	(%rcx), %r8d
	movl	%edx, %r11d
	orl	%r8d, %r11d
	je	.L33
	movl	$h, %esi
	testl	%r8d, %r8d
	jne	.L23
.L18:
	movl	(%rcx), %r8d
	movl	%edx, %r10d
	orl	%r8d, %r10d
	je	.L48
	testl	%r8d, %r8d
	jne	.L23
	movl	$h, %esi
	testl	%edx, %edx
	jne	.L27
.L21:
	movl	$1, %edi
	movl	$g, %ecx
.L27:
	orl	(%rcx), %edx
	jne	.L23
	xorl	%ecx, %ecx
	movl	%ecx, f(%rip)
	movl	$g, %ecx
	jmp	.L24
.L47:
	movl	$h, %esi
	testl	%r8d, %r8d
	jne	.L23
	jmp	.L14
.L48:
	cmpl	$0, g(%rip)
	je	.L21
	xorl	%edx, %edx
	movl	$g, %ecx
	movl	$h, %esi
	movl	%edx, f(%rip)
	jmp	.L24
.L31:
	movl	$1, %edi
	movl	$g, %ecx
	jmp	.L15
.L32:
	movl	$1, %edi
	movl	$g, %ecx
	jmp	.L16
.L33:
	movl	$1, %edi
	movl	$g, %ecx
	jmp	.L18
.L46:
	movl	$h, %esi
	jmp	.L12

gcc-13.2.0 -O3 eliminates the call to foo:

main:
	movl	c(%rip), %eax
	movzwl	d(%rip), %edi
	testl	%eax, %eax
	je	.L13
	.p2align 4,,10
	.p2align 3
.L2:
	movq	i(%rip), %rdx
	xorl	%ecx, %ecx
	movl	(%rdx), %esi
	testl	%esi, %esi
	setne	%cl
	movl	%ecx, g(%rip)
	jne	.L7
	testw	%di, %di
	je	.L14
	movl	(%rdx), %edx
	testl	%edx, %edx
	je	.L14
.L7:
	movl	$0, f(%rip)
.L6:
	subl	$1, %eax
	movl	%eax, c(%rip)
	jne	.L2
.L13:
	xorl	%eax, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L14:
	movl	$0, f(%rip)
	movq	$g, i(%rip)
	jmp	.L6

Bisects to r14-2228-g02460c0b8c9


---


### compiler : `gcc`
### title : `[14 Regression] Missed Dead Code Elimination since r14-3414-g0cfc9c953d0`
### open_at : `2023-09-05T15:01:10Z`
### last_modified_date : `2023-09-05T17:40:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111293
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/9aj7sr5d6

Given the following code:

void foo(void);
static int a, d;
static short e;
static int(f)(int b, int c) { return c == 0 ? b : b / c; }
static char g(char h) {
    char i = 1;
    int j = 8;
    e = 3;
    for (; d; ++d) {
        if (f(0 == j, e)) return a;
        for (; i; i--) {
            if (j) {
                if (e)
                    ;
                else if (a)
                    continue;
                else
                    foo();
                j = -h;
                break;
            }
            __builtin_unreachable();
            break;
        }
    }
    return 1;
}
int main() {
    g(-1);
    a = e;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	movl	d(%rip), %ecx
	movl	$3, %edx
	movw	%dx, e(%rip)
	testl	%ecx, %ecx
	je	.L6
	movl	$3, %edx
	.p2align 4,,10
	.p2align 3
.L2:
	testw	%dx, %dx
	jne	.L3
	movl	a(%rip), %eax
	testl	%eax, %eax
	je	.L18
.L5:
	addl	$1, d(%rip)
	jne	.L5
.L6:
	movswl	e(%rip), %eax
	movl	%eax, a(%rip)
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
	.p2align 4,,10
	.p2align 3
.L18:
	call	foo
.L3:
	addl	$1, d(%rip)
	movzwl	e(%rip), %edx
	jne	.L2
	jmp	.L6

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	movl	d(%rip), %edx
	movl	$3, %eax
	movw	%ax, e(%rip)
	testl	%edx, %edx
	je	.L3
	movl	$0, d(%rip)
.L3:
	movl	$3, a(%rip)
	xorl	%eax, %eax
	ret

Bisects to r14-3414-g0cfc9c953d0


---


### compiler : `gcc`
### title : `[14 Regression] Missed Dead Code Elimination since  r14-573-g69f1a8af45d`
### open_at : `2023-09-05T15:05:36Z`
### last_modified_date : `2023-09-25T17:27:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111294
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/7GTzj8Ea1

Given the following code:

void foo(void);
static short a;
static int b, c, d;
static int *e, *f = &d;
static int **g = &e;
static unsigned char h;
static short(i)(short j, int k) { return j > k ?: j; }
static char l() {
    if (a) return b;
    return c;
}
int main() {
    b = 0;
    for (; b < 5; ++b)
        ;
    h = l();
    if (a ^ 3 >= i(h, 11))
        a = 0;
    else {
        *g = f;
        if (e == &d & b) {
            __builtin_unreachable();
        } else
            foo();
        ;
    }
}

gcc-trunk -Os does not eliminate the call to foo:

main:
	movswl	a(%rip), %eax
	xorl	%edx, %edx
	movl	$5, b(%rip)
	testw	%ax, %ax
	je	.L2
	movl	$5, %edx
.L2:
	cmpl	$3, %edx
	setle	%dl
	movzbl	%dl, %edx
	cmpl	%edx, %eax
	je	.L3
	movw	$0, a(%rip)
	xorl	%eax, %eax
	ret
.L3:
	pushq	%rax
	movq	$d, e(%rip)
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	ret

gcc-13.2.0 -Os eliminates the call to foo:

main:
	movl	$5, b(%rip)
	xorl	%eax, %eax
	movw	$0, a(%rip)
	ret

Bisects to r14-573-g69f1a8af45d


---


### compiler : `gcc`
### title : `RFE: builtin to construct va_list`
### open_at : `2023-09-06T14:48:06Z`
### last_modified_date : `2023-09-13T13:33:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111307
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c`
### version : `14.0`
### severity : `normal`
### contents :
A common pattern of defining a pair of varargs functions looks like:

  void somefuncv(char *spec, va_list ap)
  {
      /* ... */
  }
  void somefunc(char *spec, ...)
  {
      va_list ap
      va_start(ap, spec);
      sumefuncv(spec, ap);
      va_end(ap);
  }

  somefunc("%d", 1234); /* => quite a bit of wasted moving bits around */

However, the resulting code for calling somefunc() is suboptimal; the arguments are arranged as specified by the psABI, only to then be rearranged into a va_list by somefunc (which can be rather complex since somefunc has no clue about type, number, or even floating-point calling conventions of its arguments.)

It would be very helpful if GCC added a builtin function to create a va_list for a given "...", with a (pseudo-)prototype like "__builtin_va_list __builtin_va_construct(...)".  This could be used to directly put together a va_list at the caller's location, e.g.:

  #define somefunc(spec, ...) somefuncv(spec, __builtin_va_construct(__VA_ARGS__))

  somefunc("%d", 1234); /* => minimal va_list to carry a single int */

Thus the round-trip through the psABI could be avoided, with the caller constructing a minimal (since it knows all the args) va_list.

(This builtin would also make inlining varargs functions much less relevant, since there are no more actual varargs functions in the latter example...)


---


### compiler : `gcc`
### title : `RISC-V: Incorrect COST model for RVV conversions`
### open_at : `2023-09-07T07:09:53Z`
### last_modified_date : `2023-09-18T20:56:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111317
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdint.h>

void foo (int32_t *__restrict a, int64_t * __restrict b, int n)
{
    for (int i = 0; i < n; i++)
      b[i] = (int64_t)a[i];
}

--param=riscv-autovec-preference=scalable -O3 -fopt-info-vec-missed:
Failed to vectorize:

<source>:5:23: missed: couldn't vectorize loop
<source>:6:24: missed: not vectorized: no vectype for stmt: _4 = *_3;

However, try -fno-vect-cost-model.

We must adjust the COST model for RVV corretly.


---


### compiler : `gcc`
### title : `RISC-V: Redundant vsetvl instructions`
### open_at : `2023-09-07T07:13:05Z`
### last_modified_date : `2023-09-12T11:23:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111318
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
The next two vsetvl instructions in assembly should be removed. Because the vadd instruction is calculated in vlmax length, but where it is used only the first avl, we can change the vl of the vadd to the avl of the instruction being used.

C Code:
#include <riscv_vector.h>

void foo1 (int32_t* restrict a, int32_t* restrict b, int32_t *restrict c, int n)
{
    for (int i = 0; i < n; i += 1)
      c[i] = a[i] + b[i];
}

Assembly Code:
foo1:
        ble     a3,zero,.L5
.L3:
        vsetvli a5,a3,e32,m2,ta,ma
        vle32.v v4,0(a0)
        vle32.v v2,0(a1)
        vsetvli a6,zero,e32,m2,ta,ma
        slli    a4,a5,2
        vadd.vv v2,v2,v4
        sub     a3,a3,a5
        vsetvli zero,a5,e32,m2,ta,ma
        vse32.v v2,0(a2)
        add     a0,a0,a4
        add     a1,a1,a4
        add     a2,a2,a4
        bne     a3,zero,.L3
.L5:
        ret

https://godbolt.org/z/jzWjK6EM9


---


### compiler : `gcc`
### title : `RISC-V: Failed combine extend + reduce sum to widen reduce sum`
### open_at : `2023-09-07T07:43:45Z`
### last_modified_date : `2023-09-14T15:43:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111320
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
vfwcvt.f.f.v and vfredosum.vs should be combined to vfwredosum.vs


C Code:
#include <riscv_vector.h>

double foo1 (float* restrict a, float* restrict b, int n)
{
    double sum = 0;
    for (int i = 0; i < n; i += 1)
      sum += a[i];
    return sum;
}

Assembly Code:
foo1:
        fmv.d.x fa0,zero
        ble     a2,zero,.L4
.L3:
        vsetvli a5,a2,e64,m1,ta,ma
        vle32.v v3,0(a0)
        vfmv.s.f        v2,fa0
        slli    a3,a5,2
        vsetvli a4,zero,e32,mf2,ta,ma
        sub     a2,a2,a5
        vfwcvt.f.f.v    v1,v3
        add     a0,a0,a3
        vsetvli zero,a5,e64,m1,ta,ma
        vfredosum.vs    v1,v1,v2
        vfmv.f.s        fa0,v1
        bne     a2,zero,.L3
        ret
.L4:
        ret


https://godbolt.org/z/dPesPnMWG


---


### compiler : `gcc`
### title : `More optimization about "(X * Y) / Y"`
### open_at : `2023-09-07T08:53:56Z`
### last_modified_date : `2023-09-18T02:36:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111324
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
For case:
------ t.c
typedef unsigned int INT;

INT
foo (INT x, INT y)
{
  if (x > 100 || y > 100)
    return 0;
  return (x * y) / y;
}
---------
gcc -O2 t.c -S -fdump-tree-optimized

  <bb 4> [local count: 467721933]:
  _1 = x_3(D) * y_4(D);
  _5 = _1 / y_4(D);

  <bb 5> [local count: 1073741824]:
  # _2 = PHI <0(2), _5(4), 0(3)>
  return _2;

While for the below case, it can be optimized.

------
typedef unsigned int INT;

INT
foo (INT x, INT y)
{
  if (x > 100 || y > 100)
    return 0;
  INT x1 = x + 1;
  INT y1 = y + 1;
  return (x1 * y1) / y1;
}
-------

The "(x1 * y1) / y1" is optimized to "x1". 

  <bb 4> [local count: 467721933]:
  x1_4 = x_2(D) + 1;

  <bb 5> [local count: 1073741824]:
  # _1 = PHI <0(2), x1_4(4), 0(3)>
  return _1;


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-376-g47a76439911`
### open_at : `2023-09-07T12:47:38Z`
### last_modified_date : `2023-10-25T23:08:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111326
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/fv3szWMTE

Given the following code:

void foo(void);
static int a = 253, d, e, f, h, i, k;
static char j, m;
static short l, n;
static char(o)(char b, char c) { return b + c; }
void(p)(int);
short(q)(short);
static short(r)(int g) {
    if (!(((g) >= 253) && ((g) <= 253))) {
        __builtin_unreachable();
    }
    return 0;
}
static void s(char ab, short) {
    int ac;
    ab = 27;
    for (; ab > 13; ab = ab - 9) {
        j |= 10 | ac;
        e = 0;
        if (o(ab ^ a, d)) {
            --l;
            f = 0;
            if (a)
                ;
            else {
                p(i);
                k ^= q(0);
            }
            i = 5;
        } else {
            ac = 0;
            for (; ac <= 38; ac = ac + 5) foo();
            k = h &= 0 >= 0;
            n = 0;
        }
    }
}
static unsigned t() {
    s(m, a);
    if (l)
        ;
    else
        r(3);
    return f;
}
int main() {
    d = a;
    t();
    r(a);
    printf("", e, n);
}

gcc-trunk -O3 does not eliminate the call to foo:

main:
	pushq	%rbp
	movl	$253, %edx
	movl	$27, %ebp
	pushq	%rbx
	subq	$8, %rsp
	movl	$253, d(%rip)
.L5:
	movl	%ebp, %eax
	orb	$42, j(%rip)
	movl	$0, e(%rip)
	xorl	$-3, %eax
	addb	%dl, %al
	je	.L6
	subw	$1, l(%rip)
	movl	$0, f(%rip)
	movl	$5, i(%rip)
.L3:
	cmpb	$18, %bpl
	je	.L4
	movl	d(%rip), %edx
	movl	$18, %ebp
	jmp	.L5
	.p2align 4,,10
	.p2align 3
.L4:
	movswl	n(%rip), %edx
	movl	e(%rip), %esi
	movl	$.LC0, %edi
	xorl	%eax, %eax
	call	printf
	addq	$8, %rsp
	xorl	%eax, %eax
	popq	%rbx
	popq	%rbp
	ret
.L6:
	movl	$8, %ebx
.L2:
	call	foo
	call	foo
	subl	$2, %ebx
	jne	.L2
	movw	$0, n(%rip)
	movl	h(%rip), %eax
	andl	$1, %eax
	movl	%eax, h(%rip)
	movl	%eax, k(%rip)
	jmp	.L3

gcc-13.2.0 -O3 eliminates the call to foo:

main:
	subq	$8, %rsp
	movswl	n(%rip), %edx
	xorl	%esi, %esi
	xorl	%eax, %eax
	movl	$.LC0, %edi
	orb	$42, j(%rip)
	subw	$2, l(%rip)
	movl	$253, d(%rip)
	movl	$0, e(%rip)
	movl	$0, f(%rip)
	movl	$5, i(%rip)
	call	printf
	xorl	%eax, %eax
	addq	$8, %rsp
	ret

Bisects to r14-376-g47a76439911


---


### compiler : `gcc`
### title : ``X <= MINMAX` pattern is missing :c on the cmp`
### open_at : `2023-09-08T21:51:40Z`
### last_modified_date : `2023-09-13T09:19:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111346
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
The pattern:
```
/* X <= MAX(X, Y) -> true
   X > MAX(X, Y) -> false 
   X >= MIN(X, Y) -> true
   X < MIN(X, Y) -> false */
(for minmax (min     min     max     max     )
     cmp    (ge      lt      le      gt      )
 (simplify
  (cmp @0 (minmax:c @0 @1))
  { constant_boolean_node (cmp == GE_EXPR || cmp == LE_EXPR, type); } ))
```
Is missing :c on the cmp which causes us to miss:
MAX(X, Y) >= X -> true
etc.


---


### compiler : `gcc`
### title : ``(a CMP b) ? minmax<a, c> : minmax<b, c>` pattern missing :c on CMP`
### open_at : `2023-09-08T21:59:52Z`
### last_modified_date : `2023-09-11T20:42:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111348
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
```
(for minmax (min max)
 (for cmp (lt le gt ge ne)
  (simplify
   (cond (cmp @1 @3) (minmax:c @1 @4) (minmax:c @2 @4))
```

Is missing :c on cmp
because we could have
(@3 < @1) ? min(@1, @4) : min(@2, @4)

Which should match for and provide the same as `@1 > @3 ? min(@1, @4) : min(@2, @4)`


---


### compiler : `gcc`
### title : ``Optimize (a CMP CST1) ? max<a,CST2> : a` pattern's cmp is missing :c`
### open_at : `2023-09-08T22:04:06Z`
### last_modified_date : `2023-09-11T14:25:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111349
### status : `RESOLVED`
### tags : `internal-improvement, missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
```
/* Optimize (a CMP CST1) ? max<a,CST2> : a */
(for cmp    (gt  ge  lt  le)
     minmax (min min max max)
 (simplify
  (cond (cmp @0 @1) (minmax:c@2 @0 @3) @4)
```
Just like PR 111348 really.


---


### compiler : `gcc`
### title : ``MAX_EXPR<a, b> <= a` is not optimized to `a >= b``
### open_at : `2023-09-10T21:11:08Z`
### last_modified_date : `2023-09-13T12:08:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111364
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
I Noticed this while looking into PR 111346:
```
_Bool test2(int a, int b)
{
        int t = a;
        if (t < b) t = b;
        return t <= a;
}
```
Should be optimized to just:
```
        return a >= b;
```

There is a pattern for constants:
/* MIN (X, C1) < C2 -> X < C2 || C1 < C2  */
But we could do the same when we know C1 == C2 and not constant (for !HONOR_NANS & !HONOR_SIGNED_ZEROS).


---


### compiler : `gcc`
### title : `conditional selection gives bad code generation`
### open_at : `2023-09-11T16:38:57Z`
### last_modified_date : `2023-09-11T17:00:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111373
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
The code

#define SWAP(i,j) do { \
  if (v[i] > v[j]) { \
    tmp_v = v[i]; v[i] = v[j]; v[j] = tmp_v;	\
    tmp_p = a[i]; a[i] = a[j]; a[j] = tmp_p;	\
    }						\
  } while(0)

void s3 (long int *p[3])
{
  long int v[3];
  long int *a[3];
  long int tmp_v;
  long int *tmp_p;
  a[0] = p[0];
  v[0] = *p[0];
  a[1] = p[1];
  v[1] = *p[1];
  a[2] = p[2];
  v[2] = *p[2];
  SWAP (0,1);
  SWAP (0,2);
  SWAP (1,2);
  p[0] = a[0];
  p[1] = a[1];
  p[2] = a[2];
}

yields, with reasonably recent trunk with -O3, code where there are
register moves right before the results are stored, for example on x86_64:

s3:
.LFB0:
        .cfi_startproc
        movq    (%rdi), %rax
        movq    8(%rdi), %rcx
        movq    16(%rdi), %rdx
        movq    (%rax), %r8
        movq    (%rcx), %rsi
        movq    (%rdx), %r9
        cmpq    %rsi, %r8
        jg      .L2
        cmpq    %r9, %r8
        jle     .L3
        movq    %rax, %r9
        movq    %rdx, %rax
        movq    %r9, %rdx
        movq    %r8, %r9
.L3:
        cmpq    %rsi, %r9
        jl      .L10
.L4:
        movq    %rax, (%rdi)
        movq    %rcx, 8(%rdi)
        movq    %rdx, 16(%rdi)
        ret
        .p2align 4,,10
        .p2align 3
.L2:
        cmpq    %r9, %rsi
        jle     .L11
        movq    %rdx, %rsi
        movq    %rax, %rdx
        movq    %rcx, 8(%rdi)
        movq    %rsi, %rax
        movq    %rdx, 16(%rdi)
        movq    %rax, (%rdi)
        ret
        .p2align 4,,10
        .p2align 3
.L11:
        movq    %r8, %rsi
        movq    %rax, %r8
        movq    %rcx, %rax
        movq    %r8, %rcx
        cmpq    %rsi, %r9
        jge     .L4
.L10:
        movq    %rcx, %rsi
        movq    %rdx, %rcx
        movq    %rax, (%rdi)
        movq    %rsi, %rdx
        movq    %rcx, 8(%rdi)
        movq    %rdx, 16(%rdi)
        ret

This seems to be a general phenomenon, see https://godbolt.org/z/xW9x75qbf for
RISC-V (POWER is similar).


---


### compiler : `gcc`
### title : `Missed optimization for comparing with exact_log2 constants`
### open_at : `2023-09-11T19:06:31Z`
### last_modified_date : `2023-09-12T15:10:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111378
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
The simple example below produces suboptimal code on many targets where exact_log2 constant can't be represented as immediate operand.
(confirmed MIPS/PPC64/SPARC/RISC-V)

extern void do_something(char* p);
extern void do_something_other(char* p);

void test(char* p, uint32_t ch)
{
        if (ch < 0x10000)
        {
            do_something(p);
        }
        else /* ch >= 0x10000 */
        {
            do_something_other(p);
        }
}

However, instead of direct comparing with constant we can use shift & compare to zero:
e.g. (ch < 0x10000) can be transformed into ((ch >> 16) == 0) which is usually shorter & faster on many targets.

The condition appears in real world rarely AFAIK - 20-30 occurences per million asm instructions. Fun fact: many of them related to unicode transformations.


---


### compiler : `gcc`
### title : `RISC-V: missed autovec MULH for signed * unsigned`
### open_at : `2023-09-12T03:36:32Z`
### last_modified_date : `2023-09-15T01:44:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111381
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
For singed * signed or unsigned * unsigned, they can be convert to .MULH, but for singed * unsigned, it is failed. If the target support singed * unsigned, I think it can be convert to .MULH and expand to a sumul<mode>3_highpart pattern.

https://godbolt.org/z/exrfYWdW9

C Cdoe:
#include <riscv_vector.h>

void foo6 (uint32_t* restrict a, uint32_t* restrict b, int* restrict pred, int n)
{
    for (int i = 0; i < n; i += 1)
      a[i] = pred[i] ? (uint32_t)(((uint64_t)a[i] * (uint64_t)b[i]) >> 32) : a[i];
}

void foo7 (int* restrict a, uint32_t* restrict b, int* restrict pred, int n)
{
    for (int i = 0; i < n; i += 1)
      a[i] = pred[i] ? (int32_t)(((int64_t)a[i] * (uint64_t)b[i]) >> 32) : a[i];
}

Optimized dump:

;; Function foo6 (foo6, funcdef_no=0, decl_uid=56325, cgraph_uid=1, symbol_order=0)

Removing basic block 6
Removing basic block 7
Removing basic block 8
void foo6 (uint32_t * restrict a, uint32_t * restrict b, int * restrict pred, int n)
{
  vector([4,4]) unsigned int * vectp_a.23;
  vector([4,4]) unsigned int vect_iftmp.22;
  vector([4,4]) unsigned int vect_patt_37.20;
  vector([4,4]) unsigned int vect__9.19;
  vector([4,4]) unsigned int * vectp_b.17;
  vector([4,4]) <signed-boolean:1> mask__38.16;
  vector([4,4]) unsigned int vect_pretmp_41.15;
  vector([4,4]) unsigned int * vectp_a.13;
  vector([4,4]) int vect__4.12;
  vector([4,4]) int * vectp_pred.10;
  unsigned long ivtmp_62;
  unsigned long _83;
  unsigned long ivtmp_84;
  unsigned long ivtmp_85;
  unsigned long _86;

  <bb 2> [local count: 118111600]:
  if (n_19(D) > 0)
    goto <bb 4>; [89.00%]
  else
    goto <bb 3>; [11.00%]

  <bb 3> [local count: 118111600]:
  return;

  <bb 4> [local count: 105119324]:
  _83 = (unsigned long) n_19(D);

  <bb 5> [local count: 955630224]:
  # vectp_pred.10_63 = PHI <vectp_pred.10_64(5), pred_20(D)(4)>
  # vectp_a.13_67 = PHI <vectp_a.13_68(5), a_21(D)(4)>
  # vectp_b.17_73 = PHI <vectp_b.17_74(5), b_23(D)(4)>
  # vectp_a.23_80 = PHI <vectp_a.23_81(5), a_21(D)(4)>
  # ivtmp_84 = PHI <ivtmp_85(5), _83(4)>
  _86 = .SELECT_VL (ivtmp_84, POLY_INT_CST [4, 4]);
  ivtmp_62 = _86 * 4;
  vect__4.12_65 = .MASK_LEN_LOAD (vectp_pred.10_63, 32B, { -1, ... }, _86, 0);
  vect_pretmp_41.15_69 = .MASK_LEN_LOAD (vectp_a.13_67, 32B, { -1, ... }, _86, 0);
  mask__38.16_71 = vect__4.12_65 != { 0, ... };
  vect__9.19_75 = .MASK_LEN_LOAD (vectp_b.17_73, 32B, mask__38.16_71, _86, 0);
  vect_patt_37.20_76 = .MULH (vect_pretmp_41.15_69, vect__9.19_75);
  vect_iftmp.22_78 = .VCOND_MASK (mask__38.16_71, vect_patt_37.20_76, vect_pretmp_41.15_69);
  .MASK_LEN_STORE (vectp_a.23_80, 32B, { -1, ... }, _86, 0, vect_iftmp.22_78);
  vectp_pred.10_64 = vectp_pred.10_63 + ivtmp_62;
  vectp_a.13_68 = vectp_a.13_67 + ivtmp_62;
  vectp_b.17_74 = vectp_b.17_73 + ivtmp_62;
  vectp_a.23_81 = vectp_a.23_80 + ivtmp_62;
  ivtmp_85 = ivtmp_84 - _86;
  if (ivtmp_85 != 0)
    goto <bb 5>; [89.00%]
  else
    goto <bb 3>; [11.00%]

}



;; Function foo7 (foo7, funcdef_no=1, decl_uid=56336, cgraph_uid=2, symbol_order=1)

Removing basic block 6
Removing basic block 7
Removing basic block 8
void foo7 (int * restrict a, uint32_t * restrict b, int * restrict pred, int n)
{
  vector([2,2]) int * vectp_a.49;
  vector([2,2]) int vect_iftmp.48;
  vector([2,2]) int vect_iftmp.47;
  vector([2,2]) long unsigned int vect__12.46;
  vector([2,2]) long unsigned int vect__11.45;
  vector([2,2]) long unsigned int vect__10.44;
  vector([2,2]) unsigned int vect__9.43;
  vector([2,2]) unsigned int * vectp_b.41;
  vector([2,2]) long unsigned int vect__7.40;
  vector([2,2]) <signed-boolean:1> mask__38.39;
  vector([2,2]) int vect_pretmp_41.38;
  vector([2,2]) int * vectp_a.36;
  vector([2,2]) int vect__4.35;
  vector([2,2]) int * vectp_pred.33;
  unsigned long ivtmp_56;
  unsigned long _80;
  unsigned long ivtmp_81;
  unsigned long ivtmp_82;
  unsigned long _83;

  <bb 2> [local count: 118111600]:
  if (n_19(D) > 0)
    goto <bb 4>; [89.00%]
  else
    goto <bb 3>; [11.00%]

  <bb 3> [local count: 118111600]:
  return;

  <bb 4> [local count: 105119324]:
  _80 = (unsigned long) n_19(D);

  <bb 5> [local count: 955630224]:
  # vectp_pred.33_57 = PHI <vectp_pred.33_58(5), pred_20(D)(4)>
  # vectp_a.36_61 = PHI <vectp_a.36_62(5), a_21(D)(4)>
  # vectp_b.41_68 = PHI <vectp_b.41_69(5), b_23(D)(4)>
  # vectp_a.49_77 = PHI <vectp_a.49_78(5), a_21(D)(4)>
  # ivtmp_81 = PHI <ivtmp_82(5), _80(4)>
  _83 = .SELECT_VL (ivtmp_81, POLY_INT_CST [2, 2]);
  ivtmp_56 = _83 * 4;
  vect__4.35_59 = .MASK_LEN_LOAD (vectp_pred.33_57, 32B, { -1, ... }, _83, 0);
  vect_pretmp_41.38_63 = .MASK_LEN_LOAD (vectp_a.36_61, 32B, { -1, ... }, _83, 0);
  mask__38.39_65 = vect__4.35_59 != { 0, ... };
  vect__7.40_66 = (vector([2,2]) long unsigned int) vect_pretmp_41.38_63;
  vect__9.43_70 = .MASK_LEN_LOAD (vectp_b.41_68, 32B, mask__38.39_65, _83, 0);
  vect__10.44_71 = (vector([2,2]) long unsigned int) vect__9.43_70;
  vect__11.45_72 = vect__7.40_66 * vect__10.44_71;
  vect__12.46_73 = vect__11.45_72 >> 32;
  vect_iftmp.47_74 = (vector([2,2]) int) vect__12.46_73;
  vect_iftmp.48_75 = .VCOND_MASK (mask__38.39_65, vect_iftmp.47_74, vect_pretmp_41.38_63);
  .MASK_LEN_STORE (vectp_a.49_77, 32B, { -1, ... }, _83, 0, vect_iftmp.48_75);
  vectp_pred.33_58 = vectp_pred.33_57 + ivtmp_56;
  vectp_a.36_62 = vectp_a.36_61 + ivtmp_56;
  vectp_b.41_69 = vectp_b.41_68 + ivtmp_56;
  vectp_a.49_78 = vectp_a.49_77 + ivtmp_56;
  ivtmp_82 = ivtmp_81 - _83;
  if (ivtmp_82 != 0)
    goto <bb 5>; [89.00%]
  else
    goto <bb 3>; [11.00%]

}


---


### compiler : `gcc`
### title : `missed optimization: GCC adds extra any extend when storing subreg#0 multiple times`
### open_at : `2023-09-12T09:25:21Z`
### last_modified_date : `2023-10-07T20:54:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111384
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Simple example:

void store_hi_twice(uint32_t src, uint16_t *dst1, uint16_t *dst2)
{
    *dst1 = src;
    *dst2 = src;
}

shows that GCC can't opt out unnecessary zero extend of the src's low half aimed to store two or more times. Many targets are affected, although x86-64 don't.


---


### compiler : `gcc`
### title : `std:.get_if variant, unnecessary branch when outside of if statement`
### open_at : `2023-09-12T11:13:29Z`
### last_modified_date : `2023-09-12T14:58:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111388
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.2.1`
### severity : `normal`
### contents :
Example of code (https://godbolt.org/z/M1bWf5sz3)


----
#include <variant>

struct interface{
    virtual ~interface()= default;

    virtual int foo() const = 0;
};

struct a : interface{
    int foo() const override;
};
struct b : interface{
    int foo() const override;
};

class a_or_b{
    std::variant<a,b> ab;
    public:
    a_or_b() = delete;
    a_or_b(a _) : ab(_){}
    a_or_b(b _) : ab(_){}
    interface* operator->() noexcept {
        if (interface* ptr = std::get_if<0>(&ab); ptr) {
            return ptr;
        } 
#if 0
        else if (interface* ptr = std::get_if<1>(&ab); ptr) {
           return ptr;
        }
#else
        return std::get_if<1>(&ab);
#endif
    }
};


int bar3(a_or_b& ab){
    return ab->foo()+1;
}
----


With `#if 1`, the generated code looks like

----
bar3(a_or_b&):
        sub     rsp, 8
        mov     rax, QWORD PTR [rdi]
        call    [QWORD PTR [rax+16]]
        add     rsp, 8
        add     eax, 1
        ret
----

while with `#if 0`, the assembly looks like

----
bar3(a_or_b&):
        cmp     BYTE PTR [rdi+8], 0
        jne     .L2
        sub     rsp, 8
        mov     rax, QWORD PTR [rdi]
        call    [QWORD PTR [rax+16]]
        add     rsp, 8
        add     eax, 1
        ret
bar3(a_or_b&) [clone .cold]:
.L2:
        mov     rax, QWORD PTR ds:0
        ud2
----

If I'm not mistake, with `return std::get_if<1>(&ab);` the compiler verifies if the return of get_if is nullptr, and if it is, then sets the return value to nullptr, which is unnecessary.

With `if 1`, the result is passed as is.

AFAIK the generated assembly is functionally equivalant, but the "more safe"(1) version less optimal


1) more safe as in "there is no UB if the class changes and the variant could be empty or hold another type".

NOTE: replacing "std::get_if<0>"/"std::get_if<1>" with "std::get_if<a>/std::get_if<b>" does not make a relevante difference, the generated code is the same for both "if 0" and "if 1".


For what is worth, clang generates the same code for "if 0" and "if 1".


---


### compiler : `gcc`
### title : `Middle-end: Missed optimization of MASK_LEN_FOLD_LEFT_PLUS`
### open_at : `2023-09-13T09:31:51Z`
### last_modified_date : `2023-09-15T06:42:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111401
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
There is a case I think I missed the optimization in the loop vectorizer:

https://godbolt.org/z/x5sjdenhM

double
foo2 (double *__restrict a,
     double init,
     int *__restrict cond,
     int n)
{
    for (int i = 0; i < n; i++)
      if (cond[i])
        init += a[i];
    return init;
}

It generate the GIMPLE IR as follows:

_60 = .SELECT_VL (ivtmp_58, 4);
...
vect__ifc__35.14_56 = .VCOND_MASK (mask__23.10_50, vect__8.13_54, { 0.0, 0.0, 0.0, 0.0 });
  _36 = .MASK_LEN_FOLD_LEFT_PLUS (init_20, vect__ifc__35.14_56, { -1, -1, -1, -1 }, _60, 0);

The mask of MASK_LEN_FOLD_LEFT_PLUS is the dummy mask {-1.-1,...-1}
I think we should forward the mask of VCOND_MASK into the MASK_LEN_FOLD_LEFT_PLUS.

Then we can eliminate the VCOND_MASK.


I don't where is the optimal place to do the optimization.

Should be the match.pd ? or the loop vectorizer code?

Thanks.


---


### compiler : `gcc`
### title : `Loop distribution fail to optimize memmove for multiple consecutive moves within a loop`
### open_at : `2023-09-13T09:47:21Z`
### last_modified_date : `2023-09-14T03:12:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111402
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
cat test.c

typedef long long v4di __attribute__((vector_size(32)));

void
foo (v4di* __restrict a, v4di *b, int n)
{
  for (int i = 0; i != n; i++)
    a[i] = b[i];
}

void
foo1 (v4di* __restrict a, v4di *b, int n)
{
  for (int i = 0; i != n; i+=2)
    {
    a[i] = b[i];
    a[i+1] = b[i+1];   
    }
}


gcc -O2 -S test.c

GCC can optimize loop in foo to memmove, but not for loop in foo1.
This is from PR111354


---


### compiler : `gcc`
### title : `Wrong code at -O3 on x86_64-linux-gnu`
### open_at : `2023-09-15T07:42:05Z`
### last_modified_date : `2023-10-17T03:53:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111422
### status : `NEW`
### tags : `missed-optimization, needs-bisection, wrong-code`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
gcc at -O3 produced the wrong code.

Compiler explorer: https://godbolt.org/z/3b4v478TG

$ cat a.c
int printf(const char *, ...);
int a, b;
int *c = &b;
unsigned d;
char e;
int f=1;
int i(int k, char *l) {
  if (k < 6)
    return a;
  l[0] = l[1] = l[k - 1] = 8;
  return 0;
}
int m(int k) {
  char g[11];
  int h = i(k, g);
  return h;
}
int main() {
  for (; b < 8; b = b + 1)
    ;
  int j;
  int *n[8];
  j = 0;
  for (;18446744073709551608U + m(*c) + *c + j < 2; j++){
    n[j] = &f;
  }
  for (; e <= 4; e++)
    d = *n[0] == f;
  printf("%d\n", d);
}
$
$ gcc -O0 a.c && ./a.out
1
$ gcc -O3 a.c && ./a.out
Segmentation fault
$ gcc -O3 -fwrapv a.c && ./a.out 
Segmentation fault
$ gcc -fsanitize=address,undefined a.c && ./a.out
1
$


---


### compiler : `gcc`
### title : `a & (a == 0) is not optimized to 0`
### open_at : `2023-09-16T02:41:05Z`
### last_modified_date : `2023-09-19T21:31:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111431
### status : `RESOLVED`
### tags : `missed-optimization, patch, xfail`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
int
foo (int a)
{
  int b = !a;
  return (a & b);
}
```
The general rule is:
(a == CST) & a
transform into:
(CST & 1) ? (a == CST) : 0

So
(simplify
 (bit_and:c (convert@2 (eq @0 INTEGER_CST@1)) @0)
 (if ((wi::to_wide (@1) & 1) != 0)
  @2
  { build_zero_cst (type); }))


---


### compiler : `gcc`
### title : ``bool & (a|1)` is not optimized to just `bool``
### open_at : `2023-09-16T04:11:36Z`
### last_modified_date : `2023-10-17T16:04:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111432
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
take:
```
int
foo3(int c, int bb)
{
  if ((bb & ~3)!=0) __builtin_unreachable();
  return (bb & (c|3));
}
int
foo_bool(int c, _Bool bb)
{
  return (bb & (c|7));
}
```
foo3 should be optimized to just `return bb`, likewise for foo_bool.

Note LLVM does this.


---


### compiler : `gcc`
### title : `some boolean related transformation to `~(a&b)``
### open_at : `2023-09-17T06:11:31Z`
### last_modified_date : `2023-09-17T06:11:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111439
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take
```
bool foo0(bool a, bool b)
{
    return (a <= b) ^ a;
}
bool foo1(bool a, bool b)
{
    return (!a | b) ^ a;
}
bool foo2(bool b, int x, int y)
{
    bool a = x == y;
    return (!a | b) ^ a;
}
```

These all should be optimized to `~(a&b)`.
Right now foo1 is optimized at -O2 but not -O1 due to PR 111149 (canonical form of a ^ b or a != b for booleans).
foo2 can be handled at -O2 if we use bitwise_inverted_equal_p instead of bitwise_equal_p (with bit_not).

foo0 will just need a new pattern really. we do need to make a decision on the canonical form, is it `~a | b` or `a <= b` but both show currently too.


---


### compiler : `gcc`
### title : `RISC-V: Missed optimized for strided load/store with stride = element width`
### open_at : `2023-09-18T03:14:50Z`
### last_modified_date : `2023-09-21T08:27:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111450
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Consider this following cases:

https://godbolt.org/z/3MPoz5q6x

#include "riscv_vector.h"

void foo (int8_t *in, int8_t *out, int n)
{
    vint8m1_t v = __riscv_vlse8_v_i8m1 (in, 1, n);
    __riscv_vsse8_v_i8m1 (out, 1, v, n);
}

void foo1 (int16_t *in, int16_t *out, int n)
{
    vint16m1_t v = __riscv_vlse16_v_i16m1 (in, 2, n);
    __riscv_vsse16_v_i16m1 (out, 2, v, n);
}

void foo3 (int32_t *in, int32_t *out, int n)
{
    vint32m1_t v = __riscv_vlse32_v_i32m1 (in, 4, n);
    __riscv_vsse32_v_i32m1 (out, 4, v, n);
}

void foo4 (int64_t *in, int64_t *out, int n)
{
    vint64m1_t v = __riscv_vlse64_v_i64m1 (in, 8, n);
    __riscv_vsse64_v_i64m1 (out, 8, v, n);
}

ASM:

foo:
        li      a5,1
        vsetvli zero,a2,e8,m1,ta,ma
        vlse8.v v1,0(a0),a5
        vsse8.v v1,0(a1),a5
        ret
foo1:
        li      a5,2
        vsetvli zero,a2,e16,m1,ta,ma
        vlse16.v        v1,0(a0),a5
        vsse16.v        v1,0(a1),a5
        ret
foo3:
        li      a5,4
        vsetvli zero,a2,e32,m1,ta,ma
        vlse32.v        v1,0(a0),a5
        vsse32.v        v1,0(a1),a5
        ret
foo4:
        li      a5,8
        vsetvli zero,a2,e64,m1,ta,ma
        vlse64.v        v1,0(a0),a5
        vsse64.v        v1,0(a1),a5
        ret


When stride = element width, vlse should be optimized into vle.
vsse should be optimized into vse.v.

So we can save a constant move instruction.


---


### compiler : `gcc`
### title : `RISC-V: Missed optimization of vrgather.vv into vrgatherei16.vv`
### open_at : `2023-09-18T03:24:42Z`
### last_modified_date : `2023-09-22T07:20:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111451
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Consider this following case:

#include <stdint.h>
typedef int32_t vnx32si __attribute__ ((vector_size (128)));

#define MASK_2(X, Y) (Y) - 1 - (X), (Y) - 2 - (X)
#define MASK_4(X, Y) MASK_2 (X, Y), MASK_2 (X + 2, Y)
#define MASK_8(X, Y) MASK_4 (X, Y), MASK_4 (X + 4, Y)
#define MASK_16(X, Y) MASK_8 (X, Y), MASK_8 (X + 8, Y)
#define MASK_32(X, Y) MASK_16 (X, Y), MASK_16 (X + 16, Y)
#define MASK_64(X, Y) MASK_32 (X, Y), MASK_32 (X + 32, Y)
#define MASK_128(X, Y) MASK_64 (X, Y), MASK_64 (X + 64, Y)

#define PERMUTE(TYPE, NUNITS)                                                  \
  __attribute__ ((noipa)) void permute_##TYPE (TYPE values1, TYPE values2,     \
					       TYPE *out)                      \
  {                                                                            \
    TYPE v                                                                     \
      = __builtin_shufflevector (values1, values2, MASK_##NUNITS (0, NUNITS)); \
    *(TYPE *) out = v;                                                         \
  }

#define TEST_ALL(T)                                                            \
  T (vnx32si, 32)                                                              \

TEST_ALL (PERMUTE)

ASM:

permute_vnx32si:
        li      a5,32
        li      a4,31
        vsetvli zero,a5,e32,m8,ta,ma
        vid.v   v8
        vle32.v v24,0(a0)
        vrsub.vx        v8,v8,a4
        vrgather.vv     v16,v24,v8
        vse32.v v16,0(a2)
        ret

https://godbolt.org/z/Mh77YY91r

Here we use:

vsetvli zero,a5,e32,m8,ta,ma
...
vrgather.vv     v16,v24,v8

The index vector register "v8" occupies 8 registers.
We should optimize it into vrgatherei16.vv which is using int16 as the index 
elements.

Then with vrgatherei16.vv, the v8 will occupy 4 registers instead of 8.
Lower the register consuming and register pressure.


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-3719-gb34f3736356`
### open_at : `2023-09-18T09:45:57Z`
### last_modified_date : `2023-09-26T15:02:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111456
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/jqqP5baY8

Given the following code:

void foo(void);
static int i;
static int *j = &i;
static char l;
static void(a)(char) {}
static short(b)(short c, short d) { return c - d; }
static short(e)(short f, int g) {
    return f < 0 || g < 0 || g >= 32 ? f : f >> g;
}
static short(h)(short f, int g) { return g >= 2 ?: f >> g; }
static char k(char m, short n) {
    short o;
    int *p = &i;
    if (!(((m) >= 1) && ((m) <= 1))) {
        __builtin_unreachable();
    }
    o = e(i, i);
    if (h(1, o))
        ;
    else {
        m = 0;
        for (; m >= -20; m = b(m, 9))
            if (a(i), n) {
                if (*p)
                    ;
                else
                    foo();
                ;
            } else
                return l;
    }
    return i;
}
int main() { k(0 <= 0 > *j, i); }

gcc-trunk -O2 does not eliminate the call to foo:

main:
	movl	i(%rip), %eax
	testw	%ax, %ax
	js	.L6
	testl	%eax, %eax
	jne	.L6
.L9:
	xorl	%eax, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L6:
	movswl	%ax, %edx
	cmpw	$1, %ax
	jg	.L9
	testl	%edx, %edx
	je	.L9
	pushq	%rbx
	movl	$3, %ebx
.L5:
	cmpl	$0, i(%rip)
	je	.L13
.L4:
	subb	$1, %bl
	jne	.L5
	xorl	%eax, %eax
	popq	%rbx
	ret
.L13:
	call	foo
	jmp	.L4

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	xorl	%eax, %eax
	ret

Bisects to r14-3719-gb34f3736356


---


### compiler : `gcc`
### title : `[14 Regression] Dead Code Elimination Regression since r14-3407-g936a12331a2`
### open_at : `2023-09-18T09:50:53Z`
### last_modified_date : `2023-09-18T15:22:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111457
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/j4M8z8MzK

Given the following code:

void foo(void);
static int a, b, c, d, h;
static short i;
static void e(int f) {
    int g = -16;
    for (; g != -6; g = g + 5) {
        b = 6;
        if (!(((f) >= 1) && ((f) <= 26))) {
            __builtin_unreachable();
        }
        f = 0;
        for (; f < 26; ++f)
            if (a) {
                foo();
                break;
            }
        d = 0;
    }
}
int main() {
    a = 0;
    e(0 == i);
    h = d;
    c = b;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	movl	$0, a(%rip)
	pushq	%rbx
	movl	$2, %ebx
.L3:
	movl	$6, b(%rip)
	movl	a(%rip), %eax
	testl	%eax, %eax
	je	.L2
	call	foo
.L2:
	movl	$0, d(%rip)
	cmpl	$1, %ebx
	jne	.L4
	xorl	%eax, %eax
	popq	%rbx
	ret
	.p2align 4,,10
	.p2align 3
.L4:
	movl	$1, %ebx
	jmp	.L3

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	movl	$0, a(%rip)
	xorl	%eax, %eax
	movl	$6, b(%rip)
	movl	$0, d(%rip)
	ret

Bisects to r14-3407-g936a12331a2


---


### compiler : `gcc`
### title : `RISC-V: redundant sign extensions despite ABI guarantees`
### open_at : `2023-09-18T22:02:15Z`
### last_modified_date : `2023-10-19T15:57:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111466
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `13.0`
### severity : `normal`
### contents :
Consider the test below:

int foo(int unused, int n, unsigned y, unsigned delta){
  int s = 0;
  unsigned int x = 0;    // if int, sext elided
  for (;x<n;x +=delta)
    s += x+y;
  return s;
}

-O2 -march=rv64gc_zba_zbb_zbs

foo2:
    sext.w    a6,a1            # 1
    beq    a1,zero,.L4
    li    a5,0
    li    a0,0
.L3:
    addw    a4,a2,a5
    addw    a5,a3,a5
    addw    a0,a4,a0
    bltu    a5,a6,.L3
    ret
.L4:
    li    a0,0
    ret

I believe the SEXT.W is not semantically needed as a1 is supposed to be sign extended already at call site as per psABI [1]. I quote

    "When passed in registers or on the stack, integer scalars narrower than XLEN bits are widened according to the sign of their type up to 32 bits, then sign-extended to XLEN bits"

However currently RISC-V backend thinks otherwise: changing @x to int, causes the the sign extend to go away. I think both the cases should behave the same (and not generate SEXT.w) given the ABI clause above. Note that this manifests in initial RTL expand itself generating/or-not-generating the sign_extend so if it is unnecessary we can avoid late fixups in REE.

Andrew Waterman confirmed that the ABI guarantees this and that the SEXT.W is redundant [1]

[1] https://gcc.gnu.org/pipermail/gcc-patches/2023-September/630811.html


---


### compiler : `gcc`
### title : `REE failing to eliminate redundant extension due to multiple reaching def(s)`
### open_at : `2023-09-18T22:32:28Z`
### last_modified_date : `2023-09-19T09:17:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111467
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
For the trivial test case below (credit goes to Palmer for mentioning this almost 2 years ago).

int
foo6(int a, int b)
{
  return a > b ? a : b;
}

-O2 -march=rv64gc

foo6:
	mv	a5,a1
	bge	a1,a0,.L5
	mv	a5,a0
.L5:
	sext.w	a0,a5
	ret

REE fails to eliminate the sign extension due to multiple reaching definitions constraint.

I don't know how involved or runtime cost relaxing the constraint is, so opening this PR to investigate.

FWIW a zba build generates a max insn, eliminating the sext.w, but the vanilla case shows where things can possibly be improved.


---


### compiler : `gcc`
### title : `RISC-V: autovec fma generate redundant vmerge instruction`
### open_at : `2023-09-19T07:20:27Z`
### last_modified_date : `2023-09-19T07:27:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111470
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Consider this code:

void foo (float *restrict a, float *restrict b, float *restrict c,
          float *restrict merged, int *restrict pred,
          float *restrict out, int n)
{
  for (int i = 0; i < n; i += 1)
    out[i] = pred[i] ? (a[i] * b[i] + c[i]) : merged[i];
}

assembly (this second vmerge is redundant):

foo:
        ble     a6,zero,.L5
        vsetvli t4,zero,e32,m1,ta,ma
        vmv.v.i v7,0
.L3:
        vsetvli a7,a6,e32,m1,ta,ma
        vle32.v v0,0(a4)
        vsetvli t4,zero,e32,m1,ta,ma
        vmseq.vi        v1,v0,0
        vmsne.vi        v2,v0,0
        vmv1r.v v0,v2
        vsetvli zero,a7,e32,m1,ta,ma
        vle32.v v6,0(a0),v0.t
        vmv1r.v v0,v1
        vle32.v v1,0(a3),v0.t
        vmv1r.v v0,v2
        vle32.v v5,0(a1),v0.t
        vle32.v v4,0(a2),v0.t
        vmerge.vvm      v3,v7,v4,v0
        vsetvli zero,zero,e32,m1,tu,mu
        vfmacc.vv       v3,v6,v5,v0.t
        vsetvli t3,zero,e32,m1,ta,ma
        slli    t1,a7,2
        vmerge.vvm      v1,v1,v3,v0
        sub     a6,a6,a7
        vsetvli zero,a7,e32,m1,ta,ma
        vse32.v v1,0(a5)
        add     a4,a4,t1
        add     a0,a0,t1
        add     a1,a1,t1
        add     a2,a2,t1
        add     a3,a3,t1
        add     a5,a5,t1
        bne     a6,zero,.L3
.L5:
        ret

From the vect dump, the .COND_LEN_ADD and .VCOND_MASK can be combined into a single .COND_LEN_ADD. For .COND_ADD and .VCOND_MASK, the match.pd has the simplify pattern (like bellow) to handle this combine. So I think we should add .COND_LEN_ADD in match.pd too.

  /* Detect cases in which a VEC_COND_EXPR effectively replaces the
   "else" value of an IFN_COND_*.  */
  (for cond_op (COND_BINARY)
   (simplify
    (vec_cond @0 (view_convert? (cond_op @0 @1 @2 @3)) @4)
    (with { tree op_type = TREE_TYPE (@3); }
     (if (element_precision (type) == element_precision (op_type))
      (view_convert (cond_op @0 @1 @2 (view_convert:op_type @4))))))
   (simplify
    (vec_cond @0 @1 (view_convert? (cond_op @2 @3 @4 @5)))
    (with { tree op_type = TREE_TYPE (@5); }
     (if (inverse_conditions_p (@0, @2)
          && element_precision (type) == element_precision (op_type))
      (view_convert (cond_op @2 @3 @4 (view_convert:op_type @1)))))))


---


### compiler : `gcc`
### title : `Missed COND_SQRT and COND_LEN_SQRT internal fn`
### open_at : `2023-09-19T08:08:06Z`
### last_modified_date : `2023-09-19T09:59:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111473
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
Currently, we cann't autovec no-fast-math sqrt code since the midend missed COND_SQRT and COND_LEN_SQRT patterns.

Reproduce on compiler explorer: https://godbolt.org/z/nd61GTK6e


---


### compiler : `gcc`
### title : `(a OP CST1) & (a == CST0) could be simplified down to (CST OP CST1) & (a == CST0)`
### open_at : `2023-09-19T21:31:12Z`
### last_modified_date : `2023-09-20T07:56:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111487
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
This is an extension of PR 111431 .

Take:
```
int g(int a)
{
        int b = a + 4;
        return (a == 0) & b;
}
```
This could be optimized down to just `return (a == 0);` as `(0+4)&1` is 1.
This can work with almost every binary (and even unary) operation that contains a constant.


---


### compiler : `gcc`
### title : `RISC-V: Dynamic LMUL picking incorrect LMUL`
### open_at : `2023-09-20T03:23:17Z`
### last_modified_date : `2023-10-15T02:53:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111492
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdbool.h>
#include <assert.h>
struct A
{
  unsigned int a;
  unsigned char c1, c2;
  bool b1 : 1;
  bool b2 : 1;
  bool b3 : 1;
};

void
foo (const struct A *x, int y)
{
  int s = 0, i;
  for (i = 0; i < y; ++i)
    {
      const struct A a = x[i];
      s += a.b1 ? 1 : 0;
    }
  assert (s == 0);
    //__builtin_abort ();
}

int
main ()
{
  struct A x[100];
  int i;
  __builtin_memset (x, -1, sizeof (x));
  for (i = 0; i < 100; i++)
    {
      x[i].b1 = false;
      x[i].b2 = false;
      x[i].b3 = false;
    }
  foo (x, 100);
  return 0;
}

--param=riscv-autovec-lmul=dynamic.


Pick LMUL = 8 incorrectly which cause horrible register spillings.


---


### compiler : `gcc`
### title : `Optimizer issue when reinitializing an object of a standard-layout class with a trivial copy constructor and a trivial destructor but with padding bytes`
### open_at : `2023-09-20T13:09:16Z`
### last_modified_date : `2023-09-21T13:50:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111496
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `13.2.1`
### severity : `normal`
### contents :
Created attachment 55948
Teste case to reproduce the optimization issue

In some cases, reinitializing an object of a standard-layout class with a trivial copy constructor and a trivial destructor needs something more complex than *this={} if this line is in a hot cycle. For some reason, gcc creates an new object on stack which is copied to the target location. 

Well, it's literally what *this={} should do, but I expected the compiler to elide the copying.
Calling the destructor explicitly and reconstructing the object with a placement new gives the desired code without the extra copying, but looks kinda ugly.

The issue seems to be present in gcc9, gcc12, and gcc13, but different contents the class trigger the generation of inferior code. Clang seems to produce the shortest and the fastest code for this kind of operation.

Using -Os makes the compiler generate the improved binary code. Also replacing "[[gnu::used]] void reset() { *this = {}; }" per "[[gnu::used]] void reset() { A2 a2; *this = a2; }" improved the binary code generated.

The reproducible example is available at:

https://godbolt.org/z/hf8qf6qan

Source code:

#include <memory>

struct A {
    [[gnu::used]] void reset() { *this = A(); }
private:
    int b[4] = {};
    char* p = {};
    int x = {};
};

struct A2 {
    [[gnu::used]] void reset() { *this = {}; }
private:
    int b[4] = {};
    char* p = {};
    int x = {};
};

struct A3 {
    [[gnu::used]] void reset() {
        this->~A3();
        new(this) A3;
    }
private:
    int b[4] = {};
    char* p = {};
    int x = {};
};

---------------------

Regards,
Rogerio


---


### compiler : `gcc`
### title : `[arm-none-eabi-gcc] / suboptimal optimization / subs followed by cmp (et alii)`
### open_at : `2023-09-20T14:55:52Z`
### last_modified_date : `2023-09-26T00:24:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111500
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
Hi!

For my STM32G030K6T I need thumb2 machine code.
arm-none-eabi-gcc can convert my C code quite nicely.

When I inspected the resulting disassembled machine code, I found some funny artifacts.
Is it worth making&sending minimal source code and/or that corresponding "preprocessed file *.i*"?
I mean: Is somebody able/willing to do something about it?

Here are the 4 artifacts (when using "-O3 -g"):
1.
a "subs r2, #1" instruction is followed by a "cmp r2, #0 / bne.n" instruction, although the "subs" already leaves the zero flag properly, doesn't it?
2.
a "b.n" instruction jumps to a "bx lr" instruction, although both have the same length... i mean: instead of "b.n" it could have written "bx lr"...
3.
a "uxth r2, r2" is followed by a "strh r2, [r3]", although "strh" doesn't look at the high half-word of r2, does it?
4.
when i use single bits in a struct (e. g.: "struct A { int a:1; int b:7; }"), gcc reserves 16 bytes on the stack and sometimes writes into this area, without ever reading from there... when i do it myself with "struct A { int ab; }" and "if (S.ab&128)..." and "if (S.ab&127)..." then it doesnt touch the stack...

Thx.

Bye.


---


### compiler : `gcc`
### title : `RISC-V: non-optimal casting when shifting`
### open_at : `2023-09-20T16:24:13Z`
### last_modified_date : `2023-09-21T06:20:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111501
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Created attachment 55949
tar file of -save-temps output

I would expect the first to be able to compile into the second:

unsigned int do_shift(unsigned long csum)
{
        return (unsigned short)(csum >> 32);
}

unsigned int do_shift2(unsigned long csum)
{
        return (csum << 16) >> 48;
}

However, the asm output is instead:

do_shift:
        srli    a0,a0,32
        slli    a0,a0,48
        srli    a0,a0,48
        ret
do_shift2:
        slli    a0,a0,16
        srli    a0,a0,48
        ret

These are the same so the first should be able to be compiled into the second.


---


### compiler : `gcc`
### title : `Suboptimal unaligned 2/4-byte memcpy on strict-align targets`
### open_at : `2023-09-20T17:51:16Z`
### last_modified_date : `2023-09-21T06:22:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111502
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.2.0`
### severity : `normal`
### contents :
I was playing with RISC-V GCC 12.2.0 from Arch Linux. I noticed inefficient-looking assembly output in code that uses memcpy to access 32-bit unaligned integers. I tried Godbolt with 16/32-bit integers and seems that the same weirdness happens with RV32 & RV64 with GCC 13.2.0 and trunk, and also on a few other targets. (Clang's output looks OK.)

For a little endian target:

#include <stdint.h>
#include <string.h>

uint32_t bytes16(const uint8_t *b)
{
    return (uint32_t)b[0]
        | ((uint32_t)b[1] << 8);
}

uint32_t copy16(const uint8_t *b)
{
    uint16_t v;
    memcpy(&v, b, sizeof(v));
    return v;
}

riscv64-linux-gnu-gcc -march=rv64gc -O2 -mtune=size

bytes16:
        lhu     a0,0(a0)
        ret

copy16:
        lhu     a0,0(a0)
        ret

That looks good because -mno-strict-align is the default.

After omitting -mtune=size, unaligned access isn't used (the output is the same as with -mstrict-align):

riscv64-linux-gnu-gcc -march=rv64gc -O2

bytes16:
        lbu     a5,1(a0)
        lbu     a0,0(a0)
        slli    a5,a5,8
        or      a0,a5,a0
        ret

copy16:
        lbu     a4,0(a0)
        lbu     a5,1(a0)
        addi    sp,sp,-16
        sb      a4,14(sp)
        sb      a5,15(sp)
        lhu     a0,14(sp)
        addi    sp,sp,16
        jr      ra

bytes16 looks good but copy16 is weird: the bytes are copied to an aligned location on stack and then loaded back.

On Godbolt it happens with GCC 13.2.0 on RV32, RV64, ARM64 (but only if using -mstrict-align), MIPS64EL, and SPARC & SPARC64 (comparison needs big endian bytes16). For ARM64 and MIPS64EL the oldest GCC on Godbolt is GCC 5.4 and the same thing happens with that too.

32-bit reads with -O2 behave similarly. With -Os a call to memcpy is emitted for copy32 but not for bytes32.

#include <stdint.h>
#include <string.h>

uint32_t bytes32(const uint8_t *b)
{
    return (uint32_t)b[0]
        | ((uint32_t)b[1] << 8)
        | ((uint32_t)b[2] << 16)
        | ((uint32_t)b[3] << 24);
}

uint32_t copy32(const uint8_t *b)
{
    uint32_t v;
    memcpy(&v, b, sizeof(v));
    return v;
}

riscv64-linux-gnu-gcc -march=rv64gc -O2

bytes32:
        lbu     a4,1(a0)
        lbu     a3,0(a0)
        lbu     a5,2(a0)
        lbu     a0,3(a0)
        slli    a4,a4,8
        or      a4,a4,a3
        slli    a5,a5,16
        or      a5,a5,a4
        slli    a0,a0,24
        or      a0,a0,a5
        sext.w  a0,a0
        ret

copy32:
        lbu     a2,0(a0)
        lbu     a3,1(a0)
        lbu     a4,2(a0)
        lbu     a5,3(a0)
        addi    sp,sp,-16
        sb      a2,12(sp)
        sb      a3,13(sp)
        sb      a4,14(sp)
        sb      a5,15(sp)
        lw      a0,12(sp)
        addi    sp,sp,16
        jr      ra

riscv64-linux-gnu-gcc -march=rv64gc -Os

bytes32:
        lbu     a4,1(a0)
        lbu     a5,0(a0)
        slli    a4,a4,8
        or      a4,a4,a5
        lbu     a5,2(a0)
        lbu     a0,3(a0)
        slli    a5,a5,16
        or      a5,a5,a4
        slli    a0,a0,24
        or      a0,a0,a5
        sext.w  a0,a0
        ret

copy32:
        addi    sp,sp,-32
        mv      a1,a0
        li      a2,4
        addi    a0,sp,12
        sd      ra,24(sp)
        call    memcpy@plt
        ld      ra,24(sp)
        lw      a0,12(sp)
        addi    sp,sp,32
        jr      ra

I probably cannot test any proposed fixes but I hope this report is still useful. Thanks!


---


### compiler : `gcc`
### title : `RISC-V: Failed to vectorize conversion from INT64 -> _Float16`
### open_at : `2023-09-21T01:23:14Z`
### last_modified_date : `2023-10-02T22:30:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111506
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdint.h>

void foo (int64_t *__restrict a, _Float16 * b, int n)
{
    for (int i = 0; i < n; i++) {
        b[i] = (_Float16)a[i];
    }
}

-march=rv64gcv_zvfh_zfh -O3  -fno-trapping-math -fopt-info-vec-missed
-march=rv64gcv_zvfh_zfh -O3  -ffast-math -fopt-info-vec-missed

<source>:5:23: missed: couldn't vectorize loop
<source>:6:27: missed: not vectorized: no vectype for stmt: _4 = *_3;
 scalar_type: int64_t
Compiler returned: 0

https://godbolt.org/z/orevoq7E1

Consider LLVM can vectorize with -fno-trapping-math.
However, LLVM can not vectorize when -ftrapping-math.

So, we need an explicit patterns from INT64 -> _Float16 with !flag_trapping_math


---


### compiler : `gcc`
### title : `[14 Regression] Missed Dead Code Elimination since r14-4089-gd45ddc2c04e`
### open_at : `2023-09-21T09:40:09Z`
### last_modified_date : `2023-10-16T11:26:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111515
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/K9bbM4fc9

Given the following code:

void foo(void);
static struct a {
    short b;
    int c;
    char d;
    int e;
    unsigned f;
} h, j, k = {0, 4274716299}, l, *aa = &j;
static char g;
static short i;
static int m, ah;
static int *ac = &j.e;
static void n(struct a o) {
    int aj = o.c;
    int *ak = &h.e;
    *ak = g && aj;
    if (!(((aj) >= 0) && ((aj) <= 4274716299))) {
        __builtin_unreachable();
    }
    i = ah;
}
int main() {
    n(l);
    *aa = h;
    m = *ac;
    if (m < 10)
        ;
    else
        foo();
    ;
    n(k);
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	xorl	%edi, %edi
	call	n.isra.0
	movdqa	h(%rip), %xmm0
	movl	h+16(%rip), %eax
	movaps	%xmm0, j(%rip)
	cmpl	$9, j+12(%rip)
	movl	%eax, j+16(%rip)
	jg	.L6
.L4:
	movl	$-20250997, %edi
	xorl	%eax, %eax
	call	n.isra.0
	addq	$8, %rsp
	ret
.L6:
	call	foo
	jmp	.L4

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	movl	$0, h+12(%rip)
	movl	h+16(%rip), %eax
	movdqa	h(%rip), %xmm0
	movl	%eax, j+16(%rip)
	movaps	%xmm0, j(%rip)

Bisects to r14-4089-gd45ddc2c04e


---


### compiler : `gcc`
### title : `missing optimization x & ~c | (y | c) -> x | (y | c)`
### open_at : `2023-09-22T16:37:43Z`
### last_modified_date : `2023-09-22T18:12:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111541
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
On this function clang generates shorter code:

unsigned foo(unsigned x, unsigned y, unsigned c)
{
    return x & ~c | (y | c);
}

Clang notices that the expression can be simplified to x | (y | c). It would be great if GCC can do the same.

https://gcc.godbolt.org/z/dMo4nEjrs

This issue is symmetric to the one described in PR 98710. The idea behind this simplification is the following: when we are working with bitsets, "|" can be read as adding bits and "&~" as removing. Therefore the expression "x & ~c | (y | c)" can be read as removing "c" from "x" and then adding "y | c".

So the simplification

x & ~c | (y | c) -> x | (y | c)

means there is no need to remove "c" if later we add something containing "c".


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] (a==0)&(b==0) into `(a|b) == 0` optimization sometimes gets in the way of other optimizations`
### open_at : `2023-09-22T18:51:13Z`
### last_modified_date : `2023-10-17T13:00:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111542
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Take:
```
_Bool f(int x,int y)
{
  _Bool a = x == 0;
  _Bool b = y == 0;
  _Bool t = a & b;
  return t & !a;
}

```

This used to optimize to 0 in GCC 6 but starting with GCC 7, we get:
```
  _5 = x_2(D) | y_3(D);
  _6 = _5 == 0;
  _1 = x_2(D) != 0;
  _7 = _1 & _6;
```


---


### compiler : `gcc`
### title : ``(a & b) & ~a` could be optimized before reassociation`
### open_at : `2023-09-22T18:52:59Z`
### last_modified_date : `2023-09-24T06:49:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111543
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
_Bool fb(int x,int y)
{
  _Bool a = x == 1;
  _Bool b = y == 2;
  _Bool t = a & b;
  return t & !a;
}
int fi(int a,int b)
{
  int t = a & b;
  return t & ~a;
}

```
We should optimize both of these to `return 0` in ccp1.


---


### compiler : `gcc`
### title : `Fix for PR106081 is not working with profile feedback on imagemagick`
### open_at : `2023-09-23T15:30:44Z`
### last_modified_date : `2023-09-24T06:41:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111551
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
As seen in https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=471.507.0&plot.1=473.507.0&plot.2=475.507.0&plot.3=477.507.0&
Fix for PR106081 improved imagemagick significantly without FDO but not with FDO.


---


### compiler : `gcc`
### title : `549.fotonik3d_r regression with -O2 -flto -march=native on zen between g:85d613da341b7630 (2022-06-21 15:51) and g:ecd11acacd6be57a (2022-07-01 16:07)`
### open_at : `2023-09-23T15:46:04Z`
### last_modified_date : `2023-09-24T06:42:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111552
### status : `UNCONFIRMED`
### tags : `lto, missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=297.527.0&plot.1=296.527.0&


---


### compiler : `gcc`
### title : `RISCV: shrink-wrapper optimization question`
### open_at : `2023-09-23T21:00:41Z`
### last_modified_date : `2023-09-26T13:26:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111558
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `13.0`
### severity : `normal`
### contents :
Created attachment 55977
shrink-synthetic-example.c

When I was studying shrink-wrapping behavior, I discovered something strange code generation.

See the code and assembly at https://godbolt.org/z/P3WfTszYn

Every branch has the same save/restore callee-saved registers.
Therefore, the shrink-wrapping optimization may reduce the speed of the function when input parameters are not zero...

I understand that this is not a synthetic code snippet... 

But may this code generation be reproduced in real code (without asm inlining)?


---


### compiler : `gcc`
### title : `Missed optimization of available expression`
### open_at : `2023-09-24T03:34:08Z`
### last_modified_date : `2023-09-24T06:51:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111560
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Hello, we found some optimizations (regarding Available Expression) that GCC may have missed. We would greatly appreicate if you can take a look and let us know what you think.
Here are three examples.


Example 1:
https://godbolt.org/z/eYcWocsPf

Given the following code: 
```c++
int test()
{
    int a,b,c,d,e,f,g;
    cin>>a>>b>>c>>d>>g;
    e=a+b+c;  //line 5
    f=d+b+c;  //"b+c" can be replaced with the value at line 5
    cout<<e<<f;
    return 0;
}
```

We note that `b+c` at line 6 in the test code can be replaced with the value at line 5, but gcc-trunk -O3 does not:
```asm
test():
        push    rbx
        mov     edi, OFFSET FLAT:_ZSt3cin
        sub     rsp, 32
        lea     rsi, [rsp+12]
        call    std::basic_istream<char, std::char_traits<char> >::operator>>(int&)
        lea     rsi, [rsp+16]
        mov     rdi, rax
        call    std::basic_istream<char, std::char_traits<char> >::operator>>(int&)
        lea     rsi, [rsp+20]
        mov     rdi, rax
        call    std::basic_istream<char, std::char_traits<char> >::operator>>(int&)
        lea     rsi, [rsp+24]
        mov     rdi, rax
        call    std::basic_istream<char, std::char_traits<char> >::operator>>(int&)
        lea     rsi, [rsp+28]
        mov     rdi, rax
        call    std::basic_istream<char, std::char_traits<char> >::operator>>(int&)
        mov     esi, DWORD PTR [rsp+16]
        mov     ebx, DWORD PTR [rsp+24]
        mov     edi, OFFSET FLAT:_ZSt4cout
        mov     eax, DWORD PTR [rsp+20]
        add     ebx, esi
        add     esi, DWORD PTR [rsp+12]
        add     esi, eax
        add     ebx, eax
        call    std::basic_ostream<char, std::char_traits<char> >::operator<<(int)
        mov     esi, ebx
        mov     rdi, rax
        call    std::basic_ostream<char, std::char_traits<char> >::operator<<(int)
        add     rsp, 32
        xor     eax, eax
        pop     rbx
        ret
```


Example 2:
https://godbolt.org/z/o61sx6dGh

Given the following code: 
```c++
int var_13; int var_14;
void test(int var_0, int var_4, int var_5) {
    var_13 = var_0 + var_4 + var_5;  //line 3
    var_14 = var_4 + var_5;  //"var_4 + var_5" can be replaced with the value at line 3
}
```

We note that `var_4 + var_5` at line 4 in the test code can be replaced with the value at line 3, but gcc-trunk -O3 does not:
```asm
test(int, int, int):
        add     edi, esi
        add     esi, edx
        add     edi, edx
        mov     DWORD PTR var_14[rip], esi
        mov     DWORD PTR var_13[rip], edi
        ret
var_14:
        .zero   4
var_13:
        .zero   4
```

Example 3:
https://godbolt.org/z/caYT9E6Mz

Similar to example 2, but this is an example that might involve shifting operations:
```c++
int var_14,var_27;
void test(int var_1, int var_3) {
    var_14 = var_1 + var_1;
    var_27 = var_3 + var_1 + var_1;
}
```
```asm
test(int, int):
        lea     eax, [rdi+rdi]
        mov     DWORD PTR var_14[rip], eax
        lea     eax, [rsi+rdi*2]
        mov     DWORD PTR var_27[rip], eax
        ret
var_27:
        .zero   4
var_14:
        .zero   4
```

Thank you very much for your time and effort! We look forward to hearing from you.


---


### compiler : `gcc`
### title : `Missed optimization of Loop Unswitch`
### open_at : `2023-09-24T03:43:29Z`
### last_modified_date : `2023-09-24T06:53:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111564
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Hello, we found some optimizations (regarding Loop Unswitch) that GCC may have missed. We would greatly appreicate if you can take a look and let us know what you think.
Here are two examples.


Example 1:
https://godbolt.org/z/G3xGvGxq1

Given the following code: 
```c++
extern int var_21;
extern int var_25;
extern int var_28;
extern int var_29;

void test(int var_0, int var_1, int var_2, int var_3, int var_4, int var_5, int var_6, int var_7, int var_8, int var_9, int var_10, int var_11, int var_12, int var_13, int var_14, int var_15, int var_16, int var_17) {
    for (int i_0 = 0/*0*/; i_0 < ((((((((var_17)) ? (((819565140) + (var_9))) : ((((var_16)) ? (2141366111) : (var_17)))))) ? (((((((var_4)) ? (var_16) : (var_17)))) ? (779921519) : (((var_1) + (var_8))))) : (((((((var_10)) ? (var_12) : (var_5)))) ? ((((var_15)) ? (var_16) : (var_12))) : ((((var_3)) ? (var_0) : (var_11))))))) - (779921500))/*19*/; i_0 += 4/*4*/) 
    {
        var_21 += var_17;
        if (var_3)
        {
            var_25 += var_17 ? -212529188 : (((var_6) ? (var_5 ? 2029341098 : var_13) : var_11));
            var_28 += var_8 ? ((var_10 ? (var_13 - var_1) : var_14 ? var_17 : var_8)) : (var_6 ? 942541005 : var_1 / var_9);
            var_29 += var_2 ? var_6 : ((var_9 ? var_8 : (var_11 ? var_2 : var_15)));
        }

    }
}

```

Because `var_3` is a loop invariant so we can hoist the if condition out of the loop, but gcc-trunk -O3 dose not (main parts):
```asm
test(int, int, int, int, int):

        ...

        .L154:
        cmp     edx, ecx
        jle     .L9
        test    r10d, r10d
        je      .L153

        ...

        .L153:
        add     ecx, 4
        mov     eax, 1
        jmp     .L154
```


Example 2:
https://godbolt.org/z/WcMnz97jv

Given the following code: 
```c++
extern int var_13;
extern int var_15;
extern int var_17;
extern int var_19;
extern int var_20;
extern int var_24;

void test(int var_0, int var_1, int var_2, int var_3, int var_4, int var_5, int var_6, int var_7, int var_8, int var_9, int var_10) {

    for (int i_0 = ((var_9) + (1516547146))/*0*/; i_0 < (((-((-((((var_4)) ? (var_6) : (var_8))))))) + (960893662))/*16*/; i_0 += ((var_0) + (1994872666))/*1*/) 
    {
        if (var_1)
        {
            var_13 += (var_0 ? var_8 : var_7) ? (var_0 ? var_7 : var_4) : ((var_7 ? var_1 : var_2) ? 1 : 2);
        }

        var_15 += (var_0 ? var_9 : var_6) ? (var_4 ? !var_5 : (var_7 ? var_3 : var_9)) : var_8;
        var_17 += var_3 ? ((var_4 ? var_5 : var_8) ? var_3 : var_1) : (var_5 ? var_8 : var_0);
        var_19 += (var_1 ? var_8 : var_2) ? (var_1 ? (var_7 ? var_3 : var_9) : !var_3) : var_9;
        var_20 += var_10 ? var_3 : (var_4 ? var_10 : var_8);
        var_24 += var_0 ? -var_5 : (var_8 ? (var_3 ? var_8 : var_1) : (var_6 ? var_6 : var_5));
    }
}
```

Similarly, `var_1` is an loop invariant and should be hoisted out of the loop as well.
But gcc-trunk -O3 does not:
```asm
test(int, int, int, int, int):
        
        ...

        .L273:
        test    ecx, ecx
        je      .L87
        mov     r8d, DWORD PTR [rsp+64]
        add     DWORD PTR [rsp-28], r8d
        test    ebp, ebp
        jne     .L120
        add     DWORD PTR [rsp-32], ecx
        test    edx, edx
        jne     .L271
.L121:
        mov     BYTE PTR [rsp-14], 1
.L89:
        mov     ebx, DWORD PTR [rsp-12]
        test    r12d, r12d
        cmovne  ebx, ecx
.L92:
        add     r9d, ebx
        test    r10d, r10d
        je      .L100
.L102:
        cmp     BYTE PTR [rsp-13], 0
        mov     ebx, edx
        jne     .L103
.L99:
        mov     ebx, ebp
.L103:
        mov     r8d, DWORD PTR [rsp+88]
        add     edi, ebx
        test    r8d, r8d
        je      .L104
.L274:
        mov     ebx, r11d
        add     eax, r13d
        add     esi, edx
        sub     ebx, r12d
        cmp     r15d, eax
        jle     .L272
.L130:
        mov     r11d, ebx
        test    r10d, r10d
        jne     .L273

        ...

```

Thank you very much for your time and effort! We look forward to hearing from you.


---


### compiler : `gcc`
### title : `-Os gives significantly bigger code than -O0`
### open_at : `2023-09-25T00:33:36Z`
### last_modified_date : `2023-09-25T00:53:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111577
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
Yes, I saw #35806, #41175 and others.

See https://godbolt.org/z/Pnh89Y3Yb

```
#include <string>

using namespace std;

int main(int argc, char* argv[]) {
    if (argv[0] == nullptr || argv[1] == nullptr) return 0;

    string zxc(argv[0]);
    string qwe(argv[1]);
    string asd(argv[2]);

    zxc = qwe + asd;

    return zxc.size();
}
```

-Os -std=c++2b -march=skylake -m64 (615 bytes)

compare size with the smae, but with options:

-O0 -std=c++2b -march=skylake -m64 (409 bytes)

-O0 - is much LESS (!) in bytes. I think it's a bug.


---


### compiler : `gcc`
### title : `[arm-none-eabi-gcc] / suboptimal optimization / b.n to bx lr`
### open_at : `2023-09-25T05:09:23Z`
### last_modified_date : `2023-09-25T06:43:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111580
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
it seems like, i should have filed this separately:
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111500

i feel like gcc should output a "bx lr" instead of a jump ("b .L1") to a "bx lr"...

the file "a.i" is:
# 1 "a.c"
# 1 "/tmp//"
# 1 "<built-in>"
# 1 "<command-line>"
# 1 "a.c"
int artiBN2BX(int a, int b) {
 if (a<b) return a+b;
 return a*b;
}

this results in this machine code:
> arm-none-eabi-gcc -v
Using built-in specs.
COLLECT_GCC=arm-none-eabi-gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/arm-none-eabi/9.3.0/lto-wrapper
Target: arm-none-eabi
Configured with: ../configure --disable-decimal-float --disable-libffi --disable-libgomp --disable-libmudflap --disable-libquadmath --disable-libssp --disable-libstdcxx-pch --disable-libstdc__-v3 --disable-nls --disable-shared --disable-threads --disable-tls --disable-werror --enable-__cxa_atexit --enable-c99 --enable-gnu-indirect-function --enable-interwork --enable-languages=c,c++ --enable-long-long --enable-multilib --enable-plugins --host= --libdir=/usr/lib --libexecdir=/usr/lib --prefix=/usr --target=arm-none-eabi --with-gmp --with-gnu-as --with-gnu-ld --with-headers=/usr/arm-none-eabi/include --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' --with-isl --with-libelf --with-mpc --with-mpfr --with-multilib-list=rmprofile --with-native-system-header-dir=/include --with-newlib --with-python-dir=share/gcc-arm-none-eabi --with-sysroot=/usr/arm-none-eabi --with-system-zlib
Thread model: single
gcc version 9.3.0 (GCC)
> arm-none-eabi-gcc -save-temps -S a.c -O3 -g -mcpu=cortex-m0plus -mthumb -Wall --specs=nosys.specs -nostdlib -fdata-sections -ffunction-sections -ffreestanding -Winline
> cat a.s
artiBN2BX:
	cmp	r0, r1
	blt	.L5
	muls	r0, r1
.L1:	bx	lr
.L5:	adds	r0, r0, r1
	b	.L1


---


### compiler : `gcc`
### title : `[arm-none-eabi-gcc] / suboptimal optimization / uxth/sxth`
### open_at : `2023-09-25T05:16:02Z`
### last_modified_date : `2023-09-25T07:09:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111581
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
it seems like, i should have filed this separately:
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111500

i think the uxth/sxth is superfluous here,
since nobody ever looks at the high half word...

> arm-none-eabi-gcc -v
Using built-in specs.
COLLECT_GCC=arm-none-eabi-gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/arm-none-eabi/9.3.0/lto-wrapper
Target: arm-none-eabi
Configured with: ../configure --disable-decimal-float --disable-libffi --disable-libgomp --disable-libmudflap --disable-libquadmath --disable-libssp --disable-libstdcxx-pch --disable-libstdc__-v3 --disable-nls --disable-shared --disable-threads --disable-tls --disable-werror --enable-__cxa_atexit --enable-c99 --enable-gnu-indirect-function --enable-interwork --enable-languages=c,c++ --enable-long-long --enable-multilib --enable-plugins --host= --libdir=/usr/lib --libexecdir=/usr/lib --prefix=/usr --target=arm-none-eabi --with-gmp --with-gnu-as --with-gnu-ld --with-headers=/usr/arm-none-eabi/include --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' --with-isl --with-libelf --with-mpc --with-mpfr --with-multilib-list=rmprofile --with-native-system-header-dir=/include --with-newlib --with-python-dir=share/gcc-arm-none-eabi --with-sysroot=/usr/arm-none-eabi --with-system-zlib
Thread model: single
gcc version 9.3.0 (GCC) 
> arm-none-eabi-gcc -save-temps -S a.c -O3 -g -mcpu=cortex-m0plus -mthumb -Wall --specs=nosys.specs -nostdlib -fdata-sections -ffunction-sections -ffreestanding -Winline
> cat a.i
# 1 "a.c"
# 1 "/tmp//"
# 1 "<built-in>"
# 1 "<command-line>"
# 1 "a.c"
void artiSXTH(volatile short * a) {
 short b = *a;
 *a = b;
}
> cat a.s
artiSXTH:
        ldrh    r3, [r0]
        sxth    r3, r3
        strh    r3, [r0]
        bx      lr


---


### compiler : `gcc`
### title : `[arm-none-eabi-gcc] / suboptimal optimization / bitfield / superfluous stack write`
### open_at : `2023-09-25T06:22:23Z`
### last_modified_date : `2023-09-25T06:49:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111582
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.3.0`
### severity : `normal`
### contents :
When I try to use a struct with a bitfield,
then it happens, that GCC writes to the stack without ever reading it:

> arm-none-eabi-gcc -v
Using built-in specs.
COLLECT_GCC=arm-none-eabi-gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/arm-none-eabi/9.3.0/lto-wrapper
Target: arm-none-eabi
Configured with: ../configure --disable-decimal-float --disable-libffi --disable-libgomp --disable-libmudflap --disable-libquadmath --disable-libssp --disable-libstdcxx-pch --disable-libstdc__-v3 --disable-nls --disable-shared --disable-threads --disable-tls --disable-werror --enable-__cxa_atexit --enable-c99 --enable-gnu-indirect-function --enable-interwork --enable-languages=c,c++ --enable-long-long --enable-multilib --enable-plugins --host= --libdir=/usr/lib --libexecdir=/usr/lib --prefix=/usr --target=arm-none-eabi --with-gmp --with-gnu-as --with-gnu-ld --with-headers=/usr/arm-none-eabi/include --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' --with-isl --with-libelf --with-mpc --with-mpfr --with-multilib-list=rmprofile --with-native-system-header-dir=/include --with-newlib --with-python-dir=share/gcc-arm-none-eabi --with-sysroot=/usr/arm-none-eabi --with-system-zlib
Thread model: single
gcc version 9.3.0 (GCC)
# arm-none-eabi-gcc -save-temps -S a.c -O3 -g -mcpu=cortex-m0plus -mthumb -Wall --specs=nosys.specs -nostdlib -fdata-sections -ffunction-sections -ffreestanding -Winline
> cat a.i
# 1 "a.c"
# 1 "/tmp//"
# 1 "<built-in>"
# 1 "<command-line>"
# 1 "a.c"

typedef unsigned char u8;
typedef unsigned int u32;
extern int fatal();
__attribute__((always_inline)) inline u32 lsb(const u8 l) { return (1U<<l)-1U; }

typedef struct { u32 a; u32 msk; u32 v; u8 rs:1; u8 aw:7; } Reg;
__attribute__((always_inline)) inline Reg GI(u32 A, u32 N, u32 RS, u8 AW) { Reg R={A+4*N,lsb(32),0,RS?1:0,AW}; return R; }
__attribute__((always_inline)) inline u32 GS(Reg R) {
   for (u32 i=0, msk=lsb(8); R.aw==1 && msk; i++, msk<<=8)
      if ( !~(R.msk | msk) ) {
         const u8 v = R.v >> (i*8);
         if (R.rs || msk==~R.msk) return (((volatile u8*)R.a)[i] = v) << (i*8);
         else if (R.v==~R.msk) return (((volatile u8*)R.a)[i] |= v) << (i*8);
         return (((volatile u8*)R.a)[i] = (((volatile u8*)R.a)[i] & (R.msk>>(i*8))) | v) << (i*8);
      }
 return 0;
}
__attribute__((always_inline)) inline Reg GU(Reg R, u32 A, u32 N, u8 o, u8 w, u32 v) {
   const u32 msk=~(lsb(w)<<o); R.msk&=msk; R.v&=msk;
   R.v |= (v<<o);
   return R;
}
u32 artiSP() { return GS(GU(GI(0xE0000000,42,0,1),0xE0000000,42,17,3,2)); }
> cat a.s
artiSP:
        sub     sp, sp, #16
        mov     r2, sp
        movs    r3, #2
        strb    r3, [r2, #12]
...
        add     sp, sp, #16
        bx      lr

I compile it on a
Intel(R) Pentium(R) Silver J5040 CPU @ 2.00GHz
running Void Linux (kernel: 6.3.13_1)
for a STM32G030.


---


### compiler : `gcc`
### title : `[aarch64] Redundant movprfx with ptrue`
### open_at : `2023-09-25T07:37:01Z`
### last_modified_date : `2023-09-25T11:00:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111584
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
* test: https://gcc.godbolt.org/z/E6Eez81jh
```
#include<arm_sve.h>

typedef svfloat32_t fvec32 __attribute__((arm_sve_vector_bits(256)));

typedef svfloat32_t __m256_;

 __m256_ _mm256_mul_ps2_z(__m256_ a, __m256_ b)
{
     __m256_ res;
     res = svmul_f32_z(svptrue_b32(), a, b);
     return res;
}
```

* llvm have same output for _mm256_mul_ps2_x and  _mm256_mul_ps2_z, while gcc doesn't has high efficient output for _mm256_mul_ps2_z


---


### compiler : `gcc`
### title : `Use relaxed atomic increment (but not decrement!) in shared_ptr`
### open_at : `2023-09-25T10:10:25Z`
### last_modified_date : `2023-10-03T13:05:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111589
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `unknown`
### severity : `normal`
### contents :
The atomic increment when copying a shared_ptr can be relaxed because it is never actually used as a synchronization operation. The current thread must already have sufficient synchronization to access the memory because it can already deref the pointer. All synchronization is done either via whatever program-provided code makes the shared_ptr object available to the thread, or in the atomic decrement (where the decrements to non-zero are releases that ensure all uses of the object happen before the final decrement to zero acquires and destroys the object).

As an argument-from-authority, libc++ already is using relaxed for increments and acq/rel for decements: https://github.com/llvm/llvm-project/blob/c649fd34e928ad01951cbff298c5c44853dd41dd/libcxx/include/__memory/shared_ptr.h#L101-L121

This will have no impact on x86 where all atomic RMWs are effectively sequentially consistent, but it will enable the use of ldadd rather than ldaddal on aarch64, and similar optimizations on other weaker architectures.


---


### compiler : `gcc`
### title : `RISC-V: Failed to fold VEC_COND_EXPR and COND_LEN_ADD`
### open_at : `2023-09-26T02:31:44Z`
### last_modified_date : `2023-09-26T12:29:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111594
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `enhancement`
### contents :
Consider this following case:


#include <stdint.h>

void single_loop_with_if_condition(uint64_t * restrict a, 
uint64_t * restrict b,
int loop_size) {
  uint64_t result = 0;

  for (int i = 0; i < loop_size; i++) {
    if (b[i] <= a[i]) {
      result += a[i];
    }
  }

  a[0] = result;
}

In ARM SVE:

vect__ifc__33.15_48 = VEC_COND_EXPR <mask__18.14_46, vect__7.13_45, { 0, ... }>;
vect__34.16_49 = .COND_ADD (loop_mask_41, vect_result_19.7_38, vect__ifc__33.15_48, vect_result_19.7_38);

will be folded into:

vect__34.16_49 = .COND_ADD (_50, vect_result_19.7_38, vect__7.13_45, vect_result_19.7_38);

However, for RVV, if failed to fold VEC_COND_EXPR + COND_LEN_ADD.

vect__ifc__44.30_96 = VEC_COND_EXPR <mask__43.29_94, vect__42.28_93, { 0, ... }>;
  vect__45.31_97 = .COND_LEN_ADD ({ -1, ... }, vect_result_35.22_78, vect__ifc__44.30_96, vect_result_35.22_78, _104, 0);

I am not sure where to do this optimization?


---


### compiler : `gcc`
### title : `detection of MIN/MAX with truncation and sign change for the result`
### open_at : `2023-09-26T05:20:49Z`
### last_modified_date : `2023-09-26T05:20:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111595
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
unsigned short f(long a, long b)
{
        short as = a;
        short bs = b;
        unsigned short asu = a;
        unsigned short bsu = b;
        if (as < bs) return asu;
        return bsu;
}
unsigned short f0(long a, long b)
{
        short as = a;
        short bs = b;
        unsigned short asu = a;
        unsigned short bsu = b;
        if (as < bs) return as;
        return bs;
}

unsigned short f1(long a, long b)
{
        short as = a;
        short bs = b;
        unsigned short asu = a;
        unsigned short bsu = b;
        signed short t;
        if (as < bs) t = as;
        else t = bs;
        return t;
}
```

Currently only f1 detects MIN here. They all should produce the same IR in the end.


---


### compiler : `gcc`
### title : `A+cst does not overflow, "(T)(A+cst)" is suboptimal for ppc64`
### open_at : `2023-09-26T07:43:36Z`
### last_modified_date : `2023-09-26T09:29:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111597
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
In match.pd there is a pattern:
/* ((T)(A)) + CST -> (T)(A + CST)  */
#if GIMPLE
  (simplify
   (plus (convert:s SSA_NAME@0) INTEGER_CST@1)
    (if (TREE_CODE (TREE_TYPE (@0)) == INTEGER_TYPE
         && TREE_CODE (type) == INTEGER_TYPE
         && TYPE_PRECISION (type) > TYPE_PRECISION (TREE_TYPE (@0))
         && int_fits_type_p (@1, TREE_TYPE (@0)))
     /* Perform binary operation inside the cast if the constant fits
        and (A + CST)'s range does not overflow.  *

But this pattern seems not in favor of all targets. 
For example, the below code hits this pattern, 

long foo1 (int x)
{
  if (x>1000)
    return 0;
  int x1 = x +1;
  return (long) x1 + 40;                                                                          
}

Compile with "-O2 -S", on ppc64le, the generated asm is:
        cmpwi 0,3,1000
        bgt 0,.L3
        addi 3,3,41
        extsw 3,3 ;; this is suboptimal
        blr
	.p2align 4,,15
.L3:
    	li 3,0
	blr
--------------
But for the below code, the generated asm seems better: without 
long foo1 (int x)
{
  if (x>1000)
    return 0;
  return (long) x + 40;                                                                                                      
}

        cmpwi 0,3,1000
        bgt 0,.L3
        addi 3,3,40
        blr
        .p2align 4,,15
.L3:
        li 3,0
        blr


---


### compiler : `gcc`
### title : `GCC twice as slow as Clang for minisweep (SPEC HPC 2021)`
### open_at : `2023-09-27T12:50:35Z`
### last_modified_date : `2023-09-30T08:04:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111612
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
The discussion came out during this year's GNU Tools Cauldron during the OpenMP/OpenACC/offloading talks, i.e.
https://gcc.gnu.org/wiki/cauldron2023#cauldron2023talks.openacc_openmp_offloading_and_gcc

In that talk, using MPI with 8 ranks gave the following
(--define model=mpi --ranks 8):

3855 s (~1.071 h) - Nvidia HPC SDK  23.5 (May 2023): 
4076 s (~1.132 h) - LLVM 17 (pre) commit 34cf263e6 (2023-08-07):
4900 s (~1.361 h) or/up to 6624 s (~1.840 h) - GCC og13 commit b003e6511 (2023-07-19)

* * *

I just tried it myself as follows - using the non SPEC-ified version
and a modified input from how-to-run readme. I have not checked whether there are any gotchas, but it should be identical and without OpenMP, MPI or similar.

Namely:

  git clone https://github.com/wdj/minisweep.git
  cmake -DCMAKE_C_FLAGS=-O2 -DCMAKE_C_COMPILER=/usr/bin/clang-14 ../..

And likewise for GCC mainline, also with -O2.

Running then:
time ./sweep --ncell_x  4 --ncell_y 8 --ncell_z 32

GCC mainline:
Normsq result: 2.82234163e+12  diff: 0.000e+00  PASS  time: 7.817  GF/s: 0.315
real    0m8,124s / user    0m7,943s / sys     0m0,180s

Clang/LLVM-14:
Normsq result: 2.82234163e+12  diff: 0.000e+00  PASS  time: 3.036  GF/s: 0.812
real    0m3,223s / user    0m3,085s / sys     0m0,137s


Using -O3 -flto, I get: 2.070s (GCC) vs. 1.053s (Clang/LLVM)


---


### compiler : `gcc`
### title : `missed optimization combining offset of array member in struct with offset inside the array`
### open_at : `2023-09-28T10:45:28Z`
### last_modified_date : `2023-09-28T19:55:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111626
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
The simple code:

struct some_struct {
    uint32_t some_member;
    uint32_t arr[4][16];
};

uint32_t fn(const struct some_struct *arr, int idx)
{
    return arr->arr[1][idx];
}

is used to showcase a suboptimal optimization on some platforms including
RISC-V and MIPS (32 & 64 bit) even with `some_member` commented out.

while GCC emits:
        addi    a1,a1,16
        slli    a1,a1,2
        add     a0,a0,a1
        lw      a0,4(a0)
        ret

Clang does better job:
        slli    a1, a1, 2
        add     a0, a0, a1
        lw      a0, 68(a0)
        ret


---


### compiler : `gcc`
### title : `Optimizing for size compiles additions to several instructions`
### open_at : `2023-09-28T13:40:40Z`
### last_modified_date : `2023-09-29T00:09:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111630
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `12.3.0`
### severity : `normal`
### contents :
Compiled example where version, options, compiler outputs etc. are specified: https://godbolt.org/z/6q9P33qf3

We are upgrading from ARM GCC 7.3.0 to 12.3.0 and are experiencing slower code with the same options.

When adding offsets to pointers inside a loop, GCC 7.3.0 simply identifies this a two add instructions (section .L3 in the assembly). The same code with GCC 12.3.0 (and back until at least 9.2.1) creates quite a few instructions in the same section.

The issue only seems to appear when -Os is selected. Choosing e.g. -O2 results in simpler assembly with additions.


---


### compiler : `gcc`
### title : `Memory copy with structure assignment from named address space should be improved`
### open_at : `2023-10-01T21:41:29Z`
### last_modified_date : `2023-10-05T15:46:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111657
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.3.1`
### severity : `normal`
### contents :
Taken from [1]. Compile the following testcase with -O2 -mno-sse:

--cut here--
struct a
{
  long arr[30];
};

__seg_gs struct a m;

void
foo (struct a *dst)
{
  *dst = m;
}
--cut here--

the produced assembly:

foo:
.LFB0:
        xorl    %eax, %eax
        cmpq    $240, %rax
        jnb     .L5
.L2:
        movzbl  %gs:m(%rax), %edx
        movb    %dl, (%rdi,%rax)
        addq    $1, %rax
        cmpq    $240, %rax
        jb      .L2
.L5:
        ret

As rightfully said in [1]:

"...and look at the end result. It's complete and utter sh*t:

<...>

to the point that I can only go "WTF"?

I mean, it's not just that it does the copy one byte at a time. It
literally compares %rax to $240 just after it has cleared it. I look
at that code, and I go "a five-year old with a crayon could have done
better".

[1] https://lore.kernel.org/lkml/CAHk-=wh+cfn58XxMLnG6dH+Eb9-2dYfABXJF2FtSZ+vfqVvWzA@mail.gmail.com/


---


### compiler : `gcc`
### title : `assign_hard_reg() routine should scale save/restore costs of callee save registers with basic block frequency`
### open_at : `2023-10-03T10:05:57Z`
### last_modified_date : `2023-10-03T10:28:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111673
### status : `UNCONFIRMED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `unknown`
### severity : `normal`
### contents :
In assign_hard_reg(), when computing the costs of the hard registers, the cost of saving/restoring a callee-save hard register in epilogue/prologue is taken into consideration. However, this cost is not scaled with the entry block frequency. Without scaling, the cost of saving/restoring is quite small and this can result in a callee-save register being chosen by assign_hard_reg() even though there are free caller-save(volatile) registers available. 

Consider the following test:

int f(int);

int advance(int dz)
{
    if (dz > 0)
        return (dz + dz) * dz;
    else
        return dz * f(dz);
}


Input RTL to IRA pass:

  set r127, r3
  set r121, r127
  set r122, compare(r121, 0)
  if (r122 le 0) jump BB4 else jump BB3

BB3:
  set r123, r121*r121
  set r119, r123<<1
  jump BB5

BB4:
  set r3, call f(r3)
  set r128, r3
  set r119, r128*r121

BB5:
  set r3, r119
  return r3


When assign_hard_reg() is called for allocno r121, the cost for r31 is 0 (obtained from ALLOCNO_UPDATED_HARD_REG_COSTS). Since r31 on PowerPC is a callee save register, we compute the cost for saving/restoring r31 in prolog/epilog and this cost is 7. So the final cost for r31 is 7. And r31 is assigned to allocno r121 since it has the lowest cost among the profitable registers.
However, among profitable registers for allocno r121, there are caller save registers (like r9) that could possibly be assigned to allocno r121. r9 has a cost of 2040 (obtained from ALLOCNO_UPDATED_HARD_REG_COSTS). So it is not chosen as cost of r31 is lesser.
But computation of save/restore costs for r31 is incorrect as it doesn’t take into consideration the frequency of the basic blocks in which the save/restore instructions will be placed. If the frequency is taken into consideration, then cost of r31 is 7000 (frequency of entry bb is 1000). And this would result in r9 being assigned to r121.

Since r31 is assigned to allocno r121, the above test does not get shrink wrapped.


---


### compiler : `gcc`
### title : ``(~a) | (a ^ b)` is not simplified to `~(a & b)``
### open_at : `2023-10-03T16:11:40Z`
### last_modified_date : `2023-10-10T17:29:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111679
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(int a, int b)
{
        return a | ((~a) ^ b);
}

int f1(int a, int b)
{
        return (~a) | (a ^ b); // ~(a & b) or (~a) | (~b)
}
```

The only difference between these 2 functions is a is bitwise inversed. f is able to be detected and simplified but not f1.

I Noticed this while working on PR 111282.


---


### compiler : `gcc`
### title : `Sub optimal code gen for initialising vector using loop`
### open_at : `2023-10-04T19:25:36Z`
### last_modified_date : `2023-10-05T07:48:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111697
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Hi,
For the following test-case:

typedef int v4si __attribute__((vector_size (sizeof (int) * 4)));
v4si f(int x)
{
  v4si v;
  for (int i = 0; i < 4; i++)
    v[i] = x;
  return v;
}

Compiling with -O2 results in following .optimized dump:

v4si f (int x)
{
  v4si v;

  <bb 2> [local count: 214748368]:
  v_16 = BIT_INSERT_EXPR <v_12(D), x_6(D), 0 (32 bits)>;
  v_20 = BIT_INSERT_EXPR <v_16, x_6(D), 32 (32 bits)>;
  v_24 = BIT_INSERT_EXPR <v_20, x_6(D), 64 (32 bits)>;
  v_2 = BIT_INSERT_EXPR <v_24, x_6(D), 96 (32 bits)>;
  return v_2;

}

and following code-gen on aarch64:
f:
        movi    v0.4s, 0
        fmov    s31, w0
        ins     v0.s[0], v31.s[0]
        ins     v0.s[1], v31.s[0]
        ins     v0.s[2], v31.s[0]
        ins     v0.s[3], v31.s[0]
        ret

which could instead be a single dup instruction:
f:
        dup     v0.4s, w0
        ret

Similarly, code-gen on x86_64:
f:
        movd    %edi, %xmm0
        movd    %edi, %xmm1
        pshufd  $225, %xmm0, %xmm0
        movss   %xmm1, %xmm0
        pshufd  $225, %xmm0, %xmm0
        pshufd  $198, %xmm0, %xmm0
        movss   %xmm1, %xmm0
        pshufd  $198, %xmm0, %xmm0
        pshufd  $39, %xmm0, %xmm0
        movss   %xmm1, %xmm0
        pshufd  $39, %xmm0, %xmm0
        ret


---


### compiler : `gcc`
### title : `Narrow memory access of compare to byte width`
### open_at : `2023-10-04T20:52:34Z`
### last_modified_date : `2023-10-25T14:30:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111698
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `12.3.1`
### severity : `normal`
### contents :
Following testcase:

--cut here--
int m;

_Bool foo (void)
{
  return m & 0x0f0000;
}
--cut here--

compiles to:

  0:   f7 05 00 00 00 00 00    testl  $0xf0000,0x0(%rip)
  7:   00 0f 00 

The test instruction can be demoted to byte test from addr+2.

Currently, the demotion works for lowest byte, so the testcase:

--cut here--
int m;

_Bool foo (void)
{
  return m & 0x0f;
}
--cut here--

compiles to:

   0:   f6 05 00 00 00 00 0f    testb  $0xf,0x0(%rip)

which is three bytes shorter.

Any half-way modern Intel and AMD cores will forward any fully contained load, so there is no danger of forwarding stall with recent CPU cores.


---


### compiler : `gcc`
### title : `Missed optimization in FRE because of weak TBAA`
### open_at : `2023-10-06T15:06:51Z`
### last_modified_date : `2023-10-10T06:58:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111715
### status : `RESOLVED`
### tags : `alias, diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Jakub reports a spurious diagnostic with tree-affine.cc with the wide-int patches:

> ./cc1plus  -quiet -O2 -Wall -fno-exceptions /tmp/t.ii
In file included from ../../gcc/coretypes.h:465,
                 from ../../gcc/tree-affine.cc:22:
In member function 'void widest_int_storage<N>::set_len(unsigned int, bool) [with int N = 576]',
    inlined from 'void wi::copy(T1&, const T2&) [with T1 = widest_int_storage<576>; T2 = generic_wide_int<wide_int_ref_storage<true, true> >]' at ../../gcc/wide-int.h:2407:13,
    inlined from 'widest_int_storage<N>& widest_int_storage<N>::operator=(const T&) [with T = int; int N = 576]' at ../../gcc/wide-int.h:1805:12,
    inlined from 'generic_wide_int<storage>& generic_wide_int<T>::operator=(const T&) [with T = int; storage = widest_int_storage<576>]' at ../../gcc/wide-int.h:1018:23,
    inlined from 'void aff_combination_remove_elt(aff_tree*, unsigned int)' at ../../gcc/tree-affine.cc:596:34:
../../gcc/wide-int.h:1856:14: warning: 'void* memcpy(void*, const void*, size_t)' offset [0, 7] is out of the bounds [0, 0] [-Warray-bounds=]
../../gcc/wide-int.h:1857:12: warning: 'void free(void*)' called on a pointer to an unallocated object '1' [-Wfree-nonheap-object]
In member function 'void widest_int_storage<N>::set_len(unsigned int, bool) [with int N = 576]',
    inlined from 'void wi::copy(T1&, const T2&) [with T1 = widest_int_storage<576>; T2 = generic_wide_int<wide_int_ref_storage<true, true> >]' at ../../gcc/wide-int.h:2407:13,
    inlined from 'widest_int_storage<N>& widest_int_storage<N>::operator=(const T&) [with T = int; int N = 576]' at ../../gcc/wide-int.h:1805:12,
    inlined from 'generic_wide_int<storage>& generic_wide_int<T>::operator=(const T&) [with T = int; storage = widest_int_storage<576>]' at ../../gcc/wide-int.h:1018:23,
    inlined from 'void aff_combination_convert(aff_tree*, tree)' at ../../gcc/tree-affine.cc:256:34:
../../gcc/wide-int.h:1856:14: warning: 'void* memcpy(void*, const void*, size_t)' offset [0, 7] is out of the bounds [0, 0] [-Warray-bounds=]
../../gcc/wide-int.h:1857:12: warning: 'void free(void*)' called on a pointer to an unallocated object '1' [-Wfree-nonheap-object]

that's caused by failing to CSE in

  MEM <struct aff_tree> [(struct widest_int_storage *)comb_13(D)].elts[_6].coef.D.132464.len = 1;
  _49 = (sizetype) _6;
  _50 = _49 * 88;
  _101 = _50 + 104;
  _74 = comb_13(D) + _101;
  *_74 = 1;
  _80 = MEM <struct aff_tree> [(struct widest_int_storage *)comb_13(D)].elts[_6].coef.D.132464.len;

where the view-convert pun in the MEM_REF makes it get the alias set of
wide-int storage which conflicts with the *_74 store of a 'long'.  We
fail to pick up the store to an 'int' (the len) and possible disambiguation
with TBAA.

That's somewhat by design since we are faced with a lot of address-taking
and dereferencing of pointers as part of C++ abstraction.  But the
"tail" is still a valid access.


---


### compiler : `gcc`
### title : `Missed optimization of '(a+a)/a'`
### open_at : `2023-10-07T11:05:10Z`
### last_modified_date : `2023-10-08T04:00:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111718
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Hello, we found some optimizations (regarding Arithmetic optimization) that GCC may have missed. We would greatly appreicate if you can take a look and let us know what you think.

Given the following code: 
https://godbolt.org/z/5de17zvz9

unsigned n1,n2;
void func1(unsigned a){
    if(a>10&&a<20){
        n1=a+a;
        n2=(a+a)/a;
    }
}

We note that `(a+a)/a` should be optimized to `2`, but gcc-trunk -O3 does not:
func1(unsigned int):
        lea     eax, [rdi-11]
        cmp     eax, 8
        ja      .L1
        lea     eax, [rdi+rdi]
        xor     edx, edx
        mov     DWORD PTR n1[rip], eax
        div     edi
        mov     DWORD PTR n2[rip], eax
.L1:
        ret


Thank you very much for your time and effort! We look forward to hearing from you.


---


### compiler : `gcc`
### title : `RISC-V: Ugly codegen in RVV`
### open_at : `2023-10-07T22:28:22Z`
### last_modified_date : `2023-10-19T13:30:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111720
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Reference: https://godbolt.org/z/YqW7Y5Yve

#include<riscv_vector.h>
vbool8_t fn() {

    uint8_t arr[32] = {1, 2, 7, 1, 3, 4, 5, 3, 1, 0, 1, 2, 4, 4, 9, 9, 1, 2, 7, 1, 3, 4, 5, 3, 1, 0, 1, 2, 4, 4, 9, 9};
    uint8_t m = 1;

    vuint8m1_t varr = __riscv_vle8_v_u8m1(arr, 32);
    vuint8m1_t vand_m = __riscv_vand_vx_u8m1(varr, m, 32);
    vbool8_t vmask = __riscv_vreinterpret_v_u8m1_b8(vand_m);

    return vmask;
}

GCC asm:

fn:
        lui     a5,%hi(.LANCHOR0)
        addi    sp,sp,-32
        vsetivli        zero,4,e64,m2,ta,ma
        addi    a5,a5,%lo(.LANCHOR0)
        li      a4,32
        vle64.v v2,0(a5)
        vse64.v v2,0(sp)
        vsetvli zero,a4,e8,m1,ta,ma
        vle8.v  v1,0(sp)
        vand.vi v1,v1,1
        vsetvli a5,zero,e8,m1,ta,ma
        vsm.v   v1,0(a0)
        addi    sp,sp,32
        jr      ra

LLVM ASM:

fn:                                     # @fn
.Lpcrel_hi0:
        auipc   a0, %pcrel_hi(.L__const.fn.arr)
        addi    a0, a0, %pcrel_lo(.Lpcrel_hi0)
        li      a1, 32
        vsetvli zero, a1, e8, m1, ta, ma
        vle8.v  v8, (a0)
        vand.vi v0, v8, 1
        ret
.L__const.fn.arr:
        .ascii  "\001\002\007\001\003\004\005\003\001\000\001\002\004\004\t\t\001\002\007\001\003\004\005\003\001\000\001\002\004\004\t\t"


---


### compiler : `gcc`
### title : `RISC-V: Failed to SLP for gather_load in RVV`
### open_at : `2023-10-07T23:18:05Z`
### last_modified_date : `2023-10-09T07:24:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111721
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/d5TPa5e5s

void __attribute__((noipa))
f (int *restrict y, int *restrict x, int *restrict indices, int n)
{
  for (int i = 0; i < n; ++i)
    {
      y[i * 2] = x[indices[i * 2]] + 1;
      y[i * 2 + 1] = x[indices[i * 2 + 1]] + 2;
    }
}

RVV ASM:

f:
        ble     a3,zero,.L5
.L3:
        vsetvli a5,a3,e32,m1,ta,ma
        vlseg2e32.v     v2,(a2)                 ----> VEC_LOAD_LANES
        vsetivli        zero,4,e32,m1,ta,ma
        vsll.vi v4,v2,2
        vsll.vi v1,v3,2
        vsetvli zero,a5,e32,m1,ta,ma
        vluxei32.v      v4,(a1),v4
        vluxei32.v      v1,(a1),v1
        vsetivli        zero,4,e32,m1,ta,ma
        slli    a4,a5,3
        vadd.vi v2,v4,1
        vadd.vi v3,v1,2
        sub     a3,a3,a5
        vsetvli zero,a5,e32,m1,ta,ma
        vsseg2e32.v     v2,(a0)                  ----> VEC_STORE_LANES
        add     a2,a2,a4
        add     a0,a0,a4
        bne     a3,zero,.L3
.L5:
        ret

Comparing to aarch64 which can SLP, RVV geneates expensive load_lanes/store_lanes.

This is because RVV is using MASK_LEN_GATHER_LOAD that we currently can didn't support SLP for it.


---


### compiler : `gcc`
### title : `[Regression] Missed optimizations probably because of too early arithmetic optimization`
### open_at : `2023-10-08T07:55:46Z`
### last_modified_date : `2023-10-09T10:34:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111724
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Hello, we found some optimizations (probably because of too early arithmetic optimization) that GCC may have missed. We would greatly appreicate if you can take a look and let us know what you think.

Given the following code:
https://godbolt.org/z/8EW8fx78K
int n;
void func(int w, int a, int b){
    for(int i=0;i<w;i++){
        n += (a+a)+(b+b);
        b += b;
    }
}

`a+a` is a loop invariant. But gcc-trunk -O3:
func(int, int, int):
        push    {r4, lr}
        subs    lr, r0, #0
        ble     .L1
        movw    r4, #:lower16:.LANCHOR0
        movt    r4, #:upper16:.LANCHOR0
        mov     ip, #0
        ldr     r0, [r4]
.L3:
        adds    r3, r1, r2
        add     ip, ip, #1
        lsls    r2, r2, #1
        cmp     lr, ip
        add     r0, r0, r3, lsl #1
        bne     .L3
        str     r0, [r4]

From original(tree):
   (void) (n = (a + b) * 2 + n) >>>>>;

This leads to:
  _1 = a_11(D) + b_18;
  _2 = _1 * 2;
  _4 = _2 + n_lsm.4_7;

So, it looks like the missed LICM is due to too early arithmetic optimization.

On gcc-6.4, it works as expected.

We found that this also affects other optimizations, such as common subexpression elimination. We can provide examples if required.

Thank you very much for your time and effort! We look forward to hearing from you.


---


### compiler : `gcc`
### title : `Emit inline SVE FSCALE instruction for ldexp`
### open_at : `2023-10-09T09:53:25Z`
### last_modified_date : `2023-10-09T09:53:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111733
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Having noticed https://github.com/llvm/llvm-project/pull/67552 in LLVM GCC should be able to emit the SVE fscale instruction [1] to implement the ldexp standard function.

There is already an ldexpm3 optab defined so it should be a relatively simple matter of wiring up the expander for TARGET_SVE

[1] https://developer.arm.com/documentation/ddi0596/2021-12/SVE-Instructions/FSCALE--Floating-point-adjust-exponent-by-vector--predicated--?lang=en


---


### compiler : `gcc`
### title : `shifts in bit field accesses don't combine with other shifts`
### open_at : `2023-10-09T18:57:03Z`
### last_modified_date : `2023-10-10T08:47:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111743
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.1.0`
### severity : `enhancement`
### contents :
(not sure it's the middle-end, picked arbitrarily)

The following code

struct bf { 
        unsigned a : 10, b : 20, c : 10;
};
unsigned fbc(struct bf bf) { return bf.b | (bf.c << 20); }


generates:

        movq    %rdi, %rax
        shrq    $10, %rdi
        shrq    $32, %rax               
        andl    $1048575, %edi
        andl    $1023, %eax
        sall    $20, %eax
        orl     %edi, %eax
        ret

It doesn't understand that the shift right can be combined with the shift left. Also not sure why the shift left is arithmetic (this should be all unsigned) 

clang does the simplification which ends up one instruction shorter:
        movl    %edi, %eax
        shrl    $10, %eax
        andl    $1048575, %eax                  # imm = 0xFFFFF
        shrq    $12, %rdi
        andl    $1072693248, %edi               # imm = 0x3FF00000
        orl     %edi, %eax
        retq


---


### compiler : `gcc`
### title : `RISC-V: RVV unexpected vectorization`
### open_at : `2023-10-10T03:19:01Z`
### last_modified_date : `2023-10-10T12:47:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111751
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdlib.h>

#define N 16

int main ()
{
  int i;
  char ia[N];
  char ic[N] = {0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45};
  char ib[N] = {0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45};

  /* Not vectorizable, multiplication */
  for (i = 0; i < N; i++)
    {
      ia[i] = ib[i] * ic[i];
    }

  /* check results:  */
  for (i = 0; i < N; i++)
    {
      if (ia[i] != (char) (ib[i] * ic[i]))
        abort ();
    }

  return 0;
}

RVV GCC ASM:

main:
        lui     a5,%hi(.LANCHOR0)
        addi    a5,a5,%lo(.LANCHOR0)
        addi    sp,sp,-48
        ld      a4,0(a5)
        ld      a5,8(a5)
        sd      a5,8(sp)
        sd      a5,24(sp)
        sd      ra,40(sp)
        addi    a5,sp,16
        sd      a4,0(sp)
        sd      a4,16(sp)
        vsetivli        zero,16,e8,m1,ta,ma
        vle8.v  v1,0(a5)
        vle8.v  v2,0(sp)
        vmul.vv v1,v1,v2
        vmv.x.s a5,v1
        andi    a5,a5,0xff
        bne     a5,zero,.L2
        vslidedown.vi   v2,v1,1
        li      a4,9
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,2
        li      a4,36
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,3
        li      a4,81
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,4
        li      a4,144
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,5
        li      a4,225
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,6
        li      a4,68
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,7
        li      a4,185
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,8
        li      a4,64
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,9
        li      a4,217
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,10
        li      a4,132
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,11
        li      a4,65
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,12
        li      a4,16
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,13
        li      a4,241
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v2,v1,14
        li      a4,228
        vmv.x.s a5,v2
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        vslidedown.vi   v1,v1,15
        li      a4,233
        vmv.x.s a5,v1
        andi    a5,a5,0xff
        bne     a5,a4,.L2
        ld      ra,40(sp)
        li      a0,0
        addi    sp,sp,48
        jr      ra
.L2:
        call    abort


ARM SVE GCC:

main:
        mov     w0, 0
        ret


---


### compiler : `gcc`
### title : `-Wfree-nonheap-object (vec.h:347:10: warning: 'free' called on unallocated object 'dest_bbs') during bootstrap with LTO`
### open_at : `2023-10-10T04:03:08Z`
### last_modified_date : `2023-10-10T09:16:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111752
### status : `UNCONFIRMED`
### tags : `diagnostic, missed-optimization`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
I'm not sure this was always there - I think I would've noticed if it was a long-standing thing. I get this -Wfree-nonheap-object warning during bootstrap.

I can reproduce it with:
```
./configure --disable-analyzer --disable-bootstrap --disable-cet --disable-default-pie --disable-default-ssp --disable-fixincludes --disable-gcov --disable-libada --disable-libatomic --disable-libgomp --disable-libitm --disable-libquadmath --disable-libsanitizer --disable-libssp --disable-libstdcxx-pch --disable-libvtv --disable-lto --disable-multilib --disable-nls --disable-objc-gc --disable-systemtap --disable-werror --enable-languages=c,c++ --prefix=/tmp/bisect --without-isl --without-zstd --with-system-zlib --enable-bootstrap --enable-lto
make BUILD_CONFIG=bootstrap-lto -j$(nproc)
```

I can only reproduce when building with bootstrap-lto.

On trunk at r14-4523-gfb124f2a23e92b, I get this:
```
/home/sam/git/gcc/host-x86_64-pc-linux-gnu/prev-gcc/xg++ -B/home/sam/git/gcc/host-x86_64-pc-linux-gnu/prev-gcc/ -B/tmp/bisect/x86_64-pc-linux-gnu/bin/ -nostdinc++ -B/home/sam/git/gcc/prev-x86_64-pc-linux-gnu/
libstdc++-v3/src/.libs -B/home/sam/git/gcc/prev-x86_64-pc-linux-gnu/libstdc++-v3/libsupc++/.libs  -I/home/sam/git/gcc/prev-x86_64-pc-linux-gnu/libstdc++-v3/include/x86_64-pc-linux-gnu  -I/home/sam/git/gcc/pre
v-x86_64-pc-linux-gnu/libstdc++-v3/include  -I/home/sam/git/gcc/libstdc++-v3/libsupc++ -L/home/sam/git/gcc/prev-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs -L/home/sam/git/gcc/prev-x86_64-pc-linux-gnu/libstdc+
+-v3/libsupc++/.libs -no-pie   -g -O2 -fno-checking -flto=jobserver -frandom-seed=1 -DIN_GCC    -fno-exceptions -fno-rtti -fasynchronous-unwind-tables -W -Wall -Wno-narrowing -Wwrite-strings -Wcast-qual -Wmis
sing-format-attribute -Wconditionally-supported -Woverloaded-virtual -pedantic -Wno-long-long -Wno-variadic-macros -Wno-overlength-strings -fno-common  -DHAVE_CONFIG_H -no-pie -static-libstdc++ -static-libgcc
  -o cc1plus \
      cp/cp-lang.o c-family/stub-objc.o cp/call.o cp/class.o cp/constexpr.o cp/constraint.o cp/coroutines.o cp/cp-gimplify.o cp/cp-objcp-common.o cp/cp-ubsan.o cp/cvt.o cp/contracts.o cp/cxx-pretty-print.o cp
/decl.o cp/decl2.o cp/dump.o cp/error.o cp/except.o cp/expr.o cp/friend.o cp/init.o cp/lambda.o cp/lex.o cp/logic.o cp/mangle.o cp/mapper-client.o cp/mapper-resolver.o cp/method.o cp/module.o cp/name-lookup.o
 cp/optimize.o cp/parser.o cp/pt.o cp/ptree.o cp/rtti.o cp/search.o cp/semantics.o cp/tree.o cp/typeck.o cp/typeck2.o cp/vtable-class-hierarchy.o attribs.o c-family/c-common.o c-family/c-cppbuiltin.o c-family
/c-dump.o c-family/c-format.o c-family/c-gimplify.o c-family/c-indentation.o c-family/c-lex.o c-family/c-omp.o c-family/c-opts.o c-family/c-pch.o c-family/c-ppoutput.o c-family/c-pragma.o c-family/c-pretty-pr
int.o c-family/c-semantics.o c-family/c-ada-spec.o c-family/c-ubsan.o c-family/known-headers.o c-family/c-attribs.o c-family/c-warn.o c-family/c-spellcheck.o i386-c.o glibc-c.o cc1plus-checksum.o libbackend.a
 main.o libcommon-target.a libcommon.a ../libcpp/libcpp.a ../libdecnumber/libdecnumber.a ../libcody/libcody.a  \
        libcommon.a ../libcpp/libcpp.a   ../libbacktrace/.libs/libbacktrace.a ../libiberty/libiberty.a ../libdecnumber/libdecnumber.a   -lmpc -lmpfr -lgmp -rdynamic  -lz
../.././gcc/spellcheck.cc: In function '_Z17get_edit_distancePKciS0_i.part.0':
../.././gcc/spellcheck.cc:71:61: warning: argument 1 value '18446744073709551615' exceeds maximum object size 9223372036854775807 [-Walloc-size-larger-than=]
   71 |   edit_distance_t *v_two_ago = new edit_distance_t[len_s + 1];
      |                                                             ^
/home/sam/git/gcc/libstdc++-v3/libsupc++/new:133:26: note: in a call to allocation function 'operator new []' declared here
  133 | _GLIBCXX_NODISCARD void* operator new[](std::size_t) _GLIBCXX_THROW (std::bad_alloc)
      |                          ^
../.././gcc/spellcheck.cc:72:61: warning: argument 1 value '18446744073709551615' exceeds maximum object size 9223372036854775807 [-Walloc-size-larger-than=]
   72 |   edit_distance_t *v_one_ago = new edit_distance_t[len_s + 1];
      |                                                             ^
/home/sam/git/gcc/libstdc++-v3/libsupc++/new:133:26: note: in a call to allocation function 'operator new []' declared here
  133 | _GLIBCXX_NODISCARD void* operator new[](std::size_t) _GLIBCXX_THROW (std::bad_alloc)
      |                          ^
../.././gcc/spellcheck.cc:73:58: warning: argument 1 value '18446744073709551615' exceeds maximum object size 9223372036854775807 [-Walloc-size-larger-than=]
   73 |   edit_distance_t *v_next = new edit_distance_t[len_s + 1];
      |                                                          ^
/home/sam/git/gcc/libstdc++-v3/libsupc++/new:133:26: note: in a call to allocation function 'operator new []' declared here
  133 | _GLIBCXX_NODISCARD void* operator new[](std::size_t) _GLIBCXX_THROW (std::bad_alloc)
In function 'release',
    inlined from 'release' at ../.././gcc/vec.h:2027:20,
    inlined from '__dt_base ' at ../.././gcc/vec.h:1686:19,
    inlined from 'can_be_handled' at ../.././gcc/tree-switch-conversion.cc:1512:1:
../.././gcc/vec.h:347:10: warning: 'free' called on unallocated object 'dest_bbs' [-Wfree-nonheap-object]
  347 |   ::free (v);
      |          ^
../.././gcc/tree-switch-conversion.cc: In function 'can_be_handled':
../.././gcc/tree-switch-conversion.cc:1484:39: note: declared here
 1484 |   auto_vec<int, m_max_case_bit_tests> dest_bbs;
      |                                       ^
[...]
```


---


### compiler : `gcc`
### title : ``(a & ~1) | 2` could be done as `(a & ~(1 | 2)) + 2` which allows to use leal`
### open_at : `2023-10-10T19:32:37Z`
### last_modified_date : `2023-10-10T19:32:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111763
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f1(int in) {
  in = (in & ~(unsigned long)1);
  in = in | 2;
  return in;
}


int f2(int in) {
  in = (in & ~(unsigned long)(1|2));
  in = in + 2;
  return in;
}
```

We currently get:
```
f1:
        movl    %edi, %eax
        andl    $-2, %eax
        orl     $2, %eax
        ret
f2:
        andl    $-4, %edi
        leal    2(%rdi), %eax
        ret
```

The leal version is better because it saves more move due to leal not being a 2 operand but 3 operand instruction so it could improve register allocation ...

I noticed this whole looking into PR 111762 (and PR 111282) and looking at clang/LLVM's code generation here .

Also I don't know how often this shows up though.


---


### compiler : `gcc`
### title : `RISC-V: Faild to vectorize gen-vect-34.c`
### open_at : `2023-10-10T22:38:40Z`
### last_modified_date : `2023-10-11T08:51:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111765
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/17qYYKWPM

float summul(int n, float *arg1, float *arg2)
{                                                  
    int i;                                             
    float res1 = 1.0;
    for(i = 0; i < n; i++) {
      if(arg2[i]) 
        res1 *= arg1[i];
    }                                                  
    return res1;                                       
}

I found both ARM SVE and RVV failed to vectorize this case wheras X86 can vectorize it with -mavx2.

It seems that we need some tricks on both SVE and RVV backend ?


---


### compiler : `gcc`
### title : `Missed optimization with __builtin_unreachable and ands`
### open_at : `2023-10-11T01:55:13Z`
### last_modified_date : `2023-10-21T00:16:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111766
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int
foo3n(int c, int bb)
{
  if ((bb & ~3)!=0) __builtin_unreachable(); // bb = [0,3]
  if ((bb & 1)==0) __builtin_unreachable(); // bb&1 == 0 // [0],[3]
  if(bb == 2) __builtin_trap();
  return bb;
}
```

The condition `bb == 2` is never true as (bb&1) has to be zero.

I Noticed this while looking into PR 111432.  Note clang/LLVM is able to optimize this ...


---


### compiler : `gcc`
### title : `predicated loads inactive lane values not modelled`
### open_at : `2023-10-11T12:38:18Z`
### last_modified_date : `2023-10-12T07:24:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111770
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
For this example:

int foo(int n, char *a, char *b) {
  int sum = 0;
  for (int i = 0; i < n; ++i) {
    sum += a[i] * b[i];
  }
  return sum;
}
 
we generate with -O3 -march=armv8-a+sve

.L3:
        ld1b    z29.b, p7/z, [x1, x3]
        ld1b    z31.b, p7/z, [x2, x3]
        add     x3, x3, x4
        sel     z31.b, p7, z31.b, z28.b
        whilelo p7.b, w3, w0
        udot    z30.s, z29.b, z31.b
        b.any   .L3
        uaddv   d30, p6, z30.s
        fmov    w0, s30
        ret

Which is pretty good, but we completely ruin it with the SEL.

In gimple this is:

  vect__7.12_81 = .MASK_LOAD (_21, 8B, loop_mask_77);
  masked_op1_82 = .VCOND_MASK (loop_mask_77, vect__7.12_81, { 0, ... });
  vect_patt_33.13_83 = DOT_PROD_EXPR <vect__3.9_78, masked_op1_82, vect_sum_19.6_74>;

The missed optimization here is that we don't model what happens with predicated operations that zero inactive lanes.

i.e. in this case .MASK_LOAD will zero the unactive lanes, so the .VCOND_MASK is  completely superfluous.

I'm not entirely sure how we should go about fixing this generally.


---


### compiler : `gcc`
### title : `boringssl performance gap between clang and gcc for x25519 operations`
### open_at : `2023-10-11T19:33:23Z`
### last_modified_date : `2023-10-11T20:02:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111774
### status : `WAITING`
### tags : `missed-optimization`
### component : `target`
### version : `13.2.0`
### severity : `normal`
### contents :
Hi folks,

I've been bisecting a performance regression for x25519 cryptographic operations with BoringSSL (https://boringssl.googlesource.com/boringssl) that causes builds with gcc (tested w/ 13.2.0) to perform significantly worse than builds with clang (tested w/ clang 11.1.0).

I've identified the regression is in this commit: https://github.com/google/boringssl/commit/d605df5b6f8462c1f3005da82d718ec067f46b70


Building the project with gcc prior to this commit (Linux 6.1.55, gcc 13.2.0, 12th Gen Intel Core i7-1280P) shows the following numbers in the boringssl performance tests:

Did 90900 Ed25519 key generation operations in 1006408us (90321.2 ops/sec)
Did 94000 Ed25519 signing operations in 1002192us (93794.4 ops/sec)
Did 33000 Ed25519 verify operations in 1029750us (32046.6 ops/sec)
Did 103000 Curve25519 base-point multiplication operations in 1005442us (102442.5 ops/sec)
Did 39000 Curve25519 arbitrary point multiplication operations in 1010017us (38613.2 ops/sec)

Building the project with gcc at the identified regression commit produces worse numbers for the same benchmarks:

Did 33744 Ed25519 key generation operations in 1006475us (33526.9 ops/sec)
Did 34000 Ed25519 signing operations in 1011973us (33597.7 ops/sec)
Did 32000 Ed25519 verify operations in 1032193us (31002.0 ops/sec)
Did 36000 Curve25519 base-point multiplication operations in 1021745us (35233.8 ops/sec)
Did 39000 Curve25519 arbitrary point multiplication operations in 1020887us (38202.1 ops/sec)

Running the same tests prior to the problematic commit but using clang 11.1.0 produces these numbers:

Did 80132 Ed25519 key generation operations in 1004593us (79765.6 ops/sec)
Did 81000 Ed25519 signing operations in 1003061us (80752.8 ops/sec)
Did 28000 Ed25519 verify operations in 1010878us (27698.7 ops/sec)
Did 87000 Curve25519 base-point multiplication operations in 1005378us (86534.6 ops/sec)
Did 38000 Curve25519 arbitrary point multiplication operations in 1004032us (37847.4 ops/sec)

And doing the same with the problematic commit and clang 11.1.0 shows:

Did 83739 Ed25519 key generation operations in 1007756us (83094.5 ops/sec)
Did 88000 Ed25519 signing operations in 1010131us (87117.4 ops/sec)
Did 31000 Ed25519 verify operations in 1013649us (30582.6 ops/sec)
Did 94000 Curve25519 base-point multiplication operations in 1008822us (93178.0 ops/sec)
Did 39000 Curve25519 arbitrary point multiplication operations in 1020461us (38218.0 ops/sec)

You can see with the reported numbers that while the clang build is a little bit slower after the problematic commit, the GCC build is much slower, suggesting something specific to GCC is causing the slow down.

I'm not confident in my ability to dissect the underlying cause, but suspect that GCC's handling of the new precomputed table representation is not as efficient as it could be relative to clang. I'm hopeful that with clear reproduction steps someone more familiar would be able to make progress.

I've already opened a bug with the BoringSSL project: https://bugs.chromium.org/p/boringssl/issues/detail?id=655 


Here are the reproduction steps:

1. Check out https://github.com/google/boringssl/commit/d605df5b6f8462c1f3005da82d718ec067f46b70
2. Configure and build the project **with GCC**:
```
CFLAGS="-Wno-error=stringop-overflow" CC= CXX= cmake -DCMAKE_BUILD_TYPE=Release -B build-release-gcc
<snipped>
make -C build-release-gcc
<snipped>
```
3. Run the `bssl speed` tool, filtering for `25519`:
```
build-release-gcc/tool/bssl speed -filter 25519
```
4. Observe slower results.
```
5. Check out https://github.com/google/boringssl/commit/4a0393fcf37d7dbd090a5bb2293601a9ec7605da - the parent commit to d605df5b6f8462c1f3005da82d718ec067f46b70
6. Repeat the process described above.
7. Observe faster results.

The same process can be undertaken with clang by substituting the `cmake` step with:

CC=clang CXX=clang++ cmake -DCMAKE_BUILD_TYPE=Release -B build-release-clang
make -C build-release-clang

Thank you!


---


### compiler : `gcc`
### title : `Fail to vectorize the struct include struct`
### open_at : `2023-10-12T07:31:39Z`
### last_modified_date : `2023-10-13T06:47:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111779
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdbool.h>
#include <assert.h>
struct C
{
    int c;
    int d;
    bool f :1;
    float e;
};

struct A
{
  unsigned int a;
  unsigned char c1, c2;
  bool b1 : 1;
  bool b2 : 1;
  bool b3 : 1;
  struct C b4;
};

void
foo (const struct A * __restrict x, int y)
{
  int s = 0, i = 0;
  for (i = 0; i < y; ++i)
    {
      const struct A a = x[i];
      s += a.b4.f ? 1 : 0;
    }
  assert (s == 0);
    //__builtin_abort ();
}

int
main ()
{
  struct A x[100];
  int i;
  __builtin_memset (x, -1, sizeof (x));
  for (i = 0; i < 100; i++)
    {
      x[i].b1 = false;
      x[i].b2 = false;
      x[i].b3 = false;
      x[i].b4.f = false;
    }
  foo (x, 100);
  return 0;
}

https://godbolt.org/z/KWb7c1n5h

Both SVE GCC and RVV GCC failed to vectorize it.
But Clang succeed on vectorization.


---


### compiler : `gcc`
### title : `Missed optimization of '(t*4)/(t*2) -> 2'`
### open_at : `2023-10-12T07:56:54Z`
### last_modified_date : `2023-10-12T08:34:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111780
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Hello, we found some optimizations (regarding Arithmetic optimization) that GCC may have missed. We would greatly appreicate if you can take a look and let us know what you think.

Given the following code:
https://godbolt.org/z/G9rWK7c3q
int n1;
void func1(int a){
    if(a>1&&a<4) n1=(a+a+a+a)/(a+a);
}

Different from PR 111718, this missed optimization appears to be due to a missed pattern: (t*4)/(t*2) -> 2

  # DEBUG BEGIN_STMT
  # RANGE [irange] int [8, 8][12, 12] NONZERO 0xc
  _3 = a_7(D) * 4;
  # RANGE [irange] int [4, 4][6, 6] NONZERO 0x6
  _4 = a_7(D) * 2;
  # RANGE [irange] int [1, 3] NONZERO 0x3
  _5 = _3 / _4;
  # .MEM_9 = VDEF <.MEM_8(D)>
  n1D.2761 = _5;

Or a more general pattern: (t*m)/(t*n) -> m/n , where m and n are constants.

Thank you very much for your time and effort! We look forward to hearing from you.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Extra move with arguments and returns and still using the argument for the last statement`
### open_at : `2023-10-12T09:43:52Z`
### last_modified_date : `2023-10-12T21:59:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111782
### status : `NEW`
### tags : `missed-optimization, needs-bisection, ra`
### component : `rtl-optimization`
### version : `14.0`
### severity : `normal`
### contents :
The testcase:
    __complex double
    foo (__complex double a, __complex double b)
    {
      return a * b;
    }

With GCC trunk at -Ofast I see on aarch64:
foo(double _Complex, double _Complex):
        fmov    d31, d1
        fmul    d1, d1, d2
        fmadd   d1, d0, d3, d1
        fmul    d31, d31, d3
        fnmsub  d0, d0, d2, d31
        ret

with GCC 10 the codegen used to be tighter:
foo(double _Complex, double _Complex):
        fmul    d4, d1, d3
        fmul    d5, d1, d2
        fmadd   d1, d0, d3, d5
        fnmsub  d0, d0, d2, d4
        ret

There's an extra fmov emitted on trunk.
I noticed this regressed with the GCC 11 series


---


### compiler : `gcc`
### title : `[14 Regression] aarch64: ldp_stp_{15,16,17,18}.c test failures since r14-4579`
### open_at : `2023-10-12T13:02:38Z`
### last_modified_date : `2023-10-23T06:16:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111784
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Since r14-4579-g0bdb9bb5607edd7df1ee74ddfcadb87324ca00c2 the following aarch64 tests are failing:

FAIL: gcc.target/aarch64/ldp_stp_15.c check-function-bodies dup_8_int32_t
FAIL: gcc.target/aarch64/ldp_stp_15.c check-function-bodies cons2_16_int32_t
FAIL: gcc.target/aarch64/ldp_stp_15.c check-function-bodies cons4_8_int32_t
FAIL: gcc.target/aarch64/ldp_stp_16.c check-function-bodies cons2_4_float
FAIL: gcc.target/aarch64/ldp_stp_17.c check-function-bodies dup_16_int64_t
FAIL: gcc.target/aarch64/ldp_stp_17.c check-function-bodies cons2_16_int64_t
FAIL: gcc.target/aarch64/ldp_stp_17.c check-function-bodies cons4_16_int64_t
FAIL: gcc.target/aarch64/ldp_stp_18.c check-function-bodies dup_8_double
FAIL: gcc.target/aarch64/ldp_stp_18.c check-function-bodies dup_16_double
FAIL: gcc.target/aarch64/ldp_stp_18.c check-function-bodies cons2_4_double
FAIL: gcc.target/aarch64/ldp_stp_18.c check-function-bodies cons2_8_double
FAIL: gcc.target/aarch64/ldp_stp_18.c check-function-bodies cons2_8_double
FAIL: gcc.target/aarch64/ldp_stp_18.c check-function-bodies cons2_8_double

E.g. for dup8_int32_t, we now generate:

dup_8_int32_t:
.LFB9:
        .cfi_startproc
        stp     w1, w1, [x0]
        stp     w1, w1, [x0, 8]
        stp     w1, w1, [x0, 16]
        stp     w1, w1, [x0, 24]
        ret

instead of a dup with a q-register stp. Most likely we need to update the costs on the aarch64 side.


---


### compiler : `gcc`
### title : `RISC-V: Strange loop vectorizaion on popcount function`
### open_at : `2023-10-12T19:56:25Z`
### last_modified_date : `2023-10-19T02:26:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111791
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Symptom:

A typical popcount implementation with Brian Kernighan’s algorithm, vectorizer has recognized that as popcount, but...come with strange vectorization result, I  know that might because I add -fno-vect-cost-model, but I still don't understand why it vectorized, so I guess maybe it's something worth to report.

NOTE:
Those bad/strange code gen will gone once scalar popcount instruction available. 

Case:
```
int popcount(unsigned long value)
  {
    int nbits;
    for (nbits = 0; value != 0; value &= value - 1)
      nbits++;
    return nbits;
  }

```

Command to reproduce:
```
$ riscv64-unknown-linux-gnu-gcc x.c -march=rv64gcv -o - -S -fno-vect-cost-model -O3
```

Sha1: g:faae30c49560f1481f036061fa2f894b0f7257f8 (some random point of top of trunk)

Current output:
```
        .globl  popcount
        .type   popcount, @function
popcount:
.LFB0:
        .cfi_startproc
        beq     a0,zero,.L4
        addi    sp,sp,-16
        .cfi_def_cfa_offset 16
        sd      ra,8(sp)
        .cfi_offset 1, -8
        call    __popcountdi2
        csrr    a2,vlenb
        sext.w  a0,a0
        srli    a2,a2,2
        vsetvli a3,zero,e32,m1,ta,ma
        vid.v   v1
.L3:
        vsetvli a5,a0,e8,mf4,ta,ma
        sub     a0,a0,a5
        vsetvli a3,zero,e32,m1,ta,ma
        vmv1r.v v3,v1
        vmv.v.x v2,a2
        vadd.vv v1,v1,v2
        bne     a0,zero,.L3
        ld      ra,8(sp)
        .cfi_restore 1
        addi    a5,a5,-1
        vadd.vi v3,v3,1
        vslidedown.vx   v3,v3,a5
        addi    sp,sp,16
        .cfi_def_cfa_offset 0
        vmv.x.s a0,v3
        jr      ra
.L4:
        li      a0,0
        ret
        .cfi_endproc
.LFE0:
        .size   popcount, .-popcount

```


---


### compiler : `gcc`
### title : `OpenMP SIMD inbranch clones for AVX512 are highly sub-optimal`
### open_at : `2023-10-13T07:28:13Z`
### last_modified_date : `2023-10-13T09:39:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111793
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `RISC-V: Missed SLP optimization due to mask mode precision`
### open_at : `2023-10-13T07:41:56Z`
### last_modified_date : `2023-10-23T16:44:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111794
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
void
f (int *restrict x, short *restrict y)
{
  x[0] = x[0] == 1 & y[0] == 2;
  x[1] = x[1] == 1 & y[1] == 2;
  x[2] = x[2] == 1 & y[2] == 2;
  x[3] = x[3] == 1 & y[3] == 2;
  x[4] = x[4] == 1 & y[4] == 2;
  x[5] = x[5] == 1 & y[5] == 2;
  x[6] = x[6] == 1 & y[6] == 2;
  x[7] = x[7] == 1 & y[7] == 2;
}

Realize that we failed to vectorize this case:

https://godbolt.org/z/rWz9fjM4r

The root cause is the mask bit precision of "small mask mode" (Potentially has 
bitsize smaller than 1 bytes).

If we remove this following adjust precision:

ADJUST_PRECISION (V1BI, 1);
ADJUST_PRECISION (V2BI, 2);
ADJUST_PRECISION (V4BI, 4);

ADJUST_PRECISION (RVVMF16BI, riscv_v_adjust_precision (RVVMF16BImode, 4));
ADJUST_PRECISION (RVVMF32BI, riscv_v_adjust_precision (RVVMF32BImode, 2));
ADJUST_PRECISION (RVVMF64BI, riscv_v_adjust_precision (RVVMF64BImode, 1));

It can vectorize such case but will cause bugs in other situations.

Is it possible to fix that in GCC?


---


### compiler : `gcc`
### title : `OMP SIMD inbranch call vectorization missing for AVX512`
### open_at : `2023-10-13T08:43:31Z`
### last_modified_date : `2023-10-13T12:30:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111795
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
We don't currently handle the following

int x[1024];

#pragma omp declare simd simdlen(16) inbranch
__attribute__((noinline)) int
foo (int a, int b)
{
  return a + b;
}

void __attribute__((noipa))
bar (void)
{
#pragma omp simd
  for (int i = 0; i < 1024; i++)
    {
      if (x[i] < 10)
        x[i] = foo (x[i], x[i]);
    }
}


---


### compiler : `gcc`
### title : `OMP SIMD call vectorization fails for arguments subject to integer promotion rules`
### open_at : `2023-10-13T10:31:56Z`
### last_modified_date : `2023-10-13T12:45:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111796
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
For example

int x[1024];

#pragma omp declare simd simdlen(8)
__attribute__((noinline)) int
foo (int a, short b)
{
  return a + b;
}

void __attribute__((noipa))
bar (void)
{
#pragma omp simd
  for (int i = 0; i < 1024; i++)
    x[i] = foo (x[i], x[i]);
}

fails to vetorize because for the scalar code we see

  _4 = x[i_12];
  _5 = (short int) _4;
  _6 = (int) _5;
  _7 = foo (_4, _6);

thus the second argument to 'foo' is promoted to 'int', but the SIMD clone
at least on x86_64 expects vector(8) short int simd.6 as argument.

vectorizable_simd_clone_call has the following, which will result in
rejecting the call.

        for (i = 0; i < nargs; i++)
          {
            switch (n->simdclone->args[i].arg_type)
              {
              case SIMD_CLONE_ARG_TYPE_VECTOR:
                if (!useless_type_conversion_p
                        (n->simdclone->args[i].orig_type,
                         TREE_TYPE (gimple_call_arg (stmt, i + arg_offset))))
                  i = -1;

This argument promotion is exposed by the frontend, controlled by a target
hook.  IIRC it is intended to allow more optimization, so maybe it can be
disabled for calls to OMP SIMD functions.

Alternatively the vectorizer needs to deal with this somehow, for example
in vectorizable_simd_clone_call by allowing this and instead peeking 
through the conversion.  Possibly also done via pattern recognizing the call
itself.


---


### compiler : `gcc`
### title : `Code generation of -march=znver2 -O3 includes frame pointer`
### open_at : `2023-10-13T11:09:58Z`
### last_modified_date : `2023-10-13T18:02:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111797
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.1.0`
### severity : `normal`
### contents :
I was a bit surprised recently when I (unintentinally) ran perf record on the exe that I work on with an -O3 build without -fno-omit-frame-pointer and I could see the callstacks.

The function prolog that I see is

0000000000000000 <function>:
       0:       4c 8d 54 24 08          lea    0x8(%rsp),%r10
       5:       48 83 e4 e0             and    $0xffffffffffffffe0,%rsp
       9:       41 ff 72 f8             push   -0x8(%r10)
       d:       55                      push   %rbp
       e:       48 89 e5                mov    %rsp,%rbp
      11:       41 57                   push   %r15
      13:       41 56                   push   %r14
      15:       41 55                   push   %r13
      17:       41 54                   push   %r12
      19:       41 52                   push   %r10
      1b:       53                      push   %rbx
      1c:       49 89 ce                mov    %rcx,%r14
      1f:       48 81 ec 40 10 00 00    sub    $0x1040,%rsp

I asked on SO and got pointed to this post

https://stackoverflow.com/questions/45423338/whats-up-with-gcc-weird-stack-manipulation-when-it-wants-extra-stack-alignment

That problem seems to be fixed

https://godbolt.org/z/qc6fqb5hn

I can't post the source code as it is proprietary, and it doesn't seem to reproduce with trivial examples (the function that I tried is 23kloc plus it #includes other stuff).

I was able to reproduce the problem with the following steps (Valgrind chosen because I'm one of the maintainers and I'm in the habit of building it).

git clone https://sourceware.org/git/valgrind.git march_zen2
cd march_zen2
./autogen.sh
./configure CFLAGS=-march=znver2
make -j 16
objdump -d --disassemble=mc_pre_clo_init mc_pre_clo_init .in_place/memcheck-amd64-linux | less

That shows

000000005800c220 <mc_pre_clo_init>:
    5800c220:   41 55                   push   %r13
    5800c222:   bf 8c 65 1d 58          mov    $0x581d658c,%edi
    5800c227:   4c 8d 6c 24 10          lea    0x10(%rsp),%r13
    5800c22c:   48 83 e4 e0             and    $0xffffffffffffffe0,%rsp
    5800c230:   41 ff 75 f8             push   -0x8(%r13)
    5800c234:   55                      push   %rbp
    5800c235:   48 89 e5                mov    %rsp,%rbp
    5800c238:   41 55                   push   %r13
    5800c23a:   48 83 ec 08             sub    $0x8,%rsp

which I believe illustrates the same problem.

mc_pre_clo_init looks like this


static void mc_pre_clo_init(void)
{
   VG_(details_name)            ("Memcheck");
   VG_(details_version)         (NULL);
   VG_(details_description)     ("a memory error detector");
   VG_(details_copyright_author)(
      "Copyright (C) 2002-2022, and GNU GPL'd, by Julian Seward et al.");
   VG_(details_bug_reports_to)  (VG_BUGS_TO);

VG_ is a macro that implements a kind of C namespace. The functions are all outputting the memcheck startup banner.

I think that I understand that there is a need for a 32byte-aligned stack and also to shuffle the return address. Is it really necessary to also use the frame pointer?


---


### compiler : `gcc`
### title : `[14 Regression] Recent change causing testsuite regression and poor code on mcore-elf`
### open_at : `2023-10-13T15:30:29Z`
### last_modified_date : `2023-10-22T22:37:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111798
### status : `WAITING`
### tags : `missed-optimization, wrong-code`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
This change:

commit 6decda1a35be5764101987c210b5693a0d914e58
Author: Richard Biener <rguenther@suse.de>
Date:   Thu Oct 12 11:34:57 2023 +0200

    tree-optimization/111779 - Handle some BIT_FIELD_REFs in SRA
    
    The following handles byte-aligned, power-of-two and byte-multiple
    sized BIT_FIELD_REF reads in SRA.  In particular this should cover
    BIT_FIELD_REFs created by optimize_bit_field_compare.
    
    For gcc.dg/tree-ssa/ssa-dse-26.c we now SRA the BIT_FIELD_REF
    appearing there leading to more DSE, fully eliding the aggregates.
    
    This results in the same false positive -Wuninitialized as the
    older attempt to remove the folding from optimize_bit_field_compare,
    fixed by initializing part of the aggregate unconditionally.
    
            PR tree-optimization/111779
    gcc/
            * tree-sra.cc (sra_handled_bf_read_p): New function.
            (build_access_from_expr_1): Handle some BIT_FIELD_REFs.
            (sra_modify_expr): Likewise.
            (make_fancy_name_1): Skip over BIT_FIELD_REF.
    
    gcc/fortran/
            * trans-expr.cc (gfc_trans_assignment_1): Initialize
            lhs_caf_attr and rhs_caf_attr codimension flag to avoid
            false positive -Wuninitialized.
    
    gcc/testsuite/
            * gcc.dg/tree-ssa/ssa-dse-26.c: Adjust for more DSE.
            * gcc.dg/vect/vect-pr111779.c: New testcase.

Causes execute/20040709-2.c to fail on mcore-elf at -O2.  It also results in what appears to be significantly poorer code generation.

Note I haven't managed to get mcore-elf-gdb to work, so debugging is, umm, painful.  And I wouldn't put a lot of faith in the simulator correctness.

I have simplified the test to this:
extern void abort (void);
extern void exit (int);

unsigned int
myrnd (void)
{
  static unsigned int s = 1388815473;
  s *= 1103515245;
  s += 12345;
  return (s / 65536) % 2048;
}

struct __attribute__((packed)) K
{
  unsigned int k:6, l:1, j:10, i:15;
};

struct K sK;

unsigned int
fn1K (unsigned int x)
{
  struct K y = sK;
  y.k += x;
  return y.k;
}

void
testK (void)
{
  int i;
  unsigned int mask, v, a, r;
  struct K x;
  char *p = (char *) &sK;
  for (i = 0; i < sizeof (sK); ++i)
    *p++ = myrnd ();
  v = myrnd ();
  a = myrnd ();
  sK.k = v;
  x = sK;
  r = fn1K (a);
  if (x.j != sK.j || x.l != sK.l)
    abort ();
}

int
main (void)
{
  testK ();
  exit (0);
}


Which should at least make the poor code gen obvious.  I don't expect to have time to debug this further anytime in the near future.


---


### compiler : `gcc`
### title : `[14 Regression] Missed Dead Code Elimination since  r14-2365-g2e406f0753e`
### open_at : `2023-10-13T15:46:14Z`
### last_modified_date : `2023-10-22T22:39:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111799
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `middle-end`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/bqcvMcbqn

Given the following code:

void foo(void);
static struct d {
    short e;
} g = {205}, *h = &g;
static int i;
static int *j, *a = &i;
static int **k = &j;
static int ****l;
static int *****m;
static char(n)(char b, char c) { return b + c; }
static char(o)(char b, char c) { return b * c; }
static short(p)(short f) {
    if (!(((f) >= 1) && ((f) <= 65459))) {
        __builtin_unreachable();
    }
    return 0;
}
static int *q(short);
static void s(struct d) { *k = q(i); }
static int *q(short ad) {
    int b = *a;
    ad = -21;
    for (; ad; ad = n(ad, 7)) p((ad ^ b && *a) <= *a);
    return *k;
}
int main() {
    i = 0;
    for (;; i = 1) {
        q(3);
        char r = o(126 | 1, g.e);
        p(r);
        s(*h);
        if (i) break;
        m = &l;
    }
    if (m)
        ;
    else
        foo();
    ;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	movl	$2, %esi
	xorl	%ecx, %ecx
	xorl	%r9d, %r9d
	movl	$8, %edi
	movl	$0, i(%rip)
	.p2align 4,,10
	.p2align 3
.L10:
	movl	$-21, %eax
.L2:
	testl	%ecx, %ecx
	je	.L29
	leal	7(%rax), %edx
	movsbw	%dl, %r8w
	testb	%dl, %dl
	je	.L23
.L5:
	movl	$8, %eax
	cmpb	$1, %dl
	je	.L2
	movl	%r8d, %eax
	leal	7(%rax), %edx
	movsbw	%dl, %r8w
	testb	%dl, %dl
	jne	.L5
	.p2align 4,,10
	.p2align 3
.L23:
	movl	$-21, %eax
	addb	$7, %al
	je	.L30
.L9:
	movsbl	%al, %edx
	cmpl	%edx, %ecx
	cmove	%edi, %eax
	cbtw
	addb	$7, %al
	jne	.L9
.L30:
	cmpl	$1, %esi
	je	.L31
	movl	$1, %esi
	movl	$1, %ecx
	movl	$1, %r9d
	jmp	.L10
	.p2align 4,,10
	.p2align 3
.L29:
	testb	$1, %al
	je	.L4
	leal	7(%rax), %edx
	movsbw	%dl, %ax
	testb	%dl, %dl
	je	.L23
.L4:
	leal	14(%rax), %edx
	movsbw	%dl, %ax
	testb	%dl, %dl
	jne	.L4
	jmp	.L23
	.p2align 4,,10
	.p2align 3
.L31:
	testb	%r9b, %r9b
	je	.L11
	movq	$l, m(%rip)
	movl	%ecx, i(%rip)
.L25:
	xorl	%eax, %eax
	ret
.L11:
	cmpq	$0, m(%rip)
	jne	.L25
	pushq	%rax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	ret

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	movq	$l, m(%rip)
	xorl	%eax, %eax
	movl	$1, i(%rip)
	ret

Bisects to r14-2365-g2e406f0753e


---


### compiler : `gcc`
### title : `[14 Regression] Missed Dead Code Elimination since r14-4141-gbf6b107e2a3`
### open_at : `2023-10-13T15:56:46Z`
### last_modified_date : `2023-10-22T22:40:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111801
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/47fbssWd3

Given the following code:

void foo(void);
static struct {
    signed a;
} b, c, *e = &b;
static struct {
    unsigned d;
} g = {109};
static int f;
static int *h;
static int **i = &h;
int main() {
    f = 1;
    if (b.a) f = 8;
    int *j = *i;
    if (j == &g == 0)
        if (!(((g.d) >= 109) && ((g.d) <= 109))) {
            __builtin_unreachable();
        }
    if (f)
        ;
    else {
        c = *e;
        foo();
    }
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	movl	$1, f(%rip)
	movl	b(%rip), %esi
	testl	%esi, %esi
	je	.L10
	movl	$8, f(%rip)
.L10:
	cmpq	$g, h(%rip)
	je	.L7
	movl	f(%rip), %ecx
	testl	%ecx, %ecx
	je	.L12
.L7:
	xorl	%eax, %eax
	ret
.L12:
	pushq	%rax
	call	foo
	xorl	%eax, %eax
	popq	%rdx
	ret

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	movl	$1, f(%rip)
	movl	b(%rip), %eax
	testl	%eax, %eax
	je	.L2
	movl	$8, f(%rip)
.L2:
	xorl	%eax, %eax
	ret

Bisects to r14-4141-gbf6b107e2a3


---


### compiler : `gcc`
### title : `suboptimal codegen of variant<string, bool>`
### open_at : `2023-10-14T05:40:10Z`
### last_modified_date : `2023-10-16T06:57:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111805
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.2.0`
### severity : `normal`
### contents :
#include<variant>
#include<string>

std::string foo() {
    std::variant<std::string, bool> v {"abc"};
    return std::get<0>(v);
}

g++-13.2 -O2 -std=c++20


foo[abi:cxx11]():
        lea     rdx, [rdi+16]
        mov     BYTE PTR [rdi+18], 99
        mov     rax, rdi
        mov     QWORD PTR [rdi], rdx
        mov     edx, 25185
        mov     WORD PTR [rdi+16], dx
        mov     QWORD PTR [rdi+8], 3
        mov     BYTE PTR [rdi+19], 0
        ret


clang++ -O2 -std=c++20

foo():                                # @foo()
        mov     rax, rdi
        mov     byte ptr [rdi], 6
        mov     dword ptr [rdi + 1], 6513249
        ret


https://godbolt.org/z/nTv5rYanM


---


### compiler : `gcc`
### title : `g++ generates better code for variant<string, bool> at -Os compared to -O3`
### open_at : `2023-10-14T06:02:01Z`
### last_modified_date : `2023-10-16T06:57:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111806
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `13.2.0`
### severity : `normal`
### contents :
#include<variant>
#include<string>
#include<iostream>

int foo() {
    std::variant<std::string, bool> v {"abc"};
    std::cout << std::get<0>(v);
    return 0;
}

g++ -O3 -std=c++20 -g0  -fno-exceptions

foo():
.LFB2484:
        push    rbx
        mov     eax, 25185
        mov     edx, 3
        mov     edi, OFFSET FLAT:_ZSt4cout
        sub     rsp, 48
        lea     rbx, [rsp+16]
        mov     WORD PTR [rsp+16], ax
        mov     rsi, rbx
        mov     QWORD PTR [rsp], rbx
        mov     BYTE PTR [rsp+18], 99
        mov     QWORD PTR [rsp+8], 3
        mov     BYTE PTR [rsp+19], 0
        mov     BYTE PTR [rsp+32], 0
        call    std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)
        cmp     BYTE PTR [rsp+32], 0
        je      .L5
.L2:
        add     rsp, 48
        xor     eax, eax
        pop     rbx
        ret
.L5:
        mov     rdi, QWORD PTR [rsp]
        cmp     rdi, rbx
        je      .L2
        mov     rax, QWORD PTR [rsp+16]
        lea     rsi, [rax+1]
        call    operator delete(void*, unsigned long)
        add     rsp, 48
        xor     eax, eax
        pop     rbx
        ret
.LFE2484:


g++ -Os -std=c++20 -g0  -fno-exceptions


foo():
.LFB2463:
        push    rbx
        mov     edx, 3
        mov     edi, OFFSET FLAT:_ZSt4cout
        sub     rsp, 48
        lea     rbx, [rsp+24]
        mov     WORD PTR [rsp+24], 25185
        mov     rsi, rbx
        mov     QWORD PTR [rsp+8], rbx
        mov     BYTE PTR [rsp+26], 99
        mov     QWORD PTR [rsp+16], 3
        mov     BYTE PTR [rsp+27], 0
        mov     BYTE PTR [rsp+40], 0
        call    std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)
        cmp     BYTE PTR [rsp+40], 0
        jne     .L2
        mov     rdi, QWORD PTR [rsp+8]
        cmp     rdi, rbx
        je      .L2
        mov     rax, QWORD PTR [rsp+24]
        lea     rsi, [rax+1]
        call    operator delete(void*, unsigned long)
.L2:
        add     rsp, 48
        xor     eax, eax
        pop     rbx
        ret
.LFE2463:


https://godbolt.org/z/3xKh35Mrv


---


### compiler : `gcc`
### title : ``a > b ? ~a : ~b` is sometimes not detected until phiopt4 (or not at all)`
### open_at : `2023-10-15T23:12:04Z`
### last_modified_date : `2023-10-15T23:12:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111827
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Take:
```
int f(int a, int b, int c)
{
        int at = ~a;
        int bt = ~b;
        int ct = ~c;
        int t = a < b ? at : bt;
        return t;        
}
```

This takes until phiopt4 (after sink) to detect ~MIN_EXPR.
Note if at is used more than once like in PR 67438, it is not detected at all which is why the main reason `~X op ~Y` patterns have a single_use on them.


---


### compiler : `gcc`
### title : `Redudant register moves inside the loop`
### open_at : `2023-10-16T05:54:58Z`
### last_modified_date : `2023-10-16T17:19:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111829
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include<immintrin.h>
int
foo (__m128i* __restrict pa, int* b,
 __m128i* __restrict pc, int n)
{
    __m128i vsum = _mm_setzero_si128();
    for (int i = 0; i != 100000; i++)
    {
        vsum = _mm_dpbusd_epi32 (vsum, pa[i], _mm_set1_epi32 (b[i]));
    }
    *pc = vsum;
    int ssum = 0;
    for (int i = 0; i != 4; i++)
      ssum += ((__v4si)vsum)[i];
    return ssum;
}

gcc -O2 -mavxvnni

foo(long long __vector(2)*, int*, long long __vector(2)*, int):
        leaq    400000(%rsi), %rax
        vpxor   %xmm0, %xmm0, %xmm0
.L2:
        vmovdqa (%rdi), %xmm2
        vmovdqa %xmm0, %xmm1 ---- redundant
        addq    $4, %rsi
        addq    $16, %rdi
        vpbroadcastd    -4(%rsi), %xmm3
        {vex} vpdpbusd  %xmm3, %xmm2, %xmm1
        vmovdqa %xmm1, %xmm0 --- redundant
        cmpq    %rax, %rsi
        jne     .L2
        vmovdqa %xmm1, (%rdx)
        leaq    -24(%rsp), %rax
        leaq    -8(%rsp), %rcx
        xorl    %edx, %edx
.L3:
        vmovdqa %xmm0, -24(%rsp)
        addq    $4, %rax
        addl    -4(%rax), %edx
        cmpq    %rax, %rcx
        jne     .L3
        movl    %edx, %eax
        ret


it can be better with


foo(long long __vector(2)*, int*, long long __vector(2)*, int):
        leaq    400000(%rsi), %rax
        vpxor   %xmm0, %xmm0, %xmm0
.L2:
        vmovdqa (%rdi), %xmm2

        addq    $4, %rsi
        addq    $16, %rdi
        vpbroadcastd    -4(%rsi), %xmm3
        {vex} vpdpbusd  %xmm3, %xmm2, %xmm0
        cmpq    %rax, %rsi
        jne     .L2
        vmovdqa %xmm0, (%rdx)
        leaq    -24(%rsp), %rax
        leaq    -8(%rsp), %rcx
        xorl    %edx, %edx
.L3:
        vmovdqa %xmm0, -24(%rsp)
        addq    $4, %rax
        addl    -4(%rax), %edx
        cmpq    %rax, %rcx
        jne     .L3
        movl    %edx, %eax
        ret


---


### compiler : `gcc`
### title : `Suboptimal codegen: zero extended load instead of sign extended one`
### open_at : `2023-10-16T14:46:49Z`
### last_modified_date : `2023-10-17T06:32:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111835
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
In this simplified example:

int test (const uint8_t * src, uint8_t * dst)
{
    int8_t tmp = (int8_t)*src;
    *dst = tmp;
    return tmp;
}

GCC prefers to use load with zero extension instead of more rational sign extended load.
Then it needs to do explicit sign extension for making return value.

I know there's a lot of bugs related to zero/sign ext, but I guessed it's rare special case, and it reproduces in any GCC version available at godbolt and any architecture except x86-64.


---


### compiler : `gcc`
### title : `missed optimization`
### open_at : `2023-10-17T04:17:37Z`
### last_modified_date : `2023-10-17T06:54:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111844
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Hello,

The following code compiles and optimizes to something reasonable under -O2 -std=c++14 with gcc trunk (Oct 16, d5cfabc677b08f38ea5d5f85deeda746b4fabb88)


#include <cstring>

extern void bar();

struct P {
    unsigned int x;
    unsigned int y;
    unsigned int z[20];
};

void foo(void* buf, int inc) {
    P p;
    memcpy(&p, buf, sizeof(p)) ;
    p.x += inc;
    memcpy(buf, &p, sizeof(p)) ;

    // bar();
}


Results in assembly that only loads the portion of data from 'buf' that corresponds to p.x.

foo(void*, int):
        movdqu  xmm0, XMMWORD PTR [rdi]
        movaps  XMMWORD PTR [rsp-104], xmm0
        add     DWORD PTR [rsp-104], esi
        movdqa  xmm0, XMMWORD PTR [rsp-104]
        movups  XMMWORD PTR [rdi], xmm0
        ret

However, reintroducing the call to bar() results in significantly worse assembly; it appears to want to copy the entire struct `p` out of buf, even though almost all of the movaps instructions are not useful.

foo(void*, int):
        movdqu  xmm0, XMMWORD PTR [rdi]
        mov     rax, QWORD PTR [rdi+80]
        movaps  XMMWORD PTR [rsp-104], xmm0
        movdqu  xmm0, XMMWORD PTR [rdi+16]
        add     DWORD PTR [rsp-104], esi
        movaps  XMMWORD PTR [rsp-88], xmm0
        movdqu  xmm0, XMMWORD PTR [rdi+32]
        mov     QWORD PTR [rsp-24], rax
        movaps  XMMWORD PTR [rsp-72], xmm0
        movdqu  xmm0, XMMWORD PTR [rdi+48]
        movaps  XMMWORD PTR [rsp-56], xmm0
        movdqu  xmm0, XMMWORD PTR [rdi+64]
        movaps  XMMWORD PTR [rsp-40], xmm0
        movdqa  xmm0, XMMWORD PTR [rsp-104]
        movups  XMMWORD PTR [rdi], xmm0
        jmp     bar()

For comparison, several versions of clang with the same flags will optimize this to:

foo(void*, int):
        add     dword ptr [rdi], esi
        jmp     bar()

I am not sure why the loads to the stack-local `P p` are not elided; my first thought was that perhaps escape analysis on &p forces the full load in case memcpy "saves" the address of `p` for use by bar(); I would have expected that wrapping the {decl/memcpy/increment/memcpy} in it's own scope would address that but it seems to have no effect.

Thanks


---


### compiler : `gcc`
### title : `RISC-V: RVV cost model pick unexpected big LMUL`
### open_at : `2023-10-17T11:03:14Z`
### last_modified_date : `2023-10-20T06:39:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111848
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
#include <stdint.h>
void
f3 (uint8_t *restrict a, uint8_t *restrict b,
   uint8_t *restrict c, uint8_t *restrict d,
   int n)
{
  for (int i = 0; i < n; ++i)
    {
      a[i * 8] = c[i * 8] + d[i * 8];
      a[i * 8 + 1] = c[i * 8] + d[i * 8 + 1];
      a[i * 8 + 2] = c[i * 8 + 2] + d[i * 8 + 2];
      a[i * 8 + 3] = c[i * 8 + 2] + d[i * 8 + 3];
      a[i * 8 + 4] = c[i * 8 + 4] + d[i * 8 + 4];
      a[i * 8 + 5] = c[i * 8 + 4] + d[i * 8 + 5];
      a[i * 8 + 6] = c[i * 8 + 6] + d[i * 8 + 6];
      a[i * 8 + 7] = c[i * 8 + 6] + d[i * 8 + 7];
      b[i * 8] = c[i * 8 + 1] + d[i * 8];
      b[i * 8 + 1] = c[i * 8 + 1] + d[i * 8 + 1];
      b[i * 8 + 2] = c[i * 8 + 3] + d[i * 8 + 2];
      b[i * 8 + 3] = c[i * 8 + 3] + d[i * 8 + 3];
      b[i * 8 + 4] = c[i * 8 + 5] + d[i * 8 + 4];
      b[i * 8 + 5] = c[i * 8 + 5] + d[i * 8 + 5];
      b[i * 8 + 6] = c[i * 8 + 7] + d[i * 8 + 6];
      b[i * 8 + 7] = c[i * 8 + 7] + d[i * 8 + 7];
    }
}

This case pick LMUL = 8 which causes horrible vector register spillings.

After experiment, the ideal LMUL should be 2.


---


### compiler : `gcc`
### title : `RISC-V: Failed to vectorize small GNU vector if zvl4096b with fixed-vlmax`
### open_at : `2023-10-18T06:19:56Z`
### last_modified_date : `2023-10-21T03:29:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111857
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `enhancement`
### contents :
typedef char vnx16i __attribute__ ((vector_size (16)));

#define MASK_16 12, 13, 14, 15, 12, 13, 14, 15, 12, 13, 14, 15, 12, 13, 14, 15

vnx16i __attribute__ ((noinline, noclone))
test_1 (vnx16i x, vnx16i y)
{
  return __builtin_shufflevector (x, y, MASK_16);
}

If we specify it as -march=rv64gcv --param=riscv-autovec-preference=fixed-vlmax.
it can vectorize.

But failed to vectorize it with -march=rv64gcv_zvl4096b --param=riscv-autovec-preference=fixed-vlmax

https://godbolt.org/z/zx816ao5o


---


### compiler : `gcc`
### title : `[12/13/14 Regression] Dead Code Elimination Regression`
### open_at : `2023-10-18T14:10:10Z`
### last_modified_date : `2023-10-18T16:15:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111864
### status : `NEW`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/PoYvWMG7T

Given the following code:

void foo(void);
static int d, g = 4, k;
static int *h, *j, *s;
static int **i = &h, **t = &s;
static int ***l = &i;
static unsigned short *m, *n;
static unsigned short **o = &n, **u = &m;
void __assert_fail() __attribute__((__noreturn__));
static short(a)(short b, int c) { return b << c; }
static int(e)(int f) {
    if (!(((f) >= 0) && ((f) <= 0))) {
        __builtin_unreachable();
    }
    return d;
}
static int ***p(int **q) {
    j = *q;
    return &i;
}
static int *r() {
    short v = a((g && p(t)) <= 0, 5);
    e(v);
    if (s) o = u;
    if (o == &m || o == &n)
        ;
    else
        __assert_fail();
    return &k;
}
int main() {
    **l = r();
    if (h)
        ;
    else
        __assert_fail();
    if (j == &g == 0)
        ;
    else
        __assert_fail();
    if (o == &m || o == &n)
        ;
    else
        foo();
    ;
}

gcc-trunk -O2 does not eliminate the call to foo:

main:
	subq	$8, %rsp
	movq	s(%rip), %rax
	movq	i(%rip), %rdx
	movq	%rax, j(%rip)
	testq	%rax, %rax
	je	.L17
	movq	$k, (%rdx)
	cmpq	$0, h(%rip)
	movq	$m, o(%rip)
	je	.L5
	cmpq	$g, %rax
	je	.L5
	movl	$m, %eax
.L10:
	cmpq	$n, %rax
	je	.L12
	cmpq	$m, %rax
	je	.L12
	call	foo
.L12:
	xorl	%eax, %eax
	addq	$8, %rsp
	ret
.L17:
	movq	o(%rip), %rax
	cmpq	$m, %rax
	je	.L18
	cmpq	$n, %rax
	jne	.L5
	movq	$k, (%rdx)
	cmpq	$0, h(%rip)
	jne	.L10
.L5:
	xorl	%eax, %eax
	call	__assert_fail
.L18:
	movq	$k, (%rdx)
	cmpq	$0, h(%rip)
	jne	.L12
	jmp	.L5

gcc-13.2.0 -O2 eliminates the call to foo:

main:
	movq	s(%rip), %rax
	movq	i(%rip), %rdx
	movq	%rax, j(%rip)
	testq	%rax, %rax
	je	.L2
	movq	$k, (%rdx)
	cmpq	$0, h(%rip)
	movq	$m, o(%rip)
	je	.L4
	cmpq	$g, %rax
	je	.L4
.L8:
	xorl	%eax, %eax
	ret
.L2:
	movq	o(%rip), %rax
	cmpq	$m, %rax
	je	.L5
	cmpq	$n, %rax
	jne	.L4
.L5:
	movq	$k, (%rdx)
	cmpq	$0, h(%rip)
	jne	.L8
.L4:
	pushq	%rax
	xorl	%eax, %eax
	call	__assert_fail

Bisects to r14-4038-gb975c0dc3be


---


### compiler : `gcc`
### title : `Missed mask_fold_left_plus with AVX512`
### open_at : `2023-10-19T08:49:36Z`
### last_modified_date : `2023-10-24T02:50:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111874
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
Currently fold-left reductions are open-coded by the vectorizer, extracting scalar elements and doing in-order adds.  That's probably as good as it can get.
For the case of conditional (or loop masked) fold-left reductions the scalar
fallback isn't implemented.  But AVX512 has vpcompress that could be used
to implement a more efficient sequence for a masked fold-left, possibly
using a loop and population count of the mask.

It might be interesting to experiment with this, not so much for the
fully masked loop case but for conditional reduction.  Maybe there's
some expert at Intel or AMD who can produce a good instruction sequence
here.


---


### compiler : `gcc`
### title : `No gather/scatter BB vectorization implemented`
### open_at : `2023-10-19T12:40:55Z`
### last_modified_date : `2023-10-19T12:52:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111879
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
void __attribute__((noipa))
f (int *restrict y, int *restrict x, int *restrict indices)
{     
  int i = 0;
  y[i * 2] = x[indices[i * 2]] + 1;
  y[i * 2 + 1] = x[indices[i * 2 + 1]] + 2;
  i++;
  y[i * 2] = x[indices[i * 2]] + 1;
  y[i * 2 + 1] = x[indices[i * 2 + 1]] + 2;
  i++;
  y[i * 2] = x[indices[i * 2]] + 1;
  y[i * 2 + 1] = x[indices[i * 2 + 1]] + 2;
  i++;
  y[i * 2] = x[indices[i * 2]] + 1;
  y[i * 2 + 1] = x[indices[i * 2 + 1]] + 2;
}   

doesn't see the gather operation vectorized (extracted from the loop in gcc.dg/vect/vect-gather-1.c).  Instead we only see the adds and the stores
vectorized.


---


### compiler : `gcc`
### title : `RISC-V: Horrible redundant number vsetvl instructions in vectorized codes`
### open_at : `2023-10-20T03:28:31Z`
### last_modified_date : `2023-10-20T05:47:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111888
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `14.0`
### severity : `normal`
### contents :
https://godbolt.org/z/9G5MMa3Tq

void
foo (int32_t *__restrict a, int32_t *__restrict b,    int32_t *__restrict c,
      int32_t *__restrict a2, int32_t *__restrict b2, int32_t *__restrict c2,
      int32_t *__restrict a3, int32_t *__restrict b3, int32_t *__restrict c3,
      int32_t *__restrict a4, int32_t *__restrict b4, int32_t *__restrict c4,
      int32_t *__restrict a5, int32_t *__restrict b5, int32_t *__restrict c5,
      int32_t *__restrict d,
      int32_t *__restrict d2,
      int32_t *__restrict d3,
      int32_t *__restrict d4,
      int32_t *__restrict d5,
      int n)
{
  for (int i = 0; i < n; i++)
    {
      a[i] = b[i] + c[i];
      b5[i] = b[i] + c[i];
      a2[i] = b2[i] + c2[i];
      a3[i] = b3[i] + c3[i];
      a4[i] = b4[i] + c4[i];
      a5[i] = a[i] + a4[i];
      d2[i] = a2[i] + c2[i];
      d3[i] = a3[i] + c3[i];
      d4[i] = a4[i] + c4[i];
      d5[i] = a[i] + a4[i];
      a[i] = a5[i] + b5[i] + a[i];

      c2[i] = a[i] + c[i];
      c3[i] = b5[i] * a5[i];
      c4[i] = a2[i] * a3[i];
      c5[i] = b5[i] * a2[i];
      c[i] = a[i] + c3[i];
      c2[i] = a[i] + c4[i];
      a5[i] = a[i] + a4[i];
      a[i] = a[i] + b5[i] + a[i] * a2[i] * a3[i] * a4[i] 
      * a5[i] * c[i] * c2[i] * c3[i] * c4[i] * c5[i]
      * d[i] * d2[i] * d3[i] * d4[i] * d5[i];
    }
}


Loop body:

        vsetvli t1,t4,e8,mf4,ta,ma
        vle32.v v1,0(a1)
        vle32.v v4,0(a2)
        vle32.v v2,0(s10)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v4,v4,v1
        vsetvli zero,t4,e32,m1,ta,ma
        vle32.v v7,0(s9)
        vle32.v v1,0(a4)
        vse32.v v4,0(t0)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v2,v7,v2
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v2,0(t5)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v5,v2,v4
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v5,0(s3)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v3,v5,v4
        vsetvli zero,t4,e32,m1,ta,ma
        vle32.v v9,0(a5)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v3,v3,v4
        vsetvli zero,t4,e32,m1,ta,ma
        vle32.v v6,0(a7)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v1,v9,v1
        vsetvli zero,t4,e32,m1,ta,ma
        vle32.v v8,0(s8)
        vse32.v v1,0(a3)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v6,v8,v6
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v6,0(a6)
        vsetvli t3,zero,e32,m1,ta,ma
        vmul.vv v11,v5,v4
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v11,0(s4)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v13,v11,v3
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v13,0(s6)
        vsetvli t3,zero,e32,m1,ta,ma
        vmul.vv v10,v6,v1
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v10,0(s5)
        vsetvli t3,zero,e32,m1,ta,ma
        vmul.vv v12,v1,v4
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v12,0(t2)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v9,v1,v9
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v9,0(s0)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v8,v6,v8
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v8,0(s1)
        vsetvli t3,zero,e32,m1,ta,ma
        vadd.vv v7,v2,v7
        vsetvli zero,t4,e32,m1,ta,ma
        vse32.v v7,0(s2)
        vsetvli t3,zero,e32,m1,ta,ma
        vmul.vv v1,v3,v1
        vmul.vv v1,v1,v6
        vadd.vv v6,v10,v3
        vmul.vv v1,v1,v2
        vadd.vv v2,v3,v2
        vmul.vv v1,v1,v2
        vmul.vv v1,v1,v13
        vsetvli zero,t1,e32,m1,ta,ma
        vse32.v v6,0(s7)
        vsetvli t3,zero,e32,m1,ta,ma
        vmul.vv v1,v1,v6
        vsetvli zero,t1,e32,m1,ta,ma
        vse32.v v2,0(t6)
        vsetvli t3,zero,e32,m1,ta,ma
        vmul.vv v1,v1,v11
        vsetvli zero,t1,e32,m1,ta,ma
        vle32.v v2,0(s11)
        vsetvli t3,zero,e32,m1,ta,ma
        slli    t3,t1,2
        vmul.vv v1,v1,v10
        vadd.vv v3,v3,v4
        vmul.vv v1,v1,v12
        sub     t4,t4,t1
        vmul.vv v1,v1,v2
        vmul.vv v1,v1,v9
        vmul.vv v1,v1,v8
        vmul.vv v1,v1,v7
        vmadd.vv        v5,v1,v3
        vsetvli zero,t1,e32,m1,ta,ma
        vse32.v v5,0(a0)

So many redundant AVL toggling. Ideally, it should be only a single vsetvl
instruction in the header of the loop. All other vsetvls should be elided.

It's known issue for a long time.
And I will be working on it recently base on refactored VSETVL PASS.


---


### compiler : `gcc`
### title : `range-op misses {L,R}ROTATE_EXPR handling`
### open_at : `2023-10-20T09:36:54Z`
### last_modified_date : `2023-10-20T09:46:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111893
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `14.0`
### severity : `normal`
### contents :
Seen when trying to check whether we exercise the constraint on the rotate amount operand.

I also noticed we miss generic.texi documentation for those codes.


---


### compiler : `gcc`
### title : `Missed vectorization opportunity`
### open_at : `2023-10-20T13:57:39Z`
### last_modified_date : `2023-10-23T09:00:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111894
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `13.2.0`
### severity : `normal`
### contents :
Consider the following code that uses views to implement a two dimensional `iota`:

    #include <array>
    #include <ranges>
    
    template<std::integral T>
    std::ranges::range auto iota2D(T xbound, T ybound) {
        auto fn = [=](T idx) { return std::tuple{idx / ybound, idx % ybound}; };
        return std::views::iota(T{0}, xbound * ybound) | std::views::transform(fn);
    }
    
    constexpr std::size_t N = 20;
    std::array<std::array<int, N>, N> data;
    
    __attribute__((noinline)) void init1() {
        for (auto i : std::views::iota(size_t{}, N)) {
            for (auto j : std::views::iota(size_t{}, N)) {
                data[i][j] = 123;
            }
        }
    }
    
    __attribute__((noinline)) void init2() {
        for (auto [i, j] : iota2D(N,N)) {
            data[i][j] = 123;
        }
    }

Using gcc 13.2 with -O3, we see that the code using a nested loop is nicely vectorized:

    init1():
            movdqa  xmm0, XMMWORD PTR .LC0[rip]
            mov     eax, OFFSET FLAT:data
    .L2:
            movaps  XMMWORD PTR [rax], xmm0
            add     rax, 80
            movaps  XMMWORD PTR [rax-64], xmm0
            movaps  XMMWORD PTR [rax-48], xmm0
            movaps  XMMWORD PTR [rax-32], xmm0
            movaps  XMMWORD PTR [rax-16], xmm0
            cmp     rax, OFFSET FLAT:data+1600
            jne     .L2
            ret

The code using iota2D is not vectorized:

    init2():
            xor     eax, eax
    .L6:
            mov     DWORD PTR data[0+rax*4], 123
            add     rax, 1
            cmp     rax, 400
            jne     .L6
            ret

Although GCC 13 produces much higher quality assembly than previous versions, it fails to vectorize the loop.


---


### compiler : `gcc`
### title : `-O3 vectorization terribly pessimizes the code for an already unrolled loop`
### open_at : `2023-10-20T23:50:02Z`
### last_modified_date : `2023-10-24T08:08:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=111905
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `13.2.0`
### severity : `normal`
### contents :
https://godbolt.org/z/rK4nEWovc

With -O2, the code is the way you'd expect it to be - with performance benefit for the code "manually unrolled", that processes data in chunks (using assumption that w > 16).

With -O3 however, auto-vecorization kicks in for the already unrolled loop, with the results being abysmal, a lot of unnecessary checks and (what I assume to be) dead code.

Clang doesn't seem to have this problem.


---
