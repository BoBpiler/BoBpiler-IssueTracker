### Total Bugs Detected: 4649
### Current Chunk: 11 of 30
### Bugs in this Chunk: 160 (From bug 1601 to 1760)
---


### compiler : `gcc`
### title : `x86 rdrand: flags not used directly when branching on success/failure`
### open_at : `2017-09-26T14:56:34Z`
### last_modified_date : `2021-12-21T11:25:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82328
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
#include <immintrin.h>
unsigned long long use_intrinsic(void) {
    unsigned long long rand;
    while(!_rdrand64_step(&rand));  // FIXME: limited retry in case RNG is broken
    return rand;
}
// https://godbolt.org/g/x7mUvj
gcc 8.0.0 20170926 -O3 -mrdrnd

        movl    $1, %edx
.L4:
        rdrand  %rax
        movq    %rax, -8(%rsp)     # spill to memory, really?
        cmovc   %edx, %eax
        testl   %eax, %eax
        je      .L4
        movq    -8(%rsp), %rax
        ret

Note that RDRAND (http://felixcloutier.com/x86/RDRAND.html) indicates failure by clearing CF *and* putting 0 in the destination register.  So this code is correct (returning a valid RDRAND result even if it was zero), just much worse than clang's:

.LBB1_1:
        rdrandq %rax
        jae     .LBB1_1
        retq


---


### compiler : `gcc`
### title : `Inefficient movabs instruction`
### open_at : `2017-09-27T08:44:25Z`
### last_modified_date : `2023-08-04T00:28:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82339
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
At least on i7-5960X in the following testcase:
__attribute__((noinline, noclone)) unsigned long long int
foo (int x)
{
  asm volatile ("" : : : "memory");
  return 1ULL << (63 - x);
}

__attribute__((noinline, noclone)) unsigned long long int
bar (int x)
{
  asm volatile ("" : : : "memory");
  return (1ULL << 63) >> x;
}

__attribute__((noinline, noclone)) unsigned long long int
baz (int x)
{
  unsigned long long int y = 1;
  asm volatile ("" : "+r" (y) : : "memory");
  return (y << 63) >> x;
}

int
main (int argc, const char **argv)
{
  int i;
  if (argc == 1)
    for (i = 0; i < 1000000000; i++)
      asm volatile ("" : : "r" (foo (13)));
  else if (argc == 2)
    for (i = 0; i < 1000000000; i++)
      asm volatile ("" : : "r" (bar (13)));
  else if (argc == 3)
    for (i = 0; i < 1000000000; i++)
      asm volatile ("" : : "r" (baz (13)));
  return 0;
}

baz is fastest as well as shortest.
So I think we should consider using movl $cst, %edx; shlq $shift, %rdx instead of movabsq $(cst << shift), %rdx.

Unfortunately I can't find in Agner Fog MOVABS and for MOV r64,i64 there is too little information, so it is unclear on which CPUs it is beneficial.
For -Os, if the destination is a %rax to %rsp register, it is one byte shorter (5+4 vs 10), for %r8 to %r15 it is the same size.
For speed optimization, the disadvantage is obviously that the shift clobbers flags register.

Peter, any information on what the MOV r64,i64 latency/throughput on various CPUs vs. MOV r32,i32; SHL r64,i8 is?


---


### compiler : `gcc`
### title : `auto-vectorizing pack of 16->8 has a redundant AND after a shift`
### open_at : `2017-09-28T21:11:48Z`
### last_modified_date : `2021-08-25T03:21:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82356
### status : `NEW`
### tags : `missed-optimization, ssemmx`
### component : `rtl-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
#include <stdint.h>
void pack_high8_baseline(uint8_t *__restrict__ dst, const uint16_t *__restrict__ src, size_t bytes) {
  uint8_t *end_dst = dst + bytes;
  do{
     *dst++ = *src++ >> 8;
  } while(dst < end_dst);
}

https://godbolt.org/g/yoJZ3C
gcc -O3  auto-vectorizes to this inner loop:

.L5:
        movdqa  (%rdx,%rax,2), %xmm0
        movdqa  16(%rdx,%rax,2), %xmm1
        psrlw   $8, %xmm0
        pand    %xmm2, %xmm0             # Redundant with the shift
        psrlw   $8, %xmm1
        pand    %xmm2, %xmm1             # Redundant with the shift
        packuswb        %xmm1, %xmm0
        movups  %xmm0, (%rcx,%rax)
        addq    $16, %rax
        cmpq    %rsi, %rax
        jne     .L5

This is mostly good, but the PAND instructions are redundant, because psrlw by 8 already leaves the high byte of each 16-bit element zeroed.

The same extra AND is present when auto-vectorizing for AVX2 (but not AVX512, where it uses a different strategy.)  Other than that, the AVX2 vectorization strategy looks very good (packus ymm, then vpermq to fix the result).

If the input is 32B-aligned (or 64B aligned for AVX2), one of the load+shifts can be *replaced* with an AND + unaligned load offset by -1.  This avoids a bottleneck on shift throughput (at least with unrolling it does; without unrolling we bottleneck on the front-end except on Ryzen).  It's even better with AVX, because load+AND can fold into one instruction.

See https://stackoverflow.com/a/46477080/224132 for more details.

This C source produces the inner loop that I think should be very good across K10, Bulldozer, Ryzen,  Nehalem, Sandybridge, HSW/SKL, Jaguar, Atom, and Silvermont.  (With SSE2 or AVX.)  i.e. this should be great for tune=generic after reaching a 32B boundary.

Not great on Core2 or K8 where non-cache-line-crossing movdqu costs more.

// take both args as uint8_t* so we can offset by 1 byte to replace a shift with an AND
// if src is 32B-aligned, we never have cache-line splits
void pack_high8_alignhack(uint8_t *restrict dst, const uint8_t *restrict src, size_t bytes) {
  uint8_t *end_dst = dst + bytes;
  do{
     __m128i v0 = _mm_loadu_si128((__m128i*)src);  // this load should be aligned
     __m128i v1_offset = _mm_loadu_si128(1+(__m128i*)(src-1));
     v0 = _mm_srli_epi16(v0, 8);
     __m128i v1 = _mm_and_si128(v1_offset, _mm_set1_epi16(0x00FF));
     __m128i pack = _mm_packus_epi16(v0, v1);
     _mm_storeu_si128((__m128i*)dst, pack);
     dst += 16;
     src += 32;  // 32 bytes
  } while(dst < end_dst);
}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] SPEC CPU2006 436.cactusADM ~7% performance deviation with trunk@251713`
### open_at : `2017-09-29T14:10:56Z`
### last_modified_date : `2023-07-07T10:32:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82362
### status : `NEW`
### tags : `deferred, missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
r251713 brings reasonable improvement to alloca. However there is a side effect of this patch - 436.cactusADM performance became unstable when compiled with
    -Ofast -march=core-avx2 -mfpmath=sse -funroll-loops
The impact is more noticeable when compiled with auto-parallelization
    -ftree-parallelize-loops=N

Comparing performance for particular 7-runs
(relative to median performance of r251711):
r251711: 92,8%   92,9%   93,0%   106,7%  107,0%  107,0%  107,2%
r251713: 99,5%   99,6%   99,8%   100,0%  100,3%  100,6%  100,6%

r251711 is prettty stable, while r251713 is +7% faster on some runs and -7% slower on other.

There are few dynamic arrays in the body of Bench_StaggeredLeapfrog2 sub in StaggeredLeapfrog2.fppized.f.
When compiled with "-fstack-arrays" (default for "-Ofast") arrays are allocated by alloca.
Allocated memory size is rounded-up to 16-bytes in r251713 with code like "size = (size + 15) & -16".
In prior revisions it differs in just one byte: "size = (size + 22) & -16"
Which actually may just waste extra 16 bytes for each array depending on initial "size" value.

Actual r251713 code, built with
    gfortran -S -masm=intel -o StaggeredLeapfrog2.fppized_r251713.s
    -O3 -fstack-arrays -march=core-avx2 -mfpmath=sse -funroll-loops
    -ftree-parallelize-loops=8 StaggeredLeapfrog2.fppized.f
------------
lea rax, [15+r13*8]             ; size = <...> + 15
shr rax, 4                      ; zero-out
sal rax, 4                      ;     lower 4 bits
sub rsp, rax
mov QWORD PTR [rbp-4984], rsp   ; Array 1
sub rsp, rax
mov QWORD PTR [rbp-4448], rsp   ; Array 2
sub rsp, rax 
mov QWORD PTR [rbp-4784], rsp   ; Array 3 ... and so on
------------

Aligning rsp to cache line size (on each allocation or even once in the beginning) brings performance to stable high values:
------------
lea rax, [15+r13*8] 
shr rax, 4
sal rax, 4
shr rsp, 6                      ; Align rsp to
shl rsp, 6                      ;     64-byte border
sub rsp, rax
mov QWORD PTR [rbp-4984], rsp
sub rsp, rax
mov QWORD PTR [rbp-4448], rsp
sub rsp, rax 
mov QWORD PTR [rbp-4784], rsp
------------

64-byte aligned version performance
compared to the same median performance of r251711:
106,7%  107,0%  107,0%  107,1%  107,1%  107,2%  107,4%

Maybe what is necessary here is some kind of option to force array aligning for gfortran (like "-align array64byte" for ifort) ?


---


### compiler : `gcc`
### title : `stack locations are not consolidated if noreturn function is on the path`
### open_at : `2017-09-29T19:16:34Z`
### last_modified_date : `2023-05-14T23:37:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82365
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.0`
### severity : `enhancement`
### contents :
Created attachment 42264
testcase

We have noticed that gcc fails to reuse stack locations in presence of noreturn attribute in the call graph.

Basically, the attached testcase has
case 1: { struct XXX localvar; bla1; break; }
case 2: { struct XXX localvar; bla2; break; }
case 3: { struct XXX localvar; bal3; break; }

With noreturn attribute:

aarch64-linux-gnu-gcc -Wall -O2 -S bz-3265.c --param asan-stack=1 -Wframe-larger-than=1 
bz-3265.c: In function ‘em28xx_dvb_init’:
bz-3265.c:99:1: warning: the frame size of 480 bytes is larger than 1 bytes [-Wframe-larger-than=]


Without noreturn attribute:
aarch64-linux-gnu-gcc -Wall -O2 -S bz-3265.c --param asan-stack=1 -Wframe-larger-than=1 -DNONORETURN
bz-3265.c: In function ‘em28xx_dvb_init’:
bz-3265.c:99:1: warning: the frame size of 128 bytes is larger than 1 bytes [-Wframe-larger-than=]


The code fragment is extracted from the linux kernel where this causes more problems with using -fsanitize=kernel-address, where this causes excessive stack usage.

I used an aarch64 compiler here, but Arnd observed similar problems on x86_64 too.


---


### compiler : `gcc`
### title : `#pragma GCC optimize is not applied to openmp-generated functions`
### open_at : `2017-09-30T13:59:28Z`
### last_modified_date : `2020-11-25T06:26:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82374
### status : `ASSIGNED`
### tags : `missed-optimization, openmp, wrong-code`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `normal`
### contents :
#pragma GCC optimize (and the function attribute optimize) is not applied to functions generated with openmp.

This is example code. Compile it with -fopenmp (with or without -O2) and you'll notice that the function "f_nonomp" is properly vectorized (i.e. it uses the "addps" instruction) and the function "f" is not vectorized (i.e. the generated function f._omp_fn.0 performs the addition on single elements using the "addss" instruction and no vectorization is done).

If we use __attribute((optimize("-O2","-ftree-vectorize"))) on the function "f", then again, the optimization is not applied to openmp-generated function "f._omp_fn.0".

If we use "-O3" or "-ftree-vectorize" on the command-line, then both functions are properly vectorized.


#pragma GCC optimize("-O2", "-ftree-vectorize")

#define SIZE    (1024 * 1024 * 1024)

float a[SIZE];
float b[SIZE];
float c[SIZE];

void f(void)
{
        int i;
#pragma omp parallel for
        for (i = 0; i < SIZE; i++)
                c[i] = a[i] + b[i];
}

void f_nonomp(void)
{
        int i;
        for (i = 0; i < SIZE; i++)
                c[i] = a[i] + b[i];
}


---


### compiler : `gcc`
### title : `Pointer imposes an optimization barrier`
### open_at : `2017-10-02T11:01:49Z`
### last_modified_date : `2021-12-12T08:57:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82394
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Following code

unsigned my_sorted_array() {
    unsigned data[6] = {1, 0, 7, 7, 7, 0};
    
    // Hand written std::min_element

    unsigned m = data[0];
    for (unsigned* it = data + 1; it != data + 6; ++it) {
        if (*it < m) {
            m = *it;
        }
    }

    return m;
}

Produces an unnecessary long assembly:

my_sorted_array():
  movabs rax, 30064771079
  lea rsi, [rsp-40]
  xor ecx, ecx
  mov QWORD PTR [rsp-32], rax
  lea rax, [rsp-40]
  xor edi, edi
  add rsi, 24
  mov QWORD PTR [rsp-40], 1
  mov QWORD PTR [rsp-24], 7
  lea rdx, [rax+4]
  mov eax, 1
  cmp ecx, eax
  cmovb eax, edi
  add rdx, 4
  cmp rdx, rsi
  je .L1
.L6:
  mov ecx, DWORD PTR [rdx]
  cmp ecx, eax
  cmovb eax, edi
  add rdx, 4
  cmp rdx, rsi
  jne .L6
.L1:
  rep ret


Changing pointers to indexes makes the assembly much better:

unsigned my_sorted_array() {
    unsigned data[6] = {1, 0, 7, 7, 7, 0};
    
    // Hand written std::min_element

    unsigned m = data[0];
    for (unsigned i = 1; i < 6; ++i) {
        if (data[i] < m) {
            m = data[i];
        }
    }

    return m;
}


Assembly:
my_sorted_array():
  xor eax, eax
  ret


Clang in both cases produces the following:
my_sorted_array(): # @my_sorted_array()
  xor eax, eax
  ret


---


### compiler : `gcc`
### title : `Division on a constant is suboptimal because of not using imul instruction`
### open_at : `2017-10-03T17:32:45Z`
### last_modified_date : `2019-06-16T19:53:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82418
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Following code 

unsigned my_div(unsigned a, unsigned b) {
    return a / 100;
}

Produces assembly:

my_div(unsigned int, unsigned int):
  mov eax, edi
  mov edx, 1374389535
  mul edx
  mov eax, edx
  shr eax, 5
  ret

Clang uses imul instead:

my_div(unsigned int, unsigned int): # @my_div(unsigned int, unsigned int)
  mov eax, edi
  imul rax, rax, 1374389535
  shr rax, 37
  ret


---


### compiler : `gcc`
### title : `Missed tree-slp-vectorization on -O2 and -O3`
### open_at : `2017-10-04T10:24:33Z`
### last_modified_date : `2021-09-27T08:26:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82426
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
Created attachment 42299
vectslp.cpp

The attached example is a simple matrix multiplication. With -O3 or -O2 -ftree-slp-vectorize the basic-block is not vectorized.

Oddly, with -Os -ftree-slp-vectorize it is.


---


### compiler : `gcc`
### title : `strcpy to stpcpy transformation disabled in strict mode`
### open_at : `2017-10-04T14:05:33Z`
### last_modified_date : `2021-03-09T20:05:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82429
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
In https://gcc.gnu.org/ml/gcc/2017-10/msg00010.html Jakub explains that the strcpy to stpcpy optimizing transformation that is normally disabled in strict conformance modes (e.g., with -std=c11 rather than -std=gnu11) is meant to be enabled by defining _GNU_SOURCE, or _POSIX_C_SOURCE=200809, or _XOPEN_SOURCE=700, or various other feature test macros that cause stpcpy to be declared in system headers.

However, as the test case below shows (from https://gcc.gnu.org/ml/gcc/2017-10/msg00015.html in the same thread), this is not what actually happens.  What appears to be necessary in addition to defining one of the feature test macros above is also explicitly declaring the stpcpy function in the program.  A declaration alone in one of the system headers is not sufficient.  This seems like a bug.  Explicitly declaring a standard function that's already declared in a system header shouldn't have an impact on the quality of emitted code.

$ cat z.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout -std=c11 -D_GNU_SOURCE z.c
#include <string.h>

#if STPCPY
extern char* stpcpy (char*, const char*);
#endif

void __attribute__ ((noclone)) f (char *d, const char *s)
{
  strcpy (d, s);   // with -D_GNU_SOURCE strcpy is expected to be transformed to stpcpy

  if (__builtin_strlen (d) != __builtin_strlen (s))
     __builtin_abort ();
}

;; Function f (f, funcdef_no=4, decl_uid=1972, cgraph_uid=4, symbol_order=4)

__attribute__((noclone))
f (char * d, const char * s)
{
  long unsigned int _1;
  long unsigned int _2;

  <bb 2> [100.00%] [count: INV]:
  strcpy (d_4(D), s_5(D));   // strcpy not transformed to stpcpy
  _1 = __builtin_strlen (d_4(D));
  _2 = __builtin_strlen (s_5(D));
  if (_1 != _2)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  return;

}


$ gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout -std=c11 -D_GNU_SOURCE -DSTPCPY z.c
;; Function f (f, funcdef_no=4, decl_uid=1975, cgraph_uid=4, symbol_order=4)

__attribute__((noclone))
f (char * d, const char * s)
{
  long unsigned int _1;
  long unsigned int _2;
  char * _8;
  long unsigned int _9;
  long unsigned int _10;

  <bb 2> [100.00%] [count: INV]:
  _8 = __builtin_stpcpy (d_4(D), s_5(D));   // strcpy transformed to stpcpy 
  _9 = (long unsigned int) _8;
  _10 = (long unsigned int) d_4(D);
  _1 = _9 - _10;
  _2 = __builtin_strlen (s_5(D));
  if (_1 != _2)
    goto <bb 3>; [0.04%] [count: 0]
  else
    goto <bb 4>; [99.96%] [count: INV]

  <bb 3> [0.04%] [count: 0]:
  __builtin_abort ();

  <bb 4> [99.96%] [count: INV]:
  return;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Missed equalities in dr_group_sort_cmp`
### open_at : `2017-10-06T09:40:36Z`
### last_modified_date : `2023-08-04T08:09:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82446
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
The fix for PR82397 causes a few missed equalities and thus different vectorizer group access detections because operand_equal_p was handling commutated operands
and data_ref_compare_tree does not.


---


### compiler : `gcc`
### title : `[IVOPTS] Consider removing cmp instruction while iterating on an array of known bound`
### open_at : `2017-10-06T10:27:26Z`
### last_modified_date : `2020-09-27T06:04:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82447
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Following code 

unsigned loop_read(unsigned char* a) {
    const unsigned size = 128;

    unsigned sum = 0;
    for (unsigned i = 0; i < size; ++i) {
        sum += a[i];
    }

    return sum;
}

Generates assembly:

loop_read(unsigned char*):
  lea rcx, [rdi+128]
  xor eax, eax
.L2:
  movzx edx, BYTE PTR [rdi]
  add rdi, 1
  add eax, edx
  cmp rdi, rcx  <=== This could be avoided
  jne .L2
  rep ret


The trick is to iterate from -128 to 0 and calling "jne .L2" right after the increment. Here's how Clang does that:

loop_read(unsigned char*): # @loop_read(unsigned char*)
  xor eax, eax
  mov rcx, -128
.LBB0_1: # =>This Inner Loop Header: Depth=1
  movzx edx, byte ptr [rdi + rcx + 128]
  add eax, edx
  add rcx, 1
  jne .LBB0_1
  ret


---


### compiler : `gcc`
### title : `Possible future performance regression in x86 for 64-bit constant expansions`
### open_at : `2017-10-06T16:57:08Z`
### last_modified_date : `2023-08-04T00:30:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82454
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Created attachment 42317
test1

As per the discussions on 

https://gcc.gnu.org/ml/gcc-patches/2017-04/msg00570.html
https://gcc.gnu.org/ml/gcc-patches/2017-08/msg00039.html
https://gcc.gnu.org/ml/gcc-patches/2017-09/msg01736.html

The patch to add a simplification of 1U << (31 - x) for PR 80131, may lead to a performance regression for x86.

I am adding the tests that Jakub mentioned as a runtime performance regression tests.


---


### compiler : `gcc`
### title : `AVX512BW instruction costs: vpmovwb is 2 uops on Skylake and not always worth using vs. vpack + vpermq lane-crossing fixup`
### open_at : `2017-10-07T00:16:56Z`
### last_modified_date : `2019-10-30T03:21:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82459
### status : `UNCONFIRMED`
### tags : `missed-optimization, ssemmx`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
gcc bottlenecks on shuffle uops when auto-vectorizing this for skylake-avx512

* Perhaps the cost model is wrong for vpmovwb (it's 2 port5 uops), or gcc doesn't consider any cheaper alternatives.  My version with 2x shift + vpacksswb + vpermq has 3x the theoretical throughput (with hot caches).  In general, AVX512BW lane crossing shuffles of 8 or 16-bit elements are multi-uop on SKX, but in-lane byte/word shuffles are single-uop just like their AVX2 versions.

* Using vmovdqu8 as a store costs a port5 ALU uop even with no masking, according to Intel (not tested).  We should always use AVX512F vmovdqu32 or 64 for unmasked loads/stores, not AVX512BW vmovdqu8 or 16.  Intel's docs indicate that current hardware doesn't handle unmasked vmovdqu8/16 as efficiently as 32/64, and there's no downside.

* Using vinserti64x4 instead of 2 separate stores is worse because it makes the shuffle bottleneck worse, and 2 stores wouldn't bottleneck on load/store throughput.  (Avoiding vpmovwb makes this moot in this case, but presumably whatever decided to shuffle + store instead of store + store will make that mistake in other cases too.)

 SKX shuts down port 1 (except for scalar integer) when there are 512b uops in flight, so extra loads/stores are relatively cheaper than using more ALU uops, compared to 256b or 128b vectors where the back-end can keep up even when 3 of the 4 uops per clock are vector-ALU (if they go to different ports).

#include <stdint.h>
#include <stddef.h>
void pack_high8_baseline(uint8_t *__restrict__ dst, const uint16_t *__restrict__ src, size_t bytes) {
  uint8_t *end_dst = dst + bytes;
  do{
     *dst++ = *src++ >> 8;
  } while(dst < end_dst);
}

// https://godbolt.org/g/kXjEp1
gcc8 -O3 -march=skylake-avx512

.L5:  # inner loop
        vmovdqa64       (%rsi,%rax,2), %zmm0
        vmovdqa64       64(%rsi,%rax,2), %zmm1
        vpsrlw  $8, %zmm0, %zmm0             # memory operand not folded: bug 82370
        vpsrlw  $8, %zmm1, %zmm1
        vpmovwb %zmm0, %ymm0                 # 2 uops each
        vpmovwb %zmm1, %ymm1
        vinserti64x4    $0x1, %ymm1, %zmm0, %zmm0
        vmovdqu8        %zmm0, (%rcx,%rax)   # Intel says this is worse than vmovdqu64
        addq    $64, %rax
        cmpq    %rax, %rdi         # using an indexed addr mode, but still doing separate add/cmp
        jne     .L5

IACA says gcc's loop will run at one 64B store per 6 clocks, bottlenecked on 6 port5 uops (including the vmovdqu8.  vmovdqu64 gives one store per 5 clocks, still bottlenecked on port5).  Using 2 stores instead of vinserti64x4 gives us one store per 4 clocks.  (Still twice as slow as with vpacksswb + vpermq, which produces one 512b vector per 2 shuffle uops instead of one 256b vector per 2 shuffle uops.)

See https://stackoverflow.com/questions/26021337/what-is-iaca-and-how-do-i-use-it for more about Intel's static analysis tool.


related: pr 82370 mentions vectorization strategies for this.

Fortunately gcc doesn't unroll the startup loop to reach an alignment boundary.  (And BTW, aligned pointers are more important with AVX512 than AVX2, in my testing with manual vectorization of other code on Skylake-avx512.)  Of course, a potentially-overlapping unaligned first vector would be much better than a scalar loop here.

----

Anyway, does gcc know that vpmovwb %zmm, %ymm is 2 uops for port 5, while vpackuswb zmm,zmm,zmm in-lane 2-input shuffle is 1 uop (for port 5)?  The xmm source version is single-uop, because it's in-lane.

Source: Intel's IACA2.3, not testing on real hardware.  SKX port-assignment spreadsheet: https://github.com/InstLatx64/InstLatx64/blob/master/AVX512_SKX_PortAssign_v102_PUB.ods
It's based on IACA output for uops, but throughputs and latencies are from real hardware AIDA64 InstLatx64, with a 2nd column for Intel's published tput/latency (which as usual doesn't always match).  vpmovwb real throughput is one per 2 clocks, which is consistent with being 2 uops for p5.

It makes some sense from a HW-design perspective that all lane-crossing shuffles with element size smaller than 32-bit are multi-uop.  It's cool that in-lane AVX512 vpshufb zmm  vpacksswb zmm are single-uop, but it means it's often better to use more instructions to do the same work in fewer total shuffle uops.  (Any loop that involves any shuffling can *easily* bottleneck on shuffle throughput.)

Related: AVX512 merge-masking can something into a 2-input shuffle.  But we vpsrlw  $8, 64(%rsi), %zmm0{k1}  doesn't work to get all our data into one vector, because it masks at word granularity, not byte.  vmovdqu8 with masking needs an ALU uop (on SKX according to IACA).

-------------

Here's a hand-crafted efficient version of the inner loop.  It doesn't use any weird tricks (I haven't thought of any that are actually a win on SKX), so it should be possible to get gcc to emit something like this.

.Lloop:
    vpsrlw  $8, 0(%rsi), %zmm0
    vpsrlw  $8, 64(%rsi), %zmm1
    vpackuswb %zmm1, %zmm0, %zmm0            # 1 uop for a 2-input shuffle
    vpermq   %zmm7, %zmm0, %zmm0             # lane-crossing fixup for vpackuswb
    vmovdqu64 %zmm0, (%rdi, %rdx)

    add   $(2*64), %rsi
    add   $64, %rdx          # counts up towards zero
    jnc .Lloop

Note that the folded loads use non-indexed addressing modes so they can stay micro-fused.

The store will stay micro-fused even with an indexed addressing mode, so we can count an index up towards zero (and index from the end of the array) saving a CMP instruction in the loop.  (add/jnc will macro-fuse on SKX).  A second pointer-increment + cmp/jcc would be ok.

IACA thinks that indexed stores don't stay micro-fused, but that's only true for SnB/IvB.  Testing on Haswell/Skylake (desktop) shows they do stay fused, and IACA is wrong about that.  I think it's a good guess that AVX512 indexed stores will not un-laminate either.

IACA analysis says it will run at 1x 64B store per 2 clocks.  If the store stays micro-fused, it's really only 7 fused-domain uops per clock, so we have front-end bandwidth to spare and the only bottleneck is on the ALU ports (p0 and p5 saturated with shift and shuffle respectively).

Because of the ALU bottleneck, I didn't need the store to be able to run on p7.  p2/p3 only need to handle 1.5 uops per 2 clocks.  Using a non-indexed store addressing mode would let it use p7, but that increases loop overhead slightly.


$ iaca -arch SKX testloop.o

Intel(R) Architecture Code Analyzer Version - 2.3 build:246dfea (Thu, 6 Jul 2017 13:38:05 +0300)
Analyzed File - testloop.o
Binary Format - 64Bit
Architecture  - SKX
Analysis Type - Throughput

Throughput Analysis Report
--------------------------
Block Throughput: 2.00 Cycles       Throughput Bottleneck: FrontEnd

Port Binding In Cycles Per Iteration:
---------------------------------------------------------------------------------------
|  Port  |  0   -  DV  |  1   |  2   -  D   |  3   -  D   |  4   |  5   |  6   |  7   |
---------------------------------------------------------------------------------------
| Cycles | 2.0    0.0  | 1.0  | 1.5    1.0  | 1.5    1.0  | 1.0  | 2.0  | 1.0  | 0.0  |
---------------------------------------------------------------------------------------

N - port number or number of cycles resource conflict caused delay, DV - Divider pipe (on port 0)
D - Data fetch pipe (on ports 2 and 3), CP - on a critical path
F - Macro Fusion with the previous instruction occurred
* - instruction micro-ops not bound to a port
^ - Micro Fusion happened

| Num Of |                    Ports pressure in cycles                     |    |
|  Uops  |  0  - DV  |  1  |  2  -  D  |  3  -  D  |  4  |  5  |  6  |  7  |    |
---------------------------------------------------------------------------------
|   2^   | 1.0       |     | 1.0   1.0 |           |     |     |     |     | CP | vpsrlw zmm0, zmmword ptr [rsi], 0x8
|   2^   | 1.0       |     |           | 1.0   1.0 |     |     |     |     | CP | vpsrlw zmm1, zmmword ptr [rsi+0x40], 0x8
|   1    |           |     |           |           |     | 1.0 |     |     | CP | vpackuswb zmm0, zmm0, zmm1
|   1    |           |     |           |           |     | 1.0 |     |     | CP | vpermq zmm0, zmm0, zmm7
|   2    |           |     | 0.5       | 0.5       | 1.0 |     |     |     |    | vmovdqu64 zmmword ptr [rdi+rdx*1], zmm0
|   1    |           | 1.0 |           |           |     |     |     |     |    | add rsi, 0x80
|   1    |           |     |           |           |     |     | 1.0 |     |    | add rdx, 0x40
|   0F   |           |     |           |           |     |     |     |     |    | jnb 0xffffffffffffffd3
Total Num Of Uops: 10

(The count is unfused-domain, which is pretty dumb to total up.)

Source for this:

#if 1
#define IACA_start  mov $111, %ebx; .byte 0x64, 0x67, 0x90
#define IACA_end    mov $222, %ebx; .byte 0x64, 0x67, 0x90
#else
#define IACA_start
#define IACA_end
#endif

.global pack_high8_avx512bw
pack_high8_avx512bw:   # (dst, src, size)

#define unroll 1   // unroll factor

.altmacro
.macro packblock count
    .if \count
        packblock %(count-1)
    .endif
#    vmovdqu8 \count*2*64 + 1(%rsi),  %ymm0 {%k1}{z}   # IACA says: x/y/zmm load uses an ALU port with masking, otherwise not.
#    vmovdqu8 \count*2*64 + 65(%rsi), %ymm1 {%k1}{z}
    vpsrlw  $8, \count*2*64 + 0(%rsi), %zmm0
    vpsrlw  $8, \count*2*64 + 64(%rsi), %zmm1
    vpackuswb %zmm1, %zmm0, %zmm0            # 1 uop for a 2-input shuffle
    vpermq   %zmm7, %zmm0, %zmm0             # lane-crossing fixup for vpackuswb
    vmovdqu64 %zmm0, 64*\count(%rdi, %rdx)
.endm

    # set up %zmm7
    #movabs $0x5555555555555555, %rax
    #kmovq   %rax, %k1              # for offset vmovdqu8 instead of shift

    # mov   $1024, %rdx

    add   %rdx, %rdi
    neg   %rdx         # then use (%rdi, %rdx) for store addresses?  No port 7, but should micro-fuse.
    IACA_start
.Lloop:
    packblock (unroll-1)
    add   $(unroll*2*64), %rsi
          # add   $(unroll*1*64), %rdi
    add   $(unroll*1*64), %rdx
    jnc .Lloop
IACA_end

    # not shown: unaligned and/or cleanup loop
    ret


---


### compiler : `gcc`
### title : `missing popcount builtin detection`
### open_at : `2017-10-08T23:56:29Z`
### last_modified_date : `2019-02-12T18:51:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82479
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc does not have support to detect builtin pop count. As a results, gcc generates bad code for

int PopCount (long b) {
    int c = 0;
 
    while (b) {
        b &= b - 1;
        c++;
    }
    return c;
}

clang seems to do that and generates (for aarch64):

_Z8PopCounty:
                fmov     d0, x0
                cnt          v0.8b, v0.8b
                uaddlv  h0, v0.8b
                fmov     w0, s0
                ret


---


### compiler : `gcc`
### title : `Eliminating loop from summation`
### open_at : `2017-10-11T15:13:47Z`
### last_modified_date : `2021-08-07T06:42:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82519
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Given:

int sum (int n)
{
  int result = 0;
  for (int i = 0; i < n; i++)
    result += i;
  return result;
}

clang manages to eliminate the loop altogether at -O1 and above:

sum(int): # @sum(int)
  testl %edi, %edi
  jle .LBB0_1
  leal -1(%rdi), %eax
  leal -2(%rdi), %ecx
  imulq %rax, %rcx
  shrq %rcx
  leal -1(%rcx,%rdi), %eax
  retq
.LBB0_1:
  xorl %eax, %eax
  retq

and can do the same for various variants of the loop body.

(Seen in Matt Godbolt's recent CppCon 2017 talk, at about 43:10 onwards:
    https://youtu.be/bSkpMdDe4g4?t=2590 )


---


### compiler : `gcc`
### title : `inefficient code generation for copy loop on falkor`
### open_at : `2017-10-12T22:26:19Z`
### last_modified_date : `2021-08-29T20:56:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82533
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 42348
testcase, to reproduce compile with -mcpu=falkor -O2 -ftree-vectorize

When lmbench stream copy is compiled with -O2 -ftree-vectorize -mcpu=falkor, the inner loop gets compiled to
.L4:
	ldr	q0, [x2, x3]
	str	q0, [x1, x3]
	add	x3, x3, 16
	cmp	x3, x4
	bne	.L4

The str qX [reg+reg] instruction is very inefficient on Falkor.  We get a 16% performance increase if we disable use of str qX [r+r], to get instead
.L4:
	ldr	q0, [x2, x3]
	add	x5, x1, x3
	add	x3, x3, 16
	cmp	x3, x4
	str	q0, [x5]
	bne	.L4

A proposed patch was posted to gcc-patches here
    https://gcc.gnu.org/ml/gcc-patches/2017-09/msg01547.html


---


### compiler : `gcc`
### title : `Optimize comparisons for __int128 on x86-64`
### open_at : `2017-10-17T13:00:56Z`
### last_modified_date : `2023-07-07T20:23:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82580
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `7.2.0`
### severity : `normal`
### contents :
Given the following simple code:

    bool foobar(unsigned __int128 lhs, unsigned __int128 rhs) {
        return lhs < rhs;
    }

GCC generates branchful code for x86-64 at -O3 optimization level:

    foobar(unsigned __int128, unsigned __int128):
    cmp rsi, rcx
    mov eax, 1
    jb .L2
    jbe .L6
    .L3:
    xor eax, eax
    .L2:
    rep ret
    .L6:
    cmp rdi, rdx
    jnb .L3
    rep ret

On the other hand, Clang is able to generate branchless code with just a few instructions at the same optimization level:

    foobar(unsigned __int128, unsigned __int128): # @foobar(unsigned __int128, unsigned __int128)
    cmp rdi, rdx
    sbb rsi, rcx
    setb al
    ret

The codegen results are equivalent for the other comparison instructions. Would it be possible to optimize these comparison instructions the same way for GCC?


---


### compiler : `gcc`
### title : `not quite optimal code for -2*x*y - 3*z: could use one less LEA for smaller code without increasing critical path latency for any input`
### open_at : `2017-10-17T16:18:54Z`
### last_modified_date : `2021-08-20T01:10:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82582
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
int foo32(int x, int y, int z) {
    return -2*x*y - 3*z;
}

gcc8.0.0 20171015 -O3   https://godbolt.org/g/tzBuHx

        imull   %esi, %edi            # x*y
        leal    0(,%rdx,4), %eax    # needs a disp32 = 0
        subl    %eax, %edx            # -3*z
        negl    %edi                  # -(x*y)
        leal    (%rdx,%rdi,2), %eax   # result

LEA runs on limited ports, and an index with no base needs a 4-byte disp32 = 0.
The critical-path latencies, assuming 2-operand imul is 3 cycles like on Intel:

x->res: imul, neg, lea = 5c
y->res: imul, neg, lea = 5c
z->res:  lea, sub, lea = 3c

This is better than gcc6.3 / gcc7.2 (which uses 3 LEA and is generally worse).  It's also different from gcc4/gcc5 (6c from x to result, but only 2c from z to result, so it's different but not worse or better in all cases).


clang5.0 does better: same latencies, smaller code size, and trades one LEA for an ADD:
        imull   %esi, %edi
        addl    %edi, %edi
        leal    (%rdx,%rdx,2), %eax
        negl    %eax
        subl    %edi, %eax

x->res: imul, add, sub = 5c
y->res: imul, add, sub = 5c
z->res:  lea, neg, sub = 3c



related: poor code-gen for 32-bit code with this.  I haven't checked other 32-bit architectures.

long long foo64(int x, int y, int z) {
    return -2LL*x*(long long)y - 3LL*(long long)z;
}
// also on the godbolt link

gcc -m32 uses a 3-operand imul-immediate for `-2`, but some clunky shifting for `-3`.  There's also a mull in there.

clang5.0 -m32 makes very nice code, using a one-operand imul for -3 and just shld/add + sub/sbb (plus some mov instructions).  One-operand mul/imul is 3 uops on Intel with 2 clock throughput, but ADC is 2 uops on Intel pre-Broadwell, so it's nice to avoid that.

related: add %esi,%esi / sbb %edi,%edi  is an interesting way to sign-extend a 32-bit input into a pair of registers while doubling it.  However, if it starts in eax,  cltd / add %eax,%eax is much better.  (sbb same,same is only recognized as dep-breaking on AMD Bulldozer-family and Ryzen.  On Intel it has a false dep on the old value of the register, not just CF).


---


### compiler : `gcc`
### title : `powerpc: Unnecessary copy of __ieee128 parameter`
### open_at : `2017-10-20T13:52:27Z`
### last_modified_date : `2019-05-13T20:58:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82636
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `7.2.1`
### severity : `normal`
### contents :
Tested with GCC 7.2.1 on powerpc64le.

The copy of vs36 (v4) to vs32 (v0) shouldn't be required, i.e. I'd expect to have xsmaddqp v4,v2,v3.

$ cat s_fmaf128-power9.c
__ieee128
__fmaf128_power9 (__ieee128 x, __ieee128 y, __ieee128 z)
{
  asm ("xsmaddqp\t%0, %1, %2" : "+v" (z) : "v" (x), "v" (y));
  return z;
}

$ gcc -mcpu=power9 -mfloat128 -O3 -c s_fmaf128-power9.c -o test.o

$ objdump -d test.o
...
0000000000000000 <__fmaf128_power9>:
   0:   97 24 04 f0     xxlor   vs32,vs36,vs36    <----
   4:   08 1b 02 fc     xsmaddqp v0,v2,v3
   8:   97 04 40 f0     xxlor   vs34,vs32,vs32
   c:   20 00 80 4e     blr
        ...


---


### compiler : `gcc`
### title : `Suboptimal codegen on AVR when right-shifting 8-bit unsigned integers.`
### open_at : `2017-10-22T10:15:31Z`
### last_modified_date : `2021-07-22T21:17:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82658
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
This issue has been validated to occur back as far as at least 5.4.0, and still occurs in trunk.

When shifting an unsigned char/uint8_t right by less than 4 bits, suboptimal code is generated. This behavior only occurs when compiling source files as C++, not as C, even when the source file is equivalent otherwise. The issue does not manifest with left shifts or with larger composite types (such as uint16_t).

Trivial test:

void test ()
{
    volatile unsigned char val;
    unsigned char local = val;
    local >>= 1;
    val = local;
}

Compiling as C++ (avr-g++ [-O3|-O2] -mmcu=atmega2560 test.cpp -S -c -o test.s) results in the following assembly sequence handling the load, shift, and store:

ldd r24,Y+1
ldi r25,0
asr r25
ror r24
std Y+1,r24

The next operation performed on r25 is a clr. Thus, ldi/asr/ror are entirely equivalent to lsr in this situation, which is what the C frontend does:

Compiling as C (avr-gcc [-O3|-O2] -mmcu=atmega2560 test.c -S -c -o test.s) results in the following assembly sequence handling the load, shift, and store:

ldd r24,Y+1
lsr r24
std Y+1,r24

This is optimal code. This is also the defined behavior in avr.c.

The issue becomes more problematic with larger shifts (up until 4, where the defined behavior takes over again), as it generates the same instruction sequence repeatedly, whereas gcc simply generates 'lsr; lsr; lsr', as expected.

Interestingly, the issue does _not_ manifest if one chooses to use an integer division instead of a shift - if one divides the unsigned char by 2 instead of shifting right 1, it emits 'lsr' as expected.


---


### compiler : `gcc`
### title : `[11/12/13/14 regression]: sum += (x>128 ? x : 0) puts the cmov on the critical path (at -O2)`
### open_at : `2017-10-22T22:56:48Z`
### last_modified_date : `2023-08-04T04:18:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82666
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
long long sumarray(const int *data)
{
    data = (const int*)__builtin_assume_aligned(data, 64);
    long long sum = 0;
    for (int c=0 ; c<32768 ; c++)
        sum += (data[c] >= 128 ? data[c] : 0);

    return sum;
}

The loop body is written to encourage gcc to make the loop-carried dep chain just an ADD, with independent branchless zeroing of each input.  But unfortunately, gcc7 and gcc8 -O2 de-optimize it back to what we get with older gcc -O3 from

        if (data[c] >= 128)  // doesn't auto-vectorize with gcc4, unlike the above
            sum += data[c];

See also https://stackoverflow.com/questions/28875325/gcc-optimization-flag-o3-makes-code-slower-then-o2.


https://godbolt.org/g/GgVp7E
gcc8.0 8.0.0 20171022  -O2 -mtune=haswell  (slow)

        leaq    131072(%rdi), %rsi
        xorl    %eax, %eax
.L3:
        movslq  (%rdi), %rdx
        movq    %rdx, %rcx
        addq    %rax, %rdx      # mov+add could have been LEA
        cmpl    $127, %ecx
        cmovg   %rdx, %rax      # sum = (x>=128 : sum+x : sum)
        addq    $4, %rdi
        cmpq    %rsi, %rdi
        jne     .L3
        ret

This version has a 3 cycle latency loop-carried dep chain, (addq %rax, %rdx  and cmov).  It's also 8 fused-domain uops (1 more than older gcc) but using LEA would fix that.


gcc6.3 -O2 -mtune=haswell (last good version of gcc on Godbolt, for this test)

        leaq    131072(%rdi), %rsi
        xorl    %eax, %eax
        xorl    %ecx, %ecx          # extra zero constant for a cmov source
.L3:
        movslq  (%rdi), %rdx
        cmpl    $127, %edx
        cmovle  %rcx, %rdx          # rdx = 0 when rdx<=128
        addq    $4, %rdi
        addq    %rdx, %rax          # sum += ... critical path 1c latency
        cmpq    %rsi, %rdi
        jne     .L3
        ret

7 fused-domain uops in the loop (cmov is 2 with 2c latency before Broadwell).  Should run at 1.75 cycles per iter on Haswell (or slightly slower due to an odd number of uops in the loop buffer), bottlenecked on the front-end.  The latency bottleneck is only 1 cycle.  (Which Ryzen might come closer to.)

Anyway, on Haswell (with -mtune=haswell), the function should be more than 1.5x slower with gcc7/8 than with gcc6 and earlier.

Moreover, gcc should try to optimize something like this:

        if (data[c] >= 128)
            sum += data[c];

into conditionally zeroing a register instead of using a loop-carried cmov dep chain.


---


### compiler : `gcc`
### title : `could use BMI2 rorx for unpacking struct { int a,b }; from a register (SysV ABI)`
### open_at : `2017-10-23T03:04:56Z`
### last_modified_date : `2021-08-19T13:53:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82668
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
struct twoint {
	int a, b;
};

int bar(struct twoint s) {
	return s.a + s.b;
}

https://godbolt.org/g/4ygAMm

        movq    %rdi, %rax
        sarq    $32, %rax
        addl    %edi, %eax
        ret

But we could have used

    rorx   $32, %rdi, %rax       # 1 uop 1c latency
    add    $edi, %eax
    ret

rorxq is only 1 uop, vs. 2 for mov + sar.  It also saves a byte a 3 byte MOV + a 4 byte SAR with a 6 byte rorx.

Without BMI2, we can shorten critical path if mov isn't zero latency, from 3 to 2 cycles (and save a byte on the REX prefix for the mov):

        movl    %edi, %eax
        sarq    $32, %rdi
        addl    %edi, %eax
        ret

This would be a better choice in general, especially for tune=generic.



Also related (let me know if I should report separately, or if gcc knowing how to use rotate to swap struct members would fix this too):

// only needs one call-preserved reg and a rotate.
long foo(int a /* edi */, int b /* esi */)
{
    struct_arg ( (struct twoint){a,b});
    struct_arg ( (struct twoint){b,a});
    return 0;
}

gcc saves two call-preserved registers so it can save a and b separately, and shift+OR them together each time.

        pushq   %rbp
        movl    %edi, %ebp
        pushq   %rbx
        movl    %esi, %ebx
        movq    %rbx, %rdi
        salq    $32, %rdi
        subq    $8, %rsp
        orq     %rbp, %rdi
        call    struct_arg
        movq    %rbp, %rdi
        salq    $32, %rdi
        orq     %rbx, %rdi
        call    struct_arg
        addq    $8, %rsp
        xorl    %eax, %eax
        popq    %rbx
        popq    %rbp
        ret


This is sub-optimal in two ways: first, on Intel SnB-family (but not silvermont or any AMD), SHRD is efficient (1 uop, 1c latency, runs on port1 only instead of p06 for other shifts/rotates).  SHL + SHRD may be better than mov + shl + or.

Second, because instead of redoing the creation of the struct, we can rotate the first one.  Even writing it as a swap of the members of a struct (instead of creation of a new struct) doesn't help.

Anyway, I think this would be better

        pushq   %rbx
        shl     $32, %rdi
        shrd    $32, %rsi, %rdi   # SnB-family alternative to mov+shl+or

        rorx    $32, %rdi, %rbx   # arg for 2nd call
        call    struct_arg
        movq    %rbx, %rdi
        call    struct_arg

        xorl    %eax, %eax
        popq    %rbx
        ret

I didn't check whether I got the correct arg as the high half, but that's not the point.


---


### compiler : `gcc`
### title : `Use cmpXXss and cmpXXsd for setcc boolean compare`
### open_at : `2017-10-23T15:50:09Z`
### last_modified_date : `2023-08-08T07:07:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82680
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
Both, ICC and clang use cmpXXss and cmpXXsd instructions for setcc boolean compare:

int g(double x, double y)
{
  	return x == y;
}

ICC:
        cmpeqsd   %xmm1, %xmm0
        movd      %xmm0, %eax
        negl      %eax
        ret

clang:
        cmpeqsd %xmm1, %xmm0
        movq    %xmm0, %rax
        andl    $1, %eax
        retq

gcc w/ -ffast-math:

        xorl    %eax, %eax
        comisd  %xmm1, %xmm0
        sete    %al
        ret

Versions with direct moves avoid partial register stalls.


---


### compiler : `gcc`
### title : `merging writes of different types to the same location`
### open_at : `2017-10-23T21:39:23Z`
### last_modified_date : `2021-08-21T23:46:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82689
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.1`
### severity : `enhancement`
### contents :
Consider the following code:

#include <cstdint>

union x
{
    std::int64_t a;
    std::uint64_t b;
};

void f(x& xx, bool flag, std::int64_t value)
{
    if (flag)
        xx.a = value;
    else
        xx.b = value;
}

The function f is written with standard compliance in mind so it tries to write only that member that will be lately used/read.

The hope is that the compiler is able to optimize the redundant if.

And most compilers (clang, MSVC, icc) are able to optimize this function to a single store instruction.

GCC optimized it to a single store (to some extend). Here what it generated:

f:
  test sil, sil
  mov QWORD PTR [rdi], rdx
  jne .L5
  rep ret
.L5:
  rep ret

The expected behavior is that the function is optimized to a single store by GCC too.


---


### compiler : `gcc`
### title : `Missing tail calls for large structs`
### open_at : `2017-10-24T19:58:28Z`
### last_modified_date : `2021-09-23T01:22:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82705
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.2.1`
### severity : `normal`
### contents :
The following code:

struct Foo {
   int o[16];
};

__attribute__((noinline))
Foo moo()
{
        return {0};
}

Foo goo()
{
        return moo();
}

with -O3 -fno-exceptions -fomit-frame-pointer compiles to:
moo():
  pxor xmm0, xmm0
  mov rax, rdi
  movups XMMWORD PTR [rdi], xmm0
  movups XMMWORD PTR [rdi+16], xmm0
  movups XMMWORD PTR [rdi+32], xmm0
  movups XMMWORD PTR [rdi+48], xmm0
  ret
goo():
  sub rsp, 8
  call moo()
  add rsp, 8
  mov rax, rdi
  ret

goo could just be:

goo():
  jmp moo

Also it seems like the "sub rsp, 8" and "add rsp, 8" are extraneous


---


### compiler : `gcc`
### title : `adjacent small objects can be initialized with a single store (but aren't for char a[] = "a")`
### open_at : `2017-10-26T08:15:46Z`
### last_modified_date : `2022-02-21T06:20:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82729
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `enhancement`
### contents :
void ext(char *, char *, char *);

void foo(void) {
    char abc[] = "abc";
    char ab[] = "ab";
    char a[] = "a";
    ext(a, ab, abc);
}

gcc 8.0.0 20171024 -O3   https://godbolt.org/g/mFNUgn

foo:   -march=bdver3  to avoid moving to 32-bit registers first
        subq    $24, %rsp
        leaq    12(%rsp), %rdx
        leaq    9(%rsp), %rsi
        leaq    7(%rsp), %rdi

        # these 4 stores only need 2 instructions
        movl    $6513249, 12(%rsp)
        movw    $25185, 9(%rsp)        
        movb    $0, 11(%rsp)           # last byte of ab[]
        movw    $97, 7(%rsp)


        call    ext
        addq    $24, %rsp
        ret

-march=haswell still avoids movw $imm16, (mem), even though Haswell doesn't have LCP stalls.  But that's not what this bug is about.

A single  push imm32  or  mov $imm32, r/m64  could store a[] and ab[], because sign-extension will produce 4 bytes of zeros in the high half.  We only need one of those zeros to terminate the string.  If you don't want to waste the extra 3 bytes of padding, simply have the next store overlap it.

Or keeping the layout identical:

        ...
        movq    $0x62610061, 7(%rsp)   # zero some of the bytes for abc[]
         #memory at 7(%rsp) = 'a', 0, 'a', 'b', 0, 0 (rsp+12), 0, 0
        movl    $6513249, 12(%rsp)     # then initialize abc[]
        ...

x86 CPUs generally have good support for overlapping stores.  e.g. store-forwarding still works from the movq to a load of a[] or ab[], and also works from the movl to a load from abc[].

related: bug 82142, padding in structs stopping store merging.  But this isn't padding, it's merging across separate objects that are / can be placed next to each other on the stack.


----


On ARM, we can take advantage of redundancy between the string data as well instead of using a string constant for ab[] and a literal pool with a pointer + abc[].

# This is dumb:  ARM gcc 6.3.0
.L3:
        .word   .LC0      # Should just store ab[] literally here
        .word   6513249
.LC0:
        .ascii  "ab\000"

You can also do stuff like  add r1, r1, 'c' LSL 16  to append the 'c' byte if you have "ab" in a register.  Or if it's a common suffix instead of prefix, left-shift and add.  Or start with the 4-byte object (including the terminating zero) and AND out the characters.  IDK if this is a common enough pattern to spend time searching for that much redundancy between constant initializers.

But I think on x86 it would be a good idea to zero a register instead of doing more than 2 or 3 repeated  movq $0, (mem)   especially when the addressing mode is RIP-relative (can't micro-fuse immediate + RIP-relative addressing mode), or otherwise uses a 32-bit displacement.  (Code-size matters.)


---


### compiler : `gcc`
### title : `extra store/reload of an XMM for every byte extracted`
### open_at : `2017-10-26T10:19:21Z`
### last_modified_date : `2021-08-19T06:25:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82730
### status : `NEW`
### tags : `missed-optimization, ssemmx`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
#include <immintrin.h>
#include <stdint.h>
#include <stdio.h>

void p128_as_u8hex(__m128i in) {
    _Alignas(16) uint8_t v[16];
    _mm_store_si128((__m128i*)v, in);

    printf("v16.u8: %#x %#x %#x %#x | %#x %#x %#x %#x | %#x %#x %#x %#x | %#x %#x %#x %#x\n",
           v[0], v[1],  v[2],  v[3],  v[4],  v[5],  v[6],  v[7],
           v[8], v[9], v[10], v[11], v[12], v[13], v[14], v[15]);
}

https://godbolt.org/g/yoikq9
-O3  (or -march= anything with -mno-sse4 for pextrb)

        subq    $288, %rsp                   # 288 bytes!!!
        movl    $.LC0, %edi
        movaps  %xmm0, 8(%rsp)               # store
        movdqa  8(%rsp), %xmm6               # reload twice...
        movdqa  8(%rsp), %xmm1
        movaps  %xmm6, 184(%rsp)             # spill somewhere else
        movzbl  199(%rsp), %eax              # v[15]
        movaps  %xmm1, 264(%rsp)
        movzbl  8(%rsp), %esi                # v[0]
        movaps  %xmm1, 248(%rsp)
        ...
        pushq   %rax                         # v[15]

        movdqa  16(%rsp), %xmm7
        movaps  %xmm7, 176(%rsp)
        movzbl  190(%rsp), %eax
        pushq   %rax                         # v[14]

        movdqa  24(%rsp), %xmm0
        movaps  %xmm0, 168(%rsp)
        movzbl  181(%rsp), %eax
        pushq   %rax
        ...
        xorl    %eax, %eax
        call    printf
        addq    $376, %rsp
        ret

This is pretty hilariously bad, especially compared to the scalar code that gcc6.3 produces:

        subq    $32, %rsp
        movq    %xmm0, %r9
        movq    %xmm0, %rcx
            # ok this is a bit silly vs. a scalar mov.
            # very few CPUs can do parallel movq so there's a resource-conflict anyway making this no better than a GP->GP mov
        movaps  %xmm0, 8(%rsp)
        movq    16(%rsp), %rax        # high half
        shrq    $32, %r9
        shrq    $16, %rcx
        movq    %xmm0, %r8
        movq    %xmm0, %rdx
        movzbl  %cl, %ecx
        movzbl  %r8b, %esi
        movzbl  %dh, %edx             # using dh to save on shifts
        movzbl  %r9b, %r9d
        shrl    $24, %r8d
        movq    %rax, %rdi
        shrq    $56, %rdi
        pushq   %rdi
        ...

Not perfect (related to bug 67072), but at least doesn't do a chain of vector copies all over the place.

--------

OTOH, we could vectorize the unpack and store to stack memory in 16B chunks.  This is much more profitable for 32-bit mode, where all args are stack args, and where a 16B vector holds 4 args instead of 2.  e.g. movzxbd or 2-step punpck with zeros.

For printing as 32-bit or 64-bit integers, we can just store the vector to the stack instead of getting each element out separately!  (Should I report that as a separate missed optimization, for 

void p128_as_u32hex(__m128i in) {
    //const uint32_t *v = (const uint32_t*) &in;
    alignas(16) uint32_t v[4];
    _mm_store_si128((__m128i*)v, in);
    printf("v4.u32: %#x %#x %#x %#x\n", v[0], v[1], v[2], v[3]);
}

where we get (with gcc -O3 -m32)

        pshufd  $255, %xmm0, %xmm1
        movd    %xmm1, %eax
        movdqa  %xmm0, %xmm1
        pushl   %eax
        punpckhdq       %xmm0, %xmm1
        movd    %xmm1, %eax
        pshufd  $85, %xmm0, %xmm1
        pushl   %eax
        ...

instead of a single movaps store.  Or for printing as uint64_t, we get

        movhps  %xmm0, 20(%esp)
        pushl   24(%esp)
        pushl   24(%esp)
        movq    %xmm0, 28(%esp)
        pushl   32(%esp)
        pushl   32(%esp)


---


### compiler : `gcc`
### title : `malloc+zeroing other than memset not optimized to calloc, so asm output is malloc+memset`
### open_at : `2017-10-26T10:52:05Z`
### last_modified_date : `2021-05-14T15:46:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82732
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
#include <string.h>
#include <stdlib.h>

int *foo(unsigned size)
{
	int *p = malloc(size*sizeof(int));
	//memset(p,0, size*sizeof(int));

	for (unsigned i=0; i<size; i++) {
		p[i]=0;
	}
	return p;
}

gcc -O3 -march=haswell    https://godbolt.org/g/bpGHoa

        pushq   %rbx
        movl    %edi, %edi       # zero-extend
        movq    %rdi, %rbx       # why 64-bit operand-size here?
        salq    $2, %rdi
        call    malloc

        movq    %rax, %rcx
        testl   %ebx, %ebx       # check that size was non-zero before looping
        je      .L6
        leal    -1(%rbx), %eax
        movq    %rcx, %rdi
        xorl    %esi, %esi
        leaq    4(,%rax,4), %rdx  # redo the left-shift
        call    memset
        movq    %rax, %rcx
.L6:
        movq    %rcx, %rax       # this is dumb, either way we get here malloc return value is already in %rax.  memset returns it.
        popq    %rbx
        ret

So gcc figures out that this is malloc+memset, but I guess not until after the pass that recognizes that as calloc.


But with explicit memset and gcc -O3, we get the zeroing loop to optimize away as well

foo:
        movl    %edi, %edi
        movl    $1, %esi
        salq    $2, %rdi
        jmp     calloc

Unfortunately at -O2 we still get a loop that stores 4 bytes at a time, *after calloc*.  I know -O2 doesn't enable all the optimizations, but I thought it would do better than this for "manual" zeroing loops.


---


### compiler : `gcc`
### title : `[10 Regression] Sort is 30% slower compared to gcc44 on presorted array`
### open_at : `2017-10-26T21:11:36Z`
### last_modified_date : `2023-07-07T07:52:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82739
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.2.0`
### severity : `normal`
### contents :
In 2013 a bug was filed to fix a large performance degradation on reverse-sorted array, which got fixed:
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58437

However there is near 60% performance degradation with GCC 7.2.0 compared to GCC 4.4.5, this time when running std::sort on a forward-sorted array.

Here is the testcase:

sort.cpp
=======================================================
#include <algorithm>
#include <vector>

using namespace std;

int main()
{
  const int num = 1000000000;                                                                                                                                                                                                                                        
  std::vector<int> v;  
  v.reserve(num);
  
  for(int i=0;i!=num;++i) v.push_back(i);
  sort(v.begin(), v.end());
//  std::sort(std::begin(v), std::end(v));
}
=======================================================

Compilation Line: g++ -O3 sort.cpp -o sort
Run: time ./sort

# Server details
$ uname -a
Linux server 2.6.18-194.el5 #1 SMP Tue Mar 16 21:52:39 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux

Results:

gcc version 4.1.2
real	0m25.511s
user	0m23.960s
sys	0m1.301s

gcc version 4.4.5
real	0m20.241s
user	0m18.716s
sys	0m1.328s

gcc version 4.8.3
real	0m26.742s
user	0m25.167s
sys	0m1.314s

gcc version 6.3.0
real	0m33.911s
user	0m32.312s
sys	0m1.266s

gcc version 7.2.0 
real	0m31.916s
user	0m30.308s
sys	0m1.299s

Is there any way to improve the run-time performance?


---


### compiler : `gcc`
### title : `Unable to optimize the loop when iteration count is unavailable.`
### open_at : `2017-10-31T00:55:29Z`
### last_modified_date : `2021-05-04T12:32:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82776
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Compiling with 

g++ -O2 --std=c++14 -msse4 -DENABLE_FORLOOP
vs.
g++ -O2 --std=c++14 -msse4

gives dramatically different results in the sense that the loop is completely optimized when for-loop is present instead of `while(true)` loop. Reproduces with g++-7.2 and g++-trunk.


$ cat test.cpp

#include <type_traits>
#include <cstdint>
#include <emmintrin.h>
#include <vector>
#include <cstdlib>
#include <array>
#include <memory>

struct Chunk {
  std::array<uint8_t,14> tags_;
  uint8_t control_;

  bool eof() const {
    return (control_ & 1) != 0;
  }

  static constexpr unsigned kFullMask = (1 << 14) - 1;

  __m128i const* tagVector() const {
    return static_cast<__m128i const*>(static_cast<void const*>(&tags_[0]));
  }

  unsigned emptyMask() const {
    auto tagV = _mm_load_si128(tagVector());
    auto emptyTagV = _mm_cmpeq_epi8(tagV, _mm_setzero_si128());
    return _mm_movemask_epi8(emptyTagV) & kFullMask;
  }

  unsigned occupiedMask() const {
    return emptyMask() ^ kFullMask;
  }
};

#define LIKELY(x) __builtin_expect((x), true)
#define UNLIKELY(x) __builtin_expect((x), false)

struct Iter {
  Chunk* chunk_;
  std::size_t index_;

  void advance() {
    // common case is packed entries
    while (index_ > 0) {
      --index_;
      if (LIKELY(chunk_->tags_[index_] != 0)) {
        return;
      }
    }

    // bar only skips the work of advance() if this loop can
    // be guaranteed to terminate
#ifdef ENABLE_FORLOOP
    for (std::size_t i = 1; i != 0; ++i) {
#else
    while (true) {
#endif
      // exhausted the current chunk
      if (chunk_->eof()) {
        chunk_ = nullptr;
        break;
      }
      ++chunk_;
      auto m = chunk_->occupiedMask();
      if (m != 0) {
        index_ = 31 - __builtin_clz(m);
        break;
      }
    }
  }
};

static Iter foo(Iter iter) {
  puts("hello");
  iter.advance();
  return iter;
}

void bar(Iter iter) {
  foo(iter);
}


---


### compiler : `gcc`
### title : `Wildly excessive calls to __tls_get_addr with optimizations enabled.`
### open_at : `2017-11-02T06:09:32Z`
### last_modified_date : `2021-08-22T08:26:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82803
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
gcc emits a call to __tls_get_addr for every iteration of the while loop below at -O1 or above.  It does not happen at -O0.

static thread_local int x;
int g();
int f(int c) {
  int *px = &x;
  while (c--)
    *px =+ g();
  return *px;
}

Compile with: -O2 -std=c++11 -fPIC -Wall -Wextra

I tried it with several versions back to 4.8.2 with the same results.  At -O0 exactly one call is emitted.  Seems like the optimizer is doing something silly with thread_locals.

https://godbolt.org/g/S9hvHQ


---


### compiler : `gcc`
### title : `Optimize x % 3 == 0 without modulo`
### open_at : `2017-11-05T19:57:22Z`
### last_modified_date : `2019-06-03T11:11:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82853
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
Ralph Levien pointed out as part of FizzBuzz optimization:

Turns out you can compute x%3 == 0 with even fewer steps, it's (x*0xaaaaaaaab) < 0x55555556 (assuming wrapping unsigned 32 bit arithmetic).

gcc currently generates the full modulo and then checks.

Could be done in match.pd I suppose.

Test case

unsigned mod3(unsigned a) { return 0==(a%3); }


---


### compiler : `gcc`
### title : `more missing simplifcations`
### open_at : `2017-11-05T21:24:42Z`
### last_modified_date : `2023-05-02T05:24:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82854
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
These all come from a paper

"Optgen: A Generator for Local Optimizations" (Buchwald et.al.).
https://pp.info.uni-karlsruhe.de/uploads/publikationen/buchwald15cc.pdf

These were found by a SAT solver.

I wrote them in partial pseudo match.pd syntax (untested, likely buggy)

I'm not sure how useful they are really for real programs, but with the auto generated matchers scaling well to more rules they wouldn't hurt I suppose.

/* x + (x & 0x80000000) -> x & 0x7fffffff */
(simplify
  (plus:c @0 (bit_and @0 integer_msb_onlyp@1))
  (bit_and @0 { @1 - 1; } ))

/* (x | 0x80000000) + 0x80000000 -> x & 0x7FFFFFFF */
(simplify
  (plus:c (bit_ior @0 integer_msb_onlyp) msb_setp)
  (bit_and @0 { msb_minus_one_val(type); } ))

/* x & (x + 0x80000000) -> x & 0x7FFFFFFF */
(simplify
  (bit_and:c (plus @0 msb_setp) @0)
  (bit_and @0 { msb_minus_one_val(type); } ))

/* x & (0x7FFFFFFF - x) -> x & 0x80000000 */
(simplify
  (bit_and:c @0 (minus msb_minus_onep @0))
  (bit_and @0 { msb_val(type); } ))

/* is_power_of_2(c1) && c0 & (2 * c1 - 1) == c1 - 1 ->
   (c0 - x) & c1 -> x & c1 */

/* x | (x + 0x80000000) -> x | 0x80000000 */
(simplify
  (bit_ior:c @0 (plus @0 msb_onlyp))
  (bit_ior @0 { msb_val(type); } ))

/* x | (0x7FFFFFFF - x) -> x | 0x7FFFFFFF */
(simplify
  (bit_ior:c @0 (minus 0x7FFFFFFF @0))
  (bit_ior @0 0x7FFFFFFF))

/* x | (x ^ y) -> x | y */
(simplify
  (bit_ior:c @0 (bit_xor:c @0 @1))
  (bit_ior @0 @1))

/* ((c0 | -c0) & ∼c1) == 0 AND (x + c0) | c1 -> x | c1 */

/* is_power_of_2(∼c1) && c0 & (2 * ∼c1 - 1) == ∼c1 - 1 AND
   (c0 - x) | c1 ->
   x | c1 */

/* -x | 0xFFFFFFFE -> x | 0xFFFFFFFE */
(simplify
  (bit_or (negate @0) 0xFFFFFFFE)
  (bit_or @0 0xFFFFFFFE))

/* 0 - (x & 0x80000000) -> x & 0x80000000 */
(simplify
  (minus 0 (bit_and:c @0 0x80000000))
  (bit_and @0 0x80000000))

/* 0x7FFFFFFF - (x & 0x80000000) -> x | 0x7FFFFFFF */
(simplify
  (minus 0x7FFFFFFF (bit_and @0 0x80000000))
  (bit_ior @0 0x7FFFFFFF))

/* 0x7FFFFFFF - (x | 0x7FFFFFFF) -> x & 0x80000000 */
(simplify
  (minus 0x7FFFFFFF (bit_ior:c @0 0x7FFFFFFF))
  (bit_and @0 0x80000000))

/* 0xFFFFFFFE - (x | 0x7FFFFFFF) -> x | 0x7FFFFFFF */
(simplify
  (minus 0xFFFFFFFE (bit_ior:c @0 0x7FFFFFFF))
  (bit_ior @0 0x7FFFFFFF))

/* (x & 0x7FFFFFFF) - x -> x & 0x80000000 */
(simplify
  (minus (bit_and:c @0 0x7FFFFFFF) @0)
  (bit_and @0 0x80000000))

/* x ^ (x + 0x80000000) -> 0x80000000 */
(simplify
  (bit_xor:c (plus:c @0 0x80000000))
  0x80000000)

/* x ^ (0x7FFFFFFF - x) -> 0x7FFFFFFF */
(simplify
  (bit_xor:c @0 (minus 0x7FFFFFFF @0))
  0x7FFFFFFF)

/* (x + 0x7FFFFFFF) ^ 0x7FFFFFFF -> -x */
(simplify
  (bit_xor:c (plus:c @0 0x7FFFFFFF) 0x7FFFFFFF)
  (negate @0))

/* -x ^ 0x80000000 -> 0x80000000 - x */
(simplify
  (bit_xor:c (negate @0) 0x80000000)
  (minus 0x80000000 @0))

/* (0x7FFFFFFF - x) ^ 0x7FFFFFFF -> x */
(simplify
  (bit_xor:c (minus 0x7FFFFFFF @0) 0x7FFFFFFF)
  @0)

/* ~(x + c) -> ~c - x */
(simplify
  (bit_not (plus:c @0 CONSTANT_CLASS_P@1))
  (minus (bit_not c) @0))

/* -x ^ 0x7FFFFFFF -> x + 0x7FFFFFFF */
(simplify
  (bit_xor (negate @0) 0x7FFFFFFF)
  (plus @0 0x7FFFFFFF))

/* (x | c) - c -> x & ∼c */
(simplify
  (minus (bit_ior @0 CONSTANT_CLASS_P@1) @1)
  (bit_and @0 (bit_not @1)))

/* ~(c - x) -> x + ∼c */
(simplify
  (bit_not (minus CONSTANT_CLASS_P@0 @1))
  (plus @1 (bit_not @0)))

/* -c0 == c1 AND (x | c0) + c1 -> x & ∼c1 */
(simplify
  (plus (bit_or @0 CONSTANT_CLASS_P@1) CONSTANT_CLASS_P@2)
  (if (...)
    (bit_and @0 (bit_not @2))

/* (c0 & ∼c1) == 0 AND (x ^ c0) | c1 -> x | c1 */

/* 0x7FFFFFFF - (x ^ c) -> x ^ (0x7FFFFFFF - c) */


---


### compiler : `gcc`
### title : `__builtin_add_overflow() generates suboptimal code with unsigned types on x86`
### open_at : `2017-11-06T11:35:37Z`
### last_modified_date : `2023-05-31T23:23:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82858
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.2.0`
### severity : `normal`
### contents :
The following function:

```
unsigned saturated_add(unsigned a, unsigned b){
    unsigned c;
    if(__builtin_add_overflow(a, b, &c)){
        return -1;
    }
    return c;
}
```

, after being compiled with `gcc -O3 -Wall -Wextra -pedantic -pedantic-errors -masm=intel`, results in:

```
saturated_add(unsigned int, unsigned int):
  add edi, esi
  jc .L3
  mov eax, edi
  ret
.L3:
  or eax, -1
  ret
```

This is suboptimal, as the branch can be simply eliminated as follows:

```
saturated_add(unsigned int, unsigned int):
  add edi, esi
  sbb eax, eax  // EAX = -1 if CF is set and 0 otherwise.
  or eax, edi
  ret
```


---


### compiler : `gcc`
### title : `eax register unnecessary consumed`
### open_at : `2017-11-07T17:04:01Z`
### last_modified_date : `2021-08-15T13:43:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82883
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Following example:

void foo(char* ch) {
    __builtin_memcpy(ch, "Hello word", 6);
}


Produces the assembly

foo(char*):
        mov     eax, 8303   <=== This is not required
        mov     DWORD PTR [rdi], 1819043144
        mov     WORD PTR [rdi+4], ax
        ret

The code could be further optimized to not use the eax register. For example clang does the following:

foo(char*):                               # @foo(char*)
        mov     word ptr [rdi + 4], 8303
        mov     dword ptr [rdi], 1819043144
        ret


---


### compiler : `gcc`
### title : `memcpy does not propagate aliasing knowledge`
### open_at : `2017-11-07T17:19:38Z`
### last_modified_date : `2021-08-01T17:07:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82885
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The following code:

void foo3(char* ch1, const char* ch2) {
    __builtin_memcpy(ch1, ch2, 1024*1024);
    ch1[0] = ch2[2];
    ch1[1] = ch2[2];
}


Produces assembly

foo3(char*, char*):
        push    rbx
        mov     edx, 1048576
        mov     rbx, rsi
        call    memcpy
        mov     rcx, rax
        movzx   eax, BYTE PTR [rbx+2]
        mov     BYTE PTR [rcx], al
        movzx   eax, BYTE PTR [rbx+2] <== This could be removed
        mov     BYTE PTR [rcx+1], al
        pop     rbx
        ret

memcpy works for non aliased data, otherwise it is UB. Knowledge that pointers do not alias could be propagated further to remove reads from ch2.


---


### compiler : `gcc`
### title : `Unnecessary sign extension of int32 to int64`
### open_at : `2017-11-07T20:22:33Z`
### last_modified_date : `2022-08-13T18:10:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82889
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.0`
### severity : `normal`
### contents :
$ cat t.cpp
#include <stdint.h>

int lol(int32_t* table, int32_t* ht, uint32_t hash, uint32_t mask) {
    for (uint64_t probe = (uint32_t)hash & mask, i = 1;; ++i) {
        int32_t pos = ht[probe];
        if (pos >= 0) {
            if (table[pos] == 42) {
                return true;
            }
        } else if (pos & 1) {
            return false;
        }
        probe += i;
        probe &= mask;
    }
    // notreached
}

compile with:
gcc -std=c++11 -O3 -s -o -


lol(int*, int*, unsigned int, unsigned int):
        andl    %ecx, %edx
        movl    $1, %r8d
        movl    %ecx, %ecx
        jmp     .L5
.L10:
        cmpl    $42, (%rdi,%rax,4)
        je      .L9
.L4:
        addq    %r8, %rdx
        addq    $1, %r8
        andq    %rcx, %rdx
.L5:
        movslq  (%rsi,%rdx,4), %rax #<------------sign extended
        testl   %eax, %eax
        jns     .L10
        testb   $1, %al
        je      .L4
        xorl    %eax, %eax
        ret
.L9:
        movl    $1, %eax
        ret


Is it possible to get rid of this?


---


### compiler : `gcc`
### title : `Aliasing knowledge is not used to replace memmove with memcpy`
### open_at : `2017-11-08T10:31:05Z`
### last_modified_date : `2020-03-20T21:05:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82898
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
This has been resolved once in Bug 21602 but now the issue appears again:

void foo(int* i, const float* f) {
    __builtin_memmove(i, f, 1024*1024);
}

Produces assembly:

foo(int*, float const*):
        mov     edx, 1048576
        jmp     memmove   <=== Could be memcpy


---


### compiler : `gcc`
### title : `vector shift forced to 32 bits`
### open_at : `2017-11-08T17:57:32Z`
### last_modified_date : `2021-08-01T08:02:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82905
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
#include <cstdint>
using namespace std;

int const count = 1024;
uint8_t p[count];

void mul(uint16_t m)
{
	for (int i = 0; i < count; ++i)
	{
		p[i] = uint16_t(p[i] * m) >> 8;
	}
}

compiled for x86-64 with -O3 generates psrad instructions instead of psrlw instructions.  Also, the pand instructions are not needed.


---


### compiler : `gcc`
### title : `No aliasing is possible on non equal pointers`
### open_at : `2017-11-09T15:41:35Z`
### last_modified_date : `2021-12-13T08:48:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82918
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Following code 

struct array {
    int data[3];
};

void foo2(array& value, const array& value2) {    
    if (&value == &value2) return;
    value.data[0] = value2.data[0];
    value.data[1] = value2.data[0];
    value.data[2] = value2.data[0];
}

produces the following assembly:

foo2(array&, array const&):
        cmp     rdi, rsi
        je      .L1
        mov     eax, DWORD PTR [rsi]
        mov     DWORD PTR [rdi], eax
        mov     eax, DWORD PTR [rsi]   <=== This is not required
        mov     DWORD PTR [rdi+4], eax
        mov     DWORD PTR [rdi+8], eax
.L1:
        rep ret

GCC already understands that value.data[1] and value.data[2] do not alias with value2.data[0].

However GCC assumes that value1.data[0] may alias value2.data[0], which is not possible, because of `if (&value == &value2) return;`

Please add the optimization, as it affects many cases, especially C++ assign and move assign operators, where checking for `this == &rhs` is a common pattern.


---


### compiler : `gcc`
### title : `Missing Optimization for Bit-Transfer`
### open_at : `2017-11-10T08:00:24Z`
### last_modified_date : `2023-05-25T18:09:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82931
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `14.0`
### severity : `enhancement`
### contents :
Created attachment 42574
Minimum complete verifying example

The attached example produces optimal code for the AVR-target: it uses the bst/bld assembler instructions. But this is only true for bit 0 (least significant) in an uint8_t type. If the same instruction sequence is used to transfer bit 1...7 less optimal code is generated.

This is unlogical to some respect since the backend recognizes the special case for bit 0, so it should be possible to use the very same optimization for other bits.

The same holds true if one used another datatype such as uint16_t and greater. No optimization takes place.


---


### compiler : `gcc`
### title : `Suboptimal code for (a & 0x7f) | (b & 0x80) on powerpc`
### open_at : `2017-11-10T15:39:41Z`
### last_modified_date : `2023-04-11T17:05:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82940
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `5.4.0`
### severity : `enhancement`
### contents :
unsigned char g(unsigned char t[], unsigned char v)
{
	return (t[v & 0x7f] & 0x7f) | (v & 0x80);
}

00000008 <g>:
   8:	54 89 06 7e 	clrlwi  r9,r4,25
   c:	7c 63 48 ae 	lbzx    r3,r3,r9
  10:	54 84 00 30 	rlwinm  r4,r4,0,0,24
  14:	54 63 06 7e 	clrlwi  r3,r3,25
  18:	7c 63 23 78 	or      r3,r3,r4
  1c:	4e 80 00 20 	blr


I would expect

00000008 <g>:
   8:	54 89 06 7e 	clrlwi  r9,r4,25
   c:	7c 63 48 ae 	lbzx    r3,r3,r9
  10:	54 84 00 30 	rlwimi  r3,r4,0,24,24
  14:	4e 80 00 20 	blr


---


### compiler : `gcc`
### title : `[7 Regression] unnecessary __multi3 call for mips64r6 linux kernel`
### open_at : `2017-11-14T05:08:49Z`
### last_modified_date : `2019-11-14T10:48:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82981
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `7.2.1`
### severity : `normal`
### contents :
Given the testcase
unsigned long func(unsigned long a, unsigned long b)
{
        return a > (~0UL) / b;
}
compiled with -march=mips64r6 -mabi=64 -mexplicit-relocs -O2 we end up with a call to __multi3 which is inefficient and inconvenient.

The testcase gets converted to 
  _1 = MUL_OVERFLOW (a_4(D), b_5(D));
  _2 = IMAGPART_EXPR <_1>;
There are no mulv patterns in the mips port, so it tries mulditi3 which fails, and then calls __multi3.

Mips64r6 does have a widening DImode multiply which should have been used instead.  The problem is that the *mulditi3 pattern is missing mips64r6 support.

Alternatively, the expander should try using a muldi3_highpart pattern when the mulditi3 pattern doesn't work.  Especially when the highpart is the only part we need as in this example.  The mips64r6 multi3_highpart is present.

Or alternatively, a mulvti3 pattern should be added to the mips port.

See for instance the thread at
    https://www.linux-mips.org/archives/linux-mips/2017-08/msg00041.html


---


### compiler : `gcc`
### title : `memcpy and strcpy return value can be assumed to be equal to first argument`
### open_at : `2017-11-14T16:21:05Z`
### last_modified_date : `2021-12-27T04:27:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82991
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
I noticed another even more straightforward optimization opportunity that the one pointed out in pr82665.  The test case below shows that GCC knows that stpcpy(p, s) returns p + strlen(s) but it doesn't "know" that strcpy(p, s) returns p, or that memcmpy(p, s, n) also returns p.

$ cat c.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout c.c
void f1 (char *p)
{
  char *q = __builtin_stpcpy (p, "123");
  unsigned n = q - p;

  if (n != 3)             // eliminated
    __builtin_abort ();
}

void f2 (char *p)
{
  char *q = __builtin_strcpy (p, "123");
  unsigned n = q - p;

  if (n)                  // not eliminated
    __builtin_abort ();
}

void f3 (char *p, const char *s)
{
  char *q = __builtin_memcpy (p, s, 3);
  unsigned n = q - p;

  if (n)                  // not eliminated
    __builtin_abort ();
}

;; Function f1 (f1, funcdef_no=0, decl_uid=1891, cgraph_uid=0, symbol_order=0)

f1 (char * p)
{
  <bb 2> [local count: 10000]:
  __builtin_memcpy (p_2(D), "123", 4); [tail call]
  return;

}



;; Function f2 (f2, funcdef_no=1, decl_uid=1896, cgraph_uid=1, symbol_order=1)

f2 (char * p)
{
  unsigned int n;
  char * q;
  long int q.2_1;
  long int p.3_2;
  long int _3;

  <bb 2> [local count: 10000]:
  q_7 = __builtin_memcpy (p_5(D), "123", 4);
  q.2_1 = (long int) q_7;
  p.3_2 = (long int) p_5(D);
  _3 = q.2_1 - p.3_2;
  n_8 = (unsigned int) _3;
  if (n_8 != 0)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 9996]:
  return;

}



;; Function f3 (f3, funcdef_no=2, decl_uid=1902, cgraph_uid=2, symbol_order=2)

f3 (char * p, const char * s)
{
  unsigned int n;
  char * q;
  long int q.4_1;
  long int p.5_2;
  long int _3;

  <bb 2> [local count: 10000]:
  q_8 = __builtin_memcpy (p_5(D), s_6(D), 3);
  q.4_1 = (long int) q_8;
  p.5_2 = (long int) p_5(D);
  _3 = q.4_1 - p.5_2;
  n_9 = (unsigned int) _3;
  if (n_9 != 0)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 9996]:
  return;

}


---


### compiler : `gcc`
### title : `[performance] Is it better to avoid extra instructions in data passing between loops?`
### open_at : `2017-11-15T15:54:32Z`
### last_modified_date : `2021-10-06T15:08:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83008
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
I found strange code generated by GCC-8.0/7.x with following command line options:
-g -Ofast -march=skylake-avx512 -ftree-vectorize

There are not vectorized two loops. 
First one doesn’t vectorized because:
test.c:6:23: note: cost model: the vector iteration cost = 1488 divided by the scalar iteration cost = 328 is greater or equal to the vectorization factor = 4.
test.c:6:23: note: not vectorized: vectorization not profitable.
test.c:6:23: note: not vectorized: vector version will never be profitable.

Second one doesn’t vectorized because:
test.c:20:23: note: step unknown.
test.c:20:23: note: reduction: not commutative/associative: sum_87 = (int) _61;
test.c:20:23: note: Unknown def-use cycle pattern.
test.c:20:23: note: Unsupported pattern.
test.c:20:23: note: not vectorized: unsupported use in stmt.
test.c:20:23: note: unexpected pattern.

If we look into asm we found strange method to passing data in “tmp” array between loops:
…loop 1 body…
 13f:   41 8d 04 13             lea    (%r11,%rdx,1),%eax
 143:   c5 f9 6e d8             vmovd  %eax,%xmm3
 147:   c5 e1 62 db             vpunpckldq %xmm3,%xmm3,%xmm3
 14b:   c5 f9 62 c0             vpunpckldq %xmm0,%xmm0,%xmm0
 14f:   c5 f1 62 c9             vpunpckldq %xmm1,%xmm1,%xmm1
 153:   c5 e9 62 d2             vpunpckldq %xmm2,%xmm2,%xmm2
 157:   c5 e9 6c d2             vpunpcklqdq %xmm2,%xmm2,%xmm2
 15b:   c5 f1 6c c9             vpunpcklqdq %xmm1,%xmm1,%xmm1
 15f:   c5 f9 6c c0             vpunpcklqdq %xmm0,%xmm0,%xmm0
 163:   c5 e1 6c db             vpunpcklqdq %xmm3,%xmm3,%xmm3
 167:   c4 e3 6d 38 c9 01       vinserti128 $0x1,%xmm1,%ymm2,%ymm1
 16d:   c4 e3 7d 38 c3 01       vinserti128 $0x1,%xmm3,%ymm0,%ymm0
 173:   62 f3 f5 48 3a c0 01    vinserti64x4 $0x1,%ymm0,%zmm1,%zmm0
 17a:   62 f1 fd 48 7f 44 24    vmovdqa64 %zmm0,-0x40(%rsp)
 181:   ff 
 182:   8b 54 24 e0             mov    -0x20(%rsp),%edx
 186:   03 54 24 f0             add    -0x10(%rsp),%edx
…loop 2 body…

if I'm not mistaken the algorithm looks like following:
1.	Do first loop and keep values in GPR
2.	Move these GPRs to XMMs
3.	Pack these XMMs into YMMs
4.	Pack these YMMs to ZMM
5.	Spill ZMM into stack
6.	Get values from stack to GPRs of the second loop

It might be better, from performance perspective, to pass values from first loop directly to the second loop with GPRs (without all these vector registers)?

The reproducer is:
     1  int test(unsigned char * input1, unsigned char * input2)
     2  {
     3      unsigned int tmp[4][4];
     4      unsigned int var0, var1, var2, var3;
     5      int sum = 0;
     6      for (int i = 0; i < 4; i++, input1 += 4, input2 += 4) {
     7          var0 = (input1[0] + input2[0]) + (input1[4] + input2[4]);
     8          var1 = (input1[1] + input2[1]) + (input1[5] + input2[5]);
     9          var2 = (input1[2] + input2[2]) + (input1[6] + input2[6]);
    10          var3 = (input1[3] + input2[3]) + (input1[7] + input2[7]);
    11          int inter0 = var0 + var1;
    12          int inter1 = var0 + var1;
    13          int inter2 = var2 + var3;
    14          int inter3 = var2 + var3;
    15          tmp[i][0] = inter0 + inter2;
    16          tmp[i][2] = inter0 + inter2;
    17          tmp[i][1] = inter1 + inter3;
    18          tmp[i][3] = inter1 + inter3;
    19      }
    20      for (int i = 0; i < 4; i++) {
    21          int inter0 = tmp[0][i] + tmp[1][i];
    22          int inter1 = tmp[0][i] + tmp[1][i];
    23          int inter2 = tmp[2][i] + tmp[3][i];
    24          int inter3 = tmp[2][i] + tmp[3][i];
    25          var0 = inter0 + inter2;
    26          var2 = inter0 + inter2;
    27          var1 = inter1 + inter3;
    28          var3 = inter1 + inter3;
    29          sum += var0 + var1 + var2 + var3;
    30      }
    31
    32      return sum;
    33  }

Sergey


---


### compiler : `gcc`
### title : `malloc & memset -> calloc is not always an optimization`
### open_at : `2017-11-16T18:40:43Z`
### last_modified_date : `2021-08-20T18:21:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83022
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 42623
exemplar

We like to optimize malloc followed by memset into a calloc call.  Even when the memset is conditional.  That's well formed, but pessimizes, and noticeable when the size is large and we do unnecessary clearing.

The attached example, compiled on x86_64 with -O results in:
_Z1mmb:
	movl	$1, %esi
	jmp	calloc

But, it causes a noticeable performance regression, as 'c' is false sufficiently often and 's' is large sufficiently often.


---


### compiler : `gcc`
### title : `missing strlen optimization for strcmp of unequal strings`
### open_at : `2017-11-16T23:29:37Z`
### last_modified_date : `2019-11-07T16:46:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83026
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Here's another strcmp optimization opportunity.  When strlen() determines that two strings are of unequal length it's safe to assume they do not compare equal.  Therefore, in the function below, the conditional with the the strcmp() call can be folded into false.

$ cat c.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout c.c
void g (const char *a, const char *b)
{
  if (__builtin_strlen (a) == __builtin_strlen (b))
    return;

  if (!__builtin_strcmp (a, b))
    __builtin_abort ();
}

;; Function g (g, funcdef_no=0, decl_uid=1892, cgraph_uid=0, symbol_order=0)

g (const char * a, const char * b)
{
  long unsigned int _1;
  long unsigned int _2;
  int _3;

  <bb 2> [local count: 10000]:
  _1 = __builtin_strlen (a_5(D));
  _2 = __builtin_strlen (b_6(D));
  if (_1 == _2)
    goto <bb 5>; [20.97%]
  else
    goto <bb 3>; [79.03%]

  <bb 3> [local count: 7903]:
  _3 = __builtin_strcmp (a_5(D), b_6(D));
  if (_3 == 0)
    goto <bb 4>; [0.04%]
  else
    goto <bb 5>; [99.96%]

  <bb 4> [count: 0]:
  __builtin_abort ();

  <bb 5> [local count: 9997]:
  return;

}


---


### compiler : `gcc`
### title : `Tail call optimization not used in cases when first part of the result is returned from function`
### open_at : `2017-11-17T11:03:15Z`
### last_modified_date : `2021-08-03T00:08:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83031
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Following code:

struct ts {int a, b; };
ts foo();

int testing1() {
    return foo().a;
}


Produces suboptimal assembly:
testing1():
        sub     rsp, 8
        call    foo()
        add     rsp, 8
        ret

More optimal would be the following assembly:
testing1():
        jmp     foo() <== `a` is already in eax


---


### compiler : `gcc`
### title : `Copy elision for returning subobject`
### open_at : `2017-11-17T15:24:21Z`
### last_modified_date : `2023-06-27T18:59:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83032
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
Following code

struct trace {
    trace(trace const &) noexcept;
    ~trace();
};

struct ts { trace a, b; };
ts foo();

trace testing() {
    return foo().a;
}


Compiles into assembly that calls copy constructor and destructors for `trace` objects returned from `foo()`.

This could be optimized: return from `foo()` already puts `a` into a correct place for return from `testing()`. So all we need is no call destructor for `b`.

Optimal assembly should look close to the following:
testing():
        push    rbx
        mov     rbx, rdi
        sub     rsp, 16
        call    foo()
        lea     rdi, [rsp+15]
        call    trace::~trace()
        add     rsp, 16
        mov     rax, rbx
        pop     rbx
        ret

Current suboptimal assembly looks like:
testing():
        push    rbx
        mov     rbx, rdi
        sub     rsp, 16
        lea     rdi, [rsp+14]
        call    foo()
        lea     rsi, [rsp+14]
        mov     rdi, rbx
        call    trace::trace(trace const&)   <== Avoid this
        lea     rdi, [rsp+15]
        call    trace::~trace()
        lea     rdi, [rsp+14]
        call    trace::~trace()              <== Avoid this
        add     rsp, 16
        mov     rax, rbx
        pop     rbx
        ret

This optimization is very useful for C++ code, where returning std::pair, std::tuple, std::variant or std::optional is a common practice. For example following code could use copy elision for std::string inside std::optional:

std::optional<std::string> foo();
std::string testing() {
    return *foo(); // No copy/move constructor call
}


---


### compiler : `gcc`
### title : `Late VRP optimization`
### open_at : `2017-11-20T13:53:15Z`
### last_modified_date : `2022-01-13T21:15:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83072
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Before r254954, cmpmul-1.c was optimized during EVRP:

int f(int a, int b, int c){
  c |= 1;
  a *= c;
  b *= c;
  return a == b;
}

After that revision, the range deduced for c|1 contains zero (we may want to revisit that at some point, but that's a separate issue), so I changed the testcase to

int f(int a, int b, int c){
  if(c==0)__builtin_unreachable();
  a *= c;
  b *= c;
  return a == b;
}

which is only optimized during forwprop3, after __builtin_unreachable() is removed. Since EVRP knows how to perform this optimization, it may be worth investigating why it fails to perform it in this case.


---


### compiler : `gcc`
### title : `Range for VR_VARYING | [1, 1]`
### open_at : `2017-11-20T15:18:07Z`
### last_modified_date : `2022-01-13T18:54:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83073
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Before r254954, for x | 1 where x is VR_VARYING, we would deduce a VR_ANTI_RANGE ~[0, 0]. Since then, we deduce [-INT_MAX, INT_MAX]. Both make sense, but it is strange that we get something different for VR_VARYING and for a full range, and Richard thinks it is worth looking into (maybe zero_nonzero_bits_from_vr).

-O2 -fdump-tree-optimized-all
int f(int x){return x|1;}


---


### compiler : `gcc`
### title : `Int compare - different asm code for different return type`
### open_at : `2017-11-23T08:25:59Z`
### last_modified_date : `2021-08-14T21:28:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83123
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
int test1(int a, int b)
{
  return (a < 0) && (b < 0);
}
bool test2(int a, int b)
{
  return (a < 0) && (b < 0);
}

This produces following code, when compiled with -O2. For some reason 2nd function performs shifts first, then and. This is not necessary, you can and first, then shift. The same issue is for checking if any of numbers is negative - or can be executed first, then shift.

test1(int, int):
  and esi, edi
  mov eax, esi
  shr eax, 31
  ret
test2(int, int):
  mov eax, edi
  shr esi, 31
  shr eax, 31
  and eax, esi
  ret


---


### compiler : `gcc`
### title : `calloc zero initialization is not taken into account by gcc`
### open_at : `2017-11-23T13:39:04Z`
### last_modified_date : `2021-06-15T08:24:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83129
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
It seems that GCC does not know that calloc initialize the memory to zero.

The the following functions could be optimized to the same assemble, but only f3 is optimized to:
        xor     eax, eax
        ret

--------------
int f1() {
  char * i = __builtin_calloc(1, 1);
  return *i;
}

int f2() {
  struct s{int i;}* a = __builtin_calloc(1, sizeof(*a));

  return a->i;
}

int f3() {
    char * i = (char*)__builtin_calloc(1, 1);
    i[0] = 0;
    return *i;
}
--------------


---


### compiler : `gcc`
### title : `Missed tail-call opportunity with memmove and other mem*/str* functions which return the first argument back`
### open_at : `2017-11-24T12:47:38Z`
### last_modified_date : `2021-12-27T04:28:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83142
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The following is not tail-called even though memmove returns dest (and the compiler knows that):

struct A { int i; int j; int k; };
void *bar (struct A *dest, struct A *src)
{
  __builtin_memmove (dest, src, sizeof (struct A));
  return dest;
}


---


### compiler : `gcc`
### title : `__builtin_popcountl ((unsigned long long)unsigned_var) is not being optimized to __builtin_popcount (unsigned_var)`
### open_at : `2017-11-26T21:43:42Z`
### last_modified_date : `2021-08-19T18:10:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83171
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
Even if the size of the std::bitset is smaller than a long long, the compiler still emits a call to __popcountdi2 to count the number of bits set to 1.

This is unnecessary for example if the size of the bitset is 8. I believe the issue lies with the fact that __popcountdi2 is not inlined/optimized, even when using -O3.

Clang (-O2, using libstdc++) and MSVC(/O2, Visual 2017) seem to handle this correctly. 
Here is a link to the compiler explorer test :

https://godbolt.org/g/QTWgdb

The code used in the test :

//------------------------------------------
#include <bitset>
#include <stdint.h>

size_t foo(uint32_t value)
{
  std::bitset<8> bset = value;
  return bset.count();
}
//------------------------------------------

gcc -v
Target: x86_64-linux-gnu
Configured with: ../gcc-7.2.0/configure --prefix /root/staging --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --disable-multilib --disable-bootstrap --disable-multiarch --with-arch-32=i586 --enable-clocale=gnu --enable-languages=c,c++,go,fortran --enable-ld=yes --enable-gold=yes --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-linker-build-id --enable-lto --enable-plugins --enable-threads=posix --with-pkgversion=GCC-Explorer-Build
Thread model: posix
gcc version 7.2.0 (GCC-Explorer-Build)


---


### compiler : `gcc`
### title : `missing strlen optimization of the empty string`
### open_at : `2017-11-28T00:40:19Z`
### last_modified_date : `2021-09-05T05:47:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83190
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The tree-ssa-strlen pass keeps track of the lengths of strings as they are created, including by initialization of local arrays.  As a result, it is able to determine that the result of the strlen() call in function f() below is 0 and perform this substitution.  But the pass misses that the same is true in function g(), and so it unnecessarily emits a call to strlen() there.  It should be possible to enhance the pass to handle this case as well and perform the same transformation in both cases.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
int f (void)
{
  char a[] = "012\0";

  return __builtin_strlen (a + 3);   // folded into 0
}

int g (void)
{
  char a[] = "012\0";

  return __builtin_strlen (a + 4);   // not folded
}

;; Function f (f, funcdef_no=0, decl_uid=1892, cgraph_uid=0, symbol_order=0)

f ()
{
  <bb 2> [local count: 1073741825]:
  return 0;

}



;; Function g (g, funcdef_no=1, decl_uid=1896, cgraph_uid=1, symbol_order=1)

g ()
{
  char a[5];
  long unsigned int _1;
  int _4;

  <bb 2> [local count: 1073741825]:
  a = "012";
  _1 = __builtin_strlen (&MEM[(void *)&a + 4B]);
  _4 = (int) _1;
  a ={v} {CLOBBER};
  return _4;

}


---


### compiler : `gcc`
### title : `Try joining operations on consecutive array elements during tree vectorization`
### open_at : `2017-11-28T21:09:06Z`
### last_modified_date : `2019-04-11T12:21:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83202
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
void test(double data[4][4])
{
  for (int i = 0; i < 4; i++)
  {
    for (int j = i; j < 4; j+=2)
    {
      data[i][j] = data[i][j] * data[i][j];
      data[i][j+1] = data[i][j+1] * data[i][j+1];
    }
  }
}

gcc creates this:

test(double (*) [4]):
  vmovsd xmm0, QWORD PTR [rdi]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi], xmm0
  vmovsd xmm0, QWORD PTR [rdi+8]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+8], xmm0
  vmovsd xmm0, QWORD PTR [rdi+16]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+16], xmm0
  vmovsd xmm0, QWORD PTR [rdi+24]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+24], xmm0
  vmovsd xmm0, QWORD PTR [rdi+40]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+40], xmm0
  vmovsd xmm0, QWORD PTR [rdi+48]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+48], xmm0
  vmovsd xmm0, QWORD PTR [rdi+56]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+56], xmm0
  vmovsd xmm0, QWORD PTR [rdi+64]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+64], xmm0
  vmovsd xmm0, QWORD PTR [rdi+80]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+80], xmm0
  vmovsd xmm0, QWORD PTR [rdi+88]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+88], xmm0
  vmovsd xmm0, QWORD PTR [rdi+120]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+120], xmm0
  vmovsd xmm0, QWORD PTR [rdi+128]
  vmulsd xmm0, xmm0, xmm0
  vmovsd QWORD PTR [rdi+128], xmm0
  ret

clang detects that it is possible to use packed operations instead of scalar ones, and produces this. Please implement similar optimization in gcc too.

test(double (*) [4]): # @test(double (*) [4])
  vmovupd xmm0, xmmword ptr [rdi]
  vmovupd xmm1, xmmword ptr [rdi + 16]
  vmovupd xmm2, xmmword ptr [rdi + 40]
  vmovupd xmm3, xmmword ptr [rdi + 56]
  vmulpd xmm0, xmm0, xmm0
  vmovupd xmmword ptr [rdi], xmm0
  vmulpd xmm0, xmm1, xmm1
  vmovupd xmmword ptr [rdi + 16], xmm0
  vmulpd xmm0, xmm2, xmm2
  vmovupd xmmword ptr [rdi + 40], xmm0
  vmulpd xmm0, xmm3, xmm3
  vmovupd xmmword ptr [rdi + 56], xmm0
  vmovupd xmm0, xmmword ptr [rdi + 80]
  vmulpd xmm0, xmm0, xmm0
  vmovupd xmmword ptr [rdi + 80], xmm0
  vmovupd xmm0, xmmword ptr [rdi + 120]
  vmulpd xmm0, xmm0, xmm0
  vmovupd xmmword ptr [rdi + 120], xmm0
  ret


---


### compiler : `gcc`
### title : `C++: struct with char-array assumed to alias with everything`
### open_at : `2017-11-29T13:54:06Z`
### last_modified_date : `2019-02-01T20:47:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83215
### status : `RESOLVED`
### tags : `alias, missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
TBAA seems to be more conservative for C++ in the following example since GCC 7:

struct mytest {
  float a;
#ifdef WITH_BUFFER
  char buf[256];
#endif
};

int foo(mytest *m, int *i) {
  int tmp = *i; // first load
  m->a = 10.0f;
  return tmp + *i; // second load since GCC 7
}

Since GCC 7 this code generates two loads with -DWITH_BUFFER, without buf in the struct there is just one load. buf isn't touched at all in this function. This only affects C++-code, not the C-frontend.

TYPE_TYPLESS_STORAGE is set for this struct, aggregates with this flag set are assumed to alias with everything in gcc/alias.c(get_alias_set).

Seems to be introduced with revision 246866 (https://gcc.gnu.org/viewcvs/gcc?view=revision&revision=246866).


---


### compiler : `gcc`
### title : `False positive from -Wstringop-overflow on simple std::vector code`
### open_at : `2017-12-01T11:01:35Z`
### last_modified_date : `2019-03-01T06:26:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83239
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 42765
Pre-processed (-save-temps) on GCC 7.2.0 [Ubuntu 17.10]

Compiling this:


#include <vector>

void fn() {
  std::vector<int> a;
  
  int num = 2;
  while ( num > 0 ) {
    const auto a_size = a.size();
    if ( a_size < 3 ) {
      a.assign( 1, 0 );
    }
    else {
      a.resize( a_size - 2 ); // <-- I think problem is here
    }
    --num;
  }
}

...with `g++ -O3 -Wall -Werror a.cpp` results in:


In function ‘void fn()’:
cc1plus: error: ‘void* __builtin_memset(void*, int, long unsigned int)’: specified size 18446744073709551608 exceeds maximum object size 9223372036854775807 [-Werror=stringop-overflow=]
cc1plus: all warnings being treated as errors


I think this is a problem for three reasons:
 1. the warning doesn't tell me the location of the problem
 2. worse, the warning name "stringop-overflow" is actively misleading because the code containing the problem isn't using strings
 3. the warning is wrong: AFAIU, it's complaining about `a_size - 2` potentially being a huge unsigned integer due to wrapping below 0 but it's in an else clause that only executes if `a_size >= 3`.

I'm seeing this problem on both GCC 8.0.0 20171130 (Godbolt) and GCC 7.2.0 (my Ubuntu).


Though there are other open bugs relating to this warning:

 * bug 79929
 * bug 82076
 * bug 82103
 * bug 82646

...I'm not sure any cover this issue (eg the first one is about Fortran).

Thanks very much.


---


### compiler : `gcc`
### title : `simplify (int)a_long < 0 when we know a_long fits in int`
### open_at : `2017-12-01T18:16:51Z`
### last_modified_date : `2023-08-10T06:18:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83247
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
This code is based on basic_string_view::_S_compare, which should probably be improved (just cast if int is big enough, otherwise always return something in {-1,0,1}), but here is the missed optimization:

bool f(long long l){
  if(l>1000) return false;
  if(l<-1000) return true;
  return (int)l<0;
}

It is equivalent to l<0, but not that easy for the compiler. Without the cast in the last line, reassoc1 manages to get rid of the test l<-1000 and reassoc2 deals with l>1000. But with the cast, we are stuck. The easiest way I can think of is in VRP to notice that l fits in int and simplify (int)l<0 to l<0 (which the reassoc passes can handle later). On the other hand, as with all those narrowing / promotion transformations, it isn't always clear if it is beneficial by itself. If long long is emulated using 2 int-sized registers, (int)l<0 is clearly cheaper than l<0, which only becomes worth it because of the simplifications it allows.


---


### compiler : `gcc`
### title : `SELECT CASE  slower than IF/ELSE`
### open_at : `2017-12-03T09:59:59Z`
### last_modified_date : `2021-10-01T03:20:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83262
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
Created attachment 42781
Test case with timings

The following test case by Tran Quoc Viet from comp.lang.fortran
run with the arguments

$ ./a.out 100000000 200000000 300000000

compiled with -O3 shows that the Fortran's select case statement
is slower by a factor of 1.33 vs. an if/else form.

 Number of input arguments:            3
          GOTO: n =100000000, ss =-0.1480800E+10, time =    0.347 (s)
   SELECT CASE: n =100000000, ss =-0.1480800E+10, time =    0.199 (s)
       IF-Goto: n =100000000, ss =-0.1480800E+10, time =    0.151 (s)
     IF-noGoto: n =100000000, ss =-0.1480800E+10, time =    0.151 (s)
       IF-ELSE: n =100000000, ss =-0.1480800E+10, time =    0.151 (s)

          GOTO: n =200000000, ss =-0.2961600E+10, time =    0.702 (s)
   SELECT CASE: n =200000000, ss =-0.2961600E+10, time =    0.401 (s)
       IF-Goto: n =200000000, ss =-0.2961600E+10, time =    0.301 (s)
     IF-noGoto: n =200000000, ss =-0.2961600E+10, time =    0.301 (s)
       IF-ELSE: n =200000000, ss =-0.2961600E+10, time =    0.301 (s)

          GOTO: n =300000000, ss =-0.4442400E+10, time =    1.054 (s)
   SELECT CASE: n =300000000, ss =-0.4442400E+10, time =    0.602 (s)
       IF-Goto: n =300000000, ss =-0.4442400E+10, time =    0.451 (s)
     IF-noGoto: n =300000000, ss =-0.4442400E+10, time =    0.452 (s)
       IF-ELSE: n =300000000, ss =-0.4442400E+10, time =    0.451 (s)

           GOTO costs totally     2.103 (s)
    SELECT CASE costs totally     1.202 (s)
        IF-Goto costs totally     0.903 (s)
      IF-noGoto costs totally     0.903 (s)
        IF-ELSE costs totally     0.903 (s)

The Fortran FE translates the SELECT CASE into a switch.


---


### compiler : `gcc`
### title : `Unnecessary mask instruction generated`
### open_at : `2017-12-04T16:10:43Z`
### last_modified_date : `2021-08-14T08:13:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83272
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
Consider the following testcase:

char foo(unsigned char n)
{
	static const char map[16] = "wxyz";
	return map[n / 16];
}

gcc-7 -O2 -march=haswell -S testcase.c generates:

foo:
	shrb	$4, %dil
	andl	$15, %edi
	movzbl	map.2295(%rdi), %eax
	ret


On this platform, CHAR_BIT = 8 and UCHAR_MAX = 255
Therefore n / 16 is guaranteed to be less than 16.
Yet, GCC generates an unnecessary mask instruction (andl $15, %edi).

https://gcc.gnu.org/ml/gcc-help/2017-11/msg00102.html


---


### compiler : `gcc`
### title : `[feature request] Pragma or special syntax for guaranteed tail calls`
### open_at : `2017-12-08T14:04:51Z`
### last_modified_date : `2021-12-04T16:14:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83324
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c`
### version : `7.0`
### severity : `enhancement`
### contents :
I've seen that gcc7 supports the CALL_EXPR_MUST_TAIL_CALL flag internally. Unfortunately it is not exposed to the frontend.

As of now it seems only possible to use the flag via a plugin (See below). Regarding the plugin I couldn't figure out quickly how to register a pragma which annotates the subsequent call. Therefore I used a variable named __musttail as marker. Is there some documentation on how to create such a pragma properly?

test.c:

#include <stdio.h>

#define MUSTTAIL(f) ({                                \
      __attribute__ ((unused)) int __musttail;        \
      return f;                                       \
    })

void g() {
  printf("g called\n");
}

void f() {
  printf("f called\n");
  MUSTTAIL(g());
}

int main() {
  f();
  return 0;
}

musttail.cc:

#include <gcc-plugin.h>
#include <plugin-version.h>
#include <tree.h>
#include <c-family/c-pragma.h>
#include <stdio.h>

int plugin_is_GPL_compatible;

static tree process_musttail(tree* tp, int* walk_subtrees, void* data ATTRIBUTE_UNUSED) {
  static bool musttail = false;
  const char* name = get_name(*tp);
  if (name) {
    if (!strcmp(name, "__musttail"))
      musttail = true;
  }
  if (musttail && TREE_CODE(*tp) == CALL_EXPR) {
    CALL_EXPR_MUST_TAIL_CALL(*tp) = true;
    musttail = false;
  }
  return NULL_TREE;
}

static void callback(void *gcc_data, void *user_data) {
  tree fndecl = (tree)gcc_data;
  gcc_assert(TREE_CODE(fndecl) == FUNCTION_DECL);
  walk_tree(&DECL_SAVED_TREE(fndecl), process_musttail, NULL, NULL);
}

int plugin_init(plugin_name_args *plugin_info, plugin_gcc_version *version) {
  if (!plugin_default_version_check(version, &gcc_version))
    return 1;
  const char *plugin_name = plugin_info->base_name;
  register_callback(plugin_name, PLUGIN_PRE_GENERICIZE, callback, NULL);
  return 0;
}


---


### compiler : `gcc`
### title : `Missed optimization in math expression: can be used std::pow function`
### open_at : `2017-12-10T11:51:01Z`
### last_modified_date : `2018-11-02T08:33:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83348
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
gcc(trunk) with optimization flags '--std=c++17 -O3 -march=native -ffast-math' for this code:


double f(double a)
{
    return a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*
           a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*
           a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*
           a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*
           a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*
           a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a*a;
}


generates this assembly:

f(double):
        vmulsd  xmm1, xmm0, xmm0
        vmulsd  xmm1, xmm1, xmm1
        vmulsd  xmm1, xmm1, xmm1
        vmulsd  xmm1, xmm1, xmm1
        vmulsd  xmm0, xmm1, xmm0
        vmulsd  xmm1, xmm0, xmm0
        vmulsd  xmm0, xmm1, xmm0
        vmulsd  xmm0, xmm0, xmm0
        ret


But here it can be simplified by using std::pow function. If you will increase length of this multiply chain, gcc just will add more and more multiplications.


---


### compiler : `gcc`
### title : `Missed optimization in math expression: aggressive optimization with std::pow`
### open_at : `2017-12-10T12:45:58Z`
### last_modified_date : `2021-12-23T07:30:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83349
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc(trunk) with '--std=c++17 -O3 -march=native -ffast-math' flags for this code:

#include <cmath>

double test(double a, double x)
{
    return pow(a, x) * a * a * a * a;
}

generates this assembly:


test(double, double):
        sub     rsp, 24
        vmovsd  QWORD PTR [rsp+8], xmm0
        call    __pow_finite
        vmovsd  xmm2, QWORD PTR [rsp+8]
        vmulsd  xmm2, xmm2, xmm2
        vmulsd  xmm2, xmm2, xmm2
        vmulsd  xmm0, xmm2, xmm0
        add     rsp, 24
        ret


As you can see, me can simplify it by adding 4 to 'x' variable and after call std::pow.


---


### compiler : `gcc`
### title : `Missed optimization in math expression: missing cube of the sum formula`
### open_at : `2017-12-10T13:03:17Z`
### last_modified_date : `2021-08-10T04:36:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83350
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc(trunk) with '--std=c++17 -O3 -march=native -ffast-math' flags for this code:


double test(double a, double b)
{
    return a*a*a + 3.0*a*a*b + 3.0*a*b*b + b*b*b;
}


generates this assembly:


test(double, double):
        vmovsd  xmm2, QWORD PTR .LC0[rip]
        vmulsd  xmm3, xmm0, xmm0
        vmovapd xmm4, xmm0
        vfmadd132sd     xmm4, xmm1, xmm2
        vmulsd  xmm4, xmm4, xmm1
        vfmadd132sd     xmm2, xmm4, xmm3
        vmulsd  xmm1, xmm2, xmm1
        vfmadd132sd     xmm0, xmm1, xmm3
        ret
.LC0:
        .long   0
        .long   1074266112


But there is formula: a*a*a + 3.0*a*a*b + 3.0*a*b*b + b*b*b == (a + b)^3. And it can be compiled in faster way.


---


### compiler : `gcc`
### title : `Missed optimization in math expression: sin^2(a) + cos^2(a) == 1`
### open_at : `2017-12-10T13:48:03Z`
### last_modified_date : `2021-12-15T21:57:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83351
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc(trunk) with '--std=c++17 -O3 -march=native -ffast-math' flags for this code:


#include <cmath>

double test(double a)
{
    return cos(a) * cos(a) + sin(a) * sin(a);
}


generates this assembly:


test(double):
        sub     rsp, 24
        mov     rsi, rsp
        lea     rdi, [rsp+8]
        call    sincos
        vmovsd  xmm1, QWORD PTR [rsp+8]
        vmovsd  xmm0, QWORD PTR [rsp]
        add     rsp, 24
        vmulsd  xmm1, xmm1, xmm1
        vfmadd132sd     xmm0, xmm1, xmm0
        ret


But there is formula: sin^2(a) + cos^2(a) == 1. And it can be compiled in faster way.


---


### compiler : `gcc`
### title : `Missed optimization in math expression: sqrt(sqrt(a)) == pow(a, 1/4)`
### open_at : `2017-12-10T14:54:26Z`
### last_modified_date : `2019-03-04T13:15:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83352
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
gcc(trunk) with '--std=c++17 -O3 -march=native -ffast-math' flags for this code:


#include <cmath>

double test(double a)
{
    return sqrt(sqrt(a));
}


generates this assembly:


test(double): # @test(double)
  vsqrtsd xmm0, xmm0, xmm0
  vsqrtsd xmm0, xmm0, xmm0
  ret


But there is formula: sqrt(sqrt(a)) == pow(a, 1/4). And it can be compiled in faster way.


---


### compiler : `gcc`
### title : `Missed optimization in math expression: sin(asin(a)) == a`
### open_at : `2017-12-10T15:47:57Z`
### last_modified_date : `2021-08-10T04:37:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83353
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc(trunk) with '--std=c++17 -O3 -march=native -ffast-math' flags for this code:


#include <cmath>

double test(double a)
{
    return sin(asin(a));
}


generates this assembly:


test(double):
        sub     rsp, 8
        call    __asin_finite
        add     rsp, 8
        jmp     sin


But sin(asin(a)) == a. So there is no reason to call anything.


---


### compiler : `gcc`
### title : `Missed optimization (x86): Bit operations should be converted to arithmetic`
### open_at : `2017-12-11T16:49:53Z`
### last_modified_date : `2021-11-28T07:11:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83377
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.1`
### severity : `enhancement`
### contents :
GCC fails to optimise

if (x & 2) y = (x &~ 2)

into

if (x & 2) y = (x - 2)

in cases where that would be advantageous.

Here's one example, which I'm sure will upset the C pedants, but is relatively common in codebases which use typed pointers based on the lower bits.

int a(void *);
struct s { struct s *s; };
int b(void *x) {
        void *p = x;
        if ((unsigned long)x & 2)
                p = ((struct s *)((unsigned long)x & ~2UL))->s;
        return a(p);
}
int c(void *x) {
        void *p = x;
        if ((unsigned long)x & 2)
                p = ((struct s *)((unsigned long)x - 2))->s;
        return a(p);
}

On x86, the difference between the assembly output is clear; function c is smaller than function b:

0000000000000000 <b>:
   0:   40 f6 c7 02             test   $0x2,%dil
   4:   74 07                   je     d <b+0xd>
   6:   48 83 e7 fd             and    $0xfffffffffffffffd,%rdi
   a:   48 8b 3f                mov    (%rdi),%rdi
   d:   e9 00 00 00 00          jmpq   12 <b+0x12>
  12:   0f 1f 40 00             nopl   0x0(%rax)
  16:   66 2e 0f 1f 84 00 00    nopw   %cs:0x0(%rax,%rax,1)
  1d:   00 00 00 

0000000000000020 <c>:
  20:   40 f6 c7 02             test   $0x2,%dil
  24:   74 04                   je     2a <c+0xa>
  26:   48 8b 7f fe             mov    -0x2(%rdi),%rdi
  2a:   e9 00 00 00 00          jmpq   2f <c+0xf>

This is true with both -O3 and -O2.  I have other functions where the savings are greater (two insns and seven bytes), but the root cause is the same; a failure to optimise an AND into a SUB (which can then be fused with a load)

I'm filing this one under tree-optimisation rather than RTL, because I think it's a common feature in CPUs to have load (reg + offset), and relatively uncommon (in fact I don't know of one) to have load (reg & mask).  I'm sure some CPUs don't even have (reg + offset) addressing modes, but they wouldn't be harmed by such an optimisation.


---


### compiler : `gcc`
### title : `Optimize heap allocation as stack allocation`
### open_at : `2017-12-12T03:43:59Z`
### last_modified_date : `2021-12-22T09:15:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83384
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc(trunk) with '--std=c++17 -O3' for this code:


int test(int a)
{
    int res = 0;
    int* i = new int;

    for(*i = 0; *i < a; *i = *i + 1)
    {
        res += *i;
    }

    return res;
}


generates this:


test(int):
        push    rbx
        mov     ebx, edi
        mov     edi, 4
        call    operator new(unsigned long)
        test    ebx, ebx
        jle     .L8
        lea     eax, [rbx-1]
        cmp     eax, 17
        jbe     .L9
        mov     edx, ebx
        movdqa  xmm1, XMMWORD PTR .LC0[rip]
        xor     eax, eax
        pxor    xmm0, xmm0
        movdqa  xmm2, XMMWORD PTR .LC1[rip]
        shr     edx, 2
.L5:
        add     eax, 1
        paddd   xmm0, xmm1
        paddd   xmm1, xmm2
        cmp     eax, edx
        jne     .L5
        movdqa  xmm1, xmm0
        mov     edx, ebx
        psrldq  xmm1, 8
        and     edx, -4
        paddd   xmm0, xmm1
        movdqa  xmm1, xmm0
        psrldq  xmm1, 4
        paddd   xmm0, xmm1
        movd    eax, xmm0
        cmp     ebx, edx
        je      .L1
.L7:
        add     eax, edx
        add     edx, 1
        cmp     ebx, edx
        jg      .L7
.L1:
        pop     rbx
        ret
.L8:
        xor     eax, eax
        pop     rbx
        ret
.L9:
        xor     eax, eax
        xor     edx, edx
        jmp     .L7
.LC0:
        .long   0
        .long   1
        .long   2
        .long   3
.LC1:
        .long   4
        .long   4
        .long   4
        .long   4


clang(trunk) with '--std=c++17 -O3':


test(int): # @test(int)
  test edi, edi
  jle .LBB0_1
  lea eax, [rdi - 1]
  lea ecx, [rdi - 2]
  imul rcx, rax
  shr rcx
  lea eax, [rcx + rdi]
  add eax, -1
  ret
.LBB0_1:
  xor eax, eax
  ret



From C++14 compiler can remove heap allocation and use allocation on stack.


---


### compiler : `gcc`
### title : `std::tie generates sub-optimal code when used to compare POD fields`
### open_at : `2017-12-12T09:40:49Z`
### last_modified_date : `2023-06-18T19:29:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83389
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `enhancement`
### contents :
Created attachment 42845
cpp file with the example

Let's consider a simple struct with two integers:
struct data { int x, y; };

Let's suppose we want to write a comparison operator. A first attempt might be
bool operator==(data const& d1, data const& d2) {
  return d1.x == d2.x && d1.y == d2.y
}

An alternative approach (maybe this is a "bad" approach? )might be to use std::tie.

bool operator==(data const& d1, data const& d2) {
  return std::tie(d1.x, d1.y) == 
         std::tie(d2.x, d2.y);
}

At O3 gcc 7.2 generates exactly the same code. So far so good.
Let's suppose now that we want to add a couple of char fields:
struct data { int x, y; char c1, c2; };

At this point gcc starts generating sub-optimal assembler in the sense it fails to see that c1 and c2 are stored into a contiguous memory address and could be compared with a single CMP WORD PTR instruction instead of two CMP BYTE PTR (one for each byte).

For operator==() I see the compiler was able to generate a cmpw (where both bytes are packed and compared together)
        movl    4(%rsi), %ecx
        cmpl    %ecx, 4(%rdi)
        jne     .L1
        movzwl  8(%rsi), %eax
        cmpw    %ax, 8(%rdi)
        sete    %al
        ret

while for the std::tie example we generate two cmpb instructions:
        movl    4(%rsi), %ecx
        cmpl    %ecx, 4(%rdi)
        jne     .L7
        movzbl  8(%rsi), %ecx
        cmpb    %cl, 8(%rdi)
        jne     .L7
        movzbl  9(%rsi), %eax
        cmpb    %al, 9(%rdi)
        sete    %al
        ret

I don't really fully understand if this is a potential problem or simply it's a "Working As Designed" but I believe that the compiler for std::tie when all types are POD should be able to generate exactly the same code as operator==().


---


### compiler : `gcc`
### title : `void f() { } has zero arguments`
### open_at : `2017-12-12T16:20:10Z`
### last_modified_date : `2021-07-19T22:45:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83397
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `unknown`
### severity : `normal`
### contents :
https://godbolt.org/g/8ZcWKk

according to c11 6.7.6.3, function prototypes without arguments don't specify anything about their parameters

gcc is assuming that f1 takes an unspecified number of arguments, and as such it's treating it as potentially vararg, and that's (probably) why it clears eax before calling it

in my understanding, this requirement only applies to prototypes and not to function definitions

furthermore, the function is static and gcc is definitely able to see what it does so it's not needed either way


---


### compiler : `gcc`
### title : `Missed register promotion opportunities in loop`
### open_at : `2017-12-12T21:12:31Z`
### last_modified_date : `2020-05-12T00:44:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83403
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 42857
Source file

One of our performance folks ran across a large performance opportunity inside libxsmm.  The attached test shows a loop that would run about 25% faster if load/store motion could promote some memory expressions to registers within a loop.

The test contains this loop nest:

  for ( l_n = 0; l_n < 9; l_n++ ) {
    for ( l_m = 0; l_m < 10; l_m++ ) { C[(l_n*10)+l_m] = 0.0; }

    for ( l_k = 0; l_k < 17; l_k++ ) {
      #pragma simd
      for ( l_m = 0; l_m < 10; l_m++ ) {
        C[(l_n*10)+l_m] += A[(l_k*20)+l_m] * B[(l_n*20)+l_k];
      }
    }
  }

The cunrolli phase unrolls the innermost loop fully, so that we have ten separate accumulators.  Inside the l_k loop, each of these accumulators could be loaded once prior to the loop, remain in a register during the loop, and be stored once upon exit.  GCC is not able to do this.  Note that A, B, and C are restrict pointers, so there shouldn't be an aliasing issue.  The ten accumulators are array elements of C (common base register, different offsets).

It looks like load/store motion is done as part of pass_lim.  I don't know whether it is intended to be able to handle array elements.  For simpler cases, it looks like this gets optimized in lim2.

Prior to lim2, the l_k loop is as follows:

  <bb 4> [local count: 97603132]:
  # l_k_71 = PHI <0(3), l_k_32(4)>
  _51 = *_290;
  _52 = l_k_71 * 20;
  _54 = (long unsigned int) _52;
  _55 = _54 * 8;
  _56 = &A[0][0] + _55;
  _57 = *_56;
  _58 = l_n_70 * 20;
  _59 = _58 + l_k_71;
  _60 = (long unsigned int) _59;
  _61 = _60 * 8;
  _62 = &B[0][0] + _61;
  _63 = *_62;
  _64 = _57 * _63;
  _65 = _51 + _64;
  *_290 = _65;
  _75 = *_299;
  _77 = _52 + 1;
  _78 = (long unsigned int) _77;
  _79 = _78 * 8;
  _80 = &A[0][0] + _79;
  _81 = *_80;
  _88 = _63 * _81;
  _89 = _75 + _88;
  *_299 = _89;
  _99 = *_308;
  _101 = _52 + 2;
  _102 = (long unsigned int) _101;
  _103 = _102 * 8;
  _104 = &A[0][0] + _103;
  _105 = *_104;
  _112 = _63 * _105;
  _113 = _99 + _112;
  *_308 = _113;
  _123 = *_317;
  _125 = _52 + 3;
  _126 = (long unsigned int) _125;
  _127 = _126 * 8;
  _128 = &A[0][0] + _127;
  _129 = *_128;
  _136 = _63 * _129;
  _137 = _123 + _136;
  *_317 = _137;
  _147 = *_326;
  _149 = _52 + 4;
  _150 = (long unsigned int) _149;
  _151 = _150 * 8;
  _152 = &A[0][0] + _151;
  _153 = *_152;
  _160 = _63 * _153;
  _161 = _147 + _160;
  *_326 = _161;
  _171 = *_335;
  _173 = _52 + 5;
  _174 = (long unsigned int) _173;
  _175 = _174 * 8;
  _176 = &A[0][0] + _175;
  _177 = *_176;
  _184 = _63 * _177;
  _185 = _171 + _184;
  *_335 = _185;
  _195 = *_344;
  _197 = _52 + 6;
  _198 = (long unsigned int) _197;
  _199 = _198 * 8;
  _200 = &A[0][0] + _199;
  _201 = *_200;
  _208 = _63 * _201;
  _209 = _195 + _208;
  *_344 = _209;
  _219 = *_353;
  _221 = _52 + 7;
  _219 = *_353;
  _221 = _52 + 7;
  _222 = (long unsigned int) _221;
  _223 = _222 * 8;
  _224 = &A[0][0] + _223;
  _225 = *_224;
  _232 = _63 * _225;
  _233 = _219 + _232;
  *_353 = _233;
  _243 = *_362;
  _245 = _52 + 8;
  _246 = (long unsigned int) _245;
  _247 = _246 * 8;
  _248 = &A[0][0] + _247;
  _249 = *_248;
  _256 = _63 * _249;
  _257 = _243 + _256;
  *_362 = _257;
  _267 = *_371;
  _269 = _52 + 9;
  _270 = (long unsigned int) _269;
  _271 = _270 * 8;
  _272 = &A[0][0] + _271;
  _273 = *_272;
  _280 = _63 * _273;
  _281 = _267 + _280;
  *_371 = _281;
  l_k_32 = l_k_71 + 1;
  if (l_k_32 == 17)
    goto <bb 5>; [5.89%]
  else
    goto <bb 4>; [94.11%]

Each of *290, *299, ..., *_371 should be independently optimizable.

At -O3, if -fgcse-sm is specified, the RTL store motion pass is able to optimize the code.  I'm told that this pass is not on by default at -O3 for (at least) reasons of inefficiency.  But this would tend to indicate that we don't have an aliasing problem, so this looks like just a missed opportunity in the GIMPLE optimizations.


---


### compiler : `gcc`
### title : `-Wformat-truncation may incorrectly report truncation`
### open_at : `2017-12-14T18:32:06Z`
### last_modified_date : `2019-08-27T18:02:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83431
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization, patch`
### component : `c++`
### version : `8.0`
### severity : `normal`
### contents :
This looks like another missing optimization - -Wformat-truncation does not take into account that there is "if" which checks that truncation will not happen.

[code]
#include <stdio.h>
#include <string.h>

struct S
{
    char str[20];
    char out[10];
};

void test(S* s)
{
    if (strlen(s->str) < sizeof(s->out) - 2)
        snprintf(s->out, sizeof(s->out), "[%s]", s->str);
}
[/code]

[out]
$ g++ -c -o test.o test.cc -O2 -Wall
test.cc: In function ‘void test(S*)’:
test.cc:10:6: warning: ‘%s’ directive output may be truncated writing up to 19 bytes into a region of size 9 [-Wformat-truncation=]
 void test(S* s)
      ^~~~
test.cc:13:17: note: ‘snprintf’ output between 3 and 22 bytes into a destination of size 10
         snprintf(s->out, sizeof(s->out), "[%s]", s->str);
         ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[/out]

g++ --version
g++ (GCC) 8.0.0 20171210 (experimental)


---


### compiler : `gcc`
### title : `Register spilling in AVX code`
### open_at : `2017-12-19T08:17:40Z`
### last_modified_date : `2018-11-16T13:35:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83479
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `7.2.0`
### severity : `normal`
### contents :
Here is snipped of code which performs some calculations on matrix. It repeatedly transforms some (N * N) matrix into (N-1 * N-1) one, and returns final scalar value. gcc for some reason is not able to detect that intermediate values are not needed anymore, and starts spilling. Code below is from gcc 7.2, trunk version also generates similar code. Code was compiled with "-O3 -march=haswell".
BTW, clang 5 properly handles this and does not spill.

[code]
#include "immintrin.h"

double test(const double data[9][8])
{
  __m256d vLastRow, vLastCol, vSqrtRow, vSqrtCol;

  __m256d v1 = _mm256_load_pd (&data[0][0]);
  __m256d v2 = _mm256_load_pd (&data[1][0]);
  __m256d v3 = _mm256_load_pd (&data[2][0]);
  __m256d v4 = _mm256_load_pd (&data[3][0]);
  __m256d v5 = _mm256_load_pd (&data[4][0]);
  __m256d v6 = _mm256_load_pd (&data[5][0]);
  __m256d v7 = _mm256_load_pd (&data[6][0]);
  __m256d v8 = _mm256_load_pd (&data[7][0]);

  // 8
  vLastRow = _mm256_load_pd (&data[9][0]);
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[1]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v2 = (v2 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[2]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v3 = (v3 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[3]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v4 = (v4 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[4]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v5 = (v5 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[5]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v6 = (v6 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[6]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v7 = (v7 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[7]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v8 = (v8 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  // 7
  vLastRow = v8;
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[1]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v2 = (v2 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[2]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v3 = (v3 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[3]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v4 = (v4 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[4]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v5 = (v5 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[5]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v6 = (v6 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[6]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v7 = (v7 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  // 6
  vLastRow = v7;
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[1]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v2 = (v2 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[2]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v3 = (v3 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[3]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v4 = (v4 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[4]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v5 = (v5 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[5]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v6 = (v6 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  // 5
  vLastRow = v6;
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[1]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v2 = (v2 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[2]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v3 = (v3 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[3]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v4 = (v4 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[4]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v5 = (v5 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  // 4
  vLastRow = v5;
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[1]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v2 = (v2 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[2]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v3 = (v3 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[3]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v4 = (v4 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  // 3
  vLastRow = v4;
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[1]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v2 = (v2 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[2]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v3 = (v3 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  // 2
  vLastRow = v3;
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;
  vLastCol = _mm256_set1_pd(vLastRow[1]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v2 = (v2 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  // 1
  vLastRow = v2;
  vSqrtRow = _mm256_sqrt_pd(vLastRow);

  vLastCol = _mm256_set1_pd(vLastRow[0]);
  vSqrtCol = _mm256_sqrt_pd(vLastCol);
  v1 = (v1 - vLastRow * vLastCol) * vSqrtRow * vSqrtCol;

  return v1[0];
}
[/code]

[out]
test(double const (*) [8]):
  lea r10, [rsp+8]
  and rsp, -32
  push QWORD PTR [r10-8]
  push rbp
  mov rbp, rsp
  push r10
  sub rsp, 1040
  vmovapd ymm7, YMMWORD PTR [rdi+576]
  vbroadcastsd ymm0, QWORD PTR [rbp-16]
  vpermpd ymm2, ymm7, 0
  vsqrtpd ymm15, ymm7
  vpermpd ymm12, ymm7, 255
  vsqrtpd ymm5, ymm2
  vsqrtpd ymm4, ymm12
  vmovapd YMMWORD PTR [rbp-560], ymm2
  vsqrtpd ymm2, ymm0
  vmovapd YMMWORD PTR [rbp-592], ymm5
  vpermpd ymm5, ymm7, 85
  vmovapd YMMWORD PTR [rbp-528], ymm5
  vsqrtpd ymm6, ymm5
  vbroadcastsd ymm5, QWORD PTR [rbp+8]
  vmovapd YMMWORD PTR [rbp-208], ymm4
  vbroadcastsd ymm4, QWORD PTR [rbp+0]
  vmovapd ymm14, ymm5
  vsqrtpd ymm9, ymm5
  vfnmadd213pd ymm14, ymm7, YMMWORD PTR [rdi+448]
  vsqrtpd ymm8, ymm4
  vmovapd YMMWORD PTR [rbp-624], ymm6
  vpermpd ymm6, ymm7, 170
  vmovapd YMMWORD PTR [rbp-496], ymm6
  vsqrtpd ymm1, ymm6
  vmulpd ymm6, ymm14, ymm15
  vmovapd YMMWORD PTR [rbp-656], ymm1
  vbroadcastsd ymm1, QWORD PTR [rbp-8]
  vsqrtpd ymm3, ymm1
  vmulpd ymm6, ymm6, ymm9
  vpermpd ymm13, ymm6, 0
  vsqrtpd ymm14, ymm6
  vsqrtpd ymm10, ymm13
  vmovapd YMMWORD PTR [rbp-464], ymm13
  vpermpd ymm13, ymm6, 170
  vmovapd YMMWORD PTR [rbp-688], ymm10
  vpermpd ymm10, ymm6, 85
  vsqrtpd ymm11, ymm10
  vmovapd YMMWORD PTR [rbp-432], ymm10
  vmovapd YMMWORD PTR [rbp-720], ymm11
  vsqrtpd ymm11, ymm13
  vmulpd ymm13, ymm6, ymm13
  vmovapd YMMWORD PTR [rbp-752], ymm11
  vpermpd ymm11, ymm6, 255
  vsqrtpd ymm9, ymm11
  vmovapd YMMWORD PTR [rbp-144], ymm11
  vmovapd YMMWORD PTR [rbp-784], ymm9
  vmulpd ymm9, ymm6, ymm4
  vfnmadd213pd ymm4, ymm7, YMMWORD PTR [rdi+384]
  vmulpd ymm4, ymm4, ymm15
  vfmsub132pd ymm4, ymm9, ymm8
  vmulpd ymm5, ymm4, ymm14
  vmulpd ymm5, ymm5, ymm8
  vpermpd ymm4, ymm5, 0
  vsqrtpd ymm11, ymm5
  vpermpd ymm10, ymm5, 255
  vsqrtpd ymm8, ymm4
  vmovapd YMMWORD PTR [rbp-400], ymm4
  vmovapd YMMWORD PTR [rbp-816], ymm8
  vpermpd ymm8, ymm5, 85
  vsqrtpd ymm9, ymm8
  vmovapd YMMWORD PTR [rbp-368], ymm8
  vsqrtpd ymm8, ymm10
  vmulpd ymm10, ymm10, ymm5
  vmovapd YMMWORD PTR [rbp-848], ymm9
  vpermpd ymm9, ymm5, 170
  vsqrtpd ymm4, ymm9
  vmovapd YMMWORD PTR [rbp-176], ymm9
  vfnmadd213pd ymm12, ymm7, YMMWORD PTR [rdi+192]
  vmovapd YMMWORD PTR [rbp-912], ymm8
  vmulpd ymm8, ymm1, ymm5
  vmovapd YMMWORD PTR [rbp-80], ymm14
  vmulpd ymm9, ymm6, ymm0
  vmovapd YMMWORD PTR [rbp-880], ymm4
  vmulpd ymm4, ymm6, ymm1
  vfnmadd213pd ymm1, ymm7, YMMWORD PTR [rdi+320]
  vmulpd ymm1, ymm1, ymm15
  vfmsub231pd ymm4, ymm1, ymm3
  vmulpd ymm4, ymm4, ymm14
  vmovapd ymm14, ymm11
  vmovapd YMMWORD PTR [rbp-112], ymm14
  vfmsub132pd ymm4, ymm8, ymm3
  vmulpd ymm4, ymm4, ymm11
  vmulpd ymm4, ymm4, ymm3
  vpermpd ymm1, ymm4, 0
  vpermpd ymm11, ymm4, 170
  vpermpd ymm8, ymm4, 255
  vsqrtpd ymm3, ymm1
  vmovapd YMMWORD PTR [rbp-336], ymm1
  vmovapd YMMWORD PTR [rbp-944], ymm3
  vpermpd ymm3, ymm4, 85
  vsqrtpd ymm1, ymm3
  vmovapd YMMWORD PTR [rbp-304], ymm3
  vmulpd ymm3, ymm0, ymm4
  vmovapd YMMWORD PTR [rbp-976], ymm1
  vsqrtpd ymm1, ymm11
  vmulpd ymm11, ymm11, ymm4
  vmovapd YMMWORD PTR [rbp-1008], ymm1
  vsqrtpd ymm1, ymm8
  vmulpd ymm8, ymm8, ymm4
  vmovapd YMMWORD PTR [rbp-1040], ymm1
  vmulpd ymm1, ymm0, ymm5
  vfnmadd213pd ymm0, ymm7, YMMWORD PTR [rdi+256]
  vmulpd ymm0, ymm0, ymm15
  vfmsub231pd ymm9, ymm0, ymm2
  vmulpd ymm9, ymm9, YMMWORD PTR [rbp-80]
  vmulpd ymm0, ymm12, ymm15
  vmovapd ymm12, YMMWORD PTR [rbp-1040]
  vfmsub231pd ymm1, ymm9, ymm2
  vmulpd ymm1, ymm1, ymm14
  vsqrtpd ymm14, ymm4
  vfmsub132pd ymm1, ymm3, ymm2
  vmulpd ymm1, ymm1, ymm14
  vmulpd ymm1, ymm1, ymm2
  vpermpd ymm2, ymm1, 0
  vpermpd ymm9, ymm1, 85
  vsqrtpd ymm3, ymm2
  vmovapd YMMWORD PTR [rbp-272], ymm2
  vsqrtpd ymm2, ymm9
  vmovapd YMMWORD PTR [rbp-240], ymm9
  vpermpd ymm9, ymm1, 170
  vmovapd YMMWORD PTR [rbp-1072], ymm3
  vsqrtpd ymm3, ymm9
  vmovapd YMMWORD PTR [rbp-1104], ymm2
  vmulpd ymm9, ymm9, ymm1
  vmovapd YMMWORD PTR [rbp-1136], ymm3
  vpermpd ymm3, ymm1, 255
  vsqrtpd ymm2, ymm3
  vmulpd ymm3, ymm3, ymm1
  vmovapd YMMWORD PTR [rbp-1168], ymm2
  vmulpd ymm2, ymm6, YMMWORD PTR [rbp-144]
  vfmsub132pd ymm0, ymm2, YMMWORD PTR [rbp-208]
  vmovapd YMMWORD PTR [rbp-144], ymm14
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-80]
  vfmsub132pd ymm0, ymm10, YMMWORD PTR [rbp-784]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-112]
  vfmsub132pd ymm0, ymm8, YMMWORD PTR [rbp-912]
  vmulpd ymm0, ymm0, ymm14
  vsqrtpd ymm14, ymm1
  vfmsub132pd ymm12, ymm3, ymm0
  vmulpd ymm2, ymm12, ymm14
  vmulpd ymm2, ymm2, YMMWORD PTR [rbp-1168]
  vsqrtpd ymm10, ymm2
  vpermpd ymm12, ymm2, 0
  vmovapd YMMWORD PTR [rbp-208], ymm12
  vmovapd YMMWORD PTR [rbp-784], ymm10
  vsqrtpd ymm10, ymm12
  vmovapd YMMWORD PTR [rbp-912], ymm10
  vpermpd ymm10, ymm2, 85
  vsqrtpd ymm8, ymm10
  vmulpd ymm10, ymm10, ymm2
  vmovapd YMMWORD PTR [rbp-1040], ymm8
  vmovapd ymm0, YMMWORD PTR [rbp-496]
  vpermpd ymm8, ymm2, 170
  vfnmadd213pd ymm0, ymm7, YMMWORD PTR [rdi+128]
  vmulpd ymm3, ymm5, YMMWORD PTR [rbp-176]
  vsqrtpd ymm12, ymm8
  vmovapd YMMWORD PTR [rbp-176], ymm14
  vmulpd ymm8, ymm8, ymm2
  vmulpd ymm0, ymm0, ymm15
  vfmsub132pd ymm0, ymm13, YMMWORD PTR [rbp-656]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-80]
  vfmsub132pd ymm0, ymm3, YMMWORD PTR [rbp-752]
  vmovapd ymm3, YMMWORD PTR [rbp-784]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-112]
  vfmsub132pd ymm0, ymm11, YMMWORD PTR [rbp-880]
  vmovapd ymm11, YMMWORD PTR [rbp-1136]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-144]
  vfmsub132pd ymm0, ymm9, YMMWORD PTR [rbp-1008]
  vmulpd ymm0, ymm0, ymm14
  vmulpd ymm14, ymm5, YMMWORD PTR [rbp-368]
  vfmsub132pd ymm11, ymm8, ymm0
  vmulpd ymm0, ymm4, YMMWORD PTR [rbp-304]
  vmovapd YMMWORD PTR [rbp-304], ymm14
  vmulpd ymm3, ymm3, ymm11
  vmulpd ymm3, ymm3, ymm12
  vmulpd ymm12, ymm1, YMMWORD PTR [rbp-240]
  vmovapd YMMWORD PTR [rbp-240], ymm0
  vmulpd ymm0, ymm6, YMMWORD PTR [rbp-432]
  vpermpd ymm8, ymm3, 0
  vsqrtpd ymm13, ymm3
  vpermpd ymm9, ymm3, 85
  vsqrtpd ymm11, ymm8
  vmovapd ymm14, ymm0
  vmovapd ymm0, YMMWORD PTR [rbp-528]
  vfnmadd213pd ymm0, ymm7, YMMWORD PTR [rdi+64]
  vmovapd YMMWORD PTR [rbp-496], ymm11
  vsqrtpd ymm11, ymm9
  vmulpd ymm9, ymm9, ymm3
  vmulpd ymm0, ymm0, ymm15
  vfmsub132pd ymm0, ymm14, YMMWORD PTR [rbp-624]
  vmovapd ymm14, YMMWORD PTR [rbp-304]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-80]
  vfmsub132pd ymm0, ymm14, YMMWORD PTR [rbp-720]
  vmovapd ymm14, YMMWORD PTR [rbp-240]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-112]
  vfmsub132pd ymm0, ymm14, YMMWORD PTR [rbp-848]
  vsqrtpd ymm14, ymm2
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-144]
  vfmsub132pd ymm0, ymm12, YMMWORD PTR [rbp-976]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-176]
  vfmsub132pd ymm0, ymm10, YMMWORD PTR [rbp-1104]
  vmulpd ymm6, ymm6, YMMWORD PTR [rbp-464]
  vmulpd ymm5, ymm5, YMMWORD PTR [rbp-400]
  vmulpd ymm4, ymm4, YMMWORD PTR [rbp-336]
  vmulpd ymm1, ymm1, YMMWORD PTR [rbp-272]
  vmulpd ymm0, ymm0, ymm14
  vfmsub132pd ymm0, ymm9, YMMWORD PTR [rbp-1040]
  vmulpd ymm2, ymm2, YMMWORD PTR [rbp-208]
  vmulpd ymm3, ymm8, ymm3
  vmulpd ymm0, ymm0, ymm13
  vmulpd ymm11, ymm0, ymm11
  vpermpd ymm0, ymm11, 0
  vsqrtpd ymm12, ymm11
  vmulpd ymm11, ymm0, ymm11
  vsqrtpd ymm10, ymm0
  vmovapd ymm0, YMMWORD PTR [rbp-560]
  vfnmadd213pd ymm7, ymm0, YMMWORD PTR [rdi]
  vmulpd ymm7, ymm7, ymm15
  vfmsub132pd ymm7, ymm6, YMMWORD PTR [rbp-592]
  vmulpd ymm7, ymm7, YMMWORD PTR [rbp-80]
  vfmsub132pd ymm7, ymm5, YMMWORD PTR [rbp-688]
  vmulpd ymm7, ymm7, YMMWORD PTR [rbp-112]
  vfmsub132pd ymm7, ymm4, YMMWORD PTR [rbp-816]
  vmulpd ymm0, ymm7, YMMWORD PTR [rbp-144]
  vfmsub132pd ymm0, ymm1, YMMWORD PTR [rbp-944]
  vmulpd ymm0, ymm0, YMMWORD PTR [rbp-176]
  vfmsub132pd ymm0, ymm2, YMMWORD PTR [rbp-1072]
  vmulpd ymm0, ymm0, ymm14
  vfmsub132pd ymm0, ymm3, YMMWORD PTR [rbp-912]
  vmulpd ymm0, ymm0, ymm13
  vfmsub132pd ymm0, ymm11, YMMWORD PTR [rbp-496]
  vmulpd ymm0, ymm0, ymm12
  vmulpd ymm0, ymm0, ymm10
  vzeroupper
  add rsp, 1040
  pop r10
  pop rbp
  lea rsp, [r10-8]
  ret
[/out]


---


### compiler : `gcc`
### title : `[8/9 Regression] Missing optimization: useless instructions should be dropped`
### open_at : `2017-12-20T21:45:31Z`
### last_modified_date : `2023-05-15T06:07:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83518
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
gcc (trunk) with '-O3 -std=c++17' for this code:

unsigned test()
{
    int arr[] = {5,4,3,2,1};
    int sum = 0;

    for(int i = 0;i < 5;++i)
    {
        for(int j = 0; j < 5; ++j)
        {
            int t = arr[i];
            arr[i] = arr[j];
            arr[j] = t;
        }
    }

    for(int i = 0; i < 5; ++i)
    {
        sum += arr[i];
    }

    return sum;
}


generates it:

test():
  movdqa xmm0, XMMWORD PTR .LC0[rip]
  movaps XMMWORD PTR [rsp-40], xmm0
  mov rax, QWORD PTR [rsp-32]
  mov DWORD PTR [rsp-32], 1
  mov QWORD PTR [rsp-40], rax
  mov DWORD PTR [rsp-28], 5
  movdqa xmm0, XMMWORD PTR [rsp-40]
  movdqa xmm1, xmm0
  psrldq xmm1, 8
  paddd xmm0, xmm1
  movdqa xmm1, xmm0
  psrldq xmm1, 4
  paddd xmm0, xmm1
  movd eax, xmm0
  add eax, 4
  ret
.LC0:
  .long 5
  .long 4
  .long 3
  .long 2


clang (trunk) with '-O3 -std=c++17':

test(): # @test()
  mov eax, 15
  ret


---


### compiler : `gcc`
### title : `Missed optimization with int overflow`
### open_at : `2017-12-21T22:28:29Z`
### last_modified_date : `2022-01-12T15:29:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83541
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc(trunk) with '-O3 -std=c++17 -ffast-math' for this:

#include <limits>

int test(int x)
{
    if(x == std::numeric_limits<int>::max())
    {
        return x+1;
    }
    return 42;
}

generates this:

test(int):
  cmp edi, 2147483647
  mov edx, -2147483648
  mov eax, 42
  cmove eax, edx
  ret


But branch with condition is UB, so you can just delete it and simply return 42.


---


### compiler : `gcc`
### title : `strlen of a local array member not optimized on some targets`
### open_at : `2017-12-22T00:00:25Z`
### last_modified_date : `2021-12-27T07:39:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83543
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Bug 83462 reports (among others) a failure in the new c-c++-common/Warray-bounds-4.c test on powerpc64le.  The failure is due to a strlen optimization that's for some reason not working on this target (and on some others, including arm-none-eabi) but that works fine on x86_64-linux.  The test case below shows the difference in cross-compiler output between these three architectures.

$ (set -x && cat z.c && for arch in '' arm-none-eabi powerpc64le-linux; do /ssd/build/$arch/gcc-git/gcc/xgcc -B /ssd/build/$arch/gcc-git/gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c; done)
+ cat z.c
struct S { char a[7]; };

void f (void)
{
  struct S s = { "12345" };
  if (__builtin_strlen (s.a) != 5)
    __builtin_abort ();
}
+ for arch in ''\'''\''' arm-none-eabi powerpc64le-linux
+ /ssd/build//gcc-git/gcc/xgcc -B /ssd/build//gcc-git/gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c

;; Function f (f, funcdef_no=0, decl_uid=1894, cgraph_uid=0, symbol_order=0)

f ()
{
  <bb 2> [local count: 1073741825]:
  return;

}


+ for arch in ''\'''\''' arm-none-eabi powerpc64le-linux
+ /ssd/build/arm-none-eabi/gcc-git/gcc/xgcc -B /ssd/build/arm-none-eabi/gcc-git/gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c

;; Function f (f, funcdef_no=0, decl_uid=4155, cgraph_uid=0, symbol_order=0)

f ()
{
  struct S s;
  unsigned int _1;

  <bb 2> [local count: 1073741825]:
  s = *.LC0;
  _1 = __builtin_strlen (&s.a);
  if (_1 != 5)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  s ={v} {CLOBBER};
  return;

}


+ for arch in ''\'''\''' arm-none-eabi powerpc64le-linux
+ /ssd/build/powerpc64le-linux/gcc-git/gcc/xgcc -B /ssd/build/powerpc64le-linux/gcc-git/gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c

;; Function f (f, funcdef_no=0, decl_uid=2784, cgraph_uid=0, symbol_order=0)

f ()
{
  struct S s;
  long unsigned int _1;

  <bb 2> [local count: 1073741825]:
  s = *.LC0;
  _1 = __builtin_strlen (&s.a);
  if (_1 != 5)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  s ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `Missed optimization opportunity for constant folding`
### open_at : `2017-12-22T02:33:26Z`
### last_modified_date : `2021-08-14T22:33:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83544
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
We compiled two programs (A1.c, A2.c) by GCC-8.0.0 with -O3 option.
We expect the resulting assembly codes would be the same, or at least
A1.c would result in simpler code.  However, the result was the opposite.


+---------------------------------+---------------------------------+
|                A1.c             |              A2.c               |
+---------------------------------+---------------------------------+
|int main (void)                  |int main (void)                  |
|{                                |{                                |
|  volatile int x = 1;            |  volatile int x = 1;            |
|                                 |  int a = 1;                     |
|                                 |                                 |
|  int t = 1 / ( 0 < ( 1/x ) );   |  int t = a / ( 0 < ( 1/x ) );   |
|  if (t != 1) __builtin_abort(); |  if (t != 1) __builtin_abort(); |
|  return 0;                      |  return 0;                      |
|}                                |}                                |
+---------------------------------+---------------------------------+

+----------------------------+----------------------------+
|A1.s (gcc-8.0.0 A1.c -O3 -S)|A2.s (gcc-8.0.0 A2.c -O3 -S)|
+----------------------------+----------------------------+
|main:                       |main:                       |
|.LFB0:                      |.LFB0:                      |
|  .cfi_startproc            |  .cfi_startproc            |
|  movl  $1, %eax            |                            |
|  movl  $1, -4(%rsp)        |  movl  $1, -4(%rsp)        |
|  movl  -4(%rsp), %ecx      |  movl  -4(%rsp), %eax      |
|  cltd                      |                            |
|  idivl %ecx                |                            |
|  cmpl  $1, %eax            |                            |
|  je    .L2                 |                            |
|  ud2                       |                            |
|  .p2align 4,,10            |                            |
|  .p2align 3                |                            |
|.L2:                        |                            |
|  xorl  %eax, %eax          |  xorl  %eax, %eax          |
|  ret                       |  ret                       |
|  .cfi_endproc              |  .cfi_endproc              |
|.LFE0:                      |.LFE0:                      |
|  .size  main, .-main       |   .size  main, .-main      |
+----------------------------+----------------------------+

gcc-8.0 (GCC) 8.0.0 20171215 (experimental)
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


---


### compiler : `gcc`
### title : `std::boyer_moore_searcher is slow searching through arrays of std::byte`
### open_at : `2017-12-28T03:07:28Z`
### last_modified_date : `2019-06-15T00:26:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83607
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `7.2.1`
### severity : `normal`
### contents :
Created attachment 42975
C++17 program demonstrating the issue

When using std::boyer_moore_searcher on an array of std::bytes, it is 3 times as slow as memmem.  When using std::boyer_moore_searcher on an array of unsigned char, it is faster them memmem.  This is because the searcher uses an array when searching through unsigned chars, but uses a hash table when searching through std::byte.  Searches through std::byte should also use an array, since it has the same size and equality semantics as unsigned char.


---


### compiler : `gcc`
### title : `Come up with __builtin_expect_with_probabilty`
### open_at : `2017-12-28T12:24:21Z`
### last_modified_date : `2021-07-17T20:10:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83610
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `4.9.0`
### severity : `normal`
### contents :
[code]
void f1();
void f2();

void test(int a, int b, int c, int d, int n, int k)
{
  int val = a & b;
  if (__builtin_expect(!!(n == k), 0))
    val &= c;
  if (__builtin_expect(!!(n == 10 - k), 0))
    val &= d;
  if (val)
    f1();
  else
    f2();
}
[/code]

This code compiled with gcc 4.8.5 generates branches as expected:

[asm]
test(int, int, int, int, int, int):
  and edi, esi
  cmp r8d, r9d
  je .L6
.L2:
  mov eax, 10
  sub eax, r9d
  cmp r8d, eax
  je .L7
.L3:
  test edi, edi
  jne .L8
  jmp f2()
.L8:
  jmp f1()
.L7:
  and edi, ecx
  jmp .L3
.L6:
  and edi, edx
  jmp .L2
[/asm]

When this code is compiled with gcc 4.9.0 or higher, it generates branchless code like below. In my case it is slower than version with branches. I wanted to   convince compiler to generate this version of code by using __builtin_expect, but for some reason it does not work.

[asm]
test(int, int, int, int, int, int):
  and esi, edi
  mov eax, 10
  and edx, esi
  cmp r8d, r9d
  cmove esi, edx
  sub eax, r9d
  and ecx, esi
  cmp r8d, eax
  cmove esi, ecx
  test esi, esi
  jne .L6
  jmp f2()
.L6:
  jmp f1()
[/asm]


---


### compiler : `gcc`
### title : `excessive strlen range after a strcat of non-empty string`
### open_at : `2017-12-31T21:34:02Z`
### last_modified_date : `2019-10-15T22:41:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83642
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The test case below (derived from bug 83640, comment #4) highlights a possible optimization opportunity in the strlen pass.  So that the difference between pointers to the first and just past the last byte of any array must be at most PTRDIFF_MAX bytes the size of the largest array is PTRDIFF_MAX bytes.  Since the length of the string literal appended to the DST array by the first strcat call below is known to be 2, then length of the DST string must be less than PTRDIFF_MAX - 2.  Therefore the test can be assumed to never evaluate to true and the call to abort can be eliminated.

$ cat a.c && gcc -O2 -S -fdump-tree-vrp=/dev/stdout a.c
char *foo (void);

void
bar (char *dst, char *src)
{
  __SIZE_TYPE__ n = __builtin_strlen (dst);

  __builtin_strcat (dst, "*/");   // implies n < PTRDIFF_MAX - 3
  __builtin_strcat (dst, src);

  if (n >= __PTRDIFF_MAX__ - 2)   // must be false
    __builtin_abort ();           // can be eliminated
}

...
Value ranges after VRP:

dst_2(D): VARYING
n_3: [0, 9223372036854775806]
_4: VARYING
_9: [2, 9223372036854775808]
_10: VARYING
dst_11: ~[0B, 0B]  EQUIVALENCES: { dst_2(D) } (1 elements)


bar (char * dst, char * src)
{
  long unsigned int n;
  char * _4;
  long unsigned int _9;
  char * _10;

  <bb 2> [local count: 1073741825]:
  n_3 = __builtin_strlen (dst_2(D));
  _4 = dst_2(D) + n_3;
  __builtin_memcpy (_4, "*/", 2);
  _9 = n_3 + 2;
  _10 = dst_2(D) + _9;
  __builtin_strcpy (_10, src_6(D));
  if (n_3 > 9223372036854775804)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  return;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] 20% slowdown of linux kernel AES cipher`
### open_at : `2018-01-02T16:59:57Z`
### last_modified_date : `2023-07-07T10:32:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83651
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.1`
### severity : `normal`
### contents :
Following the discussion on PR83356, I did some more performance analysis of the AES code with various compiler versions, by running the in-kernel crypto selftest (kvm -kernel linux/arch/x86/boot/bzImage -append "tcrypt.mode=200 tcrypt.sec=1 console=ttyS0"  -nographic -serial mon:stdio), which showed a very clear slowdown at gcc-7.2 (dated 20171130) compared to 7.1, all numbers are in cycles/byte for AES256+CBC on a 3.1GHz AMD Threadripper, lower numbers are better:

                default      ubsan         patched        patched+ubsan
gcc-4.3.6 -O2    14.9        ----           14.9         ----
gcc-4.6.4 -O2    15.0        ----           15.8         ----
gcc-4.9.4 -O2    15.5        20.7           15.9         20.9
gcc-5.5.0 -O2    15.6        47.3           86.4         48.8
gcc-6.3.1 -O2    14.6        49.4           94.3         50.9
gcc-7.1.1 -O2    13.5        54.6           15.2         52.0
gcc-7.2.1 -O2    16.8       124.7           92.0         52.2
gcc-8.0.0 -O2    14.6        56.6           15.3         53.5
gcc-7.1.1 -O1    14.6        53.8
gcc-7.2.1 -O1    15.5        55.9
gcc-8.0.0 -O1    15.0        50.7
clang-5 -O1      21.7        58.3
clang-5 -O2      15.5        49.1
handwritten asm  16.4

The 'patched' columns are with '-ftree-pre and -ftree-sra' disabled in the sources, which happened to help on gcc-7.2.1 for performance and to work around PR83356 but made things worse for most other cases.

For better reproducibility, I tried doing the same with the libressl implementation of the same cipher, which also has interesting but unfortunately very different results:

gcc-5.5.0 -O2    49.0
gcc-6.3.1 -O2    48.8
gcc-7.1.1 -O2    59.7
gcc-7.2.1 -O2    60.3
gcc-8.0.0 -O2    59.6

gcc-5.5.0 -O1    59.5
gcc-6.3.1 -O1    48.5
gcc-7.1.1 -O1    51.6
gcc-7.2.1 -O1    51.6
gcc-8.0.0 -O1    51.6

The source code is apparently derived from a common source, but has evolved in different ways, and the version from the kernel appears to be much faster overall. In both cases, we see a ~20% degradation between gcc-6.3.1 and gcc-7.2.1, but gcc-7.1.1 happens to produce the best results for the kernel version and very bad results for the libressl sources. The stack consumption problem from PR83356 does not appear with the libressl sources. I have not managed to run a ubsan-enabled libressl binary for testing.

To put this in context, both libressl and Linux come with architecture-specific versions using SIMD registers for most architectures, and those tend to be much faster, but the C version is used on old x86 CPUs and minor architectures that lack SIMD registers or an AES implementation for them.

If there is enough interest in addressing the slowdown, it should be possible to create a version of the kernel AES implementation that can be run in user space, as the current method of reproducing the results is fairly tedious.


---


### compiler : `gcc`
### title : `sincos does not handle sin(2x)`
### open_at : `2018-01-03T07:35:26Z`
### last_modified_date : `2021-11-28T00:26:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83661
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Hi,
For the following test-case:

double f(double x)
{
  return __builtin_sin(2*x) + __builtin_sin(x);
}

optimzied dump with -O2 -funsafe-math-optimizations -ffast-math shows:
;; Function f (f, funcdef_no=0, decl_uid=1952, cgraph_uid=0, symbol_order=0)

  <bb 2> [local count: 1073741825]:
  _1 = __builtin_sin (x_4(D));
  _2 = x_4(D) * 2.0e+0;
  _3 = __builtin_sin (_2);
  _5 = _1 + _3;
  return _5;

Would it be a good idea to enhance sincos pass to recognize the
identity sin(2*x) = 2*sin(x)*cos(x) and thus eliminate one call
to __builtin_sin ?

Writing 2*sin(x)*cos(x) explicitly in the source yields following optimized dump:

  <bb 2> [local count: 1073741825]:
  sincostmp_8 = __builtin_cexpi (x_5(D));
  _1 = IMAGPART_EXPR <sincostmp_8>;
  _2 = REALPART_EXPR <sincostmp_8>;
  _3 = _1 * _2;
  _4 = _3 * 2.0e+0;
  _6 = _2 + _4;
  return _6;

I agree in general that adding math identities like sin(x)**2 + cos(x)**2 = 1
isn't a good idea since user would almost always write the "optimized" version in practice. However for the above case, would the transform make sense ?

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `strcpy folding of small strings defeats strlen optimization`
### open_at : `2018-01-03T19:33:15Z`
### last_modified_date : `2019-10-15T21:35:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83674
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Here's another test case showing the detrimental effect of early folding into MEM_REFs that the strlen pass isn't equipped to handle.  The first dump that shows the MEM_REF is forwprop1.  The interesting aspect of this test case is that whether or not the optimization takes place depends on both the size of the destination and the lengths of the source strings.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
char d[8];

#define A1 "123456"
#define A2 "654321"

const char a1[] = A1;
const char a2[] = A2;

void f7 (int i)
{
  __builtin_strcpy (d, i < 0 ? A1 : A2);

  if (__builtin_strlen (d) != sizeof A1 - 1)   // optimized
    __builtin_abort ();
}

void g7 (int i)
{
  __builtin_strcpy (d, i < 0 ? a1 : a2);

  if (__builtin_strlen (d) != sizeof a1 - 1)   // optimized
    __builtin_abort ();
}


#define B1 "1234567"
#define B2 "7654321"

const char b1[] = B1;
const char b2[] = B2;

void f8 (int i)
{
  __builtin_strcpy (d, i < 0 ? B1 : B2);

  if (__builtin_strlen (d) != sizeof B1 - 1)   // not optimized
    __builtin_abort ();
}

void g8 (int i)
{
  __builtin_strcpy (d, i < 0 ? B1 : B2);

  if (__builtin_strlen (d) != sizeof b1 - 1)   // not optimized
    __builtin_abort ();
}


;; Function f7 (f7, funcdef_no=0, decl_uid=1955, cgraph_uid=0, symbol_order=3)

f7 (int i)
{
  char[7] * iftmp.0_7;

  <bb 2> [local count: 1073741825]:
  if (i_3(D) < 0)
    goto <bb 4>; [36.00%]
  else
    goto <bb 3>; [64.00%]

  <bb 3> [local count: 687194769]:

  <bb 4> [local count: 1073312329]:
  # iftmp.0_7 = PHI <"123456"(2), "654321"(3)>
  __builtin_memcpy (&d, iftmp.0_7, 7); [tail call]
  return;

}



;; Function g7 (g7, funcdef_no=1, decl_uid=1958, cgraph_uid=1, symbol_order=4)

g7 (int i)
{
  const char * iftmp.1_7;

  <bb 2> [local count: 1073741825]:
  if (i_3(D) < 0)
    goto <bb 4>; [36.00%]
  else
    goto <bb 3>; [64.00%]

  <bb 3> [local count: 687194769]:

  <bb 4> [local count: 1073312329]:
  # iftmp.1_7 = PHI <&a1(2), &a2(3)>
  __builtin_memcpy (&d, iftmp.1_7, 7); [tail call]
  return;

}



;; Function f8 (f8, funcdef_no=2, decl_uid=1963, cgraph_uid=2, symbol_order=7)

f8 (int i)
{
  long unsigned int _1;
  long unsigned int _4;

  <bb 2> [local count: 1073741825]:
  if (i_3(D) < 0)
    goto <bb 3>; [36.00%]
  else
    goto <bb 4>; [64.00%]

  <bb 3> [local count: 386547056]:

  <bb 4> [local count: 1073741825]:
  # _4 = PHI <13847469359445559(2), 15540725856023089(3)>
  MEM[(char * {ref-all})&d] = _4;
  _1 = __builtin_strlen (&d);
  if (_1 != 7)
    goto <bb 5>; [0.00%]
  else
    goto <bb 6>; [99.96%]

  <bb 5> [count: 0]:
  __builtin_abort ();

  <bb 6> [local count: 1073312327]:
  return;

}



;; Function g8 (g8, funcdef_no=5, decl_uid=1966, cgraph_uid=3, symbol_order=8)

g8 (int i)
{
  <bb 2> [local count: 1073741826]:
  f8 (i_2(D)); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `Missed optimization in math expression: optimize double comparing`
### open_at : `2018-01-06T15:01:12Z`
### last_modified_date : `2021-12-17T08:04:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83715
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
gcc trunk with '-O3 -ffast-math -std=c++17' for this code:


double test(double x, double y)
{
    if(x != y)
    {
        return 42.0;
    }
    return x/y;
}


generates this code:

test(double, double):
  comisd xmm0, xmm1
  jne .L3
  divsd xmm0, xmm1
  ret
.L3:
  movsd xmm0, QWORD PTR .LC0[rip]
  ret
.LC0:
  .long 0
  .long 1078263808


but we can optimize here divide operation and just return 1.0.


---


### compiler : `gcc`
### title : `Missed optimization with bitfield`
### open_at : `2018-01-11T05:49:22Z`
### last_modified_date : `2023-07-19T04:07:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83784
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Created attachment 43095
test case

The layout of bitfields in memory is, of course, undefined in the C standard and is implementation-dependent.  But when I happen to guess how gcc will lay it out correctly, I would like for these pack and unpack functions to compile-out.  I'm only doing this because I happen to need to be able to know what 32-bit portion of a 64-bit value has one of the fields (for futex operations) and bitfields are syntactically easier to work with.  But due to this flaw, I have to go back to shifting, ANDing, ORing, etc.

The attached test case is probably not as simple as it could be as I'm testing both 32 and 64-bit code on x86, but the below is probably a descent summary (for 64-bits):

union u
{
    unsigned long ulong_val;
    struct {
        unsigned long a:4;
        unsigned long b:60;
    };
};

union u pack(union u in)
{
    union u ret;
    ret.ulong_val  |= in.b;
    ret.ulong_val <<= 4;
    ret.ulong_val  |= in.a;
    return ret;
}

The above pack function compiles into the no-op I would expect:
pack:
.LFB12:
        .cfi_startproc
        movq    %rdi, %rax
        ret
        .cfi_endproc


But if I use three bitfields, my pack function is no longer a no-op:

union u
{
    unsigned long ulong_val;
    struct {
        unsigned long a:4;
        unsigned long b:30;
        unsigned long c:30;
    };
};

union u pack( union u in )
{
    union u ret;
    ret.ulong_val   = in.c;
    ret.ulong_val <<= 30;
    ret.ulong_val  |= in.b;
    ret.ulong_val <<= 4;
    ret.ulong_val  |= in.a;
    return ret;
}

And here's the output (with hex immediates for ANDs)
pack:
pack:
.LFB11:
        .cfi_startproc
        movq    %rdi, %rax
        movq    %rdi, %rdx
        andl    $0xf, %edi
        shrq    $34, %rax
        shrq    $4, %rdx
        salq    $30, %rax
        andl    $0x3fff, %edx
        orq     %rdx, %rax
        salq    $4, %rax
        orq     %rdi, %rax
        ret
        .cfi_endproc


Possibly related to bug #15596 and maybe even a duplicate of bug #35363, but I'm uncertain.  I have only tested on gcc 5.4.0 and 8 from git so far and only x86, but I'm going to *guess* this is a tree-optimization issue and not the x86 backend.


---


### compiler : `gcc`
### title : `[meta-bug] missing strlen optimizations`
### open_at : `2018-01-12T17:35:32Z`
### last_modified_date : `2022-01-05T00:52:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83819
### status : `NEW`
### tags : `meta-bug, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
This meta-bug groups optimizations missing strlen optimizations.


---


### compiler : `gcc`
### title : `local aggregate initialization defeats strlen optimization`
### open_at : `2018-01-12T17:55:19Z`
### last_modified_date : `2019-10-16T19:26:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83821
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
For the test case below, GCC optimizes the strlen call in f() but fails to do the same in g().  It appears because the maybe_invalidate() function in the pass considers the initialization of/assignment to b.i as possibly clobbering the value of b.s, not realizing that there is no way for the two members to alias.

$ cat z.c && gcc -O2 -S -fdump-tree-optimized=/dev/stdout z.c 
#define STR "0123456789"

struct A
{
  char s[sizeof STR];
};

void f (void)
{
  struct A a = { STR };
  if (__builtin_strlen (a.s) != sizeof STR - 1)   // folded
    __builtin_abort ();                           // eliminated
}

struct B
{
  char s[sizeof STR];
  int i;
};

void g (void)
{
  struct B b = { STR, 123 };
  if (__builtin_strlen (b.s) != sizeof STR - 1)   // not folded
    __builtin_abort ();                           // not eliminated
}


;; Function f (f, funcdef_no=0, decl_uid=1952, cgraph_uid=0, symbol_order=0)

f ()
{
  <bb 2> [local count: 1073741825]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1959, cgraph_uid=1, symbol_order=1)

g ()
{
  struct B b;
  long unsigned int _1;

  <bb 2> [local count: 1073741825]:
  b.s = "0123456789";
  b.i = 123;
  _1 = __builtin_strlen (&b.s);
  if (_1 != 10)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  b ={v} {CLOBBER};
  return;

}


---


### compiler : `gcc`
### title : `vector load/store with struct in registers`
### open_at : `2018-01-13T11:39:33Z`
### last_modified_date : `2021-08-16T08:15:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83827
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
(first seen with complex numbers, quite likely a dup of some other vector PR, could be target instead of rtl-opt)

typedef double vec __attribute__((vector_size(16)));
struct A { double a, b; };
vec f(A x){
  vec v = { x.a, x.b };
  return v;
}
A add(A x, A y){
  return { x.a+y.a, x.b+y.b };
}

In f, we build v with
  _1 = x.a;
  _2 = x.b;
  v_4 = {_1, _2};
while in add, we do
  vect__1.2_10 = MEM[(double *)&x];
  vect__2.5_11 = MEM[(double *)&y];
  vect__3.6_12 = vect__1.2_10 + vect__2.5_11;
  MEM[(double *)&D.2881] = vect__3.6_12;

The first version yields (g++ -O3 -march=skylake) the nice
	vunpcklpd	%xmm1, %xmm0, %xmm0
while add has the lengthy
	vmovq	%xmm0, -40(%rsp)
	vmovq	%xmm1, -32(%rsp)
	vmovapd	-40(%rsp), %xmm5
	vmovq	%xmm2, -24(%rsp)
	vmovq	%xmm3, -16(%rsp)
	vaddpd	-24(%rsp), %xmm5, %xmm4
	vmovaps	%xmm4, -40(%rsp)
	vmovsd	-32(%rsp), %xmm1
	vmovsd	-40(%rsp), %xmm0

With -O2, we also turn
A g(vec x){
  return { x[0], x[1] };
}
into the nice
	vunpckhpd	%xmm0, %xmm0, %xmm1


(this PR is independent of whether it was a good idea or not for SLP to vectorize add)


---


### compiler : `gcc`
### title : `missing strlen optimization for non-zero memset followed by a nul byte store`
### open_at : `2018-01-16T20:28:14Z`
### last_modified_date : `2022-05-15T22:06:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=83907
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The memset call in the function below together with the store of the terminating NUL byte create a string of known length that the strlen pass could determine but doesn't.  This is a potential (albeit minor) optimization opportunity that could be exploited.

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
extern char a[];

void f (void)
{
  __SIZE_TYPE__ n = 12;

  __builtin_memset (a, 'x', n);
  a[n] = '\0';

  if (__builtin_strlen (a) != n)   // can be folded to false
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1794, cgraph_uid=0, symbol_order=0)

f ()
{
  long unsigned int _1;

  <bb 2> [100.00%]:
  __builtin_memset (&a, 120, 12);
  a[12] = 0;
  _1 = __builtin_strlen (&a);
  if (_1 != 12)
    goto <bb 3>; [0.04%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [0.04%]:
  __builtin_abort ();

  <bb 4> [99.96%]:
  return;

}


---


### compiler : `gcc`
### title : `Optimize switch table with run-time relocation`
### open_at : `2018-01-23T23:06:55Z`
### last_modified_date : `2021-09-05T00:29:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84011
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 43225
Switch test case

With -O2 and -fPIE switch tables are never optimized as well as they could. I assume that's to reduce the number of relocations in code sections for COW page support. It makes sense for -fPIC but not -fPIE. The Linux kernel x86_64 PIE prototype ends with big switch tables when it could be optimized to few lines.

With -fno-PIE -O2:

0000000000000000 <phy_modes>:
   0:	b8 00 00 00 00       	mov    $0x0,%eax
			1: R_X86_64_32	.rodata.str1.1
   5:	83 ff 16             	cmp    $0x16,%edi
   8:	77 0a                	ja     14 <phy_modes+0x14>
   a:	89 ff                	mov    %edi,%edi
   c:	48 8b 04 fd 00 00 00 	mov    0x0(,%rdi,8),%rax
  13:	00 
			10: R_X86_64_32S	.rodata
  14:	c3                   	retq 

With -fPIE -O2:

0000000000000000 <phy_modes>:
   0:	83 ff 16             	cmp    $0x16,%edi
   3:	0f 87 87 01 00 00    	ja     190 <phy_modes+0x190>
   9:	48 8d 15 00 00 00 00 	lea    0x0(%rip),%rdx        # 10 <phy_modes+0x10>
			c: R_X86_64_PC32	.rodata-0x4
  10:	89 ff                	mov    %edi,%edi
  12:	48 63 04 ba          	movslq (%rdx,%rdi,4),%rax
  16:	48 01 d0             	add    %rdx,%rax
  19:	ff e0                	jmpq   *%rax
  1b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
  20:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 27 <phy_modes+0x27>
			23: R_X86_64_PC32	.LC1-0x4
  27:	c3                   	retq   
  28:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  2f:	00 
  30:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 37 <phy_modes+0x37>
			33: R_X86_64_PC32	.LC22-0x4
  37:	c3                   	retq   
  38:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  3f:	00 
  40:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 47 <phy_modes+0x47>
			43: R_X86_64_PC32	.LC21-0x4
  47:	c3                   	retq   
  48:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  4f:	00 
  50:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 57 <phy_modes+0x57>
			53: R_X86_64_PC32	.LC20-0x4
  57:	c3                   	retq   
  58:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  5f:	00 
  60:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 67 <phy_modes+0x67>
			63: R_X86_64_PC32	.LC19-0x4
  67:	c3                   	retq   
  68:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  6f:	00 
  70:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 77 <phy_modes+0x77>
			73: R_X86_64_PC32	.LC18-0x4
  77:	c3                   	retq   
  78:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  7f:	00 
  80:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 87 <phy_modes+0x87>
			83: R_X86_64_PC32	.LC17-0x4
  87:	c3                   	retq   
  88:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  8f:	00 
  90:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 97 <phy_modes+0x97>
			93: R_X86_64_PC32	.LC16-0x4
  97:	c3                   	retq   
  98:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  9f:	00 
  a0:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # a7 <phy_modes+0xa7>
			a3: R_X86_64_PC32	.LC15-0x4
  a7:	c3                   	retq   
  a8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  af:	00 
  b0:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # b7 <phy_modes+0xb7>
			b3: R_X86_64_PC32	.LC14-0x4
  b7:	c3                   	retq   
  b8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  bf:	00 
  c0:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # c7 <phy_modes+0xc7>
			c3: R_X86_64_PC32	.LC13-0x4
  c7:	c3                   	retq   
  c8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  cf:	00 
  d0:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # d7 <phy_modes+0xd7>
			d3: R_X86_64_PC32	.LC12-0x4
  d7:	c3                   	retq   
  d8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  df:	00 
  e0:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # e7 <phy_modes+0xe7>
			e3: R_X86_64_PC32	.LC11-0x4
  e7:	c3                   	retq   
  e8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  ef:	00 
  f0:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # f7 <phy_modes+0xf7>
			f3: R_X86_64_PC32	.LC10-0x4
  f7:	c3                   	retq   
  f8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
  ff:	00 
 100:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 107 <phy_modes+0x107>
			103: R_X86_64_PC32	.LC9-0x4
 107:	c3                   	retq   
 108:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 10f:	00 
 110:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 117 <phy_modes+0x117>
			113: R_X86_64_PC32	.LC8-0x4
 117:	c3                   	retq   
 118:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 11f:	00 
 120:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 127 <phy_modes+0x127>
			123: R_X86_64_PC32	.LC7-0x4
 127:	c3                   	retq   
 128:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 12f:	00 
 130:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 137 <phy_modes+0x137>
			133: R_X86_64_PC32	.LC6-0x4
 137:	c3                   	retq   
 138:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 13f:	00 
 140:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 147 <phy_modes+0x147>
			143: R_X86_64_PC32	.LC5-0x4
 147:	c3                   	retq   
 148:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 14f:	00 
 150:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 157 <phy_modes+0x157>
			153: R_X86_64_PC32	.LC4-0x4
 157:	c3                   	retq   
 158:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 15f:	00 
 160:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 167 <phy_modes+0x167>
			163: R_X86_64_PC32	.LC3-0x4
 167:	c3                   	retq   
 168:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 16f:	00 
 170:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 177 <phy_modes+0x177>
			173: R_X86_64_PC32	.LC2-0x4
 177:	c3                   	retq   
 178:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 17f:	00 
 180:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 187 <phy_modes+0x187>
			183: R_X86_64_PC32	.LC23-0x4
 187:	c3                   	retq   
 188:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
 18f:	00 
 190:	48 8d 05 00 00 00 00 	lea    0x0(%rip),%rax        # 197 <phy_modes+0x197>
			193: R_X86_64_PC32	.LC0-0x4
 197:	c3                   	retq

Similar options for clang:

0000000000000000 <phy_modes>:
   0:   83 ff 16                cmp    $0x16,%edi
   3:   77 0f                   ja     14 <phy_modes+0x14>
   5:   48 63 c7                movslq %edi,%rax
   8:   48 8d 0d 00 00 00 00    lea    0x0(%rip),%rcx        # f <phy_modes+0xf>
                        b: R_X86_64_PC32        .data.rel.ro-0x4
   f:   48 8b 04 c1             mov    (%rcx,%rax,8),%rax
  13:   c3                      retq   
  14:   48 8d 05 00 00 00 00    lea    0x0(%rip),%rax        # 1b <phy_modes+0x1b>
                        17: R_X86_64_PC32       .L.str.23-0x4
  1b:   c3                      retq


---


### compiler : `gcc`
### title : `[8/9 Regression] Spec2000 regression around Jan 14 and Jan 19 2018`
### open_at : `2018-01-24T09:25:55Z`
### last_modified_date : `2019-04-11T08:32:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84016
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
There is noticeable regression in specfp2000 in mgrid (4700-4600), facerec (8000->6600), crafty 4520->4420 at Jan 19
Galgel 14450->14150, apsi 4700->4550, gamess (from 2016) 34->32 at Jan 14-16
On Haswell.

All visible at https://gcc.opensuse.org/gcc-old/SPEC/CFP/sb-czerny-head-64/recent.html and https://gcc.opensuse.org/gcc-old/SPEC/CFP/sb-czerny-head-64-2006/recent.html


---


### compiler : `gcc`
### title : `IVOPTS doesn't optimize int indexes on some PowerPC code starting with svn id r250482`
### open_at : `2018-01-25T18:01:29Z`
### last_modified_date : `2020-01-02T06:16:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84042
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
This is a follow-on to PR target/81550.

In that bug, the following code generated more code.  That in turn caused the test to fail that was looking for a loop to be 8 instructions or less to test the TARGET_ASM_LOOP_ALIGN_MAX_SKIP target hook:

void f(double *a, double *b, double *c, int n) {
  int i;
  for (i=0; i < n; i++)
    a[i] = b[i] + c[i];
}

I rewrote the test to use unsigned long instead of int so that the # of instructions is under 8, and it validates the target hook.

But we really should fix the regression.

As per PR target/81550, the original code on powerpc little endian was:

        .file   "loop_align.c"
        .abiversion 2
        .section        ".text"
        .align 2
        .p2align 4,,15
        .globl f
        .type   f, @function
f:
        cmpwi 7,6,0
        blelr 7
        addi 6,6,-1
        li 9,0
        rldicl 6,6,0,32
        addi 10,6,1
        mtctr 10
        .p2align 5,,31
.L3:
        lfdx 0,4,9
        lfdx 12,5,9
        fadd 0,0,12
        stfdx 0,3,9
        addi 9,9,8
        bdnz .L3
        blr
        .file   "loop_align.c"
        .abiversion 2
        .section        ".text"
        .align 2
        .p2align 4,,15
        .globl f
        .type   f, @function
f:
        cmpwi 7,6,0
        blelr 7
        addi 6,6,-1
        addi 9,4,-8
        rldic 6,6,3,29
        addi 5,5,-8
        add 4,4,6
        addi 3,3,-8
        .p2align 4,,15
.L3:
        lfdu 0,8(9)
        lfdu 12,8(5)
        cmpld 7,9,4
        fadd 0,0,12
        stfdu 0,8(3)
        beqlr 7
        lfdu 0,8(9)
        lfdu 12,8(5)
        cmpld 7,9,4
        fadd 0,0,12
        stfdu 0,8(3)
        bne 7,.L3
        blr

The first observation is the count down loop is no longer done (i.e. no BDNZ).  Because it no longer is a count down loop, reorder blocks decided to clone the block and have an early exit.  This all makes the loop to be 12 instructions, which causes the TARGET_ASM_LOOP_ALIGN_MAX_SKIP target hook to fail.


---


### compiler : `gcc`
### title : `RTl partitioning fixup should drag very small blocks back to hot partition`
### open_at : `2018-01-26T09:15:52Z`
### last_modified_date : `2022-05-27T08:14:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84058
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
In LTO/FDO bootstrapped compiler we have cold functions like this:
000000000065b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>:
  65b2e8:       31 c0                   xor    %eax,%eax
  65b2ea:       c3                      retq   
  65b2eb:       eb fb                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>
  65b2ed:       eb f9                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>
  65b2ef:       eb f7                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>
  65b2f1:       eb f5                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>
  65b2f3:       eb f3                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>
  65b2f5:       eb f1                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>
  65b2f7:       eb ef                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>
  65b2f9:       eb ed                   jmp    65b2e8 <_Z27is_gimple_invariant_addressPK9tree_node.cold.318>

Obviously there is no good to offload empty BB into cold partition. We also have couple cases consisting only of ret or one reg-reg move and jump/ret

I suppose this is because it was not empty at the partitioning time but then we did not bring it back.  Doing so in bb-reorder is bit late, we should do at least one sanitization pre regalloc, so I suppose partitioning fixup is good place.


---


### compiler : `gcc`
### title : `[9 Regression] gcc.dg/tree-ssa/loop-15.c XFAIL`
### open_at : `2018-01-26T10:36:43Z`
### last_modified_date : `2022-05-27T08:14:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84061
### status : `RESOLVED`
### tags : `missed-optimization, testsuite-fail, xfail`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
After r257077 we fail to fold (int) ((unsigned int) n_5 + 4294967295) * n_5 + n_5
to n_5 * n_5.  Before the rev. we managed to fold to (int)((unsigned)n_5 * (unsigned)n_5) via fold_plusminus_mult_expr.

The rev. contains everything necessary to fold this during late VRP to n_5 * n_5
but the substitute-and-fold engine doesn't fold all stmts.

The following fixes this regression:

Index: gcc/tree-ssa-propagate.c
===================================================================
--- gcc/tree-ssa-propagate.c    (revision 257048)
+++ gcc/tree-ssa-propagate.c    (working copy)
@@ -1056,13 +1056,17 @@ substitute_and_fold_dom_walker::before_d
                           && gimple_call_noreturn_p (stmt));
 
       /* Replace real uses in the statement.  */
-      did_replace |= substitute_and_fold_engine->replace_uses_in (stmt);
+      if (substitute_and_fold_engine->replace_uses_in (stmt))
+       {
+         did_replace = true;
+         gimple_set_modified (stmt, true);
+       }
 
-      /* If we made a replacement, fold the statement.  */
-      if (did_replace)
+      /* Fold the statement.  */
+      if (fold_stmt (&i, follow_single_use_edges))
        {
-         fold_stmt (&i, follow_single_use_edges);
          stmt = gsi_stmt (i);
+         did_replace = true;
          gimple_set_modified (stmt, true);
        }
 

queued for GCC 9.


---


### compiler : `gcc`
### title : `false positive for -Wmaybe-uninitialized with __asm__`
### open_at : `2018-01-27T22:03:32Z`
### last_modified_date : `2022-11-28T22:35:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84078
### status : `RESOLVED`
### tags : `diagnostic, inline-asm, missed-optimization`
### component : `middle-end`
### version : `8.0.1`
### severity : `normal`
### contents :
Created attachment 43265
testcase

Hello,

The attached testcase compiled with -O2 -Wall produces

test.c:30:2: warning: 'c' may be used uninitialized in this function [-Wmaybe-uninitialized]
  printf("%d %d %d\n", a, b, c);
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
test.c:30:2: warning: 'b' may be used uninitialized in this function [-Wmaybe-uninitialized]
test.c:30:2: warning: 'a' may be used uninitialized in this function [-Wmaybe-uninitialized]

while they are only used when `err' is equal to zero, and in the only case where that happens, they do get initialized.

This is the minimal testcase I could reduce to, it seems both the asm snippet and testing its result does make a difference, as well as the two ifs and the three variables.

I could verify this with gcc snapshot 20180124 (Debian buster x86_64)

Samuel


---


### compiler : `gcc`
### title : `[missed optimization] loop-invariant strlen() not hoisted out of loop`
### open_at : `2018-01-28T11:44:35Z`
### last_modified_date : `2022-03-20T11:21:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84083
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
Consider the following code:

#include <string.h>

void bar(char c);

void foo(const char* __restrict__ ss) 
{
    for (int i = 0; i < strlen(ss); ++i) 
    {
        bar(*ss);
    }
}

To my understanding, the fact that ss is __restrict__ed (and the fact that it isn't written through or that it's a const pointer) is sufficient to allow the compiler to assume the memory accessible via ss remains constant, and thus that strlen(ss) will return the same value.

But - that's not what happens (with GCC 7.3):

.L6:
        movsx   edi, BYTE PTR [rbp+0]
        mov     rbx, r12
        call    bar(char)
.L3:
        mov     rdi, rbp
        lea     r12, [rbx+1]
        call    strlen
        cmp     rax, rbx
        ja      .L6

(obtained with https://godbolt.org/g/vdGSBe )

Now, I'm no compiler expert, so maybe there are considerations I'm ignoring, but it seems to me the compiler should be able to hoist the heavier code up above the loop.

Cf. https://stackoverflow.com/q/48482003/1593077


---


### compiler : `gcc`
### title : `[8 Regression] -O3 and -ftree-vectorize trying too hard for function returning trivial pair-of-uint64_t-structure`
### open_at : `2018-01-29T13:06:40Z`
### last_modified_date : `2021-05-14T10:30:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84101
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.1.0`
### severity : `normal`
### contents :
The following:

  typedef struct uint64_pair uint64_pair_t ;
  struct uint64_pair
  {
    uint64_t  w0 ;
    uint64_t  w1 ;
  } ;

  uint64_pair_t pair(int num)
  {
    uint64_pair_t p ;

    p.w0 = num << 1 ;
    p.w1 = num >> 1 ;

    return p ;
  }

for recent x86_64, under v7.1.0, using "-O3", compiles to:

  pair:
   lea    (%rdi,%rdi,1),%eax
   sar    %edi
   movslq %edi,%rdi
   cltq   
   mov    %rax,-0x18(%rsp)
   movq   -0x18(%rsp),%xmm0
   mov    %rdi,-0x18(%rsp)
   movhps -0x18(%rsp),%xmm0
   movaps %xmm0,-0x18(%rsp)
   mov    -0x18(%rsp),%rax
   mov    -0x10(%rsp),%rdx
   retq

using "-O3 -fno-tree-vectorize", compiles to:

  pair:
   lea    (%rdi,%rdi,1),%eax
   sar    %edi
   movslq %edi,%rdx
   cltq   
   retq

I note that v6.3 produces the shorter code without the "-fno-tree-vectorize".


---


### compiler : `gcc`
### title : `global reassociation pass prevents fma usage, generates slower code`
### open_at : `2018-01-29T18:41:26Z`
### last_modified_date : `2019-09-03T17:33:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84114
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Created attachment 43279
Test case

The example code comes from milc in SPEC2006.

GCC on x86 or aarch64 generates better code with -O3 than it does with -Ofast or '-O3 -ffast-math'.  On x86 compiling with '-mfma -O3' I get 5 vfmadd231sd instructions, 1 vmulsd instruction and 6 vmovsd.  With '-mfma -Ofast' I get 3 vfmadd231sd, 2 vaddsd, 3 vmulsd, and 6 vmovsd.  That is two extra instructions.

The problem seems to be that -Ofast turns on -ffast-math and that enables
the global reassociation pass (tree-ssa-reassoc.c) and the code changes
done there create some temporary variables which inhibit the recognition
and use of fma instructions.

Using -O3 and -Ofast on aarch64 shows the same change.


---


### compiler : `gcc`
### title : `[6 Regression] g++ generates two identical loads in a volatile-qualified member function.`
### open_at : `2018-01-31T16:01:45Z`
### last_modified_date : `2021-12-02T04:43:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84151
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `7.2.1`
### severity : `normal`
### contents :
The g++ generates two identical loads when the "foo" member function is called in the following test case:

struct A {
    static int& bar(int& a) {
        return a;
    }

    int foo() volatile {
        int v = c;
        return bar(v);
    }

    int c;
};

A a;

int main() {
    a.c = 2;
    a.foo();

    return 0;
}


The -O2 options is used to get the following assembler:
	.file	"test_atomic.cpp"
	.section	.text.startup,"ax",@progbits
	.p2align 4,,15
	.globl	main
	.type	main, @function
main:
.LFB2:
	.cfi_startproc
	movl	$2, a(%rip)
	movl	a(%rip), %eax    
	movl	a(%rip), %eax  // !!!DUPLICATED
	xorl	%eax, %eax
	ret
	.cfi_endproc
.LFE2:
	.size	main, .-main
	.globl	a
	.bss
	.align 4
	.type	a, @object
	.size	a, 4
a:
	.zero	4
	.ident	"GCC: (GNU) 7.2.1 20170915 (Red Hat 7.2.1-2)"
	.section	.note.GNU-stack,"",@progbits

The "movl a(%rip), %eax" line is repeated twice. The https://godbolt.org/g/yhQtDw reproduces the issue on many versions newer 4.8.


---


### compiler : `gcc`
### title : `[8/9 Regression] r256888 causes 30% performance regression of 519.lbm_r at -Ofast generic tuning on Zen/9`
### open_at : `2018-02-04T20:31:19Z`
### last_modified_date : `2019-04-11T08:51:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84200
### status : `RESOLVED`
### tags : `deferred, missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Revision 256888 caused 30% run-time regression of 519.lbm_r when
compiled with -Ofast (with generic march and tuning) and when run on a
Zen CPU.  I have observed it on both a Ryzen and an EPYC.
Surprisingly, the revision does not seem to have any effect on this
particular benchmark on my Intel Sandy Bridge desktop.

The changelog for the revision is below, I believe it is the first
listed change that is the culprit.

  2018-01-19  Martin Liska  <mliska@suse.cz>
    
            * predict.def (PRED_LOOP_EXIT): Change from 85 to 89.
            (PRED_LOOP_EXIT_WITH_RECURSION): Change from 72 to 78.
            (PRED_LOOP_EXTRA_EXIT): Change from 83 to 67.
            (PRED_OPCODE_POSITIVE): Change from 64 to 59.
            (PRED_TREE_OPCODE_POSITIVE): Change from 64 to 59.
            (PRED_CONST_RETURN): Change from 69 to 65.
            (PRED_NULL_RETURN): Change from 91 to 71.
            (PRED_LOOP_IV_COMPARE_GUESS): Change from 98 to 64.
            (PRED_LOOP_GUARD): Change from 66 to 73.


---


### compiler : `gcc`
### title : `recip and slp passes conflict`
### open_at : `2018-02-05T14:19:32Z`
### last_modified_date : `2021-12-24T21:25:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84214
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
In https://gcc.gnu.org/ml/gcc-patches/2018-01/msg02443.html I reported a regression on aarch64 in gcc.dg/cse_recip.c.

In https://gcc.gnu.org/ml/gcc-patches/2018-01/msg02446.html, richi explained that a possible fix would be to make the recip pass run before vectorization.

It would fix the testcase by preventing vectorization, but since we are in stage4, it's a bit late to make such a change.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Performance regression in gcc 8/9/10/11/12 when comparing floating point numbers`
### open_at : `2018-02-06T19:59:34Z`
### last_modified_date : `2023-08-04T00:50:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84251
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
See this simple function and compile it with -O2 on x86-64:
#include <stdlib.h>
#include <math.h>

int cmp(double a, double b)
{
        if (isnan(a) || isnan(b))
                abort();
        return a == b;
}

On gcc-7, we get optimal code:
0000000000000000 <cmp>:
   0:   66 0f 2e c1             ucomisd %xmm1,%xmm0
   4:   7a 07                   jp     d <cmp+0xd>
   6:   0f 94 c0                sete   %al
   9:   0f b6 c0                movzbl %al,%eax
   c:   c3                      retq   
   d:   48 83 ec 08             sub    $0x8,%rsp
  11:   e8 00 00 00 00          callq  16 <cmp+0x16>

On gcc-8, we get inoptimal code. See the nonsensical "setnp" instruction in a block of code where it is known that the "P" flag must be clear.
0000000000000000 <cmp>:
   0:   48 83 ec 08             sub    $0x8,%rsp
   4:   66 0f 2e c1             ucomisd %xmm1,%xmm0
   8:   0f 8a 00 00 00 00       jp     e <cmp+0xe>
   e:   0f 9b c0                setnp  %al
  11:   ba 00 00 00 00          mov    $0x0,%edx
  16:   0f b6 c0                movzbl %al,%eax
  19:   0f 45 c2                cmovne %edx,%eax
  1c:   48 83 c4 08             add    $0x8,%rsp
  20:   c3                      retq   
Disassembly of section .text.unlikely:
0000000000000000 <cmp.cold.0>:
   0:   e8 00 00 00 00          callq  5 <cmp.cold.0+0x5>


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Performance regression in g++-7 with Eigen for non-AVX2 CPUs`
### open_at : `2018-02-08T10:40:37Z`
### last_modified_date : `2023-07-07T10:33:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84280
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.2.1`
### severity : `normal`
### contents :
Hello,

I noticed today what may look like quite a large performance regression
with Eigen (3.3.4) matrix multiplication. It only seems to occur on
non-AVX2 code paths, meaning that if I compile with -march=native on my
core-i7 with AVX2, then it's blazingly fast on both g++ versions, but not
on an older core-i5 with only AVX, or if I use -march=core2.

Here are some example timings, but it applies to all matrix sizes that the
benchmark script tests (see end of the message for the code):

g++-5 gemm_test.cpp -std=c++17 -I 3rdparty/eigen/ -march=core2 -O3 -o
gcc5_gemm_test

1124 1215 1465
elapsed_ms: 1970
--------
1730 1235 1758
elapsed_ms: 3505

g++-7 gemm_test.cpp -std=c++17 -I 3rdparty/eigen/ -march=core2 -O3
-march=core2 -o gcc7_gemm_test

1124 1215 1465
elapsed_ms: 2998
--------
1730 1235 1758
elapsed_ms: 4628

It's even worse if I test this on a i5-3550, which has AVX, but not AVX2:

g++-5 gemm_test.cpp -std=c++17 -I 3rdparty/eigen/ -march=native -O3 -o
gcc5_gemm_test
1124 1215 1465
elapsed_ms: 941
--------
1730 1235 1758
elapsed_ms: 1780


g++-7 gemm_test.cpp -std=c++17 -I 3rdparty/eigen/ -march=native -O3 -o
gcc7_gemm_test

1124 1215 1465
elapsed_ms: 1988
--------
1730 1235 1758
elapsed_ms: 3740

I tried the same with -O2 and it gave the same results. That's a drop to
nearly half the speed in matrix multiplication on AVX CPUs. Or maybe I've
done something wrong. :-) I realise the benchmark might be a bit crude
(better use Google Benchmark or something like that...) But the results I'm
getting are pretty consistent on various CPUs, compilers, and with various
flags.


=== Benchmark code:
// gemm_test.cpp
#include <array>
#include <chrono>
#include <iostream>
#include <random>
#include <Eigen/Dense>

using RowMajorMatrixXf = Eigen::Matrix<float, Eigen::Dynamic,
Eigen::Dynamic, Eigen::RowMajor>;
using ColMajorMatrixXf = Eigen::Matrix<float, Eigen::Dynamic,
Eigen::Dynamic, Eigen::ColMajor>;

template <typename Mat>
void run_test(const std::string& name, int s1, int s2, int s3)
{
    using namespace std::chrono;
    float checksum = 0.0f; // to prevent compiler from optimizing
everything away
    const auto start_time_ns =
high_resolution_clock::now().time_since_epoch().count();
    for (size_t i = 0; i < 10; ++i)
    {
        Mat a_rm(s1, s2);
        Mat b_rm(s2, s3);
        const auto c_rm = a_rm * b_rm;
        checksum += c_rm(0, 0);
    }
    const auto end_time_ns =
high_resolution_clock::now().time_since_epoch().count();
    const auto elapsed_ms = (end_time_ns - start_time_ns) / 1000000;
    std::cout << name << " (checksum: " << checksum << ") elapsed_ms: " <<
elapsed_ms << std::endl;
}
int main()
{
    //std::random_device rd;
    //std::mt19937 gen(0);
    //std::uniform_int_distribution<> dis(1, 2048);
    std::vector<int> vals = { 1124, 1215, 1465, 1730, 1235, 1758, 1116,
1736, 868, 1278, 1323, 788 };
    for (std::size_t i = 0; i < 12; ++i)
    {
        int s1 = vals[i++];//dis(gen);
        int s2 = vals[i++];//dis(gen);
        int s3 = vals[i];//dis(gen);
        std::cout << s1 << " " << s2 << " " << s3 << std::endl;
        run_test<ColMajorMatrixXf>("col major", s1, s2, s3);
        run_test<RowMajorMatrixXf>("row major", s1, s2, s3);
        std::cout << "--------" << std::endl;
    }
    return 0;
}
===


---


### compiler : `gcc`
### title : `call_once uses TLS even when once_flag is set`
### open_at : `2018-02-11T15:41:28Z`
### last_modified_date : `2020-11-03T18:46:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84323
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `7.3.1`
### severity : `normal`
### contents :
Disassembly of the following code:

#include <mutex>

std::once_flag once;

int* foo() {
    static int* p{};
    std::call_once(once,[](){
        p = 0;
    });

    return p;
}

shows that a lot of work is going on the hot path. TLS is used (twice) and there is a function call:

  mov QWORD PTR [rsp+8], rax
  mov rax, QWORD PTR std::__once_callable@gottpoff[rip]
  mov QWORD PTR fs:[rax], rdx
  mov rax, QWORD PTR std::__once_call@gottpoff[rip]
  mov QWORD PTR fs:[rax], OFFSET FLAT:void std::call_once<foo()::{lambda()#1}>(std::once_flag&, foo()::{lambda()#1}&&)::{lambda()#2}::_FUN()
  mov eax, OFFSET FLAT:__gthrw___pthread_key_create(unsigned int*, void (*)(void*))
  test rax, rax
  je .L6
  mov esi, OFFSET FLAT:__once_proxy
  mov edi, OFFSET FLAT:once
  call __gthrw_pthread_once(int*, void (*)())

This seems to be suboptimal, as double-checked-like locking could be used without TLS + 'call' usage on a hot path. std::call_once could be implemented just like thread safe static local variables resulting in a much better disassembly on a hot path:

  movzx eax, BYTE PTR once
  test al, al
  je .L9    ; not called


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] -finline-small-functions and inline keyword lead to slowdown since version 6`
### open_at : `2018-02-12T12:40:25Z`
### last_modified_date : `2023-07-07T10:33:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84328
### status : `WAITING`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `normal`
### contents :
Created attachment 43393
optimizeFlags.cpp

I have a function looking like this:

unsigned int interleaveTwoZeros( unsigned int n )
{
    n&= 0x000003ff;
    n = (n ^ (n << 16)) & 0xFF0000FF;
    n = (n ^ (n <<  8)) & 0x0300F00F;
    n = (n ^ (n <<  4)) & 0x030C30C3;
    n = (n ^ (n <<  2)) & 0x09249249;
    return n;
}

On g++ < 6 it takes 5.7s for all optimization levels >= O1.
Since g++6 it takes 6.2s when the -finline-small-functions option is specified or if the `inline` keyword is added in front of `interleaveTwoZeros`.
Very interestingly, this bug also disappears when changing the function body!? 
What I mean is, this function:

unsigned int interleaveZeros( unsigned int n )
{
	n &= 0x0000ffff;
	n = (n | (n << 8)) & 0x00FF00FF;
	n = (n | (n << 4)) & 0x0F0F0F0F;
	n = (n | (n << 2)) & 0x33333333;
	n = (n | (n << 1)) & 0x55555555;
	return n;
}

is exactly as fast as interleaveTwoZeros, but it isn't being slowed down by the inlining bug which appears since version 6, which seems to mean, that the the change of the constants doesn't lead to any change to the internal logic, but somehow still influences the change done by inlining.

Here are the full benchmarks on my system as done with:

for function in '' '-DTWO_ZEROS_VERSION'  '-DTWO_ZEROS_VERSION -DMANUAL_INLINE'; do
    for GPP in g++-4.9 g++-5 g++-6 g++-7 g++-8; do
        $GPP --version | head -1;
        for flag in -O1 '-O1 -finline-small-functions'; do
            echo -n "$flag "
            $GPP $flag $function -std=c++11 optimizeFlags.cpp &&
            ./a.out
        done
    done
done

interleaveZeros:
  4.9.4 -O1                          5.67675s
  4.9.4 -O1 -finline-small-functions 5.65597s
  5.5.0 -O1                          5.63532s
  5.5.0 -O1 -finline-small-functions 5.66475s
  6.4.0 -O1                          5.64871s
  6.4.0 -O1 -finline-small-functions 5.74504s
  7.3.0 -O1                          5.70723s
  7.3.0 -O1 -finline-small-functions 5.7509s
  8.0.1 -O1                          5.73126s
  8.0.1 -O1 -finline-small-functions 5.65887s
interleaveTwoZeros:
  4.9.4 -O1                          5.68634s
  4.9.4 -O1 -finline-small-functions 5.67831s
  5.5.0 -O1                          5.70178s
  5.5.0 -O1 -finline-small-functions 5.67027s
  6.4.0 -O1                          5.77438s
  6.4.0 -O1 -finline-small-functions 6.16534s -> 10% slower!
  7.3.0 -O1                          5.74391s
  7.3.0 -O1 -finline-small-functions 6.15133s -> 10% slower!
  8.0.1 -O1                          5.76954s
  8.0.1 -O1 -finline-small-functions 6.13896s -> 10% slower!
inline interleaveTwoZeros:
  4.9.4 -O1                          5.6749s
  4.9.4 -O1 -finline-small-functions 5.64078s
  5.5.0 -O1                          5.73546s
  5.5.0 -O1 -finline-small-functions 5.7754s
  6.4.0 -O1                          6.1316s  -> 10% slower!
  6.4.0 -O1 -finline-small-functions 6.13555s -> 10% slower!
  7.3.0 -O1                          6.12899s -> 10% slower!
  7.3.0 -O1 -finline-small-functions 6.15963s -> 10% slower!
  8.0.1 -O1                          6.17762s -> 10% slower!
  8.0.1 -O1 -finline-small-functions 6.15857s -> 10% slower!


---


### compiler : `gcc`
### title : `Fails to use vfmaddsub* for complex multiplication`
### open_at : `2018-02-13T12:29:51Z`
### last_modified_date : `2023-07-21T12:31:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84361
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
I see

        vfmadd132ps     %ymm12, %ymm8, %ymm2
        vfmsub132ps     %ymm12, %ymm8, %ymm7
        vblendps        $170, %ymm2, %ymm7, %ymm7

generated from

  _298 = -vect__174.663_871;
  vect__38.664_872 = vect__173.659_831 * vect__178.660_844 + _298;
  vect__38.665_873 = vect__173.659_831 * vect__178.660_844 + vect__174.663_871;
  _874 = VEC_PERM_EXPR <vect__38.664_872, vect__38.665_873, { 0, 9, 2, 11, 4, 13, 6, 15 }>;

which is similar to the addsub cases we already handle.  combine sees

(insn 391 390 392 21 (set (reg:V8SF 845 [ vect__38.664 ])
        (fma:V8SF (reg:V8SF 440 [ vect__173.659 ])
            (reg:V8SF 445 [ vect__178.660 ])
            (neg:V8SF (reg:V8SF 457 [ vect__174.663 ])))) 1886 {*fma_fmsub_v8sf}
     (nil))
(insn 392 391 393 21 (set (reg:V8SF 846 [ vect__38.665 ])
        (fma:V8SF (reg:V8SF 440 [ vect__173.659 ])
            (reg:V8SF 445 [ vect__178.660 ])
            (reg:V8SF 457 [ vect__174.663 ]))) 1842 {*fma_fmadd_v8sf}
     (expr_list:REG_DEAD (reg:V8SF 457 [ vect__174.663 ])
        (expr_list:REG_DEAD (reg:V8SF 445 [ vect__178.660 ])
            (expr_list:REG_DEAD (reg:V8SF 440 [ vect__173.659 ])
                (nil)))))
(insn 393 392 394 21 (set (reg:V8SF 460 [ _874 ])
        (vec_merge:V8SF (reg:V8SF 846 [ vect__38.665 ])
            (reg:V8SF 845 [ vect__38.664 ])
            (const_int 170 [0xaa]))) 3885 {avx_blendps256}
     (expr_list:REG_DEAD (reg:V8SF 846 [ vect__38.665 ])
        (expr_list:REG_DEAD (reg:V8SF 845 [ vect__38.664 ])
            (nil))))

I can find <avx512>_fmaddsub_<mode>_mask<round_name> which looks like
a patter for AVX512 but I miss the AVX256 case?  The non-fma
patterns look like

(define_insn "avx_addsubv8sf3"
  [(set (match_operand:V8SF 0 "register_operand" "=x")
        (vec_merge:V8SF
          (minus:V8SF
            (match_operand:V8SF 1 "register_operand" "x")
            (match_operand:V8SF 2 "nonimmediate_operand" "xm"))
          (plus:V8SF (match_dup 1) (match_dup 2))
          (const_int 85)))]
  "TARGET_AVX"
  "vaddsubps\t{%2, %1, %0|%0, %1, %2}"


This occurs in polyhedron capacita in the hot loop in fourir.  If you
build with -Ofast -march=core-avx2 -fno-vect-cost-model you should see the above.


---


### compiler : `gcc`
### title : `[8 Regression] Auto-vectorization regression when accessing member variable through getter/accessor`
### open_at : `2018-02-13T13:44:24Z`
### last_modified_date : `2021-05-14T10:30:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84362
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Consider the following code:

    constexpr unsigned int capacity = 1000;

    struct vec
    {
        int values[capacity];
        unsigned int _size = 0;

    #ifdef ALWAYS_INLINE
        __attribute__((always_inline))
    #endif
        unsigned int size() const noexcept { return _size; }

        void push(int x)
        {
            values[size()] = x;
            ++_size;
        }
    };

    int main()
    {
        vec v;
        for(unsigned int i{0}; i != capacity; ++i)
        {
            v.push(i);
        }

        asm volatile("" : : "g"(&v) : "memory");
    }

When compiling with g++7.x or g++ trunk (-Ofast -std=c++2a -fno-exceptions -fno-rtti), the compiler fails to produce optimized auto-vectorized assembly, unless any of the following changes are made:

 * `values[size()]` -> `values[_size]`

 * Add `__attribute__((always_inline))` to `size()`

godbolt.org comparison between g++7 (with and without `always_inline`) and g++6:
https://godbolt.org/g/a2udWW

Related SO question:
https://stackoverflow.com/questions/48767743

---

It seems that the introduction of a trivial `.size()` member variable defeats the auto-vectorization.


---


### compiler : `gcc`
### title : `Missed optimization: static guard variable generated for empty virtual destructor`
### open_at : `2018-02-15T19:01:20Z`
### last_modified_date : `2023-03-24T00:56:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84411
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.0.1`
### severity : `enhancement`
### contents :
Consider the following program:

struct Base {
    constexpr Base() = default;
    virtual int foo();
    // virtual ~Base() = default;
};
struct Derived : public Base {
    constexpr Derived() = default;
    virtual int foo() override;
};
Base& get_base() {
    static Derived d;
    return d;
}

This is a simplified version of the current idioms around `std::error_category` and `std::pmr::memory_resource`, except that for now I have commented-out the virtual destructor.
Notice that because `Derived d` is trivially destructible, no atomic guard variable is generated to register its destructor with __cxa_atexit.

Now uncomment the defaulted virtual destructor of `Base`. The semantics of `Derived d` have not changed: we are still expecting to call ~Derived by non-virtual dispatch, and ~Derived is still known statically to be a no-op. But now suddenly GCC decides to generate an atomic guard variable and a call to `__cxa_atexit`!

I think GCC could do better here, and if it did, it would eliminate a lot of atomic instructions in the critical path for things like `std::error_code{}` and `std::pmr::new_delete_resource()`.


---


### compiler : `gcc`
### title : `Suboptimal code for masked shifts (x86/x86-64)`
### open_at : `2018-02-17T21:57:37Z`
### last_modified_date : `2021-07-27T00:05:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84431
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `normal`
### contents :
In x86 and x86-64, the assumption is that upper bits of the CL register are unused (i.e., masked) when doing a shift operation. It is not possible to do shift for more than (WORD_BITS - 1) positions. Normally, the compiler has to check whether the specified shift value exceeds the word size before generating corresponding shld/shl commands (shrd/shr, etc).

Now, if the shift value is given by some variable, it is normally unknown at compile time whether it is exceeding (WORD_BITS - 1), so the compiler has to generate corresponding checks. On the other hand, it is very easy to give a hint to the compiler (if it is known that the shift < WORD_BITS) by masking shift value like this (the example below is for i386; for x86-64 the type will be __uint128_t and mask 63):

unsigned long long func(unsigned long long a, unsigned shift)
{
   return a << (shift & 31);
}

In the ideal scenario, the compiler has to just load value to CL without even masking it because it is implied already by the shift operation.

Note that clang/LLVM recognizes this pattern (at least for i386) by generating the following assembly code:
func:                                   # @func
    pushl   %esi
    movl    8(%esp), %esi
    movb    16(%esp), %cl
    movl    12(%esp), %edx
    movl    %esi, %eax
    shldl   %cl, %esi, %edx
    shll    %cl, %eax
    popl    %esi
    retl


GCC generates suboptimal code in this case:
func:
    pushl   %esi
    pushl   %ebx
    movl    20(%esp), %ecx
    movl    16(%esp), %esi
    movl    12(%esp), %ebx
    andl    $31, %ecx
    movl    %esi, %edx
    shldl   %ebx, %edx
    movl    %ebx, %eax
    xorl    %ebx, %ebx
    sall    %cl, %eax
    andl    $32, %ecx
    cmovne  %eax, %edx
    cmovne  %ebx, %eax
    popl    %ebx
    popl    %esi
    ret


---


### compiler : `gcc`
### title : `[8/9 Regression] Missed optimization with switch on enum constants returning the same value`
### open_at : `2018-02-18T12:37:54Z`
### last_modified_date : `2019-01-08T11:16:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84436
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `7.3.0`
### severity : `enhancement`
### contents :
The following snippet:

    enum class E
    {
        A, B, C,
    };

    int foo(E e)
    {
        switch (e)
        {
            case E::A: return 0;
            case E::B: return 1;
            case E::C: return 2;
        }
    }

Produces the following ineffective code:

    foo(E):
      cmp edi, 1
      mov eax, 1
      je .L1
      cmp edi, 2
      mov eax, 2
      je .L1
      xor eax, eax
    .L1:
      rep ret

There is no reason why it should not produce a single `mov eax, edi`
and ret instruction.


---


### compiler : `gcc`
### title : `powerpc suboptimal code generation (shrink wrap unlikely path) for Linux spinlocks`
### open_at : `2018-02-18T19:44:25Z`
### last_modified_date : `2022-05-27T08:16:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84443
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0.1`
### severity : `normal`
### contents :
Created attachment 43452
testcase with comment at the top describing desired output

A small fast path code gets several non-volatile registers saved on stack, and a sub-optimal restore in the return path.

The example is derived from (but not identical to) the Linux kernel spinlock implementation.

Tested with gcc version 8.0.1 20180207 (experimental) [trunk revision 257435] (Debian 8-20180207-2).


---


### compiler : `gcc`
### title : `Choosing between Integer and NEON for 64-bit operations`
### open_at : `2018-02-19T15:49:11Z`
### last_modified_date : `2021-06-01T11:22:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84467
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `normal`
### contents :
This is a follow up report to bug 82989
The comment bug 82989, comment 12 details about the need for early decisions to be made about choosing to take either NEON code or ARM code. This means that at the expand phase, we should be able to make a clear choice and avoid mixing the two.


---


### compiler : `gcc`
### title : `test for address of member being null not eliminated`
### open_at : `2018-02-19T20:34:34Z`
### last_modified_date : `2023-06-25T21:03:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84470
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
The address of a member subobject can never be null.  If it were, the address if the enclosing object would either have to be null which would make the member access expression (i.e., p->member) undefined, or the address of the object would have such that adding the offset of the member to it would wrap around zero, which would make the addition undefined.

As a result, tests for the address of a member object being null can be eliminated.

The test case below shows that GCC doesn't take advantage of this invariant.  In constrast, Clang does take advantage of it for all but the first member.

$ cat a.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout a.c
struct A
{
  char a[4];
};

void f (struct A *p)
{
  if (p->a == 0)          // could only be true when p is null
    __builtin_abort ();   // can be eliminated
}

struct B
{
  char a[4];
};


void b (struct B *p)
{
  if (p->a == 0)          // can never be true
    __builtin_abort ();   // can be eliminated (as Clang does)
}


;; Function f (f, funcdef_no=0, decl_uid=1959, cgraph_uid=0, symbol_order=0)

f (struct A * p)
{
  char[4] * _1;

  <bb 2> [local count: 1073741825]:
  _1 = &p_2(D)->a;
  if (_1 == 0B)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  return;

}



;; Function b (b, funcdef_no=1, decl_uid=1964, cgraph_uid=1, symbol_order=1)

b (struct B * p)
{
  char[4] * _1;

  <bb 2> [local count: 1073741825]:
  _1 = &p_2(D)->a;
  if (_1 == 0B)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [99.96%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073312327]:
  return;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] 429.mcf with -O2 regresses by ~6% and ~4%, depending on tuning, on Zen compared to GCC 7.2`
### open_at : `2018-02-20T13:52:26Z`
### last_modified_date : `2023-07-07T10:33:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84481
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Our friends at AMD reported that, compared to gcc 7.2, 429.mcf is
about 6% slower when compiled with just -O2 with trunk and run on Zen
CPU, which I can confirm.  I have managed to bisect this to Honza's
r255395.

The benchmark also regresses compared to gcc 7.2 with -O2
-march=native -mtune=native, by over 4% but I was not able to pin this
down to a single commit.


---


### compiler : `gcc`
### title : `[11/12/13/14 regression] 436.cactusADM regressed by 6-8% percent with -Ofast on Zen and Haswell, compared to gcc 7.2`
### open_at : `2018-02-20T17:47:26Z`
### last_modified_date : `2023-07-07T10:33:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84490
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0`
### severity : `normal`
### contents :
Compared to gcc 7.2, 436.cactusADM is about 8% and 6% slower when run
on an AND Ryzen and an AMD EPYC respectively after compiling it on
trunk with -Ofast (and generic march and tuning).

So far I have not done any bisecting, only verified that this is most
probably unrelated to PR 82362 because r251713 does not seem to have
any effect on the benchmark (the benchmark is already slow with that
revision).


---


### compiler : `gcc`
### title : `powerpc sub optimal condition register reuse with extended inline asm`
### open_at : `2018-02-22T11:58:13Z`
### last_modified_date : `2023-01-30T19:45:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84514
### status : `NEW`
### tags : `inline-asm, missed-optimization`
### component : `middle-end`
### version : `8.0.1`
### severity : `normal`
### contents :
Created attachment 43488
test case with description in comment at the top

There seem to be some missed opportunities reusing condition register over extended asm (that does not clobber cc).


---


### compiler : `gcc`
### title : `missed optimization: expected loop merging`
### open_at : `2018-02-22T14:14:23Z`
### last_modified_date : `2021-08-11T05:16:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84515
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
i expected f1 and f2 to compile to the same code:

unsigned g;

void f1(unsigned count) {
    unsigned i;
    for (i = 0; i < count  ; i++) g++;
}

void f2(unsigned count) {
    unsigned i;
    for (i = 0; i < count/2; i++) g++;
    for (     ; i < count  ; i++) g++;
}


but with -O3 the asm is

f1:
  testl %edi, %edi
  je .L1
  addl %edi, g(%rip)
.L1:
  ret

f2:
  movl %edi, %eax
  shrl %eax
  je .L8
  addl %eax, g(%rip)
.L8:
  cmpl %eax, %edi
  jbe .L7
  subl %eax, %edi
  addl %edi, g(%rip)
.L7:
  ret

(on aarch64 even the address of g is recomputed for the second loop with adrp)


---


### compiler : `gcc`
### title : `Suboptimal code for int128 masked shifts (ARM64)`
### open_at : `2018-02-25T06:23:17Z`
### last_modified_date : `2021-04-19T11:15:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84547
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.3.1`
### severity : `enhancement`
### contents :
Partially related to the Bug 84431 (see description of the problem there) but observed on ARM64 instead of x86/x86-64. (Not sure about ARM32.)

Test example:

__uint128_t func(__uint128_t a, unsigned shift)
{
       return a << (shift & 63);
}

aarch64-linux-gnu-gcc-7 -Wall -O2 -S test.c

GCC generates:
func:
    and w2, w2, 63
    mov w4, 63
    sub w5, w4, w2
    lsr x4, x0, 1
    sub w3, w2, #64
    lsl x1, x1, x2
    cmp w3, 0
    lsr x4, x4, x5
    orr x1, x4, x1
    lsl x4, x0, x3
    lsl x0, x0, x2
    csel    x1, x4, x1, ge
    csel    x0, x0, xzr, lt
    ret


While clang/llvm generates better code:

func:                                   // @func
// BB#0:
    and w8, w2, #0x3f
    lsr x9, x0, #1
    eor x11, x8, #0x3f
    lsl x10, x1, x8
    lsr x9, x9, x11
    orr     x1, x10, x9
    lsl x0, x0, x8
    ret


Another interesting case when __builtin_unreachable() is used:

__uint128_t func(__uint128_t a, unsigned shift)
{
    if (shift > 63)
        __builtin_unreachable();
    return a << shift;
}

But in this case, neither clang/llvm, nor gcc seem to be able to optimize code well.


---


### compiler : `gcc`
### title : `snprintf with null buffer not eliminated when return value is in a known range`
### open_at : `2018-02-26T19:49:30Z`
### last_modified_date : `2023-07-07T07:52:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84577
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
The sprintf pass eliminates snprintf calls with a null buffer and zero size when whose return value is a constant but it does not eliminate calls whose return value is in some range.  Both calls can be eliminated.

$ cat b.c && gcc -O2 -S -Wall -Wextra -fdump-tree-optimized=/dev/stdout b.c
void f (void)
{
  int n = __builtin_snprintf (0, 0, "%hhx", 123);   // eliminated
  if (n < 0 || 2 < n)
    __builtin_abort ();
}

void g (int i)
{
  int n = __builtin_snprintf (0, 0, "%hhx", i);   // not eliminated but could be
  if (n < 0 || 2 < n)
    __builtin_abort ();
}


;; Function f (f, funcdef_no=0, decl_uid=1957, cgraph_uid=0, symbol_order=0)

f ()
{
  <bb 2> [local count: 1073741825]:
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1961, cgraph_uid=1, symbol_order=1)

g (int i)
{
  <bb 2> [local count: 1073741825]:
  __builtin_snprintf (0B, 0, "%hhx", i_3(D)); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `powerpc toc register is reloaded unnecessarily`
### open_at : `2018-03-01T03:12:46Z`
### last_modified_date : `2019-02-12T04:32:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84626
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `enhancement`
### contents :
gcc version 8.0.1 20180207 (experimental) [trunk revision 257435] (Debian 8-20180207-2):

Test case:

void test(void (*fn)(void), unsigned long i)
{ 
        while (i--)
                fn();
}

Generates code:

  2c:   18 00 41 f8     std     r2,24(r1)
  30:   00 00 00 60     nop
  34:   00 00 00 60     nop
  38:   00 00 00 60     nop
  3c:   00 00 42 60     ori     r2,r2,0
  40:   78 f3 cc 7f     mr      r12,r30
  44:   a6 03 c9 7f     mtctr   r30
  48:   ff ff ff 3b     addi    r31,r31,-1
  4c:   21 04 80 4e     bctrl
  50:   18 00 41 e8     ld      r2,24(r1)
  54:   ff ff bf 2f     cmpdi   cr7,r31,-1
  58:   e8 ff 9e 40     bne     cr7,40 <test+0x40>

The r2 load could be moved out of the loop.


---


### compiler : `gcc`
### title : `Missed optimisation for hoisting conditions outside nested loops`
### open_at : `2018-03-01T10:40:06Z`
### last_modified_date : `2023-05-14T21:44:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84646
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
This is a missed optimisation opportunity.  In a discussion about the "best" way to break out of a nested loop, I tested this code with gcc:

int foo(const int * p, const int * q, int m, int n) {
    int sum = 0;
    bool running = true;
    const int max = 20000;
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < m; j++) {
            if (running) {
                sum += (p[i] * q[j]);
                if (sum >= max) {
                    running = false;
                    sum = max;
                }
            }
        }
    }
    return sum;
}


The test for "running" is hoisted outside the inner loop, so that the generated code is changed to approximately:

int foo(const int * p, const int * q, int m, int n) {
    int sum = 0;
    bool running = true;
    const int max = 20000;
    for (int i = 0; i < n; i++) {
loop:
        if (running) {
            for (int j = 0; j < m; j++) {
                sum += (p[i] * q[j]);
                if (sum >= max) {
                    running = false;
                    sum = max;
                    goto loop;
                }
            }
        }
    }
    return sum;
}

This is definitely a good step - avoiding the check for "running" in the inner loop, and breaking out of it when the max condition is reached is a clear win.  But it would be even nicer if the check could be hoisted further - the "running" flag could be completely eliminated to give the transformation:

int foo(const int * p, const int * q, int m, int n) {
    int sum = 0;
    //bool running = true;
    const int max = 20000;
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < m; j++) {
            if (running) {
                sum += (p[i] * q[j]);
                if (sum >= max) {
                    //running = false;
                    sum = max;
                    goto exit;
                }
            }
        }
    }
exit:
    return sum;
}


Testing was done with -O2 and -O3, on a variety of gcc versions and targets (thanks, godbolt.org!) up to version 8.0 (trunk at this time).  The generated code showed approximately the same transformations and optimisations.


---


### compiler : `gcc`
### title : `Missed optimization : loop not removed.`
### open_at : `2018-03-01T14:34:34Z`
### last_modified_date : `2018-11-20T08:40:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84648
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
The loop below is not eliminated:

int main() {
    for (unsigned i = 0; i < (1u << 31); ++i) {
    }
    return 0;
}

Compiled with -O3:

main:
  xor eax, eax
.L2:
  add eax, 1
  jns .L2
  xor eax, eax
  ret

The loop is removed for other bounds, e.g. (1u << 31) + 1 or (1u << 31) - 1, or when < is replaced with <=.

Allow me to make a guess of the underlying problem: The optimization that uses jns to detect when i reaches (10...0)_2 ends up by blocking the other optimization that eliminates the loop altoghether.

Same issue when using unsigned long long and (1ull << 63).

FWIW: clang has the same issue (in C but not in C++).


---


### compiler : `gcc`
### title : `Overcomplicated code generation for a chain of mutually exclusive conditions`
### open_at : `2018-03-02T14:19:00Z`
### last_modified_date : `2023-08-05T17:30:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84673
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.3.1`
### severity : `normal`
### contents :
This function

int has_bad_chars(unsigned char *str, __SIZE_TYPE__ len)
{
  for (unsigned char *c = str; c < str + len; c++)
    {
      unsigned char x = *c;
      if (__builtin_expect (x <= 0x1f || x == 0x5c || x == 0x7f,
                            0))
        return 1;
    }
  return 0;
}

compiles with GCC 7.3.1 at -Os -march=native on a current-generation x86-64 to

has_bad_chars:
	addq	%rdi, %rsi
.L2:
	cmpq	%rsi, %rdi
	jnb	.L7
	movb	(%rdi), %al
	cmpb	$31, %al
	setbe	%cl
	cmpb	$92, %al
	sete	%dl
	orb	%dl, %cl
	jne	.L5
	cmpb	$127, %al
	je	.L5
	incq	%rdi
	jmp	.L2
.L7:
	xorl	%eax, %eax
	ret
.L5:
	movl	$1, %eax
	ret

It is six bytes shorter, and also I think more efficient, to generate this instead:

has_bad_chars:
.LFB0:
	.cfi_startproc
	addq	%rdi, %rsi
.L2:
	cmpq	%rsi, %rdi
	jnb	.L7
	movb	(%rdi), %al
	cmpb	$31, %al
        jbe     .L5
	cmpb	$92, %al
        je      .L5
	cmpb	$127, %al
	je	.L5
	incq	%rdi
	jmp	.L2
.L7:
	xorl	%eax, %eax
	ret
.L5:
	movl	$1, %eax
	ret

The same thing happens at -O2, but also a chunk of the loop body gets pointlessly duplicated above the loop (it looks like it tried to unroll the loop and got stuck halfway).


---


### compiler : `gcc`
### title : `tree-ter moving code too much`
### open_at : `2018-03-02T18:22:39Z`
### last_modified_date : `2021-08-01T16:47:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84681
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `normal`
### contents :
The following code (derived from a hot loop in a Huffman encoder, reported by Fabian Giesen) suffers from TER activity too much on x86-64. TER lifts loads+zero_extends to the BB head, sinking variable-length shifts and increasing register pressure too badly.

Not being very familiar with TER, I think it would be good to understand why loads are lifted all the way up to BB head like that. That's probably not supposed to happen (and may be fixable without a TER overhaul?)

unsigned long long f(unsigned char *from,
                     unsigned char *from_end,
                     unsigned long long *codes,
                     unsigned char *lens)
{
    unsigned char sym0, sym1, sym2;
    unsigned long long bits0=0, bits1=0, bits2=0;
    unsigned char count0=0, count1=0, count2=0;
    do {
    sym0 = *from++; bits0 |= codes[sym0] << count0; count0 += lens[sym0];
    sym1 = *from++; bits1 |= codes[sym1] << count1; count1 += lens[sym1];
    sym2 = *from++; bits2 |= codes[sym2] << count2; count2 += lens[sym2];
    sym0 = *from++; bits0 |= codes[sym0] << count0; count0 += lens[sym0];
    sym1 = *from++; bits1 |= codes[sym1] << count1; count1 += lens[sym1];
    sym2 = *from++; bits2 |= codes[sym2] << count2; count2 += lens[sym2];
    sym0 = *from++; bits0 |= codes[sym0] << count0; count0 += lens[sym0];
    sym1 = *from++; bits1 |= codes[sym1] << count1; count1 += lens[sym1];
    sym2 = *from++; bits2 |= codes[sym2] << count2; count2 += lens[sym2];
    sym0 = *from++; bits0 |= codes[sym0] << count0; count0 += lens[sym0];
    sym1 = *from++; bits1 |= codes[sym1] << count1; count1 += lens[sym1];
    sym2 = *from++; bits2 |= codes[sym2] << count2; count2 += lens[sym2];
    sym0 = *from++; bits0 |= codes[sym0] << count0; count0 += lens[sym0];
    sym1 = *from++; bits1 |= codes[sym1] << count1; count1 += lens[sym1];
    sym2 = *from++; bits2 |= codes[sym2] << count2; count2 += lens[sym2];
    } while(from != from_end);
    return bits0+bits1+bits2;
}


---


### compiler : `gcc`
### title : `Missed evaluating to constant at tree level`
### open_at : `2018-03-05T12:49:22Z`
### last_modified_date : `2021-08-14T22:44:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84712
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Hi,
It seems GCC does not evaluate the following function to a constant at the tree level:

int sum(void)
{
  int a[] = {1, 2, 3, -1};
  int x = 0;

  for (int i = 0; i < 4; i++)
    if (a[i] < 0)
      break;
    else
      x += a[i];

  return x;
}

optimized dump shows:
sum ()
{
  int x;
  int a[4];
  int _25;
  int _33;
  int _41;

  <bb 2> [local count: 261993005]:
  MEM[(int *)&a] = { 1, 2, 3, -1 };
  _25 = a[1];
  if (_25 < 0)
    goto <bb 5>; [7.91%]
  else
    goto <bb 3>; [92.09%]

  <bb 3> [local count: 246744733]:
  x_30 = _25 + 1;
  _33 = a[2];
  if (_33 < 0)
    goto <bb 5>; [7.91%]
  else
    goto <bb 4>; [92.09%]

  <bb 4> [local count: 232383926]:
  x_38 = x_30 + _33;
  _41 = a[3];
  if (_41 < 0)
    goto <bb 5>; [7.91%]
  else
    goto <bb 6>; [92.09%]

  <bb 5> [local count: 47244641]:
  # x_17 = PHI <x_30(3), 1(2), x_38(4)>
  goto <bb 7>; [100.00%]

  <bb 6> [local count: 218858940]:
  x_10 = x_38 + _41;

  <bb 7> [local count: 261993005]:
  # x_2 = PHI <x_17(5), x_10(6)>
  a ={v} {CLOBBER};
  return x_2;

}

However at RTL, cprop seems to do the constant folding and set return value register to 6.

Thanks,
Prathamesh


---


### compiler : `gcc`
### title : `GCC does not fold xxswapd followed by vperm`
### open_at : `2018-03-07T21:19:09Z`
### last_modified_date : `2022-05-27T08:16:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84753
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.2.0`
### severity : `normal`
### contents :
I'm working on GCC112 from the compile farm. It is ppc64-le machine. It has both GCC 4.8.5 and GCC 7.2.0 installed. The issue is present on both.

We are trying to recover missing 1 to 2 cpb performance when using Power8 SHA built-ins. Part of the code to load a message into the message schedule looks like so:

   uint8_t msg[64] = {...};
   __vector unsigned char mask = {3,2,1,0, 7,6,5,4, 11,10,9,8, 15,14,13,12};

   __vector unsigned int t = vec_vsx_ld(0, msg);
   t = vec_perm(t, t, mask);

When I compile at -O3 and disassemble it, I see:

    100008bc:   99 26 20 7c     lxvd2x  vs33,0,r4
    ...
    100008d0:   57 0a 21 f0     xxswapd vs33,vs33
    100008d8:   2b 08 21 10     vperm   v1,v1,v1,v0

Calling xxswapd followed by vperm seems to be a lot like calling shuffle_epi32 followed by shuffle_epi8 on an x86 machine. It feels like the two permutes should be folded into one.

On x86 I would manually fold the two shuffles. On PPC I cannot because xxswapd is generated as part of the load, and then I call vperm. I have not figured out how to avoid the xxswapd. (I even tried to issue my own xxswapd to cancel out the one being generated by the compiler).

**********

Here's a minimal case, but the optimizer is removing the code of interest. The real code suffers it, and it can be found at https://github.com/noloader/SHA-Intrinsics/blob/master/sha256-p8.cxx .

$ cat test.cxx
#include <stdint.h>
#if defined(__ALTIVEC__)
# include <altivec.h>
# undef vector
# undef pixel
# undef bool
#endif

typedef __vector unsigned char uint8x16_p8;
typedef __vector unsigned int  uint32x4_p8;

// Unaligned load
template <class T> static inline
uint32x4_p8 VectorLoad32x4u(const T* data, int offset)
{
  return vec_vsx_ld(offset, (uint32_t*)data);
}

// Unaligned store
template <class T> static inline
void VectorStore32x4u(const uint32x4_p8 val, T* data, int offset)
{
  vec_vsx_st(val, offset, (uint32_t*)data);
}

static inline
uint32x4_p8 VectorPermute32x4(const uint32x4_p8 val, const uint8x16_p8 mask)
{
  return (uint32x4_p8)vec_perm(val, val, mask);
}

int main(int argc, char* argv[])
{
  uint8_t M[64];
  uint32_t W[64];

  uint8_t* m = M;
  uint32_t* w = W;

  const uint8x16_p8 mask = {3,2,1,0, 7,6,5,4, 11,10,9,8, 15,14,13,12};
  for (unsigned int i=0; i<16; i+=4, m+=4, w+=4)
    VectorStore32x4u(VectorPermute32x4(VectorLoad32x4u(m, 0), mask), w, 0);

  return 0;
}


---


### compiler : `gcc`
### title : `Multiplication done twice just to get upper and lower parts of product`
### open_at : `2018-03-08T07:22:05Z`
### last_modified_date : `2021-09-05T03:16:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84756
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `7.2.0`
### severity : `enhancement`
### contents :
Consider the following C code valid for both x86 and amd64 targets:

#ifdef __SIZEOF_INT128__
typedef __uint128_t Longer;
#else
typedef unsigned long long Longer;
#endif
typedef unsigned long Shorter;

Shorter mul(Shorter a, Shorter b, Shorter* upper)
{
    *upper=(Longer)a*b >> 8*sizeof(Shorter);
    return (Longer)a*b;
}

Longer lmul(Shorter a, Shorter b)
{
    return (Longer)a*b;
}

From lmul function I get the expected good assembly:

lmul:
	mov	eax, DWORD PTR [esp+8]
	mul	DWORD PTR [esp+4]
	ret

But for mul gcc generates two multiplications instead of one:

mul:
	push	ebx
	mov	ecx, DWORD PTR [esp+8]
	mov	ebx, DWORD PTR [esp+12]
	mov	eax, ecx
	mul	ebx
	mov	eax, DWORD PTR [esp+16]
	mov	DWORD PTR [eax], edx
	mov	eax, ecx
	imul	eax, ebx
	pop	ebx
	ret

Here 'mul ebx' is used to get the upper part of the result, and `imul eax, ebx` is supposed to ge the lower part, although it has already been present right after `mul ebx` in eax register.

Similar problem happens when I use -m64 option for gcc to get amd64 code.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Useless MOVs and PUSHes to store results of MUL`
### open_at : `2018-03-08T07:47:45Z`
### last_modified_date : `2023-08-04T08:23:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84757
### status : `NEW`
### tags : `deferred, missed-optimization, ra`
### component : `target`
### version : `7.3.0`
### severity : `normal`
### contents :
Consider the following C code:

#ifdef __SIZEOF_INT128__
typedef __uint128_t Longer;
#else
typedef unsigned long long Longer;
#endif
typedef unsigned long Shorter;

Shorter mulSmarter(Shorter a, Shorter b, Shorter* upper)
{
    const Longer ab=(Longer)a*b;
    *upper=ab >> 8*sizeof(Shorter);
    return ab;
}

On amd64 with -m64 option I get identical assembly on both gcc 7.x and 6.3. But on x86 (or amd64 with -m32) assembly is different, and on gcc 7.x is less efficient. See to compare:

# gcc 6.3
mulSmarter:
  mov eax, DWORD PTR [esp+8]
  mul DWORD PTR [esp+4]
  mov ecx, edx
  mov edx, DWORD PTR [esp+12]
  mov DWORD PTR [edx], ecx
  ret

# gcc 7.3
mulSmarter:
  push esi
  push ebx
  mov eax, DWORD PTR [esp+16]
  mul DWORD PTR [esp+12]
  mov esi, edx
  mov edx, DWORD PTR [esp+20]
  mov ebx, eax
  mov eax, ebx
  mov DWORD PTR [edx], esi
  pop ebx
  pop esi
  ret

The gcc 6.3 version is already not perfect, but it's much better than that of 7.3.


---


### compiler : `gcc`
### title : `Calculation of quotient and remainder with constant denominator uses __umoddi3+__udivdi3 instead of __udivmoddi4`
### open_at : `2018-03-08T11:52:52Z`
### last_modified_date : `2021-08-15T11:13:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84759
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `7.3.0`
### severity : `normal`
### contents :
Starting from GCC 7, code calculating both quotient and remainder of a loong division calls a single __udivmodti4. But this only happens for general values of denominator, while for specific constants for some reason GCC still generates calls to __umodti3 and __udivti3. See the following code (the picture is the same for x86 and amd64 targets):

#ifdef __SIZEOF_INT128__
typedef __uint128_t Longer;
#else
typedef unsigned long long Longer;
#endif
typedef unsigned long Shorter;

Shorter divmod(Longer numerator, Shorter denominator, Shorter* remainder)
{
    *remainder = numerator%denominator;
    return numerator/denominator;
}

Shorter divmodConst(Longer numerator, Shorter* remainder)
{
    const Shorter denominator = 100;
    *remainder = numerator%denominator;
    return numerator/denominator;
}

Here divmod is optimized, while divmodConst appears not optimized:

divmod:
	sub	esp, 28
	xor	edx, edx
	mov	eax, DWORD PTR [esp+40]
	lea	ecx, [esp+8]
	sub	esp, 12
	push	ecx
	push	edx
	push	eax
	push	DWORD PTR [esp+60]
	push	DWORD PTR [esp+60]
	call	__udivmoddi4
	mov	edx, DWORD PTR [esp+76]
	mov	ecx, DWORD PTR [esp+40]
	mov	DWORD PTR [edx], ecx
	add	esp, 60
	ret
divmodConst:
	push	edi
	push	esi
	sub	esp, 4
	mov	esi, DWORD PTR [esp+16]
	mov	edi, DWORD PTR [esp+20]
	push	0
	push	100
	push	edi
	push	esi
	call	__umoddi3
	add	esp, 16
	mov	edx, DWORD PTR [esp+24]
	mov	DWORD PTR [edx], eax
	push	0
	push	100
	push	edi
	push	esi
	call	__udivdi3
	add	esp, 20
	pop	esi
	pop	edi
	ret


---


### compiler : `gcc`
### title : `-Os inhibits all vectorization`
### open_at : `2018-03-09T10:41:25Z`
### last_modified_date : `2018-11-20T14:48:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84777
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
Neither the command-line flag -ftree-loop-vectorize nor -fopenmp combined with "#pragma omp simd" works when -Os is active.

It seems that it when specified manually vectorization should be work even in -Os mode. I can almost see why -ftree-loop-vectorize wouldn't work, which is why I tried the manual marking of loops to vectorize, but the latter didn't work either.

I would suggest documenting this behavior and fix at least vectorizing manually marked loops.


---


### compiler : `gcc`
### title : `[missed optimization] ignore bitmask after movemask`
### open_at : `2018-03-09T13:10:09Z`
### last_modified_date : `2021-08-03T00:14:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84781
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `enhancement`
### contents :
Testcase: https://godbolt.org/g/S3tfrL

#include <x86intrin.h>

int f(__m128  a) { return _mm_movemask_ps(a)    & 0xf; }
int f(__m128d a) { return _mm_movemask_pd(a)    & 0x3; }
int f(__m128i a) { return _mm_movemask_epi8(a)  & 0xffffu; }
int f(__m256  a) { return _mm256_movemask_ps(a) & 0xff; }
int f(__m256d a) { return _mm256_movemask_pd(a) & 0xf; }

In all of these functions, the bitmask is a no-op since the movemask cannot yield bits in any of the masked-off places. Consequently, the bitwise and should be dropped.


---


### compiler : `gcc`
### title : `memcpy/memmove between a struct and its first member not eliminated`
### open_at : `2018-03-09T21:26:03Z`
### last_modified_date : `2021-09-06T02:43:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84794
### status : `NEW`
### tags : `missed-optimization, TREE`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
GCC eliminates most calls to memcpy where the first two arguments are guaranteed to be the same.  However, it fails to eliminate such calls when one of the arguments points to a struct object and the other to the first member of the same struct object.

$ cat z.c && gcc -O2 -S -Wall -Wextra -Wpedantic  -Wrestrict -fdump-tree-optimized=/dev/stdout z.c
extern void* memcpy (void* restrict, const void* restrict, __SIZE_TYPE__);

struct S {
  union {
    char a[4], b[4];
  };
} s;

void f (struct S *p)
{
  if ((void*)p != p->a || (void*)p != p->b)   // folded to false (good)
    __builtin_abort ();                       // eliminated
}

void g (struct S *p, unsigned n)
{
  memcpy (p->a, p->b, n);   // undefined and eliminated (good)
}

void h (struct S *p, unsigned n)
{
  memcpy (p, p->b, n);   // also undefined but not eliminated
}



;; Function f (f, funcdef_no=0, decl_uid=1968, cgraph_uid=0, symbol_order=1)

f (struct S * p)
{
  <bb 2> [local count: 1073741825]:
  return;

}


z.c: In function ‘g’:
z.c:17:3: warning: ‘memcpy’ source argument is the same as destination [-Wrestrict]
   memcpy (p->a, p->b, n);   // -Wrestrict (good)
   ^~~~~~~~~~~~~~~~~~~~~~

;; Function g (g, funcdef_no=1, decl_uid=1972, cgraph_uid=1, symbol_order=2)

g (struct S * p, unsigned int n)
{
  <bb 2> [local count: 1073741825]:
  return;

}



;; Function h (h, funcdef_no=2, decl_uid=1976, cgraph_uid=2, symbol_order=3)

h (struct S * p, unsigned int n)
{
  long unsigned int _1;
  char[4] * _2;

  <bb 2> [local count: 1073741825]:
  _1 = (long unsigned int) n_3(D);
  _2 = &p_4(D)->D.1965.b;
  memcpy (p_4(D), _2, _1); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `DCE fails to remove dead code of std::function constructor`
### open_at : `2018-03-12T03:43:06Z`
### last_modified_date : `2023-09-24T16:47:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84824
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `normal`
### contents :
#include <functional>
#include <stdio.h>

std::function<int(int, int)> getFunc(int i) {
  auto f = [=] (int a, int b) {
    return a + b + i;
  };
  return f;
}

int main() {
  printf("%d", getFunc(1)(1, 1));
  return 0;
}

In this example gcc generate code with redundant store operations and exception handling block like:
main:
.LFB1411:
	.cfi_startproc
	.cfi_personality 0x9b,DW.ref.__gxx_personality_v0
	.cfi_lsda 0x1b,.LLSDA1411
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	leaq	.LC0(%rip), %rsi
	movl	$3, %edx
	movl	$1, %edi
	subq	$48, %rsp
	.cfi_def_cfa_offset 64
	movq	%fs:40, %rax
	movq	%rax, 40(%rsp)
	xorl	%eax, %eax
	leaq	_ZNSt17_Function_handlerIFiiiEZ7getFunciEUliiE_E9_M_invokeERKSt9_Any_dataOiS6_(%rip), %rax
	movl	$1, (%rsp)
	movq	%rax, 24(%rsp)
	leaq	_ZNSt14_Function_base13_Base_managerIZ7getFunciEUliiE_E10_M_managerERSt9_Any_dataRKS3_St18_Manager_operation(%rip), %rax
	movq	%rax, 16(%rsp)
	xorl	%eax, %eax
.LEHB0:
	call	__printf_chk@PLT
.LEHE0:
	movq	16(%rsp), %rax
	testq	%rax, %rax
	je	.L10
	movq	%rsp, %rdi
	movl	$3, %edx
	movq	%rdi, %rsi
	call	*%rax
.L10:
	xorl	%eax, %eax
	movq	40(%rsp), %rcx
	xorq	%fs:40, %rcx
	jne	.L23
	addq	$48, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 16
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret


But clang manage to remove these code
main:                                   # @main
	.cfi_startproc
# %bb.0:
	pushq	%rax
	.cfi_def_cfa_offset 16
	movl	$.L.str, %edi
	movl	$3, %esi
	xorl	%eax, %eax
	callq	printf
	xorl	%eax, %eax
	popq	%rcx
	retq


compiler flags are: -O2 -S

It seems gcc insist on construct std::function<int(int,int)> object on stack.


---


### compiler : `gcc`
### title : `[8 Regression] bogus -Warray-bounds on a memcpy in a loop`
### open_at : `2018-03-14T01:50:14Z`
### last_modified_date : `2020-05-21T19:26:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84859
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
The following test case was reduced from the report at https://lkml.org/lkml/2018/3/13/383.

In g() GCC correctly figures out the memcpy call is safe.  In h(), though, it thinks it overflows.

$ cat x.c && gcc -S -O2 -Wall x.c
void f (void*);

void __attribute__ ((noinline, noclone))
g (const void *p, unsigned n)
{
  unsigned char a[8];
  if (n > sizeof a)
    return;

  for (; n > 0; n -= *a)
  {
     *a = n > 255 ? 255 : n;

    __builtin_memcpy (a, p, *a);   // no warning (good)
  }
}

void __attribute__ ((noinline, noclone))
h (const void *p, unsigned n)
{
  unsigned char a[8];
  if (n > sizeof a)
    return;

  for (; n > 0; n -= *a)
  {
    if (n > 255)
      *a = 255;
    else
      *a = n;

    __builtin_memcpy (a, p, *a);   // bogus -Warray-bounds
  }
}
x.c: In function ‘h’:
x.c:32:5: warning: ‘__builtin_memcpy’ forming offset [9, 255] is out of the bounds [0, 8] of object ‘a’ with type ‘unsigned char[8]’ [-Warray-bounds]
     __builtin_memcpy (a, p, *a);   // bogus -Warray-bounds
     ^~~~~~~~~~~~~~~~~~~~~~~~~~~
x.c:21:17: note: ‘a’ declared here
   unsigned char a[8];
                 ^


---


### compiler : `gcc`
### title : `int loads not eliminated against larger stores`
### open_at : `2018-03-19T18:46:19Z`
### last_modified_date : `2022-02-23T11:49:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84958
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
[ As discussed here: https://gcc.gnu.org/ml/gcc-patches/2018-03/msg00800.html ]

This test-case:
...
int foo()
{
  int a[10];
  for(int i = 0; i < 10; ++i)
    a[i] = i*i;
  int res = 0;
  for(int i = 0; i < 10; ++i)
    res += a[i];
  return res;
}
...

compiled with -O3 results in this gimple at optimized:
...
  MEM[(int *)&a] = { 0, 1 };
  MEM[(int *)&a + 8B] = { 4, 9 };
  MEM[(int *)&a + 16B] = { 16, 25 };
  MEM[(int *)&a + 24B] = { 36, 49 };
  MEM[(int *)&a + 32B] = { 64, 81 };
  _6 = a[0];
  _28 = a[1];
  res_29 = _6 + _28;
  _35 = a[2];
  res_36 = res_29 + _35;
  _42 = a[3];
  res_43 = res_36 + _42;
  _49 = a[4];
  res_50 = res_43 + _49;
  _56 = a[5];
  res_57 = res_50 + _56;
  _63 = a[6];
  res_64 = res_57 + _63;
  _70 = a[7];
  res_71 = res_64 + _70;
  _77 = a[8];
  res_78 = res_71 + _77;
  _2 = a[9];
  res_11 = _2 + res_78;
  a ={v} {CLOBBER};
  return res_11;
...

Loop vectorization has no effect, and the scalar loops are completely unrolled. Then slp vectorization vectorizes the stores. 

When disabling slp vectorization, we have instead:
...
return 285;
...


[ FWIW, adding an extra fre pass here also results in optimal gimple:
...
diff --git a/gcc/passes.def b/gcc/passes.def
index 3ebcfc30349..6b64f600c4a 100644
--- a/gcc/passes.def
+++ b/gcc/passes.def
@@ -325,6 +325,7 @@ along with GCC; see the file COPYING3.  If not see
       NEXT_PASS (pass_tracer);
       NEXT_PASS (pass_thread_jumps);
       NEXT_PASS (pass_dominator, false /* may_peel_loop_headers_p */);
+      NEXT_PASS (pass_fre);
       NEXT_PASS (pass_strlen);
       NEXT_PASS (pass_thread_jumps);
       NEXT_PASS (pass_vrp, false /* warn_array_bounds_p */);
...
]


---


### compiler : `gcc`
### title : `Optimize integer operations on floating point constants without -ffast-math but with -fno-trapping-math`
### open_at : `2018-03-20T17:19:19Z`
### last_modified_date : `2023-08-09T09:47:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84997
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
If it is known at compile time that changing the floating point constant to integral constant results in exactly the same value, then replace the floating point operation with integral operation.

For example

int test2(int lhs) {
    lhs += 2.0;
    return lhs;
}

clang optimizes to the following assembly

test2(int): # @test2(int)
  lea eax, [rdi + 2]
  ret

, while GCC produces a suboptimal assembly:

test(int):
        pxor    xmm0, xmm0
        cvtsi2sd        xmm0, edi
        addsd   xmm0, QWORD PTR .LC0[rip]
        cvttsd2si       eax, xmm0
        ret
.LC0:
        .long   0
        .long   1073741824




More complex examples (where clang fails):

#include <limits>

int test2(int lhs) {
    lhs += 2.000000000000001;
    return lhs;
}

int test3(int lhs) {
    constexpr auto fp = 2.001;
    constexpr int max = std::numeric_limits<int>::max();
    constexpr int min = std::numeric_limits<int>::min();

    // Checking that the result is same even with max * epsilon
    static_assert(
        static_cast<int>(fp + max - fp) == max
        && static_cast<int>(fp + min - fp) == min,
        ""
    );

    return fp + lhs - fp;
}


---


### compiler : `gcc`
### title : `Inline built-in fdim for -fno-math-errno`
### open_at : `2018-03-20T17:49:32Z`
### last_modified_date : `2021-12-27T03:44:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85003
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0`
### severity : `enhancement`
### contents :
GCC should support inline code generation for the fdim / fdimf / fdiml functions, given -fno-math-errno.

glibc's bits/mathinline.h headers for powerpc and sparc (only) have inlines of the form:

__MATH_INLINE double
__NTH (fdim (double __x, double __y))
{
  return __x <= __y ? 0 : __x - __y;
}

We're moving away from such inlines in glibc, preferring to leave it to the compiler to inline standard functions under appropriate conditions.  Now, the above (which lacks a -fno-math-errno conditional) isn't actually correct; you need to use an unordered comparison, so the appropriate expansion (given -fno-math-errno) is of the form (considered as an inline not a macro, so avoiding multiple evaluations of arguments):

__builtin_islessequal (x, y) ? 0 : x - y

Note: as well as -fno-math-errno, for correctness this also requires that, if standard-conforming excess precision has been requested, the back end does not add any implicit excess precision for the type in question, as specified by the targetm.c.excess_precision hook (so it's not correct to do this inline expansion for 32-bit x86 with x87 floating point for float or double if -fexcess-precision=standard, in particular).


---


### compiler : `gcc`
### title : `GCC does not inline extern "C" functions defined in a different namespace`
### open_at : `2018-03-21T05:20:49Z`
### last_modified_date : `2021-07-22T23:41:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85012
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `7.3.1`
### severity : `normal`
### contents :
$ cat x.cpp
namespace foo { extern "C" void f(); }
extern "C" void f() {}
void f1()
{
    foo::f();
    foo::f();
}
void f2()
{
    f();
    f();
}

$ g++ -v -S -O1 x.cpp
Using built-in specs.
COLLECT_GCC=g++
Target: x86_64-pc-linux-gnu
Configured with: /build/gcc/src/gcc/configure --prefix=/usr --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --enable-languages=c,c++,ada,fortran,go,lto,objc,obj-c++ --enable-shared --enable-threads=posix --enable-libmpx --with-system-zlib --with-isl --enable-__cxa_atexit --disable-libunwind-exceptions --enable-clocale=gnu --disable-libstdcxx-pch --disable-libssp --enable-gnu-unique-object --enable-linker-build-id --enable-lto --enable-plugin --enable-install-libiberty --with-linker-hash-style=gnu --enable-gnu-indirect-function --enable-multilib --disable-werror --enable-checking=release --enable-default-pie --enable-default-ssp
Thread model: posix
gcc version 7.3.1 20180312 (GCC) 
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-S' '-O1' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/cc1plus -E -quiet -v -D_GNU_SOURCE x.cpp -mtune=generic -march=x86-64 -O1 -fpch-preprocess -o x.ii
ignoring nonexistent directory "/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/../../../../x86_64-pc-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/../../../../include/c++/7.3.1
 /usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/../../../../include/c++/7.3.1/x86_64-pc-linux-gnu
 /usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/../../../../include/c++/7.3.1/backward
 /usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/include
 /usr/local/include
 /usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/include-fixed
 /usr/include
End of search list.
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-S' '-O1' '-shared-libgcc' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/cc1plus -fpreprocessed x.ii -quiet -dumpbase x.cpp -mtune=generic -march=x86-64 -auxbase x -O1 -version -o x.s
GNU C++14 (GCC) version 7.3.1 20180312 (x86_64-pc-linux-gnu)
	compiled by GNU C version 7.3.1 20180312, GMP version 6.1.2, MPFR version 4.0.1, MPC version 1.1.0, isl version isl-0.18-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
GNU C++14 (GCC) version 7.3.1 20180312 (x86_64-pc-linux-gnu)
	compiled by GNU C version 7.3.1 20180312, GMP version 6.1.2, MPFR version 4.0.1, MPC version 1.1.0, isl version isl-0.18-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 7db9d5d3f13a1b145edda774ba2b8b66
COMPILER_PATH=/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/:/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/../../../../lib/:/lib/../lib/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.1/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-S' '-O1' '-shared-libgcc' '-mtune=generic' '-march=x86-64'


The generated assembly for f2 contains only a return.  The generated assembly for f1 contains 2 calls to f.  Since ::f and foo::f refer to the same function^[1], the same optimizations should be applied to calls using either name.

[1] n4659 section 10.5 [dcl.link] paragraph 6: Two declarations for a function with C language linkage with the same function name (ignoring the namespace names that qualify it) that appear in different namespace scopes refer to the same function.


---


### compiler : `gcc`
### title : `Missed constant propagation to index of array reference`
### open_at : `2018-03-21T08:16:17Z`
### last_modified_date : `2021-08-15T01:23:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85017
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `enhancement`
### contents :
I found extra instruction generated on x86_64 platform. I have no performance data to prove performance gap for this but other compilers generates more optimal code from my point of view.

Command line (the same with -O2 and -O3):
gcc -c -O1 test.c


The source code:

char arr[400];

char foo(int c)
{
    int i = c + 20;
    return arr[i];
}

Main trunk GCC generates following:
  addl    $20, %edi
  movslq  %edi, %rdi
  movzbl  arr(%rdi), %eax
  ret

but other compilers (example with clang):
  movslq  %edi, %rax
  movb    arr+20(%rax), %al
  retq

The constant $20 is better to propagate into array loading instead adding it to the register explicitly.

Could someone please advice me a workaround or patch? I will provide a performance data for this.

Sergey


---


### compiler : `gcc`
### title : `[missed optimization] vector conversions`
### open_at : `2018-03-23T11:25:48Z`
### last_modified_date : `2023-03-31T07:28:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85048
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `enhancement`
### contents :
The following testcase lists all integer and/or float conversions applied to vector builtins of the same number of elements. All of those functions can be compiled to a single instruction (the function's name plus `ret`) when `-march=skylake-avx512` is active. AFAICS many conversion instructions in the SSE and AVX ISA extensions are also unsupported.

I would expect this code to compile to optimal conversion sequences even on -O2 (and lower) since the conversion is applied directly on vector builtins. If this is not in scope, I'd like to open a feature request for something like clang's __builtin_convertvector (could be even done via static_cast) that produces optimal conversion instruction sequences on vector builtins without the auto-vectorizer.

#include <cstdint>

template <class T, int N, int Size = N * sizeof(T)>
using V [[gnu::vector_size(Size)]] = T;

template <class From, class To> V<To, 2> cvt2(V<From, 2> x) {
    return V<To, 2>{To(x[0]), To(x[1])};
}
template <class From, class To> V<To, 4> cvt4(V<From, 4> x) {
    return V<To, 4>{To(x[0]), To(x[1]), To(x[2]), To(x[3])};
}
template <class From, class To> V<To, 8> cvt8(V<From, 8> x) {
    return V<To, 8>{
        To(x[0]), To(x[1]), To(x[2]), To(x[3]),
        To(x[4]), To(x[5]), To(x[6]), To(x[7])
    };
}
template <class From, class To> V<To, 16> cvt16(V<From, 16> x) {
    return V<To, 16>{
        To(x[0]), To(x[1]), To(x[2]), To(x[3]),
        To(x[4]), To(x[5]), To(x[6]), To(x[7]),
        To(x[8]), To(x[9]), To(x[10]), To(x[11]),
        To(x[12]), To(x[13]), To(x[14]), To(x[15])
    };
}
template <class From, class To> V<To, 32> cvt32(V<From, 32> x) {
    return V<To, 32>{
        To(x[0]), To(x[1]), To(x[2]), To(x[3]),
        To(x[4]), To(x[5]), To(x[6]), To(x[7]),
        To(x[8]), To(x[9]), To(x[10]), To(x[11]),
        To(x[12]), To(x[13]), To(x[14]), To(x[15]),
        To(x[16]), To(x[17]), To(x[18]), To(x[19]),
        To(x[20]), To(x[21]), To(x[22]), To(x[23]),
        To(x[24]), To(x[25]), To(x[26]), To(x[27]),
        To(x[28]), To(x[29]), To(x[30]), To(x[31])
    };
}
template <class From, class To> V<To, 64> cvt64(V<From, 64> x) {
    return V<To, 64>{
        To(x[ 0]), To(x[ 1]), To(x[ 2]), To(x[ 3]),
        To(x[ 4]), To(x[ 5]), To(x[ 6]), To(x[ 7]),
        To(x[ 8]), To(x[ 9]), To(x[10]), To(x[11]),
        To(x[12]), To(x[13]), To(x[14]), To(x[15]),
        To(x[16]), To(x[17]), To(x[18]), To(x[19]),
        To(x[20]), To(x[21]), To(x[22]), To(x[23]),
        To(x[24]), To(x[25]), To(x[26]), To(x[27]),
        To(x[28]), To(x[29]), To(x[30]), To(x[31]),
        To(x[32]), To(x[33]), To(x[34]), To(x[35]),
        To(x[36]), To(x[37]), To(x[38]), To(x[39]),
        To(x[40]), To(x[41]), To(x[42]), To(x[43]),
        To(x[44]), To(x[45]), To(x[46]), To(x[47]),
        To(x[48]), To(x[49]), To(x[50]), To(x[51]),
        To(x[52]), To(x[53]), To(x[54]), To(x[55]),
        To(x[56]), To(x[57]), To(x[58]), To(x[59]),
        To(x[60]), To(x[61]), To(x[62]), To(x[63]),
    };
}

#define _(name, from, to, size) \
auto name(V<from, size> x) { return cvt##size<from, to>(x); }
// integral -> integral; truncation
_(vpmovqd , uint64_t, uint32_t,  2)
_(vpmovqd , uint64_t, uint32_t,  4)
_(vpmovqd , uint64_t, uint32_t,  8)
_(vpmovqd ,  int64_t, uint32_t,  2)
_(vpmovqd ,  int64_t, uint32_t,  4)
_(vpmovqd ,  int64_t, uint32_t,  8)
_(vpmovqd_, uint64_t,  int32_t,  2)
_(vpmovqd_, uint64_t,  int32_t,  4)
_(vpmovqd_, uint64_t,  int32_t,  8)
_(vpmovqd_,  int64_t,  int32_t,  2)
_(vpmovqd_,  int64_t,  int32_t,  4)
_(vpmovqd_,  int64_t,  int32_t,  8)

_(vpmovqw , uint64_t, uint16_t,  2)
_(vpmovqw , uint64_t, uint16_t,  4)
_(vpmovqw , uint64_t, uint16_t,  8)
_(vpmovqw ,  int64_t, uint16_t,  2)
_(vpmovqw ,  int64_t, uint16_t,  4)
_(vpmovqw ,  int64_t, uint16_t,  8)
_(vpmovqw_, uint64_t,  int16_t,  2)
_(vpmovqw_, uint64_t,  int16_t,  4)
_(vpmovqw_, uint64_t,  int16_t,  8)
_(vpmovqw_,  int64_t,  int16_t,  2)
_(vpmovqw_,  int64_t,  int16_t,  4)
_(vpmovqw_,  int64_t,  int16_t,  8)

_(vpmovqb , uint64_t,  uint8_t,  2)
_(vpmovqb , uint64_t,  uint8_t,  4)
_(vpmovqb , uint64_t,  uint8_t,  8)
_(vpmovqb ,  int64_t,  uint8_t,  2)
_(vpmovqb ,  int64_t,  uint8_t,  4)
_(vpmovqb ,  int64_t,  uint8_t,  8)
_(vpmovqb_, uint64_t,   int8_t,  2)
_(vpmovqb_, uint64_t,   int8_t,  4)
_(vpmovqb_, uint64_t,   int8_t,  8)
_(vpmovqb_,  int64_t,   int8_t,  2)
_(vpmovqb_,  int64_t,   int8_t,  4)
_(vpmovqb_,  int64_t,   int8_t,  8)

_(vpmovdw , uint32_t, uint16_t,  4)
_(vpmovdw , uint32_t, uint16_t,  8)
_(vpmovdw , uint32_t, uint16_t, 16)
_(vpmovdw ,  int32_t, uint16_t,  4)
_(vpmovdw ,  int32_t, uint16_t,  8)
_(vpmovdw ,  int32_t, uint16_t, 16)
_(vpmovdw_, uint32_t,  int16_t,  4)
_(vpmovdw_, uint32_t,  int16_t,  8)
_(vpmovdw_, uint32_t,  int16_t, 16)
_(vpmovdw_,  int32_t,  int16_t,  4)
_(vpmovdw_,  int32_t,  int16_t,  8)
_(vpmovdw_,  int32_t,  int16_t, 16)

_(vpmovdb , uint32_t,  uint8_t,  4)
_(vpmovdb , uint32_t,  uint8_t,  8)
_(vpmovdb , uint32_t,  uint8_t, 16)
_(vpmovdb ,  int32_t,  uint8_t,  4)
_(vpmovdb ,  int32_t,  uint8_t,  8)
_(vpmovdb ,  int32_t,  uint8_t, 16)
_(vpmovdb_, uint32_t,   int8_t,  4)
_(vpmovdb_, uint32_t,   int8_t,  8)
_(vpmovdb_, uint32_t,   int8_t, 16)
_(vpmovdb_,  int32_t,   int8_t,  4)
_(vpmovdb_,  int32_t,   int8_t,  8)
_(vpmovdb_,  int32_t,   int8_t, 16)

_(vpmovwb , uint16_t,  uint8_t,  8)
_(vpmovwb , uint16_t,  uint8_t, 16)
_(vpmovwb , uint16_t,  uint8_t, 32)
_(vpmovwb ,  int16_t,  uint8_t,  8)
_(vpmovwb ,  int16_t,  uint8_t, 16)
_(vpmovwb ,  int16_t,  uint8_t, 32)
_(vpmovwb_, uint16_t,   int8_t,  8)
_(vpmovwb_, uint16_t,   int8_t, 16)
_(vpmovwb_, uint16_t,   int8_t, 32)
_(vpmovwb_,  int16_t,   int8_t,  8)
_(vpmovwb_,  int16_t,   int8_t, 16)
_(vpmovwb_,  int16_t,   int8_t, 32)

// integral -> integral; zero extension
_(vpmovzxbw , uint8_t,  int16_t,  8)
_(vpmovzxbw , uint8_t,  int16_t, 16)
_(vpmovzxbw , uint8_t,  int16_t, 32)
_(vpmovzxbw_, uint8_t, uint16_t,  8)
_(vpmovzxbw_, uint8_t, uint16_t, 16)
_(vpmovzxbw_, uint8_t, uint16_t, 32)

_(vpmovzxbd ,  uint8_t,  int32_t,  4)
_(vpmovzxbd ,  uint8_t,  int32_t,  8)
_(vpmovzxbd ,  uint8_t,  int32_t, 16)
_(vpmovzxwd , uint16_t,  int32_t,  4)
_(vpmovzxwd , uint16_t,  int32_t,  8)
_(vpmovzxwd , uint16_t,  int32_t, 16)
_(vpmovzxbd_,  uint8_t, uint32_t,  4)
_(vpmovzxbd_,  uint8_t, uint32_t,  8)
_(vpmovzxbd_,  uint8_t, uint32_t, 16)
_(vpmovzxwd_, uint16_t, uint32_t,  4)
_(vpmovzxwd_, uint16_t, uint32_t,  8)
_(vpmovzxwd_, uint16_t, uint32_t, 16)

_(vpmovzxbq ,  uint8_t,  int64_t, 2)
_(vpmovzxbq ,  uint8_t,  int64_t, 4)
_(vpmovzxbq ,  uint8_t,  int64_t, 8)
_(vpmovzxwq , uint16_t,  int64_t, 2)
_(vpmovzxwq , uint16_t,  int64_t, 4)
_(vpmovzxwq , uint16_t,  int64_t, 8)
_(vpmovzxdq , uint32_t,  int64_t, 2)
_(vpmovzxdq , uint32_t,  int64_t, 4)
_(vpmovzxdq , uint32_t,  int64_t, 8)
_(vpmovzxbq_,  uint8_t, uint64_t, 2)
_(vpmovzxbq_,  uint8_t, uint64_t, 4)
_(vpmovzxbq_,  uint8_t, uint64_t, 8)
_(vpmovzxwq_, uint16_t, uint64_t, 2)
_(vpmovzxwq_, uint16_t, uint64_t, 4)
_(vpmovzxwq_, uint16_t, uint64_t, 8)
_(vpmovzxdq_, uint32_t, uint64_t, 2)
_(vpmovzxdq_, uint32_t, uint64_t, 4)
_(vpmovzxdq_, uint32_t, uint64_t, 8)

// integral -> integral; sign extension
_(vpmovsxbw , int8_t,  int16_t,  8)
_(vpmovsxbw , int8_t,  int16_t, 16)
_(vpmovsxbw , int8_t,  int16_t, 32)
_(vpmovsxbw_, int8_t, uint16_t,  8)
_(vpmovsxbw_, int8_t, uint16_t, 16)
_(vpmovsxbw_, int8_t, uint16_t, 32)

_(vpmovsxbd ,  int8_t,  int32_t,  4)
_(vpmovsxbd ,  int8_t,  int32_t,  8)
_(vpmovsxbd ,  int8_t,  int32_t, 16)
_(vpmovsxwd , int16_t,  int32_t,  4)
_(vpmovsxwd , int16_t,  int32_t,  8)
_(vpmovsxwd , int16_t,  int32_t, 16)
_(vpmovsxbd_,  int8_t, uint32_t,  4)
_(vpmovsxbd_,  int8_t, uint32_t,  8)
_(vpmovsxbd_,  int8_t, uint32_t, 16)
_(vpmovsxwd_, int16_t, uint32_t,  4)
_(vpmovsxwd_, int16_t, uint32_t,  8)
_(vpmovsxwd_, int16_t, uint32_t, 16)

_(vpmovsxbq ,  int8_t,  int64_t, 2)
_(vpmovsxbq ,  int8_t,  int64_t, 4)
_(vpmovsxbq ,  int8_t,  int64_t, 8)
_(vpmovsxwq , int16_t,  int64_t, 2)
_(vpmovsxwq , int16_t,  int64_t, 4)
_(vpmovsxwq , int16_t,  int64_t, 8)
_(vpmovsxdq , int32_t,  int64_t, 2)
_(vpmovsxdq , int32_t,  int64_t, 4)
_(vpmovsxdq , int32_t,  int64_t, 8)
_(vpmovsxbq_,  int8_t, uint64_t, 2)
_(vpmovsxbq_,  int8_t, uint64_t, 4)
_(vpmovsxbq_,  int8_t, uint64_t, 8)
_(vpmovsxwq_, int16_t, uint64_t, 2)
_(vpmovsxwq_, int16_t, uint64_t, 4)
_(vpmovsxwq_, int16_t, uint64_t, 8)
_(vpmovsxdq_, int32_t, uint64_t, 2)
_(vpmovsxdq_, int32_t, uint64_t, 4)
_(vpmovsxdq_, int32_t, uint64_t, 8)

// integral -> double
_(vcvtdq2pd ,  int32_t, double, 2)
_(vcvtdq2pd ,  int32_t, double, 4)
_(vcvtdq2pd ,  int32_t, double, 8)
_(vcvtudq2pd, uint32_t, double, 2)
_(vcvtudq2pd, uint32_t, double, 4)
_(vcvtudq2pd, uint32_t, double, 8)
_(vcvtqq2pd ,  int64_t, double, 2)
_(vcvtqq2pd ,  int64_t, double, 4)
_(vcvtqq2pd ,  int64_t, double, 8)
_(vcvtuqq2pd, uint64_t, double, 2)
_(vcvtuqq2pd, uint64_t, double, 4)
_(vcvtuqq2pd, uint64_t, double, 8)

// integral -> float
_(vcvtdq2ps ,  int32_t, float,  4)
_(vcvtdq2ps ,  int32_t, float,  8)
_(vcvtdq2ps ,  int32_t, float, 16)
_(vcvtudq2ps, uint32_t, float,  4)
_(vcvtudq2ps, uint32_t, float,  8)
_(vcvtudq2ps, uint32_t, float, 16)
_(vcvtqq2ps ,  int64_t, float,  4)
_(vcvtqq2ps ,  int64_t, float,  8)
_(vcvtqq2ps ,  int64_t, float, 16)
_(vcvtuqq2ps, uint64_t, float,  4)
_(vcvtuqq2ps, uint64_t, float,  8)
_(vcvtuqq2ps, uint64_t, float, 16)

// float <-> double
_( cvttpd2ps, double, float,  2)
_(vcvttpd2ps, double, float,  4)
_(vcvttpd2ps, double, float,  8)
_( cvttps2pd, float, double,  2)
_(vcvttps2pd, float, double,  4)
_(vcvttps2pd, float, double,  8)

// float -> integral
_( cvttps2dq, float, int32_t,  4)
_(vcvttps2dq, float, int32_t,  8)
_(vcvttps2dq, float, int32_t, 16)
_( cvttps2qq, float, int64_t,  4)
_(vcvttps2qq, float, int64_t,  8)
_(vcvttps2qq, float, int64_t, 16)

_( cvttps2udq, float, uint32_t,  4)
_(vcvttps2udq, float, uint32_t,  8)
_(vcvttps2udq, float, uint32_t, 16)
_( cvttps2uqq, float, uint64_t,  4)
_(vcvttps2uqq, float, uint64_t,  8)
_(vcvttps2uqq, float, uint64_t, 16)

// double -> integral
_( cvttpd2dq, double, int32_t, 2)
_(vcvttpd2dq, double, int32_t, 4)
_(vcvttpd2dq, double, int32_t, 8)
_(vcvttpd2qq, double, int64_t, 2)
_(vcvttpd2qq, double, int64_t, 4)
_(vcvttpd2qq, double, int64_t, 8)

_(vcvttpd2udq, double, uint32_t, 2)
_(vcvttpd2udq, double, uint32_t, 4)
_(vcvttpd2udq, double, uint32_t, 8)
_(vcvttpd2uqq, double, uint64_t, 2)
_(vcvttpd2uqq, double, uint64_t, 4)
_(vcvttpd2uqq, double, uint64_t, 8)

// no change in type; nop
_(nop,   int8_t,   int8_t, 16)
_(nop,  uint8_t,  uint8_t, 16)
_(nop,   int8_t,   int8_t, 32)
_(nop,  uint8_t,  uint8_t, 32)
_(nop,   int8_t,   int8_t, 64)
_(nop,  uint8_t,  uint8_t, 64)
_(nop,  int16_t,  int16_t,  8)
_(nop, uint16_t, uint16_t,  8)
_(nop,  int16_t,  int16_t, 16)
_(nop, uint16_t, uint16_t, 16)
_(nop,  int16_t,  int16_t, 32)
_(nop, uint16_t, uint16_t, 32)
_(nop,  int32_t,  int32_t,  4)
_(nop, uint32_t, uint32_t,  4)
_(nop,  int32_t,  int32_t,  8)
_(nop, uint32_t, uint32_t,  8)
_(nop,  int32_t,  int32_t, 16)
_(nop, uint32_t, uint32_t, 16)
_(nop,  int64_t,  int64_t,  2)
_(nop, uint64_t, uint64_t,  2)
_(nop,  int64_t,  int64_t,  4)
_(nop, uint64_t, uint64_t,  4)
_(nop,  int64_t,  int64_t,  8)
_(nop, uint64_t, uint64_t,  8)
_(nop,   double,   double,  2)
_(nop,   double,   double,  4)
_(nop,   double,   double,  8)
_(nop,    float,    float,  4)
_(nop,    float,    float,  8)
_(nop,    float,    float, 16)


---


### compiler : `gcc`
### title : `GCC fails to vectorize code unless dummy loop is added`
### open_at : `2018-03-23T21:08:04Z`
### last_modified_date : `2021-07-22T20:54:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85057
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.2.0`
### severity : `normal`
### contents :
I have a class which represents short vectors (1D, 2D, 3D) and does numeric computations using the expression template engine PETE[1]. The attached example is stripped down to support only 1D vectors, which is the simplest case but still demonstrates the issue. In my application, vector computations are executed in a loop which is subject to vectorization, as in:

 int const N = 100000;
 Vector<1, double> a[N];
 // initialize a 
 for (int i=0; i<N; i++)
   a[i] = 0.5*a[i];

The PETE machinery causes each loop iteration to evaluate an expression in a function evaluate(), which (for 1D vectors) looks like this:

 template <int N, typename T, typename Op, typename RHS>
 inline void evaluate(Vector<N,T> &lhs, Op const &op, Expression<RHS> const &rhs)
 {
     op(lhs(0), forEach(rhs, EvalVectorLeaf<N>(0), OpCombine()));
 }

The issue is that GCC is not able to vectorize above loop, i.e., the assembly code of the loop body is "vmulsd  xmm0, xmm1, QWORD PTR [rax]". However, and now comes the crux, GCC can vectorize the loop ("vmulpd  ymm0, ymm1, YMMWORD PTR [rax]") if I add a seemingly meaningless dummy loop to the funtion body, as in:

 template <int N, typename T, typename Op, typename RHS>
 inline void evaluate(Vector<N,T> &lhs, Op const &op, Expression<RHS> const &rhs)
 {
   for (int i=0; i<1; i++)
     op(lhs(i), forEach(rhs, EvalVectorLeaf<N>(i), OpCombine()));
 }

Attached is the code which does not vectorize. A vectorizing version can easily be constructed by adding the loop as shown above.


g++ command line: g++ -O3 -mavx
System type: x86_64-pc-linux-gnu


[1]: The official website of PETE seems to be gone, but a mirror can be found here: https://github.com/erdc/daetk/tree/master/pete/pete-2.1.0


---


### compiler : `gcc`
### title : `std::min_element does not optimize well with inlined predicate`
### open_at : `2018-03-29T01:17:48Z`
### last_modified_date : `2021-09-05T04:05:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85116
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.3.0`
### severity : `enhancement`
### contents :
According to godbolt (https://godbolt.org/g/igzsnL), the following code:

#define SIZE 1000
std::array<double, SIZE> testArray;

int getMinIdxCPPStyle(double offset)
{
    auto minElement = std::min_element(std::cbegin(testArray), std::cend(testArray), [offset](auto a, auto b) { return std::abs(a - offset) < std::abs(b - offset); });
    return std::distance(std::cbegin(testArray), minElement );
}

generates as the following under -O3

getMinIdxCPPStyle(double):
  movq xmm3, QWORD PTR .LC1[rip]
  mov eax, OFFSET FLAT:testArray
  mov edx, OFFSET FLAT:testArray+8
.L11:
  movsd xmm1, QWORD PTR [rdx]
  movsd xmm2, QWORD PTR [rax]
  subsd xmm1, xmm0
  subsd xmm2, xmm0
  andpd xmm1, xmm3
  andpd xmm2, xmm3
  ucomisd xmm2, xmm1
  cmova rax, rdx
  add rdx, 8
  cmp rdx, OFFSET FLAT:testArray+8000
  jne .L11
  sub rax, OFFSET FLAT:testArray
  sar rax, 3
  ret

The problem being that the typical c-style loop beats this easily due to caching the minimum value and not fetching it and recomputing it. Is there a reason that the generated code should not cache the minimum value in a register instead of probably causing a cache miss by fetching it and then unnecessarily running the computations on it again?


---


### compiler : `gcc`
### title : `Loop limit prevents (auto)vectorization`
### open_at : `2018-03-31T16:06:09Z`
### last_modified_date : `2021-08-07T23:30:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85143
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `7.3.0`
### severity : `normal`
### contents :
I expected that it generates a vectorized version potentially specializing to the boundary. LLVM produces strange looking (vectorized) code so I guess it's a but this time :)

Works fine if the hardcoded boundary is removed.

void boxIntersectionSimdNative(
    bool*__restrict__ res,
    double*__restrict__ a, double*__restrict__ b,
    int n
) {
    for( int i = 0; i < n && i < 1337; i++) {
        res[i] = a[i] > b[i];
        
    }
}


output

boxIntersectionSimdNative(bool*, double*, double*, int):
  test ecx, ecx
  jle .L34
  mov eax, 1
  jmp .L30
.L35:
  cmp r8d, 1336
  jg .L34
.L30:
  vmovsd xmm0, QWORD PTR [rsi-8+rax*8]
  mov r8d, eax
  vcomisd xmm0, QWORD PTR [rdx-8+rax*8]
  seta BYTE PTR [rdi-1+rax]
  add rax, 1
  cmp ecx, r8d
  jg .L35
.L34:
  rep ret


---


### compiler : `gcc`
### title : `GCC generates mvn/and instructions instead of bic on aarch64`
### open_at : `2018-04-02T20:05:50Z`
### last_modified_date : `2023-06-02T04:53:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85160
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `8.0`
### severity : `normal`
### contents :
With this test case:

int foo(int a, int b, int *c, int i, int j)
{
	int x,y;
	x = ((a & (~c[i])) >> 7) |
	     ((a & (~c[j])) >> 9);
	y = ((b & (~c[i])) >> 9) |
	     ((b & (~c[j])) >> 7);
	return x | y;
}

GCC -O2 generates 2 'mvn' instructions and 4 'and' instructions.
LLVM -O2 generates 4 'bic' instructions instead.

GCC:

foo:
	ldr	w3, [x2, w3, sxtw 2]
	ldr	w2, [x2, w4, sxtw 2]
	mvn	w3, w3
	mvn	w2, w2
	and	w4, w3, w1
	and	w1, w2, w1
	and	w3, w3, w0
	and	w2, w2, w0
	asr	w4, w4, 9
	asr	w1, w1, 7
	orr	w3, w4, w3, asr 7
	orr	w2, w1, w2, asr 9
	orr	w0, w3, w2
	ret

LLVM:

foo:
	ldr	w8, [x2, w3, sxtw #2]
	ldr	w9, [x2, w4, sxtw #2]
	bic	w10, w0, w8
	bic	w8, w1, w8
	asr	w8, w8, #9
	bic	w11, w0, w9
	orr	w8, w8, w10, asr #7
	bic	w9, w1, w9
	orr	w8, w8, w11, asr #9
	orr	w0, w8, w9, asr #7
	ret


I am not sure if this should be considered target specific or not, the 'bic'
instruction is aarch64 specific but GCC knows how to use it.  I think combine
didn't try to replace the mvn instructions because it is used by two subsequent
instructions and that may be a generic combine issue.


---


### compiler : `gcc`
### title : `jump threading can rotate loops affecting loop form, and causing vectorization not to happen`
### open_at : `2018-04-04T04:12:44Z`
### last_modified_date : `2021-12-13T13:57:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85186
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `minor`
### contents :
Created attachment 43833
original non-vectorizing case: 1

In a build for x86_64-linux (likely also other targets; this originates elsewhere) at r258635, drop in attached four test-cases into the dg/vect subdir.

Observe, when running vect.exp:
Running x/gcc/testsuite/gcc.dg/vect/vect.exp ...
FAIL: gcc.dg/vect/prXXXXX-1.c scan-tree-dump-times vect "vectorized 1 loops" 1
FAIL: gcc.dg/vect/prXXXXX-1.c -flto -ffat-lto-objects  scan-tree-dump-times vect "vectorized 1 loops" 1

I.e. prXXXXX-1.c does not vectorize, while -2..4 does.

Partial analysis: On inspection using gdb and dumps, this appears to be because the initialization appears to affect ifcvt (just as a catalyst) causing it to sort-of peel the first iteration of the to-be-vectorized loop, or something to that effect, anyway changing the form of the code such that it is not be recognizable for the loop vectorizer pass that comes next.

1: original (derived from some other gcc test long ago)
2: candidate loop with non-conditional contents: vectorizes.
3: arrays in candidate loop are moved global (i.e. aren't initialized right before the loop): vectorizes.
4: asm("" : : : "memory") inserted after initialization: vectorizes.

All-in-all this PR isn't important (with the test-cases being artificial), but when revisiting if-conversion and/or investigates ifcvt vs. loop-vectorization issues, this PR may provide simplified test-cases of an underlying bug.


---


### compiler : `gcc`
### title : `Performance issue with PHP on ppc64 systems`
### open_at : `2018-04-05T00:13:00Z`
### last_modified_date : `2022-03-08T16:20:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85216
### status : `REOPENED`
### tags : `missed-optimization`
### component : `target`
### version : `7.3.0`
### severity : `normal`
### contents :
On ppc64[el] systems, GCC emits suboptimal code for the PHP VM (version 7.2.3) that results in significant performance loss versus a standard x86 machine.  This centers around the HYBRID_BREAK() function converting into two slow instructions on ppc64el versus one fast instruction on x86.

x86 generated assembly example:

.L33600:
# php7.2-7.2.3/Zend/zend_vm_execute.h:59809:                          ZEND_VERIFY_ABSTRACT_CLASS_SPEC_HANDLER(ZEND_OPCODE_HANDLER_ARGS_PASSTHRU);
        .loc 2 59809 0
        call    ZEND_VERIFY_ABSTRACT_CLASS_SPEC_HANDLER #
        jmp     *(%r15) # opline.199_67->handler

ppc64el generated assembly example:

.L35825:
 # php7.2-7.2.3/Zend/zend_vm_execute.h:59809:                          ZEND_VERIFY_ABSTRACT_CLASS_SPEC_HANDLER(ZEND_OPCODE_HANDLER_ARGS_PASSTHRU);
        .loc 2 59809 0
        bl ZEND_VERIFY_ABSTRACT_CLASS_SPEC_HANDLER       #
 # php7.2-7.2.3/Zend/zend_vm_execute.h:59810:                          HYBRID_BREAK();
        .loc 2 59810 0
        ld 9,0(29)       # opline.200_67->handler, gotovar.1505_2678
        mtctr 9  # gotovar.1505_2678, gotovar.1505_2678
        bctr

Note the additional assembly instructions emitted for HYBRID_BREAK(); perf indicates these are consuming considerable amounts of time on ppc64el while the equivalent jmp on x86 consumes almost no time at all.

I'm not sure if this is a POWER9 quirk (cache problem) or what the correct assembler should be, just that there is a serious performance loss with the current emitted assembler versus the x86 equivalent.

Thanks!


---


### compiler : `gcc`
### title : `x86_64 missed optimisation opportunity for (-1 * !!x)`
### open_at : `2018-04-05T08:49:53Z`
### last_modified_date : `2023-09-21T11:16:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85224
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `enhancement`
### contents :
Input:

int f(int x) {
  return -1 * !!x;
}

Trunk with -O3 gives:

f(int):
  xor eax, eax
  test edi, edi
  setne al
  neg eax
  ret

While clang -O3 gives:

f: # @f
  neg edi
  sbb eax, eax
  ret

gcc -O1 gives a different sequence which is slightly longer:

f(int):
  test edi, edi
  setne al
  movzx eax, al
  neg eax
  ret


---


### compiler : `gcc`
### title : `[og7, openacc, nvptx] Too much shared memory claimed for long vector length`
### open_at : `2018-04-05T13:33:36Z`
### last_modified_date : `2019-01-15T09:22:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85231
### status : `UNCONFIRMED`
### tags : `missed-optimization, openacc`
### component : `target`
### version : `8.0`
### severity : `enhancement`
### contents :
On the og7 branch, with the current state of the vector-length patches applied, for vector-length-128-1.c we generate:
...
.shared .align 8 .u8 __oacc_bcast[64];

   ...

cvta.shared.u64 %r55,__oacc_bcast;
st.u64 [%r55],%r36;
st.u64 [%r55+8],%r37;
st.u64 [%r55+16],%r38;
st.u64 [%r55+24],%r39;

   ...

cvta.shared.u64 %r54,__oacc_bcast;
ld.u64 %r36,[%r54];
ld.u64 %r37,[%r54+8];
ld.u64 %r38,[%r54+16];
ld.u64 %r39,[%r54+24];
...

It seems we claim double (64) of what we need (32).

This patch (already applicable on og7 branch) fixes that:
...
diff --git a/gcc/config/nvptx/nvptx.c b/gcc/config/nvptx/nvptx.c
index ba8d3bec1d7..3cf110cd1ed 100644
--- a/gcc/config/nvptx/nvptx.c
+++ b/gcc/config/nvptx/nvptx.c
@@ -4058,7 +4058,8 @@ nvptx_shared_propagate (bool pre_p, bool is_call, basic_block block,
       emit_insn_after (init, insn);
 
       unsigned int psize = ROUND_UP (data.offset, oacc_bcast_align);
-      unsigned int pnum = (nvptx_mach_vector_length () > PTX_WARP_SIZE
+      unsigned int pnum = ((nvptx_mach_vector_length () > PTX_WARP_SIZE
+                           && nvptx_mach_max_workers () > 1)
                           ? nvptx_mach_max_workers () + 1
                           : 1);
...


---


### compiler : `gcc`
### title : `missed optimisation opportunity for (x >> CST)!=0 is not optimized to   (((unsigned)x) >=  (1<<CST)`
### open_at : `2018-04-05T14:46:19Z`
### last_modified_date : `2023-09-21T14:02:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85234
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `enhancement`
### contents :
Input:

int x;

int f()
{
    return (~x >> 3) ? 1030355390 : 1367354703;
}

With -O3, trunk outputs:

f():
  movl x(%rip), %eax
  notl %eax
  sarl $3, %eax
  cmpl $1, %eax
  sbbl %eax, %eax
  andl $336999313, %eax
  addl $1030355390, %eax
  ret

Clang (also at -O3), however, outputs:

f(): # @f()
  cmpl $-8, x(%rip)
  movl $1030355390, %ecx # imm = 0x3D69F9BE
  movl $1367354703, %eax # imm = 0x51802D4F
  cmovbl %ecx, %eax
  retq


---


### compiler : `gcc`
### title : `missed optimisation opportunity for large/negative shifts`
### open_at : `2018-04-05T17:21:41Z`
### last_modified_date : `2021-08-08T23:22:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85237
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.0.1`
### severity : `enhancement`
### contents :
Input:

int f(int x)
{
    return 100 >> (10000 * (x == 1));
}

With -O3 I get:

f(int):
  cmpl $1, %edi
  movl $100, %edx
  movl $0, %eax
  cmovne %edx, %eax
  ret

However, (x == 1) must always be 0, since the shift would be too large (and cause UB) otherwise. Clang is able to see this and always outputs:

f(int): # @f(int)
  movl $100, %eax
  retq

I believe a similar example would be simply:

int f(int x)
{
    return 100 >> (INT_MAX * x);
}

where again, the only valid (non-UB) value for x is 0.


---


### compiler : `gcc`
### title : `copyheader peels off almost the entire iteration`
### open_at : `2018-04-07T08:05:54Z`
### last_modified_date : `2018-12-19T11:10:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85275
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
I expected predcom to eliminate one of the loads in this loop at -O3:

int is_sorted(int *a, int n)
{
  for (int i = 0; i < n - 1; i++)
    if (a[i] > a[i + 1])
      return 0;
  return 1;
}

Unfortunately, predcom bails out since the loads it sees are not always-executed. Ideally loop header copying would make this a suitable do-while loop, but in this case it duplicates too much:


;; Loop 1
;;  header 5, latch 4
;;  depth 1, outer 0
;;  nodes: 5 4 3
;; 2 succs { 5 }
;; 3 succs { 6 4 }
;; 4 succs { 5 }
;; 5 succs { 3 6 }
;; 6 succs { 1 }
Analyzing loop 1
Loop 1 is not do-while loop: latch is not empty.
    Will duplicate bb 5
    Will duplicate bb 3
  Not duplicating bb 4: it is single succ.
Duplicating header of the loop 1 up to edge 3->4, 12 insns.
[...]
  <bb 2> [local count: 114863532]:
  _17 = n_12(D) + -1;
  if (_17 > 0)
    goto <bb 3>; [94.50%]
  else
    goto <bb 6>; [5.50%]

  <bb 3> [local count: 108546038]:
  _18 = 0;
  _19 = _18 * 4;
  _20 = a_13(D) + _19;
  _21 = *_20;
  _22 = _18 + 1;
  _23 = _22 * 4;
  _24 = a_13(D) + _23;
  _25 = *_24;
  if (_21 > _25)
    goto <bb 6>; [5.50%]
  else
    goto <bb 5>; [94.50%]

  <bb 4> [local count: 906139986]:
  _1 = (long unsigned int) i_15;
  _2 = _1 * 4;
  _3 = a_13(D) + _2;
  _4 = *_3;
  _5 = _1 + 1;
  _6 = _5 * 4;
  _7 = a_13(D) + _6;
  _8 = *_7;
  if (_4 > _8)
    goto <bb 6>; [5.50%]
  else
    goto <bb 5>; [94.50%]

  <bb 5> [local count: 958878293]:
  # i_26 = PHI <0(3), i_15(4)>
  i_15 = i_26 + 1;
  _9 = n_12(D) + -1;
  if (_9 > i_15)
    goto <bb 4>; [94.50%]
  else
    goto <bb 6>; [5.50%]



(throttling it down with --param max-loop-header-insns=5 gives the expected optimization)


---


### compiler : `gcc`
### title : `Generates 20 lines of assembly while only one assembly instruction is enough.`
### open_at : `2018-04-08T07:56:45Z`
### last_modified_date : `2021-11-28T06:57:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85283
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
GCC version: trunk/20180407 (also older versions)
Target: x86_64-linux-gnu
Compile options: -Ofast -mavx2 -mfma -Wall -Wextra -Wpedantic

Build options: --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --disable-bootstrap --enable-multiarch --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --enable-clocale=gnu --enable-languages=c,c++,fortran --enable-ld=yes --enable-gold=yes --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-linker-build-id --enable-lto --enable-plugins --enable-threads=posix --with-pkgversion=GCC-Explorer-Build 

The exact code (no #include s):
typedef struct {
  float x, y;
} Vec2;

Vec2 vec2_add(Vec2 a, Vec2 b) {
  Vec2 out = {a.x + b.x, 
              a.y + b.y};
  return out;
}

Produced assembly with line numbers:

1 vec2_add:
2  vmovq rcx, xmm0
3  vmovq rsi, xmm1
...
21 vmovq xmm0, QWORD PTR [rsp-24]
22 ret

Expected assembly (as compiled by Clang 6.0 with -Ofast -mavx2 -mfma):

1 vec2_add: # @vec2_add
2   vaddps xmm0, xmm1, xmm0
3   ret

(Yes, only three lines)

^^^^^^

(These can be experimented here: https://godbolt.org/g/tTwusV)

See also (for other inefficiencies): https://godbolt.org/g/AtWNgf


---


### compiler : `gcc`
### title : `bitfield check causes maybe-uninitialized warning`
### open_at : `2018-04-09T12:17:04Z`
### last_modified_date : `2022-11-28T22:36:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85301
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
A Linux kernel patch that changed a few flags from type 'int' to a single-bit bitfield caused a false-positive warning. I reduced a test case to

struct tick_sched {
#ifdef USE_BITFIELD
  unsigned int tick_stopped : 1;
  unsigned int idle_active : 1;
#else
  int tick_stopped;
  int idle_active;
#endif
};
long ktime_get();
void __tick_nohz_idle_restart_tick(long);
struct tick_sched tick_nohz_idle_exit_ts;
void tick_nohz_idle_exit(void) {
  long now;
  if (tick_nohz_idle_exit_ts.idle_active || tick_nohz_idle_exit_ts.tick_stopped)
    now = ktime_get();
  if (tick_nohz_idle_exit_ts.tick_stopped)
    __tick_nohz_idle_restart_tick(now);
}

$ gcc  -c tick-sched.c -Wall -O2 -DUSE_BITFIELD
tick-sched.c: In function ‘tick_nohz_idle_exit’:
tick-sched.c:19:5: warning: ‘now’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     __tick_nohz_idle_restart_tick(now);
$ gcc  -c tick-sched.c -Wall -O2
# no warning

It's easy to work around the warning, e.g. by copying the flag into a temporary variable, but it looks like this is something that gcc could handle better.

I looked through the list of false-positive Wmaybe-uninitialized bug reports, but couldn't find one that looks related to this particular one.


---


### compiler : `gcc`
### title : `missed range optimisation opportunity for derefences where index must be 0 or otherwise constrained`
### open_at : `2018-04-10T07:58:34Z`
### last_modified_date : `2020-11-26T07:26:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85315
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0.1`
### severity : `normal`
### contents :
Input:

extern int x;
extern int a;
extern int b;

int f()
{
    int y = x;
    return *(&y + (a + b));
}

With -O3, trunk outputs:

f():
  movl x(%rip), %eax
  movl %eax, -4(%rsp)
  movl b(%rip), %eax
  addl a(%rip), %eax
  cltq
  movl -4(%rsp,%rax,4), %eax
  ret

Clang, on the other hand, infers that (a + b) == 0:

f(): # @f()
  movl x(%rip), %eax
  retq

From richi on IRC:

"""
we don't do this kind of optimization at the moment
a related one would be to place a if (a+b != 0) link_error (); after the memory access
similarly for array accesses an if (i not-in-range) link_error ()
thus, we do not derive ranges for address-computation parts [at dereference sites]
"""


---


### compiler : `gcc`
### title : `[meta-bug] VRP range propagation missed cases`
### open_at : `2018-04-10T09:37:05Z`
### last_modified_date : `2023-08-10T02:38:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85316
### status : `NEW`
### tags : `meta-bug, missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `enhancement`
### contents :
bug tracking all missed value range propagation issues (as opposed to PR18373 tracking the need for such a pass)


---


### compiler : `gcc`
### title : `SSE/AVX/AVX512 shift by 0 not optimized away`
### open_at : `2018-04-10T13:04:59Z`
### last_modified_date : `2020-01-24T21:49:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85323
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.0.1`
### severity : `normal`
### contents :
In the following test case, all three functions should compile to just `ret`:

#include <x86intrin.h>

__m128i f(__m128i x) {
    x = _mm_sll_epi64(x, __m128i());
    x = _mm_sll_epi32(x, __m128i());
    x = _mm_sll_epi16(x, __m128i());
    x = _mm_srl_epi64(x, __m128i());
    x = _mm_srl_epi32(x, __m128i());
    x = _mm_srl_epi16(x, __m128i());
    x = _mm_sra_epi64(x, __m128i());
    x = _mm_sra_epi32(x, __m128i());
    x = _mm_sra_epi16(x, __m128i());
    x = _mm_slli_epi64(x, 0);
    x = _mm_slli_epi32(x, 0);
    x = _mm_slli_epi16(x, 0);
    x = _mm_srli_epi64(x, 0);
    x = _mm_srli_epi32(x, 0);
    x = _mm_srli_epi16(x, 0);
    x = _mm_srai_epi64(x, 0);
    x = _mm_srai_epi32(x, 0);
    x = _mm_srai_epi16(x, 0);
    return x;
}

__m256i f(__m256i x) {
    x = _mm256_sll_epi64(x, __m128i());
    x = _mm256_sll_epi32(x, __m128i());
    x = _mm256_sll_epi16(x, __m128i());
    x = _mm256_srl_epi64(x, __m128i());
    x = _mm256_srl_epi32(x, __m128i());
    x = _mm256_srl_epi16(x, __m128i());
    x = _mm256_sra_epi64(x, __m128i());
    x = _mm256_sra_epi32(x, __m128i());
    x = _mm256_sra_epi16(x, __m128i());
    x = _mm256_slli_epi64(x, 0);
    x = _mm256_slli_epi32(x, 0);
    x = _mm256_slli_epi16(x, 0);
    x = _mm256_srli_epi64(x, 0);
    x = _mm256_srli_epi32(x, 0);
    x = _mm256_srli_epi16(x, 0);
    x = _mm256_srai_epi64(x, 0);
    x = _mm256_srai_epi32(x, 0);
    x = _mm256_srai_epi16(x, 0);
    return x;
}

__m512i f(__m512i x) {
    x = _mm512_sll_epi64(x, __m128i());
    x = _mm512_sll_epi32(x, __m128i());
    x = _mm512_sll_epi16(x, __m128i());
    x = _mm512_srl_epi64(x, __m128i());
    x = _mm512_srl_epi32(x, __m128i());
    x = _mm512_srl_epi16(x, __m128i());
    x = _mm512_sra_epi64(x, __m128i());
    x = _mm512_sra_epi32(x, __m128i());
    x = _mm512_sra_epi16(x, __m128i());
    x = _mm512_slli_epi64(x, 0);
    x = _mm512_slli_epi32(x, 0);
    x = _mm512_slli_epi16(x, 0);
    x = _mm512_srli_epi64(x, 0);
    x = _mm512_srli_epi32(x, 0);
    x = _mm512_srli_epi16(x, 0);
    x = _mm512_srai_epi64(x, 0);
    x = _mm512_srai_epi32(x, 0);
    x = _mm512_srai_epi16(x, 0);
    return x;
}


---
