### Total Bugs Detected: 4649
### Current Chunk: 13 of 30
### Bugs in this Chunk: 160 (From bug 1921 to 2080)
---


### compiler : `gcc`
### title : `GCC fails to consider end of automatic object lifetime when determining sibcall eligibility`
### open_at : `2018-10-18T00:10:13Z`
### last_modified_date : `2019-01-25T15:49:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87639
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Test case:

void bar();
void bah(void *);
void foo2()
{
    {
        char buf[1000];
        bah(buf);
    }
    bar();
}

Since buf's address leaked to bah, gcc concludes that it cannot generate a sibcall to bar. However, at the end of the block in which its declaration is contained, buf's lifetime has ended.

The above test case is mildly artificial, but the same thing happens with inlining, and results in long-lived large stack frames (and possibly stack overflow if tail-recursion was desired), when a function comparable to the block containing buf gets inlined into the function that should end with a sibcall. This imposes the need for manual barriers against inlining or unnatual splitting of functions when the stack usage is a problem.


---


### compiler : `gcc`
### title : `suboptimal codegen for testing low bit`
### open_at : `2018-10-19T00:28:49Z`
### last_modified_date : `2023-06-13T06:13:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87650
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
int pow(int x, unsigned int n)
{
    int y = 1;
    while (n > 1)
    {
        auto m = n%2;
        n = n/2;
        if (m)
            y *= x;
        x = x*x;
    }
    return x*y;
}

produces
mov edx, esi
and edx, 1
test edx, edx

instead of just
test sil, 1

while clang chooses a branchless version:
https://godbolt.org/z/L6VUZ1

Interestingly gcc does use test sil,1 if you get rid of m:
godbolt.org/z/9oL1oc

Assembly analysis:
https://stackoverflow.com/a/52877279/594456


---


### compiler : `gcc`
### title : `Information about constants from condition is not propagated`
### open_at : `2018-10-19T09:51:47Z`
### last_modified_date : `2021-12-23T08:00:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87654
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
Consider the following example:

unsigned align_func1(bool big, unsigned value) {
    const unsigned mx_ = (big ? 1 << 14 : 1 << 12);
    return value / mx_ * mx_;
}

It is equivalent to the following code:

unsigned align_func2(bool big, unsigned value) {
    if (big) {
        const unsigned mx_ = 1 << 14;
        return value / mx_ * mx_;
    }

    const unsigned mx_ = 1 << 12;
    return value / mx_ * mx_;
}

Assembly for the align_func2 seems optimal:
  mov eax, esi
  and esi, -4096
  and eax, -16384
  test dil, dil
  cmove eax, esi
  ret

While the assembly for the first function is far from optimal:
align_func1(bool, unsigned int):
  cmp dil, 1
  mov eax, esi
  sbb ecx, ecx
  xor edx, edx
  and ecx, -12288
  add ecx, 16384
  div ecx         ; <=== too bad
  imul eax, ecx
  ret


---


### compiler : `gcc`
### title : ``i = i % constant` for static local `i` is not optimized`
### open_at : `2018-10-19T10:18:26Z`
### last_modified_date : `2021-12-31T16:48:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87655
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
Consider the following example:

unsigned next_trivial() {
    static int i = 0;

    auto ret = i;
    ++i;
    i = i % 10;
    return ret;
}

For that example a very suboptimal assembly with multiplication, many registers usage and multiple instructions is generated.

However, the above example could be rewritten in the following way:
unsigned next_trivial_optim() {
    static int i = 0;

    auto ret = i;
    ++i;
    if (i == 10) { i = 0; }
    return ret;
}

For the above code snippet a very short and clear assembly is produced, without any multiplications and unnecessary instructions:
  mov eax, DWORD PTR next_trivial_optim()::i[rip]
  mov ecx, 0
  lea edx, [rax+1]
  cmp eax, 9
  cmove edx, ecx
  mov DWORD PTR next_trivial_optim()::i[rip], edx
  ret

Please, add an optimization to do that transformation.


---


### compiler : `gcc`
### title : `invariant in loop after optimization`
### open_at : `2018-10-20T11:13:53Z`
### last_modified_date : `2021-08-14T08:06:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87664
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Compile the following code with the current trunk or gcc 8.2.1:

#include <array>
#include <numeric>

int rs(int s) {
  std::array<int,100> ar;
  std::iota(ar.begin(), ar.end(), s);
  return std::accumulate(ar.begin(), ar.end(), 0);
}


With -O2 this leads on x86-64 to the following code:


0000000000000000 <rs(int)>:
   0:	48 81 ec 20 01 00 00 	sub    $0x120,%rsp
   7:	48 8d 44 24 88       	lea    -0x78(%rsp),%rax
   c:	0f 1f 40 00          	nopl   0x0(%rax)
  10:	89 38                	mov    %edi,(%rax)
  12:	48 8d 8c 24 18 01 00 	lea    0x118(%rsp),%rcx
  19:	00 
  1a:	48 83 c0 04          	add    $0x4,%rax
  1e:	ff c7                	inc    %edi
  20:	48 39 c8             	cmp    %rcx,%rax
  23:	75 eb                	jne    10 <rs(int)+0x10>
  25:	48 8d 54 24 88       	lea    -0x78(%rsp),%rdx
  2a:	31 c0                	xor    %eax,%eax
  2c:	0f 1f 40 00          	nopl   0x0(%rax)
  30:	03 02                	add    (%rdx),%eax
  32:	48 8d b4 24 18 01 00 	lea    0x118(%rsp),%rsi
  39:	00 
  3a:	48 83 c2 04          	add    $0x4,%rdx
  3e:	48 39 f2             	cmp    %rsi,%rdx
  41:	75 ed                	jne    30 <rs(int)+0x30>
  43:	48 81 c4 20 01 00 00 	add    $0x120,%rsp
  4a:	c3                   	retq   


The relevant parts are the loop starting at offsets 10 and 30.  The respective lea instructions to compute the end address of the loop at offset 12 and 32 are invariant and should be hoisted out of the loops.


---


### compiler : `gcc`
### title : `Small program produces 160 meg .o file`
### open_at : `2018-10-22T01:43:46Z`
### last_modified_date : `2022-01-07T00:46:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87680
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.2.0`
### severity : `normal`
### contents :
This small program takes over a minute to compile and produces a 160 meg object file. sizeof(Bar) is large and it looks like the compiler wants to write down a precompiled version of it to the object file for the benefit of Bar's constructor. There are no static variables or anything like that so the object file really should be modest in size.

% cat test.cc
constexpr int size = 10000000;

class Foo {
  int a = 10;
  int b = 11;
  int c = 12;
  int d = 13;
};

class Bar {
  Foo foo[size];
};

void somefunc(const Bar *);

int main() {
  Bar *bar = new Bar();
  somefunc(bar);
}


% time g++ -O2 -c test.cc

real    1m9.696s
user    1m6.486s
sys     0m2.806s


% ls -l test.o
-rw-rw-r-- 1 kosak kosak 160001760 Oct 22 01:41 test.o


---


### compiler : `gcc`
### title : `Reuse guard variable for multiple initializations`
### open_at : `2018-10-22T17:29:08Z`
### last_modified_date : `2021-08-30T02:22:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87692
### status : `NEW`
### tags : `ABI, missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
Consider the following example:

int produce1();
int produce2(int ) noexcept;


auto test() {
    static int val1 = produce1();
    static int val2 = produce2(val1); // noexcept

    return val2 + val2;
}

For the above example two guard variables are generated: for `val1` and `val2`. However the `val2` initialization always succeeds and may not happen before `val1` initialization.

So instead of having two variables, `val2` could reuse the guard variable for `val1`.

Using one variable has advantages: less code is generated and no need in multiple guard variables checks.

Such optimization will reduce the code size and improve overall performance.


---


### compiler : `gcc`
### title : `[9 Regression] ira-shrinkwrap-prep-[12].c testcases fail after r265398`
### open_at : `2018-10-23T14:45:59Z`
### last_modified_date : `2018-11-16T19:04:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87708
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
These fail on many targets, and simply because the
split_live_ranges_for_shrink_wrap code can only move direct copies from the
hard argument registers, but they now usually are move to a pseudo first.

Ideally this code will improve, but in the meantime, maybe we should xfail
the test?


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] FAIL: gcc.target/i386/pr57193.c scan-assembler-times movdqa 2`
### open_at : `2018-10-24T00:09:46Z`
### last_modified_date : `2023-07-07T10:34:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87716
### status : `NEW`
### tags : `missed-optimization, ra, testsuite-fail, xfail`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
On x86, r265398 caused:

FAIL: gcc.target/i386/pr57193.c scan-assembler-times movdqa 2

	movdqa	(%rdi), %xmm2
	pavgb	(%rsi), %xmm2
	movdqa	%xmm0, %xmm3 <<<?
	movdqa	%xmm2, %xmm0 <<<?
	punpckhbw	%xmm1, %xmm2
	punpcklbw	%xmm1, %xmm0


---


### compiler : `gcc`
### title : `[9 Regression] FAIL: gcc.target/i386/avx512vl-concatv2si-1.c scan-assembler vpunpckldq[^\n\r]*%xmm17[^\n\r]*%xmm16[^\n\r]*%xmm3`
### open_at : `2018-10-24T00:28:12Z`
### last_modified_date : `2018-11-12T08:38:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87717
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
On x86-64, r265398 caused:

FAIL: gcc.target/i386/avx512vl-concatv2si-1.c scan-assembler vpunpckldq[^\n\r]*%xmm17[^\n\r]*%xmm16[^\n\r]*%xmm3

Before:

f1:
.LFB0:
	.cfi_startproc
	movl	%edi, -4(%rsp)
	vmovd	-4(%rsp), %xmm16
	movl	%esi, -4(%rsp)
	vmovd	-4(%rsp), %xmm17
	vpunpckldq	%xmm17, %xmm16, %xmm3
	ret
	.cfi_endproc
.LFE0:
	.size	f1, .-f1

After:

f1:
.LFB0:
	.cfi_startproc
	movl	%edi, -4(%rsp)
	vmovd	-4(%rsp), %xmm16
	movl	%esi, -4(%rsp)
	vmovd	-4(%rsp), %xmm17
	vmovd	%xmm17, %eax
	vmovdqa32	%zmm16, %zmm0
	vmovd	%xmm16, -4(%rsp)
	vpinsrd	$1, %eax, %xmm0, %xmm3
	ret
	.cfi_endproc
.LFE0:
	.size	f1, .-f1


---


### compiler : `gcc`
### title : `[9 Regression] FAIL: gcc.target/i386/avx512dq-concatv2si-1.c`
### open_at : `2018-10-24T00:32:05Z`
### last_modified_date : `2021-09-27T07:05:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87718
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
On x86, r265398 caused:

FAIL: gcc.target/i386/avx512dq-concatv2si-1.c scan-assembler-times vpinsrd[^\n\r]*\\$1[^\n\r]*%xmm16[^\n\r]*%xmm3 2
FAIL: gcc.target/i386/avx512dq-concatv2si-1.c scan-assembler vpunpckldq[^\n\r]*%xmm17[^\n\r]*%xmm16[^\n\r]*%xmm3

Before:

	.file	"avx512dq-concatv2si-1.c"
	.text
	.p2align 4
	.globl	f1
	.type	f1, @function
f1:
.LFB0:
	.cfi_startproc
	movl	%edi, -4(%rsp)
	vmovd	-4(%rsp), %xmm16
	movl	%esi, -4(%rsp)
	vmovd	-4(%rsp), %xmm17
	vpunpckldq	%xmm17, %xmm16, %xmm3
	ret
	.cfi_endproc
.LFE0:
	.size	f1, .-f1

After:

	.file	"avx512dq-concatv2si-1.c"
	.text
	.p2align 4
	.globl	f1
	.type	f1, @function
f1:
.LFB0:
	.cfi_startproc
	movl	%edi, -4(%rsp)
	vmovd	-4(%rsp), %xmm16
	movl	%esi, -4(%rsp)
	vmovd	-4(%rsp), %xmm17
	vmovd	%xmm17, %eax
	vpinsrd	$1, %eax, %xmm16, %xmm3
	ret
	.cfi_endproc
.LFE0:
	.size	f1, .-f1


---


### compiler : `gcc`
### title : `[9 regression] gcc.target/sparc/overflow-2.c FAILs`
### open_at : `2018-10-24T12:33:19Z`
### last_modified_date : `2018-12-21T16:14:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87727
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Between 20181022 (r265393) and 20181023 (r265430), gcc.target/sparc/overflow-2.c
started to FAIL:

+FAIL: gcc.target/sparc/overflow-2.c scan-assembler-not save\\t%

I'm attaching old and current assembler output.


---


### compiler : `gcc`
### title : `inline asm not optimized on GIMPLE`
### open_at : `2018-10-24T12:34:30Z`
### last_modified_date : `2021-09-13T21:33:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87728
### status : `NEW`
### tags : `alias, inline-asm, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
PR 63900 points out a case where RTL CSE fails to clean up redundant loads in presence of BLKmode accesses, but there really isn't anything in that testcase that GCC shouldn't be able to clean up in GIMPLE. It seems GIMPLE optimizations are too conservative with regards to asm statements.

For the simple testcase

int f()
{
    int a=0, b;
    asm("# %0 %1" : "=m"(b) : "m"(a));
    return a;
}

the asm is dead and this should become 'return 0;', but in .optimized dump we still have

f ()
{
  int b;
  int a;
  int _4;

  <bb 2> :
  a = 0;
  __asm__("# %0 %1" : "=m" b : "m" a);
  _4 = a;
  a ={v} {CLOBBER};
  b ={v} {CLOBBER};
  return _4;
}


---


### compiler : `gcc`
### title : `Vectorizer doesn't support conversion of different sizes`
### open_at : `2018-10-25T05:51:04Z`
### last_modified_date : `2021-07-21T14:59:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87743
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-efi-2 prpr87317]$ cat x.c 
#define MAX 4

long long int dst[MAX];
int src[MAX];

void
foo (void)
{
  int i;
  for (i = 0; i < MAX; i++)
    dst[i] = src[i];
}
[hjl@gnu-efi-2 prpr87317]$ gcc -S  -O3 -march=haswell x.c
[hjl@gnu-efi-2 prpr87317]$ cat x.s
	.file	"x.c"
	.text
	.p2align 4,,15
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movslq	src(%rip), %rax
	movslq	src+8(%rip), %rcx
	movslq	src+12(%rip), %rdx
	vmovq	%rax, %xmm0
	movslq	src+4(%rip), %rax
	vmovq	%rcx, %xmm1
	vpinsrq	$1, %rdx, %xmm1, %xmm1
	vpinsrq	$1, %rax, %xmm0, %xmm0
	vinserti128	$0x1, %xmm1, %ymm0, %ymm0
	vmovdqu	%ymm0, dst(%rip)
	vzeroupper
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.comm	src,16,16
	.comm	dst,32,32
	.ident	"GCC: (GNU) 8.2.1 20181011 (Red Hat 8.2.1-4)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-efi-2 prpr87317]$


---


### compiler : `gcc`
### title : `[9 regression][MIPS] New FAIL: gcc.target/mips/fix-r4000-10.c   -O1 start with r265398`
### open_at : `2018-10-26T06:30:46Z`
### last_modified_date : `2019-03-27T16:18:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87761
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
paulhua@gcc122:~/debug_r265398$ cat fix-r4000-10.i
# 1 "/home/xuchenghua/GCC/gcc_git_trunk/gcc/testsuite/gcc.target/mips/fix-r4000-10.c"
# 1 "<built-in>"
# 1 "<command-line>"
# 1 "/usr/include/stdc-predef.h" 1 3 4
# 1 "<command-line>" 2
# 1 "/home/xuchenghua/GCC/gcc_git_trunk/gcc/testsuite/gcc.target/mips/fix-r4000-10.c"





typedef unsigned long long uint64_t;
typedef unsigned int uint128_t __attribute__((mode(TI)));
__attribute__((nomips16)) uint128_t foo (uint64_t x, uint64_t y) { return (uint128_t) x * y; }


paulhua@gcc122:~/debug_r265398$ ~/build/gcc_397_obj/gcc/cc1 -fpreprocessed fix-r4000-10.i -mel -quiet -dumpbase fix-r4000-10.c -dp -mips3 -mgp64 -mno-micromips -mno-mips3d -mno-dsp -mno-dspr2 -mfix-r4000 -mno-paired-single -mno-synci -mnan=legacy -mabi=64 -mllsc -mno-shared -auxbase-strip fix-r4000-10.s -O1 -version -fdiagnostics-color=never -fno-diagnostics-show-caret -fno-diagnostics-show-line-numbers -fno-split-wide-types -ffat-lto-objects -fno-ident -o fix-r4000-10.s -mno-abicalls

good:
paulhua@gcc122:~/debug_r265398$ cat fix-r4000-10.s 
        .file   1 "fix-r4000-10.c"
        .section .mdebug.abi64
        .previous
        .nan    legacy
        .gnu_attribute 4, 1
        .text
        .align  2
        .globl  foo
        .set    nomips16
        .ent    foo
        .type   foo, @function
foo:
        .frame  $sp,0,$31               # vars= 0, regs= 0/0, args= 0, gp= 0
        .mask   0x00000000,0
        .fmask  0x00000000,0
        dmultu  $4,$5    # 17   [c=48 l=12]  umulditi3_r4000
        mflo    $2
        mfhi    $3
        jr      $31      # 39   [c=0 l=8]  *simple_return
        .end    foo
        .size   foo, .-foo

after r265398 bad asm:
paulhua@gcc122:~/debug_r265398$ cat fix-r4000-10.s 

        .file   1 "fix-r4000-10.c"
        .section .mdebug.abi64
        .previous
        .nan    legacy
        .text
        .align  2
        .globl  foo
        .set    nomips16
        .ent    foo
        .type   foo, @function
foo:
        .frame  $sp,0,$31               # vars= 0, regs= 0/0, args= 0, gp= 0
        .mask   0x00000000,0
        .fmask  0x00000000,0
        move    $6,$5    # 34   [c=4 l=4]  *movdi_64bit/0
        move    $2,$4    # 10   [c=4 l=4]  *movdi_64bit/0
        dmultu  $2,$6    # 17   [c=48 l=12]  umulditi3_r4000
        mflo    $2
        mfhi    $3
        jr      $31      # 43   [c=0 l=8]  *simple_return
        .end    foo
        .size   foo, .-foo


---


### compiler : `gcc`
### title : `Missing AVX512 memory broadcast for constant vector`
### open_at : `2018-10-26T20:34:40Z`
### last_modified_date : `2021-09-18T05:03:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87767
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-efi-2 broadcast-4]$ cat c.c
#include <immintrin.h>

__m512
foo (__m512 x)
{
  return _mm512_add_ps (x, _mm512_set1_ps (2.0f));
}
[hjl@gnu-efi-2 broadcast-4]$ make c.s
/export/build/gnu/tools-build/gcc-test/build-x86_64-linux/gcc/xgcc -B/export/build/gnu/tools-build/gcc-test/build-x86_64-linux/gcc/ -O2 -mavx512f -S c.c
[hjl@gnu-efi-2 broadcast-4]$ cat c.s
	.file	"c.c"
	.text
	.p2align 4
	.globl	foo
	.type	foo, @function
foo:
.LFB5186:
	.cfi_startproc
	vbroadcastss	.LC0(%rip), %zmm1
	vaddps	%zmm1, %zmm0, %zmm0
	ret
	.cfi_endproc
.LFE5186:
	.size	foo, .-foo
	.section	.rodata.cst16,"aM",@progbits,16
	.align 16
.LC0:
	.long	1073741824
	.long	0
	.long	0
	.long	0
	.ident	"GCC: (GNU) 9.0.0 20181026 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-efi-2 broadcast-4]$ 

vbroadcastss should be replaced by memory broadcast.


---


### compiler : `gcc`
### title : `Unnecessary zero-initialization of constexpr unions in arrays`
### open_at : `2018-10-29T11:08:48Z`
### last_modified_date : `2018-10-30T08:05:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87791
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.2.1`
### severity : `normal`
### contents :
Created attachment 44918
Preprocessed code

Given a type that uses a constexpr-enabled union to store either a value, or an empty dummy struct as followed.


>struct Container
>{
>    struct Dummy{};
>    union Storage
>    {   
>        constexpr Storage(): dummy(Dummy())
>        {   
>
>        }   
>        Dummy dummy;
>        int value;
>    };  
>
>    Storage storage;
>};

This code is a legal way to initialise the union with nothing, meaning that it's legal for the compiler to leave the data uninitialised, even in constexpr contexts.

However, if the above Container type is used like so:

>using Arr = std::array<Container, 4000>;
>
>Arr makeA()
>{
>    Arr a;
>    return a;
>}

Then the compiled code fills the array with zeroes, using memset.

Excerpt from compiled code:

>_Z5makeAv:
>.LFB1099:
>    .cfi_startproc
>    subq    $8, %rsp
>    .cfi_def_cfa_offset 16
>    movl    $16000, %edx
>    xorl    %esi, %esi
>    call    memset@PLT   <----unnecessary runtime overhead
>    addq    $8, %rsp
>    .cfi_def_cfa_offset 8
>    ret 
>    .cfi_endproc
>.LFE1099:
>    .size   _Z5makeAv, .-_Z5makeAv
>    .section    .text.startup,"ax",@progbits
>    .p2align 4,,15
>    .globl  main
>    .type   main, @function

This is unnecessary and problematic as it incurs runtime overhead when using the union-based storage as a way to store items that can possibly be uninitialised, as a constexpr-friendly alternative to placement-new.

Happens even with -O3 and only happens if the `constexpr` keyword is present.

Preprocessed code attached.

Godbolt CE link for illustrative purpose: https://godbolt.org/z/FPnEy3




Output of gcc -v -save-temps -O3 main.cpp:

Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: /build/gcc/src/gcc/configure --prefix=/usr --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --enable-languages=c,c++,ada,fortran,go,lto,objc,obj-c++ --enable-shared --enable-threads=posix --enable-libmpx --with-system-zlib --with-isl --enable-__cxa_atexit --disable-libunwind-exceptions --enable-clocale=gnu --disable-libstdcxx-pch --disable-libssp --enable-gnu-unique-object --enable-linker-build-id --enable-lto --enable-plugin --enable-install-libiberty --with-linker-hash-style=gnu --enable-gnu-indirect-function --enable-multilib --disable-werror --enable-checking=release --enable-default-pie --enable-default-ssp --enable-cet=auto
Thread model: posix
gcc version 8.2.1 20180831 (GCC) 
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/cc1plus -E -quiet -v -D_GNU_SOURCE main.cpp -mtune=generic -march=x86-64 -O3 -fpch-preprocess -o main.ii
ignoring nonexistent directory "/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../x86_64-pc-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../include/c++/8.2.1
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../include/c++/8.2.1/x86_64-pc-linux-gnu
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../include/c++/8.2.1/backward
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/include
 /usr/local/include
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/include-fixed
 /usr/include
End of search list.
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/cc1plus -fpreprocessed main.ii -quiet -dumpbase main.cpp -mtune=generic -march=x86-64 -auxbase main -O3 -version -o main.s
GNU C++14 (GCC) version 8.2.1 20180831 (x86_64-pc-linux-gnu)
	compiled by GNU C version 8.2.1 20180831, GMP version 6.1.2, MPFR version 4.0.1, MPC version 1.1.0, isl version isl-0.19-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
GNU C++14 (GCC) version 8.2.1 20180831 (x86_64-pc-linux-gnu)
	compiled by GNU C version 8.2.1 20180831, GMP version 6.1.2, MPFR version 4.0.1, MPC version 1.1.0, isl version isl-0.19-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: a03a3250bc7a24a525a6f08bf70a1ad6
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-mtune=generic' '-march=x86-64'
 as -v --64 -o main.o main.s
GNU assembler version 2.31.1 (x86_64-pc-linux-gnu) using BFD version (GNU Binutils) 2.31.1
COMPILER_PATH=/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/:/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/
LIBRARY_PATH=/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/:/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../lib/:/lib/../lib/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/collect2 -plugin /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/liblto_plugin.so -plugin-opt=/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/lto-wrapper -plugin-opt=-fresolution=main.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s --build-id --eh-frame-hdr --hash-style=gnu -m elf_x86_64 -dynamic-linker /lib64/ld-linux-x86-64.so.2 -pie /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../lib/Scrt1.o /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../lib/crti.o /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/crtbeginS.o -L/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1 -L/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../lib -L/lib/../lib -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../.. main.o -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/crtendS.o /usr/lib/gcc/x86_64-pc-linux-gnu/8.2.1/../../../../lib/crtn.o
COLLECT_GCC_OPTIONS='-v' '-save-temps' '-O3' '-mtune=generic' '-march=x86-64'


---


### compiler : `gcc`
### title : `Guard variable is not eliminated when there's nothing to guard`
### open_at : `2018-10-31T14:13:24Z`
### last_modified_date : `2021-08-30T02:23:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87831
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
Consider the example:

struct base {
    base() {}
};

base& test() {
    static base val;
    return val;
}

For that example a lot of assembly is generated, including empty initialization under the guard:

.L14:
     ; nothing to initialize
  mov edi, OFFSET FLAT:guard variable for test()::val
  call __cxa_guard_release
  mov eax, OFFSET FLAT:_ZZ4testvE3val
  add rsp, 8
  ret

Consider removing all the guard variable instructions if there's no instructions for initialization.


---


### compiler : `gcc`
### title : `cselib_hasher::hash function does not match with cselib_hasher::equal operator`
### open_at : `2018-11-01T11:52:43Z`
### last_modified_date : `2019-06-07T11:08:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87845
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
As mentioned in the following sub-thread: https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01878.html
we have situations where ::equal returns true for a pair of value, which their ::hash value is different.

Let me demonstrate that on the following example:

$ cat toreduce.i
int a, c, d, e, f, i, j, k, l;
typedef struct {
  char b[8];
} g;
g h;

int o(int, int, int);
int p();
int q();

void m() {
  int n, b = o(a, d, e);
  if (b)
    goto ac;
  if (a)
    p();
  if (c)
    if (d)
      if (l == 0 && n == 0)
        ;
      else {
        h.b[7] = n >> 24;
        h.b[6] = n >> 16;
        h.b[5] = n >> 8;
        h.b[4] = n;
      }
  return;
ac:
  q(k, f, 1, l, &n, i, j);
}

If I apply following patch:
diff --git a/gcc/cselib.c b/gcc/cselib.c
index 6d3a4078c68..821bee6aa86 100644
--- a/gcc/cselib.c
+++ b/gcc/cselib.c
@@ -101,7 +101,7 @@ struct cselib_hasher : nofree_ptr_hash <cselib_val>
 inline hashval_t
 cselib_hasher::hash (const cselib_val *v)
 {
-  return v->hash;
+  return 0;
 }
 
 /* The equality test for our hash table.  The first argument V is a table

I see a divergence in:

$ diff -u /tmp/before/toreduce.i.279r.postreload /tmp/after/toreduce.i.279r.postreload
--- /tmp/before/toreduce.i.279r.postreload	2018-11-01 12:48:01.046221868 +0100
+++ /tmp/after/toreduce.i.279r.postreload	2018-11-01 12:48:27.778779308 +0100
@@ -1,7 +1,6 @@
 
 ;; Function m (m, funcdef_no=0, decl_uid=1924, cgraph_uid=1, symbol_order=10)
 
-rescanning insn with uid = 46.
 starting the processing of deferred insns
 ending the processing of deferred insns
 
@@ -16,7 +15,7 @@
...@@ -181,7 +180,8 @@
 (insn 46 67 47 8 (parallel [
             (set (reg:CCZ 17 flags)
                 (compare:CCZ (ior:SI (reg:SI 0 ax [orig:101 l ] [101])
-                        (reg:SI 1 dx))
+                        (mem/c:SI (plus:DI (reg/f:DI 7 sp)
+                                (const_int 12 [0xc])) [1 n+0 S4 A32]))
                     (const_int 0 [0])))
             (clobber (reg:SI 0 ax [orig:101 l ] [101]))
         ]) "/home/marxin/Programming/tramp3d/toreduce.i":19:10 451 {*iorsi_3}

which eventually leads to following assembly divergence:

diff -u 1 2
--- 1	2018-11-01 12:52:07.839338756 +0100
+++ 2	2018-11-01 12:52:07.855339094 +0100
@@ -24,9 +24,9 @@
 	movl	d(%rip), %eax
 	testl	%eax, %eax
 	je	.L1
-	movl	l(%rip), %eax
-	orl	12(%rsp), %eax
 	movl	12(%rsp), %edx
+	movl	l(%rip), %eax
+	orl	%edx, %eax
 	je	.L1
 	movl	%edx, h+4(%rip)
 .L1:

Can please anybody familiar with cselib help me here?
Thanks


---


### compiler : `gcc`
### title : `Missed optimization: useless for-loop must be eliminated`
### open_at : `2018-11-01T12:59:49Z`
### last_modified_date : `2021-08-10T04:39:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87849
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
gcc(trunk) with -O3 -std=c++17 for this code:

#include <vector>
#include <algorithm>

int foo(std::vector<int> v) {
    int l = v[0];
    for(const auto& x : v) {
        l = std::min(l, x);
    }

    for(const auto& x : v) {
        l = std::max(l, x);
    }

    return l;
}

gcc doesn't eliminate first loop, but gcc can, because first loop has no effect in this function.


---


### compiler : `gcc`
### title : `Unrolled loop leads to excessive code bloat with -Os on ARC EM.`
### open_at : `2018-11-02T18:32:33Z`
### last_modified_date : `2018-11-07T08:23:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87869
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `8.2.0`
### severity : `normal`
### contents :
Consider the following code:

  % cat >test.c <<'EOF'
  #include <stdint.h>
  
  void do_stuff_12iter(void)
  {
     volatile uint32_t *blah = (void *)0xf0000000;
     unsigned i;
  
     for (i = 0; i < 12; i++) {
        blah[i] = 3;
     }
  }
  
  void do_stuff_11iter(void)
  {
     volatile uint32_t *blah = (void *)0xf0000000;
     unsigned i;
  
     for (i = 0; i < 11; i++) {
        blah[i] = 3;
     }
  }
EOF

When I compile this with gcc:

  % arc-unknown-elf-gcc -v
  Using built-in specs.
  COLLECT_GCC=/usr/x86_64-pc-linux-gnu/arc-unknown-elf/gcc-bin/8.2.0/arc-unknown-elf-gcc
  COLLECT_LTO_WRAPPER=/usr/libexec/gcc/arc-unknown-elf/8.2.0/lto-wrapper
  Target: arc-unknown-elf
  Configured with: /var/tmp/portage/cross-arc-unknown-elf/gcc-8.2.0-r3/work/gcc-8.2.0/configure --host=x86_64-pc-linux-gnu --target=arc-unknown-elf --build=x86_64-pc-linux-gnu --prefix=/usr --bindir=/usr/x86_64-pc-linux-gnu/arc-unknown-elf/gcc-bin/8.2.0 --includedir=/usr/lib/gcc/arc-unknown-elf/8.2.0/include --datadir=/usr/share/gcc-data/arc-unknown-elf/8.2.0 --mandir=/usr/share/gcc-data/arc-unknown-elf/8.2.0/man --infodir=/usr/share/gcc-data/arc-unknown-elf/8.2.0/info --with-gxx-include-dir=/usr/lib/gcc/arc-unknown-elf/8.2.0/include/g++-v8 --with-python-dir=/share/gcc-data/arc-unknown-elf/8.2.0/python --enable-languages=c,c++ --enable-obsolete --enable-secureplt --disable-werror --with-system-zlib --enable-nls --without-included-gettext --enable-checking=release --with-bugurl=https://bugs.gentoo.org/ --with-pkgversion='Gentoo 8.2.0-r3' --disable-esp --enable-libstdcxx-time --enable-poison-system-directories --disable-libstdcxx-time --with-sysroot=/usr/arc-unknown-elf --disable-bootstrap --with-newlib --enable-multilib --disable-altivec --disable-fixed-point --disable-libgomp --disable-libmudflap --disable-libssp --disable-libmpx --disable-systemtap --disable-vtable-verify --disable-libvtv --disable-libquadmath --enable-lto --without-isl --disable-libsanitizer --disable-default-pie --enable-default-ssp
  Thread model: single
  gcc version 8.2.0 (Gentoo 8.2.0-r3) 

  % arc-unknown-elf-gcc -c -Os -mcpu=arcem -mno-sdata -mcode-density -mq-class -mbarrel-shifter -mmpy-option=3 -mswap test.c

The 11-iteration loop gets fully unrolled with pretty horrible results:

00000018 <do_stuff_11iter>:
  18:	730c                	mov_s	r0,3
  1a:	1e00 7000 f000 0000 	st	r0,[0xf0000000]
  22:	1e00 7000 f000 0004 	st	r0,[0xf0000004]
  2a:	1e00 7000 f000 0008 	st	r0,[0xf0000008]
  32:	1e00 7000 f000 000c 	st	r0,[0xf000000c]
  3a:	1e00 7000 f000 0010 	st	r0,[0xf0000010]
  42:	1e00 7000 f000 0014 	st	r0,[0xf0000014]
  4a:	1e00 7000 f000 0018 	st	r0,[0xf0000018]
  52:	1e00 7000 f000 001c 	st	r0,[0xf000001c]
  5a:	1e00 7000 f000 0020 	st	r0,[0xf0000020]
  62:	1e00 7000 f000 0024 	st	r0,[0xf0000024]
  6a:	1e00 7000 f000 0028 	st	r0,[0xf0000028]
  72:	7ee0                	j_s	[blink]

That's almost five times the size of the 12-iteration one which didn't
get unrolled:

00000000 <do_stuff_12iter>:
   0:	41c3 f000 0000      	mov_s	r1,0xf0000000
   6:	734c                	mov_s	r2,3
   8:	d80c                	mov_s	r0,0xc
   a:	240a 7000           	mov	lp_count,r0
   e:	20a8 0140           	lp	10	;16 <do_stuff_12iter+0x16>
  12:	1904 0090           	st.ab	r2,[r1,4]
  16:	7ee0                	j_s	[blink]

That one's pretty good.  This specific example could be a _tiny_
bit better, because the constant values moved to r2 and r0 could be
immediates in the instructions where those registers are used but
I'm not bothered by that.

Since I requested size optimizations, it would be nice if my code
size didn't get quintupled like this.


---


### compiler : `gcc`
### title : `ppc64le generates poor code when loading constants into TImode vars`
### open_at : `2018-11-02T20:31:49Z`
### last_modified_date : `2018-12-17T22:10:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87870
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
bergner@pike:~/gcc/BUGS/PR87507$ cat bug.i 
__int128
foo (void)
{
  return 1;
}
bergner@pike:~/gcc/BUGS/PR87507$ /home/bergner/gcc/build/gcc-fsf-mainline-pr87507-base-debug/gcc/xgcc -B/home/bergner/gcc/build/gcc-fsf-mainline-pr87507-base-debug/gcc -O2 -S bug.i 
bergner@pike:~/gcc/BUGS/PR87507$ cat bug.s 
foo:
.LCF0:
0:	addis 2,12,.TOC.-.LCF0@ha
	addi 2,2,.TOC.-.LCF0@l
	addis 9,2,.LC0@toc@ha
	addi 9,9,.LC0@toc@l
	ld 3,0(9)
	ld 4,8(9)
	blr

If you compile with -mno-altivec, then we get the two li insn code you'd expect.  The problem is that vsx_mov<mode>_64 doesn't have support for loading constants into gprs, whereas movti_ppc64 (which is used when not targeting a cpu with a vector unit) does.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] testcases fail after r265398 on arm`
### open_at : `2018-11-02T20:50:37Z`
### last_modified_date : `2023-07-07T10:34:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87871
### status : `WAITING`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The following tests fail on arm after r265398 (combine: Do not combine moves from hard registers).

    gcc.c-torture/execute/920428-2.c   -O2  execution test
    gcc.c-torture/execute/920428-2.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/920428-2.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.c-torture/execute/920428-2.c   -O3 -g  execution test
    gcc.c-torture/execute/920501-7.c   -O2  execution test
    gcc.c-torture/execute/920501-7.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/920501-7.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.c-torture/execute/920501-7.c   -O3 -g  execution test
    gcc.c-torture/execute/built-in-setjmp.c   -O2  execution test
    gcc.c-torture/execute/built-in-setjmp.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/built-in-setjmp.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.c-torture/execute/built-in-setjmp.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.c-torture/execute/built-in-setjmp.c   -O3 -g  execution test
gcc.c-torture/execute/builtins/memcpy-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/memcpy-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/memcpy-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/memcpy-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/memmove-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/memmove-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/memmove-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/memmove-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/mempcpy-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/mempcpy-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/mempcpy-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/mempcpy-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/mempcpy-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/memset-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/memset-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/memset-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/memset-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/memset-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/snprintf-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/snprintf-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/snprintf-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/snprintf-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/snprintf-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/sprintf-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/sprintf-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/sprintf-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/sprintf-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/sprintf-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/stpcpy-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/stpcpy-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/stpcpy-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/stpcpy-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/stpcpy-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/stpncpy-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/stpncpy-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/stpncpy-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/stpncpy-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/strcat-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/strcat-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/strcat-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/strcat-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/strcat-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/strcpy-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/strcpy-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/strcpy-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/strcpy-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/strcpy-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/strncat-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/strncat-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/strncat-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/strncat-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/strncat-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/strncpy-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/strncpy-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/strncpy-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/vsnprintf-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/vsnprintf-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/vsnprintf-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/vsnprintf-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/vsnprintf-chk.c execution,  -O3 -g 
    gcc.c-torture/execute/builtins/vsprintf-chk.c execution,  -O2 
    gcc.c-torture/execute/builtins/vsprintf-chk.c execution,  -O2 -flto -fno-use-linker-plugin -flto-partition=none 
    gcc.c-torture/execute/builtins/vsprintf-chk.c execution,  -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects 
    gcc.c-torture/execute/builtins/vsprintf-chk.c execution,  -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions 
    gcc.c-torture/execute/builtins/vsprintf-chk.c execution,  -O3 -g 

    gcc.c-torture/execute/comp-goto-2.c   -O2  execution test
    gcc.c-torture/execute/comp-goto-2.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/comp-goto-2.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.c-torture/execute/comp-goto-2.c   -O3 -g  execution test
    gcc.c-torture/execute/nestfunc-5.c   -O2  execution test
    gcc.c-torture/execute/nestfunc-5.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/nestfunc-5.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.c-torture/execute/nestfunc-5.c   -O3 -g  execution test
    gcc.c-torture/execute/pr24135.c   -O2  execution test
    gcc.c-torture/execute/pr24135.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/pr24135.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.c-torture/execute/pr24135.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.c-torture/execute/pr24135.c   -O3 -g  execution test
    gcc.c-torture/execute/pr51447.c   -O2  execution test
    gcc.c-torture/execute/pr51447.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/pr51447.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.c-torture/execute/pr51447.c   -O3 -g  execution test
    gcc.c-torture/execute/pr60003.c   -O2  execution test
    gcc.c-torture/execute/pr60003.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.c-torture/execute/pr60003.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.c-torture/execute/pr60003.c   -O3 -g  execution test

    gcc.dg/ira-shrinkwrap-prep-1.c scan-rtl-dump ira "Split live-range of register"
    gcc.dg/ira-shrinkwrap-prep-1.c scan-rtl-dump pro_and_epilogue "Performing shrink-wrapping"
    gcc.dg/ira-shrinkwrap-prep-2.c scan-rtl-dump ira "Split live-range of register"
    gcc.dg/non-local-goto-1.c execution test
    gcc.dg/non-local-goto-2.c execution test



    gcc.dg/torture/stackalign/comp-goto-1.c   -O2  execution test
    gcc.dg/torture/stackalign/comp-goto-1.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/comp-goto-1.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/comp-goto-1.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/comp-goto-1.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/comp-goto-1.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/comp-goto-1.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/comp-goto-1.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/nested-5.c   -O2  execution test
    gcc.dg/torture/stackalign/nested-5.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/nested-5.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/nested-5.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/nested-5.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/nested-5.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/nested-5.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/nested-5.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O2  execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/non-local-goto-1.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O2  execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/non-local-goto-2.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O2  execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/non-local-goto-3.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O2  execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/non-local-goto-4.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O2  execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/non-local-goto-5.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O2  execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects  execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O2 -flto -fuse-linker-plugin -fno-fat-lto-objects -fpic execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions -fpic execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/setjmp-1.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O2  execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions -fpic execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/setjmp-3.c   -O3 -g -fpic execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O2  execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none  execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O2 -flto -fno-use-linker-plugin -flto-partition=none -fpic execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O2 -fpic execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions  execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O3 -fomit-frame-pointer -funroll-loops -fpeel-loops -ftracer -finline-functions -fpic execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O3 -g  execution test
    gcc.dg/torture/stackalign/setjmp-4.c   -O3 -g -fpic execution test


    gcc.target/arm/addr-modes-float.c scan-assembler vst3.8\t{d[02468], d[02468], d[02468]}, \\[r[0-9]+\\]!
    gcc.target/arm/armv8_2-fp16-move-1.c scan-assembler-times strh\\tr[0-9]+ 2
    gcc.target/arm/armv8_2-fp16-move-1.c scan-assembler-times vst1\\.16\\t{d[0-9]+\\[[0-9]+\\]}, \\[r[0-9]+\\] 2


---


### compiler : `gcc`
### title : `malloc + memset to calloc doesn't work for aggregate initialization`
### open_at : `2018-11-06T11:35:51Z`
### last_modified_date : `2018-11-08T06:37:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87900
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
struct S { int a[1024]; };
struct S *foo ()
{
  struct S *p = (struct S *)__builtin_malloc (sizeof (struct S));
  *p = (struct S){};
  return p;
}

is not transformed to calloc, it only works when doing

  __builtin_memset (p, 0, sizeof (struct S));

the strlen pass is doing this transform (also DSEing zero-inits after calloc).


---


### compiler : `gcc`
### title : `partial DSE of memset doesn't work for other kind of stores`
### open_at : `2018-11-06T11:44:18Z`
### last_modified_date : `2018-11-08T06:38:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87901
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
When transforming the memsets in tree-ssa/ssa-dse-25.c to = {} and = 0 inits
the DSE pass no longer knows how to trim them.

Similarly for

int i;
void foo ()
{
  i = 0;
  *((short *)&i + sizeof (int) - sizeof (short)) = 1;
}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Shrink-wrapping multiple conditions`
### open_at : `2018-11-06T12:46:41Z`
### last_modified_date : `2023-07-07T10:34:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87902
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
I've noticed that r265830 fails to shrink-wrap multiple early returns in gcc/testsuite/gcc.target/s390/nobp-return-mem-z900.c, while r264877 managed to do so just fine.

After reload we end up with the following code for those conditions:

;; basic block 2
(note 5 1 3 2 [bb 2] NOTE_INSN_BASIC_BLOCK)
(note 3 5 2 2 NOTE_INSN_FUNCTION_BEG)
(insn 2 3 7 2 (set (reg/v:DI 12 %r12 [orig:63 aD.2191+-4 ] [63])
        (reg:DI 2 %r2 [72])) "gcc/testsuite/gcc.target/s390/nobp-return-mem-z900.c":14:1 1269 {*movdi_64}
     (nil))
(insn 7 2 8 2 (set (reg:CCZ 33 %cc)
        (compare:CCZ (reg:SI 12 %r12 [orig:63 aD.2191 ] [63])
            (const_int 42 [0x2a]))) "gcc/testsuite/gcc.target/s390/nobp-return-mem-z900.c":17:6 1232 {*cmpsi_cct}
     (nil))
(jump_insn 8 7 9 2 (set (pc)
        (if_then_else (eq (reg:CCZ 33 %cc)
                (const_int 0 [0]))
            (label_ref:DI 33)
            (pc))) "gcc/testsuite/gcc.target/s390/nobp-return-mem-z900.c":17:6 1896 {*cjump_64}
     (int_list:REG_BR_PROB 225163668 (nil))
 -> 33)

;; basic block 3
(note 9 8 12 3 [bb 3] NOTE_INSN_BASIC_BLOCK)
(insn 12 9 13 3 (set (reg:CCS 33 %cc)
        (compare:CCS (reg:SI 12 %r12 [orig:63 aD.2191 ] [63])
            (const_int 0 [0]))) "gcc/testsuite/gcc.target/s390/nobp-return-mem-z900.c":20:3 1222 {*tstsi_cconly2}
     (nil))
(jump_insn 13 12 14 3 (set (pc)
        (if_then_else (le (reg:CCS 33 %cc)
                (const_int 0 [0]))
            (label_ref:DI 33)
            (pc))) "gcc/testsuite/gcc.target/s390/nobp-return-mem-z900.c":20:3 1896 {*cjump_64}
     (int_list:REG_BR_PROB 118111604 (nil))
 -> 33)

Note that comparisons use a copy in caller-saved %r12, and not %r2.  Then, prepare_shrink_wrap () successfully propagates it to basic block 2. Basic block 3 is not affected - this seems to be by design, since prepare_shrink_wrap () only concerns itself with the first basic block.

In the past reload used to eliminate the copy and use %r2 directly in both comparisons, but this seems to be no longer the case.


---


### compiler : `gcc`
### title : `max(n, 1) code generation`
### open_at : `2018-11-07T02:29:09Z`
### last_modified_date : `2023-05-30T15:44:31Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87913
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
unsigned int f(unsigned int num)
{
    return num < 1 ? 1 : num;
}

int f2(int num)
{
    return num < 1 ? 1 : num;
}

unsigned int g(unsigned int num)
{
    return num + !num;
}

$ gcc -O3

f(unsigned int):
        mov     eax, edi
        test    edi, edi
        mov     edx, 1
        cmove   eax, edx
f2(int):
        test    edi, edi
        mov     eax, 1
        cmovg   eax, edi
g(unsigned int):
        xor     eax, eax
        test    edi, edi
        sete    al
        add     eax, edi

f and g could be:
f:      test    edi, edi
        mov     eax, 1
        cmovne  eax, edi
g:      cmp     edi, 1
        adc     edi, 0
        mov     eax, edi

https://godbolt.org/z/YJWjsQ


---


### compiler : `gcc`
### title : `gcc fails to vectorize bitreverse code`
### open_at : `2018-11-07T04:16:45Z`
### last_modified_date : `2018-11-07T15:01:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87914
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
$ gcc -fopenmp-simd -O3 -march=haswell -fopt-info-vec-omp-optimized-missed

template <typename T>
T reverseBits(T x)
{
	unsigned int s = sizeof(x) * 8;
	T mask = ~T(0);
	while ((s >>= 1) > 0)
	{
		mask ^= (mask << s);
		x = ((x >> s) & mask) | ((x << s) & ~mask); // unsupported use in stmt
	}
    return x;
}

void test_reverseBits(unsigned* x)
{
    #pragma omp simd aligned(x:32)
    for (int i = 0; i < 16; ++i)
        x[i] = reverseBits(x[i]); // couldn't vectorize loop
}

clang and icc vectorize this:
https://godbolt.org/z/ROJZGZ


---


### compiler : `gcc`
### title : `[missed-optimization] Distinct-value if-then-else chains treated differently than switch statements`
### open_at : `2018-11-07T22:23:17Z`
### last_modified_date : `2018-11-10T07:18:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87925
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Have a look at this GodBolt example: https://gcc.godbolt.org/z/zR03rA

On one hand, we have:

    void foo(int i) {
        switch (i) {
            case 1: boo<1>(); break;
            case 2: boo<2>(); break;
            case 3: boo<3>(); break;
            case 4: boo<4>(); break;
            // etc. etc.
        }
    }

on the other hand we have the same, but using an if-then-else chain:

    void bar(int i) {
        if      (i == 1) boo<1>();
        else if (i == 2) boo<2>();
        else if (i == 3) boo<3>();
        else if (i == 4) boo<4>();
        // etc. etc.
    }

The switch statement gets a jump table; the if-then-else chain - does not. At the link, there are 20 cases; g++ starts using a jump table with 4 switch values.

This is not just a matter of programmers needing to remember to prefer switch statements (which it's better not to require of them), but rather that if-then-else chains are sometimes generated by expansion of templated code, e.g. this example for checking for membership in a set of values (= all values of an enum):

    https://stackoverflow.com/a/53191264/1593077

while switch() statements of variable do not get generated AFAICT. It would thus be quite useful if such generated code would not result in highly-inefficient long chains of comparisons.


---


### compiler : `gcc`
### title : `bad array-index warning breaks --disable-checking bootstrap`
### open_at : `2018-11-07T22:38:23Z`
### last_modified_date : `2019-11-21T09:03:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87926
### status : `NEW`
### tags : `build, diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 44967
cc1plus bug.ii -O2 -W -Wall -quiet

x86_64-linux --disable-checking bootstrap fails with:
../../../src/gcc/bitmap.c: In function ‘unsigned int bitmap_last_set_bit(const_bitmap)’:
../../../src/gcc/bitmap.c:1191:26: error: array subscript -1 is below array bounds of ‘const BITMAP_WORD [2]’ {aka ‘const long unsigned int [2]’} [-Werror=array-bounds]
  1191 |       word = elt->bits[ix];
       |              ~~~~~~~~~~~~^

Attached is a reduced testcase

Adding:
bitmap.o-warn = -Wno-error
to gcc/Makefile.in works around the problem.


---


### compiler : `gcc`
### title : `by_pieces infra does not use movmisalign optab`
### open_at : `2018-11-08T14:13:38Z`
### last_modified_date : `2018-11-10T06:02:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87941
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
by_pieces code uses only mov_optab, never checking movmisalign_optab, so on STRICT_ALIGNMENT targets such as arm it does not use available misaligned load/store patterns. It results in suboptimal code for e.g.

void f(char *c)
{
  __builtin_memcpy(c, "foo", 4);
}

where with -O2 -march=armv6t2 gcc emits

f:
        movw    r3, #:lower16:.LANCHOR0
        mov     r2, r0
        movt    r3, #:upper16:.LANCHOR0
        ldr     r0, [r3]
        str     r0, [r2]        @ unaligned
        bx      lr
        .size   f, .-f
        .section        .rodata
        .align  2
        .set    .LANCHOR0,. + 0
.LC0:
        .ascii  "foo\000"

while optimal code is emitted for the equivalent

void f(char *c)
{
  int t;
  __builtin_memcpy(&t, "foo", 4);
  __builtin_memcpy(c,     &t, 4);
}

f:
        movw    r3, #28518
        movt    r3, 111
        str     r3, [r0]        @ unaligned
        bx      lr
        .size   f, .-f


---


### compiler : `gcc`
### title : `GCC warns about reaching end of non-void function when all switch cases are completely handled`
### open_at : `2018-11-09T03:05:57Z`
### last_modified_date : `2022-11-17T02:50:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87950
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `c`
### version : `unknown`
### severity : `normal`
### contents :
Noticed this for a while. If a function has a single switch statement that handles all enum values & returns a value GCC will warn about the function not returning a value whereas clang does not.  GCC requires an explicit __builtin_unreachable() annotation after the switch. Maybe it's an intentional disagreement over how to interpret the spec? AFAICT via objdump the generated assembly is identical between clang & GCC even when compiled with optimizations.

> cat test.c
enum Enum {
  A,
  B,
};

int CoverMyBases(enum Enum x) {
	switch (x) {
		case A:
			return 1;
		case B:
			return 0;
	}
}

int main(int argc, const char **argv) {
	CoverMyBases(A);
	CoverMyBases(B);
	return 0;
}

> gcc-8 -Wall test.c
test.c: In function 'CoverMyBases':
test.c:16:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^

> clang -Wall test.c
> gcc-8 --version
gcc-8 (Homebrew GCC 8.2.0) 8.2.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
> clang --version
Apple LLVM version 10.0.0 (clang-1000.11.45.2)
Target: x86_64-apple-darwin17.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

This applies to both C & C++.


---


### compiler : `gcc`
### title : `Missed optimization for std::get_if on std::variant`
### open_at : `2018-11-09T06:40:30Z`
### last_modified_date : `2021-09-13T00:25:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87952
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `normal`
### contents :
Here is the sample code:

#include <variant>

int test(std::variant<int, bool> &v)
{
    return *std::get_if<int>(&v);
}


gcc generates (on x86-64):

test(std::variant<int, bool>&):
        cmp     BYTE PTR [rdi+4], 0
        jne     .L2
        mov     eax, DWORD PTR [rdi]
        ret
test(std::variant<int, bool>&) [clone .cold]:
.L2:
        mov     eax, DWORD PTR ds:0
        ud2

It seems like the undefined behavior branch can be optimized away leaving (as clang does):

test(std::variant<int, bool>&):               # @test(std::variant<int, bool>&)
        mov     eax, dword ptr [rdi]
        ret


The reason this is useful is that if I know ahead of time (but the compiler can't prove) that the variant holds a certain alternative, calling get_if this way will provide no overhead access to the alternative. (The standard doesn't allow this kind of access for std::get.) Since dereferencing the null pointer (as would happen if the variant didn't hold the alternative) is undefined behavior anyway, we could optimize it away and assume that the alternative is valid.


---


### compiler : `gcc`
### title : `VRP can transform a * b where a,b are [0,1] to a & b`
### open_at : `2018-11-09T09:08:58Z`
### last_modified_date : `2019-06-06T08:01:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87954
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Code snippet reported by kernel developers:

$ cat mul.c
#define __GFP_DMA 1u
#define __GFP_RECLAIM 0x10u

#define KMALLOC_DMA 2
#define KMALLOC_RECLAIM 1

unsigned int and(unsigned int flags)
{
        int is_dma, type_dma, is_rec;

        is_dma = !!(flags & __GFP_DMA);
        type_dma = is_dma * KMALLOC_DMA;
        is_rec = !!(flags & __GFP_RECLAIM);

        return type_dma + (is_rec & !is_dma) * KMALLOC_RECLAIM;
}

unsigned int imul(unsigned int flags)
{
        int is_dma, type_dma, is_rec;

        is_dma = !!(flags & __GFP_DMA);
        type_dma = is_dma * KMALLOC_DMA;
        is_rec = !!(flags & __GFP_RECLAIM);

        return type_dma + (is_rec * !is_dma) * KMALLOC_RECLAIM;
}

The first function ends with fast and operation:
and:
.LFB0:
	.cfi_startproc
	movl	%edi, %edx
	movl	%edi, %eax
	andl	$1, %edi
	shrl	$4, %edx
	notl	%eax
	andl	%edx, %eax
	andl	$1, %eax
	leal	(%rax,%rdi,2), %eax
	ret

while the second one with imull
imul:
.LFB1:
	.cfi_startproc
	movl	%edi, %eax
	movl	%edi, %edx
	andl	$1, %edi
	shrl	$4, %edx
	notl	%eax
	andl	$1, %eax
	andl	$1, %edx
	imull	%edx, %eax
	leal	(%rax,%rdi,2), %eax
	ret
	.cfi_endproc

VRP knows about the range of the operands:
  _7 = _6 * is_rec_12;

_6: [0, 1]
is_rec_12: [0, 1]


---


### compiler : `gcc`
### title : `[i386] Sub-optimal code generation for _mm256_set1_epi64()`
### open_at : `2018-11-12T01:38:11Z`
### last_modified_date : `2021-08-29T20:00:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87976
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
In the following code, Clang and ICC emit a very optimal function that consists of three instructions (including the tail call). MSVC emits a pretty good equivalent with a bit more function overhead, but no memory access

GCC emits a completely unnecessary memory access.

Code:
====
#include <immintrin.h>
#include <stdint.h>

#ifndef _MSC_VER
#define __vectorcall
#endif
void __vectorcall f(__m256i value256);

void g(uint64_t value)
{
    f( _mm256_set1_epi64x(value));
}
====

Clang and ICC (optimal) output:
g:
        vmovd     %rdi, %xmm0
        vpbroadcastq %xmm0, %ymm0
        jmp       f

GCC:
g:
        pushq   %r13
        leaq    16(%rsp), %r13
        andq    $-32, %rsp
        pushq   -8(%r13)
        pushq   %rbp
        movq    %rsp, %rbp
        pushq   %r13
        movq    %rdi, -24(%rbp)
        vpbroadcastq    -24(%rbp), %ymm0
        popq    %r13
        popq    %rbp
        leaq    -16(%r13), %rsp
        popq    %r13
        jmp     f

Godbolt link for all compilers: https://gcc.godbolt.org/z/-gNvec


---


### compiler : `gcc`
### title : `Missed optimization with ranged-for loop on a constexpr array`
### open_at : `2018-11-12T15:26:29Z`
### last_modified_date : `2021-12-17T04:29:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87987
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
This simple program:
```c++
static constexpr bool table[] = { 1, 0, 0, 1, 1, 0, 1, 0 };

int check()
  {
    int sum = 0;
    for(auto value : table) {
      sum += value;
    }
    return sum;
  }
```

after being compiled by GCC 8.2 with `-std=c++11 -O2 -Wall -Wextra -Wpedantic -Werror`, yields a loop:

```asm
check():
        mov     edx, OFFSET FLAT:table
        xor     eax, eax
.L2:
        movzx   ecx, BYTE PTR [rdx]
        add     rdx, 1
        add     eax, ecx
        cmp     rdx, OFFSET FLAT:table+8
        jne     .L2
        ret
```

, while Clang 6.0 optimizes the body to a constant:

```asm
check():                              # @check()
        mov     eax, 4
        ret
```

( Online comparison can be viewed here:  https://gcc.godbolt.org/z/oaSr6j )

Making the function `constexpr` however overcomes this obstacle:
```c++
static constexpr bool table[] = { 1, 0, 0, 1, 1, 0, 1, 0 };

// This requires only C++11.
constexpr int check_constexpr(const bool *p, int n)
  {
    return (n == 0) ? 0 : *p + check_constexpr(p + 1, n - 1);
  }

int check()
  {
    return check_constexpr(table, sizeof(table));
  }
```

( And here is the online comparison for this one: https://gcc.godbolt.org/z/HZjBSh )


---


### compiler : `gcc`
### title : `can't vectorize rgb to grayscale conversion code`
### open_at : `2018-11-14T05:23:49Z`
### last_modified_date : `2021-12-21T16:48:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88013
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `7.2.1`
### severity : `enhancement`
### contents :
#include <stdint.h>

void reference_convert(uint8_t * __restrict dest, uint8_t * __restrict src, int n)
{
  for (int i=0; i<n; i++)
  {
    int r = *src++;
    int g = *src++;
    int b = *src++;
    int y = (r*77)+(g*151)+(b*28);
    *dest++ = y/256;
  }
}

$ arm-gcc -march=armv7-a -O3 -ffast-math -fopt-info-vec-omp-optimized-missed

gives the following notes on the loop line:
unsupported data-type int
can't determine vectorization factor.
vector alignment may not be reachable
Aligned load, but unsupported type.
not vectorized: relevant stmt not supported: _1 = *src_31;
bad operation or unsupported loop bound.
vector alignment may not be reachable
no array mode for DI[3]
extract even/odd not supported by target
bad operation or unsupported loop bound.

Vectorization works for x64.
https://godbolt.org/z/FPG3k_


---


### compiler : `gcc`
### title : `Missing optimization of endian conversion`
### open_at : `2018-11-19T15:58:05Z`
### last_modified_date : `2021-12-15T22:36:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88097
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
I have found some old code network code which looked like this:

[code]
#include <stdint.h>
#include <arpa/inet.h>

struct Test
{
    uint16_t Word1;
    uint16_t Word2;
};

uint32_t test(Test* ip)
{
    return ((ntohs(ip->Word1) << 16) | ntohs(ip->Word2));
}
[/code]

gcc 8.2 compiles it in following way (with -O3):

[asm]
test(Test*):
        movzx   eax, WORD PTR [rdi]
        movzx   edx, WORD PTR [rdi+2]
        rorw $8, ax
        rorw $8, dx
        sal     eax, 16
        movzx   edx, dx
        or      eax, edx
        ret
[/asm]

clang 7.0.0 recognizes that both 16-bit fields are next to each other, so 32-bit byte swap can be used:

[asm]
test(Test*):                          # @test(Test*)
        mov     eax, dword ptr [rdi]
        bswap   eax
        ret
[/asm]

And this is with -mmovbe added:

[asm]
test(Test*):                          # @test(Test*)
        movbe   eax, dword ptr [rdi]
        ret
[/asm]


---


### compiler : `gcc`
### title : `GCC keeps unnecessary calls to new`
### open_at : `2018-11-20T17:51:29Z`
### last_modified_date : `2021-12-10T22:24:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88118
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `enhancement`
### contents :
With the code below clang generates:

main: # @main
  mov eax, 2
  ret

GCC will get to the correct result, but will keep the new calls:

main:
        sub     rsp, 8
        mov     edi, 24
        call    operator new(unsigned long)
        mov     edi, 24
        call    operator new(unsigned long)
        mov     edi, 24
        call    operator new(unsigned long)
        mov     eax, 2
        add     rsp, 8
        ret


https://godbolt.org/z/Q3nV2x

namespace {
struct Node {
    int value{};
    Node* left{};
    Node* right{};
    constexpr Node(int i=0) noexcept : value(i) {};
};

auto constexpr left = &Node::left;
auto constexpr right = &Node::right;

template<typename T, typename... TP>
constexpr Node* traverse(T np, TP... paths) noexcept
{
    return (np ->* ... ->* paths);
}
}
int main()
{
    Node* const root = new Node{0};
    root->left = new Node{1};
    root->left->right = new Node{2};

    Node* const node = traverse(root, left, right);
    return node->value;
}


---


### compiler : `gcc`
### title : `optimize SSE & AVX char compares with subsequent movmskb`
### open_at : `2018-11-22T15:43:40Z`
### last_modified_date : `2019-05-15T14:38:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88152
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Testcase (https://godbolt.org/z/YNPZyf):

#include <x86intrin.h>

template <typename T, size_t N>
using V [[gnu::vector_size(N)]] = T;

// the following should be optimized to:
// vpxor %xmm1, %xmm1, %xmm1
// vpcmpgtb %[xy]mm0, %[xy]mm1, %[xy]mm0
// ret

auto cmp0(V<unsigned char, 16> a) { return a > 0x7f; }
auto cmp0(V<unsigned char, 32> a) { return a > 0x7f; }
auto cmp1(V<unsigned char, 16> a) { return a >= 0x80; }
auto cmp1(V<unsigned char, 32> a) { return a >= 0x80; }
auto cmp0(V<  signed char, 16> a) { return a < 0; }
auto cmp0(V<  signed char, 32> a) { return a < 0; }
auto cmp1(V<  signed char, 16> a) { return a <= -1; }
auto cmp1(V<  signed char, 32> a) { return a <= -1; }
auto cmp0(V<         char, 16> a) { return a < 0; }
auto cmp0(V<         char, 32> a) { return a < 0; }
auto cmp1(V<         char, 16> a) { return a <= -1; }
auto cmp1(V<         char, 32> a) { return a <= -1; }

// the following should be optimized to:
// vpmovmskb %[xy]mm0, %eax
// ret

int f0(V<unsigned char, 16> a) {
  return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a >  0x7f));
}
int f0(V<unsigned char, 32> a) {
  return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a >  0x7f));
}
int f1(V<unsigned char, 16> a) {
  return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a >= 0x80));
}
int f1(V<unsigned char, 32> a) {
  return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a >= 0x80));
}
int f0(V<  signed char, 16> a) {
  return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a <  0));
}
int f0(V<  signed char, 32> a) {
  return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a <  0));
}
int f1(V<  signed char, 16> a) {
  return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a <= -1));
}
int f1(V<  signed char, 32> a) {
  return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a <= -1));
}
int f0(V<         char, 16> a) {
  return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a <  0));
}
int f0(V<         char, 32> a) {
  return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a <  0));
}
int f1(V<         char, 16> a) {
  return _mm_movemask_epi8   (reinterpret_cast<__m128i>(a <= -1));
}
int f1(V<         char, 32> a) {
  return _mm256_movemask_epi8(reinterpret_cast<__m256i>(a <= -1));
}

Compile with `-O2 -mavx2` (the same is relevant with -msse2 if you remove the AVX overloads).

Motivation:
This pattern is relevant for vectorized UTF-8 decoding, where all bytes with MSB == 0 can simply be zero extended to UTF-16/32. Such code could just skip the compare and call movemask directly on `a`. However:
std::experimental::simd doesn't (and any other general purpose SIMD abstraction shouldn't) expose a "munch sign bits into bitmask integer" function. Such a function is too ISA specific.
In the interest of making code readable (and thus maintainable) I strongly believe it should read: `n_ascii_chars = find_first_set(a > 0x7f)` while still getting the optimization.

Similar test cases can be constructed for movmskp[sd] after 32/64 bit integer compares.


---


### compiler : `gcc`
### title : `sqrt() is not vectorized`
### open_at : `2018-11-22T16:55:27Z`
### last_modified_date : `2021-10-01T03:23:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88153
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
Sequence of calls to sqrt() is not vectorized.

I found Bug 21466 that claims that it was fixed in GCC 4.3, but looks that change was reverted - at least 4.4.7 it also is not vectorized. I suspect that after that change errors were not reported correctly - non-vectorized code uses sqrtsd, and for negative numbers it also calls sqrt for its side effects.

I wrote following code snippet as a possible solution for SSE instructions. I did not check all details how errors should be reported for sequence of sqrt calls, so it may need some changes.

#include <emmintrin.h>
#include <math.h>

#define SIZE 8
double d1[SIZE];
double d2[SIZE];

void test()
{
    int m = 0;
    for (int n = 0; n < SIZE; n += 2)
    {
        __m128d v = _mm_load_pd(&d1[n]);
        __m128d vs = _mm_sqrt_pd(v);
        __m128d vn = _mm_cmplt_pd(v, _mm_setzero_pd());
        m |= _mm_movemask_pd(vn);
        _mm_store_pd(&d2[n], vs);
    }

    if (m)
        sqrt(-1.0);
}


---


### compiler : `gcc`
### title : `ix86_expand_sse_movcc and blend for scalars`
### open_at : `2018-11-25T07:45:38Z`
### last_modified_date : `2018-11-28T08:57:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88189
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
double f(double a,double b){
  return (a<0)?a:b;
}
typedef double vec __attribute__((vector_size(16)));
vec g(vec a,vec b){
  return (a<0)?a:b;
}

I am compiling with -O3, and the most interesting pass is ce1 with noce_try_cmove. Using -msse2, both functions generate similar code

	andpd	%xmm2, %xmm0
	andnpd	%xmm1, %xmm2
	orpd	%xmm2, %xmm0

With -mxop they also generate similar code

	vpcmov	%xmm2, %xmm1, %xmm0, %xmm0

However, with -msse4, they differ, the vector version gets

	blendvpd	%xmm0, %xmm2, %xmm1

while the scalar version is stuck with the SSE2 and+andn+or. Is there a particular reason for this inconsistency?


---


### compiler : `gcc`
### title : `[9 regression] gcc.target/i386/pr22076.c etc. FAIL`
### open_at : `2018-11-26T18:04:49Z`
### last_modified_date : `2018-11-29T16:26:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88207
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Between 20181123 (r266403) and 20181124 (r266429), two tests regressed on 32-bit
x86 (seen on i386-pc-solaris2.11, also reports for i586-unknown-freebsd11.2,
i686-pc-linux-gnu, x86_64-pc-linux-gnu:

+FAIL: gcc.target/i386/pr22076.c scan-assembler-not movl

+FAIL: gcc.target/i386/pr81563.c scan-assembler-times movl[\\\\t ]*-4\\\\(%ebp\\\\),[\\\\t ]*%edi 1
+FAIL: gcc.target/i386/pr81563.c scan-assembler-times movl[\\\\t ]*-8\\\\(%ebp\\\\),[\\\\t ]*%esi 1


---


### compiler : `gcc`
### title : `Inefficient array initialization`
### open_at : `2018-11-26T20:38:28Z`
### last_modified_date : `2018-11-28T08:40:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88209
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
The following was tested on avr-gcc, but this behavior should not be different on other platforms.

Consider the following array declaration with initialization of all elements to 0:

int main(void)
{
   char arr[256] = {0};
   return 0;
}

In avr-gcc 8.2.0 and before the following asm code is generated:

  9a:	de 01       	movw	r26, r28
  9c:	11 96       	adiw	r26, 0x01	; 1
  9e:	80 e0       	ldi	r24, 0x00	; 0
  a0:	91 e0       	ldi	r25, 0x01	; 1
  a2:	fd 01       	movw	r30, r26
  a4:	9c 01       	movw	r18, r24
  a6:	11 92       	st	Z+, r1
  a8:	21 50       	subi	r18, 0x01	; 1
  aa:	30 40       	sbci	r19, 0x00	; 0
  ac:	e1 f7       	brne	.-8      	; 0xa6 <main+0x1e>

This behavior is what to expect. 256 Bytes of 0 (register r1) is pushed on the stack.

In avr-gcc 9.0.0 (20181118) this code is generated. Here a 256-bytes data field is generated in section .rodata
I don't fully understand the code because in the last three lines, still 0 (register r1) is pushed on the stack! 

  b0:	80 91 00 01 	lds	r24, 0x0100	; 0x800100 <__data_start>
  b4:	90 91 01 01 	lds	r25, 0x0101	; 0x800101 <__data_start+0x1>
  b8:	9a 83       	std	Y+2, r25	; 0x02
  ba:	89 83       	std	Y+1, r24	; 0x01
  bc:	fe 01       	movw	r30, r28
  be:	33 96       	adiw	r30, 0x03	; 3
  c0:	8e ef       	ldi	r24, 0xFE	; 254
  c2:	df 01       	movw	r26, r30
  c4:	1d 92       	st	X+, r1
  c6:	8a 95       	dec	r24
  c8:	e9 f7       	brne	.-6      	; 0xc4 <main+0x26>

In this case RAM is wated because of the 256 bytes reserved in .rodata
The examples were compiled with -Os. At first I was thinking this new behavior is the result of some runtime optimization but for -Os, the focus should be laid on code size / memory consumption. So I consider this as a bug!


---


### compiler : `gcc`
### title : `IRA Register Coalescing not working for the testcase`
### open_at : `2018-11-27T04:54:47Z`
### last_modified_date : `2023-08-23T15:54:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88212
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
When compiling the following on aarch64 with -O2:
#include <arm_neon.h>
void g(int32_t *p, int32x2x2_t val, int x)
{
     vst2_lane_s32(p,val,0);
}

generates:
	.cfi_startproc
	mov	v2.8b, v0.8b
	mov	v3.8b, v1.8b
	st2	{v2.s - v3.s}[0], [x0]
	ret

clang produces:
	st2	{ v0.s, v1.s }[0], [x0]
	ret

Essentially the problem is that access to part-registers doesn't get
coalesced, so IRA generates moves which aren't actually required.


---


### compiler : `gcc`
### title : `combine fails to merge insns leaving unneeded reg copies`
### open_at : `2018-11-27T22:13:40Z`
### last_modified_date : `2019-09-23T04:55:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88233
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
bergner@pike:~/gcc/BUGS/PR69493$ cat test.i
typedef struct { double a[2]; } A;
A
foo (const A *a)
{
  return *a;
}
bergner@pike:~$ .../xgcc -B.../gcc -O2 -S test.i
bergner@pike:~$ cat test.s 
	ld 10,0(3)
	ld 11,8(3)
	mtvsrd 1,10
	mtvsrd 2,11
	blr

Entering combine, we have the following rtl:
...
insn 6 3 9 2 (set (reg:TI 121 [ D.2829 ])
        (mem:TI (reg/v/f:DI 124 [ aD.2825 ]) [1 *a_2(D)+0 S16 A64])) "test.i":5:10 1048 {*vsx_le_perm_load_ti}
     (expr_list:REG_DEAD (reg/v/f:DI 124 [ aD.2825 ])
        (nil)))
(insn 9 6 10 2 (set (reg:DF 122 [ <retval> ])
        (subreg:DF (reg:TI 121 [ D.2829 ]) 0)) "test.i":5:10 515 {*movdf_hardfloat64}
     (nil))
(insn 10 9 14 2 (set (reg:DF 123 [ <retval>+8 ])
        (subreg:DF (reg:TI 121 [ D.2829 ]) 8)) "test.i":5:10 515 {*movdf_hardfloat64}
     (expr_list:REG_DEAD (reg:TI 121 [ D.2829 ])
        (nil)))
...

Combine should be able to merge insn 6 with both insn 9 and insn 10 to produce:

(insn ... (set (reg:DF 122)
               (mem:DF (reg:DI 124))
(insn ... (set (reg:DF 123)
               (mem:DF (plus:DI (reg:DI 124)
                                (const_int 8)))

By not merging them, we end up with two unneeded register copies we cannot remove.


---


### compiler : `gcc`
### title : `Thumb-1: GCC too aggressive on mul->lsl/sub/add optimization`
### open_at : `2018-11-29T06:07:30Z`
### last_modified_date : `2018-11-29T09:45:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88255
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
I might be wrong, but it appears that GCC is too aggressive in its conversion from multiplication to shift+add when targeting Thumb-1

It is true that, for example, the Cortex-M0 can have the small multiplier and a 16 cycle shift sequence would be faster. However, I was targeting arm7tdmi (-march=armv4t -mthumb -O3 -mtune=arm7tdmi) which, if I am not mistaken, uses one cycle for every 8 bits.

http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0234b/i102180.html

However, looking in the source code, I notice that the loop is dividing by 4. I think it might be a bug that is causing the otherwise 7 (I think) cycle sequence in the code below to be considered as having a weight of 18 cycles.

https://github.com/gcc-mirror/gcc/blob/master/gcc/config/arm/arm.c#L8959

I could be wrong, but one of the things I noticed is that very old versions of GCC (2.95) will not perform this many shifts, and that Clang, when given the  transpiled output in C and targeted for the same platform, will actually convert it back into a ldr/mul.

However, when targeting cortex-m0plus.small-multiply, it will still turn it into multiplication.

Code example: 

  unsigned MultiplyByPrime(unsigned val)
  {
      return val * 2246822519U;
  }

  MultiplyByPrime:
     lsls    r3, r0, #7 @ unsigned ret = val << 7;
     subs    r3, r3, r0 @ ret -= val;
     lsls    r3, r3, #5 @ ret <<= 5;
     subs    r3, r3, r0 @ ret -= val;
     lsls    r3, r3, #2 @ ret <<= 2;
     adds    r3, r3, r0 @ ret += val;
     lsls    r2, r3, #3 @ unsigned tmp = ret << 3;
     adds    r3, r3, r2 @ ret += tmp;
     lsls    r3, r3, #1 @ ret <<= 1;
     adds    r3, r3, r0 @ ret += val;
     lsls    r3, r3, #6 @ ret <<= 6;
     adds    r3, r3, r0 @ ret += val;
     lsls    r2, r3, #4 @ tmp = ret << 4;
     subs    r3, r2, r3 @ ret = tmp - ret;
     lsls    r3, r3, #3 @ ret <<= 3;
     subs    r0, r3, r0 @ ret -= val;
     bx      lr         @ return ret;


---


### compiler : `gcc`
### title : `vectorization failure for a typical loop for getting max value and index`
### open_at : `2018-11-29T08:39:11Z`
### last_modified_date : `2021-03-11T11:13:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88259
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC -O3 can't vectorize the following typical loop for getting max value and index from an array.

void test_vec(int *data, int n) {
        int best_i, best = 0;

        for (int i = 0; i < n; i++) {
                if (data[i] > best) {
                        best = data[i];
                        best_i = i;
                }
        }

        data[best_i] = data[0];
        data[0] = best;
}

The code generated in the kernel loop is as below,

.L4:
        ldr     w4, [x0, x2, lsl 2]
        cmp     w3, w4
        csel    w6, w4, w3, lt
        csel    w5, w2, w5, lt
        add     x2, x2, 1
        mov     w3, w6
        cmp     w1, w2
        bgt     .L4

If n is a constant like 1024, gcc -O3 still fails to vectorize it.

If we only get the max value and keep only one statement in the if statement inside the loop,

void test_vec(int *data, int n) {
        int best = 0;
        for (int i = 0; i < n; i++) {
                if (data[i] > best) {
                        best = data[i];
                }
        }

        data[0] = best;
}

"gcc -O3" can do vectorization and the kernel loop is like below,

.L4:
        ldr     q1, [x2], 16
        smax    v0.4s, v0.4s, v1.4s
        cmp     x2, x3
        bne     .L4


---


### compiler : `gcc`
### title : `Omit test instruction after add`
### open_at : `2018-11-29T18:40:49Z`
### last_modified_date : `2018-12-10T21:11:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88271
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
[code]
int data[8];

void test(int k)
{
    int level = 0;
    int val = 1;
    while (1)
    {
        if (val)
        {
            val = data[level] << 1;
            ++level;
        }
        else
        {
            --level;
            val = data[level];
        }
    }
}
[/code]

This code compiled using gcc 8.2 with options -O3 -march=skylake-avx512 produces this:

[asm]
test(int):
        mov     edx, 1
        xor     eax, eax
.L2:
        test    edx, edx
        je      .L3
.L6:
        movsx   rdx, eax
        mov     edx, DWORD PTR data[0+rdx*4]
        inc     eax
        add     edx, edx
        test    edx, edx
        jne     .L6
.L3:
        dec     eax
        movsx   rdx, eax
        mov     edx, DWORD PTR data[0+rdx*4]
        jmp     .L2
data:
        .zero   32
[/asm]

I checked that add instruction updates CPU flags, so test instruction before "jne .L6" could be omitted.


---


### compiler : `gcc`
### title : `AVX512: reorder bit ops to get free and operation`
### open_at : `2018-11-30T09:48:56Z`
### last_modified_date : `2018-11-30T11:13:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88276
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
[code]
#include <immintrin.h>
#include <stdint.h>

int test1(const __m128i* src, int mask)
{
    __m128i v = _mm_load_si128(src);
    int cmp = _mm_cmpeq_epi16_mask(v, _mm_setzero_si128());
    return (cmp << 1) & mask;
}

int test2(const __m128i* src, int mask)
{
    __m128i v = _mm_load_si128(src);
    int cmp = _mm_cmpeq_epi16_mask(v, _mm_setzero_si128());
    return (cmp & (mask >> 1)) << 1;
}
[/code]

test1() shifts result of _mm_cmpeq_epi16_mask() first, then and it with mask. In test2() mask is shifted first, then and-ed with cmp result, and then shifted again. In this case result of _mm_cmpeq_epi16_mask uses 8 bits only, so both code versions are equivalent.

This compiles to following asm code, using gcc 8.2 with -O3 -march=skylake-avx512:

[asm]
test1(long long __vector(2) const*, int):
        vpxor   xmm0, xmm0, xmm0
        vpcmpeqw        k1, xmm0, XMMWORD PTR [rdi]
        kmovb   edx, k1
        lea     eax, [rdx+rdx]
        and     eax, esi
        ret
test2(long long __vector(2) const*, int):
        mov     eax, esi
        sar     eax
        vpxor   xmm0, xmm0, xmm0
        kmovb   k2, eax
        vpcmpeqw        k1{k2}, xmm0, XMMWORD PTR [rdi]
        kmovb   eax, k1
        add     eax, eax
        ret
[/asm]

Such change may lead to more effective code, as with AVX512 this and op can be merged into vpcmpeqw instruction. In my case this was part of bigger function which was performing series of such calculations on array, and after this change it started working faster.


---


### compiler : `gcc`
### title : `Fails to elide zeroing of upper vector register`
### open_at : `2018-11-30T11:02:18Z`
### last_modified_date : `2018-12-03T09:02:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88278
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
typedef unsigned char v16qi __attribute__((vector_size(16)));
typedef unsigned char v8qi __attribute__((vector_size(8)));

v16qi __GIMPLE foo (unsigned char *p)
{
  v8qi _2;
  v16qi _3;

bb_2:
  _2 = __MEM <v8qi, 8> (p_1(D));
  _3 = _Literal (v16qi) { _2, _Literal (v8qi) { _Literal (unsigned char) 0, _Literal (unsigned char) 0, _Literal (unsigned char) 0, _Literal (unsigned char) 0, _Literal (unsigned char) 0, _Literal (unsigned char) 0, _Literal (unsigned char) 0 } };
  return _3;
}

and

typedef unsigned int v4si __attribute__((vector_size(16)));
typedef unsigned int v2si __attribute__((vector_size(8)));

v4si __GIMPLE bar (unsigned int *p)
{
  v2si _2;
  v4si _3;

bb_2:
  _2 = __MEM <v2si, 32> (p_1(D));
  _3 = _Literal (v4si) { _2, _Literal (v2si) { 0u, 0u } };
  return _3;
}

show that trying to code a movq (%rax), %xmm0 fails and we end up with

        movq    (%rdi), %xmm0
        pxor    %xmm1, %xmm1
        punpcklqdq      %xmm1, %xmm0
        ret

for both testcases with -O2 -fgimple

combine fails to match

(set (reg:V4SI 87)
    (vec_concat:V4SI (reg:V2SI 88 [ MEM[(unsigned int *)p_1(D)] ])
        (const_vector:V2SI [
                (const_int 0 [0]) repeated x2
            ])))

or

(set (reg:V4SI 87)
    (vec_concat:V4SI (mem:V2SI (reg:DI 90) [1 MEM[(unsigned int *)p_1(D)]+0 S8 A32])
        (const_vector:V2SI [
                (const_int 0 [0]) repeated x2
            ])))


---


### compiler : `gcc`
### title : `missing folding of logical and bitwise AND`
### open_at : `2018-11-30T12:13:03Z`
### last_modified_date : `2023-10-12T17:16:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88280
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
The following functions should all produce the same code with optimisations on (it is the same if `a' is replaced with a constant) 
-------------------------
int e(int i, int a) {
    return i && i & a;
}

int f(int i, int a) {
    return i & a && i;
}

int g(int i, int a) {
    int j = i & a;

    return j && i;
}

int h(int i, int a) {
    int j = i & a;

    return i && j;
}
-------------------------

But currently only f produces good assembly:

f(int, int):
        xor     eax, eax
        test    edi, esi
        setne   al
        ret


---


### compiler : `gcc`
### title : `SLP permutation check fails to fall back to strided loads`
### open_at : `2018-11-30T12:31:41Z`
### last_modified_date : `2018-11-30T12:31:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88281
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The following is not vectorized due to a group size of 17 and unsupported load permutation:

typedef unsigned char uint8_t;
static int x264_pixel_sad_8x8( uint8_t *pix1, int i_stride_pix1, uint8_t *pix2, int i_stride_pix2 )
{
  int i_sum = 0;
  for( int y = 0; y < 8; y++ )
    {
      for( int x = 0; x < 8; x++ )
        i_sum += __builtin_abs( pix1[x] - pix2[x] );
      pix1 += 17;
      pix2 += i_stride_pix2;
    }
  return i_sum;
}
void x264_pixel_sad_x4_8x8( uint8_t *fenc, uint8_t *pix0, uint8_t *pix1, uint8_t *pix2, uint8_t *pix3, int i_stride, int scores[4] )
{
  *scores = x264_pixel_sad_8x8( fenc, 16, pix0, i_stride );
}


---


### compiler : `gcc`
### title : `[8 Regression] Optimization regression with undefined unsigned overflow`
### open_at : `2018-12-01T18:09:29Z`
### last_modified_date : `2021-05-14T11:14:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88301
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.0`
### severity : `normal`
### contents :
I noticed gcc 7.* did a really nice optimization that allowed me to communicate I want even some unsigned overflows to be
undefined:

	#define ADD_NW(A,B) (__extension__({ __typeof(A+B) R; if(__builtin_add_overflow(A,B,&R)) __builtin_unreachable(); R ;}))
	_Bool a_b(unsigned A,  unsigned B) { return A+B >= B; }
	_Bool a_b2(unsigned A,  unsigned B) { return ADD_NW(A,B) >= B; }

resulted in:

	a_b:
			add     edi, esi
			setnc   al
			ret
	a_b2:
			mov     eax, 1
			ret

But on gcc 8.* it's

	a_b:
		add     edi, esi
		setnc   al
		ret
	a_b2:
		add     edi, esi
		setnc   al
		ret

again.


---


### compiler : `gcc`
### title : `range calculation of shift`
### open_at : `2018-12-03T13:36:02Z`
### last_modified_date : `2021-08-13T22:59:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88314
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
gcc is not able to determine that the possible values of res are 0, 8, 16, 32 and so the following function is not optimized :

------------------
bool f(bool a, bool b, bool c) {
    int res = (a + b) << (1 + c) << 2;

    return res > 0 && res < 8; 
}
-----------------


---


### compiler : `gcc`
### title : `[7/8 Regression] R31 is unconditionally saved/restored on powerpc-darwin even when it's not necessary.`
### open_at : `2018-12-03T20:48:54Z`
### last_modified_date : `2019-05-14T18:46:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88343
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
This was fallout from the fix for PR 71469.

a trivial 

void foo (void) {}

at -O2 produces

$ more t.s
        .machine ppc7400
        .text
        .align  2
        .globl _foo
_foo:
        stw r31,-4(r1)
        lwz r31,-4(r1)
        blr
        .subsections_via_symbols


---


### compiler : `gcc`
### title : `-Os overrides -falign-functions=N on the command line`
### open_at : `2018-12-04T00:34:38Z`
### last_modified_date : `2023-09-12T15:21:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88345
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
The -Os effect on -falign-functions is documented in a confusing or even contradictory way:

    Optimize for size. -Os enables all -O2 optimizations except those that often increase code size:

    -falign-functions  -falign-jumps 
    -falign-labels  -falign-loops 
    -fprefetch-loop-arrays  -freorder-blocks-algorithm=stc

    It also enables -finline-functions, causes the compiler to tune for code size rather than execution speed, and performs further optimizations designed to reduce code size.

The first sentence implies -Os is doesn't enable -finline-functions, but the second sentence contradicts it.

The documentation for -finline-functions=n doesn't help.  It just says:

  If n is not specified or is zero, use a machine-dependent default.

I haven't been able to find a way to determine the machine-dependent default except by testing.

An experiment with an x86_64 compiler shows that -Os overrides whatever -falign-functions option is specified on the command line.  Curiously, though, with -O2 GCC does honor the -falign-functions=n setting.

Clang, on the other hand, does the opposite: it honors the -falign-functions=n setting with -Os and ignores it with -O2.

$ cat u.c && gcc -Os -c -Wall -falign-functions=2 u.c && objdump -d u.o
void f (void) { }
void g (void) { }


u.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 <f>:
   0:	c3                   	retq

0000000000000001 <g>:
   1:	c3                   	retq


---


### compiler : `gcc`
### title : `gcc does not unroll loop`
### open_at : `2018-12-04T23:44:09Z`
### last_modified_date : `2021-12-27T04:41:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88361
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
[code]
#include "immintrin.h"

#define SIZE 9

int src[SIZE][SIZE] __attribute__((aligned(16)));
int dst1[SIZE][SIZE] __attribute__((aligned(16)));
int dst2[SIZE][SIZE] __attribute__((aligned(16)));

void test1()
{
    for (int i = 0; i < SIZE; ++i)
    {
        for (int j = 0; j < SIZE; ++j)
        {
            dst1[i][j] = src[i][j];
            dst2[i][j] = 1u << src[i][j];
        }
    }
}

#pragma GCC push_options
#pragma GCC optimize ("unroll-loops")
void test2()
{
    int n = 0;
    for (; n < SIZE*SIZE-3; n += 4)
    {
        // Copy data
        __m128i v = _mm_load_si128((const __m128i*)(&src[0][0] + n));
        _mm_store_si128((__m128i*)(&dst1[0][0] + n), v);

        // Calculate bitmasks
        v = _mm_sllv_epi32(_mm_set1_epi32(1), v);
        _mm_store_si128((__m128i*)(&dst2[0][0] + n), v);
    }

    for (; n < SIZE*SIZE; n++)
    {
        int x = *(&src[0][0] + n);
        *((&dst1[0][0] + n)) = x;
        *((&dst2[0][0] + n)) = 1 << x;
    }
}
#pragma GCC pop_options
[/code]

When code above is compiled using gcc 8.2 with -O3 -mavx2 -mprefer-avx128, loops in test1() are unrolled and vectorized as expected. However in test2() loops are not unrolled completely, even with unroll pragma:

[asm]
test2():
  mov eax, OFFSET FLAT:dst1
  mov esi, OFFSET FLAT:src
  mov ecx, 40
  xor edx, edx
  mov rdi, rax
  vmovdqa xmm1, XMMWORD PTR .LC0[rip]
  rep movsq
.L4:
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rdx]
  lea rax, [rdx+16]
  vmovaps XMMWORD PTR dst2[rdx], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rdx+16]
  vmovaps XMMWORD PTR dst2[rax], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rdx+32]
  vmovaps XMMWORD PTR dst2[rdx+32], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rax+32]
  lea rdx, [rax+144]
  vmovaps XMMWORD PTR dst2[rax+32], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rax+48]
  vmovaps XMMWORD PTR dst2[rax+48], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rax+64]
  vmovaps XMMWORD PTR dst2[rax+64], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rax+80]
  vmovaps XMMWORD PTR dst2[rax+80], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rax+96]
  vmovaps XMMWORD PTR dst2[rax+96], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rax+112]
  vmovaps XMMWORD PTR dst2[rax+112], xmm0
  vpsllvd xmm0, xmm1, XMMWORD PTR src[rax+128]
  vmovaps XMMWORD PTR dst2[rax+128], xmm0
  cmp rax, 176
  jne .L4
  mov ecx, DWORD PTR src[rip+320]
  mov eax, 1
  sal eax, cl
  mov DWORD PTR dst1[rip+320], ecx
  mov DWORD PTR dst2[rip+320], eax
  ret
[/asm]

This issue also exists in gcc 8.2 for AARCH64. I found it there first, and then checked that on x86_64 it is also present.


---


### compiler : `gcc`
### title : `Possible code optimization when right shift count >= width of type`
### open_at : `2018-12-06T11:06:25Z`
### last_modified_date : `2023-08-05T03:34:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88387
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
When signed int is shifted right by more than its width, results will be either 0 or -1. This can used to simplify conditions like in test() to sign check as in test2(). Example code below was compiled using gcc 8.2 with -O3 -march=skylake-avx512.

[code]
void f();
void g();

void test(int n)
{
    if (n >> 2222)
        f();
    else
        g();
}

void test2(int n)
{
    if (n < 0)
        f();
    else
        g();
}
[/code]

[asm]
test(int):
        mov     eax, -82
        sarx    edi, edi, eax
        test    edi, edi
        je      .L2
        jmp     f()
.L2:
        jmp     g()
test2(int):
        test    edi, edi
        js      .L6
        jmp     g()
.L6:
        jmp     f()
[/code]


---


### compiler : `gcc`
### title : `attribute malloc ignored on function pointers when alloc_size is accepted`
### open_at : `2018-12-07T03:16:08Z`
### last_modified_date : `2022-11-14T06:29:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88397
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
I noticed this while testing a fix for bug 88372.

GCC accepts attributes alloc_align and alloc_size on declarations of function pointers but ignores (with a warning) attribute malloc.  Besides being inconsistent, it makes it impossible to get GCC to emit optimally efficient code for calls via function pointers to malloc-like functions (such as malloc or aligned_alloc).  The same limitation applies to defining function types with attribute malloc.

$ cat u.c && gcc -O2 -S -Wall -Wextra u.c
typedef __SIZE_TYPE__ size_t;

__attribute__ ((alloc_align (1), alloc_size (2)))
void* (*p)(size_t, size_t) = __builtin_aligned_alloc;

__attribute__ ((alloc_align (1), alloc_size (2), malloc))
void* (*q)(size_t, size_t) = __builtin_aligned_alloc;

u.c:7:1: warning: ‘malloc’ attribute ignored [-Wattributes]
    7 | void* (*q)(size_t, size_t) = __builtin_aligned_alloc;
      | ^~~~


---


### compiler : `gcc`
### title : `vectorization failure for a small loop to do byte comparison`
### open_at : `2018-12-07T04:35:34Z`
### last_modified_date : `2022-03-16T02:00:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88398
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
For the small case below, GCC -O3 can't vectorize the small loop to do byte comparison in func2.

void *malloc(long unsigned int);
typedef struct {
        unsigned char *buffer;
} data;

static unsigned char *func1(data *d)
{
        return d->buffer;
}

static int func2(int max, int pos, unsigned char *cur)
{
        unsigned char *p = cur + pos;
        int len = 0;
        while (++len != max)
                if (p[len] != cur[len])
                        break;
        return cur[len];
}

int main (int argc) {
        data d;
        d.buffer = malloc(2*argc);
        return func2(argc, argc, func1(&d));
}

At the moment, the following code is generated for this loop,

  4004d4:       38616862        ldrb    w2, [x3,x1]
  4004d8:       6b00005f        cmp     w2, w0
  4004dc:       540000a1        b.ne    4004f0 <main+0x50>
  4004e0:       38616880        ldrb    w0, [x4,x1]
  4004e4:       6b01027f        cmp     w19, w1
  4004e8:       91000421        add     x1, x1, #0x1
  4004ec:       54ffff41        b.ne    4004d4 <main+0x34>

In fact, this loop can be vectorized by checking if the comparison size is aligned to SIMD register length. It may introduce run time overhead, but cost model could make decision on doing it or not.


---


### compiler : `gcc`
### title : `inefficient code generation for mask from CC`
### open_at : `2018-12-07T12:55:40Z`
### last_modified_date : `2023-09-04T08:23:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88402
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
For sth like

unsigned long foo (int a, int b)
{
  return a < b ? -1ul : 0;
}

we produce at -O[23]

        xorl    %eax, %eax
        cmpl    %esi, %edi
        setl    %al
        negq    %rax
        ret

(partial register stall?)

and at -O

        cmpl    %esi, %edi
        setl    %al
        movzbl  %al, %eax
        negq    %rax

while we could use sbbq %rax, %rax if the suggestion at
https://lwn.net/Articles/744257/ is correct.


---


### compiler : `gcc`
### title : `The gcse and division/mod by 1000 prevents if-conversion`
### open_at : `2018-12-07T13:18:25Z`
### last_modified_date : `2023-06-02T04:44:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88403
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `7.3.0`
### severity : `enhancement`
### contents :
Created attachment 45183
mark.c

If I compile the marck.c with aarch64-linux-gnu-gcc -O3 or mipsel-linux-gnu-gcc -mips32r6 -mbranch-cost=5 the inner if does not get transformed to use conditional moves. 

mips32r6 example:

	li	$13,5000			# 0x1388
$L4:
...
	subu	$3,$2,$3
	bgec	$13,$3,$L3
	addiu	$2,$2,5000
	sra	$4,$2,31 <-- inserted by gcse
$L3:

	muh	$2,$2,$9
	sra	$2,$2,12
	subu	$2,$4,$2
...

The if-conversion fails because it refuses to speculate both instructions whose set regs are alive. If I use -fno-gcse or -ftree-loop-if-convert the problem disappears.

Is there something that can be done here w/o resolving to ftree-loop-if-convert ?

Tried:

mipsel-linux-gnu-gcc --version
mipsel-linux-gnu-gcc-7 (Debian 7.3.0-28) 7.3.0

xgcc --version
xgcc (GCC) 9.0.0 20181203 (experimental)


---


### compiler : `gcc`
### title : `Missed DSE opportunity`
### open_at : `2018-12-07T14:35:40Z`
### last_modified_date : `2018-12-10T08:51:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88405
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
For the following code:

#define MATRIX_SIZE 512

static double a[MATRIX_SIZE][MATRIX_SIZE];
static double b[MATRIX_SIZE][MATRIX_SIZE];
static double c[MATRIX_SIZE][MATRIX_SIZE];

double
foo (void) {
  double s;
  int i, j, k;
  /* Section A */
  for (i = 0; i < MATRIX_SIZE; i++) {
    for (j = 0; j < MATRIX_SIZE; j++) {
      a[i][j] = (double)i * (double)j;
      b[i][j] = (double)i / (double)(j+5);
    }
  }
  /* Section B */
  for (j = 0; j < MATRIX_SIZE; j++) {
    for (i = 0; i < MATRIX_SIZE; i++) {
      s = 0;
      for (k = 0; k < MATRIX_SIZE; k++) {
        s += a[i][k] * b[k][j];
      }
      c[i][j] = s;
    }
  }
  s = 0.0; // (1)
#if 0
  /* Section C */
  for (i = 0; i < MATRIX_SIZE; i++) {
    for (j = 0; j < MATRIX_SIZE; j++) {
      s += c[i][j];
    }
  }
#endif
  return s;
}

GCC does not manage to eliminate the code up to (1) and retains the expensive Section A.

Clang manages to eliminate much more and produces:
foo:                                    // @foo
// %bb.0:                               // %entry
        orr     w8, wzr, #0x200
.LBB0_1:                                // %vector.ph
                                        // =>This Inner Loop Header: Depth=1
        subs    x8, x8, #1              // =1
        b.ne    .LBB0_1
// %bb.2:                               // %for.cond20.preheader.preheader
        fmov    d0, xzr
        ret


on aarch64. This happens at -O3 as well as -O2 as well as other targets (occurs also on x86)


---


### compiler : `gcc`
### title : `suboptimal code for a<imm?-1:0`
### open_at : `2018-12-09T19:33:18Z`
### last_modified_date : `2018-12-13T16:28:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88425
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
GCC does not manage to optimize (unsigned)a<b?-1:0 to a cmp-sbb sequence for x86 when b is an immediate (RTL sees a <= (b-1) comparison instead).

unsigned long baz (unsigned int a)
{
  return a < 123 ? -1ul : 0;
}

Optimal code would be

baz:
        cmpl    123, %edi
        sbbq    %rax, %rax
        ret

but we generate (at -O1, xor/cmp/setbe/neg at -O2+)

baz:
        cmpl    $122, %edi
        setbe   %al
        movzbl  %al, %eax
        negq    %rax
        ret

In the common case the optimization is performed by combine:

unsigned long baz (unsigned int a, unsigned b)
{
  return a < b ? -1ul : 0;
}

baz:
        cmpl    %esi, %edi
        sbbq    %rax, %rax
        ret


---


### compiler : `gcc`
### title : `Fails to consider lea -1(%rax), %rax compared to sub 1, %rax failing to CSE test`
### open_at : `2018-12-10T12:09:35Z`
### last_modified_date : `2021-12-27T15:01:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88428
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
The following GIMPLE test shows non-optimal assembly

long mask;
void bar ();
__GIMPLE () void foo (int a, int b)
{
  long _3;
  _3 = a_1(D) < b_2(D) ? _Literal (long) -1l : 0l;
  mask = _3;
  if (a_1(D) < b_2(D))
    goto bb1;
  else
    goto bb2;

bb1:
    bar ();

bb2:
  return;
}

foo:
.LFB0:
        .cfi_startproc
        xorl    %eax, %eax
        cmpl    %esi, %edi
        setge   %al
        subq    $1, %rax
        movq    %rax, mask(%rip)
        cmpl    %esi, %edi
        jl      .L5
...

here subq clobbers flags and thus the cmpl has to be repeated.  I believe
we could use lea which also has the same size

        leaq    -0x1(%rax), %rax

here instead and elide the redundant cmpl.  For my purpose the store to
mask is unnecessary, it was placed to simplify the testcase.  A GIMPLE
testcase was necessary to get the COND_EXPR and non-jumpy code through
optimization.

I'm not sure at which point during RTL we commit to using a CC clobbering
sub vs. a non-CC clobbering lea, but maybe cmpelim could replace one
with the other here?


---


### compiler : `gcc`
### title : `size optimization of memcpy-like code`
### open_at : `2018-12-11T00:01:26Z`
### last_modified_date : `2021-02-16T15:41:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88440
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
https://godbolt.org/z/RTji7B

void foo(char* restrict dst, const char* buf) {
    for (int i=0; i<8; ++i)
        *dst++ = *buf++;
}

$ gcc -Os
$ gcc -O2
.L2:
        mov     dl, BYTE PTR [rsi+rax]
        mov     BYTE PTR [rdi+rax], dl
        inc     rax
        cmp     rax, 8
        jne     .L2

$ gcc -O3
        mov     rax, QWORD PTR [rsi]
        mov     QWORD PTR [rdi], rax

$ arm-none-eabi-gcc -O3 -mthumb -mcpu=cortex-m4
        ldr     r3, [r1]  @ unaligned
        ldr     r2, [r1, #4]      @ unaligned
        str     r2, [r0, #4]      @ unaligned
        str     r3, [r0]  @ unaligned

The -O3 code is both faster and smaller for both ARM and x64:
"note: Loop 1 distributed: split to 0 loops and 1 library calls."

Should be considered for -O2 and -Os as well.


---


### compiler : `gcc`
### title : `noexcept(expr) should return true with -fno-exceptions`
### open_at : `2018-12-11T08:13:52Z`
### last_modified_date : `2018-12-11T09:35:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88445
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `9.0`
### severity : `normal`
### contents :
Consider the following example:

#include <type_traits>

struct test {
    test();
    test(test&&);
    test& operator=(test&&);
};

void test_func() {
    static_assert(noexcept(test{test()}), "");
    static_assert(std::is_nothrow_move_constructible<test>::value, "");
    static_assert(std::is_nothrow_move_assignable<test>::value, "");
}


The static assertions fail with the -fno-exceptions flag however no exception could happen because all the exceptions are disabled.

Please adjust the noexcept(expr) logic for the -fno-exceptions flag.

Such adjustment is essential because the standard library heavily relies on the type traits and chooses the suboptimal algorithms in -fno-exceptions environments.


---


### compiler : `gcc`
### title : `vectorization failure for a simple sum reduction loop`
### open_at : `2018-12-12T07:10:42Z`
### last_modified_date : `2018-12-12T09:30:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88459
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
For the simple loop below, gcc -O3 fails to vectorize it.

unsigned int tmp[1024];
unsigned int test_vec(int n)
{
        int sum = 0;
        for(int i = 0; i < 1024; i++)
        {
                sum += tmp[i];
        }
        return sum;
}

The kernel loop is,

.L2:
        ldr     w2, [x1], 4
        add     w0, w0, w2
        cmp     x3, x1
        bne     .L2


But if we change the data type of sum from "int" to "unsigned int" as below,

unsigned int tmp[1024];
unsigned int test_vec(int n)
{
        unsigned int sum = 0;
        for(int i = 0; i < 1024; i++)
        {
                sum += tmp[i];
        }
        return sum;
}

gcc can vectorize it, and the kernel loop is like,

.L2:
        ldr     q1, [x0], 16
        add     v0.4s, v0.4s, v1.4s
        cmp     x1, x0
        bne     .L2


---


### compiler : `gcc`
### title : `AVX512: gcc should keep value in kN registers if possible`
### open_at : `2018-12-12T09:54:25Z`
### last_modified_date : `2018-12-13T08:09:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88461
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
I tried to write piece of code which used new AVX512 logic instructions which works on kN registers. It turned out that gcc was moving intermediate values back and forth between kN and eax, what resulted in very poor code.

Example was compiled using gcc 8.2 with -O3 -march=skylake-avx512

[code]
#include <immintrin.h>
#include <stdint.h>

int test(uint16_t* data, int a)
{
    __m128i v = _mm_load_si128((const __m128i*)data);
    __mmask8 m = _mm_testn_epi16_mask(v, v);
    m = _kshiftli_mask16(m, 1);
    m = _kandn_mask16(m, a);
    return m;
}
[/code]

[asm]
test(unsigned short*, int):
        vmovdqa64       xmm0, XMMWORD PTR [rdi]
        kmovw   k5, esi
        vptestnmw       k1, xmm0, xmm0
        kmovb   eax, k1
        kmovw   k2, eax
        kshiftlw        k0, k2, 1
        kmovw   eax, k0
        movzx   eax, al
        kmovw   k4, eax
        kandnw  k3, k4, k5
        kmovw   eax, k3
        movzx   eax, al
        ret
[/asm]


---


### compiler : `gcc`
### title : `AVX-512 vectorization of masked scatter failing with "not suitable for scatter store"`
### open_at : `2018-12-12T13:46:16Z`
### last_modified_date : `2018-12-19T10:16:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88464
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
Hi,

I have the following simple loop which I want to compile for Skylake (AVX-512):

================================
#pragma GCC ivdep
for (int i = 0; i < n; ++i)
{
    if (b[off1[i]] < b[off2[i]])
        a[off1[i]] = b[off1[i]];
    else
        a[off2[i]] = b[off2[i]];
}
================================

Given AVX-512 masked scatter instructions and the absence of data conflicts ("ivdep"), vectorization should be possible along the lines of:
1. gather b[off1[i]] into zmm1
2. gather b[off2[i]] into zmm2
3. compare zmm1 and zmm2 with "<" and store result in mask1
4. compare zmm1 and zmm2 with ">=" and store result in mask2
5. scatter zmm1 to a[off1[i]] with mask1
6. scatter zmm2 to a[off2[i]] with mask2

However, GCC is not able to vectorize this loop (failing with "not vectorized: not suitable for scatter store"). I have tested this with the latest GCC trunk but the issue also occurs with all previous versions. If you want to have a look, here's a Godbolt example: https://godbolt.org/z/Is7Zml

I understand that this loop is not a trivial case for vectorization and AVX-512 hasn't been around for too long, so it's likely that it isn't fully supported yet. But still, I'm wondering:
1. Am I missing some flags or hints to GCC in order to vectorize this loop? (I can imagine something related to the cost model, etc..)
2. Or is GCC currently just not capable of vectorizing it?

If the answer is "2.":
3. Can we estimate to amount of work needed to support this?
4. Is there any plan on when this kind of pattern will be supported? 
5. If it's realistic for a non-GCC developer to look into this, is there anything I can do to help?


Many thanks in advance,
Moritz


---


### compiler : `gcc`
### title : `AVX512: optimize loading of constant values to kN registers`
### open_at : `2018-12-12T14:53:38Z`
### last_modified_date : `2018-12-13T08:10:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88465
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
When constant value is loaded into kN register, gcc puts it into eax first, and then moved to kN register:

[code]
#include <immintrin.h>
#include <stdint.h>

__mmask8 test(__mmask8 m)
{
    __mmask8 m2 = _kand_mask8(m, 3);
    return m2;
}
[/code]

[asm]
test(unsigned char):
        mov     eax, 3
        kmovb   k1, eax
        kmovb   k2, edi
        kandb   k0, k1, k2
        kmovb   eax, k0
        ret
[/asm]

icc uses one instruction for this. https://godbolt.org/ displayed it as "null", but most probably this is wrong name:

[asm]
test(unsigned char):
        vkmovb    k0, edi                                       #6.19
        null      k1, 3                                         #6.19
        kandb     k2, k0, k1                                    #6.19
        vkmovb    eax, k2                                       #6.19
        ret                                                     #7.12
[/asm]

You can also use instructions kxor and kxnor to load 0 and -1.


---


### compiler : `gcc`
### title : `AVX512: constant folding on mask does not remove unnecessary instructions`
### open_at : `2018-12-12T21:04:22Z`
### last_modified_date : `2021-09-07T15:42:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88473
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
[code]
#include <immintrin.h>

void test(void* data, void* data2)
{
    __m128i v = _mm_load_si128((__m128i const*)data);
    __mmask8 m = _mm_testn_epi16_mask(v, v);
    m = _kor_mask8(m, 0x0f);
    m = _kor_mask8(m, 0xf0);
    v = _mm_maskz_add_epi16(m, v, v);
    _mm_store_si128((__m128i*)data2, v);
}
[/code]

Code compiled using gcc 8.2 with -O3 -march=skylake-avx512 . gcc was able to fold constant expressions and simplify masked vector add to non-masked one. However original version of folded expression is still present in output:

[asm]
test(void*, void*):
  vmovdqa64 xmm0, XMMWORD PTR [rdi]
  mov eax, 15
  vptestnmw k1, xmm0, xmm0
  kmovb k2, eax
  vpaddw xmm0, xmm0, xmm0
  mov eax, -16
  kmovb k3, eax
  vmovaps XMMWORD PTR [rsi], xmm0
  korb k0, k1, k2
  korb k0, k0, k3
  ret
[/asm]

clang properly cleaned it up:

[asm]
test(void*, void*): # @test(void*, void*)
  vmovdqa xmm0, xmmword ptr [rdi]
  vpaddw xmm0, xmm0, xmm0
  vmovdqa xmmword ptr [rsi], xmm0
  ret
[/asm]


---


### compiler : `gcc`
### title : `Inline built-in hypot for -ffast-math`
### open_at : `2018-12-12T21:41:38Z`
### last_modified_date : `2022-05-27T08:23:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88474
### status : `REOPENED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
GCC should support inline code generation for hypot functions, under appropriate fast-math conditions.

glibc's bits/mathinline.h, for 32-bit non-SSE fast-math x86 only, has:

/* The argument range of the inline version of hypotl is slightly reduced.  */
__inline_mathcodeNP2 (hypot, __x, __y,
		      return __libc_sqrtl (__x * __x + __y * __y))

We're moving away from such inlines in glibc, preferring to leave it to the compiler to inline standard functions under appropriate conditions.  Although this is filed as a target issue, inlining hypot functions (given -funsafe-math-optimizations, as it may change results, and -ffinite-math-only, as it won't be correct for Inf, NaN arguments) is actually reasonably generic.  There are separate cases for when it's possible to do the * + sqrt operations on a wider (hardware) type, so avoiding reducing the argument range, and when a wider hardware type is unavailable or inappropriate to use, in which case the argument range would be reduced by inlining (although that glibc inline still does it for long double).


---


### compiler : `gcc`
### title : `Optimize expressions which uses vector, mask and general purpose registers`
### open_at : `2018-12-12T22:47:49Z`
### last_modified_date : `2021-09-04T13:51:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88476
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
I was playing with Compiler Explorer to see how compilers can optimize various pieces of code. I found next version of clang (version 8.0.0 (trunk 348905)) can optimize expressions which uses vector, mask and general purpose registers. Such approach opens new optimization possibilities. Here are two example functions which demonstrates this:

[code]
#include <immintrin.h>

void test1(void* data1, void* data2)
{
    __m128i v1 = _mm_load_si128((__m128i const*)data1);
    __m128i v2 = _mm_load_si128((__m128i const*)data2);
    __mmask8 m1 = _mm_testn_epi16_mask(v1, v1);
    __mmask8 m2 = _mm_testn_epi16_mask(v2, v2);
    __mmask8 m = (m1 | 3) & (m2 | 3);
    v1 = _mm_maskz_add_epi16(m, v1, v2);
    _mm_store_si128((__m128i*)data2, v1);
}

void test2(void* data1, void* data2)
{
    __m128i v1 = _mm_load_si128((__m128i const*)data1);
    __m128i v2 = _mm_load_si128((__m128i const*)data2);
    __mmask8 m1 = _mm_testn_epi16_mask(v1, v1);
    __mmask8 m2 = _mm_testn_epi16_mask(v2, v2);
    m1 = _kor_mask8(m1, 3);
    m2 = _kor_mask8(m2, 3);
    __mmask8 m = _kand_mask8(m1, m2);
    v1 = _mm_maskz_add_epi16(m, v1, v2);
    _mm_store_si128((__m128i*)data2, v1);
}
[/code]

When compiled using clang with -O3 -march=skylake-avx512, both are optimized to the same code:

[asm]
test(void*, void*): # @test(void*, void*)
  vmovdqa xmm0, xmmword ptr [rdi]
  vmovdqa xmm1, xmmword ptr [rsi]
  vpor xmm2, xmm1, xmm0
  vptestnmw k0, xmm2, xmm2
  mov al, 3
  kmovd k1, eax
  korb k1, k0, k1
  vpaddw xmm0 {k1} {z}, xmm1, xmm0
  vmovdqa xmmword ptr [rsi], xmm0
  ret
[/asm]

gcc 9.0.0 20181211 (experimental) produces this:

[asm]
test1(void*, void*):
  vmovdqa64 xmm1, XMMWORD PTR [rsi]
  vmovdqa64 xmm0, XMMWORD PTR [rdi]
  vptestnmw k1, xmm1, xmm1
  vptestnmw k2{k1}, xmm0, xmm0
  kmovb eax, k2
  or eax, 3
  kmovb k3, eax
  vpaddw xmm0{k3}{z}, xmm0, xmm1
  vmovaps XMMWORD PTR [rsi], xmm0
  ret
test2(void*, void*):
  vmovdqa64 xmm0, XMMWORD PTR [rdi]
  vmovdqa64 xmm1, XMMWORD PTR [rsi]
  vptestnmw k1, xmm0, xmm0
  vptestnmw k3, xmm1, xmm1
  mov eax, 3
  kmovb k2, eax
  korb k1, k1, k2
  korb k0, k3, k2
  kandb k1, k1, k0
  vpaddw xmm0{k1}{z}, xmm0, xmm1
  vmovaps XMMWORD PTR [rsi], xmm0
  ret
[/asm]


---


### compiler : `gcc`
### title : `union prevents autovectorization`
### open_at : `2018-12-13T17:04:44Z`
### last_modified_date : `2021-08-10T22:59:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88487
### status : `ASSIGNED`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
When pointer to data is inside union, loops are not autovectorized. This also happen when I removed "i" field from union, so it had only one field. Code compiled with -O3 -mavx

[code]
struct S1
{
    union
    {
        double* __restrict__ * __restrict__ d;
        int* __restrict__ * __restrict__ i;
    } u;
};

struct S2
{
    double* __restrict__ * __restrict__ d;
};

void test1(S1* __restrict__ s1, S1* __restrict__ s2)
{
    for (int n = 0; n < 2; ++n)
    {
        s1->u.d[n][0] = s2->u.d[n][0];
        s1->u.d[n][1] = s2->u.d[n][1];
    }
}

void test2(S2* __restrict__ s1, S2* __restrict__ s2)
{
    for (int n = 0; n < 2; ++n)
    {
        s1->d[n][0] = s2->d[n][0];
        s1->d[n][1] = s2->d[n][1];
    }
}
[/code]

[asm]
test1(S1*, S1*):
        mov     rdx, QWORD PTR [rsi]
        mov     rax, QWORD PTR [rdi]
        mov     rsi, QWORD PTR [rdx]
        mov     rcx, QWORD PTR [rax]
        mov     rdx, QWORD PTR [rdx+8]
        mov     rax, QWORD PTR [rax+8]
        vmovsd  xmm0, QWORD PTR [rsi]
        vmovsd  QWORD PTR [rcx], xmm0
        vmovsd  xmm0, QWORD PTR [rsi+8]
        vmovsd  QWORD PTR [rcx+8], xmm0
        vmovsd  xmm0, QWORD PTR [rdx]
        vmovsd  QWORD PTR [rax], xmm0
        vmovsd  xmm0, QWORD PTR [rdx+8]
        vmovsd  QWORD PTR [rax+8], xmm0
        ret
test2(S2*, S2*):
        mov     rdx, QWORD PTR [rsi]
        mov     rax, QWORD PTR [rdi]
        mov     rcx, QWORD PTR [rdx]
        mov     rdx, QWORD PTR [rdx+8]
        vmovupd xmm0, XMMWORD PTR [rcx]
        mov     rcx, QWORD PTR [rax]
        mov     rax, QWORD PTR [rax+8]
        vmovups XMMWORD PTR [rcx], xmm0
        vmovupd xmm0, XMMWORD PTR [rdx]
        vmovups XMMWORD PTR [rax], xmm0
        ret
[/asm]


---


### compiler : `gcc`
### title : `Missed autovectorization when indices are different`
### open_at : `2018-12-13T18:06:53Z`
### last_modified_date : `2021-08-10T22:59:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88490
### status : `NEW`
### tags : `alias, missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Code below reads and writes data using different indices what is checked by "if" above loop. This can be autovectorized, as both memory areas do not overlap. Code compiled with -O3 -march=skylake-avx512

[code]
struct S
{
    double* __restrict__ * __restrict__ d;
};

void test(S* __restrict__ s, int n, int k)
{
    if (n > k)
    {
        for (int n = 0; n < 2; ++n)
        {
            s->d[n][0] = s->d[k][0];
            s->d[n][1] = s->d[k][1];
        }
    }
}
[/code]

[asm]
test(S*, int, int):
        cmp     esi, edx
        jle     .L3
        mov     rcx, QWORD PTR [rdi]
        movsx   rdx, edx
        mov     rax, QWORD PTR [rcx+rdx*8]
        mov     rdx, QWORD PTR [rcx]
        vmovsd  xmm0, QWORD PTR [rax]
        vmovsd  QWORD PTR [rdx], xmm0
        vmovsd  xmm0, QWORD PTR [rax+8]
        vmovsd  QWORD PTR [rdx+8], xmm0
        vmovsd  xmm0, QWORD PTR [rax]
        mov     rdx, QWORD PTR [rcx+8]
        vmovsd  QWORD PTR [rdx], xmm0
        vmovsd  xmm0, QWORD PTR [rax+8]
        vmovsd  QWORD PTR [rdx+8], xmm0
.L3:
        ret
[/asm]


---


### compiler : `gcc`
### title : `SLP optimization generates ugly code`
### open_at : `2018-12-14T05:16:54Z`
### last_modified_date : `2022-01-04T19:23:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88492
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
For aarch64, SLP optimization generates ugly code for the case below,

int test_slp( unsigned char *b )
{
        unsigned int tmp[4][4];
        int sum = 0;
        for( int i = 0; i < 4; i++, b += 4 )
        {
                tmp[i][0] = b[0];
                tmp[i][2] = b[1];
                tmp[i][1] = b[2];
                tmp[i][3] = b[3];
        }
        for( int i = 0; i < 4; i++ )
        {
                sum += tmp[0][i] + tmp[1][i] + tmp[2][i] + tmp[3][i];
        }
        return sum;
}

With command line "gcc -O3", the following code is generated,

0000000000000000 <test_slp>:
   0:   90000001        adrp    x1, 0 <test_slp>
   4:   d10103ff        sub     sp, sp, #0x40
   8:   3dc00001        ldr     q1, [x0]
   c:   3dc00020        ldr     q0, [x1]
  10:   4e000021        tbl     v1.16b, {v1.16b}, v0.16b
  14:   2f08a422        uxtl    v2.8h, v1.8b
  18:   6f08a421        uxtl2   v1.8h, v1.16b
  1c:   2f10a443        uxtl    v3.4s, v2.4h
  20:   6f10a442        uxtl2   v2.4s, v2.8h
  24:   2f10a420        uxtl    v0.4s, v1.4h
  28:   6f10a421        uxtl2   v1.4s, v1.8h
  2c:   9e660060        fmov    x0, d3
  30:   ad000be3        stp     q3, q2, [sp]
  34:   b9401be8        ldr     w8, [sp, #24]
  38:   ad0107e0        stp     q0, q1, [sp, #32]
  3c:   9e660022        fmov    x2, d1
  40:   d360fc01        lsr     x1, x0, #32
  44:   9e660040        fmov    x0, d2
  48:   294117e6        ldp     w6, w5, [sp, #8]
  4c:   d360fc43        lsr     x3, x2, #32
  50:   b9402be2        ldr     w2, [sp, #40]
  54:   d360fc07        lsr     x7, x0, #32
  58:   9e660000        fmov    x0, d0
  5c:   0ea18400        add     v0.2s, v0.2s, v1.2s
  60:   0b0100e7        add     w7, w7, w1
  64:   0b0800c6        add     w6, w6, w8
  68:   b9401fe8        ldr     w8, [sp, #28]
  6c:   d360fc00        lsr     x0, x0, #32
  70:   1e260001        fmov    w1, s0
  74:   0ea28460        add     v0.2s, v3.2s, v2.2s
  78:   0b000063        add     w3, w3, w0
  7c:   0b070063        add     w3, w3, w7
  80:   29471fe0        ldp     w0, w7, [sp, #56]
  84:   1e260004        fmov    w4, s0
  88:   0b000042        add     w2, w2, w0
  8c:   b9402fe0        ldr     w0, [sp, #44]
  90:   0b060042        add     w2, w2, w6
  94:   0b040021        add     w1, w1, w4
  98:   0b070000        add     w0, w0, w7
  9c:   0b030021        add     w1, w1, w3
  a0:   0b0800a3        add     w3, w5, w8
  a4:   0b020021        add     w1, w1, w2
  a8:   0b030000        add     w0, w0, w3
  ac:   0b000020        add     w0, w1, w0
  b0:   910103ff        add     sp, sp, #0x40
  b4:   d65f03c0        ret

In the code, vectorization code is generated, but there are ugly instructions generated as well, e.g. memory store and register copy from SIMD register to general purpose register.

With command line "gcc -O3 -fno-tree-slp-vectorize", the following code can be generated, and it looks pretty clean. Usually, this code sequence is friendly to hardware prefetch.

0000000000000000 <test_slp>:
   0:   39402004        ldrb    w4, [x0, #8]
   4:   39401002        ldrb    w2, [x0, #4]
   8:   39403001        ldrb    w1, [x0, #12]
   c:   39400003        ldrb    w3, [x0]
  10:   39402806        ldrb    w6, [x0, #10]
  14:   0b040021        add     w1, w1, w4
  18:   39401805        ldrb    w5, [x0, #6]
  1c:   0b020063        add     w3, w3, w2
  20:   39403804        ldrb    w4, [x0, #14]
  24:   0b030021        add     w1, w1, w3
  28:   39400802        ldrb    w2, [x0, #2]
  2c:   39400403        ldrb    w3, [x0, #1]
  30:   0b060084        add     w4, w4, w6
  34:   39402407        ldrb    w7, [x0, #9]
  38:   0b050042        add     w2, w2, w5
  3c:   39401406        ldrb    w6, [x0, #5]
  40:   0b020084        add     w4, w4, w2
  44:   39403405        ldrb    w5, [x0, #13]
  48:   0b040021        add     w1, w1, w4
  4c:   0b060063        add     w3, w3, w6
  50:   39400c02        ldrb    w2, [x0, #3]
  54:   0b0700a5        add     w5, w5, w7
  58:   39403c04        ldrb    w4, [x0, #15]
  5c:   0b050063        add     w3, w3, w5
  60:   39401c06        ldrb    w6, [x0, #7]
  64:   39402c05        ldrb    w5, [x0, #11]
  68:   0b030021        add     w1, w1, w3
  6c:   0b060040        add     w0, w2, w6
  70:   0b050082        add     w2, w4, w5
  74:   0b020000        add     w0, w0, w2
  78:   0b000020        add     w0, w1, w0
  7c:   d65f03c0        ret

Anyway, it looks the heuristic rule to enable SLP optimization needs to be improved.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] polyhedron 10% mdbx runtime regression`
### open_at : `2018-12-14T13:28:44Z`
### last_modified_date : `2023-07-07T10:34:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88494
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Between r266526 (good) and r266587 (bad) polyhedron mdbx runtime regressed from
6s to 6.7s on a Haswell machine with -Ofast -march=native -funroll-loops.

https://gcc.opensuse.org/gcc-old/c++bench-czerny/pb11/pb11-summary.txt-2-0.html

There are not many interesting changes in the revision range but I sofar didn't
reproduce elsewhere nor bisected the above revs.


---


### compiler : `gcc`
### title : `Unnecessary stack adjustment with -mavx512f`
### open_at : `2018-12-14T13:57:15Z`
### last_modified_date : `2021-08-16T02:13:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88496
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-1 gcc]$ cat /tmp/x.i
struct B
{
  char a[12];
  int b;
};

struct B
f2 (void)
{
  struct B x = {};
  return x;
}
[hjl@gnu-cfl-1 gcc]$ ./xgcc -B./ -O2 -S /tmp/x.i -mavx2
[hjl@gnu-cfl-1 gcc]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4
	.globl	f2
	.type	f2, @function
f2:
.LFB0:
	.cfi_startproc
	xorl	%eax, %eax
	xorl	%edx, %edx
	ret
	.cfi_endproc
.LFE0:
	.size	f2, .-f2
	.ident	"GCC: (GNU) 9.0.0 20181214 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 gcc]$ ./xgcc -B./ -O2 -S /tmp/x.i -mavx512f
[hjl@gnu-cfl-1 gcc]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4
	.globl	f2
	.type	f2, @function
f2:
.LFB0:
	.cfi_startproc
	subq	$16, %rsp
	.cfi_def_cfa_offset 24
	xorl	%eax, %eax
	xorl	%edx, %edx
	addq	$16, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:
	.size	f2, .-f2
	.ident	"GCC: (GNU) 9.0.0 20181214 (experimental)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 gcc]$ 

subq and addq aren't necessary.


---


### compiler : `gcc`
### title : `Inline built-in asinh, acosh, atanh for -ffast-math`
### open_at : `2018-12-14T21:24:00Z`
### last_modified_date : `2023-07-22T02:53:42Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88502
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
GCC should support inline code generation for asinh, acosh, atanh functions, under appropriate fast-math conditions.

glibc's bits/mathinline.h, for 32-bit non-SSE fast-math x86 only, has:

/* The argument range of the inline version of asinhl is slightly reduced.  */
__inline_mathcodeNP (asinh, __x, \
  register long double  __y = __fabsl (__x);				      \
  return (log1pl (__y * __y / (__libc_sqrtl (__y * __y + 1.0) + 1.0) + __y)   \
	  * __sgn1l (__x)))

__inline_mathcodeNP (acosh, __x, \
  return logl (__x + __libc_sqrtl (__x - 1.0) * __libc_sqrtl (__x + 1.0)))

__inline_mathcodeNP (atanh, __x, \
  register long double __y = __fabsl (__x);				      \
  return -0.5 * log1pl (-(__y + __y) / (1.0 + __y)) * __sgn1l (__x))

We're moving away from such inlines in glibc, preferring to leave it to the compiler to inline standard functions under appropriate conditions.  This inlining probably only makes sense when logl / log1pl are themselves expanded inline (but in principle it's otherwise generic; note this x86 code uses long double, so avoiding reducing the argument range for built-in functions for narrower types).  (__sgn1l is another inline function, copying the sign of x to the value 1.0L.)


---


### compiler : `gcc`
### title : `Missing optimization of tls initialization`
### open_at : `2018-12-15T03:43:03Z`
### last_modified_date : `2018-12-15T17:00:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88509
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `8.2.1`
### severity : `normal`
### contents :
Given

struct foo {
  foo();
};
static thread_local foo bar;
foo *f() { return &bar; }
foo *g() {
  static thread_local foo *bar_ptr;
  if (bar_ptr == nullptr) {
    [&]() { bar_ptr = &bar; }();
  }
  return bar_ptr;
}

GCC has to make sure bar is only initialized once. For the function f it produces

        pushq   %rbx
        cmpb    $0, %fs:__tls_guard@tpoff
        movq    %fs:0, %rbx
        je      .L6
        leaq    _ZL3bar@tpoff(%rbx), %rax
        popq    %rbx
        ret
.L6:
       <do_the_initialization>

for g, the common code path is somewhat simpler:

        movq    %fs:_ZZ1gvE7bar_ptr@tpoff, %rax
        testq   %rax, %rax
        je      .L15
        ret
.L15:
       <do_the_initialization>


The optimization is to use the a pointer to the object as a guard instead of using a boolean. As far as I can tell this can be applied to any static tls variable (where the ABI is not a problem).


---


### compiler : `gcc`
### title : `GCC generates inefficient U64x2/v2di scalar multiply for NEON32`
### open_at : `2018-12-15T04:38:19Z`
### last_modified_date : `2023-07-07T08:17:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88510
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
Note: I use these typedefs here for brevity.

typedef uint64x2_t U64x2;
typedef uint32x2_t U32x2;
typedef uint32x2x2_t U32x2x2;
typedef uint32x4_t U32x4;

GCC and Clang both have issues with this code on ARMv7a NEON, and will switch to scalar:

U64x2 multiply(U64x2 top, U64x2 bot)
{
    return top * bot;
}

gcc-8 -mfloat-abi=hard -mfpu=neon -O3 -S -march=armv7-a 

multiply:
        push    {r4, r5, r6, r7, lr}
        sub     sp, sp, #20
        vmov    r0, r1, d0  @ v2di
        vmov    r6, r7, d2  @ v2di
        vmov    r2, r3, d1  @ v2di
        vmov    r4, r5, d3  @ v2di
        mul     lr, r0, r7
        mla     lr, r6, r1, lr
        mul     ip, r2, r5
        umull   r0, r1, r0, r6
        mla     ip, r4, r3, ip
        add     r1, lr, r1
        umull   r2, r3, r2, r4
        strd    r0, [sp]
        add     r3, ip, r3
        strd    r2, [sp, #8]
        vld1.64 {d0-d1}, [sp:64]
        add     sp, sp, #20
        pop     {r4, r5, r6, r7, pc}

Clang's is worse, and you can compare the output, as well as the i386 SSE4.1 code here: https://godbolt.org/z/35owtL

Related LLVM bug 39967: https://bugs.llvm.org/show_bug.cgi?id=39967

I started the discussion in LLVM, as it had the worse problem, and we have come up with a few options for faster code that does not require scalar. You can also find the benchmark file (with outdated tests) and results results. They are from Clang, but since they use intrinsics, results are similar.

While we don't have vmulq_u64, we do have faster ways to multiply without going scalar.

I have benchmarked the code, and have found this option, based on the code emitted for SSE4.1:

U64x2 goodmul_sse(U64x2 top, U64x2 bot)
{
    U32x2 topHi = vshrn_n_u64(top, 32);     // U32x2 topHi  = top >> 32;
    U32x2 topLo = vmovn_u64(top);           // U32x2 topLo  = top & 0xFFFFFFFF;
    U32x2 botHi = vshrn_n_u64(bot, 32);     // U32x2 botHi  = bot >> 32;
    U32x2 botLo = vmovn_u64(bot);           // U32x2 botLo  = bot & 0xFFFFFFFF;

    U64x2 ret64 = vmull_u32(topHi, botLo);  // U64x2 ret64   = (U64x2)topHi * (U64x2)botLo;
    ret64 = vmlal_u32(ret64, topLo, botHi); //       ret64  += (U64x2)topLo * (U64x2)botHi;
    ret64 = vshlq_n_u64(ret64, 32);         //       ret64 <<= 32;
    ret64 = vmlal_u32(ret64, topLo, botLo); //       ret64  += (U64x2)topLo * (U64x2)botLo;
    return ret64;
}

If GCC can figure out how to interleave one or two of the operands, for example, changing this:

    U64x2 inp1 = vld1q_u64(p);
    U64x2 inp2 = vld1q_u64(q);
    vec = goodmul_sse(inp1, inp2);

to this (if it knows inp1 and/or inp2 are only used for multiplication):

    U32x2x2 inp1 = vld2_u32(p);
    U32x2x2 inp2 = vld2_u32(q);
    vec = goodmul_sse_interleaved(inp1, inp2)

then we can do this and save 4 cycles:

U64x2 goodmul_sse_interleaved(const U32x2x2 top, const U32x2x2 bot)
{
    U64x2 ret64 = vmull_u32(top.val[1], bot.val[0]);  // U64x2 ret64   = (U64x2)topHi * (U64x2)botLo;
    ret64 = vmlal_u32(ret64, top.val[0], bot.val[1]); //       ret64  += (U64x2)topLo * (U64x2)botHi;
    ret64 = vshlq_n_u64(ret64, 32);                   //       ret64 <<= 32;
    ret64 = vmlal_u32(ret64, top.val[0], bot.val[0]); //       ret64  += (U64x2)topLo * (U64x2)botLo;
    return ret64;
}

Another user posted this (typos fixed).

It seems to use two fewer cycles when not interleaved (not 100% sure about it),
but two cycles slower when it is fully interleaved.

U64x2 twomul(U64x2 top, U64x2 bot)
{
    U32x2 top_low = vmovn_u64(top);
    U32x2 bot_low = vmovn_u64(bot);
    U32x4 top_re = vreinterpretq_u32_u64(top);
    U32x4 bot_re = vrev64q_u32(vreinterpretq_u32_u64(bot));
    U32x4 prod = vmulq_u32(top_re, bot_re);
    U64x2 paired = vpaddlq_u32(prod);
    U64x2 shifted = vshlq_n_u64(paired, 32);
    return vmlal_u32(shifted, top_low, bot_low);
}

Either one of these is faster than scalar.


---


### compiler : `gcc`
### title : `G++ clears the return register on x86_64 when returning an empty class`
### open_at : `2018-12-17T08:53:01Z`
### last_modified_date : `2021-06-21T15:13:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88529
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `enhancement`
### contents :
The following code causes G++ to emit an "xorl %eax, %eax" on x86_64. I believe the Itanium ABI does not require it, so this is a missed optimisation.

class A{};

A f() { return {}; }


---


### compiler : `gcc`
### title : `Index data types when targeting AVX-512 vectorization with gather/scatter`
### open_at : `2018-12-17T12:09:07Z`
### last_modified_date : `2021-08-07T14:42:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88531
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
Hi,

I realized that GCC fails to vectorize simple loops if there are indirect loads (or stores) and the index used for the indirect access doesn't match a very small subset of possible integer data types. I'm targeting AVX-512. This is the MWE (only an indirect load, but a direct store):

==============================
#include <cstdint>

using loop_t = uint32_t;
using idx_t = uint32_t;

void loop(double * const __restrict__ dst,
          double const * const __restrict__ src,
          idx_t const * const __restrict__ idx,
          loop_t const begin,
          loop_t const end)
{
    for (loop_t i = begin; i < end; ++i)
    {
        dst[i] = 42.0 * src[idx[i]];
    }
}
==============================
See: https://godbolt.org/z/Ps-sOv

This only vectorizes if idx_t is int32_t, int64_t, or uint64_t.

My suspicion is this goes back to the gather/scatter instructions of AVX-512 that come in two flavors: with 32 and 64 bit signed integers for the indices.
Unsigned 64 bit probably works (on a 64 bit architecture) because it looks like it's just treated as a signed 64 bit value, which probably is due to (from the documentation):
"... The scaled index may require more bits to represent than the address bits used by the processor (e.g., in 32-bit mode, if the scale is greater than one). In this case, the most significant bits beyond the number of address bits are ignored. ..."

Unfortunately, for int16_t, uint16_t, and uint32_t, this does not vectorize. Although the 32 bit version of gather/scatter could be used -- with proper zero padding -- for int16_t and uint16_t. Likewise, the 64 bit version could be used with indices of type uint32_t.

Although the code example only uses idx[i] for loading, it appears to be the exact same issue when using idx[i] for storing (meaning: when scatter would be required).

Are there any plans to get this working?
Or did I maybe miss something and this should already work?

Many thanks in advance

Florian


---


### compiler : `gcc`
### title : `[9 Regression] Higher performance penalty of array-bounds checking for sparse-matrix vector multiply`
### open_at : `2018-12-17T20:37:50Z`
### last_modified_date : `2018-12-19T12:13:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88533
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 45249
Fortran code

I am seeing an increased performance penalty due to array-bounds checking,
in particular for sparse-matrix (CSC) vector multiplication.

The attached, semi-reduced test case, which only needs the provided meta-data
but otherwise uses random elements, should be sufficient for demonstration.

I have tested on an i5-8250U and tuned the "outer loop" so that the testcase
runs in 1-2 seconds on that machine.  For that purpose, I have used some
feedback provided to my initial posting on gcc-help, see
https://gcc.gnu.org/ml/gcc-help/2018-12/msg00041.html

Tested compilers:

gcc-7.3.1 20180323 [gcc-7-branch revision 258812]
gcc-8.2.1 20181202
gcc-9.0.0 20181214

baseline options: -O2 -ftree-vectorize -g -march=skylake -mfpmath=sse

7: 1.12
8: 1.12
9: 1.12

baseline + -funroll-loops :

7: 1.00
8: 1.00
9: 0.99

baseline + -funroll-loops -fcheck=bounds :

7: 1.56
8: 1.56
9: 1.93

baseline + -funroll-loops -fcheck=bounds -fno-tree-ch :

7: 1.78
8: 1.80
9: 1.83


baseline + -funroll-loops -fno-tree-ch :

7: 1.05
8: 1.09
9: 1.09

Preliminary conclusions:

- -funroll-loops is helpful here
- -fcheck=bounds is quite expensive with current 9.0
- -fno-tree-ch brings the different versions in line,
   it benefits 9, but is worse for 7 and 8
- there a no options above that bring 9 to the level of 7 and 8
  as long as bounds-checking is desired.


---


### compiler : `gcc`
### title : `Issues with vectorization of min/max operations`
### open_at : `2018-12-18T16:32:46Z`
### last_modified_date : `2023-07-31T11:31:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88540
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
1st issue:

[code]
#define SIZE 2

void test(double* __restrict d1, double* __restrict d2, double* __restrict d3)
{
    for (int n = 0; n < SIZE; ++n)
    {
        d3[n] = d1[n] < d2[n] ? d1[n] : d2[n];
    }
}
[code]

When this is compiled with for SSE2, gcc produces non vectorized code:

[asm]
test(double*, double*, double*):
        vmovsd  xmm0, QWORD PTR [rdi]
        vminsd  xmm0, xmm0, QWORD PTR [rsi]
        vmovsd  QWORD PTR [rdx], xmm0
        vmovsd  xmm0, QWORD PTR [rdi+8]
        vminsd  xmm0, xmm0, QWORD PTR [rsi+8]
        vmovsd  QWORD PTR [rdx+8], xmm0
        ret
[/asm]

When SIZE is changed to 3 or greater, code gets vectorized properly. I thought that this may be some workaround for old CPU which was slower there, but this also happen when compiling with "-O3 -march=skylake". I also checked with SIZE 6, and got 1 AVX op and 2 scalar SSE ones. Looks that this is an off-by-one bug.

The same happen for code with other relational operators (>, <=, >=).

2nd issue: when compiling for AVX512, gcc does not use new instructions which use ZMM registers, it still generates code for YMM ones.


---


### compiler : `gcc`
### title : `Optimize symmetric range check`
### open_at : `2018-12-18T17:18:23Z`
### last_modified_date : `2021-11-17T09:23:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88542
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `normal`
### contents :
[code]
#include <math.h>

bool test1(double d, double max)
{
    return (d < max) && (d > -max);
}

bool test2(double d, double max)
{
    return fabs(d) < max;
}
[/code]

When code checks if some number d is in (or outside of) symmetric range like (-max, max), code from test1() can be replaced with one from test2(). This of course assumes that expression does not produce any side effects. This can be done nicely for floating point numbers stored in IEEE format, what leads to faster code:

[asm]
test1(double, double):
        vcomisd xmm1, xmm0
        jbe     .L6
        vxorpd  xmm1, xmm1, XMMWORD PTR .LC0[rip]
        vcomisd xmm0, xmm1
        seta    al
        ret
.L6:
        xor     eax, eax
        ret
test2(double, double):
        vandpd  xmm0, xmm0, XMMWORD PTR .LC1[rip]
        vcomisd xmm1, xmm0
        seta    al
        ret
[/asm]

For integer types stored in two's complement format similar change gives slower code. However on platforms which uses different integer format with dedicated sign bit this optimizations may be beneficial.


---


### compiler : `gcc`
### title : `missed optimization for vector comparisons`
### open_at : `2018-12-18T22:13:15Z`
### last_modified_date : `2019-01-30T17:09:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88547
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
typedef signed svec __attribute__((vector_size(16)));
typedef unsigned uvec __attribute__((vector_size(16)));

svec les(svec x, svec y) {
    return x <= y;
}

uvec leu(uvec x, uvec y) {
    return x <= y;
}

currently assemble to 

les:
        vpcmpgtd  %xmm1, %xmm0, %xmm0
        vpcmpeqd  %xmm1, %xmm1, %xmm1
        vpandn    %xmm1, %xmm0, %xmm0

leu:
        vmovdqa64 .LC0(%rip), %xmm2
        vpsubd    %xmm2, %xmm1, %xmm1
        vpsubd    %xmm2, %xmm0, %xmm0
        vpcmpgtd  %xmm1, %xmm0, %xmm0
        vpcmpeqd  %xmm1, %xmm1, %xmm1
        vpandn    %xmm1, %xmm0, %xmm0

By using the transformation min(x, y) == x we can produce

les:
        vpminsd   %xmm1, %xmm0, %xmm1
        vpcmpeqd  %xmm1, %xmm0, %xmm0

leu:
        vpminud   %xmm1, %xmm0, %xmm1
        vpcmpeqd  %xmm0, %xmm1, %xmm0

This can be used to reduce unsigned comparisons without requiring
the use of a constant bias vector.  At least when the given min insn
is available in the architecture.


---


### compiler : `gcc`
### title : `Inline built-in sinh, cosh, tanh for -ffast-math`
### open_at : `2018-12-19T21:10:00Z`
### last_modified_date : `2023-07-22T02:54:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88556
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
GCC should support inline code generation for sinh, cosh, tanh functions, under appropriate fast-math conditions.

glibc's bits/mathinline.h, for 32-bit non-SSE fast-math x86 only, has:

/* The argument range of the inline version of sinhl is slightly reduced.  */
__inline_mathcodeNP (sinh, __x, \
  register long double __exm1 = __expm1l (__fabsl (__x));                     \
  return 0.5 * (__exm1 / (__exm1 + 1.0) + __exm1) * __sgn1l (__x))

__inline_mathcodeNP (cosh, __x, \
  register long double __ex = __expl (__x);                                   \
  return 0.5 * (__ex + 1.0 / __ex))

__inline_mathcodeNP (tanh, __x, \
  register long double __exm1 = __expm1l (-__fabsl (__x + __x));              \
  return __exm1 / (__exm1 + 2.0) * __sgn1l (-__x))

We're moving away from such inlines in glibc, preferring to leave it to the compiler to inline standard functions under appropriate conditions.  This inlining probably only makes sense when expm1l / expl are themselves expanded inline (but in principle it's otherwise generic; note this x86 code uses long double, so avoiding reducing the argument range for built-in functions for narrower types).

flag_unsafe_math_optimizations should be required for all these expansions; the sinh one is specifically unsafe for infinite arguments so should also require flag_finite_math_only.


---


### compiler : `gcc`
### title : `[9 Regression] armv8_2-fp16-move-1.c and related regressions after r266385`
### open_at : `2018-12-20T10:45:42Z`
### last_modified_date : `2019-02-11T16:54:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88560
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
Created attachment 45266
Code generated for armv8_2-fp16-move-1.c with r266385

Several regressions were seen on arm-none-linux-gnueabihf and arm-none-eabi after r260385.

FAIL: gcc.target/arm/armv8_2-fp16-move-1.c scan-assembler-times 
ldrh\\tr[0-9]+ 2
FAIL: gcc.target/arm/armv8_2-fp16-move-1.c scan-assembler-times 
vld1\\.16\\t{d[0-9]+\\[[0-9]+\\]}, \\[r[0-9]+\\] 2
FAIL: gcc.target/arm/armv8_2-fp16-move-1.c scan-assembler-times 
vmov\\.f16\\ts[0-9]+, r[0-9]+ 2
FAIL: gcc.target/arm/fp16-aapcs-1.c scan-assembler 
vmov(\\.f16)?\\tr[0-9]+, s[0-9]+
FAIL: gcc.target/arm/fp16-aapcs-1.c scan-assembler vmov(\\.f16)?\\ts0, 
r[0-9]+
FAIL: gcc.target/arm/fp16-aapcs-3.c scan-assembler-times vmov\\tr[0-9]+, 
s[0-2] 2
FAIL: gcc.target/arm/fp16-aapcs-3.c scan-assembler-times vmov\\ts0, 
r[0-9]+ 2

Full command line used to compile and test armv8_2-fp16-move-1.c (done by make check-gcc):

bin/gcc armv8_2-fp16-move-1.c -fno-diagnostics-show-caret 
-fno-diagnostics-show-line-numbers -fdiagnostics-color=never -O2 
-mfpu=fp-armv8 -march=armv8.2-a+fp16 -mfloat-abi=hard -ffat-lto-objects 
-fno-ident -S -o armv8_2-fp16-move-1.s.


---


### compiler : `gcc`
### title : `Track relations between variable values`
### open_at : `2018-12-21T15:54:23Z`
### last_modified_date : `2021-12-13T01:44:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88569
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
This example comes from code which could be compiled for various CPUs, and had dedicated sections for AVX and SSE2. I left original ifdefs in comments. When 1st loop (for AVX) ends, following relations is true: (cnt - n <= 3). Similarly after 2nd loop this is true: (cnt - n <= 1). With such knowledge it is possible to optimize code of bar() to baz(). This eliminates two condition checks (after 2nd and 3rd loop), and one increment (for 3rd loop). It would be nice if gcc could perform such transformation automatically.

[code]
void foo(int n);

void bar(int cnt)
{
    int n = 0;
//#ifdef __AVX__
    for (; n < cnt - 3; n += 4)
        foo(n);
//#endif
//#ifdef __SSE2__
    for (; n < cnt - 1; n += 2)
        foo(n);
//#endif
    for (; n < cnt; n += 1)
        foo(n);
}

void baz(int cnt)
{
    int n = 0;
    for (; n < cnt - 3; n += 4)
        foo(n);
    if (n < cnt - 1)
    {
        foo(n);
        n += 2;
    }
    if (n < cnt)
        foo(n);
}
[/code]

[asm]
bar(int):
        push    r13
        push    r12
        mov     r12d, edi
        push    rbp
        lea     ebp, [rdi-3]
        push    rbx
        xor     ebx, ebx
        sub     rsp, 8
        test    ebp, ebp
        jle     .L5
.L2:
        mov     edi, ebx
        add     ebx, 4
        call    foo(int)
        cmp     ebx, ebp
        jl      .L2
        lea     eax, [r12-4]
        shr     eax, 2
        lea     ebx, [4+rax*4]
.L5:
        lea     ebp, [r12-1]
        cmp     ebp, ebx
        jle     .L3
        mov     edi, ebx
        lea     r13d, [rbx+2]
        call    foo(int)
        cmp     ebp, r13d
        jle     .L8
        mov     edi, r13d
        call    foo(int)
.L8:
        lea     edi, [r12-2]
        sub     edi, ebx
        mov     ebx, edi
        and     ebx, -2
        add     ebx, r13d
.L3:
        cmp     r12d, ebx
        jle     .L14
        mov     edi, ebx
        call    foo(int)
        lea     edi, [rbx+1]
        cmp     r12d, edi
        jg      .L17
.L14:
        add     rsp, 8
        pop     rbx
        pop     rbp
        pop     r12
        pop     r13
        ret
.L17:
        add     rsp, 8
        pop     rbx
        pop     rbp
        pop     r12
        pop     r13
        jmp     foo(int)
baz(int):
        push    r12
        mov     r12d, edi
        push    rbp
        lea     ebp, [rdi-3]
        push    rbx
        xor     ebx, ebx
        test    ebp, ebp
        jle     .L19
.L20:
        mov     edi, ebx
        add     ebx, 4
        call    foo(int)
        cmp     ebx, ebp
        jl      .L20
        lea     eax, [r12-4]
        shr     eax, 2
        lea     ebx, [4+rax*4]
.L19:
        lea     eax, [r12-1]
        cmp     eax, ebx
        jg      .L27
        cmp     ebx, r12d
        jl      .L28
.L25:
        pop     rbx
        pop     rbp
        pop     r12
        ret
.L27:
        mov     edi, ebx
        add     ebx, 2
        call    foo(int)
        cmp     ebx, r12d
        jge     .L25
.L28:
        mov     edi, ebx
        pop     rbx
        pop     rbp
        pop     r12
        jmp     foo(int)
[/asm]


---


### compiler : `gcc`
### title : `AVX512: when calculating logical expression with all values in kN registers, do not use GPRs`
### open_at : `2018-12-21T17:05:45Z`
### last_modified_date : `2021-08-29T20:30:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88571
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `enhancement`
### contents :
This is a side effect of finding Bug 88570. I have noticed that when gcc has to generate code for logical expression with all values already stored in kN registers, it moves them to GPRs, performs calculation on them and moved result back. Such situation may happen as a side effect of optimizations in gcc. It is also move convenient to use C/C++ operators to write expressions instead of intrinsics, so some people may prefer to use them. It probably can also happen as a side effect of interaction of code optimized by gcc with user code.

When logical expression is written using intrinsics, values stays in kN registers as expected.

Code below was compiled with -O3 -march=skylake-avx512. test1 and test2 are examples of code with C/C++ operators. test3 is an example of not introduced by gcc during optimization. This last example is also in Bug 88570, which I logged to fix inefficient optimizations.

[code]
#include <immintrin.h>

void test1(int*__restrict n1, int*__restrict n2,
    int*__restrict n3, int*__restrict n4)
{
    __m256i v = _mm256_loadu_si256((__m256i*)n1);
    __mmask8 m = _mm256_cmpgt_epi32_mask(v, _mm256_set1_epi32(1));
    m = ~m;
    _mm256_mask_storeu_epi32((__m256i*)n2, m, v);
}

void test2(int*__restrict n1, int*__restrict n2,
    int*__restrict n3, int*__restrict n4)
{
    __m256i v1 = _mm256_loadu_si256((__m256i*)n1);
    __m256i v2 = _mm256_loadu_si256((__m256i*)n1);
    __m256i v0 = _mm256_set1_epi32(2);
    __mmask8 m1 = _mm256_cmpgt_epi32_mask(v1, _mm256_set1_epi32(1));
    __mmask8 m2 = _mm256_cmpgt_epi32_mask(v2, _mm256_set1_epi32(2));
    __mmask8 m = ~(m1 | m2);
    _mm256_mask_storeu_epi32((__m256i*)n2, m, v1);
}

void test3(double*__restrict d1, double*__restrict d2,
    double*__restrict d3, double*__restrict d4)
{
    for (int n = 0; n < 4; ++n)
    {
        if (d1[n] > 0.0)
            d2[n] = d3[n];
        else
            d2[n] = d4[n];
    }
}
[/code]

[asm]
test1(int*, int*, int*, int*):
        vmovdqu64       ymm0, YMMWORD PTR [rdi]
        vpcmpgtd        k1, ymm0, YMMWORD PTR .LC0[rip]
        kmovb   eax, k1
        not     eax
        kmovb   k2, eax
        vmovdqu32       YMMWORD PTR [rsi]{k2}, ymm0
        vzeroupper
        ret
test2(int*, int*, int*, int*):
        vmovdqu64       ymm1, YMMWORD PTR [rdi]
        vpcmpgtd        k1, ymm1, YMMWORD PTR .LC0[rip]
        vpcmpgtd        k2, ymm1, YMMWORD PTR .LC1[rip]
        kmovb   edx, k1
        kmovb   eax, k2
        or      eax, edx
        not     eax
        kmovb   k3, eax
        vmovdqu32       YMMWORD PTR [rsi]{k3}, ymm1
        vzeroupper
        ret
test3(double*, double*, double*, double*):
        vmovupd ymm0, YMMWORD PTR [rdi]
        vxorpd  xmm1, xmm1, xmm1
        vcmppd  k1, ymm0, ymm1, 14
        vcmpltpd        ymm1, ymm1, ymm0
        kmovb   eax, k1
        not     eax
        vmovupd ymm2{k1}{z}, YMMWORD PTR [rdx]
        kmovb   k2, eax
        vmovupd ymm0{k2}{z}, YMMWORD PTR [rcx]
        vblendvpd       ymm0, ymm0, ymm2, ymm1
        vmovupd YMMWORD PTR [rsi], ymm0
        vzeroupper
        ret
.LC0:
        .long   1
        .long   1
        .long   1
        .long   1
        .long   1
        .long   1
        .long   1
        .long   1
.LC1:
        .long   2
        .long   2
        .long   2
        .long   2
        .long   2
        .long   2
        .long   2
        .long   2
[/asm]


---


### compiler : `gcc`
### title : `gcc got confused by different comparison operators`
### open_at : `2018-12-22T10:50:47Z`
### last_modified_date : `2023-06-09T22:28:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88575
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
In test() gcc is not able to determine that for a==b it does not have to evaluate 2nd comparison and can use value of a if 1st comparison is true. When operators are swapped like in test2() or are the same, code is optimized.

[code]
double test(double a, double b)
{
    if (a <= b)
        return a < b ? a : b;
    return 0.0;
}

double test2(double a, double b)
{
    if (a < b)
        return a <= b ? a : b;
    return 0.0;
}
[/code]

[asm]
test(double, double):
  vcomisd xmm1, xmm0
  jnb .L10
  vxorpd xmm0, xmm0, xmm0
  ret
.L10:
  vminsd xmm0, xmm0, xmm1
  ret

test2(double, double):
  vcmpnltsd xmm1, xmm0, xmm1
  vxorpd xmm2, xmm2, xmm2
  vblendvpd xmm0, xmm0, xmm2, xmm1
  ret
[/asm]


---


### compiler : `gcc`
### title : `simplification of multiplication by 1 or 0 fails`
### open_at : `2018-12-26T09:01:41Z`
### last_modified_date : `2019-07-01T07:54:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88598
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
g++ fails to optimize the code below
even with -Ofast https://godbolt.org/z/mYRgVX
independently of vectorization options https://godbolt.org/z/XMnCNz
clang optimizes (return zero for "foo" and v[1] for "bar") even for just
-ffinite-math-only -fno-signed-zeros -O2
  https://godbolt.org/z/KU5f-x

float foo(float const * __restrict__ v) {
  float j[5] = {0.,0.,0.,0.,0.};
  float ret=0.;
  for (int i=0; i<5; ++i) ret +=j[i]*v[i];
  return ret;
}


float bar(float const * __restrict__ v) {
  float j[5] = {0.,1.,0.,0.,0.};
  float ret=0.;
  for (int i=0; i<5; ++i) ret +=j[i]*v[i];
  return ret;
}


---


### compiler : `gcc`
### title : `optimization missed for saturation arithmetic add`
### open_at : `2018-12-26T17:22:06Z`
### last_modified_date : `2023-05-31T23:22:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88603
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `enhancement`
### contents :
example:

#include <inttypes.h>

uint32_t saturation_add(uint32_t a, uint32_t b)
{
    const uint64_t tmp = (uint64_t)a + b;
    if (tmp > UINT32_MAX)
    {
        return UINT32_MAX;
    }
    return tmp;
}

output:
        mov     edx, esi
        mov     eax, edi
        add     edi, esi # Why need to add two times here
        add     rax, rdx # and here ?
        mov     edx, 4294967295
        cmp     rax, rdx
        mov     eax, -1 # Why? edx already have this value. -1 and 4294967295 are same
        cmovbe  eax, edi
        ret

better do something like this:
        add     edi, esi
        mov     eax, -1
        cmovae  eax, edi
        ret


---


### compiler : `gcc`
### title : `vector extensions: Widening or conversion generates inefficient or scalar code.`
### open_at : `2018-12-27T02:27:18Z`
### last_modified_date : `2023-05-12T06:13:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88605
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
If you want to, say, convert a u32x2 vector to a u64x2 while avoiding intrinsics, good luck.

GCC doesn't have a builtin like __builtin_convertvector, and doing the conversion manually generates scalar code. This makes clean generic vector code difficult.

SSE and NEON both have plenty of conversion instructions, such as pmovzxdq or vmovl.32, but GCC will not emit them. 

typedef unsigned long long U64;
typedef U64 U64x2 __attribute__((vector_size(16)));
typedef unsigned int U32;
typedef U32 U32x2 __attribute__((vector_size(8)));

U64x2 vconvert_u64_u32(U32x2 v)
{
    return (U64x2) { v[0], v[1] };
}

x86_32:

Flags: -O3 -m32 -msse4.1
Clang Trunk (revision 350063)

vconvert_u64_u32:
        pmovzxdq        xmm0, qword ptr [esp + 4] # xmm0 = mem[0],zero,mem[1],zero
        ret

GCC (GCC-Explorer-Build) 9.0.0 20181225 (experimental)
convert_u64_u32:
        push    ebx
        sub     esp, 40
        movq    QWORD PTR [esp+8], mm0
        mov     ecx, DWORD PTR [esp+8]
        mov     ebx, DWORD PTR [esp+12]
        mov     DWORD PTR [esp+8], ecx
        movd    xmm0, DWORD PTR [esp+8]
        mov     DWORD PTR [esp+20], ebx
        movd    xmm1, DWORD PTR [esp+20]
        mov     DWORD PTR [esp+16], ecx
        add     esp, 40
        punpcklqdq      xmm0, xmm1
        pop     ebx
        ret
I can't even understand what is going on here, except it is wasting 44 bytes of stack for no good reason.

x86_64: 

Flags: -O3 -m64 -msse4.1

Clang:

vconvert_u64_u32:
        pmovzxdq        xmm0, xmm0      # xmm0 = xmm0[0],zero,xmm0[1],zero
        ret

GCC:
vconvert_u64_u32:
        movq    rax, xmm0
        movd    DWORD PTR [rsp-28], xmm0
        movd    xmm0, DWORD PTR [rsp-28]
        shr     rax, 32
        pinsrq  xmm0, rax, 1
        ret

ARMv7 NEON:
Flags: -march=armv7-a -mfloat-abi=hard -mfpu=neon -O3

Clang (with --target=arm-none-eabi):
vconvert_u64_u32:
        vmovl.u32       q0, d0
        bx      lr

arm-unknown-linux-gnueabi-gcc (GCC) 8.2.0:
vconvert_u64_u32:
        mov     r3, #0
        sub     sp, sp, #16
        add     r2, sp, #8
        vst1.32 {d0[0]}, [sp]
        vst1.32 {d0[1]}, [r2]
        str     r3, [sp, #4]
        str     r3, [sp, #12]
        vld1.64 {d0-d1}, [sp:64]
        add     sp, sp, #16
        bx      lr

aarch64 NEON:
Flags: -O3

Clang (with --target=aarch64-none-eabi):
vconvert_u64_u32:
        ushll   v0.2d, v0.2s, #0
        ret

aarch64-unknown-linux-gnu-gcc 8.2.0:

vconvert_u64_u32:
        umov    w1, v0.s[0]
        umov    w0, v0.s[1]
        uxtw    x1, w1
        uxtw    x0, w0
        dup     v0.2d, x1
        ins     v0.d[1], x0
        ret

Some other things include things like getting a standalone pmuludq.

In clang, this always generates pmuludq:
U64x2 pmuludq(U64x2 v1, U64x2 v2)
{
    return (v1 & 0xFFFFFFFF) * (v2 & 0xFFFFFFFF);
}

But GCC generates this:
pmuludq:
        movdqa  xmm2, XMMWORD PTR .LC0[rip]
        pand    xmm0, xmm2
        pand    xmm2, xmm1
        movdqa  xmm4, xmm2
        movdqa  xmm1, xmm0
        movdqa  xmm3, xmm0
        psrlq   xmm4, 32
        psrlq   xmm1, 32
        pmuludq xmm0, xmm4
        pmuludq xmm1, xmm2
        pmuludq xmm3, xmm2
        paddq   xmm1, xmm0
        psllq   xmm1, 32
        paddq   xmm3, xmm1
        movdqa  xmm0, xmm3
        ret
.LC0:
        .quad   4294967295
        .quad   4294967295

and that is the best code it generates. Much worse code is generated depending on how you write it.

Meanwhile, while it has some struggles with sse2 and x86_64, there is a reliable way to get Clang to generate pmuludq, and the NEON equivalent, vmull.u32, 
https://godbolt.org/z/H_tOi1


---


### compiler : `gcc`
### title : `missed opportunity in integer conditional`
### open_at : `2019-01-03T14:26:58Z`
### last_modified_date : `2022-11-28T22:00:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88676
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Take the following code:

int f(unsigned b)
{
  int r;
  if (b >= 2)
    __builtin_unreachable();
  switch (b) {
  case 0:
    r = 1;
    break;
  case 1:
    r = 2;
    break;
  default:
    r = 0;
    break;
  }
  return r;
}

Compiled using the current trunk gcc and gcc 8.2.1 with -O3 on x86_64 the following code is produced:

0000000000000000 <f>:
   0:	31 c0                	xor    %eax,%eax
   2:	83 ff 01             	cmp    $0x1,%edi
   5:	0f 94 c0             	sete   %al
   8:	ff c0                	inc    %eax
   a:	c3                   	retq   

This is quite good but it should be something like

    leal 1(%edi),%eax
    ret

The first three instructions test for 0 or 1 and load into %eax the values 0 or 1 respectively.  This should be just a move.


---


### compiler : `gcc`
### title : `[8/9/10/11 regression] We do terrible job optimizing IsHTMLWhitespace from Firefox`
### open_at : `2019-01-04T23:44:32Z`
### last_modified_date : `2020-12-02T07:38:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88702
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
compiling:

int IsHTMLWhitespace(int aChar) {                         
  return aChar == 0x0009 || aChar == 0x000A ||              
         aChar == 0x000C || aChar == 0x000D ||              
         aChar == 0x0020;                                             
}
q(int a,int b, int c)
{
  return IsHTMLWhitespace (a) && IsHTMLWhitespace (b) && IsHTMLWhitespace (c);
}

we do quite funny things by ipa-splitting IsHTMLWhitespace:

IsHTMLWhitespace (int aChar)
{
  unsigned int aChar.1_1;
  unsigned int _2;
  _Bool _3;
  _Bool _4;
  _Bool _5;
  int iftmp.0_6;
  int iftmp.0_8;

  <bb 2> [100.00%]:
  aChar.1_1 = (unsigned int) aChar_7(D);
  _2 = aChar.1_1 + 4294967287;
  _3 = _2 <= 1;
  _4 = aChar_7(D) == 12;
  _5 = _3 | _4;
  if (_5 != 0)
    goto <bb 4>; [46.00%]
  else
    goto <bb 3>; [54.00%]

  <bb 3> [54.00%]:
  iftmp.0_8 = IsHTMLWhitespace.part.0 (aChar_7(D));

  <bb 4> [100.00%]:
  # iftmp.0_6 = PHI <1(2), iftmp.0_8(3)>
  return iftmp.0_6;

}

this is partly caused by the fact that we are not able to optimize early the sequence of compares to shift. In Firefox later we fail to inline the functions and produce some terrible code which slows down rerf-reftest/dep-check-1.html Firefox benchmark


---


### compiler : `gcc`
### title : `[ARM][Generic Vector Extensions] float32x4/float64x2 vector operator overloads scalarize on NEON`
### open_at : `2019-01-05T03:02:57Z`
### last_modified_date : `2021-12-27T05:38:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88705
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
For some reason, GCC scalarizes float32x4_t and float64x2_t on ARM32 NEON when using vector extensions. 

typedef float f32x4 __attribute__((vector_size(16)));
typedef double f64x2 __attribute__((vector_size(16)));

f32x4 fmul (f32x4 v1, f32x4 v2)
{
   return v1 * v2;
}
f64x2 dmul (f64x2 v1, f64x2 v2)
{
   return v1 * v2;
}

Expected output:

arm-none-eabi-gcc (git commit 640647d4, not the latest) -O3 -S -march=armv7-a -mfloat-abi=hard -mfpu=neon

fmul:
    vmul.f32 q0, q0, q1
    bx lr
dmul:
    vmul.f64 d1, d1, d3
    vmul.f64 d0, d0, d2
    bx lr

Actual output:

fmul:
	vmov.32	r3, d0[0]
	sub	sp, sp, #16
	vmov	s12, r3
	vmov.32	r3, d2[0]
	vmov	s9, r3
	vmov.32	r3, d0[1]
	vmul.f32	s12, s12, s9
	vstr.32	s12, [sp]
	vmov	s13, r3
	vmov.32	r3, d2[1]
	vmov	s10, r3
	vmov.32	r3, d1[0]
	vmul.f32	s13, s13, s10
	vstr.32	s13, [sp, #4]
	vmov	s14, r3
	vmov.32	r3, d1[1]
	vmov	s15, r3
	vmov.32	r3, d3[0]
	vmov	s11, r3
	vmov.32	r3, d3[1]
	vmul.f32	s14, s14, s11
	vstr.32	s14, [sp, #8]
	vmov	s0, r3
	vmul.f32	s0, s15, s0
	vstr.32	s0, [sp, #12]
	vld1.64	{d0-d1}, [sp:64]
	add	sp, sp, #16
	bx	lr
dmul:
	push	{r4, r5, r6, r7}
	sub	sp, sp, #96
	vstr	d0, [sp, #64]
	vstr	d1, [sp, #72]
	vstr	d2, [sp, #48]
	vstr	d3, [sp, #56]
	vldr.64	d17, [sp, #64]
	vldr.64	d19, [sp, #48]
	vldr.64	d16, [sp, #72]
	vldr.64	d18, [sp, #56]
	vmul.f64	d17, d17, d19
	vmul.f64	d16, d16, d18
	vstr.64	d17, [sp, #32]
	ldrd	r0, [sp, #32]
	mov	r4, r0
	mov	r5, r1
	strd	r4, [sp]
	vstr.64	d16, [sp, #40]
	ldr	r2, [sp, #40]
	ldr	ip, [sp, #44]
	str	r2, [sp, #8]
	str	ip, [sp, #12]
	vld1.64	{d0-d1}, [sp:64]
	add	sp, sp, #96
	pop	{r4, r5, r6, r7}
	bx	lr

The same thing happens for other operators.

Oddly, according to Godbolt, GCC 4.5 actually did 32-bit float vectors properly, but regressed more and more each release starting in 4.6.


---


### compiler : `gcc`
### title : `Improve store-merging`
### open_at : `2019-01-05T14:40:41Z`
### last_modified_date : `2019-05-10T09:05:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88709
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
As shown in:
struct S { char buf[8]; };
void bar (struct S *);

void
foo (void)
{
  struct S s = {};
  s.buf[1] = 1;
  s.buf[3] = 2;
  bar (&s);
}

or

struct val_t
{
  char data[16];
};

void optimize_me (val_t);
void optimize_me3 (val_t, val_t, val_t);

void
good ()
{
  optimize_me ({ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 });
}

void
bad ()
{
  optimize_me ({ 1, 2, 3, 4, 5 });
}

void
why ()
{
  optimize_me ({ 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 });
}

void
srsly ()
{
  optimize_me3 ({ 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
		{ 11, 12, 13, 14, 15, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10 },
		{ 21, 22, 23, 24, 25, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20 });
}

void
srsly_not_one_missing ()
{
  optimize_me3 ({ 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
		{ 11, 12, 13, 14, 15, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10 },
		{ 21, 22, 23, 24, 25, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 11 });
}

there is room for improvement in store-merging.  In the first testcase, we ignore the clearing because !lhs_valid_for_store_merging_p, the lhs is in that case the whole VAR_DECL rather than a component of it.  And in the second testcase, we sometimes punt because of the same reason, sometimes because rhs_valid_for_store_merging_p is false.  Handling these = {} storage clearings (or perhaps even __builtin_memset calls) is something we could handle, though with extra care, we don't want to take apart those clears if it doesn't reduce the amount of needed stores.


---


### compiler : `gcc`
### title : `Optimization: mov edx, 0 not replaced with xor edx, edx in this case`
### open_at : `2019-01-05T23:46:20Z`
### last_modified_date : `2021-08-15T21:41:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88712
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
The code: 

---snip
int func(int val, const int *ptr)
{
  int res = val + 1234;
  if (res == *ptr)
  {
    res = 0;
  }
  return res;
}
---

generates the following ASM on all version of GCC back to 4.9.x:

---
func(int, int const*):
        lea     eax, [rdi+1234]
        mov     edx, 0
        cmp     DWORD PTR [rsi], eax
        cmove   eax, edx
        ret
---

The `mov edx, 0` is surprising to me. All the other compilers I tested (see https://godbolt.org/z/Nt9pKp for more details) use the common `xor edx, edx` (or `xor eax, eax`) idiom for zeroing edx.

Is this a missed optimization in the case of a cmov being generated, or am I missing something subtle?


---


### compiler : `gcc`
### title : `Vectorized code slow vs. flang`
### open_at : `2019-01-06T06:51:18Z`
### last_modified_date : `2023-07-22T03:08:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88713
### status : `REOPENED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `enhancement`
### contents :
Created attachment 45350
Fortran version of vectorization test.

I am attaching Fortran and C++ translations of a simple working example.

The C++ version is vectorized, while the Fortran version is not.

The code consists of two functions. One simply runs a for loop, calling the other function.
The function is vectorizable across loop iterations. g++ does this succcesfully.

However, gfortran does not, because it repacks data with
	call	_gfortran_internal_pack@PLT
so that it can no longer be vectorized across iterations.


I compiled with:

gfortran -Ofast -march=skylake-avx512 -mprefer-vector-width=512 -fno-semantic-interposition -shared -fPIC -S vectorization_test.cpp -o gfortvectorization_test.s

g++ -Ofast -march=skylake-avx512 -mprefer-vector-width=512 -shared -fPIC -S vectorization_test.cpp -o gppvectorization_test.s


LLVM (via flang and clang) successfully vectorizes both versions.


---


### compiler : `gcc`
### title : `Unnecessary vzeroupper`
### open_at : `2019-01-06T14:18:46Z`
### last_modified_date : `2019-01-08T17:41:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88717
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-1 tmp]$ cat x.i
typedef float __v16sf __attribute__ ((__vector_size__ (64)));
typedef float __m512 __attribute__ ((__vector_size__ (64), __may_alias__));

void
foo (float *p, __m512 x)
{
  *p = ((__v16sf)x)[0];
}
[hjl@gnu-cfl-1 tmp]$ gcc -mavx512f -S x.i -O2
[hjl@gnu-cfl-1 tmp]$ cat x.s
	.file	"x.i"
	.text
	.p2align 4,,15
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	vmovss	%xmm0, (%rdi)
	vzeroupper
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 8.2.1 20181215 (Red Hat 8.2.1-6)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 tmp]$ 

Since __m512 is passed to foo, vzeroupper isn't needed.


---


### compiler : `gcc`
### title : `Performance regression reload vs lra`
### open_at : `2019-01-08T08:48:42Z`
### last_modified_date : `2020-02-18T14:33:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88751
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
There is a big performance drop in OpenJ9 after they have updated from GCC 4.8.5 to GCC 7.3.0.

- The performance regression disappears after compiling the byte code interpreter loop with -mno-lra.
https://github.com/eclipse/openj9/blob/master/runtime/vm/BytecodeInterpreter.hpp

- The problem comes from the frequently accessed _pc and _sp variables being assigned to stack slots instead of registers. With GCC 4.8 both variables end up in hard regs.

- The problem can be seen on x86 as well as on S/390.

- In LRA the root cause of the problem is a threshold which prevents LRA from running the full register coloring step (ira.c):

   /* If there are too many pseudos and/or basic blocks (e.g. 10K
      pseudos and 10K blocks or 100K pseudos and 1K blocks), we will
      use simplified and faster algorithms in LRA.  */
  lra_simple_p = (ira_use_lra_p && max_reg_num () >= (1 << 26) /
  last_basic_block_for_fn (cfun));

  For the huge run() function in the byte code interpreter the numbers are:

  (gdb) p max_reg_num()
  $6 = 27089
  (gdb) p last_basic_block_for_fn(cfun)
  $7 = 4799

  Forcing GCC to run the full coloring pass makes the _pc and _sp variables to get hard regs assigned again.


As a quick workaround we might want to turn this threshold into a parameter.

Long-term it would be good if we could either enable the heuristic to estimate whether full coloring would be beneficial or improve the fallback coloring to cover such important cases.


---


### compiler : `gcc`
### title : `GCC unrolling is suboptimal`
### open_at : `2019-01-08T17:09:04Z`
### last_modified_date : `2019-11-27T15:33:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88760
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
One of the hot loops in 510.parest_r from SPEC2017 can be approximated through:
unsigned int *colnums;
double *val;

struct foostruct
{
  unsigned int rows;
  unsigned int *colnums;
  unsigned int *rowstart;
};

struct foostruct *cols;

void
foo (double *dst, const double *src)
{
  const unsigned int n_rows = cols->rows;
  const double *val_ptr = &val[cols->rowstart[0]];
  const unsigned int *colnum_ptr = &cols->colnums[cols->rowstart[0]];  

  double *dst_ptr = dst;
  for (unsigned int row=0; row<n_rows; ++row)
    {
      double s = 0.;
      const double *const val_end_of_row = &val[cols->rowstart[row+1]];
      while (val_ptr != val_end_of_row)
        s += *val_ptr++ * src[*colnum_ptr++];
      *dst_ptr++ = s;
    }
}


At -Ofast -mcpu=cortex-a57 on aarch64 GCC generates a tight FMA loop:
.L4:
        ldr     w3, [x7, x2, lsl 2]
        cmp     x6, x2
        ldr     d2, [x5, x2, lsl 3]
        add     x2, x2, 1
        ldr     d1, [x1, x3, lsl 3]
        fmadd   d0, d2, d1, d0
        bne     .L4


LLVM unrolls the loop more intelligently:
.LBB0_8:                                // %vector.body
                                        //   Parent Loop BB0_2 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
        ldp     w21, w22, [x20, #-8]
        ldr     d5, [x1, x21, lsl #3]
        ldp     d3, d4, [x7, #-16]
        ldr     d6, [x1, x22, lsl #3]
        ldp     w21, w22, [x20], #16
        fmadd   d2, d6, d4, d2
        fmadd   d1, d5, d3, d1
        ldr     d5, [x1, x21, lsl #3]
        ldr     d6, [x1, x22, lsl #3]
        add     x5, x5, #4              // =4
        adds    x19, x19, #2            // =2
        ldp     d3, d4, [x7], #32
        fmadd   d1, d5, d3, d1
        fmadd   d2, d6, d4, d2
        b.ne    .LBB0_8


With -funroll-loops GCC does do unrolling, but it does it differently:
<snip>
        ands    x12, x11, 7
        beq     .L70
        cmp     x12, 1
        beq     .L55
        cmp     x12, 2
        beq     .L57
        cmp     x12, 3
        beq     .L59
        cmp     x12, 4
        beq     .L61
        cmp     x12, 5
        beq     .L63
        cmp     x12, 6
        bne     .L72
.L65:
        ldr     w14, [x4, x2, lsl 2]
        ldr     d3, [x3, x2, lsl 3]
        add     x2, x2, 1
        ldr     d4, [x1, x14, lsl 3]
        fmadd   d0, d3, d4, d0
.L63:
        ldr     w5, [x4, x2, lsl 2]
        ldr     d5, [x3, x2, lsl 3]
        add     x2, x2, 1
        ldr     d6, [x1, x5, lsl 3]
        fmadd   d0, d5, d6, d0
.L61:
        ldr     w9, [x4, x2, lsl 2]
        ldr     d7, [x3, x2, lsl 3]
        add     x2, x2, 1
        ldr     d16, [x1, x9, lsl 3]
        fmadd   d0, d7, d16, d0
<snip>

On the whole of 510.parest_r this makes LLVM about 6% faster than GCC on Cortex-A57.

Perhaps this can be used as a motivating testcase to move the GCC unrolling discussions forward?


---


### compiler : `gcc`
### title : `powerpc64le-linux-gnu sub-optimal code generation for builtin atomic ops`
### open_at : `2019-01-09T05:47:54Z`
### last_modified_date : `2022-03-08T16:21:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88765
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
gcc version 8.2.0 (Debian 8.2.0-4) 

Linux uses a lot of non-trivial operations, and implementing them with compare_exchange results in sub-optimal code generation. A common one is add_unless_zero, which is commonly used with RCU to take a reference count, or fail if the last reference had already gone (which is a very rare case).

---
#include <stdbool.h>
  
bool add_unless_zero(unsigned long *mem, unsigned long inc)
{
        unsigned long val;
        do {
                val = __atomic_load_n(mem, __ATOMIC_RELAXED);
                if (__builtin_expect(val == 0, false))
                        return false;
        } while (__builtin_expect(!__atomic_compare_exchange_n(mem,
                                &val, val + inc, true,
                                __ATOMIC_RELAXED, __ATOMIC_RELAXED), false));

        return true;
}
---
This compiles to:

add_unless_zero:
.L4:
        ld 10,0(3)
        cmpdi 7,10,0
        add 8,10,4
        beq 7,.L5
        ldarx 9,0,3
        cmpd 0,9,10
        bne 0,.L3
        stdcx. 8,0,3
.L3:
        bne 0,.L4
        li 3,1
        blr
.L5:
        li 3,0
        blr

Better would be

add_unless_zero:
.L4:
        ldarx 9,0,3
        cmpdi 0,9,0
        add 9,9,4
        bne 0,.L5
        stdcx. 8,0,3
        bne 0,.L4
        li 3,1
        blr
.L5:
        li 3,0
        blr

Using extended inline asm to implement these is an adequate workaround. Unfortunately that does not work on 128 bit powerpc because no register pair constraint, and much worse code generation with builtins. Changing types to __int128_t gives a bad result:

add_unless_zero:
        lq 10,0(3)
        mr 6,3
        or. 9,10,11
        addc 3,11,4
        mr 7,11
        adde 11,10,5
        beq 0,.L16
        std 28,-32(1)
        std 29,-24(1)
        std 30,-16(1)
        std 31,-8(1)
.L12:
        lqarx 28,0,6
        xor 9,29,7
        xor 10,28,10
        or. 9,9,10
        bne 0,.L4
        mr 30,11
        mr 31,3
        stqcx. 30,0,6
.L4:
        mfcr 3,128
        rlwinm 3,3,3,1
        bne 0,.L17
.L2:
        ld 28,-32(1)
        ld 29,-24(1)
        ld 30,-16(1)
        ld 31,-8(1)
        blr
.L17:
        lq 10,0(6)
        or. 9,10,11
        addc 3,11,4
        mr 7,11
        adde 11,10,5
        bne 0,.L12
        li 3,0
        b .L2
.L16:
        li 3,0
        blr

Closer to ideal would be

add_unless_zero:
.Lagain:
       lqarx   6,0,3
       or.     8,6,7
       addc    6,6,4
       adde    7,7,5
       beq     0,.Lfail
       stqcx.  6,0,3
       bne     0,.Lagain
       li      3,1
       blr
.Lfail:
       li      3,0
       blr


---


### compiler : `gcc`
### title : `Redundant load opt. or CSE pessimizes code`
### open_at : `2019-01-09T12:46:34Z`
### last_modified_date : `2021-06-07T05:13:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88770
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `8.2.1`
### severity : `normal`
### contents :
For this code (-xc -std=c99 or -xc++ -std=c++17):

    struct guu { int a; int b; float c; char d; };

    extern void test(struct guu);

    void caller()
    {
        test( (struct guu){.a = 3, .b = 5, .c = 7, .d = 9} );
        test( (struct guu){.a = 3, .b = 5, .c = 7, .d = 9} );
    }

CSE (or some other form of redundant loads optimization) pessimizes the code. Problem occurs on optimization levels -O1 and higher, including -Os.

If the function "caller" calls test() just once, the resulting code is (-O3 -fno-optimize-sibling-calls, stack alignment/push/pops omitted for brevity):

        movabs  rdi, 21474836483
        movabs  rsi, 39743127552
        call    test

If "caller" calls test() twice, the code is a lot longer and not just twice as long. (Stack alignment/push/pops omitted for brevity):

        movabs  rbp, 21474836483
        mov     rdi, rbp
        movabs  rbx, 38654705664
        mov     rsi, rbx
        or      rbx, 1088421888
        or      rsi, 1088421888
        call    test
        mov     rsi, rbx
        mov     rdi, rbp
        call    test

If we change caller() such that the parameters in the two calls are not identical:

    void caller()
    {
        test( (struct guu){.a = 3, .b = 5, .c = 7, .d = 9} );
        test( (struct guu){.a = 3, .b = 6, .c = 7, .d = 10} );
    }

The generated code is optimal again as expected:

        movabs  rdi, 21474836483
        movabs  rsi, 39743127552
        call    test
        movabs  rdi, 25769803779
        movabs  rsi, 44038094848
        call    test

The problem in the first examples is that the compiler sees that the same parameter is used twice, and it tries to save it in a callee-saves register, in order to reuse the same values on the second call. However re-initializing the registers from scratch would have been more efficient.

The problem occurs on GCC versions 4.8.1 and newer. It does not occur in GCC version 4.7.4, which generated different code that is otherwise inefficient.

For reference, the problem also exists in Clang versions 3.5 and newer, but not in versions 3.4 and earlier.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Optimize std::string assignment`
### open_at : `2019-01-09T20:48:17Z`
### last_modified_date : `2023-07-07T10:34:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88775
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
#include <bits/c++config.h>
#undef _GLIBCXX_EXTERN_TEMPLATE
#define _GLIBCXX_EXTERN_TEMPLATE 0
#include<string>
__attribute__((flatten))
std::string f(){
  std::string s;
  s="hello";
  return s;
}

Yes, I have to go through some lengths to convince gcc to at least try to optimize...

With gcc-7, I get

  <bb 2> [14.44%]:
  _3 = &MEM[(struct basic_string *)s_2(D)].D.21635._M_local_buf;
  MEM[(struct _Alloc_hider *)s_2(D)]._M_p = _3;
  MEM[(size_type *)s_2(D) + 8B] = 0;
  MEM[(char_type &)s_2(D) + 16] = 0;
  if (_3 != "hello")
    goto <bb 3>; [75.00%]
  else
    goto <bb 4>; [25.00%]

  <bb 3> [1.43%]:
  __builtin_memcpy (_3, "hello", 5);
  goto <bb 5>; [100.00%]

  <bb 4> [0.97%]:
  __builtin_memcpy ("hello", &MEM[(void *)"hello" + 5B], 5);

  <bb 5> [14.43%]:
  MEM[(size_type *)s_2(D) + 8B] = 5;
  MEM[(char_type &)s_2(D) + 21] = 0;
  return s_2(D);

which is kind of OK. It would be much better if we folded _3 != "hello", but it is already small enough.

With gcc-9, I get something that starts with

  __x.7_6 = (long unsigned int) "hello";
  __y.8_7 = (long unsigned int) _3;
  if (__x.7_6 < __y.8_7)
    goto <bb 4>; [50.00%]
  else
    goto <bb 3>; [50.00%]

  <bb 3> [local count: 38463891]:
  if (__x.7_6 > __y.8_7)
    goto <bb 4>; [50.00%]
  else
    goto <bb 5>; [50.00%]

ifcombine would kindly turn this into __x.7_6 != __y.8_7, but it doesn't look like this yet when ifcombine runs. We also have equivalent blocks (reached by goto, not fallthrough)

  <bb 4> [local count: 19039626]:
  __builtin_memcpy (_3, "hello", 5);
  goto <bb 16>; [100.00%]

  <bb 6> [local count: 3173271]:
  __builtin_memcpy (_3, "hello", 5);
  goto <bb 16>; [100.00%]

  <bb 16> [local count: 114817586]:
  # prephitmp_14 = PHI <pretmp_16(13), pretmp_25(15), _3(4), _3(8), _3(6), _3(14)>

that we fail to merge. In the end, we have 4 times more code than we used to...

This is most likely caused by a change in libstdc++, but I am categorizing it as tree-optimization because I believe we need some improvements there, whatever libstdc++ decides to do.


---


### compiler : `gcc`
### title : `Odd Complex float load`
### open_at : `2019-01-09T21:31:28Z`
### last_modified_date : `2023-06-25T22:15:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88778
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-1 fp16-9]$ cat y.i
_Complex float
foo (_Complex float *p)
{
  return *p;
}
[hjl@gnu-cfl-1 fp16-9]$ gcc -S -O2 y.i
[hjl@gnu-cfl-1 fp16-9]$ cat y.s
	.file	"y.i"
	.text
	.p2align 4,,15
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movss	4(%rdi), %xmm0
	movss	(%rdi), %xmm1
	movss	%xmm0, -4(%rsp)
	movss	%xmm1, -8(%rsp)
	movq	-8(%rsp), %xmm0
	ret
	.cfi_endproc

A single load should be used.


---


### compiler : `gcc`
### title : `Middle end is missing some optimizations about unsigned`
### open_at : `2019-01-10T06:44:04Z`
### last_modified_date : `2023-08-05T17:11:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88784
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
For both operands are unsigned, the following optimizations are valid, and missing:
1. X > Y && X != 0 --> X > Y
2. X > Y || X != 0 --> X != 0
3. X <= Y || X != 0 --> true
4. X <= Y || X == 0 --> X <= Y
5. X > Y && X == 0 --> false

unsigned foo(unsigned x, unsigned y) { return x > y && x != 0; }
should fold to x > y, but I found we haven't done it right now.
I compile the code with the following command.
g++ unsigned.cpp -Ofast -c -S -o unsigned.s -fdump-tree-all


---


### compiler : `gcc`
### title : `Expand vector copysign (and xorsign) operations in the vectoriser`
### open_at : `2019-01-10T09:42:13Z`
### last_modified_date : `2019-01-11T12:51:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88786
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Currently every target defines the copysign optab for vector modes to emit very similar sequences of extracting the sign bit in RTL. This leads to almost identical code for AArch64 Adv SIMD, SVE, aarch32 NEON etc.

We should teach the vectoriser to expand a vector copysign operation at the tree level to benefit from more optimisations early on. Care needs to be taken to make sure the xorsign optimisation (currently done late in widen_mult) still triggers for vectorised code. This will allow us to a lot of duplicate code in the MD patterns and only implement them if the target can actually do a smarter sequence than the default.

This is similar in principle to the multiplication-by-constant expansion we already do in tree-vect-patterns.c

See, for example, the gcc.target/aarch64/vect-xorsign_exec.c testcase for the kind of input for this.


---


### compiler : `gcc`
### title : `-fstack-protector* kills RTL DSE opportunities`
### open_at : `2019-01-10T18:32:31Z`
### last_modified_date : `2019-01-14T13:44:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88796
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Marek has noticed that the gcc.target/i386/pr87370.c testcase FAILs when built with -fstack-protector-strong (
make check-gcc RUNTESTFLAGS='--target_board=unix\{-m64,-m64/-fstack-protector-strong\} i386.exp=pr87370.c'
).  It isn't a wrong-code, but seems quite severe anyway.  Before the RTL DSE pass, the dumps look roughly comparable, except that there is the store to the stack canary at the beginning and read from the stack canary at the end of the function.  In DSE dump for f2 function, one can see without -fstack-protector-strong:
starting the processing of deferred insns
ending the processing of deferred insns
df_analyze called

**scanning insn=5
mems_found = 0, cannot_delete = true
     
**scanning insn=14
  mem: (plus:DI (reg/f:DI 19 frame)
    (const_int -16 [0xfffffffffffffff0]))

   after canon_rtx address: (plus:DI (reg/f:DI 19 frame)
    (const_int -16 [0xfffffffffffffff0]))
  gid=0 offset=-16
 processing const base store gid=0[-16..0)
mems_found = 1, cannot_delete = false

**scanning insn=11
mems_found = 0, cannot_delete = true

**scanning insn=12
mems_found = 0, cannot_delete = false
Locally deleting insn 14
deferring deletion of insn with uid = 14.
group 0 is frame related group 0(16+0): n 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 p
starting the processing of deferred insns
ending the processing of deferred insns
df_analyze called
df_worklist_dataflow_doublequeue: n_basic_blocks 3 n_edges 2 count 3 (    1)

but with -fstack-protector-strong instead:

starting the processing of deferred insns
ending the processing of deferred insns
df_analyze called

**scanning insn=4
  mem: (const_int 40 [0x28])

   after canon_rtx address: (const_int 40 [0x28])

   after cselib_expand address: (const_int 40 [0x28])

   after canon_rtx address: (const_int 40 [0x28])
  varying cselib base=1:4426 offset = 0
 processing cselib load mem:(mem/f:DI (const_int 40 [0x28]) [4 MEM[(<address-space-1> long unsigned int *)40B]+0 S8 A64 AS1])
  mem: (plus:DI (reg/f:DI 19 frame)
    (const_int -8 [0xfffffffffffffff8]))

   after canon_rtx address: (plus:DI (reg/f:DI 19 frame)
    (const_int -8 [0xfffffffffffffff8]))
  gid=0 offset=-8
 processing const base store gid=0[-8..0)
mems_found = 1, cannot_delete = true

**scanning insn=7
mems_found = 0, cannot_delete = true

**scanning insn=24
  mem: (plus:DI (reg/f:DI 19 frame)
    (const_int -32 [0xffffffffffffffe0]))

   after canon_rtx address: (plus:DI (reg/f:DI 19 frame)
    (const_int -32 [0xffffffffffffffe0]))
  gid=0 offset=-32
 processing const base store gid=0[-32..-16)
    trying store in insn=4 gid=0[-8..0)
mems_found = 1, cannot_delete = false

**scanning insn=13
mems_found = 0, cannot_delete = true
**scanning insn=15
  mem: (const_int 40 [0x28])

   after canon_rtx address: (const_int 40 [0x28])

   after cselib_expand address: (const_int 40 [0x28])

   after canon_rtx address: (const_int 40 [0x28])
  varying cselib base=1:4426 offset = 0
 processing cselib load mem:(mem/f:DI (const_int 40 [0x28]) [4 MEM[(<address-space-1> long unsigned int *)40B]+0 S8 A64 AS1])
 processing cselib load against insn 24
removing from active insn=24 has store
 processing cselib load against insn 4
removing from active insn=4 has store
 adding wild read, volatile or barrier.
mems_found = 0, cannot_delete = true

**scanning insn=16
mems_found = 0, cannot_delete = true

**scanning insn=17
  mem: (symbol_ref:DI ("__stack_chk_fail") [flags 0x41]  <function_decl 0x7f43b618b500 __stack_chk_fail>)

   after canon_rtx address: (symbol_ref:DI ("__stack_chk_fail") [flags 0x41]  <function_decl 0x7f43b618b500 __stack_chk_fail>)
  gid=1 offset=0
 processing const load gid=1[0..1)

**scanning insn=20
mems_found = 0, cannot_delete = false
group 0 is frame related group 0(24+0): n 1, 2, 3, 4, 5, 6, 7, 8, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32 p
group 1(0+0): n  p
starting the processing of deferred insns
ending the processing of deferred insns
df_analyze called
doing global processing
df_worklist_dataflow_doublequeue: n_basic_blocks 5 n_edges 4 count 5 (    1)

So, the
(insn 15 13 16 2 (parallel [
            (set (reg:CCZ 17 flags)
                (unspec:CCZ [
                        (mem/v/f/c:DI (plus:DI (reg/f:DI 19 frame)
                                (const_int -8 [0xfffffffffffffff8])) [3 D.1950+0 S8 A64])
                        (mem/f:DI (const_int 40 [0x28]) [4 MEM[(<address-space-1> long unsigned int *)40B]+0 S8 A64 AS1])
                    ] UNSPEC_SP_TEST))
            (clobber (scratch:DI))
        ]) "pr87370.c":30:1 978 {stack_protect_test_di}
     (nil))
stack canary read invalidates the opportunity to remove the dead
(insn 24 7 13 2 (set (mem/c:V1TI (plus:DI (reg/f:DI 19 frame)
                (const_int -32 [0xffffffffffffffe0])) [5 D.1928+0 S16 A128])
        (reg:V1TI 86)) "pr87370.c":29:10 -1
     (expr_list:REG_DEAD (reg:V1TI 86)
        (nil)))
store.  Any thoughts how we could arrange for RTL DSE to understand how the ssp  set and test patterns work and the test pattern only makes the ssp canary set insn necessary and no other stores?


---


### compiler : `gcc`
### title : `[9 Regression] Unneeded branch added when function is inlined (function runs faster if not inlined)`
### open_at : `2019-01-10T20:42:08Z`
### last_modified_date : `2022-05-27T08:24:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88797
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `normal`
### contents :
Consider:

void use(unsigned);
bool f(unsigned x, unsigned y) {
    return x < 1111 + (y <= 2222);
}
void test_f(unsigned x, unsigned y) {
    for (unsigned i = 0; i < 3333; ++i)
        use(f(x++, y++));
}

The generated code for f seems fine and the there's no branch to test y <= 2222:

f(unsigned int, unsigned int):
  xorl %eax, %eax
  cmpl $2222, %esi
  setbe %al
  addl $1111, %eax
  cmpl %edi, %eax
  seta %al
  ret

However, when f is inlined in test_f, a branch is introduced to decide whether x should be compared to 1111 or 1112 (code cut for brevity)

test_f(unsigned int, unsigned int):
  [...]
  jmp .L6
.L14:
  cmpl $1111, %eax
.L12:
  [...]
.L6:
  [...]
  cmpl $2222, %ebx
  jbe .L14
  cmpl $1110, %eax
  jmp .L12
  [...]

See https://godbolt.org/z/_EC992 use -O3.

This seems to be a regression: it used to be OK up to 6.3 and then degraded in 7.1 (according to godbolt).


---


### compiler : `gcc`
### title : `AVX512BW code does not use bit-operations that work on mask registers`
### open_at : `2019-01-10T22:59:10Z`
### last_modified_date : `2022-02-08T02:02:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88798
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `normal`
### contents :
Hi!

AVX512BW-related issue: the C compiler generates superfluous moves from 64-bit
mask registers to 64-bit GPRs and then performs basic bit-ops, while the
AVX512BW supports bit-ops for mask registers (instructions: korq, kandq, kxorq).

I guess the main reason is C does not define a bit-or for type __mask64
and there's always an implicit conversion to uint64_t.

Below is a sample program compiled for Cannon Lake --- the CPU does have
(at least) AVX512BW, AVX512VBMI and AVX512VL.

---perf.c---
#include <immintrin.h>
#include <stdint.h>

uint64_t any_whitespace(__m512i string) {
    return _mm512_cmpeq_epu8_mask(string, _mm512_set1_epi8(' '))
         | _mm512_cmpeq_epu8_mask(string, _mm512_set1_epi8('\n'))
         | _mm512_cmpeq_epu8_mask(string, _mm512_set1_epi8('\r'));
}
---eof--

$ gcc --version
gcc (Debian 8.2.0-13) 8.2.0

$ gcc perf.c -O3 -march=cannonlake -S
$ cat perf.s # redacted
any_whitespace:
	vpcmpub	$0, .LC0(%rip), %zmm0, %k1
	vpcmpub	$0, .LC1(%rip), %zmm0, %k2
	vpcmpub	$0, .LC2(%rip), %zmm0, %k3
	kmovq	%k1, %rcx
	kmovq	%k2, %rdx
	orq	%rcx, %rdx
	kmovq	%k3, %rax
	orq	%rdx, %rax
	vzeroupper
	ret

I'd rather expect to get something like:

any_whitespace:
	vpcmpub	$0, .LC0(%rip), %zmm0, %k1
	vpcmpub	$0, .LC1(%rip), %zmm0, %k2
	vpcmpub	$0, .LC2(%rip), %zmm0, %k3
	korq	%k1, %k2, %k1
	korq	%k1, %k3, %k3
	kmovq	%k3, %rax
	vzeroupper
	ret

best regards
Wojciech


---


### compiler : `gcc`
### title : `[9 Regression] Performance regression on 473.astar on aarch64`
### open_at : `2019-01-11T11:38:47Z`
### last_modified_date : `2019-01-15T13:52:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88801
### status : `RESOLVED`
### tags : `missed-optimization, needs-bisection`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
We've noticed a performance regression recently on 473.astar on aarch64 (a Cortex-A57 platform).
On our systems we see about 8% slowdown and I also see a 4% slowdown (among other similar slowdowns) at:
https://lnt.opensuse.org/db_default/v4/SPEC/1947
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=13.260.0&highlight_run=1947

I suspect this is something to do with the recent inlining tweaks.


---


### compiler : `gcc`
### title : `bitwise operators on AVX512 masks fail to use the new mask instructions`
### open_at : `2019-01-11T16:28:12Z`
### last_modified_date : `2023-10-12T03:18:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88808
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Test case (https://godbolt.org/z/gyCN12):
#include <x86intrin.h>

using V [[gnu::vector_size(16)]] = float;

auto f(V x) {
    auto mask = _mm_fpclass_ps_mask(x, 16) | _mm_fpclass_ps_mask(x, 8);
    return _mm_mask_blend_ps(mask, x, x + 1);
}

auto g(V x) {
    auto mask = _kor_mask8(_mm_fpclass_ps_mask(x, 16), _mm_fpclass_ps_mask(x, 8));
    return _mm_mask_blend_ps(mask, x, x + 1);
}

Function f should compile to the same code as g does, i.e. use korb instead of kmovb + orl + kmovb. Similar test cases can be constructed for kxor, kand, and kandn as well as for masks of 8 and 16 bits (likely for 32 and 64 as well, but I have not tested it). For kand it's a bit trickier to trigger, but e.g. the following shows it:

__mmask8 foo = 0;
auto f(V x) {
    auto mask0 = _mm_fpclass_ps_mask(x, 16);
    auto mask1 = _mm_fpclass_ps_mask(x, 8);
    foo = mask0 | mask1;
    return _mm_mask_blend_ps(mask0 & mask1, x, x + 1);
}


---


### compiler : `gcc`
### title : `do not use rep-scasb for inline strlen/memchr`
### open_at : `2019-01-11T18:00:57Z`
### last_modified_date : `2021-04-07T08:25:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88809
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Performance difference between libc strlen and x86 rep-scasb has grown too large and seems unlikely to improve anytime soon.

On most x86 cores, microcode for rep-scasb is not too sophisticated and runs at 0.5 bytes per cycle or worse (according to Agner Fog's research; with SkylakeX managing 1 b/c), plus some overhead for entering/leaving the microcode loop (I think on the order of 20 cycles, but don't have exact info).

Whereas libc strlen typically has small overhead for short strings and uses register-wide operations on long strings, sustaining on the order of 4-8 b/c only with integer registers or even in the ballpark of 16-64 b/c with SSE/AVX (sorry, don't have exact figures here).

A call to strlen is also shorter by itself (rep-scasb needs extra instructions to setup %rax and fixup %rcx).
(although to be fair, a call to strlen prevents use of redzone and clobbers more registers)

Therefore I suggest we don't use rep-scasb for inline strlen anymore by default (we currently do at -Os). This is in part motivated by PR 88793 and the Redhat bug referenced from there.


---


### compiler : `gcc`
### title : `snprintf less optimal than sprintf for %s with big enough destination`
### open_at : `2019-01-11T23:13:41Z`
### last_modified_date : `2019-01-12T18:09:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88813
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The discussion of the background on pr88793 (https://bugzilla.redhat.com/show_bug.cgi?id=1480664) made me realize that GCC could do a better job optimizing some common snprintf calls.

Specifically, it transforms calls to sprintf(d, "%s", s) to strcpy(d, s), and it similarly transforms calls to snprintf(d, sizeof d, "%s", s) to memcpy(d, s, strlen(s) + 1) if it can tell that strlen(s) is less than sizeof(d).

Unfortunately, it can only tell that for constant strings, and it doesn't consider array sizes.  It should be able to both (a) track string lengths (by relying on the strlen pass), and also (b) make use of array sizes and transform snprintf(d, sizeof d, "%s", s) to strcpy(d, s) whenever sizeof(s) <= sizeof(d).  (a) is planned for GCC 10.  (b) 

Below is a test case where GCC could emit optimal code for both functions:

$ cat z.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout z.c
char d[8];
char s[8];

void f (void)
{
  __builtin_sprintf (d, "%s", s);   // optimized
}


void g (void)
{
  __builtin_snprintf (d, sizeof d, "%s", s);   // not optimized
}

;; Function f (f, funcdef_no=0, decl_uid=1908, cgraph_uid=1, symbol_order=2)

f ()
{
  <bb 2> [local count: 1073741824]:
  __builtin_strcpy (&d, &s); [tail call]
  return;

}



;; Function g (g, funcdef_no=1, decl_uid=1911, cgraph_uid=2, symbol_order=3)

g ()
{
  <bb 2> [local count: 1073741824]:
  __builtin_snprintf (&d, 8, "%s", &s); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `transform snprintf into memccpy`
### open_at : `2019-01-11T23:53:40Z`
### last_modified_date : `2021-02-15T15:34:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88814
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
In addition to bug 88813, prompted by the discussion of the background on pr88793 (https://bugzilla.redhat.com/show_bug.cgi?id=1480664), another optimization opportunity is to replace snprintf(d, n, "%s", s) calls with non-zero n with:

  memccpy(d, s, 0, n - 1);
  d[n - 1] = 0;

Memccpy is not a standard C function but it is in POSIX so GCC configury would have to detect support for it in order to enable it.  GCC also doesn't recognize memccpy as a built-in so adding such support could yield additional improvements.


---


### compiler : `gcc`
### title : `Inefficient update of the first element of vector registers`
### open_at : `2019-01-13T18:55:42Z`
### last_modified_date : `2019-06-24T14:44:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88828
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-cfl-1 pr88778]$ cat u4.i
typedef float __v4sf __attribute__ ((__vector_size__ (16)));
typedef float __m128 __attribute__ ((__vector_size__ (16), __may_alias__));

__m128
foo (__m128 x, float f)
{
  __m128 y =  __extension__ (__m128)(__v4sf)
  { f, ((__v4sf) x)[1], ((__v4sf) x)[2], ((__v4sf) x)[3] };
  return y;
}
[hjl@gnu-cfl-1 pr88778]$ gcc -S -O2 u4.i
[hjl@gnu-cfl-1 pr88778]$ cat u4.s
	.file	"u4.i"
	.text
	.p2align 4,,15
	.globl	foo
	.type	foo, @function
foo:
.LFB0:
	.cfi_startproc
	movaps	%xmm0, %xmm4
	movaps	%xmm0, %xmm3
	shufps	$85, %xmm0, %xmm4
	unpckhps	%xmm0, %xmm3
	unpcklps	%xmm4, %xmm1
	shufps	$255, %xmm0, %xmm0
	unpcklps	%xmm0, %xmm3
	movaps	%xmm1, %xmm0
	movlhps	%xmm3, %xmm0
	ret
	.cfi_endproc
.LFE0:
	.size	foo, .-foo
	.ident	"GCC: (GNU) 8.2.1 20190109 (Red Hat 8.2.1-7)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-cfl-1 pr88778]$ 

A simple movss will do.


---


### compiler : `gcc`
### title : `[SVE] Redundant moves for WHILELO-based loops`
### open_at : `2019-01-14T09:59:26Z`
### last_modified_date : `2019-07-27T09:54:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88833
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Compiling this function with -O3 -march=armv8-a+sve:

subroutine foo(x, y, z)
  real :: x(100), y(100), z(100)
  x = y + z(1)
end subroutine foo

gives:

foo_:
.LFB0:
        .cfi_startproc
        mov     x4, 100
        mov     x5, x4        // Redundant
        mov     x3, 0
        ptrue   p1.s, all
        whilelo p0.s, xzr, x4
        ld1rw   z1.s, p1/z, [x2]
        .p2align 3,,7
.L2:
        ld1w    z0.s, p0/z, [x1, x3, lsl 2]
        fadd    z0.s, z0.s, z1.s
        st1w    z0.s, p0, [x0, x3, lsl 2]
        incw    x3
        whilelo p0.s, x3, x5
        bne     .L2
        ret
        .cfi_endproc

There's no need for the move here.  We should just be able to use x4 for both WHILELOs.

Although the move itself shouldn't be expensive in context, it suggests that the RA isn't seeing an accurate picture, which could hurt in more complex cases.


---


### compiler : `gcc`
### title : `[SVE] Poor addressing mode choices for LD2 and ST2`
### open_at : `2019-01-14T10:21:31Z`
### last_modified_date : `2019-10-18T08:37:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88834
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Compiling this code with -O3 -march=armv8-a+sve:

void
f (int *restrict x, int *restrict y, int *restrict z, int n)
{
  for (int i = 0; i < n; i += 2)
    {
      x[i] = y[i] + z[i];
      x[i + 1] = y[i + 1] - z[i + 1];
    }
}

gives:

f:
.LFB0:
        .cfi_startproc
        cmp     w3, 0
        ble     .L1
        sub     w4, w3, #1
        cntw    x3
        ptrue   p1.s, all
        lsr     w4, w4, 1
        add     w4, w4, 1
        whilelo p0.s, xzr, x4
        .p2align 3,,7
.L3:
        ld2w    {z4.s - z5.s}, p0/z, [x1]
        ld2w    {z2.s - z3.s}, p0/z, [x2]
        add     z0.s, z4.s, z2.s
        sub     z1.s, z5.s, z3.s
        st2w    {z0.s - z1.s}, p0, [x0]
        incb    x1, all, mul #2
        whilelo p0.s, x3, x4
        incb    x0, all, mul #2
        incb    x2, all, mul #2
        incw    x3
        ptest   p1, p0.b
        bne     .L3
.L1:
        ret
        .cfi_endproc

Rather than have one INCB per address, we should have a single IV
that tracks the index, something like:

        ld2w    {z4.s - z5.s}, p0/z, [x1, x4, lsl #2]
        ld2w    {z2.s - z3.s}, p0/z, [x2, x4, lsl #2]
        add     z0.s, z4.s, z2.s
        sub     z1.s, z5.s, z3.s
        st2w    {z0.s - z1.s}, p0, [x0, x4, lsl #2]
        incw    x4, all, mul #2     // or inch

I think this will need work in both the target code and ivopts.


---


### compiler : `gcc`
### title : `[SVE] Redundant PTEST in loop test`
### open_at : `2019-01-14T10:46:25Z`
### last_modified_date : `2021-01-15T17:06:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88836
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Compiling this code with -O3 -march=armv8-a+sve:

void
f (int *restrict x, int *restrict y, int *restrict z, int n)
{
  for (int i = 0; i < n; i += 2)
    {
      x[i] = y[i] + z[i];
      x[i + 1] = y[i + 1] - z[i + 1];
    }
}

gives:

f:
.LFB0:
        .cfi_startproc
        cmp     w3, 0
        ble     .L1
        sub     w4, w3, #1
        cntw    x3
        ptrue   p1.s, all
        lsr     w4, w4, 1
        add     w4, w4, 1
        whilelo p0.s, xzr, x4
        .p2align 3,,7
.L3:
        ld2w    {z4.s - z5.s}, p0/z, [x1]
        ld2w    {z2.s - z3.s}, p0/z, [x2]
        add     z0.s, z4.s, z2.s
        sub     z1.s, z5.s, z3.s
        st2w    {z0.s - z1.s}, p0, [x0]
        incb    x1, all, mul #2
        whilelo p0.s, x3, x4
        incb    x0, all, mul #2
        incb    x2, all, mul #2
        incw    x3
        ptest   p1, p0.b
        bne     .L3
.L1:
        ret
        .cfi_endproc

PR88834 is tracking the poor addressing mode choices.  But there's also no need for that PTEST.  We should be able to use the flags set by the WHILELO directly.


---


### compiler : `gcc`
### title : `[SVE] Poor vector construction code in VL-specific mode`
### open_at : `2019-01-14T11:09:59Z`
### last_modified_date : `2019-07-27T09:52:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88837
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
The reduction testcases in gcc.target/aarch64/sve/slp_5.c require an initial vector in which all elements except the first two are zero.  For the default VL-agnostic mode we generate reasonable code, e.g.:

vec_slp_int32_t:
.LFB4:
        .cfi_startproc
        ldp     s2, s1, [x1]
        cmp     w2, 0
        ble     .L19
        mov     x3, 0
        sbfiz   x2, x2, 1, 32
        mov     z0.b, #0
        whilelo p0.s, xzr, x2
        insr    z0.s, s1
        ptrue   p1.s, all
        insr    z0.s, s2
        .p2align 3,,7
.L20:
        ld1w    z1.s, p0/z, [x0, x3, lsl 2]
        incw    x3
        add     z0.s, p0/m, z0.s, z1.s
        whilelo p0.s, x3, x2
        bne     .L20

But with -msve-vector-bits=256 the code is much worse:

vec_slp_int32_t:
.LFB4:
        .cfi_startproc
        ldp     w5, w4, [x1]
        cmp     w2, 0
        ble     .L31
        sub     sp, sp, #32
        .cfi_def_cfa_offset 32
        mov     z0.b, #0
        str     z0, [sp]
        mov     x3, 0
        sbfiz   x2, x2, 1, 32
        whilelo p0.s, xzr, x2
        ldr     x6, [sp]
        bfi     x6, x5, 0, 32
        mov     x5, x6
        bfi     x5, x4, 32, 32
        str     x5, [sp]
        ldr     z0, [sp]
        .p2align 3,,7
.L28:
        ld1w    z1.s, p0/z, [x0, x3, lsl 2]
        add     x3, x3, 8
        add     z0.s, p0/m, z0.s, z1.s
        whilelo p0.s, x3, x2
        bne     .L28

We should try to optimise this, probably by implementing vec_init_optab for fixed-length vectors.


---


### compiler : `gcc`
### title : `[SVE] Use 32-bit WHILELO in LP64 mode`
### open_at : `2019-01-14T11:43:09Z`
### last_modified_date : `2019-07-27T09:53:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88838
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Compiling this test with -O3 -march=armv8-a+sve:

void
f (int *restrict x, int *restrict y, int *restrict z, int n)
{
  for (int i = 0; i < n; i += 1)
    x[i] = y[i] + z[i];
}

produces:

f:
.LFB0:
        .cfi_startproc
        cmp     w3, 0
        ble     .L1
        mov     x4, 0
        sxtw    x3, w3
        whilelo p0.s, xzr, x3
        .p2align 3,,7
.L3:
        ld1w    z1.s, p0/z, [x1, x4, lsl 2]
        ld1w    z0.s, p0/z, [x2, x4, lsl 2]
        add     z0.s, z0.s, z1.s
        st1w    z0.s, p0, [x0, x4, lsl 2]
        incw    x4
        whilelo p0.s, x4, x3
        bne     .L3
.L1:
        ret

We could (and should) avoid the SXTW by using WHILELO on W registers instead of X registers.

vect_verify_full_masking checks which IV widths are supported for WHILELO but prefers to go to Pmode width.  This is because using Pmode allows ivopts to reuse the IV for indices (as in the loads and store above).  However, it would be better to use a 32-bit WHILELO with a truncated 64-bit IV if:

(a) the limit is extended from 32 bits.

(b) the detection loop in vect_verify_full_masking detects that using a 32-bit IV would be correct.

The thing to avoid is when using a 32-bit IV might wrap (see vect_set_loop_masks_directly).  In that case it would be better to stick with 64-bit WHILELOs.


---


### compiler : `gcc`
### title : `[SVE] Poor implementation of blend-like permutes`
### open_at : `2019-01-14T11:55:39Z`
### last_modified_date : `2019-09-05T08:20:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88839
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Compiling this code with -O3 -msve-vector-bits=256:

typedef int v8si __attribute__((vector_size(32)));
v8si
f (v8si x, v8si y, v8si sel)
{
  return __builtin_shuffle (x, y, (v8si) { 0, 9, 2, 11, 4, 13, 6, 15 });
}

produces an inefficient TBL-based sequence.

In these blend-like cases, where index I of the output comes from index I of one of the inputs, we should be able to use a SEL with an appropriate predicate constant.  The preferred implementation of the above would be:

        ptrue    p0.d, vl4        // { 1, 0, 1, 0, ... } when used as p0.s
        sel      res, p0, y, x

This will also be useful for the default VL-agnostic mode when implementing support for 2-operation SLP.


---


### compiler : `gcc`
### title : `Missed optimization transforming cascading ||s into a bit select`
### open_at : `2019-01-14T15:46:56Z`
### last_modified_date : `2023-02-18T05:28:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88841
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
It seems around GCC 7 an optimization was added turning multiple comparisons of a small range into a bit-select. This optimization seems to be sensitive to the ordering of the comparisons, which seems like a missed opportunity.

On x86_64 GCCs 7 and above (tested with trunk 9.0 too) with -O2 :

---snip

bool isspc_1(char c)
{
    return c == ' '
        || c == '\n'
        || c == '\r'
        || c == '\t';
}

bool isspc_2(char c)
{
    return c == ' '
        || c == '\r'
        || c == '\n'
        || c == '\t';
}

--- end snip (see https://godbolt.org/z/ovB_Oz )

...the isspc_2 is optimized using the bit selection optimization, but the isspc_1 is not. The only difference is the order of the comparisons. It's not clear to me which is actually faster, but my instinct is the results of these two functions should be the same code.


---


### compiler : `gcc`
### title : `missing optimization CSE, reassociation`
### open_at : `2019-01-14T15:48:30Z`
### last_modified_date : `2023-09-24T03:47:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88842
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `enhancement`
### contents :
from retweet danluu
↓
https://twitter.com/johnregehr/status/923682400676093952
not every day you run across a … easy optimization missing from all of LLVM, GCC, and Intel CC…
↓
 http://lists.llvm.org/pipermail/llvm-dev/2017-October/118476.html 
[llvm-dev] [PATCH/RFC] Modifying reassociate for improved CSE: … perf gains

>> “…several people asked if this is safe; here's how LLVM safely reassociates addition with undefined signed overflow…”
  

i dont know to what degree is track their development/findings
or was this only llvm-side


>> “… to convince another person that reassociate is a safe pass…”

also may this apply outside of -ffast-math? for GCC?


---


### compiler : `gcc`
### title : `poor range info for number of loop iterations with a known upper bound`
### open_at : `2019-01-14T17:02:35Z`
### last_modified_date : `2021-08-10T04:48:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88844
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
The number of iterations of the loop below is bounded by the size of i so the value of n on loop exit is in the range [0, 4] but get_range_info() reports it as [0, +INF].  As a result, the if statement is not eliminated.

void f (unsigned i)
{ 
  unsigned n = 0;

  while (i) {
    i >>= 4;
    ++n;
  }

  if (n > sizeof i)
    __builtin_abort ();
}


---


### compiler : `gcc`
### title : `redundant store after load that would makes aliasing UB`
### open_at : `2019-01-15T10:40:58Z`
### last_modified_date : `2022-05-07T08:25:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88854
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Test cases:

This is optimized at -O1 and with GCC 5 at -O2. -fdisable-tree-fre1 and -fno-strict-aliasing also remove the store to a.

void f(int *a, float *b) {
    int x = *a;
    *b = 0;
    x = *a;
    *a = x;
}

The following is an extension that reloads *a after store to b into a different variable. Still the store to a must be dead, since otherwise the read of a would be UB.

int g(int *a, float *b) {
    int x = *a;
    *b = 0;
    int r = *a;
    *a = x;
    return r;
}


---


### compiler : `gcc`
### title : `[SSE] pshufb can be omitted for a specific pattern`
### open_at : `2019-01-15T18:39:01Z`
### last_modified_date : `2021-08-21T22:07:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88868
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.0`
### severity : `enhancement`
### contents :
SSSE3 instruction PSHUFB (and the AVX2 counterpart VPSHUFB) acts as a no-operation
when its argument is a sequence 0..15. Such invocation does not alter shuffled
register, thus PSHUFB can be safely omitted

BTW, clang does this optimization, but ICC doesn't.

---pshufb.c---
#include <immintrin.h>

__m128i shuffle(__m128i x) {
    const __m128i noop = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
    return _mm_shuffle_epi8(x, noop);
}
---eof---

$ gcc --version
gcc (Debian 8.2.0-13) 8.2.0

$ gcc -O3 -march=skylake -S pshufb.c 
$ cat pshufb.s
shuffle:
	vpshufb	.LC0(%rip), %xmm0, %xmm0
	ret
.LC0:
	.byte	0
	.byte	1
	.byte	2
	.byte	3
	.byte	4
	.byte	5
	.byte	6
	.byte	7
	.byte	8
	.byte	9
	.byte	10
	.byte	11
	.byte	12
	.byte	13
	.byte	14
	.byte	15

An expected output:

shuffle:
    ret


---


### compiler : `gcc`
### title : `missing vectorization for decomposed operations on a vector type`
### open_at : `2019-01-16T09:51:47Z`
### last_modified_date : `2023-08-04T15:24:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88873
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
To compute a vectorized fma, one needs to apply it on the decomposed vector components. Here's an example with a structure type and with a vector type. The structure type solution is just given for comparison. This bug is about the vector type solution.

#include <math.h>

typedef struct { double x, y; } s_t;

typedef double v2df __attribute__ ((vector_size (2 * sizeof(double))));

s_t foo (s_t a, s_t b, s_t c)
{
  return (s_t) { fma(a.x, b.x, c.x), fma (a.y, b.y, c.y) };
}

v2df bar (v2df a, v2df b, v2df c)
{
  v2df r;

  r[0] = fma (a[0], b[0], c[0]);
  r[1] = fma (a[1], b[1], c[1]);
  return r;
}

With -O3, I get on x86_64:

* For function foo (struct type):

[...]
        vfmadd132pd     -40(%rsp), %xmm7, %xmm6
[...]

This is vectorized as expected, though this solution is affected by bug 65847.

* For function bar (vector type):

bar:
.LFB1:
        .cfi_startproc
        vmovapd %xmm0, %xmm3
        vunpckhpd       %xmm0, %xmm0, %xmm0
        vfmadd132sd     %xmm1, %xmm2, %xmm3
        vunpckhpd       %xmm1, %xmm1, %xmm1
        vunpckhpd       %xmm2, %xmm2, %xmm2
        vfmadd132sd     %xmm1, %xmm2, %xmm0
        vunpcklpd       %xmm0, %xmm3, %xmm0
        ret
        .cfi_endproc

This is not vectorized: one has 2 vfmadd132sd instead of a single vfmadd132pd.

Note: The problem is the same with addition, but in the addition case, one can simply do a + b. This is not possible with fma.

This bug seems similar to bug 77399.


---


### compiler : `gcc`
### title : `[11/12 Regression] Bogus maybe-uninitialized warning on class field (missed CSE)`
### open_at : `2019-01-17T20:58:22Z`
### last_modified_date : `2023-07-07T10:34:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88897
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `8.2.1`
### severity : `normal`
### contents :
Created attachment 45452
testcase

Compiling the attached testcase with "g++ -c -O1 -Wall test.ii" produces

test.ii:631:7: warning: ‘<anonymous>.seastar::temporary_buffer::_buffer’ may be used uninitialized in this function [-Wmaybe-uninitialized]
 class temporary_buffer {

But in the reduced testcase the only constructor of temporary_buffer is out of line, so the compiler has no way of knowing if it is initialized or not.

The warning goes away with -O2 or with very minor changes to the source code.


---


### compiler : `gcc`
### title : `Try smaller vectorisation factors in scalar fallback`
### open_at : `2019-01-18T16:31:31Z`
### last_modified_date : `2019-11-27T08:48:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88915
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The get_ref hot function in 525.x264_r inlines a hot helper that performs a vector average:
void pixel_avg( unsigned char *dst, int i_dst_stride,
                               unsigned char *src1, int i_src1_stride,
                               unsigned char *src2, int i_src2_stride,
                               int i_width, int i_height )
 {
     for( int y = 0; y < i_height; y++ )
     {
         for( int x = 0; x < i_width; x++ )
             dst[x] = ( src1[x] + src2[x] + 1 ) >> 1;
         dst += i_dst_stride;
         src1 += i_src1_stride;
         src2 += i_src2_stride;
     }
 }

GCC 9 already knows how to generate vector average instructions (PR 85694).
For aarch64 it generates a 16x vectorised loop.
Runtime profiling of the arguments to this function, however, show that the >50% of the time the i_width has value 8 during runtime and therefore the vector loop is skipped in favour of a scalar fallback:
32.07%  40ed2c		ldrb	w3, [x0,x5]
11.41%  40ed30		ldrb	w11, [x4,x5]
        40ed34		add	w3, w3, w11
        40ed38		add	w3, w3, #0x1
        40ed3c		asr	w3, w3, #1
0.71%   40ed40		strb	w3, [x2,x5]
        40ed44		add	x5, x5, #0x1
        40ed48		cmp	w6, w5
        40ed4c		b.gt	<loop>

The most frequent runtime combinations of inputs to this function are:
29240545 i_height: 8, i_width: 8, i_dst_stride: 16, i_src1_stride: 1344, i_src2_stride: 1344
22714355 i_height: 16, i_width: 16, i_dst_stride: 16, i_src1_stride: 1344, i_src2_stride: 1344
19669512 i_height: 8, i_width: 8, i_dst_stride: 16, i_src1_stride: 704, i_src2_stride: 704
3689216 i_height: 16, i_width: 8, i_dst_stride: 16, i_src1_stride: 1344, i_src2_stride: 1344
3670639 i_height: 8, i_width: 16, i_dst_stride: 16, i_src1_stride: 1344, i_src2_stride: 1344

That's a shame. AArch64 supports the V8QI form of the vector average instruction (and advertises it through optabs).
With --param vect-epilogues-nomask=1 we already generate something like:
if (bytes_left > 16)
{
  while (bytes_left > 16)
    16x_vectorised;
  if (bytes_left > 8)
    8x_vectorised;
  unrolled_scalar_epilogue;
}
else
  scalar_loop;

Could we perhaps generate:
  while (bytes_left > 16)
    16x_vectorised;
  if (bytes_left > 8)
    8x_vectorised;
  unrolled_scalar_epilogue; // or keep it as a rolled scalar_loop to save on codesize?

Basically I'm looking for a way to take advantage of the 8x vectorised form.


---


### compiler : `gcc`
### title : `[x86] suboptimal code generated for integer comparisons joined with boolean operators`
### open_at : `2019-01-18T17:12:17Z`
### last_modified_date : `2022-11-27T06:52:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88916
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `8.2.0`
### severity : `enhancement`
### contents :
Let's consider these two simple, yet pretty useful functions:

--test.c---
int both_nonnegative(long a, long b) {
    return (a >= 0) && (b >= 0);
}

int both_nonzero(unsigned long a, unsigned long b) {
    return (a > 0) && (b > 0);
}
---eof--

$ gcc --version
gcc (Debian 8.2.0-13) 8.2.0

$ gcc -O3 test.c -march=skylake -S
$ cat test.s
both_nonnegative:
    notq    %rdi
    movq    %rdi, %rax
    notq    %rsi
    shrq    $63, %rax
    shrq    $63, %rsi
    andl    %esi, %eax
    ret

both_nonzero:
    testq   %rdi, %rdi
    setne   %al
    xorl    %edx, %edx
    testq   %rsi, %rsi
    setne   %dl
    andl    %edx, %eax
    ret

I checked different target machines (haswell, broadwell and cannonlake),
however the result remained the same. Also GCC trunk on godbolt.org 
produces the same assembly code.

The first function, `both_nonnegative`, can be rewritten as:

    (((unsigned long)(a) | (unsigned long)(b)) >> 63) ^ 1

yielding something like this:

    both_nonnegative:
        orq     %rsi, %rdi
        movq    %rdi, %rax
        shrq    $63, %rax
        xorl    $1, %eax
        ret

It's also possible to use this expression:
(long)(unsigned long)a | (unsigned long)b) < 0,
but the assembly output is almost the same.

The condition from `both_nonzero` can be expressed as:

    ((unsigned long)a | (unsigned long)b) != 0

GCC compiles it to:

    both_nonzero:
        xorl    %eax, %eax
        orq     %rsi, %rdi
        setne   %al
        retq


---


### compiler : `gcc`
### title : `Merge identical constants with different modes`
### open_at : `2019-01-18T21:43:30Z`
### last_modified_date : `2021-08-21T23:28:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88922
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `9.0`
### severity : `normal`
### contents :
#include <stdint.h>
typedef int64_t v64 __attribute__((vector_size(16)));
typedef int32_t v32 __attribute__((vector_size(16)));
typedef int16_t v16 __attribute__((vector_size(16)));

int main(){
  volatile v64 x = {1,2};
  volatile v32 y = {1,0,2,0};
  volatile v16 z = {1,0,0,0,2,0,0,0};
}

	movdqa	.LC0(%rip), %xmm0
	xorl	%eax, %eax
	movaps	%xmm0, -56(%rsp)
	movdqa	.LC1(%rip), %xmm0
	movaps	%xmm0, -40(%rsp)
	movdqa	.LC2(%rip), %xmm0
	movaps	%xmm0, -24(%rsp)

gcc fails to notice that the 3 constants are the same. I would have expected the constant pool to be handled as an untyped collection of bits at the end.


---


### compiler : `gcc`
### title : `ivopts with some NOP conversions`
### open_at : `2019-01-20T01:15:46Z`
### last_modified_date : `2019-01-21T09:25:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88926
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
Starting from gcc.dg/tree-ssa/ivopts-lt-2.c:

/* { dg-do compile } */
/* { dg-options "-O2 -fdump-tree-ivopts" } */
/* { dg-skip-if "PR68644" { hppa*-*-* powerpc*-*-* } } */

void
f1 (int *p, unsigned int i)
{
  p += i;
  do
    {
      *p = 0;
      p += 1;
      i++;
    }
  while (i < 100);
}

/* { dg-final { scan-tree-dump-times "PHI" 1 "ivopts" } } */
/* { dg-final { scan-tree-dump-times "PHI <p_" 1 "ivopts"} } */
/* { dg-final { scan-tree-dump-times "p_\[0-9\]* <" 1 "ivopts" } } */

ivopts normally manages to get rid of i in the loop. However, if I replace "p += i" with

  __PTRDIFF_TYPE__ o = i;
  o *= 4;
  p = (char*)p + o;

(i.e. I compute i*4 in type 'long' instead of 'unsigned long')
ivopts doesn't find an invariant and doesn't eliminate i anymore.

This will be useful if I try to resurrect https://gcc.gnu.org/ml/gcc-patches/2017-05/msg01641.html . I am not familiar with ivopts (I don't see why the instruction p+=i is relevant, it only changes the initial value of p, but without it we do not eliminate either) so this is likely one of the last cases I'll try to fix.


---


### compiler : `gcc`
### title : `regex_search doesn't fail early when given a non-matching pattern with a start-of-input anchor`
### open_at : `2019-01-21T13:04:47Z`
### last_modified_date : `2023-06-09T09:49:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88947
### status : `NEW`
### tags : `missed-optimization`
### component : `libstdc++`
### version : `8.2.0`
### severity : `enhancement`
### contents :
I first raised this on SO (https://stackoverflow.com/q/54237547/560648), on which I have posted some benchmarks to back up the claim(s) below.

Take the following:

#include <regex>
int main()
{
  static const size_t BufSize = 100;
  char buf[BufSize] = {};
  auto begin = std::cbegin(buf), end = std::cend(buf);

  std::cmatch groups;
  std::regex::flag_type flags = std::regex_constants::ECMAScript;
  std::regex re("^what", flags);
  std::regex_search(begin, end, groups, re);
}

This attempts to match the pattern "^what" against a block of 100 characters. The match is not expected to succeed (in this case, the input is simply 100 '\0's, but the problem exists for any non-matching input).

However, I expect the match to fail as soon as the first character of input is examined. By adjusting BufSize to increasingly large values, we observe that the execution time increases also, suggesting that the regex engine is examining the entire input even though the presence of the anchor "^" guarantees that a match will never be found. It only needed to examine the first character to know this. When BufSize reaches larger values like 100KB, this becomes quite problematic.

It is clear from the implementation code (https://github.com/gcc-mirror/gcc/blob/464ac146f6d2aaab847f653edde3ae84a8366c94/libstdc%2B%2B-v3/include/bits/regex_executor.tcc#L37-L54) that there is simply no logic in place to "fail fast" or "fail early" in a case like this: the only way a "no match" result is returned is after examining the whole input, regardless of the pattern.

It is my opinion that this is a quality of implementation issue, and one that only appears in C++ implementations of regular expressions. This problem is common to libstdc++, libc++ and also Visual Studio's stdlib impl. (I am raising bugs against all three.)

As a workaround I'm having to artificially select a prefix of the input data in order to get a fast result -- in the example above, that could be:

  auto begin = std::cbegin(buf), end = std::cbegin(buf)+4;

However, not all examples are so trivial (indeed, the example above would be much better approached with a simple string prefix comparison) and the workaround not always so easy. When the pattern is more complex, it is not always easy to find the best number of characters to send to the regex engine, and the resulting code not particularly elegant. It would be much better if the engine could be given the whole input without having to worry about scale.

Hopefully my expectation isn't unreasonable; Safari's implementation of regex behaves as I'd expect. That is, the time to return a "no match" result is constant (and fast) given the JS equivalent of the above example.

Is it possible that the regex_match implementation could be given a little more intelligence?

(Apologies that I am not sufficiently familiar with libstdc++ version history to select an appropriate version number for this bug.)


---


### compiler : `gcc`
### title : `gcc generates terrible code for vectors of 64+ length which are not natively supported`
### open_at : `2019-01-21T23:23:15Z`
### last_modified_date : `2021-08-25T06:37:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88963
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `9.0`
### severity : `normal`
### contents :
[code]
typedef int VInt __attribute__((vector_size(64)));

void test(VInt*__restrict a, VInt*__restrict b, 
    VInt*__restrict c)
{
    *a = *b + *c;
}
[/code]

This code compiled with -O3 -march=skylake in following way:

[asm]
test(int __vector(16)*, int __vector(16)*, int __vector(16)*):
  push rbp
  mov rbp, rsp
  and rsp, -64
  sub rsp, 136
  vmovdqa xmm3, XMMWORD PTR [rsi]
  vmovdqa xmm4, XMMWORD PTR [rsi+16]
  vmovdqa xmm5, XMMWORD PTR [rsi+32]
  vmovdqa xmm6, XMMWORD PTR [rsi+48]
  vmovdqa xmm7, XMMWORD PTR [rdx]
  vmovaps XMMWORD PTR [rsp-56], xmm3
  vmovdqa xmm1, XMMWORD PTR [rdx+16]
  vmovaps XMMWORD PTR [rsp-40], xmm4
  vmovdqa ymm4, YMMWORD PTR [rsp-56]
  vmovdqa xmm2, XMMWORD PTR [rdx+32]
  vmovaps XMMWORD PTR [rsp-8], xmm6
  vmovaps XMMWORD PTR [rsp+8], xmm7
  vmovdqa xmm3, XMMWORD PTR [rdx+48]
  vmovaps XMMWORD PTR [rsp-24], xmm5
  vmovaps XMMWORD PTR [rsp+24], xmm1
  vpaddd ymm0, ymm4, YMMWORD PTR [rsp+8]
  vmovdqa ymm5, YMMWORD PTR [rsp-24]
  vmovaps XMMWORD PTR [rsp+40], xmm2
  vmovaps XMMWORD PTR [rsp+56], xmm3
  vmovdqa xmm2, xmm0
  vmovdqa YMMWORD PTR [rsp-120], ymm0
  vpaddd ymm0, ymm5, YMMWORD PTR [rsp+40]
  vmovdqa xmm6, XMMWORD PTR [rsp-104]
  vmovdqa YMMWORD PTR [rsp-88], ymm0
  vmovdqa xmm7, XMMWORD PTR [rsp-72]
  vmovaps XMMWORD PTR [rdi], xmm2
  vmovaps XMMWORD PTR [rdi+16], xmm6
  vmovaps XMMWORD PTR [rdi+32], xmm0
  vmovaps XMMWORD PTR [rdi+48], xmm7
  vzeroupper
  leave
  ret
[/asm]

Other compilers (clang, icc) produces nice code. This is from clang:

[asm]
test(int __vector(16)*, int __vector(16)*, int __vector(16)*): # @test(int __vector(16)*, int __vector(16)*, int __vector(16)*)
  vmovdqa ymm0, ymmword ptr [rdx]
  vmovdqa ymm1, ymmword ptr [rdx + 32]
  vpaddd ymm0, ymm0, ymmword ptr [rsi]
  vpaddd ymm1, ymm1, ymmword ptr [rsi + 32]
  vmovdqa ymmword ptr [rdi + 32], ymm1
  vmovdqa ymmword ptr [rdi], ymm0
  vzeroupper
  ret
[/asm]

gcc produces pretty code for -O3 -march=skylake-avx512. Pretty code is also for vector size 32 with AVX disabled. However for vector size 128 and -O3 -march=skylake-avx512 code is again ugly.


---


### compiler : `gcc`
### title : `popcnt of limited 128-bit number with unnecessary zeroing`
### open_at : `2019-01-22T10:49:24Z`
### last_modified_date : `2019-01-22T12:10:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88972
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Compile the following code on x86-64 with -Ofast -march=haswell:

int f(__uint128_t m)
{
  if (m < 64000)
    return __builtin_popcount(m);
  return -1;
}


The generated code with the trunk gcc looks like this:

   0:	b8 ff f9 00 00       	mov    $0xf9ff,%eax
   5:	48 39 f8             	cmp    %rdi,%rax
   8:	b8 00 00 00 00       	mov    $0x0,%eax
   d:	48 19 f0             	sbb    %rsi,%rax
  10:	72 0e                	jb     20 <f+0x20>
  12:	31 c0                	xor    %eax,%eax
  14:	f3 0f b8 c7          	popcnt %edi,%eax
  18:	c3                   	retq   
  19:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
  20:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
  25:	c3                   	retq   


The instruction at offset 12 is unnecessary.  I guess this is a left-over from the popcnt of the upper half which is recognized to be unnecessary and left out.  There is no addition anymore but somehow the register clearing survived.


---


### compiler : `gcc`
### title : `Failed outer loop vectorization with grouped accesses`
### open_at : `2019-01-22T14:28:41Z`
### last_modified_date : `2019-03-05T01:31:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88978
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
We fail to vectorize outer loops when there are grouped accesses in the
inner loop:

int a[1024];
int b[1024][1024];

void foo ()
{
  for (int i = 0; i < 512; ++i)
    {
      int a1 = a[2*i];
      int a2 = a[2*i+1];
      for (int j = 0; j < 1024; ++j)
        {
          b[j][2*i] = a1;
          b[j][2*i+1] = a2;
        }
    }
}

This is mostly because we cannot do SLP here (for implementation reasons).
We are vectorizing the following just fine, applying interleaving to the
outer loop accesses:

int a[1024];
int b[1024][1024];

void foo ()
{
  for (int i = 0; i < 512; ++i)
    {
      int a1 = a[2*i];
      int a2 = a[2*i+1];
      for (int j = 0; j < 1024; ++j)
        b[j][i] = a1+a2;
    }
}

The guard in question is the following which is premature (before SLP
would be even tried) and somewhat inaccurate since it is grouped
accesses in the inner loop when doing outer loop vectorization rather
than grouped accesses in an outer loop that fail.

static bool
vect_analyze_data_ref_access (dr_vec_info *dr_info)
{
...
  if (loop && nested_in_vect_loop_p (loop, stmt_info))
    {
      if (dump_enabled_p ())
        dump_printf_loc (MSG_NOTE, vect_location,
                         "grouped access in outer loop.\n");
      return false;
    }


---


### compiler : `gcc`
### title : `[SVE] Implement generic vector average expansion`
### open_at : `2019-01-23T08:53:02Z`
### last_modified_date : `2020-02-06T11:24:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89007
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC 9 knows how to recognise vector average operations since PR 85694. Some targets have optabs to do it in one instruction.

For the targets that don't, we could still do better than the fallback widening -> arithmetic -> narrowing sequence though. Maybe we could implement a generic expansion for the case when there is no target optab.

For example:
#define N 1024
unsigned char dst[N];
unsigned char in1[N];
unsigned char in2[N];

void
foo ()
{
  for( int x = 0; x < N; x++ )
    dst[x] = (in1[x] + in2[x] + 1) >> 1;
}

For aarch64 -march=armv8-a+sve -O3 we generate:
.L2:
        ld1b    z0.b, p0/z, [x5, x0]
        ld1b    z2.b, p0/z, [x4, x0]
        uunpklo z1.h, z0.b
        uunpklo z3.h, z2.b
        uunpkhi z0.h, z0.b
        uunpkhi z2.h, z2.b
        add     z1.h, z1.h, z3.h
        add     z0.h, z0.h, z2.h
        add     z1.h, z1.h, #1
        add     z0.h, z0.h, #1
        lsr     z1.h, z1.h, #1
        lsr     z0.h, z0.h, #1
        uzp1    z0.b, z1.b, z0.b
        st1b    z0.b, p0, [x2, x0]
        incb    x0
        whilelo p0.b, x0, x3
        bne     .L2


But we could generate the more optimal:
    ld1b    {z0.b}, p0/z, [x0, x4]
    ld1b    {z2.b}, p0/z, [x1, x4]
    orr     z4.d, z0.d, z2.d         // use and for floor rounding
    and     z4.b, z4.b, #1
    lsr     z0.b, z0.b, #1            // use asr for signed numbers
    lsr     z2.b, z2.b, #1            // likewise
    add     z0.b, z0.b, z2.b
    add     z0.b, z0.b, z4.b
    st1b    {z0.b}, p0, [x2, x4]

I think this doesn't require too much fancy target support, just some vector masking operations


---


### compiler : `gcc`
### title : `common subexpression present in both branches of condition is not factored out`
### open_at : `2019-01-23T21:16:13Z`
### last_modified_date : `2023-05-06T21:29:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89018
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
A common transformation used in a C condition expression is not detected and
code is duplicated. Below are a few examples:

---condition.c---
long transform(long);

long negative_max(long a, long b) {
    return (a >= b) ? -a : -b;
}

long fun_max(long a, long b) {
    return (a >= b) ? 47*a : 47*b;
}

long transform_max(long a, long b) {
    return (a >= b) ? transform(a) : transform(b);
}
---eof---

In both branches a scalar value is a part of the same expression. So, it
would be more profitable when, for instance "(a >= b) ? -a : -b" would be
compiled as "-((a >= b) ? a : b))". Of course, a programmer might factor it
out, but in case of macros or auto-generated code such silly repetition
might occur.

Below is assembly code generated for x86 by a pretty fresh GCC 9.
BTW the jump instruction from `fun_max` and `transform_max` can
be replaced with a condition move.

$ gcc --version
gcc (GCC) 9.0.0 20190117 (experimental)

$ gcc -O3 -march=skylake -c -S condition.c && cat condition.s

negative_max:
	movq	%rdi, %rdx
	movq	%rsi, %rax
	negq	%rdx
	negq	%rax
	cmpq	%rsi, %rdi
	cmovge	%rdx, %rax
	ret

fun_max:
	cmpq	%rsi, %rdi
	jl	.L6
	imulq	$47, %rdi, %rax
	ret
.L6:
	imulq	$47, %rsi, %rax
	ret

transform_max:
	cmpq	%rsi, %rdi
	jge	.L11
	movq	%rsi, %rdi
.L11:
	jmp	transform


---


### compiler : `gcc`
### title : `8-byte loop isn't vectorized`
### open_at : `2019-01-24T02:58:47Z`
### last_modified_date : `2022-01-09T00:35:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89028
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
[hjl@gnu-skx-1 v64-2]$ cat y.i
void
rsqrt(char* restrict r, char* restrict a){
    for (int i = 0; i < 8; i++){
        r[i] += a[i];
    }
}
[hjl@gnu-skx-1 v64-2]$ gcc -S -O2 y.i
[hjl@gnu-skx-1 v64-2]$ cat y.s
	.file	"y.i"
	.text
	.p2align 4,,15
	.globl	rsqrt
	.type	rsqrt, @function
rsqrt:
.LFB0:
	.cfi_startproc
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L2:
	movzbl	(%rsi,%rax), %edx
	addb	%dl, (%rdi,%rax)
	addq	$1, %rax
	cmpq	$8, %rax
	jne	.L2
	ret
	.cfi_endproc
.LFE0:
	.size	rsqrt, .-rsqrt
	.ident	"GCC: (GNU) 8.2.1 20190109 (Red Hat 8.2.1-7)"
	.section	.note.GNU-stack,"",@progbits
[hjl@gnu-skx-1 v64-2]$


---


### compiler : `gcc`
### title : `strcat (strcpy (d, a), b) not folded to stpcpy (strcpy (d, a), b)`
### open_at : `2019-01-24T16:46:08Z`
### last_modified_date : `2021-09-12T08:19:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89043
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `enhancement`
### contents :
GCC folds a small subset of calls to C strcpy to the POSIX stpcpy when doing so is profitable, such as in function f below.  But it doesn't do the same folding in the common case when strcat is being called with the result of strcpy as the first argument, as in function g below.

$ cat u.c && gcc -O2 -S -Wall -fdump-tree-optimized=/dev/stdout u.c
extern char* stpcpy (char*, const char*);
extern char* strcat (char*, const char*);
extern char* strcpy (char*, const char*);
extern char* strchr (const char*, int);


extern char *a, *b, *d;

char* f (char *d, const char *s)
{
  strcpy (d, s);          // folded to 'p = stpcpy (d, s);'
  return strchr (d, 0);   // folded to 'return p;'
}

void g (void)
{
  // could be folded to 'strcpy (stpcpy (d, a));'
  strcat (strcpy (d, a), b);
}

;; Function f (f, funcdef_no=0, decl_uid=1922, cgraph_uid=1, symbol_order=0)

f (char * d, const char * s)
{
  char * _7;

  <bb 2> [local count: 1073741824]:
  _7 = __builtin_stpcpy (d_2(D), s_3(D));
  return _7;

}



;; Function g (g, funcdef_no=1, decl_uid=1925, cgraph_uid=2, symbol_order=1)

g ()
{
  char * b.0_1;
  char * a.1_2;
  char * d.2_3;
  char * _4;

  <bb 2> [local count: 1073741824]:
  b.0_1 = b;
  a.1_2 = a;
  d.2_3 = d;
  _4 = strcpy (d.2_3, a.1_2);
  strcat (_4, b.0_1); [tail call]
  return;

}


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Unexpected vectorization`
### open_at : `2019-01-24T20:01:52Z`
### last_modified_date : `2023-07-07T10:34:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89049
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
int bar (float *p) { float r = 0; for (int i = 0; i < 1024; ++i) r += p[i]; return r; }
with -O2 -mavx2 -ftree-vectorize starting with r256639 is vectorized as:
.L6:
        vmovups (%rdi), %xmm4
        vinsertf128     $0x1, 16(%rdi), %ymm4, %ymm2
        addq    $32, %rdi
        vaddss  %xmm4, %xmm0, %xmm0
        vshufps $85, %xmm4, %xmm4, %xmm3
        vshufps $255, %xmm4, %xmm4, %xmm1
        vaddss  %xmm3, %xmm0, %xmm0
        vunpckhps       %xmm4, %xmm4, %xmm3
        vaddss  %xmm3, %xmm0, %xmm0
        vaddss  %xmm1, %xmm0, %xmm0
        vextractf128    $0x1, %ymm2, %xmm1
        vshufps $85, %xmm1, %xmm1, %xmm2
        vaddss  %xmm1, %xmm0, %xmm0
        vaddss  %xmm2, %xmm0, %xmm0
        vunpckhps       %xmm1, %xmm1, %xmm2
        vshufps $255, %xmm1, %xmm1, %xmm1
        vaddss  %xmm2, %xmm0, %xmm0
        vaddss  %xmm1, %xmm0, %xmm0
        cmpq    %rdi, %rax
        jne     .L6
The only vector thing in the loop is the vector unaligned load, all the rest are either extractions from the vector or scalar operations.  At least for -O2 I'd hope we don't do this, I strongly believe scalar loop would be faster, and if we don't decide to unroll it, even much smaller.  Either the costs are computed wrongly here, or the vectorizer uses them wrongly.


---


### compiler : `gcc`
### title : `[9 Regression] AArch64 ld3 st4 less optimized`
### open_at : `2019-01-25T09:38:13Z`
### last_modified_date : `2021-05-17T08:55:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89057
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `8.2.1`
### severity : `normal`
### contents :
When using the vld3_u8 and vst4_u8 instrinsics, the code generated with gcc8 is less efficient than the code generated with gcc7. One has 3 moves, and the other 9 moves.

The code in question is:

#include <stdint.h>
#include <arm_neon.h>

void qt_convert_rgb888_to_rgb32_neon(unsigned *dst, const unsigned char *src, int len)
{
    if (!len)
        return;

    const unsigned *const end = dst + len;

    // align dst on 64 bits
    const int offsetToAlignOn8Bytes = (reinterpret_cast<uintptr_t>(dst) >> 2) & 0x1;
    for (int i = 0; i < offsetToAlignOn8Bytes; ++i) {
        *dst++ = 0xff000000 | (src[0] << 16) | (src[1] << 8) | src[2];
        src += 3;
    }

    if ((len - offsetToAlignOn8Bytes) >= 8) {
        const unsigned *const simdEnd = end - 7;
        // non-inline asm version (uses more moves)
        uint8x8x4_t dstVector;
        dstVector.val[3] = vdup_n_u8(0xff);
        do {
            uint8x8x3_t srcVector = vld3_u8(src);
            src += 3 * 8;
            dstVector.val[0] = srcVector.val[2];
            dstVector.val[1] = srcVector.val[1];
            dstVector.val[2] = srcVector.val[0];
            vst4_u8((uint8_t*)dst, dstVector);
            dst += 8;
        } while (dst < simdEnd);
    }

    while (dst != end) {
        *dst++ = 0xff000000 | (src[0] << 16) | (src[1] << 8) | src[2];
         src += 3;
     }
}


With gcc 7.3 the inner loop is:
.L5:
        ld3     {v4.8b - v6.8b}, [x1]
        add     x1, x1, 24
        orr     v3.16b, v7.16b, v7.16b
        mov     v0.8b, v6.8b
        mov     v1.8b, v5.8b
        mov     v2.8b, v4.8b
        st4     {v0.8b - v3.8b}, [x0]
        add     x0, x0, 32
        cmp     x3, x0
        bhi     .L5

With gcc 8.2 the inner loop is:
.L5:
        ld3     {v4.8b - v6.8b}, [x1]
        adrp    x3, .LC1
        add     x1, x1, 24
        ldr     q3, [x3, #:lo12:.LC1]
        mov     v16.8b, v6.8b
        mov     v7.8b, v5.8b
        mov     v4.8b, v4.8b
        ins     v16.d[1], v17.d[0]
        ins     v7.d[1], v17.d[0]
        ins     v4.d[1], v17.d[0]
        mov     v0.16b, v16.16b
        mov     v1.16b, v7.16b
        mov     v2.16b, v4.16b
        st4     {v0.8b - v3.8b}, [x0]
        add     x0, x0, 32
        cmp     x2, x0
        bhi     .L5


---


### compiler : `gcc`
### title : `Once we emit switchconf tables, we don't optimize them anymore`
### open_at : `2019-01-25T11:10:40Z`
### last_modified_date : `2021-08-29T09:12:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89059
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
For the following testcase:

static inline int
foo (int a)
{
  switch (a)
  {
  case 0:
    return 0;
  case 1 ... 3:
    return 1;
  case 4 ... 10:
    return -1;
  default:
    return 42;
  }
}

int
bar (int a)
{
  if (a < 0 || a > 3)
    __builtin_unreachable ();
  return foo (a);
}

we emit unnecessary code, I understand we want to run switchconf early, so that inliner can already see simple code, but in this case we don't know value ranges of the switch condition.  So, I wonder if we couldn't mark the CSWTCH tables with some attribute or whatever and in late GIMPLE reconsider if we can't emit something simpler, whether it is a bit test or simple comparison.
If we commit to a bit test, I wonder if we are able to simplify it later too.


---


### compiler : `gcc`
### title : `Improve tail call optimization`
### open_at : `2019-01-25T11:33:28Z`
### last_modified_date : `2019-05-08T20:54:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89060
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
void qux (char *);
int baz (void);

int
foo (void)
{
  char buf[64];
  qux (buf);
  return baz ();
}

int
bar (void)
{
  {
    char buf[64];
    qux (buf);
  }
  return baz ();
}

While we must not tail call optimize the baz call in foo, because an address of a local variable escapes and it could be live during the tail call, in bar we don't tail call optimize it either, but we should.
The IL difference in *.tailc between foo and bar is:
   char buf[64];
   int _5;

   <bb 2> [local count: 1073741824]:
   qux (&buf);
+  buf ={v} {CLOBBER};
   _5 = baz ();
-  buf ={v} {CLOBBER};
   return _5;

Right now, we just do:
  /* Make sure the tail invocation of this function does not indirectly
     refer to local variables.  (Passing variables directly by value
     is OK.)  */
  FOR_EACH_LOCAL_DECL (cfun, idx, var)
    {
      if (TREE_CODE (var) != PARM_DECL
          && auto_var_in_fn_p (var, cfun->decl)
          && may_be_aliased (var)
          && (ref_maybe_used_by_stmt_p (call, var)
              || call_may_clobber_ref_p (call, var)))
        return;
    }

Perhaps we could record all these variables in a bitmap and if there are any, perform a variable life analysis like cfgexpand.c does to determine variable conflicts or what has been discussed for PR86214, and if we can prove that those variables escaped, but are not live at the point of the tail call, we could still tail call optimize this.


---


### compiler : `gcc`
### title : `[x86] lack of support for BEXTR from BMI extension`
### open_at : `2019-01-25T19:54:22Z`
### last_modified_date : `2021-12-29T05:03:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89063
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
Instruction BEXTR extracts an arbitrary unsigned bit field from 32- or 64-bit
value. As I see in `config/i386.md`, there's support for the immediate
variant available in AMD's TBM (TARGET_TBM).

Intel's variant gets parameters from a register. Although this variant
won't be profitable in all cases -- as we need an extra move to setup
the bit-field parameters in a register -- I bet bit-field-intensive
code might benefit from BEXTR.

---bextr.c---
#include <immintrin.h>
#include <stdint.h>

uint64_t test(uint64_t x) {
    const uint64_t a0 = (x & 0x3f);
    const uint64_t a1 = (x >> 11) & 0x3f;
    const uint64_t a2 = (x >> 22) & 0x3f;
    return a0 + a1 + a2;
}

uint64_t test_intrinsics(uint64_t x) {
    const uint64_t a0 = (x & 0x3f);
    const uint64_t a1 = _bextr_u64(x, 11, 6);
    const uint64_t a2 = _bextr_u64(x, 22, 6);
    return a0 + a1 + a2;
}
---eof---

$ gcc --version
gcc (GCC) 9.0.0 20190117 (experimental)

$ gcc -O3 -mbmi -march=skylake bextr.c -c && objdump -d bextr.o

0000000000000000 <test>:
   0:	48 89 fa             	mov    %rdi,%rdx
   3:	48 c1 ea 0b          	shr    $0xb,%rdx
   7:	48 89 f8             	mov    %rdi,%rax
   a:	48 89 d1             	mov    %rdx,%rcx
   d:	48 c1 e8 16          	shr    $0x16,%rax
  11:	83 e0 3f             	and    $0x3f,%eax
  14:	83 e1 3f             	and    $0x3f,%ecx
  17:	48 8d 14 01          	lea    (%rcx,%rax,1),%rdx
  1b:	83 e7 3f             	and    $0x3f,%edi
  1e:	48 8d 04 3a          	lea    (%rdx,%rdi,1),%rax
  22:	c3                   	retq   

0000000000000030 <test_intrinsics>:
  30:	b8 0b 06 00 00       	mov    $0x60b,%eax
  35:	c4 e2 f8 f7 d7       	bextr  %rax,%rdi,%rdx
  3a:	b8 16 06 00 00       	mov    $0x616,%eax
  3f:	c4 e2 f8 f7 c7       	bextr  %rax,%rdi,%rax
  44:	83 e7 3f             	and    $0x3f,%edi
  47:	48 01 d0             	add    %rdx,%rax
  4a:	48 01 f8             	add    %rdi,%rax
  4d:	c3                   	retq


---


### compiler : `gcc`
### title : `AVX vcvtsd2ss lets us avoid PXOR dependency breaking for scalar float<->double and other scalar xmm,xmm instructions`
### open_at : `2019-01-26T10:37:40Z`
### last_modified_date : `2022-05-09T05:36:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89071
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
float cvt(double unused, double xmm1) { return xmm1; }

g++ (GCC-Explorer-Build) 9.0.0 20190120 (experimental):

        vxorps  %xmm0, %xmm0, %xmm0
        vcvtsd2ss       %xmm1, %xmm0, %xmm0    # merge into XMM0

clang7.0
        vcvtsd2ss       %xmm1, %xmm1, %xmm0    # both sources are from XMM1, no false dep

gcc already uses this trick for SQRTSS/SD, but not for float<->double conversion.  I haven't checked all the other scalar instructions, but roundss for floor() does neither and has a false dependency.  (i.e. it chooses the output register as the merge-target, not the actual input.)

 return floorf(x);  ->   vroundss        $9, %xmm1, %xmm0, %xmm0

Some testcases:

https://godbolt.org/z/-rqUVZ


---

In SSE, one-input scalar instructions like CVT* and SQRTSS/SD have an output dependency because of Intel's short-sighted ISA design optimizing for Pentium-III's 64-bit SIMD: zero-extending to fill the destination XMM register would have cost an extra uop to write the upper half of the destination.

For consistency(?), SSE2 scalar instructions (new with Pentium 4 which had 128-bit SIMD execution units / register file) have the same behaviour of merging into the low 64 bits of the destination, even conversion between double and float between two xmm registers, which didn't exist before SSE2.  (Previously conversion instructions were only between float in XMM and integers in scalar or MMX regs, or packed-integer <-> ps which filled the whole XMM reg and thus avoided a false dependency).

(Fortunately this isn't a problem for 2-input instructions like ADDSS: the operation already depends on both registers.)

---

The VEX encoding makes the merge-target separate from the actual destination, so we can finally avoid false dependencies without wasting an instruction breaking it.  (When the source is already in an XMM register).


For instructions where the source isn't an XMM register (e.g. memory or integer reg for int->FP conversions), one zeroed register can be used as a read-only merge target by any number of scalar AVX instructions, including in a loop.  That's bug 80571.


(It's unfortunate that Intel didn't take the opportunity to give the AVX versions subtly different semantics, and zero-extend into the target register.  That would probably have enabled vcvtsd2ss to be single-uop instead of 2 on Sandybridge-family.  IDK if they didn't think of that, or if they wanted strict consistency with the semantics of the SSE version, or if they thought decoding / internals would be easier if they didn't have to omit the merge-into-destination part of the scalar operation.  At least they made the extra dependency an explicit input, so we can choose a register other than the destination, but it's so rarely useful to actually merge into the low 64 or 32 of another reg that it's just long-term harmful to gimp the ISA with an extra dependency for these instructions, especially integer->FP.)



(I suspect that most of the dep-breaking gcc does isn't gaining any speed, but the trick is figuring out when we can omit it while being sure that we don't couple things into one big loop-carried chain, or serialize some things that OoO exec could otherwise benefit from hiding.  Within one function with no calls, we might be able to prove that a false dep isn't serializing anything important (e.g. if there's already enough ILP and something else breaks a dep on that register between loop iterations), but in general it's hard if we can't pick a register that was already part of the dep chain that led to the input for this operation, and thus is harmless to introduce a dep on.)

----

Relevant instructions that can exist in scalar xmm,xmm form:

VROUNDSS/SD  (gcc leaves a false dep, clang gets it right)

VSQRTSS/SD  (gcc already gets this right)
VRCPSS
VRSQRTSS  haven't checked

[V]CVTSS2SD xmm,xmm  (Skylake: SRC1/output dependency is a separate 1c latency 32-bit merge uop)
  The memory-source version is still 2 uops.

[V]CVTSD2SS xmm,xmm  (Skylake: SRC1/output dependency is the main 4c conversion uop, the extra uop is first, maybe extracting 32 bits from the src?)
 The memory-source version of [V]CVTSD2SS is only 1 uop!

So avoiding a false dep by loading with MOVSS/MOVSD and then using the reg-reg version is a bad idea for CVTSD2SS.  It's actually much better to PXOR and then CVTSD2SS (mem), %xmm, so clang's strategy of loading and then reg-reg conversion is a missed-optimization.

I haven't checked on micro-fusion of indexed addressing modes with either of those.

It doesn't look like a scalar load and then using a packed conversion would be good either.  CVTPS2PD and PD2PS xmm,xmm both need a port 5 uop as well as the FMA-port uop to actually do the conversion.  (Maybe to shuffle from/to 2 floats in the low 64 vs. 2 doubles filling a register?)

---

I've mostly only looked at Skylake numbers for this, not AMD or KNL, or earlier Intel.  Using the same input as both source operands when the source is XMM is just pure win everywhere, though, when VEX encoding is available.

Tricks for what to do without AVX, or with a memory source, might depend on -mtune, but this bug report is about making sure we use SRC1=SRC2 for one-input V...SS and V...SD instructions, choosing the input that's already needed as the merge target to grab the upper bits from.


----

Other conversions are only packed, or only between GP/mem and XMM.  Note that Agner Fog's skylake numbers for CVTSI2SD are wrong: it's 1c throughput for xmm,r32/r64 and 0.5c throughput for xmm, m32/m64.  He lists it as 2c throughput for both.

CVTDQ2PD with a memory source is 1 micro-fused uop, not 2.  The xmm,xmm version is 2 uops, including a port 5 shuffle(?).  I guess the memory-source version uses a broadcast load to get the data where the FMA/convert unit wants it.

Other than than, Agner Fog's numbers (https://agner.org/optimize) on Skylake match my testing with perf counters.


---


### compiler : `gcc`
### title : `[x86] suboptimal code generated for condition expression returning negation`
### open_at : `2019-01-27T17:35:49Z`
### last_modified_date : `2021-08-19T20:27:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89081
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `middle-end`
### version : `9.0`
### severity : `enhancement`
### contents :
Let's consider this trivial function:

---clamp.c---
#include <stdint.h>

uint64_t clamp1(int64_t x) {
    return (x < 0) ? -x : 0;
}
---eof---

$ gcc --version
gcc (GCC) 9.0.0 20190117 (experimental)

$ gcc -O3 -march=skylake clamp.c -c -S && cat clamp.s
clamp1:
	movq	%rdi, %rax
	negq	%rax
	movl	$0, %edx
	testq	%rdi, %rdi
	cmovns	%rdx, %rax
	ret

This procedure can be way shorter, like this

clamp1:
    xorq   %rax, %rax     # res = 0
    negq   %rdi           # -x, sets SF
    cmovns %rdi, %rax
    ret

One thing I observed recently when looking at assembly is that GCC
never modifies input registers %rdi or %rsi, always makes their
copies -- -thus the proposed shorter version is not possible.
However, clang modifies these registers, so seems the ABI allows this.


---


### compiler : `gcc`
### title : `rtx_cost of VEC_SELECT, VEC_CONCAT and VEC_DUPLICATE with memory operands is wrong`
### open_at : `2019-01-30T09:45:19Z`
### last_modified_date : `2020-02-14T08:59:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89114
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
Split out from PR89049.  On its testcase combine is willing to elide an
unnecessary %ymm build-up but the targets RTX cost makes that not profitable.

See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89049#c5

So with (the bogus)

Index: gcc/config/i386/i386.c
===================================================================
--- gcc/config/i386/i386.c      (revision 268383)
+++ gcc/config/i386/i386.c      (working copy)
@@ -40848,7 +40848,7 @@ ix86_rtx_costs (rtx x, machine_mode mode
         recognizable.  In which case they all pretty much have the
         same cost.  */
      *total = cost->sse_op;
-     return true;
+     return false;
     case VEC_MERGE:
       mask = XEXP (x, 2);
       /* This is masked instruction, assume the same cost,

we get combine to do

Trying 11 -> 25:
   11: r105:V8SF=vec_concat(r106:V4SF,[r85:DI+0x10])
   25: r111:V4SF=vec_select(r105:V8SF,parallel)
      REG_DEAD r105:V8SF
Successfully matched this instruction:
(set (reg:V4SF 111)
    (mem:V4SF (plus:DI (reg:DI 85 [ ivtmp.11 ])
            (const_int 16 [0x10])) [1 MEM[base: _2, offset: 0B]+16 S16 A32]))
allowing combination of insns 11 and 25
original costs 16 + 12 = 28
replacement cost 12

and we elide the %ymm build:

.L2:
        vmovups (%rdi), %xmm1
        addq    $32, %rdi
        vaddss  %xmm1, %xmm0, %xmm0
        vshufps $85, %xmm1, %xmm1, %xmm2
        vaddss  %xmm2, %xmm0, %xmm0
        vunpckhps       %xmm1, %xmm1, %xmm2
        vshufps $255, %xmm1, %xmm1, %xmm1
        vaddss  %xmm2, %xmm0, %xmm0
        vaddss  %xmm1, %xmm0, %xmm0
        vmovups -16(%rdi), %xmm1
        vshufps $85, %xmm1, %xmm1, %xmm2
        vaddss  %xmm1, %xmm0, %xmm0
        vaddss  %xmm2, %xmm0, %xmm0
        vunpckhps       %xmm1, %xmm1, %xmm2
        vshufps $255, %xmm1, %xmm1, %xmm1
        vaddss  %xmm2, %xmm0, %xmm0
        vaddss  %xmm1, %xmm0, %xmm0
        cmpq    %rdi, %rax
        jne     .L2

the patch is bogus because the intention of not scanning sub-rtxen was
to match the various shuffle patterns which do sth like
(vec_select (vec_concat ..) ...).

Not sure if there's a helper in i386.c to extract/cost a single MEM
sub-rtx, but the course of action would be to properly do this
somehow.


---


### compiler : `gcc`
### title : `A missing optimization opportunity for a simple branch in loop`
### open_at : `2019-01-31T04:01:55Z`
### last_modified_date : `2021-12-12T12:56:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89134
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
For this simple case,

__attribute__((pure)) __attribute__((noinline)) int inc(int i)
{
        /* Do something else here */

        return i+1;
}
extern int do_something(void);
extern int b;
void test(int n)
{
        for (int i=0; i<n; i=inc(i))
        {
                if (b) {
                        b = do_something();
                }
        }
}


"GCC -O3" is generating a loop as below,

.L6:
        cbz     w1, .L5
        bl      do_something
        mov     w1, w0
        mov     w0, w19
        str     w1, [x21]
        bl      inc
        mov     w19, w0
        cmp     w20, w0
        bgt     .L6
.L3:
        ldp     x19, x20, [sp, 16]
        ldr     x21, [sp, 32]
        ldp     x29, x30, [sp], 48
        ret
.L5:
        mov     w0, w19
        bl      inc
        mov     w19, w0
        cmp     w20, w0
        ble     .L3
        mov     w0, w19
        bl      inc
        mov     w19, w0
        cmp     w20, w0
        bgt     .L5
        b       .L3
.L13:
        ret

But the loop with head at label .L5 can be completely optimized away, because inc is a pure function and it is essentially an empty loop doing nothing. Therefore the code can be optimized to be like,

.L6:
        cbz     w1, .L3
        bl      do_something
        mov     w1, w0
        mov     w0, w19
        str     w1, [x21]
        bl      inc
        mov     w19, w0
        cmp     w20, w0
        bgt     .L6
.L3:
        ldp     x19, x20, [sp, 16]
        ldr     x21, [sp, 32]
        ldp     x29, x30, [sp], 48
        ret


---


### compiler : `gcc`
### title : `[9 Regression] comparison of abs(i) against excessive constant less than UXXX_MAX no longer folded`
### open_at : `2019-01-31T19:08:24Z`
### last_modified_date : `2019-08-09T04:22:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89143
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
While looking into bug 89127 I noticed that while GCC 8 and prior fold the comparison to false in the if controlling expression below due to the limited range of i's type, GCC 9 no longer performs this folding unless the constant is  UCHAR_MAX and greater.  Same for short and SHRT_MAX + 1.

$ cat u.c && gcc -S -O2 -Wall -Wextra -Wtype-limits -fdump-tree-optimized=/dev/stdout u.c
void f (signed char i)
{
  if (__builtin_abs (i) > 128)
    __builtin_abort ();
}

;; Function f (f, funcdef_no=0, decl_uid=1906, cgraph_uid=1, symbol_order=0)

f (signed char i)
{
  unsigned char _1;

  <bb 2> [local count: 1073741824]:
  _1 = ABSU_EXPR <i_2(D)>;
  if (_1 > 128)
    goto <bb 3>; [0.00%]
  else
    goto <bb 4>; [100.00%]

  <bb 3> [count: 0]:
  __builtin_abort ();

  <bb 4> [local count: 1073741824]:
  return;

}


---


### compiler : `gcc`
### title : `GCC does not assume that two different external variables have different addresses`
### open_at : `2019-01-31T22:13:31Z`
### last_modified_date : `2019-02-01T09:02:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89145
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
The module:

**
extern int p;
extern int q;

int foo (void)
{
    return &p == &q;
}
**

is compiled by `gcc -O3' to:

**
foo:
        movl    $p, %eax
        cmpq    $q, %rax
        sete    %al
        movzbl  %al, %eax
        ret
**

When at least one of the variables is declared static, gcc's optimizer kicks in and yields:

**
foo:
        xorl    %eax, %eax
        ret
**

Either, `gcc' is missing an optimization in the case of external variables, or the addresses of different external variables can be the same, which sounds strange to me.

A test with clang showed that it always produces the latter, optimized version.


---


### compiler : `gcc`
### title : `Wrapping values in structures can make the optimizer blind`
### open_at : `2019-02-01T17:04:03Z`
### last_modified_date : `2019-02-04T09:33:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89152
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
GCC compiles the following C module

**
typedef void (*Cont) (void *f, int a);

int quux (int a);

static void g (Cont c, Cont d, int a)
{
    if (quux (a))
      g (c, d, a + 1);
    ((Cont) c) (d, a);
}

void bar (Cont, int a);

static void h (Cont d, int a)
{
    if (d != (Cont) bar)
      ((Cont) d) (d, a);
}

void foo (int a)
{
    g ((Cont) h, (Cont) bar, a);
}
**

to

**
h:
        cmpq    $bar, %rdi
        je      .L1
        jmp     *%rdi
.L1:
        ret
g.constprop.0:
        pushq   %rbx
        movl    %edi, %ebx
.L6:
        movl    %ebx, %edi
        call    quux
        testl   %eax, %eax
        jne     .L8
        popq    %rbx
        ret
.L8:
        addl    $1, %ebx
        jmp     .L6
foo:
        jmp     g.constprop.0
**

Apart from the fact that `h' should be eliminated (see bug ipa/89139), the resulting code looks rather optimal (maybe except for the unnecessary jump from f into g).

However, when I wrap the functions pointers that are passed around into a structure (to avoid having to do typecasts in the code above, for example) as in the following module

**
typedef struct cont
{
    void (*f) (struct cont, int a);
} Cont;

int quux (int a);

static void g (Cont c, Cont d, int a)
{
    if (quux (a))
      g (c, d, a + 1);
    c.f (d, a);
}

void bar (struct cont, int a);

static void h (Cont d, int a)
{
    if (d.f != bar)
      d.f (d, a);
}

void foo (int a)
{
    g ((Cont) { h }, (Cont) { bar }, a);
}
**

a lot of optimizations are missed:

**
h:
        cmpq    $bar, %rdi
        je      .L1
        jmp     *%rdi
.L1:
        ret
g.constprop.0:
        pushq   %r13
        movq    %rdi, %r13
        movl    %edx, %edi
        pushq   %r12
        movq    %rsi, %r12
        pushq   %rbp
        movl    %edx, %ebp
        call    quux
        testl   %eax, %eax
        jne     .L10
        movl    %ebp, %esi
        movq    %r12, %rdi
        popq    %rbp
        popq    %r12
        popq    %r13
        jmp     h
.L10:
        movq    %r12, %rsi
        movq    %r13, %rdi
        leal    1(%rbp), %edx
        call    g.constprop.0
        movl    %ebp, %esi
        movq    %r12, %rdi
        popq    %rbp
        popq    %r12
        popq    %r13
        jmp     h
foo:
        movl    %edi, %edx
        movl    $bar, %esi
        movl    $h, %edi
        jmp     g.constprop.0
**

Both compilations were done at -O2; -O3 makes things no better.

For a comparison, clang optimizes also the latter code, namely to:

**
foo:                                    # @foo
        pushq   %rbx
        movl    %edi, %ebx
.LBB0_1:                                # =>This Inner Loop Header: Depth=1
        movl    %ebx, %edi
        callq   quux
        addl    $1, %ebx
        testl   %eax, %eax
        jne     .LBB0_1
        popq    %rbx
        retq
**


---


### compiler : `gcc`
### title : `Suboptimal code generation for SSE intrinsics based rsqrt`
### open_at : `2019-02-01T22:02:47Z`
### last_modified_date : `2021-08-19T20:32:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89155
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `enhancement`
### contents :
#include <x86intrin.h>
float rsqrtf_a(float x) {
  return _mm_cvtss_f32(_mm_rsqrt_ss(_mm_set_ps1(x)));
}
float rsqrtf_b(float x) {
  return _mm_cvtss_f32(_mm_rsqrt_ss(_mm_set_ss(x)));
}
float rsqrtf_c(float x) {
  return _mm_cvtss_f32(_mm_rsqrt_ss(_mm_set_ps(0, 0, 0, x)));
}

https://godbolt.org/z/VrF-vM

All these functions should result in a single rsqrtss instruction, but currently GCC produces suboptimal code (Clang 3.9+ optimizes it perfectly). Related to bug 55016.


---


### compiler : `gcc`
### title : `Bogus -Wformat-overflow warning with value range known`
### open_at : `2019-02-02T18:04:14Z`
### last_modified_date : `2023-10-01T03:32:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89161
### status : `NEW`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `7.4.0`
### severity : `normal`
### contents :
The following code

$ cat bogus1.c
#include <stdio.h>

static char* print(const unsigned short a[2])
{
    static char buf[3];
    if (a[0]  &&  a[0] < a[1])
        sprintf(buf, ".%1u", (10 * a[0]) / a[1]);
    else
        *buf = '\0';
    return buf;
}

unsigned short array[2];

int main()
{
    printf("%s\n", print(array));
    return 0;
}

produces a lot of noise when compiled optimized (no warning without):

$ gcc -Wall -O6 -c bogus1.c
bogus1.c: In function ‘main’:
bogus1.c:7:24: warning: ‘%1u’ directive writing between 1 and 10 bytes into a region of size 2 [-Wformat-overflow=]
         sprintf(buf, ".%1u", (10 * a[0]) / a[1]);
                        ^~~
bogus1.c:7:22: note: directive argument in the range [0, 2147483647]
         sprintf(buf, ".%1u", (10 * a[0]) / a[1]);
                      ^~~~~~
bogus1.c:7:9: note: ‘sprintf’ output between 3 and 12 bytes into a destination of size 3
         sprintf(buf, ".%1u", (10 * a[0]) / a[1]);
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

even though the compiler could have figured out that because of the "if", the value range of the integer division expression is actually [0..9], which perfectly fits into the buffer provided for the sprintf() statement.

$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-pc-cygwin/7.4.0/lto-wrapper.exe
Target: x86_64-pc-cygwin
Configured with: /cygdrive/i/szsz/tmpp/gcc/gcc-7.4.0-1.x86_64/src/gcc-7.4.0/configure --srcdir=/cygdrive/i/szsz/tmpp/gcc/gcc-7.4.0-1.x86_64/src/gcc-7.4.0 --prefix=/usr --exec-prefix=/usr --localstatedir=/var --sysconfdir=/etc --docdir=/usr/share/doc/gcc --htmldir=/usr/share/doc/gcc/html -C --build=x86_64-pc-cygwin --host=x86_64-pc-cygwin --target=x86_64-pc-cygwin --without-libiconv-prefix --without-libintl-prefix --libexecdir=/usr/lib --enable-shared --enable-shared-libgcc --enable-static --enable-version-specific-runtime-libs --enable-bootstrap --enable-__cxa_atexit --with-dwarf2 --with-tune=generic --enable-languages=ada,c,c++,fortran,lto,objc,obj-c++ --enable-graphite --enable-threads=posix --enable-libatomic --enable-libcilkrts --enable-libgomp --enable-libitm --enable-libquadmath --enable-libquadmath-support --disable-libssp --enable-libada --disable-symvers --with-gnu-ld --with-gnu-as --with-cloog-include=/usr/include/cloog-isl --without-libiconv-prefix --without-libintl-prefix --with-system-zlib --enable-linker-build-id --with-default-libstdcxx-abi=gcc4-compatible --enable-libstdcxx-filesystem-ts
Thread model: posix
gcc version 7.4.0 (GCC)


---
