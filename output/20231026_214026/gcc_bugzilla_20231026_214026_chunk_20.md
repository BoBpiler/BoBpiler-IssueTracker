### Total Bugs Detected: 4649
### Current Chunk: 20 of 30
### Bugs in this Chunk: 160 (From bug 3041 to 3200)
---


### compiler : `gcc`
### title : `[8/9/10 Regression] [x86] Suboptimal optimization of stack usage when function call does not occur`
### open_at : `2020-12-15T14:46:17Z`
### last_modified_date : `2023-09-21T10:41:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98289
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
void f(bool cond)
{
    if (cond)
        __builtin_abort();
}

On x86 with current trunk and -O3, this results in :

f(bool):
  sub rsp, 8
  test dil, dil
  jne .L3
  add rsp, 8
  ret
f(bool) [clone .cold]:
.L3:
  call abort

This seems like a regression over GCC 7.5, which outputs :

f(bool):
  test dil, dil
  jne .L7
  rep ret
.L7:
  sub rsp, 8
  call abort

Along with LLVM, which has similar output. Only emitting the code to begin the call upon being asked to do seems quicker in the case where the call doesn't occur.


---


### compiler : `gcc`
### title : `multiple scalar FP accumulators auto-vectorize worse than scalar, including vector load + merge instead of scalar + high-half insert`
### open_at : `2020-12-15T15:51:26Z`
### last_modified_date : `2021-01-04T09:48:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98291
### status : `RESOLVED`
### tags : `missed-optimization, ssemmx`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
An FP reduction loop with 2 scalar accumulators auto-vectorizes into a mess, instead of effectively mapping each scalar to an element of one vector accumulator.  (Unless we use -ffast-math, then that happens.  clang gets it right even without -ffast-math).

double dotprod(const double *a, const double *b, unsigned long long n)
{
  double d1 = 0.0;
  double d2 = 0.0;

  for (unsigned long long i = 0; i < n; i += 2) {
    d1 += a[i] * b[i];
    d2 += a[i + 1] * b[i + 1];
  }

  return (d1 + d2);
}

https://godbolt.org/z/Kq48j9

With -ffast-math the nice sane loop we expect

.L3:
        movupd  (%rsi,%rax), %xmm0
        movupd  (%rdi,%rax), %xmm3
        addq    $1, %rdx
        addq    $16, %rax
        mulpd   %xmm3, %xmm0
        addpd   %xmm0, %xmm1
        cmpq    %rcx, %rdx
        jb      .L3


without: 

...
main loop
.L4:
        movupd  (%rcx,%rax), %xmm1        # 16-byte load
        movupd  (%rsi,%rax), %xmm3 
        movhpd  16(%rcx,%rax), %xmm1      # overwrite the high half of it!!
        movhpd  16(%rsi,%rax), %xmm3
        mulpd   %xmm3, %xmm1
        movupd  16(%rsi,%rax), %xmm3
        movlpd  8(%rsi,%rax), %xmm3
        addsd   %xmm1, %xmm2
        unpckhpd        %xmm1, %xmm1
        addsd   %xmm1, %xmm2
        movupd  16(%rcx,%rax), %xmm1
        movlpd  8(%rcx,%rax), %xmm1
        addq    $32, %rax
        mulpd   %xmm3, %xmm1
        addsd   %xmm1, %xmm0
        unpckhpd        %xmm1, %xmm1
        addsd   %xmm1, %xmm0
        cmpq    %rdx, %rax
        jne     .L4

The overall strategy is insane, but even some of the details are insane.  e.g. a 16-byte load into XMM1, and then overwriting the high half of that with a different double before reading it.  That's bad enough, but you'd expect movsd / movhpd to manually gather 2 doubles, without introducing the possibility of a cache-line split load for zero benefit.

Similarly, movupd / movlpd should have just loaded in the other order.  (Or since they're contiguous, movupd  8(%rsi,%rax), %xmm3 / shufpd.)

So beyond the bad overall strategy (which is likely worse than unrolled scalar), it might be worth checking for some of this kind of smaller-scale insanity somewhere later to make it less bad if some other inputs can trigger similar behaviour.

(This small-scale detecting of movupd / movhpd and using movsd / movhpd could be a separate bug, but if it's just a symptom of something that should never happen in the first place then it's not really its own bug at all.)


---


### compiler : `gcc`
### title : `Optimize away C return; in function returning integral/pointer`
### open_at : `2020-12-15T16:29:36Z`
### last_modified_date : `2023-09-21T13:55:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98292
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
This is related to the PR94779, but that one talks just about switches.

int
foo (int x)
{
  if (x > 20)
    return x;
}

int
bar (int x)
{
  if (x > 30)
    return 30;
}

int
baz (int x)
{
  if (x > 40)
    return x + 1U;
}

int
qux (int x)
{
  if (x > 50)
    return x | 32;
}

is optimized to just the return statement in C++ (because we add __builtin_unreachable() in that case), but not in C.
I think we should do that also in C, provided that there are no non-debug non-CLOBBER stmts on the branch with GIMPLE_RETURN without argument, and provided that on the other branch there are just very few cheap stmts guaranteed not to invoke UB/trap (appart from debug/CLOBBER stmts) plus the GIMPLE_RETURN with argument.
LLVM seems to optimize that (though not sure if it isn't because it incorrectly adds something like __builtin_unreachable () even for C).


---


### compiler : `gcc`
### title : `Failure to optimize sub loop into modulo-based pattern`
### open_at : `2020-12-15T18:57:24Z`
### last_modified_date : `2023-09-21T10:40:33Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98299
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f1(int n)
{
    while (n >= 45)
        n -= 45;

    return n;
}

This can be optimized into a modulo-based pattern, for example this :

int f2(int n)
{
    int tmp = n > 44 ? n : 44;
    return ((tmp % 45) - tmp) + n;
}

This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `cmov seems not to swap condition during RA`
### open_at : `2020-12-15T21:23:52Z`
### last_modified_date : `2023-09-21T10:39:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98303
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
int f1(int n)
{
    while (n >= 64)
        n -= 64;

    return n;
}

On x86, with -O3, this generates :

f1(int):
  mov eax, edi
  and eax, 63
  cmp edi, 63
  cmovle eax, edi
  ret

This code :

int f2(int n)
{
    return (n <= 63) ? n : (n & 63);
}

which behaves equivalently, seems to result in poorer generated code :

f2(int):
  mov edx, edi
  mov eax, edi
  and edx, 63
  cmp edi, 63
  cmovg eax, edx
  ret


---


### compiler : `gcc`
### title : `Failure to optimize bitwise arithmetic pattern`
### open_at : `2020-12-15T21:54:11Z`
### last_modified_date : `2023-09-21T10:38:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98304
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f1(int n)
{
    return n - (((n > 63) ? n : 63) & -64);
}

This can be optimized to `return (n <= 63) ? n : (n & 63);` (and presumably the same optimization should be doable with other powers of 2).

PS: I found this optimization while looking at how this :

int f1(int n)
{
    while (n >= 64)
        n -= 64;

    return n;
}

is optimized. LLVM outputs the first example I gave here, while GCC outputs the optimization I gave here.


---


### compiler : `gcc`
### title : `[AVX512] Missing expander for ldexpm3.`
### open_at : `2020-12-16T05:58:53Z`
### last_modified_date : `2021-08-16T09:42:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98309
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Cat test.c

#include<math.h>
double
foo (double r, int n)
{
    return ldexp(r, n);
}

gcc -Ofast -mavx512f generate

        jmp     ldexp

But it could be better with

        vcvtsi2sd xmm16, xmm16, edi
        vscalefsd xmm0, xmm0, xmm16
        ret  

Similar for ldexpf, except there could be some precison loss in the conversion from 32-bit integer to float.
Also the instruction vscalefsd would response for situations
  1.the result overflow or underflow.
  2.the operands is NAN

SCALE(SRC1, SRC2)
{
; Check for denormal operands
TMP_SRC2 := SRC2
TMP_SRC1 := SRC1
IF (SRC2 is denormal AND MXCSR.DAZ) THEN TMP_SRC2=0
IF (SRC1 is denormal AND MXCSR.DAZ) THEN TMP_SRC1=0
/* SRC2 is a 64 bits floating-point value */
DEST[63:0] := TMP_SRC1[63:0] * POW(2, Floor(TMP_SRC2[63:0]))
}

So limit both expander under flag_unsafe_math_optimizations?
Similar for vector version.



cut from Intel SDM
VSCALEFPD—Scale Packed Float64 Values With Float64 Values
-----------------------
Performs a floating-point scale of the packed double-precision floating-point value in the first source operand by
multiplying it by 2 power of the double-precision floating-point value in second source operand.
The equation of this operation is given by:
xmm1 := xmm2*2 floor(xmm3) . ------------------------ Here



Floor(xmm3) means maximum integer value ≤ xmm3.
If the result cannot be represented in double precision, then the proper overflow response (for positive scaling
operand), or the proper underflow response (for negative scaling operand) is issued. The overflow and underflow
responses are dependent on the rounding mode (for IEEE-compliant rounding), as well as on other settings in
MXCSR (exception mask bits, FTZ bit), and on the SAE bit.
EVEX encoded version: The first source operand is an XMM register. The second source operand is an XMM register
or a memory location. The destination operand is an XMM register conditionally updated with writemask k1.


---


### compiler : `gcc`
### title : `Failure to optimally optimize add loop to mul`
### open_at : `2020-12-16T18:48:42Z`
### last_modified_date : `2023-09-21T10:37:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98334
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int i, unsigned int n) {
    int result = 0;
    while (n > 0) {
        result += i;
        n -= 1;
    }
    return result;
}

GCC optimizes this function to code that effectively does `return (n == 0) ? 0 : (i - 1) * n + n;`. It could instead emit the more optimal `return (n == 0) ? 0 : i * n;`, or just `return i * n;` on platforms where multiplication is fast. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[9/10/11/12 Regression] Poor code generation for partial struct initialization`
### open_at : `2020-12-16T22:02:17Z`
### last_modified_date : `2022-03-14T18:44:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98335
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `9.0`
### severity : `normal`
### contents :
Consider the following code:

  struct Data {
    char a;
    int b;
  };

  char c;

  Data val(int idx) {
    return { c };
  }

On x86-64 (with sizeof(char) = 1 and sizeof(int) = 4), val() can be implemented with a single mov to %rax. With -O3, g++ produces the following inefficient output:

    xorl	%eax, %eax
    movb	$0, -18(%rsp)
    movabsq	$72057594037927935, %rdx
    movw	%ax, -20(%rsp)
    movl	$0, -24(%rsp)
    andq	-24(%rsp), %rdx
    movq	%rdx, %rax
    salq	$8, %rax
    movb	c(%rip), %al
    ret

Similar outputs are seen for any level of optimization above O0 on GCC 9, 10, and 11. The bug is not present in GCC 8.

Reasonable code is generated if the second field of the struct is explicitly initialized to a constant, either in the struct definition or the initializer.


---


### compiler : `gcc`
### title : `Failure to optimize out on-stack array construction when unneeded`
### open_at : `2020-12-16T22:25:51Z`
### last_modified_date : `2023-09-21T10:36:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98337
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(int x) {
    int a[] = {0, 1};
    return a[x];
}

On AMD64, with -O3, this is generated :

f(int):
  mov rax, QWORD PTR .LC0[rip]
  movsx rdi, edi
  mov QWORD PTR [rsp-8], rax
  mov eax, DWORD PTR [rsp-8+rdi*4]
  ret
.LC0:
  .long 0
  .long 1


LLVM generates this :

f(int): # @f(int)
  movsxd rax, edi
  mov eax, dword ptr [4*rax + .L__const.f(int).a]
  ret
.L__const.f(int).a:
  .long 0 # 0x0
  .long 1 # 0x1

It seems to me like not copying the entire array on the stack makes for faster code.


---


### compiler : `gcc`
### title : `GCC could not vectorize loop with conditional reduced add and store`
### open_at : `2020-12-17T06:03:16Z`
### last_modified_date : `2021-01-04T15:57:51Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98339
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For testcase

void foo(                
                int* restrict x,                      
                int n,
                int start,   
                int m,
                int* restrict ret
)   
{
    for (int i = 0; i < n; i++)
    {
        int pos = start + i;
        if ( pos <= m)
            ret[0] += x[i];    
    }
}

with -O3 -mavx2 it could not be vectorized because ret[0] += x[i] is zero step MASK_STORE inside loop, and dr analysis failed for zero step store.

But with manually loop store motion

void foo2(                
                int* restrict x,                      
                int n,
                int start,   
                int m,
                int* restrict ret
)   
{
    int tmp = 0;

    for (int i = 0; i < n; i++)
    {
        int pos = start + i;
        if (pos <= m)
            tmp += x[i];    
    }

    ret[0] += tmp;
}

could be vectorized. 

godbolt: https://godbolt.org/z/Kcv8hP

There is no LIM between ifcvt and vect, and current LIM could not handle MASK_STORE. Is there any possibility to vectorize foo, like by doing loop store motion in ifcvt instead of creating MASK_STORE?


---


### compiler : `gcc`
### title : `[10 Regression] GCC 10.2 AVX512 Mask regression from GCC 9`
### open_at : `2020-12-17T14:43:30Z`
### last_modified_date : `2022-01-12T06:15:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98348
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
In GCC 9, vector comparisons on 128 and 256bit vectors on a AVX512 machine used vpcmpeqd without any masks.

In GCC 10, for 128bit and 256bit vectors, AVX512 mask instructions are used.
https://gcc.godbolt.org/z/1sPzM5

GCC 10 should follow GCC 9 for vector comparisons when a mask is not needed.

The reason why is https://uops.info/table.html shows that using mask registers makes 128/256/512 operations have a throughput of 1 and a latency of 3.

However, using a vector comparison directly has a throughput of 2 and a latency of 1.


---


### compiler : `gcc`
### title : `Reassociation breaks FMA chains`
### open_at : `2020-12-17T15:24:41Z`
### last_modified_date : `2023-05-30T06:03:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98350
### status : `NEW`
### tags : `missed-optimization, patch`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Consider the testcase:

#define N 1024
double a[N];
double b[N];
double c[N];
double d[N];
double e[N];
double f[N];
double g[N];
double h[N];
double j[N];
double k[N];
double l[N];
double m[N];
double o[N];
double p[N];


void
foo (void)
{
  for (int i = 0; i < N; i++)
  {
    a[i] += b[i]* c[i] + d[i] * e[i] + f[i] * g[i] + h[i] * j[i] + k[i] * l[i] + m[i]* o[i] + p[i];
  }
}

For -Ofast --param=tree-reassoc-width=1 GCC generates the loop:
.L2:
        ldr     q1, [x1, x0]
        ldr     q0, [x12, x0]
        ldr     q3, [x14, x0]
        fadd    v0.2d, v0.2d, v1.2d
        ldr     q1, [x13, x0]
        ldr     q2, [x11, x0]
        fmla    v0.2d, v3.2d, v1.2d
        ldr     q1, [x10, x0]
        ldr     q3, [x9, x0]
        fmla    v0.2d, v2.2d, v1.2d
        ldr     q1, [x8, x0]
        ldr     q2, [x7, x0]
        fmla    v0.2d, v3.2d, v1.2d
        ldr     q1, [x6, x0]
        ldr     q3, [x5, x0]
        fmla    v0.2d, v2.2d, v1.2d
        ldr     q1, [x4, x0]
        ldr     q2, [x3, x0]
        fmla    v0.2d, v3.2d, v1.2d
        ldr     q1, [x2, x0]
        fmla    v0.2d, v2.2d, v1.2d
        str     q0, [x1, x0]
        add     x0, x0, 16
        cmp     x0, 8192
        bne     .L2

with --param=tree-reassoc-width=4 it generates:
.L2:
        ldr     q5, [x11, x0]
        ldr     q4, [x7, x0]
        ldr     q0, [x3, x0]
        ldr     q3, [x12, x0]
        ldr     q1, [x8, x0]
        ldr     q2, [x4, x0]
        fmul    v3.2d, v3.2d, v5.2d
        fmul    v1.2d, v1.2d, v4.2d
        fmul    v2.2d, v2.2d, v0.2d
        ldr     q16, [x1, x0]
        ldr     q18, [x14, x0]
        ldr     q17, [x13, x0]
        ldr     q0, [x2, x0]
        ldr     q7, [x10, x0]
        ldr     q6, [x9, x0]
        ldr     q5, [x6, x0]
        ldr     q4, [x5, x0]
        fmla    v3.2d, v18.2d, v17.2d
        fadd    v0.2d, v0.2d, v16.2d
        fmla    v1.2d, v7.2d, v6.2d
        fmla    v2.2d, v5.2d, v4.2d
        fadd    v0.2d, v0.2d, v3.2d
        fadd    v1.2d, v1.2d, v2.2d
        fadd    v0.2d, v0.2d, v1.2d
        str     q0, [x1, x0]
        add     x0, x0, 16
        cmp     x0, 8192
        bne     .L2

The reassociation is evident. The problem here is that the fmla chains are something we'd want to preserve.
Is there a way we can get the reassoc pass to handle FMAs more intelligently?


---


### compiler : `gcc`
### title : `Bounds check not eliminated`
### open_at : `2020-12-17T17:09:55Z`
### last_modified_date : `2021-09-05T00:21:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98357
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
#include <stdlib.h>
char foo(char* data, size_t len, size_t i, size_t j) {
    if (i < len && j <= i) {
        if (j < len) {
            return data[j];
        } else {
            exit(1);
        }
    } else {
        return 0;
    }
}


compiles to:

foo(char*, unsigned long, unsigned long, unsigned long):
        cmp     rdx, rsi
        jnb     .L4
        cmp     rdx, rcx
        jb      .L4
        cmp     rsi, rcx
        jbe     .L3
        movzx   eax, BYTE PTR [rdi+rcx]
        ret
.L4:
        xor     eax, eax
        ret
.L3:
        push    rax
        mov     edi, 1
        call    exit

The call to exit should be eliminated.


---


### compiler : `gcc`
### title : `[modules] unnneded global constructors are emitted for a module`
### open_at : `2020-12-17T23:43:40Z`
### last_modified_date : `2022-01-01T05:48:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98364
### status : `NEW`
### tags : `missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `enhancement`
### contents :
the module will generate _ZGIW5helloEv which is dead code. There is no way to remove them.

naive binary without module:

#include<cstdio>

int main()
{
	puts("Hello World\n");
}

Disassembly of section .text:

0000000000401040 <main>:
  401040:	48 83 ec 08          	sub    $0x8,%rsp
  401044:	bf 04 20 40 00       	mov    $0x402004,%edi
  401049:	e8 e2 ff ff ff       	callq  401030 <puts@plt>
  40104e:	31 c0                	xor    %eax,%eax
  401050:	48 83 c4 08          	add    $0x8,%rsp
  401054:	c3                   	retq   
  401055:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
  40105c:	00 00 00 
  40105f:	90                   	nop

0000000000401060 <set_fast_math>:
  401060:	f3 0f 1e fa          	endbr64 
  401064:	0f ae 5c 24 fc       	stmxcsr -0x4(%rsp)
  401069:	81 4c 24 fc 40 80 00 	orl    $0x8040,-0x4(%rsp)
  401070:	00 
  401071:	0f ae 54 24 fc       	ldmxcsr -0x4(%rsp)
  401076:	c3                   	retq   
  401077:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
  40107e:	00 00 


module binary:

0000000000401050 <main>:
  401050:	48 83 ec 08          	sub    $0x8,%rsp
  401054:	bf 04 20 40 00       	mov    $0x402004,%edi
  401059:	e8 d2 ff ff ff       	callq  401030 <puts@plt>
  40105e:	31 c0                	xor    %eax,%eax
  401060:	48 83 c4 08          	add    $0x8,%rsp
  401064:	c3                   	retq   
  401065:	66 66 2e 0f 1f 84 00 	data16 nopw %cs:0x0(%rax,%rax,1)
  40106c:	00 00 00 00 

0000000000401070 <_GLOBAL__sub_I_main>:
  401070:	e9 0b 00 00 00       	jmpq   401080 <_ZGIW5helloEv>
  401075:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
  40107c:	00 00 00 
  40107f:	90                   	nop

0000000000401080 <_ZGIW5helloEv>:
  401080:	80 3d b2 2f 00 00 00 	cmpb   $0x0,0x2fb2(%rip)        # 404039 <_ZZ13_ZGIW5helloEvE9__in_chrg>
  401087:	75 07                	jne    401090 <_ZGIW5helloEv+0x10>
  401089:	c6 05 a9 2f 00 00 01 	movb   $0x1,0x2fa9(%rip)        # 404039 <_ZZ13_ZGIW5helloEvE9__in_chrg>
  401090:	c3                   	retq   
  401091:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
  401098:	00 00 00 
  40109b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

00000000004010a0 <set_fast_math>:
  4010a0:	f3 0f 1e fa          	endbr64 
  4010a4:	0f ae 5c 24 fc       	stmxcsr -0x4(%rsp)
  4010a9:	81 4c 24 fc 40 80 00 	orl    $0x8040,-0x4(%rsp)
  4010b0:	00 
  4010b1:	0f ae 54 24 fc       	ldmxcsr -0x4(%rsp)
  4010b6:	c3                   	retq   
  4010b7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
  4010be:	00 00


---


### compiler : `gcc`
### title : `Miss vectoization for signed char ifcvt`
### open_at : `2020-12-18T01:13:28Z`
### last_modified_date : `2021-09-17T06:41:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98365
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c

int foo (char a[64], char c[64])
{
  int i;
  char cnt=0;
  for (int i = 0;i != 64; i++)
    if (a[i] == c[i])
      cnt++;
  return cnt;
}

with -Ofast -mavx2 gcc failed to vectorize the loop due to 

dump of loop body:
-----------
  # cnt_21 = PHI <cnt_9(7), 0(15)>
  # i_22 = PHI <i_17(7), 0(15)>
  # ivtmp_19 = PHI <ivtmp_18(7), 64(15)>
  _1 = (sizetype) i_22;
  _2 = a_14(D) + _1;
  _3 = *_2;
  _5 = c_15(D) + _1;
  _6 = *_5;
  cnt.1_7 = (unsigned char) cnt_21;
  _8 = cnt.1_7 + 1;
  cnt_16 = (char) _8;
  cnt_9 = _3 == _6 ? cnt_16 : cnt_21;
  i_17 = i_22 + 1;
  ivtmp_18 = ivtmp_19 - 1;
----------

-fopt-info
---------
test.c:5:20: note:   vec_stmt_relevant_p: stmt live but not relevant.
test.c:5:20: note:   mark relevant 1, live 1: cnt_9 = _3 == _6 ? cnt_16 : cnt_21;
test.c:5:20: note:   init: stmt relevant? i_17 = i_22 + 1;
test.c:5:20: note:   init: stmt relevant? ivtmp_18 = ivtmp_19 - 1;
test.c:5:20: note:   init: stmt relevant? if (ivtmp_18 != 0)
test.c:5:20: note:   worklist: examine stmt: cnt_9 = _3 == _6 ? cnt_16 : cnt_21;
test.c:5:20: note:   vect_is_simple_use: operand *_2, type of def: internal
test.c:5:20: note:   mark relevant 1, live 0: _3 = *_2;
test.c:5:20: note:   vect_is_simple_use: operand *_5, type of def: internal
test.c:5:20: note:   mark relevant 1, live 0: _6 = *_5;
test.c:5:20: note:   vect_is_simple_use: operand (char) _8, type of def: internal
test.c:5:20: note:   mark relevant 1, live 0: cnt_16 = (char) _8;
test.c:5:20: note:   vect_is_simple_use: operand cnt_21 = PHI <cnt_9(7), 0(15)>, type of def: unknown
test.c:5:20: missed:   Unsupported pattern.
----------------
Shouldn't cnt_21 = PHI <cnt_9(7), 0(15)>, stmt relevant?


BTW: with extra -fwrapv, gcc successfully vectorized the loop.


---


### compiler : `gcc`
### title : `GCC >= 6 cannot inline _mm_cmp_ps on SSE targets`
### open_at : `2020-12-19T05:36:39Z`
### last_modified_date : `2020-12-20T02:53:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98387
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
https://gcc.godbolt.org/z/493ead

GCC since version 6.1 cannot inline _mm_cmp_ps on targets supporting only SSE (Nehalem, Tremont etc). From >= SandyBridge, everything inlines fine.

_mm_cmp_ps is called by using it as a function argument (ie auto function).

All SSE only machines use a jmp to _mm_cmp_ps, but it should be inlined.

O3 ffast-math is also used, and the function is declared inline.


---


### compiler : `gcc`
### title : `x86: Awful code generation for shifting vectors`
### open_at : `2020-12-20T14:13:11Z`
### last_modified_date : `2023-09-21T10:36:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98399
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef char U __attribute__((vector_size(16)));

U f(U u)
{
  return u >> (u & 1);
}

When compiled with -O3, on LLVM, this code generates this :

.LCPI0_0:
  .zero 16,1
f(char __vector(16)): # @f(char __vector(16))
  movdqa xmm3, xmmword ptr [rip + .LCPI0_0] # xmm3 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
  pand xmm3, xmm0
  punpckhbw xmm1, xmm0 # xmm1 = xmm1[8],xmm0[8],xmm1[9],xmm0[9],xmm1[10],xmm0[10],xmm1[11],xmm0[11],xmm1[12],xmm0[12],xmm1[13],xmm0[13],xmm1[14],xmm0[14],xmm1[15],xmm0[15]
  psllw xmm3, 5
  punpckhbw xmm4, xmm3 # xmm4 = xmm4[8],xmm3[8],xmm4[9],xmm3[9],xmm4[10],xmm3[10],xmm4[11],xmm3[11],xmm4[12],xmm3[12],xmm4[13],xmm3[13],xmm4[14],xmm3[14],xmm4[15],xmm3[15]
  pxor xmm2, xmm2
  pxor xmm5, xmm5
  pcmpgtw xmm5, xmm4
  movdqa xmm6, xmm5
  pandn xmm6, xmm1
  psraw xmm1, 4
  pand xmm1, xmm5
  por xmm1, xmm6
  paddw xmm4, xmm4
  pxor xmm5, xmm5
  pcmpgtw xmm5, xmm4
  movdqa xmm6, xmm5
  pandn xmm6, xmm1
  psraw xmm1, 2
  pand xmm1, xmm5
  por xmm1, xmm6
  paddw xmm4, xmm4
  pxor xmm5, xmm5
  pcmpgtw xmm5, xmm4
  movdqa xmm4, xmm5
  pandn xmm4, xmm1
  psraw xmm1, 1
  pand xmm1, xmm5
  por xmm1, xmm4
  psrlw xmm1, 8
  punpcklbw xmm0, xmm0 # xmm0 = xmm0[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7]
  punpcklbw xmm3, xmm3 # xmm3 = xmm3[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7]
  pxor xmm4, xmm4
  pcmpgtw xmm4, xmm3
  movdqa xmm5, xmm4
  pandn xmm5, xmm0
  psraw xmm0, 4
  pand xmm0, xmm4
  por xmm0, xmm5
  paddw xmm3, xmm3
  pxor xmm4, xmm4
  pcmpgtw xmm4, xmm3
  movdqa xmm5, xmm4
  pandn xmm5, xmm0
  psraw xmm0, 2
  pand xmm0, xmm4
  por xmm0, xmm5
  paddw xmm3, xmm3
  pcmpgtw xmm2, xmm3
  movdqa xmm3, xmm2
  pandn xmm3, xmm0
  psraw xmm0, 1
  pand xmm0, xmm2
  por xmm0, xmm3
  psrlw xmm0, 8
  packuswb xmm0, xmm1
  ret

Which is rather long, however GCC generates this :

f(char __vector(16)):
  push r15
  movd edx, xmm0
  push r14
  push r13
  push r12
  push rbp
  push rbx
  sub rsp, 400
  movdqa xmm1, XMMWORD PTR .LC0[rip]
  movaps XMMWORD PTR [rsp+376], xmm0
  movzx ebx, BYTE PTR [rsp+377]
  pand xmm1, xmm0
  movaps XMMWORD PTR [rsp+344], xmm0
  movzx ebp, BYTE PTR [rsp+346]
  movd ecx, xmm1
  movaps XMMWORD PTR [rsp+360], xmm1
  sar dl, cl
  movzx ecx, BYTE PTR [rsp+361]
  movaps XMMWORD PTR [rsp+328], xmm1
  movaps XMMWORD PTR [rsp+312], xmm0
  movzx edx, dl
  movzx r12d, BYTE PTR [rsp+315]
  sar bl, cl
  movzx ecx, BYTE PTR [rsp+330]
  movaps XMMWORD PTR [rsp+296], xmm1
  movaps XMMWORD PTR [rsp+280], xmm0
  movzx ebx, bl
  movzx r13d, BYTE PTR [rsp+284]
  sar bpl, cl
  movzx ecx, BYTE PTR [rsp+299]
  movaps XMMWORD PTR [rsp+264], xmm1
  movaps XMMWORD PTR [rsp+248], xmm0
  movzx ebp, bpl
  movzx r14d, BYTE PTR [rsp+253]
  sar r12b, cl
  movzx ecx, BYTE PTR [rsp+268]
  movaps XMMWORD PTR [rsp+232], xmm1
  movaps XMMWORD PTR [rsp+216], xmm0
  movzx r12d, r12b
  movzx r15d, BYTE PTR [rsp+222]
  sar r13b, cl
  movzx ecx, BYTE PTR [rsp+237]
  movaps XMMWORD PTR [rsp+200], xmm1
  movzx r13d, r13b
  sar r14b, cl
  movzx ecx, BYTE PTR [rsp+206]
  movaps XMMWORD PTR [rsp+184], xmm0
  movzx eax, BYTE PTR [rsp+191]
  movaps XMMWORD PTR [rsp+168], xmm1
  movzx r14d, r14b
  sar r15b, cl
  movzx ecx, BYTE PTR [rsp+175]
  movaps XMMWORD PTR [rsp+120], xmm0
  movzx edi, BYTE PTR [rsp+129]
  movaps XMMWORD PTR [rsp+152], xmm0
  movzx esi, BYTE PTR [rsp+160]
  movzx r15d, r15b
  sar al, cl
  movaps XMMWORD PTR [rsp+136], xmm1
  movzx ecx, BYTE PTR [rsp+144]
  movaps XMMWORD PTR [rsp+104], xmm1
  sar sil, cl
  movzx ecx, BYTE PTR [rsp+113]
  movaps XMMWORD PTR [rsp+88], xmm0
  mov BYTE PTR [rsp-89], sil
  sar dil, cl
  movaps XMMWORD PTR [rsp+72], xmm1
  movzx ecx, BYTE PTR [rsp+82]
  movzx esi, dil
  movzx edi, BYTE PTR [rsp+98]
  movaps XMMWORD PTR [rsp+56], xmm0
  movzx r8d, BYTE PTR [rsp+67]
  movaps XMMWORD PTR [rsp+40], xmm1
  sar dil, cl
  movzx ecx, BYTE PTR [rsp+51]
  movaps XMMWORD PTR [rsp+24], xmm0
  movzx r9d, BYTE PTR [rsp+36]
  movaps XMMWORD PTR [rsp+8], xmm1
  movzx edi, dil
  sar r8b, cl
  movzx ecx, BYTE PTR [rsp+20]
  movaps XMMWORD PTR [rsp-8], xmm0
  movzx r10d, BYTE PTR [rsp+5]
  movaps XMMWORD PTR [rsp-24], xmm1
  movzx r8d, r8b
  sar r9b, cl
  movzx ecx, BYTE PTR [rsp-11]
  mov BYTE PTR [rsp-120], al
  movaps XMMWORD PTR [rsp-40], xmm0
  movzx r9d, r9b
  sar r10b, cl
  movaps XMMWORD PTR [rsp-56], xmm1
  movzx ecx, BYTE PTR [rsp-42]
  movzx r11d, BYTE PTR [rsp-26]
  movaps XMMWORD PTR [rsp-72], xmm0
  movzx eax, BYTE PTR [rsp-57]
  movzx r10d, r10b
  sar r11b, cl
  movaps XMMWORD PTR [rsp-88], xmm1
  movzx ecx, BYTE PTR [rsp-73]
  movzx r11d, r11b
  sar al, cl
  movzx ecx, al
  movzx eax, BYTE PTR [rsp-120]
  sal rcx, 8
  sal rax, 8
  or rcx, r11
  or rax, r15
  sal rax, 8
  or rax, r14
  sal rax, 8
  or rax, r13
  sal rax, 8
  or rax, r12
  sal rax, 8
  or rax, rbp
  sal rax, 8
  or rax, rbx
  movzx ebx, BYTE PTR [rsp-89]
  sal rax, 8
  sal rcx, 8
  or rcx, r10
  or rax, rdx
  sal rcx, 8
  mov QWORD PTR [rsp-120], rax
  or rcx, r9
  sal rcx, 8
  or rcx, r8
  sal rcx, 8
  or rcx, rdi
  sal rcx, 8
  or rcx, rsi
  sal rcx, 8
  or rcx, rbx
  mov QWORD PTR [rsp-112], rcx
  movdqa xmm0, XMMWORD PTR [rsp-120]
  add rsp, 400
  pop rbx
  pop rbp
  pop r12
  pop r13
  pop r14
  pop r15
  ret
.LC0:
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1
  .byte 1

Using such flags as `-mavx2` seems to marginally improve the situation, but on LLVM results in far better code generation :

.LCPI0_0:
  .zero 16,1
f(char __vector(16)): # @f(char __vector(16))
  vpand xmm1, xmm0, xmmword ptr [rip + .LCPI0_0]
  vpsllw xmm1, xmm1, 5
  vpunpckhbw xmm2, xmm1, xmm1 # xmm2 = xmm1[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
  vpunpckhbw xmm3, xmm0, xmm0 # xmm3 = xmm0[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
  vpsraw xmm4, xmm3, 4
  vpblendvb xmm3, xmm3, xmm4, xmm2
  vpsraw xmm4, xmm3, 2
  vpaddw xmm2, xmm2, xmm2
  vpblendvb xmm3, xmm3, xmm4, xmm2
  vpsraw xmm4, xmm3, 1
  vpaddw xmm2, xmm2, xmm2
  vpblendvb xmm2, xmm3, xmm4, xmm2
  vpsrlw xmm2, xmm2, 8
  vpunpcklbw xmm1, xmm1, xmm1 # xmm1 = xmm1[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7]
  vpunpcklbw xmm0, xmm0, xmm0 # xmm0 = xmm0[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7]
  vpsraw xmm3, xmm0, 4
  vpblendvb xmm0, xmm0, xmm3, xmm1
  vpsraw xmm3, xmm0, 2
  vpaddw xmm1, xmm1, xmm1
  vpblendvb xmm0, xmm0, xmm3, xmm1
  vpsraw xmm3, xmm0, 1
  vpaddw xmm1, xmm1, xmm1
  vpblendvb xmm0, xmm0, xmm3, xmm1
  vpsrlw xmm0, xmm0, 8
  vpackuswb xmm0, xmm0, xmm2
  ret


---


### compiler : `gcc`
### title : `ldist might punt on too large expressions for detected patterns with -Os`
### open_at : `2020-12-20T21:08:38Z`
### last_modified_date : `2021-01-04T09:13:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98404
### status : `REOPENED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
int rotate_argv (const char** argv, int first, int mid, int end)
{
    const char** p = argv+first;
    int n1 = mid-first;
    int n2 = end-mid;
    int nm = n1+n2-1;
    for (int j = 0; j < n2; ++j) {
        const char* v = p[nm];
        for (int i = 0; i < nm; ++i)
            p[nm-i] = p[nm-i-1];
        p[0] = v;
    }
    return n1;
}

This bit of code unexpectedly emits a call to memmove to replace the inner copy loop. Such behavior is highly inappropriate, breaking the "what-you-see-is-what-you-get" spirit of C. Sure, the loop is equivalent to a memmove call, but if I wanted to call memmove, I would have called memmove. Doing it behind my back brings in code paths that may cause problems impossible to understand by looking at the code. Worse yet, the compiler only does this in the optimized build (-Os, -O2, and -O3, but not -O1 or -O0), making debugging of the resulting problem a beat-your-head-on-the-desk frustrating exercise.

The bug in my code was causing memory corruption in argv to happen in that inner loop, but looking at the code above will not reveal the problem, no matter how much you scream at the debugger. The bug was in my memmove implementation returning the wrong value, which the compiler then helpfully reloaded into p. Naturally, it's a good thing that I fixed the bug; having never used the return value of memmove myself I doubt I would have discovered it anytime soon. But this illustrates how a malicious exploit could be introduced into that loop without anybody being able to figure it out. Let's remember that we still have that LD_PRELOAD abomination.

On a more mundane note, replacing the loop with memmove causes the compiled code to grow from 107 bytes to 166. This is using the -Os, switch, of course. I have complained many times about how gcc doesn't care about size optimization and doesn't inline stuff because it can't understand that inserting a function call into code that currently has none has great costs of register saving and all that. I have by now resigned to having to #define inline inline __attribute__((always_inline)) everywhere, but will you perhaps someday reconsider your position that size optimization does not matter? If 55% code bloat in this example doesn't convince you, what will?

Finally, calling memmove will make the code slower, not faster, due to its much higher startup overhead that is justifiable for copying large blocks, but not for copying one or two elements, which is what the code above is made for. The conceit of the compiler, in thinking it knows better, thus results in worse outcome all around; in size, speed, and security.


---


### compiler : `gcc`
### title : `Superfluous sign-extend for constrained integer`
### open_at : `2020-12-23T10:19:15Z`
### last_modified_date : `2023-09-27T19:27:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98425
### status : `UNCONFIRMED`
### tags : `ABI, missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Hello everyone,

A small missed optimization I noticed while toying around with the difference between signed and unsigned integers. The following code

int
baz(int *p, int i) {
  int j;
  if (i >= 0) {
    j = i + 4;
    return p[j];
  } else
    __builtin_unreachable();
}

is compiled with `gcc -O3 -S` to

baz:
  addl  $4, %esi
  movslq  %esi, %rsi
  movl  (%rdi,%rsi,4), %eax
  ret

The movslq instruction is unnecessary since i is constrained to never be negative and therefore no sign extension is needed. This probably also prevents the addl instructions to be removed and the offset being put into the movl. 

For comparison, clang (with the same options) compiles the code to 

baz:
  movl %esi, %eax
  movl 16(%rdi, %rax, 4), %eax
  ret

Optimal would probably be

baz:
  movl 16(%rdi, %rsi, 4), %eax
  ret


---


### compiler : `gcc`
### title : `Some FMA expressions are evaluated less efficiently when -ffast-math is active`
### open_at : `2020-12-23T17:22:17Z`
### last_modified_date : `2021-09-06T18:37:02Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98429
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.2.1`
### severity : `normal`
### contents :
If you compile an expression like

    x - x*y

using fast-math optimizations it appears to get re-arranged to 

   x*(1-y)

which is (probably) never more efficient than the original expression, especially if the target supports FMA operations.

Simple godbolt demonstration: https://godbolt.org/z/4q1oj9

(This is very likely not C++ specific, but I'm not sure what the correct component is -- I also just tried this for AVX-FMA)


---


### compiler : `gcc`
### title : `C++20 module binary bloat by introducing iostream silently.`
### open_at : `2020-12-23T17:48:56Z`
### last_modified_date : `2021-12-13T14:14:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98430
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 49838
Preprocessed file

OH NO!!! PLEASE. NO IOSTREAM.!! We ban iostream and we do not use it.

g++ -S hello.cc main.cc -Ofast -std=c++20 -s -fmodules-ts

.LFE5373:
	.size	_GLOBAL__sub_I_main, .-_GLOBAL__sub_I_main
	.section	.init_array,"aw"
	.align 8
	.quad	_GLOBAL__sub_I_main
	.globl	_ZTVSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE
	.section	.rodata
	.align 8
	.type	_ZTVSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE, @object
	.size	_ZTVSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE, 120
_ZTVSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE:
	.quad	0
	.quad	_ZTISt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE
	.quad	_ZNSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEED1Ev
	.quad	_ZNSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEED0Ev
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRb
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRl
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRt
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRj
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRm
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRx
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRy
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRf
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRd
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRe
	.quad	_ZNKSt7num_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE6do_getES3_S3_RSt8ios_baseRSt12_Ios_IostateRPv
	.globl	_ZTVSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE
	.align 8
	.type	_ZTVSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE, @object
	.size	_ZTVSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE, 96
_ZTVSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE:
	.quad	0
	.quad	_ZTISt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE
	.quad	_ZNSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEED1Ev
	.quad	_ZNSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEED0Ev
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewb
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewl
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewm
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewx
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewy
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewd
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewe
	.quad	_ZNKSt7num_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE6do_putES3_RSt8ios_basewPKv
	.hidden	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE16_St13basic_ostreamIwS1_E
	.globl	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE16_St13basic_ostreamIwS1_E
	.align 8
	.type	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE16_St13basic_ostreamIwS1_E, @object
	.size	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE16_St13basic_ostreamIwS1_E, 80
_ZTCSt14basic_iostreamIwSt11char_traitsIwEE16_St13basic_ostreamIwS1_E:
	.quad	8
	.quad	0
	.quad	_ZTISt13basic_ostreamIwSt11char_traitsIwEE
	.quad	0
	.quad	0
	.quad	-8
	.quad	-8
	.quad	_ZTISt13basic_ostreamIwSt11char_traitsIwEE
	.quad	0
	.quad	0
	.hidden	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE0_St13basic_istreamIwS1_E
	.globl	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE0_St13basic_istreamIwS1_E
	.align 8
	.type	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE0_St13basic_istreamIwS1_E, @object
	.size	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE0_St13basic_istreamIwS1_E, 80
_ZTCSt14basic_iostreamIwSt11char_traitsIwEE0_St13basic_istreamIwS1_E:
	.quad	24
	.quad	0
	.quad	_ZTISt13basic_istreamIwSt11char_traitsIwEE
	.quad	0
	.quad	0
	.quad	-24
	.quad	-24
	.quad	_ZTISt13basic_istreamIwSt11char_traitsIwEE
	.quad	0
	.quad	0
	.globl	_ZTTSt14basic_iostreamIwSt11char_traitsIwEE
	.align 8
	.type	_ZTTSt14basic_iostreamIwSt11char_traitsIwEE, @object
	.size	_ZTTSt14basic_iostreamIwSt11char_traitsIwEE, 56
_ZTTSt14basic_iostreamIwSt11char_traitsIwEE:
	.quad	_ZTVSt14basic_iostreamIwSt11char_traitsIwEE+24
	.quad	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE0_St13basic_istreamIwS1_E+24
	.quad	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE0_St13basic_istreamIwS1_E+64
	.quad	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE16_St13basic_ostreamIwS1_E+24
	.quad	_ZTCSt14basic_iostreamIwSt11char_traitsIwEE16_St13basic_ostreamIwS1_E+64
	.quad	_ZTVSt14basic_iostreamIwSt11char_traitsIwEE+104
	.quad	_ZTVSt14basic_iostreamIwSt11char_traitsIwEE+64
	.globl	_ZTVSt14basic_iostreamIwSt11char_traitsIwEE
	.align 8
	.type	_ZTVSt14basic_iostreamIwSt11char_traitsIwEE, @object
	.size	_ZTVSt14basic_iostreamIwSt11char_traitsIwEE, 120
_ZTVSt14basic_iostreamIwSt11char_traitsIwEE:
	.quad	24
	.quad	0
	.quad	_ZTISt14basic_iostreamIwSt11char_traitsIwEE
	.quad	_ZNSt14basic_iostreamIwSt11char_traitsIwEED1Ev
	.quad	_ZNSt14basic_iostreamIwSt11char_traitsIwEED0Ev
	.quad	8
	.quad	-16
	.quad	_ZTISt14basic_iostreamIwSt11char_traitsIwEE
	.quad	_ZThn16_NSt14basic_iostreamIwSt11char_traitsIwEED1Ev
	.quad	_ZThn16_NSt14basic_iostreamIwSt11char_traitsIwEED0Ev
	.quad	-24
	.quad	-24
	.quad	_ZTISt14basic_iostreamIwSt11char_traitsIwEE
	.quad	_ZTv0_n24_NSt14basic_iostreamIwSt11char_traitsIwEED1Ev
	.quad	_ZTv0_n24_NSt14basic_iostreamIwSt11char_traitsIwEED0Ev
	.globl	_ZTTSt13basic_istreamIwSt11char_traitsIwEE
	.align 8
	.type	_ZTTSt13basic_istreamIwSt11char_traitsIwEE, @object
	.size	_ZTTSt13basic_istreamIwSt11char_traitsIwEE, 16
_ZTTSt13basic_istreamIwSt11char_traitsIwEE:
	.quad	_ZTVSt13basic_istreamIwSt11char_traitsIwEE+24
	.quad	_ZTVSt13basic_istreamIwSt11char_traitsIwEE+64
	.globl	_ZTVSt13basic_istreamIwSt11char_traitsIwEE
	.align 8
	.type	_ZTVSt13basic_istreamIwSt11char_traitsIwEE, @object
	.size	_ZTVSt13basic_istreamIwSt11char_traitsIwEE, 80
_ZTVSt13basic_istreamIwSt11char_traitsIwEE:
	.quad	16
	.quad	0
	.quad	_ZTISt13basic_istreamIwSt11char_traitsIwEE
	.quad	_ZNSt13basic_istreamIwSt11char_traitsIwEED1Ev
	.quad	_ZNSt13basic_istreamIwSt11char_traitsIwEED0Ev
	.quad	-16
	.quad	-16
	.quad	_ZTISt13basic_istreamIwSt11char_traitsIwEE
	.quad	_ZTv0_n24_NSt13basic_istreamIwSt11char_traitsIwEED1Ev
	.quad	_ZTv0_n24_NSt13basic_istreamIwSt11char_traitsIwEED0Ev
	.globl	_ZTTSt13basic_ostreamIwSt11char_traitsIwEE
	.align 8
	.type	_ZTTSt13basic_ostreamIwSt11char_traitsIwEE, @object
	.size	_ZTTSt13basic_ostreamIwSt11char_traitsIwEE, 16
_ZTTSt13basic_ostreamIwSt11char_traitsIwEE:
	.quad	_ZTVSt13basic_ostreamIwSt11char_traitsIwEE+24
	.quad	_ZTVSt13basic_ostreamIwSt11char_traitsIwEE+64
	.globl	_ZTVSt13basic_ostreamIwSt11char_traitsIwEE
	.align 8
	.type	_ZTVSt13basic_ostreamIwSt11char_traitsIwEE, @object
	.size	_ZTVSt13basic_ostreamIwSt11char_traitsIwEE, 80
_ZTVSt13basic_ostreamIwSt11char_traitsIwEE:
	.quad	8
	.quad	0
	.quad	_ZTISt13basic_ostreamIwSt11char_traitsIwEE
	.quad	_ZNSt13basic_ostreamIwSt11char_traitsIwEED1Ev
	.quad	_ZNSt13basic_ostreamIwSt11char_traitsIwEED0Ev
	.quad	-8
	.quad	-8
	.quad	_ZTISt13basic_ostreamIwSt11char_traitsIwEE
	.quad	_ZTv0_n24_NSt13basic_ostreamIwSt11char_traitsIwEED1Ev
	.quad	_ZTv0_n24_NSt13basic_ostreamIwSt11char_traitsIwEED0Ev
	.globl	_ZTVSt9basic_iosIwSt11char_traitsIwEE
	.align 8
	.type	_ZTVSt9basic_iosIwSt11char_traitsIwEE, @object
	.size	_ZTVSt9basic_iosIwSt11char_traitsIwEE, 32
_ZTVSt9basic_iosIwSt11char_traitsIwEE:
	.quad	0
	.quad	_ZTISt9basic_iosIwSt11char_traitsIwEE
	.quad	_ZNSt9basic_iosIwSt11char_traitsIwEED1Ev
	.quad	_ZNSt9basic_iosIwSt11char_traitsIwEED0Ev
	.globl	_ZTVSt15basic_streambufIwSt11char_traitsIwEE
	.align 8
	.type	_ZTVSt15basic_streambufIwSt11char_traitsIwEE, @object
	.size	_ZTVSt15basic_streambufIwSt11char_traitsIwEE, 128
_ZTVSt15basic_streambufIwSt11char_traitsIwEE:
	.quad	0
	.quad	_ZTISt15basic_streambufIwSt11char_traitsIwEE
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEED1Ev
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEED0Ev
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE5imbueERKSt6locale
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE6setbufEPwl
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE7seekoffElSt12_Ios_SeekdirSt13_Ios_Openmode
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE7seekposESt4fposI11__mbstate_tESt13_Ios_Openmode
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE4syncEv
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE9showmanycEv
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE6xsgetnEPwl
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE9underflowEv
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE5uflowEv
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE9pbackfailEj
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE6xsputnEPKwl
	.quad	_ZNSt15basic_streambufIwSt11char_traitsIwEE8overflowEj
	.globl	_ZTVNSt7__cxx118messagesIwEE
	.align 8
	.type	_ZTVNSt7__cxx118messagesIwEE, @object
	.size	_ZTVNSt7__cxx118messagesIwEE, 56
_ZTVNSt7__cxx118messagesIwEE:
	.quad	0
	.quad	_ZTINSt7__cxx118messagesIwEE
	.quad	_ZNSt7__cxx118messagesIwED1Ev
	.quad	_ZNSt7__cxx118messagesIwED0Ev
	.quad	_ZNKSt7__cxx118messagesIwE7do_openERKNS_12basic_stringIcSt11char_traitsIcESaIcEEERKSt6locale
	.quad	_ZNKSt7__cxx118messagesIwE6do_getEiiiRKNS_12basic_stringIwSt11char_traitsIwESaIwEEE
	.quad	_ZNKSt7__cxx118messagesIwE8do_closeEi
	.globl	_ZTVNSt7__cxx118numpunctIwEE
	.align 8
	.type	_ZTVNSt7__cxx118numpunctIwEE, @object
	.size	_ZTVNSt7__cxx118numpunctIwEE, 72
_ZTVNSt7__cxx118numpunctIwEE:
	.quad	0
	.quad	_ZTINSt7__cxx118numpunctIwEE
	.quad	_ZNSt7__cxx118numpunctIwED1Ev
	.quad	_ZNSt7__cxx118numpunctIwED0Ev
	.quad	_ZNKSt7__cxx118numpunctIwE16do_decimal_pointEv
	.quad	_ZNKSt7__cxx118numpunctIwE16do_thousands_sepEv
	.quad	_ZNKSt7__cxx118numpunctIwE11do_groupingEv
	.quad	_ZNKSt7__cxx118numpunctIwE11do_truenameEv
	.quad	_ZNKSt7__cxx118numpunctIwE12do_falsenameEv
	.globl	_ZTVNSt7__cxx118numpunctIcEE
	.align 8
	.type	_ZTVNSt7__cxx118numpunctIcEE, @object
	.size	_ZTVNSt7__cxx118numpunctIcEE, 72
_ZTVNSt7__cxx118numpunctIcEE:
	.quad	0
	.quad	_ZTINSt7__cxx118numpunctIcEE
	.quad	_ZNSt7__cxx118numpunctIcED1Ev
	.quad	_ZNSt7__cxx118numpunctIcED0Ev
	.quad	_ZNKSt7__cxx118numpunctIcE16do_decimal_pointEv
	.quad	_ZNKSt7__cxx118numpunctIcE16do_thousands_sepEv
	.quad	_ZNKSt7__cxx118numpunctIcE11do_groupingEv
	.quad	_ZNKSt7__cxx118numpunctIcE11do_truenameEv
	.quad	_ZNKSt7__cxx118numpunctIcE12do_falsenameEv
	.weak	_ZTSN7fast_io14error_reporterE
	.section	.rodata._ZTSN7fast_io14error_reporterE,"aG",@progbits,_ZTSN7fast_io14error_reporterE,comdat
	.align 16


---


### compiler : `gcc`
### title : `[AVX512] Missing expander for vashl<VI2_AVX512BW>, vlshr<VI2_AVX512BW>, vashr{v32hi,v16hi,v4di,v8di}`
### open_at : `2020-12-24T06:30:33Z`
### last_modified_date : `2021-06-24T05:03:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98434
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :



---


### compiler : `gcc`
### title : `[ARM NEON] Missed optimization in expanding vector constructor`
### open_at : `2020-12-24T06:56:54Z`
### last_modified_date : `2021-07-12T07:55:19Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98435
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
For the following test-case:

#include <arm_neon.h>

bfloat16x4_t f1 (bfloat16_t a)
{
  return vdup_n_bf16 (a);
}

bfloat16x4_t f2 (bfloat16_t a)
{
  return (bfloat16x4_t) {a, a, a, a};
}

Compiling with arm-linux-gnueabi -O3 -mfpu=neon -mfloat-abi=softfp  -march=armv8.2-a+bf16+fp16 results in f2 not being vectorized:

f1:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        vdup.16 d16, r0
        vmov    r0, r1, d16  @ v4bf
        bx      lr


f2:
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        mov     r3, r0  @ __bf16
        adr     r1, .L4
        ldrd    r0, [r1]
        mov     r2, r3  @ __bf16
        mov     ip, r3  @ __bf16
        bfi     r1, r2, #0, #16
        bfi     r0, ip, #0, #16
        bfi     r1, r3, #16, #16
        bfi     r0, r2, #16, #16
        bx      lr


.optimized dump shows:
bfloat16x4_t f1 (bfloat16_t a)
{
  __simd64_bfloat16_t _3;

  <bb 2> [local count: 1073741824]:
  _3 = __builtin_neon_vdup_nv4bf (a_2(D)); [tail call]
  return _3;

}

bfloat16x4_t f2 (bfloat16_t a)
{
  bfloat16x4_t _2;

  <bb 2> [local count: 1073741824]:
  _2 = {a_1(D), a_1(D), a_1(D), a_1(D)};
  return _2;
}


---


### compiler : `gcc`
### title : `Rather bad optimization of midpoint implementation for __int128 (and other types)`
### open_at : `2020-12-24T16:03:47Z`
### last_modified_date : `2023-09-21T10:35:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98438
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
_Tp midpoint(_Tp __a, _Tp __b) noexcept
{
    using _Up = std::make_unsigned_t<_Tp>;
    constexpr _Up __bitshift = std::numeric_limits<_Up>::digits - 1;

    _Up __diff = _Up(__b) - _Up(__a);
    _Up __sign_bit = __b < __a;

    _Up __half_diff = (__diff / 2) + (__sign_bit << __bitshift) + (__sign_bit & __diff);

    return __a + __half_diff;
}

For `_Tp` `int`, this results in somewhat bad code generation on x86, presumably due to the fact that GCC does not seem to know that `sub` generates flags :

midpoint(int, int):
  mov edx, esi
  xor ecx, ecx
  sub edx, edi
  cmp esi, edi
  setl cl
  mov eax, edx
  mov esi, ecx
  shr eax
  and edx, ecx
  sal esi, 31
  add edx, edi
  add eax, esi
  add eax, edx
  ret

Whereas LLVM has better results :

midpoint(int, int): # @midpoint(int, int)
  xor eax, eax
  sub esi, edi
  setl al
  mov ecx, esi
  and esi, eax
  shl eax, 31
  shr ecx
  add eax, edi
  add eax, ecx
  add eax, esi
  ret

This seems to however be even worse with such types as __int128, where this is the code generation from GCC :

midpoint(__int128, __int128):
  mov r10, rdx
  mov r11, rcx
  mov rax, rcx
  push r14
  sub r10, rdi
  push rbx
  mov r8, rdi
  mov r9, rsi
  sbb r11, rsi
  xor ebx, ebx
  cmp rdx, rdi
  mov ecx, 1
  sbb rax, rsi
  jl .L2
  xor ecx, ecx
.L2:
  mov rax, r10
  xor esi, esi
  mov r14, rcx
  mov rdx, r11
  shrd rax, r11, 1
  sal r14, 63
  shr rdx
  add rax, rsi
  adc rdx, r14
  and rcx, r10
  mov rsi, rcx
  mov rcx, r11
  and rcx, rbx
  add rsi, r8
  pop rbx
  pop r14
  mov rdi, rcx
  adc rdi, r9
  add rax, rsi
  adc rdx, rdi
  ret

And this is the code generation from LLVM :

midpoint(__int128, __int128): # @midpoint(__int128, __int128)
  mov rax, rdx
  sub rax, rdi
  sbb rcx, rsi
  mov r8, rax
  setl dl
  mov r9, rcx
  shr r8
  shr rcx
  shl r9, 63
  movzx edx, dl
  and eax, edx
  shl rdx, 63
  or r9, r8
  add rdx, rsi
  add r9, rdi
  adc rdx, rcx
  add rax, r9
  adc rdx, 0
  ret

With the GCC version requiring such a large amount of registers that it even has to use some callee saved registers.


---


### compiler : `gcc`
### title : `Failure to optimize out vector operations into a constant when possible`
### open_at : `2020-12-25T06:08:12Z`
### last_modified_date : `2023-09-21T10:32:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98443
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef char A __attribute__((vector_size(32)));

int foo(A w)
{
    w *= (A){0, 1};
    return w[0];
}

This can be optimized into `return 0;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `aarch64: Missed opportunity for STP for vec_duplicate`
### open_at : `2020-12-27T12:24:37Z`
### last_modified_date : `2023-06-25T13:23:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98453
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef long long v2di __attribute__((vector_size (16)));
typedef int v2si __attribute__((vector_size (8)));

void
foo (v2di *x, long long a)
{
  v2di tmp = {a, a};
  *x = tmp;
}

void
foo2 (v2si *x, int a)
{
  v2si tmp = {a, a};
  *x = tmp;
}

at -O2 on aarch64 gives:
foo:
        dup     v0.2d, x1
        str     q0, [x0]
        ret

foo2:
        dup     v0.2s, w1
        str     d0, [x0]
        ret

These could just be: stp x1, x1, [x0] and stp w1, w1, [x0]
Combine already tries and fails to match:
(set (mem:V2DI (reg:DI 97) [1 *x_4(D)+0 S16 A128])
    (vec_duplicate:V2DI (reg:DI 98)))
and
(set (mem:V2SI (reg:DI 97) [2 *x_4(D)+0 S8 A64])
    (vec_duplicate:V2SI (reg:SI 98)))

So can be fixed by some new patterns in aarch64-simd.md.
We should make sure to handle the other 32-bit and 64-bit modes as well


---


### compiler : `gcc`
### title : `Bogus -Wstringop-overread with -std=gnu++20 -O2 and std::string::insert`
### open_at : `2020-12-28T18:36:57Z`
### last_modified_date : `2022-02-17T10:37:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98465
### status : `RESOLVED`
### tags : `alias, diagnostic, missed-optimization, patch`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
Hi,

The following C++ code compiled with -std=gnu++20 -O2 -Werror=stringop-overread emits a bogus warning/error with gcc trunk, while it works without problem when using gnu++17 instead. Tested on compiler explorer on x64 Linux:

#include <string>

const char constantString[] = {42, 53};

void f(std::string& s)
{
    s.insert(0, static_cast<const char*>(constantString), 2);
}



In file included from /opt/compiler-explorer/gcc-trunk-20201228/include/c++/11.0.0/string:40,
                 from <source>:1:
In static member function 'static constexpr std::char_traits<char>::char_type* std::char_traits<char>::copy(std::char_traits<char>::char_type*, const char_type*, std::size_t)',
    inlined from 'static void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_S_copy(_CharT*, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-trunk-20201228/include/c++/11.0.0/bits/basic_string.h:351:21,
    inlined from 'static void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_S_copy(_CharT*, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-trunk-20201228/include/c++/11.0.0/bits/basic_string.h:346:7,
    inlined from 'std::__cxx11::basic_string<_CharT, _Traits, _Allocator>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_replace(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-trunk-20201228/include/c++/11.0.0/bits/basic_string.tcc:481:20,
    inlined from 'std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::replace(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-trunk-20201228/include/c++/11.0.0/bits/basic_string.h:1946:19,
    inlined from 'std::__cxx11::basic_string<_CharT, _Traits, _Alloc>& std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::insert(std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type, const _CharT*, std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]' at /opt/compiler-explorer/gcc-trunk-20201228/include/c++/11.0.0/bits/basic_string.h:1693:29,
    inlined from 'void f(std::string&)' at <source>:7:60:
/opt/compiler-explorer/gcc-trunk-20201228/include/c++/11.0.0/bits/char_traits.h:402:56: error: 'void* __builtin_memcpy(void*, const void*, long unsigned int)' reading 2 bytes from a region of size 0 [-Werror=stringop-overread]
  402 |         return static_cast<char_type*>(__builtin_memcpy(__s1, __s2, __n));
      |                                        ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
<source>: In function 'void f(std::string&)':
<source>:3:12: note: at offset 2 into source object 'constantString' of size 2
    3 | const char constantString[] = {42, 53};
      |            ^~~~~~~~~~~~~~
cc1plus: some warnings being treated as errors
Compiler returned: 1


Note that gcc 10 doesn't issue any warning even with -Wall -Wextra with gnu++20.

Cheers,
Romain


---


### compiler : `gcc`
### title : `aarch64: Unnecessary GPR -> FPR moves for conditional select`
### open_at : `2020-12-30T09:35:35Z`
### last_modified_date : `2021-08-26T14:38:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98477
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Code like
void
foo (int a, double *b)
{
  *b = a ? 10000.0 : 200.0;
}

generates:
foo:
        cmp     w0, 0
        mov     x2, 149533581377536
        movk    x2, 0x40c3, lsl 48
        mov     x0, 4641240890982006784
        fmov    d0, x2
        fmov    d1, x0
        fcsel   d0, d0, d1, ne
        str     d0, [x1]
        ret

We don't need to do the FCSEL on the FPR side if we're just storing it to memory. We can just do a GPR CSEL and avoid the FMOVs.
I've seen this pattern in the disassembly of some math library routines.
Maybe we should add a =w,w,w alternative to the CSEL patterns in the backend?


---


### compiler : `gcc`
### title : `Missed optimization opportunity for unsigned __int128 modulo`
### open_at : `2020-12-30T14:03:10Z`
### last_modified_date : `2021-01-05T10:47:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98479
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c`
### version : `9.3.0`
### severity : `normal`
### contents :
I have found that manually calculating the % operator on __int128 is significantly faster than the built-in compiler operator. I will show you how to calculate modulo 9, but the method can be used to calculate modulo any other number.

First, consider the built-in compiler operator:

uint64_t mod9_v1(unsigned __int128 n)
{
	return n % 9;
}

Now consider my manual implementation:

uint64_t mod9_v2(unsigned __int128 n)
{
	uint64_t r = 0;

	r += (uint32_t)(n);
	r += (uint32_t)(n >> 32) * (uint64_t)4;
	r += (uint32_t)(n >> 64) * (uint64_t)7;
	r += (uint32_t)(n >> 96);

	return r % 9;
}

Measuring over 100,000,000 random numbers gives the following results:

    mod9_v1 | 3.986052 secs
    mod9_v2 | 1.814339 secs

GCC 9.3.0 with -march=native -O3 was used on AMD Ryzen Threadripper 2990WX.

Note that 2^32 == 4 (mod 9), 2^64 == 7 (mod 9), etc.


---


### compiler : `gcc`
### title : `graphite messes up profile counts`
### open_at : `2021-01-02T03:12:01Z`
### last_modified_date : `2021-09-20T22:41:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98497
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
In the following code generated by gcc 10.2
```
.L2:
        movups  xmm3, XMMWORD PTR [rax]
        add     rax, 16
        addps   xmm0, xmm3
        cmp     rax, rdx
        je      .L6
        jmp     .L2

matrix_sum_column_major.cold:
.L6:
        movaps  xmm2, xmm0
# .....

```

I think `jne .L2; jmp.L6` should be more efficient as it avoids one instruction in the hot path.

c code:
```
float matrix_sum_column_major(float* x, int n) {
    n = 32767;
    float sum = 0;
    for (int i = 0; i < n; i++)
        for (int j = 0; j < n; j++)
            sum += x[j * n + i];
    return sum;
}
```

gcc -Ofast -floop-nest-optimize -o -
```
matrix_sum_column_major:
        mov     eax, 4294836212
        lea     rdx, [rdi+131056]
        pxor    xmm1, xmm1
        lea     rcx, [rdi+rax]
.L3:
        mov     rax, rdi
        pxor    xmm0, xmm0
.L2:
        movups  xmm3, XMMWORD PTR [rax]
        add     rax, 16
        addps   xmm0, xmm3
        cmp     rax, rdx
        je      .L6
        jmp     .L2
matrix_sum_column_major.cold:
.L6:
        movaps  xmm2, xmm0
        addss   xmm1, DWORD PTR [rax+8]
        lea     rdx, [rax+131068]
        add     rdi, 131068
        movhlps xmm2, xmm0
        addps   xmm2, xmm0
        movaps  xmm0, xmm2
        shufps  xmm0, xmm2, 85
        addps   xmm0, xmm2
        movss   xmm2, DWORD PTR [rax+4]
        addss   xmm2, DWORD PTR [rax]
        addss   xmm1, xmm2
        addss   xmm1, xmm0
        cmp     rdx, rcx
        jne     .L3
        movaps  xmm0, xmm1
        ret
```


Link to godbolt: https://gcc.godbolt.org/z/ac7YY1


---


### compiler : `gcc`
### title : `potential optimization for base<->derived pointer casts`
### open_at : `2021-01-02T22:56:36Z`
### last_modified_date : `2021-01-07T13:13:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98501
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.1`
### severity : `normal`
### contents :
Consider this code:

struct base1 { int a; };
struct base2 { int b; };
struct derived : base1, base2 {};

derived& to_derived_bad(base2* b)
{
    return *static_cast<derived*>(b);
}

derived& to_derived_good(base2* b)
{
    return static_cast<derived&>(*b);
}

I believe both of these functions are functionally equivalent and should generate the same code. Both functions cast pointer from base to derived if it is not nullptr and both cause undefined behavior if it is nullptr.

GCC optimizes to_derived_good() to a single subtraction, but it inserts nullptr-check into to_derived_bad():

to_derived_good(base2*):
    lea rax, [rdi-4]
    ret
to_derived_bad(base2*):
    lea rax, [rdi-4]
    test rdi, rdi
    mov edx, 0
    cmove rax, rdx
    ret

Could GCC omit the nullptr-check in to_derived_bad?


---


### compiler : `gcc`
### title : `[10 Regression] Double-counting of reduction cost`
### open_at : `2021-01-05T10:30:42Z`
### last_modified_date : `2021-03-03T10:37:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98526
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
The vect_body cost of most reductions is added twice,
once in vect_model_reduction_cost and once in whichever
vectorizable_* routine handles the reduction statement.
E.g. for:

long
f (long *x, int n)
{
  long res = 0;
  for (int i = 0; i < n; ++i)
    res += x[i];
  return res;
}

we have:

0x3c40a60 _4 + res_14 1 times scalar_to_vec costs 1 in prologue
0x3c40a60 _4 + res_14 1 times vector_stmt costs 1 in body           <----
0x3c40a60 _4 + res_14 1 times vector_stmt costs 1 in epilogue
0x3c40a60 _4 + res_14 1 times vec_to_scalar costs 2 in epilogue
0x3c40a60 *_3 1 times vector_load costs 1 in body
0x3c40a60 _4 + res_14 1 times vector_stmt costs 1 in body           <----

I don't yet have a case where this tips the balance though.


---


### compiler : `gcc`
### title : `Use load/store pairs for 2-element vector in memory permutes`
### open_at : `2021-01-05T14:08:29Z`
### last_modified_date : `2023-05-12T05:34:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98532
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
I've seen these patterns while looking at some disassemblies but I believe it can be reproduced in C with:
typedef long v2di __attribute__((vector_size (16)));

void
foo (v2di *a, v2di *b)
{
  v2di tmp = {(*a)[1], (*a)[0]};
  *b = tmp;
}

This, for aarch64 -O2 generates:
foo:
        ldr     d0, [x0, 8]
        ld1     {v0.d}[1], [x0]
        str     q0, [x1]
        ret

clang does:
foo:                                    // @foo
        ldr     q0, [x0]
        ext     v0.16b, v0.16b, v0.16b, #8
        str     q0, [x1]
        ret

I suspect we can do better in these cases with:
ldp x2, x3, [x0]
stp x3, x2, [x1]
or something similar.
In the combine phase we already try and fail to match:
Failed to match this instruction:
(set (reg:V2DI 97 [ tmp ])
    (vec_concat:V2DI (mem/j:DI (plus:DI (reg/v/f:DI 95 [ a ])
                (const_int 8 [0x8])) [1 BIT_FIELD_REF <*a_4(D), 64, 64>+0 S8 A64])
        (mem/j:DI (reg/v/f:DI 95 [ a ]) [1 BIT_FIELD_REF <*a_4(D), 64, 0>+0 S8 A128])))


so maybe we can solve this purely in the backend?


---


### compiler : `gcc`
### title : `Redundant loads in vectorised loop`
### open_at : `2021-01-05T16:17:03Z`
### last_modified_date : `2021-01-07T10:44:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98542
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
For the testcase below, based loosely on one from 450.soplex,
the vectoriser loads the v and i fields twice:

struct s { double v; long i; };
double
f (struct s *x, double *y, int n)
{
  double res = 0;
  for (int i = 0; i < n; ++i)
    res += x[i].v * y[x[i].i];
  return res;
}

SVE loop:

        add     x5, x0, 8
        ...
.L4:
        ld2d    {z2.d - z3.d}, p0/z, [x5, x4, lsl 3]
        ld2d    {z4.d - z5.d}, p0/z, [x0, x4, lsl 3]
        ld1d    z2.d, p0/z, [x1, z2.d, lsl 3]
        incd    x3
        fmla    z0.d, p0/m, z4.d, z2.d
        incw    x4
        whilelo p0.d, w3, w2
        b.any   .L4

where z5 from the second ld2d is the same as z2
from the first ld2d.

In the soplex example, "i" is instead a 32-bit value,
but I guess we need to fix this case first.


---


### compiler : `gcc`
### title : `Make more use of __builtin_undefined for assuring that variables do not change`
### open_at : `2021-01-05T22:09:49Z`
### last_modified_date : `2021-12-23T09:51:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98552
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Consider

void foo (int *);

void bar (int n)
{
  int i;
  for (i=0; i<n; i++)
    foo(&i);
}

void baz(int n)
{
  int i, j;
  for (i=0; i<n; i++)
    {
      j = i;
      foo (&i);
      if (j /= i)
	__builtin_unreachable();
    }
}

Assembly for bar and baz are identical, the loop is

.L9:
        leaq    12(%rsp), %rdi
        call    foo
        movl    12(%rsp), %eax
        addl    $1, %eax
        movl    %eax, 12(%rsp)
        cmpl    %ebx, %eax
        jl      .L9

In function bar, things are clear - the value of i has to be
reloaded from the stack, foo might have changed it.

However, this is not possible in baz.  j cannot be changed, and the
__builtin_unreachable ensures that i has the same value before and
after the call to foo. It need not be reloaded from the stack.

(The reason why I'm submitting this is another way to approach PR 31593 -
the Fortran front end could annotate code like this to inform
the middle end that DO loops variables are, in fact, invariant).


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] vectorization fails while it worked on gcc 9 and earlier since since r10-2271-gd81ab49d0586fca0`
### open_at : `2021-01-06T14:53:21Z`
### last_modified_date : `2023-07-07T10:38:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98563
### status : `NEW`
### tags : `missed-optimization, openmp`
### component : `tree-optimization`
### version : `10.1.0`
### severity : `normal`
### contents :
I have found what seems to be a regression.

The following code is not compiled to 256-bit AVX when compiled with -fopenmp-simd, while it is fully vectorized without!

Here are the resulting code with different options, with gcc 10.1:
-O3 -fopenmp-simd  => xmm
-O3                => ymm
-O3 -fopenmp-simd -fno-signed-zeros  => ymm

gcc 9 and earlier always vectorize to full-width (ymm)

#include <complex>
typedef std::complex<double> cplx;

void test(cplx* __restrict__ a, const cplx* b, double c, int N)
{
    #pragma omp simd
    for (int i=0; i<8*N; i++) {
        a[i] = c*(a[i]-b[i]);
    }
}

See the result on godbolt: https://godbolt.org/z/9ThqKE

Also, I discover that no avx512 code is generated for this loop. Is this intended? Is there an option to enable avx512 vectorization?


---


### compiler : `gcc`
### title : `Failure to optimize using ZF flag from blsi`
### open_at : `2021-01-06T16:32:53Z`
### last_modified_date : `2023-09-21T10:30:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98567
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
bool f(__UINT64_TYPE__ x)
{
    return (-x & x) == 0;
}

With -O3 -mbmi, LLVM compiles this to:

f:
  blsi rax, rdi
  sete al
  ret

GCC compiles this to:

f:
  blsi rdi, rdi
  test rdi, rdi
  sete al
  ret

The usage of `test` can be entirely removed, as `blsi` already sets ZF accordingly.


---


### compiler : `gcc`
### title : `reassociation sometimes gets in the way of recognizing uabd`
### open_at : `2021-01-07T12:52:05Z`
### last_modified_date : `2023-07-21T23:16:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98581
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef signed int *__restrict__ pSINT;
typedef unsigned int *__restrict__ pUINT;

#define MIN(a, b) ((a) < (b) ? (a) : (b))
#define MAX(a, b) ((a) > (b) ? (a) : (b))

void saba_s (pSINT a, pSINT b, pSINT c)
{
  int i;
  for (i = 0; i < 4; i++)
    c[i] += (MAX (a[i], b[i]) - MIN (a[i], b[i]));
}

void saba_u (pUINT a, pUINT b, pUINT c)
{
  int i;
  for (i = 0; i < 4; i++)
    c[i] += (MAX (a[i], b[i]) - MIN (a[i], b[i]));
}

On aarch64 at -O3 generates:
saba_s:
        ldr     q0, [x0]
        ldr     q1, [x1]
        ldr     q2, [x2]
        sabd    v0.4s, v0.4s, v1.4s
        add     v0.4s, v0.4s, v2.4s
        str     q0, [x2]
        ret

saba_u:
        ldr     q1, [x0]
        ldr     q2, [x1]
        ldr     q3, [x2]
        umax    v0.4s, v1.4s, v2.4s
        umin    v1.4s, v1.4s, v2.4s
        add     v0.4s, v0.4s, v3.4s
        sub     v0.4s, v0.4s, v1.4s
        str     q0, [x2]
        ret

I would expect the (MAX (a[i], b[i]) - MIN (a[i], b[i])) part to match a uabd instruction for the unsigned case, but it looks like the add and sub operations are swapped which prevents the RTL pattern matching the operation.
This comes out this way out of GIMPLE. At expand the signed version is:
  vect__4.6_40 = MEM <vector(4) int> [(int *)c_16(D)];
  vect__6.9_37 = MEM <vector(4) int> [(int *)b_17(D)];
  vect__8.12_34 = MEM <vector(4) int> [(int *)a_18(D)];
  vect__9.13_33 = MAX_EXPR <vect__8.12_34, vect__6.9_37>;
  vect__10.14_32 = MIN_EXPR <vect__8.12_34, vect__6.9_37>;
  vect__11.15_31 = vect__9.13_33 - vect__10.14_32;
  vect__12.16_30 = vect__11.15_31 + vect__4.6_40;
  MEM <vector(4) int> [(int *)c_16(D)] = vect__12.16_30;
  return;


the unsigned is:
  vect__4.25_38 = MEM <vector(4) unsigned int> [(unsigned int *)c_16(D)];
  vect__6.28_35 = MEM <vector(4) unsigned int> [(unsigned int *)b_17(D)];
  vect__8.31_32 = MEM <vector(4) unsigned int> [(unsigned int *)a_18(D)];
  vect__9.32_31 = MAX_EXPR <vect__8.31_32, vect__6.28_35>;
  vect__10.33_30 = MIN_EXPR <vect__8.31_32, vect__6.28_35>;
  vect__13.34_29 = vect__9.32_31 + vect__4.25_38;
  vect__12.35_28 = vect__13.34_29 - vect__10.33_30;
  MEM <vector(4) unsigned int> [(unsigned int *)c_16(D)] = vect__12.35_28;
  return;


---


### compiler : `gcc`
### title : `registers not reused on RISC-V`
### open_at : `2021-01-08T08:17:33Z`
### last_modified_date : `2023-09-12T18:37:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98596
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `enhancement`
### contents :
Hi,

in this minimal example code


	#define CLINT_MSIP (*((volatile unsigned *)0x02000000U))
	void trigger_soft_irq(void)
	{
		CLINT_MSIP = 1;
		while (CLINT_MSIP == 1) { }
	}

GCC generates the following assembly when compiling with riscv-none-elf-gcc -S -Os poc.c:

	trigger_soft_irq:
		li	a5,33554432
		li	a4,1
		sw	a4,0(a5)
		li	a3,33554432
		li	a5,1
	.L2:
		lw	a4,0(a3)
		beq	a4,a5,.L2
		ret

I think that GCC should instead reuse the values 1 and 0x02000000 previously stored in register a5 and a4, rather than storing the exact same values again in registers a3 and a5. So it should look instead like this:

	trigger_soft_irq:
		li	a5,33554432
		li	a4,1
		sw	a4,0(a5)
	.L2:
		lw	a3,0(a5)
		beq	a3,a4,.L2
		ret


---


### compiler : `gcc`
### title : `Missed opportunity to optimize dependent loads in loops`
### open_at : `2021-01-08T08:24:59Z`
### last_modified_date : `2021-12-23T09:49:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98598
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
As we know, dependent loads are not friendly to cache. Especially when in nested loops, dependent loads such as pa->pb->pc->val may be repeated many times. For example:

typedef struct C { int val; } C;
typedef struct B { C *pc; } B;
typedef struct A { B *pb; } A;

int foo (int n, int m, A *pa) {
  int sum;

  for (int i = 0; i < n; i++) {
    for (int j = 0; j < m; j++) {
      sum += pa[j].pb->pc->val;  // each value is repeatedly loaded "n" times
      // ...
    }
    // ...
  }

  return sum;
}

Such access pattern can be found in real applications and benchmarks, and this can be critical to performance.

Can we cache the loaded value and avoid repeated dependent loads? E.g. transform above case into following (suppose there is no alias issue or other clobber, and "n" is big enough):

int foo (int n, int m, A *pa) {
  int *cache = (int *) malloc(m * sizeof(int));
  for (int j = 0; j < m; j++) {
    cache[j] = pa[j].pb->pc->val;
  }
 
  int sum;

  for (int i = 0; i < n; i++) {
    for (int j = 0; j < m; j++) {
      sum += cache[j];   // pa[j].pb->pc->val;
      // ...
    }
    // ...
  }

  free(cache);
  return sum;
}

This should improve performance a lot.


---


### compiler : `gcc`
### title : `Failure to optimise vector “x > -100 ? x : -100” to MAX_EXPR`
### open_at : `2021-01-08T13:22:29Z`
### last_modified_date : `2021-06-29T06:43:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98602
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
We don't fold:

  typedef int v4si __attribute__ ((vector_size(16)));
  v4si f (v4si x) { return x > -100 ? x : -100; }

to a MAX_EXPR, even though we do if -100 is replaced with a
nonnegative constant or a variable.  This may be related to
the remaining part of PR95906.


---


### compiler : `gcc`
### title : `Failure to optimize out convertion from float to vector type`
### open_at : `2021-01-13T00:57:05Z`
### last_modified_date : `2023-09-21T10:30:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98647
### status : `UNCONFIRMED`
### tags : `ABI, missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
float f(float val)
{
    return _mm_cvtss_f32(_mm_and_ps(_mm_set_ss(val), _mm_castsi128_ps(_mm_set1_epi32(0x7fffffff))));
}

This can be optimized to avoid the conversions in the emitted assembly code. This optimization is done by LLVM, but not by GCC.

LLVM code generation :

.LCPI1_0:
  .long 0x7fffffff # float NaN
  .long 0x7fffffff # float NaN
  .long 0x7fffffff # float NaN
  .long 0x7fffffff # float NaN
f(float): # @f(float)
  andps xmm0, xmmword ptr [rip + .LCPI1_0]
  ret

GCC code generation :

f(float):
  pxor xmm1, xmm1
  movss xmm1, xmm0
  movaps xmm0, XMMWORD PTR .LC1[rip]
  andps xmm0, xmm1
  ret

.LC1:
  .long 2147483647
  .long 2147483647
  .long 2147483647
  .long 2147483647


---


### compiler : `gcc`
### title : `Failure to optimize out no-op vector operation using andnot`
### open_at : `2021-01-13T01:02:48Z`
### last_modified_date : `2023-09-21T10:29:39Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98648
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
__m128 f(__m128 val) 
{
    return _mm_andnot_ps(_mm_set_ps1(0.0f), val);
}

This can be optimized to `return val;`. This optimization is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Trivial jump table not eliminated`
### open_at : `2021-01-13T01:42:45Z`
### last_modified_date : `2021-10-02T21:52:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98649
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
Trivial jump tables where all entries are the same are sometimes not eliminated. E.g. the following example

```
struct Base { virtual void run( float f ) = 0; };
struct T0: Base { void run( float f ); };
struct T1: Base { void run( float f ); };
struct T2: Base { void run( float f ); };
struct T3: Base { void run( float f ); };
struct T4: Base { void run( float f ); };

template<int I> struct mp_int {};

struct variant
{
    unsigned index_;

    union
    {
        T0 t0_;
        T1 t1_;
        T2 t2_;
        T3 t3_;
        T4 t4_;
    };

    T0& get( mp_int<0> ) { return t0_; }
    T1& get( mp_int<1> ) { return t1_; }
    T2& get( mp_int<2> ) { return t2_; }
    T3& get( mp_int<3> ) { return t3_; }
    T4& get( mp_int<4> ) { return t4_; }
};

template<int I> decltype(auto) get( variant& v )
{
    return v.get( mp_int<I>() );
}

void f1( variant& v, float f )
{
    switch( v.index_ )
    {
        case 0: get<0>(v).run( f ); break;
        case 1: get<1>(v).run( f ); break;
        case 2: get<2>(v).run( f ); break;
        case 3: get<3>(v).run( f ); break;
        case 4: get<4>(v).run( f ); break;
        default: __builtin_unreachable();
    }
}

```

(https://godbolt.org/z/MxzGh8)

results in

```
f1(variant&, float):
        mov     eax, DWORD PTR [rdi]
        lea     r8, [rdi+8]
        jmp     [QWORD PTR .L4[0+rax*8]]
.L4:
        .quad   .L3
        .quad   .L3
        .quad   .L3
        .quad   .L3
        .quad   .L3
.L3:
        mov     rax, QWORD PTR [rdi+8]
        mov     rdi, r8
        mov     rax, QWORD PTR [rax]
        jmp     rax
```

This case may seem contrived, but it's not that rare in practice, because code using std::variant or equivalent (such as Boost.Variant2, from which the example has been reduced) is becoming more and more common nowadays.


---


### compiler : `gcc`
### title : `pass fre4 inhibit pass dom3 to create much more optimized code`
### open_at : `2021-01-14T09:32:24Z`
### last_modified_date : `2023-03-21T23:59:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98673
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 49962
bug test file

a, compiler option:

cc1 -mabi=lp64d -march=rv64gc -O2 -S

b, hot code in function t_run_test:

        j       .L30
.L39:
        mv      a4,a3
.L30:
        ld      a2,8(a5)
        addi    a3,a4,1
        slli    t3,a4,3
        ble     a2,a1,.L28
        ld      t5,0(a5)
        bge     a1,t5,.L50
.L28:
        addi    a5,a5,8
        bne     a3,a0,.L39    : hot code loop to .L39

better code in version 8.4 with same compiler option:
=====================================================

.L30:
        ld      t1,8(a4)
        slli    a7,a5,3
        ble     t1,a3,.L28
        ld      t4,0(a4)
        bge     a3,t4,.L50
.L28:
        addi    a5,a5,1
        addi    a4,a4,8
        bne     a5,t3,.L30    : hot code loop to .L30

v10.2.0 gcc has more one instruction than v8.4.0.

analize gcc pass of source code in v10.2.0:
===========================================

before pass fr4:
----------------

<bb 8> [local count: 82176881]:
  engLoad.11_20 = engLoad;
  loadValue.13_26 = loadValue;
  _410 = (unsigned long) numXEntries.17_218;
  _409 = _410 + 18446744073709551615;
  _408 = (long int) _409;

... ...

<bb 12> [local count: 986782143]:
  i1_174 = i1_6 + 1;
  if (i1_174 != _408)
    goto <bb 9>; [94.50%]
  else
    goto <bb 13>; [5.50%]

  <bb 13> [local count: 54273018]:
  # i1_420 = PHI <i1_174(12)>
  _433 = (long unsigned int) i1_420;
  _434 = _433 + 1;
  _435 = _434 * 8;
  _436 = i1_420 + 1;
  _440 = _435 - 8;
  _442 = engLoad.11_20 + _440;
  goto <bb 15>; [100.00%]

after pass fr4:
---------------

<bb 8> [local count: 82176881]:
engLoad.11_20 = engLoad;
loadValue.13_26 = loadValue;
_410 = (unsigned long) numXEntries.17_218;
_409 = _410 + 18446744073709551615;

... ...

<bb 12> [local count: 986782143]:
  i1_174 = i1_6 + 1;
  if (i1_174 != _213)
    goto <bb 9>; [94.50%]
  else
    goto <bb 13>; [5.50%]

<bb 13> [local count: 54273018]:
_433 = (long unsigned int) i1_174;
_434 = _433 + 1;
_435 = _434 * 8;
_436 = i1_174 + 1;
_440 = _435 - 8;
_442 = engLoad.11_20 + _440;
goto <bb 15>; [100.00%]

pass fr4 remove 'Removing dead stmt _408 = (long int) _409;',
pass dom3 can't optimize this <bb 13> about '_433 = (long unsigned int) i1_174;'

if <bb 13> use i1_174 node same as <bb 12>, so that conflict will be happened in pass expand on processing coalesced ssa/phi nodes, and then will split edge.


need help ....:)


---


### compiler : `gcc`
### title : `[10 Regression] vectorizer failed for compilation time alias`
### open_at : `2021-01-14T10:19:49Z`
### last_modified_date : `2023-07-07T09:22:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98674
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Cat test.c, refer to https://godbolt.org/z/fx9cMq
---
void swap(short *p, int cnt) {
    cnt = 1000;
    while (cnt-- > 0) {
        *p = ((*p << 8) & 0xFF00) |
             ((*p >> 8) & 0x00FF);
        ++p;
    }
}
----


test.c:3:16: missed: couldn't vectorize loop
test.c:4:8: missed: not vectorized: compilation time alias: load_dst_23 = MEM[(short int *)p_21];
*p_21 = _8;

but they have the same address, and of course they alias.

---
  <bb 3> [local count: 1063004409]:
  # p_21 = PHI <p_16(5), p_12(D)(2)>
  # cnt_24 = PHI <cnt_14(5), 999(2)>
  # ivtmp_26 = PHI <ivtmp_2(5), 1000(2)>
  load_dst_23 = MEM[(short int *)p_21];
  bswapdst_11 = load_dst_23 r>> 8;
  _8 = (short int) bswapdst_11;
  *p_21 = _8;
  p_16 = p_21 + 2;
  cnt_14 = cnt_24 + -1;
  ivtmp_2 = ivtmp_26 - 1;
  if (ivtmp_2 != 0)
----

So does vectorizer assume there won't be 2 DRs with same reference?

Successfully vectorized with below hack
---
modified   gcc/tree-vect-data-refs.c
@@ -3302,6 +3302,10 @@ vect_compile_time_alias (dr_vec_info *a, dr_vec_info *b,
   const_length_a += access_size_a;
   const_length_b += access_size_b;
 
+  /* It's ok for the same reference.  */
+  if (known_eq (const_length_a, const_length_b))
+    return 0;
+
   if (ranges_known_overlap_p (offset_a, const_length_a,
 			      offset_b, const_length_b))
     return 1;

---


---


### compiler : `gcc`
### title : `shl not vectorized for v16qi and v8hi with MVE`
### open_at : `2021-01-15T10:44:03Z`
### last_modified_date : `2021-01-22T16:17:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98697
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
As described in https://gcc.gnu.org/pipermail/gcc-patches/2020-December/562235.html we currently fail to vectorize

dest[i] = a[i] << b[i]
when the element types are v16qi or v8hi

I've committed the support for shl auto-vectorization for MVE in g:7432f255b70811dafaf325d94036ac580891de69 (r11-6707)

I haven't yet found why the v16qi and v8hi tests are not vectorized.
With dest[i] = a[i] << b[i] and:
  {
    int i;
    unsigned int i.24_1;
    unsigned int _2;
    int16_t * _3;
    short int _4;
    int _5;
    int16_t * _6;
    short int _7;
    int _8;
    int _9;
    int16_t * _10;
    short int _11;
    unsigned int ivtmp_42;
    unsigned int ivtmp_43;

    <bb 2> [local count: 119292720]:

    <bb 3> [local count: 954449105]:
    i.24_1 = (unsigned int) i_23;
    _2 = i.24_1 * 2;
    _3 = a_15(D) + _2;
    _4 = *_3;
    _5 = (int) _4;
    _6 = b_16(D) + _2;
    _7 = *_6;
    _8 = (int) _7;
    _9 = _5 << _8;
    _10 = dest_17(D) + _2;
    _11 = (short int) _9;
    *_10 = _11;
    i_19 = i_23 + 1;
    ivtmp_42 = ivtmp_43 - 1;
    if (ivtmp_42 != 0)
      goto <bb 5>; [87.50%]
    else
      goto <bb 4>; [12.50%]

    <bb 5> [local count: 835156386]:
    goto <bb 3>; [100.00%]

    <bb 4> [local count: 119292720]:
    return;

  }
the vectorizer says:
mve-vshl.c:37:96: note:   ==> examining statement: _5 = (int) _4;
mve-vshl.c:37:96: note:   vect_is_simple_use: operand *_3, type of def: internal
mve-vshl.c:37:96: note:   vect_is_simple_use: vectype vector(8) short int
mve-vshl.c:37:96: missed:   conversion not supported by target.
mve-vshl.c:37:96: note:   vect_is_simple_use: operand *_3, type of def: internal
mve-vshl.c:37:96: note:   vect_is_simple_use: vectype vector(8) short int
mve-vshl.c:37:96: note:   vect_is_simple_use: operand *_3, type of def: internal
mve-vshl.c:37:96: note:   vect_is_simple_use: vectype vector(8) short int
mve-vshl.c:37:117: missed:   not vectorized: relevant stmt not supported: _5 = (int) _4;
mve-vshl.c:37:96: missed:  bad operation or unsupported loop bound.
mve-vshl.c:37:96: note:  ***** Analysis failed with vector mode V8HI


---


### compiler : `gcc`
### title : `atomic load to FPU registers`
### open_at : `2021-01-15T14:47:29Z`
### last_modified_date : `2021-12-15T01:13:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98698
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
#include <atomic>
std::atomic<double> a;
double f(){ return a.load(std::memory_order_relaxed); }

is compiled by g++ to

	movq	a(%rip), %rax
	movq	%rax, %xmm0
	ret

As far as I understand, a direct movsd to xmm0 would still be atomic, and that's indeed what llvm outputs.


---


### compiler : `gcc`
### title : `Failure to optimize out non-zero check after multiplication overflow check`
### open_at : `2021-01-16T03:22:58Z`
### last_modified_date : `2023-09-21T10:28:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98703
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f1(unsigned x, unsigned y, unsigned *res)
{
    return __builtin_mul_overflow(x, y, res) && x;
}

This can be optimized to `return __builtin_mul_overflow(x, y, res);`. This transformation is done by LLVM, but not by GCC.

PS: I originally found this while looking at the code generation for this code (from https://gcc.gnu.org/PR95852) :

bool f2(unsigned x, unsigned y, unsigned *res)
{
    *res = x * y;
    return x && ((*res / x) != y);
}

f2 is equivalent to `return __builtin_mul_overflow(x, y, res);`, but the code emitted is more like f1.


---


### compiler : `gcc`
### title : `gcc optimizes bitwise operations, but doesn't optimize logical ones`
### open_at : `2021-01-17T01:31:15Z`
### last_modified_date : `2022-08-10T11:46:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98709
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `enhancement`
### contents :
GCC 10.2 produces very good code for this function noticing that both sides of conjuntion are the same:

unsigned foo_bitwise(unsigned a, unsigned b)
{
    return (~a ^ b) & ~(a ^ b);
}

foo_bitwise(unsigned int, unsigned int):
        xor     edi, esi
        mov     eax, edi
        not     eax
        ret

But when I write a similar function with logical operations it doesn't notice that:

bool foo_logical(bool a, bool b)
{
    return (!a ^ b) & !(a ^ b);
}

foo_logical(bool, bool):
        mov     eax, esi
        xor     eax, edi
        xor     eax, 1
        cmp     dil, sil
        sete    dl
        and     eax, edx
        ret

I believe that in a similar manner it can be optimized to something like this:

foo_logical(bool, bool):
        xor     edi, esi
        mov     eax, edi
        xor     eax, 1
        ret


---


### compiler : `gcc`
### title : `missing optimization (x | c) & ~(y | c) -> x & ~(y | c)`
### open_at : `2021-01-17T01:59:17Z`
### last_modified_date : `2023-09-22T11:31:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98710
### status : `RESOLVED`
### tags : `missed-optimization, patch`
### component : `middle-end`
### version : `10.2.1`
### severity : `enhancement`
### contents :
On this function clang generates slightly shorter code

unsigned foo(unsigned x, unsigned y, unsigned c)
{
    return (x | c) & ~(y | c);
}

because it notices that the expression can be simplified to x & ~(y | c). It would be great if GCC can do the same.

https://godbolt.org/z/3ob6eb


---


### compiler : `gcc`
### title : `Failure to generate branch version of abs if user requested it`
### open_at : `2021-01-17T20:08:53Z`
### last_modified_date : `2021-09-26T09:43:01Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98713
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
int branch_abs(int v) {
    return __builtin_expect(v > 0, 1) ? v : -v;
}

GCC -O2 now:

branch_abs:
  mov eax, edi
  neg eax
  cmovs eax, edi
  ret


Expected:
branch_abs:
  mov eax, edi
  test edi, edi
  js .LBB0_1
  ret
.LBB0_1:
  neg eax
  ret


Same for min/max.


---


### compiler : `gcc`
### title : `Atomic operation on x86 no optimized to use flags`
### open_at : `2021-01-18T19:59:19Z`
### last_modified_date : `2022-01-14T11:07:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98737
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Consider the following code:

long a;

_Bool f(long b)
{
  return __atomic_sub_fetch(&a, b, __ATOMIC_RELEASE) == 0;
}

_Bool g(long b)
{
  return (a -= b) == 0;
}


When compiling for x86-64 with the current HEAD as of 20210118 the resulting code is:

0000000000000000 <f>:
   0:	48 f7 df             	neg    %rdi
   3:	48 89 f8             	mov    %rdi,%rax
   6:	f0 48 0f c1 05 00 00 	lock xadd %rax,0x0(%rip)        # f <f+0xf>
   d:	00 00 
   f:	48 01 f8             	add    %rdi,%rax
  12:	0f 94 c0             	sete   %al
  15:	c3                   	retq   
  16:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
  1d:	00 00 00 

0000000000000020 <g>:
  20:	48 29 3d 00 00 00 00 	sub    %rdi,0x0(%rip)        # 27 <g+0x7>
  27:	0f 94 c0             	sete   %al
  2a:	c3                   	retq   

The code for f is far too complicated.  All that needs to be different from the code in g is that the lock prefix must be used for sub.

Probably all __atomic_* builtins have problems with using flags when possible.

This is not an esoteric problem.  I was specifically looking at optimizing the std::latch implementation for C++20 and this is what would be needed.  Without a fix a special version would be needed or the current, much worse code is used.


---


### compiler : `gcc`
### title : `Widening patterns causing missed vectorization`
### open_at : `2021-01-20T15:26:57Z`
### last_modified_date : `2021-02-11T15:07:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98772
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
Disabling widening patterns (widening_mult, widening_plus, widening_minus)
allows some testcases to be vectorized better. Currently mixed scalar and vector code is produced, due to the patterns being recognized and substituted but vectorization failing 'no optab'. When they are recognized 16bytes -> 16 shorts, using a pair 8byte->8short instructions is presumed, the datatypes chosen in 'vectorizable_conversion' are 'vectype_in' 8 bytes, 'vectype out' 8 shorts. This causes the scalar code to be emitted where these patterns were recognized.


For the following testcases with: gcc -O3

#include <stdint.h>
extern void wdiff( int16_t d[16], uint8_t *restrict pix1, uint8_t *restrict pix2)
{
   for( int y = 0; y < 4; y++ )
  {    
    for( int x = 0; x < 4; x++ )
      d[x + y*4] = pix1[x] * pix2[x];
    pix1 += 16;  
    pix2 += 16;
 }

The following output is seen, processing 8 elements per cycle using scalar instructions and 8 elements per cycle using vector instructions.

wdiff:
.LFB0:
        .cfi_startproc
        ldrb    w3, [x1, 32]
        ldrb    w6, [x2, 32]
        ldrb    w8, [x1, 33]
        ldrb    w5, [x2, 33]
        ldrb    w4, [x1, 34]
        mul     w3, w3, w6
        ldrb    w7, [x1, 35]
        fmov    s0, w3
        ldrb    w3, [x2, 34]
        mul     w8, w8, w5
        ldrb    w9, [x2, 35]
        ldrb    w6, [x2, 48]
        ldrb    w5, [x1, 49]
        ins     v0.h[1], w8
        mul     w3, w4, w3
        mul     w7, w7, w9
        ldrb    w4, [x1, 48]
        ldrb    w8, [x2, 49]
        ldrb    w9, [x2, 50]
        ins     v0.h[2], w3
        ldrb    w3, [x1, 51]
        mul     w6, w6, w4
        ldrb    w4, [x1, 50]
        mul     w5, w5, w8
        ldrb    w8, [x2, 51]
        ldr     d2, [x1]
        ins     v0.h[3], w7
        ldr     d1, [x2]
        mul     w4, w4, w9
        ldr     d4, [x1, 16]
        ldr     d3, [x2, 16]
        mul     w1, w3, w8
        ins     v0.h[4], w6
        zip1    v2.2s, v2.2s, v4.2s
        zip1    v1.2s, v1.2s, v3.2s
        ins     v0.h[5], w5
        umull   v1.8h, v1.8b, v2.8b
        ins     v0.h[6], w4
        ins     v0.h[7], w1
        stp     q1, q0, [x0]
        ret


if the widening multiply instruction is disabled e.g.:

-  { vect_recog_widen_mult_pattern, "widen_mult" },
+  //{ vect_recog_widen_mult_pattern, "widen_mult" },
in tree-vect-patterns.c

then the same testcase is able to process 16 elements per cycle using vector instructions. 

wdiff:
.LFB0:
        .cfi_startproc
        ldr     b3, [x1, 33]
        ldr     b2, [x2, 33]
        ldr     b1, [x1, 32]
        ldr     b0, [x2, 32]
        ldr     b5, [x1, 34]
        ins     v1.b[1], v3.b[0]
        ldr     b4, [x2, 34]
        ins     v0.b[1], v2.b[0]
        ldr     b3, [x1, 35]
        ldr     b2, [x2, 35]
        ldr     b19, [x1, 48]
        ins     v1.b[2], v5.b[0]
        ldr     b17, [x2, 48]
        ins     v0.b[2], v4.b[0]
        ldr     b18, [x1, 49]
        ldr     b16, [x2, 49]
        ldr     b7, [x1, 50]
        ins     v1.b[3], v3.b[0]
        ldr     b6, [x2, 50]
        ins     v0.b[3], v2.b[0]
        ldr     b5, [x1, 51]
        ldr     b4, [x2, 51]
        ldr     d3, [x1]
        ins     v1.b[4], v19.b[0]
        ldr     d2, [x2]
        ins     v0.b[4], v17.b[0]
        ldr     d19, [x1, 16]
        ldr     d17, [x2, 16]
        ins     v1.b[5], v18.b[0]
        zip1    v3.2s, v3.2s, v19.2s
        ins     v0.b[5], v16.b[0]
        zip1    v2.2s, v2.2s, v17.2s
        ins     v1.b[6], v7.b[0]
        umull   v2.8h, v2.8b, v3.8b
        ins     v0.b[6], v6.b[0]
        ins     v1.b[7], v5.b[0]
        ins     v0.b[7], v4.b[0]
        umull   v0.8h, v0.8b, v1.8b
        stp     q2, q0, [x0]
        ret
        .cfi_endproc

note the use of 2 umull instructions.



The same can be seen for widening plus and widening minus.

It appears to be due to the way than the vectype_in is chosen in vectorizable conversion, 

in vectorizable conversion, tree-vect-stmts.c:4626

vect_is_simple_use fills the &vectype1_in parameter, which fills the vectype_in parameter.

 

during slp vectorization vect_is_simple_use uses the slp tree vectype:

tree-vect-stmts.c:
11369 if (slp_node)
11370 {
11371 slp_tree child = SLP_TREE_CHILDREN (slp_node)[operand]; |
11372 *slp_def = child;
11373 *vectype = SLP_TREE_VECTYPE (child);
11374 if (SLP_TREE_DEF_TYPE (child) == vect_internal_def)
11375 { | |11376 *op = gimple_get_lhs (SLP_TREE_REPRESENTATIVE (child)->stmt); | |11377 return vect_is_simple_use (*op, vinfo, dt, def_stmt_info_out); | |11378 }

 

for 'vect' vectorization, the def_stmt_info is used.


---


### compiler : `gcc`
### title : `gcc -O3 does not vectorize some operations`
### open_at : `2021-01-20T22:13:50Z`
### last_modified_date : `2022-12-26T06:39:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98774
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50014
nbody-update-velocity.cpp

In the following sample GCC (-O3 -ffast-math) fails to vectorize operations. The results is that GCC 10.2 does 8 mulsd, while clang 11.0 does 4 mulpd.

struct vec3 { double x, y, z; };

void update_velocities(vec3* __restrict velocity,
                       double const* __restrict mass,
                       vec3 const* __restrict dpos,
                       double const* __restrict mag)
{
    velocity[0] -= dpos[0] * (mass[1] * mag[0]);
    velocity[1] += dpos[0] * (mass[0] * mag[0]);
}

See an attachment for the complete sample.


---


### compiler : `gcc`
### title : `missing optimization opportunity on nbody`
### open_at : `2021-01-20T22:41:01Z`
### last_modified_date : `2021-09-13T00:25:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98775
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Created attachment 50015
nbody.cpp

On the attached sample (208 LOC), clang 11.0 generates the code that is almost twice as fast as the one generated by GCC 10.2 (-O3 -ffast-math -flto).

$ ./nbody 50000000
4.0s for clang vs 7.5s for GCC.

A quick look at the generated code shows that clang aggressively unrolled all inner loops. If I unroll all inner loops manually I get:

$ ./nbody-unrolled 50000000
3.7s for clang vs 6.3s for GCC.
17.6B instructions for clang vs 29.6B instructions for GCC.

While the first sample is a subject to unrolling heuristic, the second is about optimizing the completely linear chunk of code with many floating point multiplications and additions.

I tried reducing the sample further, but I only came up with PR98774.


---


### compiler : `gcc`
### title : `[11 Regression] Bad interaction between IPA frequences and IRA resulting in spills due to changes in BB frequencies`
### open_at : `2021-01-21T14:31:25Z`
### last_modified_date : `2023-07-14T20:06:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98782
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50020
dumps and source

Hi,

James and I have been investigating the exchange2 regression that has been haunting Trunk

since:

        commit 1118a3ff9d3ad6a64bba25dc01e7703325e23d92
        Author: Jan Hubicka <jh@suse.cz>
        Date:   Tue Aug 11 12:02:32 2020 +0200
    Do not combine PRED_LOOP_GUARD and PRED_LOOP_GUARD_WITH_RECURSION
	
That patch fixed the prediction frequency for the basic blocks in some
of the recursively specialized functions in exchange2. Unfortunately, it
also created a large (12+%) performance regression on multiple targets (including x86).

After initially blaming the new prediction frequencies, and significant work
from Jan and Martin we have good confidence in the probabilities, however they
appear to be exposing issues with the probability-based cost models in IRA
causing additional spills after artificially limiting register pressure by
excluding caller-saved registers across a call site in the loop.

This testcase (tuned) for AArch64 shows the issue:

    void bar (int, int, int, int);
    int foo (int x, char* foo) {
      int tmp = x * 753;
      int t2 = tmp + 7;
      int t3 = tmp * 7;
      int c1 = 753;
      int c2 = c1 + 7;
      int c3 = c3 * 7;
      for (int i = 0; i < 1024; i++) {
        if (__builtin_expect_with_probability (foo[i] != 0, 1, SPILLER))
          bar(x, tmp, t2, t3);
        c1 += foo[i+1];
        c2 *= foo[i+1];
        c3 += c2;
      }
      return c1 + c2 + c3;
    }

You can see the difference in the basic block labeled with L2 (using ffixed
to provoke register pressure):

Good compile:
  gcc -DSPILLER=0.5 -fno-shrink-wrap -fno-schedule-insns -O3   -ffixed-x23 -ffixed-x24 -ffixed-x25 -ffixed-x26 -ffixed-x27 -ffixed-x28 -fno-reorder-blocks

    .L2:
        ldrb    w0, [x19, 1]!
        add     w22, w22, w0
        mul     w20, w20, w0
        add     w21, w21, w20
        cmp     x7, x19
        bne     .L5
		
Bad compile:
  gcc -DSPILLER=0.51 -fno-shrink-wrap -fno-schedule-insns -O3   -ffixed-x23 -ffixed-x24 -ffixed-x25 -ffixed-x26 -ffixed-x27 -ffixed-x28 -fno-reorder-blocks

    .L2:
        ldrb    w0, [x19, 1]!
        add     w21, w21, w0
        mul     w20, w20, w0
        ldr     x0, [sp, 64]         <<<<<< Reload of x0
        add     w22, w22, w20
        cmp     x0, x19
        bne     .L5
		

Neither of us are an expert in this area by any means, I think what we're seeing can
be explained by this line in the IRA dump:

Good:
   Allocno a5r104 of GENERAL_REGS(24) has 17 avail. regs  4-15 18-22, node:  4-15 18-22 (confl regs =  0-3 16-17 23-85)
Bad:
    Allocno a5r104 of GENERAL_REGS(24) has 4 avail. regs  19-22, node:  19-22 (confl regs =  0-3 16-17 23-85)

In the bad case it looks like all the available registers go down, but these particular ones have so few left over that it causes the spill to occur.

The change in available registers comes from this code in setup_profitable_hardregs:

              if (ALLOCNO_UPDATED_MEMORY_COST (a) < costs[j]
                  /* Do not remove HARD_REGNO for static chain pointer
                     pseudo when non-local goto is used.  */
                  && ! non_spilled_static_chain_regno_p (ALLOCNO_REGNO (a)))
                CLEAR_HARD_REG_BIT (data->profitable_hard_regs,
                                    hard_regno);

Both sides of the ALLOCNO_UPDATED_MEMORY_COST (a) < costs[j] calculation
make use of frequency, but there is some asymmetry.

In the case of the bigger exchange2 regression if just revert 1118a3ff9d3ad6a64bba25dc01e7703325e23d92 which affects the BB frequencies we
get a slightly higher score than in GCC 10, indicating that the changes in IPA are indeed sound.

It also gives us a comparison where the entire sequence up to reload is exactly the same, aside from the counts in the BB and the frequencies.

Good BB:

;;  succ:       66 [always]  count:53687092 (estimated locally) (FALLTHRU)
;; lr  out 	 29 [x29] 31 [sp] 64 [sfp] 65 [ap] 1438
;; live  out 	 29 [x29] 31 [sp] 64 [sfp] 65 [ap] 1438

;; basic block 66, loop depth 0, count 107374184 (estimated locally), maybe hot
;;  prev block 65, next block 67, flags: (REACHABLE, RTL, MODIFIED)
;;  pred:       64 [50.0% (guessed)]  count:53687092 (estimated locally)
;;              65 [always]  count:53687092 (estimated locally) (FALLTHRU)

Bad BB:

;;  succ:       66 [always]  count:3487081 (estimated locally) (FALLTHRU)
;; lr  out 	 29 [x29] 31 [sp] 64 [sfp] 65 [ap] 1438
;; live  out 	 29 [x29] 31 [sp] 64 [sfp] 65 [ap] 1438

;; basic block 66, loop depth 0, count 6974163 (estimated locally), maybe hot
;;  prev block 65, next block 67, flags: (REACHABLE, RTL, MODIFIED)
;;  pred:       64 [50.0% (guessed)]  count:3487082 (estimated locally)
;;              65 [always]  count:3487081 (estimated locally) (FALLTHRU)

From there everything seems to change including the costs for register classes

Good:

a8(r112,l0) costs: GENERAL_REGS:0,0 FP_LO8_REGS:1445,14170 FP_LO_REGS:1445,14170 FP_REGS:1445,14170 POINTER_AND_FP_REGS:1445,14170 MEM:1156,11336
a45(r112,l1) costs: GENERAL_REGS:0,0 FP_LO8_REGS:12725,12725 FP_LO_REGS:12725,12725 FP_REGS:12725,12725 POINTER_AND_FP_REGS:12725,12725 MEM:10180,10180
Allocno a8r112 of GENERAL_REGS(30) has 26 avail. regs  1-15 18-28, node:  1-15 18-28 (confl regs =  0 16-17 29-85)

Bad:

a8(r112,l0) costs: GENERAL_REGS:0,0 FP_LO8_REGS:85,610 FP_LO_REGS:85,610 FP_REGS:85,610 POINTER_AND_FP_REGS:85,610 MEM:68,488
a45(r112,l1) costs: GENERAL_REGS:0,0 FP_LO8_REGS:525,525 FP_LO_REGS:525,525 FP_REGS:525,525 POINTER_AND_FP_REGS:525,525 MEM:420,420
Allocno a8r112 of GENERAL_REGS(30) has 10 avail. regs  19-28, node:  19-28 (confl regs =  0 16-17 29-85)

and a new spill a89 is created for a value that is actually live through all branches inside the loop:

Good IRA sets:

    Hard reg set forest:
      0:( 0-28 30 32-63)@0
        1:( 0-17 19-25 27-28 30)@112080
          2:( 1-15 19-25 27-28)@196432
            3:( 19-25 27-28)@280

Bad IRA sets:

    Hard reg set forest:
      0:( 0-28 30 32-63)@0
        1:( 0-22 24 28 30)@121976
          2:( 1-15 18-22 24 28)@116576
            3:( 19-22 24 28)@8864
      Spill a87(r99,l2)
      Spill a89(r112,l2)
	  
In the exchange2 example the function call seems thousands of instructions away, but is part of the main loop.
The function call is self recursive call (before specialization) and only needs x0.  So it looks like caller saved registers are being severely
penalized here.

That is to say, the likelihood of RA needing a caller-saved register is so high it should just use it, even in the
presence of the function call.  Since whether it spills the caller-saved or the temp register as it's trying now would
make no difference if it has to spill at the call site.  But if it doesn't then this would result in better code.

We would appreciate any tips/help/workarounds we could do to mitigate this as the regression is quite significant.

I have attached for convenience the dump files and generated code for the good and bad cases for the example above.

In exchange2 this code is generated in the __brute_force_MOD_digits_2.constprop.4 specialization.


---


### compiler : `gcc`
### title : `Fail to use SHRN instructions for narrowing shift on aarch64`
### open_at : `2021-01-22T11:27:56Z`
### last_modified_date : `2021-09-02T10:21:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98792
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
#define N 1024
unsigned short res[N];
unsigned int in[N];

void
foo (void)
{
  for (int i = 0; i < N; i++)
    res[i] = in[i] >> 3;
}

with -O3 -mcpu=neoverse-n1 on aarch64 generates the loop:
.L2:
        ldp     q1, q0, [x0]
        add     x0, x0, 32
        ushr    v1.4s, v1.4s, 3
        ushr    v0.4s, v0.4s, 3
        xtn     v2.4h, v1.4s
        xtn2    v2.8h, v0.4s
        str     q2, [x1], 16
        cmp     x0, x2
        bne     .L2

it could be using the SHRN narrowing shift instruction insted. LLVM can do it (some other inefficiencies aside):
.LBB0_1:                                // %vector.body
                                        // =>This Inner Loop Header: Depth=1
        add     x11, x10, x8
        ldp     q0, q1, [x11]
        add     x8, x8, #32                     // =32
        cmp     x8, #1, lsl #12                 // =4096
        shrn    v0.4h, v0.4s, #3
        shrn    v1.4h, v1.4s, #3
        stp     d0, d1, [x9, #-8]
        add     x9, x9, #16                     // =16
        b.ne    .LBB0_1

Some backend patterns can probably handle it, but maybe the vectoriser can do something useful earlier as well?


---


### compiler : `gcc`
### title : `loop is sub-optimized if index is unsigned int with offset`
### open_at : `2021-01-25T03:27:34Z`
### last_modified_date : `2021-01-27T08:46:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98813
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
For the below code:
---t.c----
void
foo (const double* __restrict__ A, const double* __restrict__ B, double* __restrict__ C,
     int n, int k, int m)
{
  for (unsigned int l_m = 0; l_m < m; l_m++)
    C[n + l_m] += A[k + l_m] * B[k];
}
------
compile with `gcc -O3 -S t.c -fopt-info`, we can see the loop was not vectorized because it may not safe to directly optimize with potential overflow.
clang could vectorize this code, while there are run-time instructions to check if it is safe to do the optimization.
```
  %1 = add nsw i64 %wide.trip.count, -1 = cnt-1
  %2 = trunc i64 %1 to i32 = (int)(cnt-1)
  %3 = xor i32 %n, -1 = n xor -1
  %4 = icmp ult i32 %3, %2 = (n xor -1) < (int)(cnt-1) 
  %5 = icmp ugt i64 %1, 4294967295 = cnt > 4294967295 (overflow?)
  %6 = or i1 %4, %5
```


---


### compiler : `gcc`
### title : `Optimize if (a != b) a = b;`
### open_at : `2021-01-25T09:20:59Z`
### last_modified_date : `2021-01-25T09:57:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98817
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
Consider the example:


void arithmetic(int& result, int value) {
    if (result != value) {
        result = value;
    }
}


GCC generates the following assembly:


arithmetic(int&, int):
  cmp DWORD PTR [rdi], esi
  je .L1
  mov DWORD PTR [rdi], esi
.L1:
  ret


The assembly seems suboptimal, because
1) cmov could be used
2) conditional jump could be totally removed, reducing the binary size and leaving only one mov instruction:

arithmetic(int&, int):
  mov DWORD PTR [rdi], esi
  ret



Godbolt playground https://godbolt.org/z/Pdz7eP with above sample and std::vector::clear() sample that would also benefit from the above optimization.


---


### compiler : `gcc`
### title : `SLP discovery does not consider all lane permutes`
### open_at : `2021-01-26T12:02:20Z`
### last_modified_date : `2021-01-27T08:22:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98837
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
While we SLP vectorize

int a[1024], b[1024], c[1024];

void foo ()
{
  for (int i = 0; i < 1024; i += 4)
    {
      c[i] = a[i] + b[i];
      c[i+1] = a[i+1] + b[i+1];
      c[i+2] = a[i+2] * b[i+2];
      c[i+3] = a[i+3] * b[i+3];
    }
}

by splitting the SLP group into two the very similar

int a[1024], b[1024], c[1024];

void foo ()
{
  for (int i = 0; i < 1024; i += 4)
    {
      c[i] = a[i] + b[i];
      c[i+1] = a[i+1] * b[i+1];
      c[i+2] = a[i+2] + b[i+2];
      c[i+3] = a[i+3] * b[i+3];
    }
}

is not SLPed because we do not consider splitting the group into
non-adjacent sets.  The same applies to basic-block SLP when
you make the data type double (so we don't need unrolling),
of course we simply fall back to a scalar build then.


---


### compiler : `gcc`
### title : `Why does baz call the delete operator for moved unique_ptr`
### open_at : `2021-01-26T15:43:19Z`
### last_modified_date : `2021-01-27T07:41:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98840
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.0`
### severity : `normal`
### contents :
I'm trying to evaluate the overhead of the `unique_ptr` and I do not understand why does Gcc execute the destructor of the `unique_ptr` passed by value?

Let's assume we have two examples of code:

C style:

```
#include <memory>

void foo(int* ptr);

void baz(int value)
{
    int* ptr = new int(value);

    try
    {
        foo(ptr);
    }
    catch(...)
    {
        delete ptr;
        throw;
    }
}
```

The asm (/O3):

```
baz(int):
        push    rbp
        push    rbx
        mov     ebx, edi
        mov     edi, 4
        sub     rsp, 8
        call    operator new(unsigned long)
        mov     DWORD PTR [rax], ebx
        mov     rdi, rax
        mov     rbp, rax
        call    foo(int*)
        add     rsp, 8
        pop     rbx
        pop     rbp
        ret
        mov     rdi, rax
        jmp     .L2

baz(int) [clone .cold]:
.L2:
        call    __cxa_begin_catch
        mov     esi, 4
        mov     rdi, rbp
        call    operator delete(void*, unsigned long)
        call    __cxa_rethrow
        mov     rbp, rax
        call    __cxa_end_catch
        mov     rdi, rbp
        call    _Unwind_Resume
```


And C++ style

```
#include <memory>

void foo(std::unique_ptr<int> ptr);

void baz(int value)
{
    foo(std::make_unique<int>(value));
}
```

The asm (/O3)

```
baz(int):
        push    rbp
        push    rbx
        mov     ebx, edi
        mov     edi, 4
        sub     rsp, 24
        call    operator new(unsigned long)
        lea     rdi, [rsp+8]
        mov     DWORD PTR [rax], ebx
        mov     QWORD PTR [rsp+8], rax
        call    foo(std::unique_ptr<int, std::default_delete<int> >)
        mov     rdi, QWORD PTR [rsp+8]
        test    rdi, rdi
        je      .L1
        mov     esi, 4
        call    operator delete(void*, unsigned long) <<<<<< Here, why do we need to call the delete operator. It is `foo` who is responsible for that
.L1:
        add     rsp, 24
        pop     rbx
        pop     rbp
        ret
        mov     rbp, rax
        jmp     .L3
baz(int) [clone .cold]:
```


---


### compiler : `gcc`
### title : `[10 regression] vectorizer failed to reduce max pattern since r9-1590`
### open_at : `2021-01-27T08:29:14Z`
### last_modified_date : `2023-07-07T09:22:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98848
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c

extern short a[9000];

int foo()
{ 
    int b;
    int i;
    b = a[0];

    for(i = 1; i < 9000; i ++) {
        if(a[i] < b) {
          b = a[i];
        }
    }
    return b;
}

gcc8 successfully vectorized the loop with option: -Ofast -march=skylake-avx512, but gcc9/10/trunk failed.

test.c:9:16: missed: couldn't vectorize loop
test.c:3:5: missed: not vectorized: relevant phi not supported: b_14 = PHI <_9(5), b_8(2)>
test.c:3:5: note: vectorized 0 loops in function.
test.c:14:10: note: ***** Analysis failed with vector mode V16HI
test.c:14:10: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V16HI

It seems vect_recog_widen_op_pattern failed to handle this???


---


### compiler : `gcc`
### title : `[11 Regression] botan XTEA is 100% slower on znver2 since r11-4428-g4a369d199bf2f34e`
### open_at : `2021-01-27T13:44:23Z`
### last_modified_date : `2021-02-05T13:03:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98855
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Since the revision the following is now slower:

$ make clean && ./configure.py --cxxflags="-Ofast -march=znver2" && make -j16 && ./botan speed XTEA

as seen here:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=245.710.1&plot.1=171.710.1&

Algorithm is implemented here:
src/lib/block/xtea/xtea.cpp


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] botan AES-128/XTS is slower by ~17% since r11-6649-g285fa338b06b804e72997c4d876ecf08a9c083af`
### open_at : `2021-01-27T14:28:25Z`
### last_modified_date : `2023-05-29T10:04:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98856
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Since the revision the following is slower:

$ make clean && ./configure.py --cxxflags="-Ofast -march=znver2 -fno-checking" && make -j16 && ./botan speed AES-128/XTS

as seen here:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=226.721.1&plot.1=14.721.1&


---


### compiler : `gcc`
### title : `Missed transform of (a >> 63) * b`
### open_at : `2021-01-28T13:38:30Z`
### last_modified_date : `2022-11-26T07:00:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98865
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
unsigned long foo (unsigned long a, unsigned long b)
{
  return (a >> 63) * b;
}

generates

foo:
.LFB0:
        .cfi_startproc
        shrq    $63, %rdi
        movq    %rdi, %rax
        imulq   %rsi, %rax
        ret

but we can do (like llvm):

foo:                                    # @foo
        .cfi_startproc
# %bb.0:
        movq    %rdi, %rax
        sarq    $63, %rax
        andq    %rsi, %rax
        retq


---


### compiler : `gcc`
### title : `Failure to use SRI instruction for shift-right-and-insert vector operations`
### open_at : `2021-01-28T15:02:50Z`
### last_modified_date : `2022-01-01T09:13:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98867
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
#define N 1024
unsigned char in[N];
unsigned char out[N];

#define SHIFT 6

void
foo (void)
{
  for (int i = 0; i < N; i++)
    {
      unsigned char mask = 255u >> SHIFT;
      unsigned char shifted = in[i] >> SHIFT;
      out[i] = (out[i] & ~mask) | shifted;
    }
}

at -O3 generates:
foo:
        adrp    x1, .LANCHOR0
        add     x1, x1, :lo12:.LANCHOR0
        movi    v2.16b, 0xfffffffffffffffc
        add     x2, x1, 1024
        mov     x0, 0
.L2:
        ldr     q0, [x1, x0]
        ldr     q1, [x0, x2]
        and     v0.16b, v0.16b, v2.16b
        ushr    v1.16b, v1.16b, 6
        orr     v0.16b, v0.16b, v1.16b
        str     q0, [x1, x0]
        add     x0, x0, 16
        cmp     x0, 1024
        bne     .L2
        ret

whereas it could use the SRI instruction as clang does (unrolled 2x):
foo:                                    // @foo
        adrp    x9, in
        adrp    x10, out
        mov     x8, xzr
        add     x9, x9, :lo12:in
        add     x10, x10, :lo12:out
.LBB0_1:                                // %vector.body
        add     x11, x9, x8
        add     x12, x10, x8
        ldp     q0, q1, [x11]
        ldp     q2, q3, [x12]
        add     x8, x8, #32                     // =32
        cmp     x8, #1024                       // =1024
        sri     v2.16b, v0.16b, #6
        sri     v3.16b, v1.16b, #6
        stp     q2, q3, [x12]
        b.ne    .LBB0_1

This may be a bit too complex for combine to match though


---


### compiler : `gcc`
### title : `[8/9/10/11 Regression] polyhedron rnflow.f90 regression since r8-2555-g344be1fd47d7d64e`
### open_at : `2021-01-28T15:26:57Z`
### last_modified_date : `2021-01-29T09:38:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98868
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Since the revision the benchmark is much slower:

$ gfortran rnflow.f90 -Ofast -march=znver1 && time ./a.out >/dev/null

0m7.690s -> 0m13.121s

One can see it here:
https://lnt.opensuse.org/db_default/v4/CPP/graph?plot.0=194.791.0&plot.1=188.791.0&plot.2=202.791.0&plot.3=154.791.0&plot.4=245.791.0&plot.5=171.791.0&


---


### compiler : `gcc`
### title : `[AArch64] Inefficient code generated for tbl NEON intrinsics`
### open_at : `2021-01-29T06:51:01Z`
### last_modified_date : `2021-08-22T10:14:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98877
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `12.0`
### severity : `enhancement`
### contents :
The use of NEON intrinsics is inefficient and leads developers to prefer inline assembly instead of intrinsics.

A similar performance bug for vmlal intrinsics was reported in https://gcc.gnu.org/PR92665
The code generated by GCC for table lookups is also inefficient:

$ cat red.c
#include "arm_neon.h"

uint8x16_t fun(uint8x16_t lo, uint8x16_t hi, uint8x16_t idx) {
  uint8x16x2_t tab = { .val = {lo, hi} };
  uint8x16_t res = vqtbl2q_u8(tab, idx);
  return res;
}

$ gcc -O3 -S -o- red.c
fun:
        mov     v4.16b, v0.16b
        mov     v5.16b, v1.16b
        tbl     v0.16b, {v4.16b - v5.16b}, v2.16b
        ret

$ clang -O3 -S -o- red.c
fun:
        tbl     v0.16b, { v0.16b, v1.16b }, v2.16b
        ret


---


### compiler : `gcc`
### title : `Implement empty struct optimisations on ARM`
### open_at : `2021-01-29T11:18:47Z`
### last_modified_date : `2021-07-21T05:38:13Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98884
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Empty "tag" structs (or classes) are useful for strong typing, function options, and so on.  The rules of C++ require these to have a non-zero size (so that addresses of different instances are valid and distinct), but they contain no significant data.  Ideally, therefore, the compiler will not generate code that sets values or copies values when passing around such types.  Unfortunately, that is not quite the case.

Consider these two examples, with foo1 creating a tag type, and foo2 passing it on:

struct Tag {
    friend Tag make_tag();
private:
    Tag() {}
};

Tag make_tag() { 
    return Tag{}; 
};

void needs_tag(Tag);

void foo1(void) {
    Tag t = make_tag();
    needs_tag(t);
}


struct Tag1 {};
struct Tag2 {};
struct Tag3 {};
struct Tag4 {};
struct Tag5 {};

void needs_tags(int x, Tag1 t1, Tag2 t2, Tag3 t3, Tag4 t4, Tag5 t5);

void foo2(Tag1 t1, Tag2 t2, Tag3 t3, Tag4 t4, Tag5 t5)
{
    needs_tags(12345, t1, t2, t3, t4, t5);
}


(Here is a godbolt link for convenience: <https://godbolt.org/z/o5K78h>)

On x86, since gcc 8, this has been quite efficient (this is all with -O2):

make_tag():
        xor     eax, eax
        ret
foo1():
        jmp     needs_tag(Tag)
foo2(Tag1, Tag2, Tag3, Tag4, Tag5):
        mov     edi, 12345
        jmp     needs_tags(int, Tag1, Tag2, Tag3, Tag4, Tag5)

The contents of the tag instances are basically ignored.  The exception is on "make_tag", where the return is given the value 0 unnecessarily.

But on ARM it is a different matter.  This is for the Cortex-M4:


make_tag():
        mov     r0, #0
        bx      lr
foo1():
        mov     r0, #0
        b       needs_tag(Tag)
foo2(Tag1, Tag2, Tag3, Tag4, Tag5):
        push    {lr}
        sub     sp, sp, #12
        mov     r2, #0
        mov     r3, r2
        strb    r2, [sp, #4]
        strb    r2, [sp]
        mov     r1, r2
        movw    r0, #12345
        bl      needs_tags(int, Tag1, Tag2, Tag3, Tag4, Tag5)
        add     sp, sp, #12
        ldr     pc, [sp], #4

The needless register and stack allocations, initialisations and copying mean that this technique has a significant overhead for something that should really "disappear in the compilation".

The x86 port manages this well.  Is it possible to get such optimisations into the ARM port too?


Oh, and for comparison, clang with the same options (-std=c++17 -Wall -Wextra -O2 -mcpu=cortex-m4) gives:

make_tag():
        bx      lr
foo1():
        b       needs_tag(Tag)
foo2(Tag1, Tag2, Tag3, Tag4, Tag5):
        movw    r0, #12345
        b       needs_tags(int, Tag1, Tag2, Tag3, Tag4, Tag5)


---


### compiler : `gcc`
### title : `[10/11 regression] Neon logical operations not vectorized in DImode since g:cdfc0e863a03698a80c74896cbdc9f5c8c652e64`
### open_at : `2021-01-29T14:49:34Z`
### last_modified_date : `2021-04-08T12:10:53Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98891
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Since g:cdfc0e863a03698a80c74896cbdc9f5c8c652e64 (r10-2761), I have noticed that vorr/vorn is no longer vectorized in DImode.

I am compiling a modified version of gcc/testsuite/gcc.target/arm/neon-vorns64.c with -mfloat-abi=hard -mcpu=cortex-a9 -mfpu=auto -O3:

====================================================
#include "arm_neon.h"
#include <stdlib.h>

int64x1_t out_int64x1_t = 0;
int64x1_t arg0_int64x1_t = (int64x1_t)0xdeadbeef00000000LL;
int64x1_t arg1_int64x1_t = (int64x1_t)(~0xdead00000000beefLL);

int main (void)
{

  out_int64x1_t = vorn_s64 (arg0_int64x1_t, arg1_int64x1_t);
  if (out_int64x1_t != (int64x1_t)0xdeadbeef0000beefLL)
    abort();
  return 0;
}
====================================================

Before that commit I get:

        vldr.64 d17, [r3]       @ int
...
        vldr.64 d16, [r3, #8]   @ int
        vorn    d16, d16, d17
...

After that commit:
        ldr     lr, [r3]
        ldr     r4, [r3, #8]
        ldr     ip, [r3, #4]
        ldr     r6, [r3, #12]
        mvn     r3, lr
        orr     r0, r4, r3
...
        mvn     r3, ip
        orr     r1, r6, r3
...


Recent trunk has:
        ldrd    r2, [r1]
        ldrd    r0, [r1, #8]
        mvn     r2, r2
        mvn     r3, r3
        orr     r2, r2, r0
...
        orr     r3, r3, r1


---


### compiler : `gcc`
### title : `C++ module generates too many dead code`
### open_at : `2021-01-29T21:46:19Z`
### last_modified_date : `2022-11-03T13:12:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98895
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
It is a general issue. Any module unit will generate a dead entry that is completely unused.

If those issues cannot be solved, I am sorry, I would only say C++20 modules failed completely.

I do not think it has anything to do with tree-optimizations. The compiler just should NOT emit any code to the middle-end because of the module tbh. The module should not affect how the assembly is generated.


---


### compiler : `gcc`
### title : `Failure to optimize abs pattern`
### open_at : `2021-02-01T07:48:46Z`
### last_modified_date : `2023-09-21T10:27:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98907
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int32_t f(int32_t x)
{
    return ((x >> 31) | 1) * x;
}

This can be optimized into `return __builtin_abs(x);` (where sizeof(int) == sizeof(int32_t)). This transformation is done by LLVM, but not GCC.


---


### compiler : `gcc`
### title : `[11 Regression] arithmetic involving struct members into operating on the entire struct fails at -O3`
### open_at : `2021-02-01T08:10:40Z`
### last_modified_date : `2023-09-21T10:27:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98908
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
struct reg {
    uint8_t l;
    uint8_t h;
};

reg f(reg x)
{
    return {(uint8_t)(x.l & 0xFE), (uint8_t)(x.h & 0x80)};
}

This can be optimized to this:

reg f(reg x)
{
    uint16_t tmp = (x.l | x.h << 8) & 0x80FE;
    return {(uint8_t)tmp, (uint8_t)(tmp >> 8)};
}

. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize odd loop pattern`
### open_at : `2021-02-01T08:47:58Z`
### last_modified_date : `2023-09-21T10:25:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98909
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(int x)
{
    bool odd = false;
    while (x > 0) { x--; odd = !odd; }
    return odd;
}

This can be optimized to `return (x > 0) && ((x & 1) != 0);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Very poor code generation for SSE 8-bit vector right shift`
### open_at : `2021-02-02T15:39:37Z`
### last_modified_date : `2023-09-21T10:24:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98934
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef char __attribute__((vector_size(16))) v16i8;

v16i8 f(v16i8 x, v16i8 y)
{
    return x >> y;
}

With -O3, LLVM outputs this:

f(char __vector(16), char __vector(16)):
  punpckhbw xmm2, xmm0 # xmm2 = xmm2[8],xmm0[8],xmm2[9],xmm0[9],xmm2[10],xmm0[10],xmm2[11],xmm0[11],xmm2[12],xmm0[12],xmm2[13],xmm0[13],xmm2[14],xmm0[14],xmm2[15],xmm0[15]
  psllw xmm1, 5
  punpckhbw xmm4, xmm1 # xmm4 = xmm4[8],xmm1[8],xmm4[9],xmm1[9],xmm4[10],xmm1[10],xmm4[11],xmm1[11],xmm4[12],xmm1[12],xmm4[13],xmm1[13],xmm4[14],xmm1[14],xmm4[15],xmm1[15]
  pxor xmm3, xmm3
  pxor xmm5, xmm5
  pcmpgtw xmm5, xmm4
  movdqa xmm6, xmm5
  pandn xmm6, xmm2
  psraw xmm2, 4
  pand xmm2, xmm5
  por xmm2, xmm6
  paddw xmm4, xmm4
  pxor xmm5, xmm5
  pcmpgtw xmm5, xmm4
  movdqa xmm6, xmm5
  pandn xmm6, xmm2
  psraw xmm2, 2
  pand xmm2, xmm5
  por xmm2, xmm6
  paddw xmm4, xmm4
  pxor xmm5, xmm5
  pcmpgtw xmm5, xmm4
  movdqa xmm4, xmm5
  pandn xmm4, xmm2
  psraw xmm2, 1
  pand xmm2, xmm5
  por xmm2, xmm4
  psrlw xmm2, 8
  punpcklbw xmm0, xmm0 # xmm0 = xmm0[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7]
  punpcklbw xmm1, xmm1 # xmm1 = xmm1[0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7]
  pxor xmm4, xmm4
  pcmpgtw xmm4, xmm1
  movdqa xmm5, xmm4
  pandn xmm5, xmm0
  psraw xmm0, 4
  pand xmm0, xmm4
  por xmm0, xmm5
  paddw xmm1, xmm1
  pxor xmm4, xmm4
  pcmpgtw xmm4, xmm1
  movdqa xmm5, xmm4
  pandn xmm5, xmm0
  psraw xmm0, 2
  pand xmm0, xmm4
  por xmm0, xmm5
  paddw xmm1, xmm1
  pcmpgtw xmm3, xmm1
  movdqa xmm1, xmm3
  pandn xmm1, xmm0
  psraw xmm0, 1
  pand xmm0, xmm3
  por xmm0, xmm1
  psrlw xmm0, 8
  packuswb xmm0, xmm2
  ret

GCC outputs this:

f(char __vector(16), char __vector(16)):
  push r15
  movd edx, xmm0
  movd ecx, xmm1
  push r14
  sar dl, cl
  push r13
  movzx edx, dl
  push r12
  push rbp
  push rbx
  sub rsp, 400
  movaps XMMWORD PTR [rsp+376], xmm0
  movzx ebx, BYTE PTR [rsp+377]
  movaps XMMWORD PTR [rsp+360], xmm1
  movzx ecx, BYTE PTR [rsp+361]
  movaps XMMWORD PTR [rsp+344], xmm0
  movzx ebp, BYTE PTR [rsp+346]
  sar bl, cl
  movaps XMMWORD PTR [rsp+328], xmm1
  movzx ecx, BYTE PTR [rsp+330]
  movaps XMMWORD PTR [rsp+312], xmm0
  movzx ebx, bl
  movzx r12d, BYTE PTR [rsp+315]
  sar bpl, cl
  movaps XMMWORD PTR [rsp+296], xmm1
  movzx ecx, BYTE PTR [rsp+299]
  movaps XMMWORD PTR [rsp+280], xmm0
  movzx ebp, bpl
  movzx r13d, BYTE PTR [rsp+284]
  sar r12b, cl
  movaps XMMWORD PTR [rsp+264], xmm1
  movzx ecx, BYTE PTR [rsp+268]
  movaps XMMWORD PTR [rsp+248], xmm0
  movzx r12d, r12b
  movzx r14d, BYTE PTR [rsp+253]
  sar r13b, cl
  movaps XMMWORD PTR [rsp+232], xmm1
  movzx ecx, BYTE PTR [rsp+237]
  movaps XMMWORD PTR [rsp+216], xmm0
  movzx r13d, r13b
  movzx r15d, BYTE PTR [rsp+222]
  sar r14b, cl
  movaps XMMWORD PTR [rsp+200], xmm1
  movzx ecx, BYTE PTR [rsp+206]
  movaps XMMWORD PTR [rsp+184], xmm0
  movzx r14d, r14b
  movaps XMMWORD PTR [rsp+168], xmm1
  sar r15b, cl
  movzx eax, BYTE PTR [rsp+191]
  movzx ecx, BYTE PTR [rsp+175]
  movaps XMMWORD PTR [rsp+152], xmm0
  movzx esi, BYTE PTR [rsp+160]
  movzx r15d, r15b
  sar al, cl
  movaps XMMWORD PTR [rsp+136], xmm1
  movzx ecx, BYTE PTR [rsp+144]
  movaps XMMWORD PTR [rsp+120], xmm0
  movzx edi, BYTE PTR [rsp+129]
  sar sil, cl
  movaps XMMWORD PTR [rsp+104], xmm1
  movzx ecx, BYTE PTR [rsp+113]
  movaps XMMWORD PTR [rsp+88], xmm0
  sar dil, cl
  mov BYTE PTR [rsp-89], sil
  movaps XMMWORD PTR [rsp+72], xmm1
  movzx esi, dil
  movzx ecx, BYTE PTR [rsp+82]
  movzx edi, BYTE PTR [rsp+98]
  movaps XMMWORD PTR [rsp+56], xmm0
  movzx r8d, BYTE PTR [rsp+67]
  sar dil, cl
  movaps XMMWORD PTR [rsp+40], xmm1
  movzx ecx, BYTE PTR [rsp+51]
  movaps XMMWORD PTR [rsp+24], xmm0
  movzx r9d, BYTE PTR [rsp+36]
  movzx edi, dil
  sar r8b, cl
  movaps XMMWORD PTR [rsp+8], xmm1
  movzx ecx, BYTE PTR [rsp+20]
  movaps XMMWORD PTR [rsp-8], xmm0
  movzx r10d, BYTE PTR [rsp+5]
  movzx r8d, r8b
  sar r9b, cl
  movaps XMMWORD PTR [rsp-24], xmm1
  movzx ecx, BYTE PTR [rsp-11]
  movaps XMMWORD PTR [rsp-40], xmm0
  movzx r11d, BYTE PTR [rsp-26]
  movzx r9d, r9b
  sar r10b, cl
  movaps XMMWORD PTR [rsp-56], xmm1
  movzx ecx, BYTE PTR [rsp-42]
  mov BYTE PTR [rsp-120], al
  movzx r10d, r10b
  movaps XMMWORD PTR [rsp-72], xmm0
  sar r11b, cl
  movzx eax, BYTE PTR [rsp-57]
  movaps XMMWORD PTR [rsp-88], xmm1
  movzx ecx, BYTE PTR [rsp-73]
  movzx r11d, r11b
  sar al, cl
  movzx ecx, al
  movzx eax, BYTE PTR [rsp-120]
  sal rcx, 8
  sal rax, 8
  or rcx, r11
  or rax, r15
  sal rax, 8
  or rax, r14
  sal rax, 8
  or rax, r13
  sal rax, 8
  or rax, r12
  sal rax, 8
  or rax, rbp
  sal rax, 8
  or rax, rbx
  movzx ebx, BYTE PTR [rsp-89]
  sal rax, 8
  sal rcx, 8
  or rcx, r10
  or rax, rdx
  sal rcx, 8
  mov QWORD PTR [rsp-120], rax
  or rcx, r9
  sal rcx, 8
  or rcx, r8
  sal rcx, 8
  or rcx, rdi
  sal rcx, 8
  or rcx, rsi
  sal rcx, 8
  or rcx, rbx
  mov QWORD PTR [rsp-112], rcx
  movdqa xmm0, XMMWORD PTR [rsp-120]
  add rsp, 400
  pop rbx
  pop rbp
  pop r12
  pop r13
  pop r14
  pop r15
  ret

I have not studied the generated code much, but I can tell at a glance that GCC's code spills to memory and does the entire operation with normal registers, whereas LLVM manages to keep it all in xmm registers, no spills to memory and 3 times less assembly code.


---


### compiler : `gcc`
### title : `Failure to optimize two reads from adjacent addresses into one`
### open_at : `2021-02-03T14:55:26Z`
### last_modified_date : `2023-09-21T10:23:10Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98953
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(uint8_t *read, size_t abs)
{
    return read[abs] + 0x100 * read[abs + 1];;
}

This can be optimized to (ignoring aliasing) `return *(uint16_t *)(read + abs)`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `((X << CST0) & CST1) == 0 is not optimized to 0 == (X & (CST1 >> CST0))`
### open_at : `2021-02-03T15:22:52Z`
### last_modified_date : `2023-09-21T10:22:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98954
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(uint8_t m)
{
    return !((m << 1) & 0xFF);
}

This can be optimized to `return !(m & 0x7F);`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize shift+compare to single shift`
### open_at : `2021-02-03T15:57:18Z`
### last_modified_date : `2023-09-21T10:21:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98955
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(uint8_t m)
{
    return m << 1 > 0xff;
}

This can be optimized to `return m >> 7;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize out boolean left shift`
### open_at : `2021-02-03T16:03:25Z`
### last_modified_date : `2023-09-21T10:21:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98956
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(bool c)
{
    return c << 1;
}

This can be optimized to `return c;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[11 Regression] [x86] Odd code generation for 8-bit right shift`
### open_at : `2021-02-03T17:42:23Z`
### last_modified_date : `2023-09-21T10:21:00Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98957
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
bool f(uint8_t m)
{
    return m >> 7;
}

With -O3, LLVM outputs this:

f(unsigned char):
  mov eax, edi
  shr al, 7
  ret

With -march=[some-amd-machine-type], GCC outputs this:

f(unsigned char):
  mov eax, edi
  shr ax, 7
  and eax, 1
  ret

Otherwise, it outputs the same thing as LLVM. I took a long look at x86-tune.def to see if there was anything related to this triggered by either from m_AMD_MULTIPLE or m_ZNVER, but couldn't find anything. Also, even if this is normal (8-bit shr is bad and 16-bit shr is better?? Since when ?? I've searched for a while and found nothing about this), GCC 10 outputs this:

f(unsigned char):
  movzx eax, dil
  shr ax, 7
  ret

making this look at the very least like a regression to me.


---


### compiler : `gcc`
### title : `Failure to optimize accumulate loop to mul`
### open_at : `2021-02-03T20:12:51Z`
### last_modified_date : `2023-06-11T01:03:08Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98960
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
int f(uint8_t max_value) {
    int sum = 0;
    for (int i = 0; i < max_value; i++) {
        sum += i;
    }
    return sum;
}

This can be optimized to `return (uint64_t)max_value * (max_value - 1) >> 1;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize successive comparisons with 0 into clz`
### open_at : `2021-02-03T20:58:38Z`
### last_modified_date : `2023-09-21T10:17:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98961
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int i, int j)
{
    return i == 0 || j == 0;
}

This can be optimized to `return (__builtin_clz(i) | __builtin_clz(j)) >> 5;` (if `clz(0)` returns 0). LLVM does this transformation, but GCC does not.

On x86, for example, with lzcnt, while this does not seem to be a net win in terms of performance (at least, not for this code alone), it also simply not a loss. As it is a win in terms of code size (which should make it a net win in most situations), I think that should make it a net win overall in actual code. See also https://gcc.gnu.org/bugzilla/show_bug.cgi?id=10588


---


### compiler : `gcc`
### title : `Perform bitops on floats directly with SSE`
### open_at : `2021-02-03T21:06:50Z`
### last_modified_date : `2021-11-28T06:49:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98962
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
(from https://stackoverflow.com/q/66023408/1918193 )

float f(float a){
  unsigned ai;
  __builtin_memcpy(&ai, &a, 4);
  unsigned ri = ai ^ (1U << 31);
  float r;
  __builtin_memcpy(&r, &ri, 4);
  return r;
}

results in

	movd	%xmm0, %eax
	addl	$-2147483648, %eax
	movd	%eax, %xmm0

while llvm simplifies it to

	xorps	.LCPI0_0(%rip), %xmm0


---


### compiler : `gcc`
### title : `Failure to optimize conditional or with 1 based on boolean condition to direct or`
### open_at : `2021-02-03T23:53:19Z`
### last_modified_date : `2023-09-21T10:16:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98966
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
int f(int m, bool c)
{
    return c ? (m | 1) : m;
}

This can be optimized to `return m | c;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Failure to optimize consecutive sub flags usage`
### open_at : `2021-02-05T14:20:19Z`
### last_modified_date : `2023-09-25T22:25:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98977
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
extern bool z, c;

uint8_t f(uint8_t dest, uint8_t src)
{
    u8 res = dest - src;
    z = !res;
    c = src > dest;
    return res;
}

With -O3, LLVM outputs this:

f(unsigned char, unsigned char):
  mov eax, edi
  sub al, sil
  sete byte ptr [rip + z]
  setb byte ptr [rip + c]
  ret

GCC outputs this:

f(unsigned char, unsigned char):
  mov eax, edi
  sub al, sil
  sete BYTE PTR z[rip]
  cmp dil, sil
  setb BYTE PTR c[rip]
  ret

It seems desirable to eliminate the `cmp`, unless there's some weird flag stall thing I'm not aware of.


---


### compiler : `gcc`
### title : `Consider packing _M_Engaged in the tail padding of T in optional<>`
### open_at : `2021-02-05T17:14:39Z`
### last_modified_date : `2023-03-25T18:44:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98978
### status : `NEW`
### tags : `ABI, missed-optimization`
### component : `libstdc++`
### version : `11.0`
### severity : `enhancement`
### contents :
Using std::optional with some types may considerably increase object sizes since it adds alignof(T) bytes worth of overhead. Sometimes it is possible to avoid this overhead if the flag indicating presence of the stored value (_M_Engaged in libstdc++ sources) is placed in the tail padding of the T object. This can be done if std::optional constructs an object of a type that derives from T, which has an additional bool data member that is initialized to true upon construction. The below code roughly illustrates the idea:

template< typename T >
struct _Optional_payload_base
{
  struct _PresentT : T
  {
    const bool _M_Engaged = true;

    // Forwarding ctors and other members
  };

  static constexpr size_t engaged_offset = offsetof(_PresentT, _M_Engaged);

  struct _AbsentT
  {
    unsigned char _M_Offset[engaged_offset];
    const bool _M_Engaged = false;
  };

  union _Storage
  {
    _AbsentT _M_Empty;
    _PresentT _M_Value;

    _Storage() : _M_Empty() {}

    // Forwarding ctors and other members
  };

  _Storage _M_payload

  bool is_engaged() const noexcept
  {
    return *reinterpret_cast< const bool* >(reinterpret_cast< const unsigned char* >(&_M_payload) + engaged_offset);
  }
};

The above relies on some implementation details, such as:

- offsetof works for the type T. It does for many types in gcc, beyond what is required by the C++ standard. Maybe there is a way to avoid offsetof, I just didn't immediately see it.
- The location of _M_Engaged in both _PresentT and _AbsentT is the same. This is a property of the target ABI, and AFAICS it should be true at least on x86 psABI and I think Microsoft ABI.

The above will only work for non-final class types, for other types, and where the above requirements don't hold true, the current code with a separate _M_Engaged flag would work.


---


### compiler : `gcc`
### title : `gcc-10.2 for RISC-V has extraneous register moves`
### open_at : `2021-02-06T00:20:17Z`
### last_modified_date : `2021-06-15T07:01:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98981
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
gcc is inserting an unnecessary register-register move for a simple max-style operation:

int a[256], b[256];
int32_t find_max_i32() {
  int32_t xme = 0, sc=0;
  for (int32_t i = 0; i < 100; i++) {
    if ((sc=a[i]+b[i]) > xme) xme=sc;
  }
  return xme;
}

This is from the SPECint2006 benchmark HMMER, in P7Viterbi(), hence the variable names sc and xme from the original source.

Under these flags:
-march=rv64imafdc -mcmodel=medany -mabi=lp64d -O3

I get this disassembly for the loop:

.L5:
  lw  a5,0(a4)
  lw  a2,0(a3)
  addi  a4,a4,4
  addi  a3,a3,4
  addw  a2,a5,a2
  mv  a5,a2  <--- unnecessary move
  bge a2,a0,.L4
  mv  a5,a0
.L4:
  sext.w  a0,a5
  bne a4,a1,.L5

If the addw targets a5, and the bge compares a5 to a0, the mv could be removed. In fact, if the variable types are changed to int64_t, that's exactly what happens:

.L13:
  ld  a5,0(a4)
  ld  a2,0(a3)
  addi  a4,a4,8
  addi  a3,a3,8
  add a5,a5,a2
  bgeu  a0,a5,.L12
  mv  a0,a5
.L12:
  bne a4,a1,.L13


---


### compiler : `gcc`
### title : `Optimizing loop variants of fixed-byte-order functions`
### open_at : `2021-02-06T17:30:45Z`
### last_modified_date : `2021-02-08T09:12:28Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98982
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `unknown`
### severity : `normal`
### contents :
These functions actually generate loops whereas they should just be optimized to no-ops.

It's the classic byte-read/write idiom for little endian encoded integers, just written as a loop, to accomodate varying sizeof(T) in the template.

template <typename T>
struct little_endian
{
   uint8_t data[sizeof(T)];

   constexpr little_endian() : data{}
   {
   }
   constexpr little_endian(T in)
   {
      for (size_t i = 0; i < sizeof(T); i++)
        data[i] = (in >> (8 * i)) & 0xFF;
   }
   constexpr operator T() const
   {
      T res = 0;
      for (size_t i = 0; i < sizeof(T); i++)
        res |= static_cast<T>(data[i]) << (8u * i);
      return res;
   }
};


-O3 or -funroll-loops correctly unrolls the encoding (the constructor), but the operator T() still ends up being

▸   endbr64
▸   movabsq▸$72057594037927935, %rdx
▸   movq▸   %rdi, %rax
▸   shrq▸   $56, %rax
▸   andq▸   %rdi, %rdx
▸   salq▸   $56, %rax
▸   orq▸%rdx, %rax
▸   ret

Seems weird to me.


---


### compiler : `gcc`
### title : `Failure to optimize out float conversion from long long->float->char conversion on -fno-trapping-math`
### open_at : `2021-02-07T07:51:45Z`
### last_modified_date : `2023-09-21T10:15:58Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98984
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
char f(unsigned long long n)
{
    return (float)n;
}

This can be optimized to `return n;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `Try matching both orders of commutative RTX operations when there is no canonical order`
### open_at : `2021-02-07T18:07:49Z`
### last_modified_date : `2021-02-10T17:02:56Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98986
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
The motivating aarch64 testcase is this:

#include <arm_neon.h>
int32x4_t
foo (int16x4_t a, int16x4_t b)
{
  int16x4_t tmp = vdup_n_s16 (vget_lane_s16 (b, 3));

  return vmull_s16 (tmp, a);
}

int32x4_t
foo2 (int16x4_t a, int16x4_t b)
{
  int16x4_t tmp = vdup_n_s16 (vget_lane_s16 (b, 3));

  return vmull_s16 (a, tmp);
}

Both functions should generate the widening-mult-by-lane form:
        smull   v0.4s, v0.4h, v1.h[3]   // 13   [c=16 l=4]  aarch64_vec_smult_lane_v4hi

However only the second function foo2 manages to match it.
We have a pattern for this in aarch64-simd.md:
(define_insn "aarch64_vec_<su>mult_lane<Qlane>"
  [(set (match_operand:<VWIDE> 0 "register_operand" "=w")
        (mult:<VWIDE>
          (ANY_EXTEND:<VWIDE>
            (match_operand:<VCOND> 1 "register_operand" "w"))
          (ANY_EXTEND:<VWIDE>
            (vec_duplicate:<VCOND>
              (vec_select:<VEL>
                (match_operand:VDQHS 2 "register_operand" "<vwx>")
                (parallel [(match_operand:SI 3 "immediate_operand" "i")]))))))]
  "TARGET_SIMD"
  {
    operands[3] = aarch64_endian_lane_rtx (<MODE>mode, INTVAL (operands[3]));
    return "<su>mull\\t%0.<Vwtype>, %1.<Vcondtype>, %2.<Vetype>[%3]";
  }
  [(set_attr "type" "neon_mul_<Vetype>_scalar_long")]
)

For foo combine tries and fails to match the vec_select in the first arm of the mult:
(set (reg:V4SI 93 [ <retval> ])
    (mult:V4SI (sign_extend:V4SI (vec_duplicate:V4HI (vec_select:HI (reg:V4HI 99)
                    (parallel:V4HI [
                            (const_int 3 [0x3])
                        ]))))
        (sign_extend:V4SI (reg:V4HI 98))))

Unfortunately, due to the sign_extends on both arm of the mult there is no canonical order for these expressions as both arms of the MULT are RTX_UNARY expressions and swap_commutative_operands_p doesn't try to swap them around.
I guess we can work around this by adding more patterns in the backend to match the two different orders we can get in this situation, but we've got 
so many similar patterns in the backend...

Do you think it's feasible to get recog or combine to try out both permutations of such commutative operations when matching without blowing up compile time?
Any other ideas for resolving this are welcome


---


### compiler : `gcc`
### title : `Potentially missed optimization. Arrays are created without need`
### open_at : `2021-02-08T19:05:26Z`
### last_modified_date : `2023-10-01T03:32:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99011
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.2.0`
### severity : `enhancement`
### contents :
Consider the code:

```
int bar(int N)
{
    return  N > 10 ? 14 : 8;
}

int foo(int N)
{
    return (const int[]){8, 14}[N > 10]; 
}


int zoo(int N)
{
    int a[] = {8,14};
    return a[N > 10]; 
}

int boo(int N)
{
    const static int a[] = {8,14};
    return a[N > 10]; 
}
```

IMO in all cases the generated code should be the same as generated for function bar. IMO arrays can be optimized out.

Compiled with -O3

foo:
        mov     rax, QWORD PTR .LC0[rip]
        mov     QWORD PTR [rsp-8], rax
        xor     eax, eax
        cmp     edi, 10
        setg    al
        mov     eax, DWORD PTR [rsp-8+rax*4]
        ret
bar:
        cmp     edi, 10
        mov     edx, 8
        mov     eax, 14
        cmovle  eax, edx
        ret
zoo:
        mov     rax, QWORD PTR .LC0[rip]
        mov     QWORD PTR [rsp-8], rax
        xor     eax, eax
        cmp     edi, 10
        setg    al
        mov     eax, DWORD PTR [rsp-8+rax*4]
        ret
boo:
        xor     eax, eax
        cmp     edi, 10
        setg    al
        mov     eax, DWORD PTR a.0[0+rax*4]
        ret
a.0:
        .long   8
        .long   14
.LC0:
        .long   8
        .long   14


gcc -O3 --verbose:
Using built-in specs.
COLLECT_GCC=/opt/compiler-explorer/gcc-10.2.0/bin/gcc
Target: x86_64-linux-gnu
Configured with: ../gcc-10.2.0/configure --prefix=/opt/compiler-explorer/gcc-build/staging --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --disable-bootstrap --enable-multiarch --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --enable-clocale=gnu --enable-languages=c,c++,fortran,ada,go,d --enable-ld=yes --enable-gold=yes --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-linker-build-id --enable-lto --enable-plugins --enable-threads=posix --with-pkgversion=Compiler-Explorer-Build
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.2.0 (Compiler-Explorer-Build) 
COLLECT_GCC_OPTIONS='-fdiagnostics-color=always' '-g' '-o' './output.s' '-masm=intel' '-S' '-v' '-O3' '-mtune=generic' '-march=x86-64'
 /opt/compiler-explorer/gcc-10.2.0/bin/../libexec/gcc/x86_64-linux-gnu/10.2.0/cc1 -quiet -v -imultiarch x86_64-linux-gnu -iprefix /opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/ <source> -quiet -dumpbase example.c -masm=intel -mtune=generic -march=x86-64 -auxbase-strip ./output.s -g -O3 -version -fdiagnostics-color=always -o ./output.s
GNU C17 (Compiler-Explorer-Build) version 10.2.0 (x86_64-linux-gnu)
	compiled by GNU C version 7.5.0, GMP version 6.1.0, MPFR version 3.1.4, MPC version 1.0.3, isl version isl-0.18-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
ignoring nonexistent directory "/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/../../../../x86_64-linux-gnu/include"
ignoring duplicate directory "/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/10.2.0/include"
ignoring nonexistent directory "/usr/local/include/x86_64-linux-gnu"
ignoring duplicate directory "/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/10.2.0/include-fixed"
ignoring nonexistent directory "/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/10.2.0/../../../../x86_64-linux-gnu/include"
#include "..." search starts here:
#include <...> search starts here:
 /opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/include
 /opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/include-fixed
 /usr/local/include
 /opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/../../include
 /usr/include/x86_64-linux-gnu
 /usr/include
End of search list.
GNU C17 (Compiler-Explorer-Build) version 10.2.0 (x86_64-linux-gnu)
	compiled by GNU C version 7.5.0, GMP version 6.1.0, MPFR version 3.1.4, MPC version 1.0.3, isl version isl-0.18-GMP

GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 870fa0f1a93c5f0ff6fd5ef23d839e5a
COMPILER_PATH=/opt/compiler-explorer/gcc-10.2.0/bin/../libexec/gcc/x86_64-linux-gnu/10.2.0/:/opt/compiler-explorer/gcc-10.2.0/bin/../libexec/gcc/x86_64-linux-gnu/:/opt/compiler-explorer/gcc-10.2.0/bin/../libexec/gcc/:/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/../../../../x86_64-linux-gnu/bin/
LIBRARY_PATH=/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/:/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/:/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/:/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/../../../../lib64/:/lib/x86_64-linux-gnu/:/lib/../lib64/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib64/:/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/../../../../x86_64-linux-gnu/lib/:/opt/compiler-explorer/gcc-10.2.0/bin/../lib/gcc/x86_64-linux-gnu/10.2.0/../../../:/lib/:/usr/lib/
COLLECT_GCC_OPTIONS='-fdiagnostics-color=always' '-g' '-o' './output.s' '-masm=intel' '-S' '-v' '-O3' '-mtune=generic' '-march=x86-64'
Compiler returned: 0


---


### compiler : `gcc`
### title : `aarch64_rtx_costs is missing tests for vector immediate forms`
### open_at : `2021-02-09T18:08:03Z`
### last_modified_date : `2021-03-02T02:09:24Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99038
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
aarch64_rtx_costs is relatively good about matching immediates
for scalar operations, but it's missing cases for the corresponding
vector operations.  E.g. PLUS of a vector constant is more expensive
(by 1 instruction) than PLUS of a register, even if the constant
is a valid immediate operand.  This contributes to the brittleness
of the costing scheme.  E.g. any change to the cost of CONST_VECTOR
moves will trigger for cases in which no CONST_VECTOR move is
actually needed.


---


### compiler : `gcc`
### title : `[[gnu::const]] function needs noexcept to be recognized as loop invariant`
### open_at : `2021-02-09T21:36:15Z`
### last_modified_date : `2021-02-10T07:31:48Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99046
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
(from https://stackoverflow.com/q/66100945/1918193)

double x[1000] = {};
[[gnu::const]] double* g(double* var);
void f() {
        for (int i = 1; i < 1000; i++) {
                g(x)[i] = (g(x)[i-1] + 1.0) * 1.001;
        }
}

g++ -O3 eliminates half of the calls to g, but fails to move it to a single call before the loop, while llvm does just that. Gcc does manage it if I mark f as noexcept or nothrow. Whether const functions may throw seems debatable, but if they do throw, I expect them to do so consistently, and since the loop has at least one iteration and starts with this call, the transformation seems safe to me.


---


### compiler : `gcc`
### title : `Missed optimization for induction variable elimination`
### open_at : `2021-02-10T23:49:16Z`
### last_modified_date : `2021-02-18T02:38:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99067
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
For RISC-V, this code snippet could eliminate the induction variable for the loop ending condition.

#include <stdint.h>
int16_t a[1000];
int64_t N = 100;
int64_t found_zero() {
  for (int i = 0; i <= N; i++) {
    if (a[i] == 0) return 1;
  }
  return 0;
}

gcc -O3 for RISC-V generates:

.L8:
  addi  a5,a5,2
  blt a2,a4,.L4
.L3:
  lh  a3,0(a5)
  addi  a4,a4,1  <-- induction variable update that can be eliminated
  bne a3,zero,.L8
  li  a0,1
  ret
.L4:
  li  a0,0
  ret

Is there a reason it doesn't do this transform (written at the C level) to do pointer comparisons:

  for (int16_t* p = &a[0]; p <= &a[N]; p++) { ... }

That C code is able to remove the extra add instruction:

.L15:
  bgtu  a5,a3,.L12
.L11:
  lh  a4,0(a5)
  addi  a5,a5,2
  bne a4,zero,.L15
  li  a0,1
  ret
.L12:
  li  a0,0
  ret

I verified the same issue occurs in PowerPC and ARM code-gen, so this isn't target-specific.


---


### compiler : `gcc`
### title : `Missed PowerPC lhau optimization`
### open_at : `2021-02-11T00:13:29Z`
### last_modified_date : `2021-02-17T00:33:43Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99068
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
(This relates to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99067 but is a distinct target optimization bug).

This code:

int16_t a[1000];
int64_t N = 100;
int found_zero_ptr(int *a, int N) {
  for (int16_t* p = &a[0]; p <= &a[N]; p++) {
    if (*p == 0) return 1;
  }
  return 0;
}

generates this PowerPC assembly under -O3:
...
.L15:
  bgt 7,.L12
.L11:
  lha 10,0(9)
  addi 9,9,2
  cmpld 7,9,8
  cmpwi 0,10,0
  bne 0,.L15
...

In a minor variation of this code, the lha and addi are merged into an lhau. Why does gcc not do that same merge in the code shown here?


---


### compiler : `gcc`
### title : `Failure to optimize bool selection pattern`
### open_at : `2021-02-11T08:21:30Z`
### last_modified_date : `2023-09-21T09:25:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99069
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
bool f(bool x, bool y)
{
    return (x ? y : false) ? x : false;
}

This can be optimized to `return x & y;`. This transformation is done by LLVM, but not by GCC.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Optimizer moves struct initialization into loop`
### open_at : `2021-02-12T01:17:52Z`
### last_modified_date : `2023-07-07T10:39:03Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99078
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Consider the following piece of code (https://godbolt.org/z/WhTcbd):

> struct S
> {
>     char c[24];
> };
> 
> void copy(S *dest, unsigned count)
> {
>     S s {};
>     for (int i = 0; i < 7; ++i)
>         s.c[i] = i;
>     for (int i = 8; i < 15; ++i)
>         s.c[i] = i;
>     for (int i = 16; i < 23; ++i)
>         s.c[i] = i;
>     while (count--)
>         *dest++ = s;
> }

The generated assembly with -O2 looks like this:

> copy(S*, unsigned int):
>         mov     QWORD PTR [rsp-24], 0
>         pxor    xmm0, xmm0
>         movups  XMMWORD PTR [rsp-40], xmm0
>         test    esi, esi
>         je      .L1
>         mov     esi, esi
>         lea     rax, [rsi+rsi*2]
>         lea     rdx, [rdi+rax*8]
> .L3:
>         mov     eax, 1541
>         mov     ecx, 3340
>         mov     esi, 5396
>         mov     DWORD PTR [rsp-39], 67305985
>         mov     WORD PTR [rsp-35], ax
>         add     rdi, 24
>         mov     DWORD PTR [rsp-32], 185207048
>         mov     WORD PTR [rsp-28], cx
>         mov     BYTE PTR [rsp-26], 14
>         movdqu  xmm1, XMMWORD PTR [rsp-40]
>         mov     DWORD PTR [rsp-24], 319951120
>         mov     WORD PTR [rsp-20], si
>         mov     BYTE PTR [rsp-18], 22
>         mov     rax, QWORD PTR [rsp-24]
>         movups  XMMWORD PTR [rdi-24], xmm1
>         mov     QWORD PTR [rdi-8], rax
>         cmp     rdi, rdx
>         jne     .L3
> .L1:
>         ret

It can be seen that the struct initialization has been moved into the loop, which is a severe pessimization.

The issue cannot be reproduced if the struct is initialized this way:

> S s;
> memset(&s, 0, sizeof(s));

But the following still reproduces the issue:

> S s {};
> memset(&s, 0, sizeof(s));

Replacing the assignment inside the loop with memcpy does not affect the result.

According to Godbolt, the generated assembly has not changed since GCC 7.2. GCC 7.1 does not use vector registers but still initializes the struct inside the loop. GCC 6.4 and earlier do not use vector registers either but do initialize the struct outside the loop, as expected.

EXPECTED RESULT

Ideally, the loop body would be optimized into something like this:

>         movdqu  xmm1, XMMWORD PTR [rsp-40]
>         mov     rax, QWORD PTR [rsp-24]
> .L3:
>         add     rdi, 24
>         movups  XMMWORD PTR [rdi-24], xmm1
>         mov     QWORD PTR [rdi-8], rax
>         cmp     rdi, rdx
>         jne     .L3
> .L1:
>         ret

Thank you.


---


### compiler : `gcc`
### title : `manual bit-field creation followed by manual extraction does not always produce good code`
### open_at : `2021-02-12T23:02:56Z`
### last_modified_date : `2023-07-19T04:14:29Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99082
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Take:
#include <stdint.h>

static inline uint16_t foo(uint32_t *r8b8g8x)
{
    const uint8_t r8 = *r8b8g8x;
    const uint8_t b8 = *r8b8g8x >> 8;
    const uint8_t g8 = *r8b8g8x >> 16;
    return
        (uint16_t)r8 >> 3 |
        ((uint16_t)(g8 >> 2) << 5) |
        ((uint16_t)(b8 >> 3) << 11);
}

uint16_t bar(
    uint8_t *r8,
    uint8_t *g8,
    uint8_t *b8)
{
   uint32_t t = (uint32_t)*r8 |
        ((uint32_t)*b8 << 8) |
        ((uint32_t)*g8 << 16);
    return foo(&t);
}
uint16_t bar1(
    uint8_t *r8,
    uint8_t *g8,
    uint8_t *b8)
{
    return
        (uint16_t)*r8 >> 3 |
        ((uint16_t)(*g8 >> 2) << 5) |
        ((uint16_t)(*b8 >> 3) << 11);
}

---- CUT ----
bar and bar1 should produce the same code but currently does not.


---


### compiler : `gcc`
### title : `Big run-time regressions of 519.lbm_r with LTO`
### open_at : `2021-02-12T23:32:35Z`
### last_modified_date : `2023-05-29T10:04:12Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99083
### status : `REOPENED`
### tags : `missed-optimization, patch, ra`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
On AMD Zen2 CPUs, 519.lbm_r is 62.12% slower when built with -O2 and
-flto than when not using LTO.  It is also 62.12% slower than when
using GCC 10 with the two options.  My measurements match those from
LNT on a different zen2:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=325.477.0&plot.1=312.477.0&plot.2=349.477.0&plot.3=278.477.0&plot.4=401.477.0&plot.5=298.477.0

On the same CPU, compiling the benchmark with -Ofast -march=native
-flto is slower than non-LTO, by 8.07% on Zen2 and 6.06% on Zen3.  The
Zen2 case has also been caught by LNT:
https://lnt.opensuse.org/db_default/v4/SPEC/graph?plot.0=295.477.0&plot.1=293.477.0&plot.2=287.477.0&plot.3=286.477.0&

I have bisected both of these regressions (on Zen2s) to:

  commit 4c61e35f20fe2ffeb9421dbd6f26c767a234a4a0
  Author: Uros Bizjak <ubizjak@gmail.com>
  Date:   Wed Dec 9 21:06:07 2020 +0100

      i386: Remove REG_ALLOC_ORDER definition

      REG_ALLOC_ORDER just defines what the default is set to.

      2020-12-09  Uroš Bizjak  <ubizjak@gmail.com>

      gcc/    
              * config/i386/i386.h (REG_ALLOC_ORDER): Remove

...which looks like it was supposed to be a no-op, but I looked at the
-O2 LTO case and the assembly generated by this commit definitely
differs from the assembly produced by the previous one in instruction
selection, spilling and even some scheduling.  For example, I see
hunks like:

@@ -994,10 +996,10 @@
        movapd  %xmm13, %xmm9
        movsd   96(%rsp), %xmm13
        subsd   %xmm12, %xmm9
-       movsd   256(%rsp), %xmm12
+       movq    %rbx, %xmm12
+       mulsd   %xmm6, %xmm12
        movsd   %xmm5, 15904(%rdx)
        movsd   72(%rax), %xmm5
-       mulsd   %xmm6, %xmm12
        mulsd   %xmm0, %xmm9
        subsd   %xmm10, %xmm5
        movsd   216(%rsp), %xmm10

The -Ofast native LTO assemblies also differ.


---


### compiler : `gcc`
### title : `suboptimal codegen for division by constant 3`
### open_at : `2021-02-13T15:33:33Z`
### last_modified_date : `2023-10-19T16:35:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99087
### status : `RESOLVED`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
These two are functionally the same, but generate different code with g++ -O2:

unsigned long long foo(unsigned long long a)
{
    return a / 3;
}

unsigned long long bar(unsigned long long a)
{
    return (unsigned __int128)a * 0xAAAA'AAAA'AAAA'AAAB >> 65;
}

foo(unsigned long long):
        movabs  rdx, -6148914691236517205
        mov     rax, rdi
        mul     rdx
        mov     rax, rdx
        shr     rax
        ret
bar(unsigned long long):
        movabs  rax, -6148914691236517205
        mul     rdi
        mov     rax, rdx
        shr     rax
        ret

For some reason for division GCC chooses different argument order which causes generation of one extra mov.


---


### compiler : `gcc`
### title : `unnecessary zero extend before AND`
### open_at : `2021-02-13T21:10:12Z`
### last_modified_date : `2022-08-19T20:54:22Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99089
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `10.0`
### severity : `normal`
### contents :
Given this testcase extracted from newlib

struct s
{
  short s;
  int i;
};

extern void sub2 (void);
extern void sub3 (void);

int
sub (struct s* t)
{
  short i = t->s;
  if ((i & 0x8) == 0)
    t->s |= 0x800;
  if ((t->s & 0x3) == 0)
    sub3 ();
  t->i = i;
  return 0;
}

compiling for riscv32-elf with -O2 -S and looking at assembly code, I see two places where we have

        slli    a5,a5,16
        srli    a5,a5,16
        andi    a5,a5,3

The zero extend before the AND is clearly unnecessary.

It seems a complex set of circumstances leads to here.  The tree level optimizer extends the lifetime of the first zero extend into a phi, which means the operation is split across basic blocks.  This also means no REG_DEAD note and no combine.  It isn't until 309 bbro that the zero extend and AND end up back in the same basic block again, and that is too late to optimize it as nothing after 309 bbro can fix this.

So it appears we need a global rtl pass that can notice and fix redundant zero extends feeding into AND operations across basic blocks.


---


### compiler : `gcc`
### title : `local array not prompted to static`
### open_at : `2021-02-13T21:55:26Z`
### last_modified_date : `2021-12-14T16:42:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99091
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.0`
### severity : `enhancement`
### contents :
Consider the following (link to compiler explorer: https://godbolt.org/z/jWsxsK)

#include <array>

static constexpr auto doy = []{
    std::array<int, 13> month_offset{};
    for (int m=1; m<=12;++m) {
        month_offset[m] = (153*(m > 2 ? m-3 : m+9) + 2)/5;
    }
    return month_offset;
}();

auto foo(int m) -> int {
    #ifdef LOCAL
    constexpr auto doy = []{
        std::array<int, 13> month_offset{};
        for (int m=1; m<=12;++m) {
            month_offset[m] = (153*(m > 2 ? m-3 : m+9) + 2)/5;
        }
        return month_offset;
    }();
    #endif

    return doy[m];
}

This is a fragment of code that does some calculation to figure out the date, which isn't super important. If LOCAL is not defined (i.e. we declare the array as a namespace-scope constexpr), the -O3 codegen of 'foo' is:

foo(int):
        movsx   rdi, edi
        mov     eax, DWORD PTR doy[0+rdi*4]
        ret

However, if we want to move the declaration of doy to be more local to the calculation and compile with -DLOCAL, the codegen instead is (on -O3):

foo(int):
        pxor    xmm0, xmm0
        mov     rax, QWORD PTR .LC0[rip]
        movsx   rdi, edi
        mov     DWORD PTR [rsp-24], 275
        movaps  XMMWORD PTR [rsp-72], xmm0
        movdqa  xmm0, XMMWORD PTR .LC1[rip]
        mov     QWORD PTR [rsp-68], rax
        movaps  XMMWORD PTR [rsp-56], xmm0
        movdqa  xmm0, XMMWORD PTR .LC2[rip]
        movaps  XMMWORD PTR [rsp-40], xmm0
        mov     eax, DWORD PTR [rsp-72+rdi*4]
        ret

This can be alleviated by marked the local variable doy as being static constexpr. Except that this prevents 'foo' from being a constexpr function (can't have static variables). 

This just seems like bad codegen, I'm not sure there's any reason that the compiler needs to do work here. It would be nice if the codegen with a local constexpr variable were the same as if it were a non-local constexpr variable.


---


### compiler : `gcc`
### title : `[missed optimization] Missed devirtualization involving internal-linkage class type (but only sometimes)`
### open_at : `2021-02-14T01:08:17Z`
### last_modified_date : `2021-02-15T08:32:27Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99093
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
// https://godbolt.org/z/hx7h7v
struct Base {
    virtual int f() { return 1; }
};

namespace {
    struct Derived1 : public Base {
        int f() override { return 2; }
    };
    struct Derived2 : public Base {};
}

int leaf_class1(Base *p) { return ((Derived1*)p)->f(); }
    // devirtualized by GCC, because PrivateDerived is provably a leaf

int leaf_class2(Base *p) { return ((Derived2*)p)->f(); }
    // not devirtualized by GCC -- this smells like a missed-optimization bug

====

GCC 4.9 started to be able to devirtualize things in the compiler, based on translation-unit-wide (but still compiler-time) information. GCC 4.9.0 is able to devirtualize the call in `leaf_class1`. This is awesome!

However, both GCC 4.9 and GCC trunk fail to apply the exact same optimization to `leaf_class2`. The only difference between `Derived1` and `Derived2` is that `Derived1::f` is declared directly in `Derived1` whereas `Derived2::f` is technically a member of `Base`. That shouldn't matter at all to the devirtualization logic. But apparently it does.

====

Barely possibly related missed-devirtualization bugs: #47316, #60674, #89924, #94243.


---


### compiler : `gcc`
### title : `Inconsistent vector length used in autovectorizer for AVX-512`
### open_at : `2021-02-15T05:23:49Z`
### last_modified_date : `2021-02-16T08:03:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99100
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
When AVX512 instruction is available, the auto-vectorizer in gcc 
sometimes generates calls to AVX2 functions instead of AVX512 functions.
-mprefer-vector-width=512 does not affect the result.


$ cat vabitest.c
#include <stdio.h>
#include <math.h>

_Pragma ("omp declare simd simdlen(8) notinbranch") 
__attribute__((const)) double myfunc(double x);

#define N 1024
__attribute__ ((__aligned__(256))) double a[N], b[N], c[N];

int main(void) {
   for (int i = 0; i < N; i++) a[i] = myfunc(b[i]);
   for (int i = 0; i < N; i++) c[i] = sin(b[i]);
}

$ gcc-10 -ffast-math -O3 -mavx512f -fopenmp vabitest.c -S -o- | grep _ZGV
         call    _ZGVdN8v_myfunc@PLT
         call    _ZGVeN8v_sin@PLT


---


### compiler : `gcc`
### title : `Stack used for concatenating values when returning struct by register`
### open_at : `2021-02-16T22:23:21Z`
### last_modified_date : `2021-11-16T02:16:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99128
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `normal`
### contents :
Consider the following C++ example (https://godbolt.org/z/deqf4T):

> struct A { int x, y; };
> 
> A makeA(int a, int b)
> {
>     return {a, b};
> }
> 
> struct B { int x, y, z; };
> 
> B makeB(int a, int b)
> {
>     return {a, b};
> }

In 'makeA' the whole struct fits into a 64-bit register and 'a' and 'b' are concatenated with arithmetic operations, as expected:

> makeA(int, int):
>         sal     rsi, 32
>         mov     eax, edi
>         or      rax, rsi
>         ret

But in 'makeB' the struct has a third field which gets returned in a different register. In this case, 'a' and 'b' are written into the stack and then moved to the return register:

> makeB(int, int):
>         mov     DWORD PTR [rsp-20], edi
>         xor     edx, edx
>         mov     DWORD PTR [rsp-16], esi
>         mov     rax, QWORD PTR [rsp-20]
>         ret

Here is another example:

> struct C {
>     union {
>         char a;
>         char b[3];
>     };
>     char c;
> };
> 
> C makeC(char a)
> {
>     return {a, 1};
> }

The assembly looks like this:

> makeC(char):
>         xor     eax, eax
>         mov     BYTE PTR [rsp-4], dil
>         mov     WORD PTR [rsp-3], ax
>         mov     BYTE PTR [rsp-1], 1
>         mov     eax, DWORD PTR [rsp-4]
>         ret

Which is pretty bad compared to what Clang produces:

> makeC(char):
>         movzx   eax, dil
>         or      eax, 16777216
>         ret

The 'makeB' example results in suboptimal x86_64 assembly in all versions of GCC available at Godbolt which can compile the code.

The 'makeC' example, though, is fine in GCC 8.3 and earlier but produces suboptimal assembly in GCC 9.1 and later.

EXPECTED RESULT

GCC should be able to concatenate primitive types with arithmetic-logic operations instead of using the stack when returning structs by register.

Thank you.


---


### compiler : `gcc`
### title : `[11 Regression] __builtin_clz match.pd transformation too greedy`
### open_at : `2021-02-17T23:11:42Z`
### last_modified_date : `2021-02-18T12:27:38Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99142
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50215
test-case  gcc.dg/tree-ssa/prXXXXX.c

See the attachment test-case, which is de-macroized from gcc.target/cris/pr93372-31.c, which started regressing with d2eb616a0f7b "match.pd: Add clz(X) == 0 -> (int)X < 0 etc. simpifications [PR94802]"

In the test-case, the result *is* used more than once (twice more besides the transformed compare) and the match.pd matching expression *does* have the s modifier: (op (clz:s @0) INTEGER_CST@1), but since the transformation doesn't result in "an expression with more than one operator" (cf. doc/match-and-simplify.texi), it's still performed.

The result is that the *input* is kept alive *after* the clz instruction.  This generally causes additional register pressure and throws away any re-use of incidentally computed condition codes.  Though the original observation was for cris-elf, where the effect is more dramatic, the effect is visible even for
x86_64 and of the same kind: losing the re-use of non-zero condition codes from the bsrl instruction, i.e. the transformation causes an additional instruction:

--- prXXXXX.s.64good    2021-02-17 02:26:57.646183108 +0100
+++ prXXXXX.s.64bad     2021-02-17 02:27:33.124979464 +0100
@@ -9,7 +9,8 @@ f:
        bsrl    %edi, %eax
        xorl    $31, %eax
        movl    %eax, (%rsi)
-       je      .L1
+       testl   %edi, %edi
+       js      .L1
        movl    %eax, (%rdx)
 .L1:
        ret

To wit, my conclusion is that the matching condition should better be gated by single_use(clz result) *everywhere*.

Alternatively, the "s" modifier adjusted somehow, but I'm not sure besides obviously just making it *exactly* single_use, and that suggestion has been shot down before.

Maybe there should be an additional *reverse* version of the "simplification", replacing "y = clz(x); if (x < 0) ...stuff using y but not x" -> "y = clz(x); if (y != 0) ...stuff using y but not x"!


---


### compiler : `gcc`
### title : `Missed optimization: Superfluous stack frame and code with noreturn or __builtin_unreachable()`
### open_at : `2021-02-18T14:45:57Z`
### last_modified_date : `2021-03-01T13:21:35Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99151
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
The following test code:

_Noreturn void r(void);
void u(void);

void g(void)
{
        r();
}


void f(void)
{
        u();
        __builtin_unreachable();
}

Produces code the following code on a sample set of architectures. There should be no stack frame set up. There should be no instructions after the no-return function calls (for example sparc).

sparc-rtems6-gcc -O2 -o - -S unreachable.c 
        .file   "unreachable.c"
        .section        ".text"
        .align 4
        .global g
        .type   g, #function
        .proc   020
g:
        save    %sp, -96, %sp
        call    r, 0
         nop
        nop
        .size   g, .-g
        .align 4
        .global f
        .type   f, #function
        .proc   020
f:
        save    %sp, -96, %sp
        call    u, 0
         nop
        nop
        .size   f, .-f
        .ident  "GCC: (GNU) 10.2.1 20210205 (RTEMS 6, RSB 61dcadee0825867ebe51f9f367430ef75b8fe9c0, Newlib d4a756f)"

arm-rtems6-gcc -O2 -o - -S unreachable.c 
        .cpu arm7tdmi
        .eabi_attribute 20, 1
        .eabi_attribute 21, 1
        .eabi_attribute 23, 3
        .eabi_attribute 24, 1
        .eabi_attribute 25, 1
        .eabi_attribute 26, 2
        .eabi_attribute 30, 2
        .eabi_attribute 34, 0
        .eabi_attribute 18, 4
        .file   "unreachable.c"
        .text
        .align  2
        .global g
        .arch armv4t
        .syntax unified
        .arm
        .fpu softvfp
        .type   g, %function
g:
        @ Function supports interworking.
        @ Volatile: function does not return.
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        push    {r4, lr}
        bl      r
        .size   g, .-g
        .align  2
        .global f
        .syntax unified
        .arm
        .fpu softvfp
        .type   f, %function
f:
        @ Function supports interworking.
        @ Volatile: function does not return.
        @ args = 0, pretend = 0, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        push    {r4, lr}
        bl      u
        .size   f, .-f
        .ident  "GCC: (GNU) 10.2.1 20210205 (RTEMS 6, RSB 61dcadee0825867ebe51f9f367430ef75b8fe9c0, Newlib d4a756f)"
powerpc-rtems6-gcc -O2 -o - -S unreachable.c 
        .file   "unreachable.c"
        .machine ppc
        .section        ".text"
        .align 2
        .globl g
        .type   g, @function
g:
.LFB0:
        .cfi_startproc
        stwu 1,-16(1)
        .cfi_def_cfa_offset 16
        mflr 0
        stw 0,20(1)
        .cfi_offset 65, 4
        bl r
        .cfi_endproc
.LFE0:
        .size   g,.-g
        .align 2
        .globl f
        .type   f, @function
f:
.LFB1:
        .cfi_startproc
        stwu 1,-16(1)
        .cfi_def_cfa_offset 16
        mflr 0
        stw 0,20(1)
        .cfi_offset 65, 4
        bl u
        .cfi_endproc
.LFE1:
        .size   f,.-f
        .ident  "GCC: (GNU) 10.2.1 20210205 (RTEMS 6, RSB 61dcadee0825867ebe51f9f367430ef75b8fe9c0, Newlib d4a756f)"
        .section        .note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `Suboptimal SVE code for ld4/st4 MLA code`
### open_at : `2021-02-19T10:27:09Z`
### last_modified_date : `2022-08-20T17:26:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99161
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
void ld_st_4 (char *x)
{
    for (int i = 0; i < 4096; i += 4)
    {
        char r = x[i];
        char g = x[i + 1];
        char b = x[i + 2];
        char a = x[i + 3];
        char smoosh = (r + g + b) * a;
        x[i] = r - smoosh;
        x[i+1] = g + smoosh;
        x[i+2] = b - smoosh;
        x[i+3] = a + smoosh;
    }
}

with -O3 (no SVE) gives a nice loop on aarch64:

ld_st_4(char*):
        add     x1, x0, 4096
.L2:
        ld4     {v0.16b - v3.16b}, [x0]
        add     v4.16b, v0.16b, v1.16b
        add     v4.16b, v4.16b, v2.16b
        mul     v4.16b, v4.16b, v3.16b
        sub     v16.16b, v0.16b, v4.16b
        add     v17.16b, v4.16b, v1.16b
        sub     v18.16b, v2.16b, v4.16b
        add     v19.16b, v4.16b, v3.16b
        st4     {v16.16b - v19.16b}, [x0], 64
        cmp     x1, x0
        bne     .L2
        ret

with -O3 -march=armv8.2-a+sve we get:
ld_st_4(char*):
        mov     x1, 0
        mov     w2, 1024
        ptrue   p0.b, all
        whilelo p1.b, wzr, w2
.L2:
        ld4b    {z0.b - z3.b}, p1/z, [x0]
        add     z4.b, z1.b, z0.b
        add     z4.b, z4.b, z2.b
        movprfx z16, z0
        mls     z16.b, p0/m, z4.b, z3.b
        movprfx z17, z1
        mla     z17.b, p0/m, z4.b, z3.b
        movprfx z18, z2
        mls     z18.b, p0/m, z4.b, z3.b
        movprfx z19, z3
        mla     z19.b, p0/m, z4.b, z3.b
        st4b    {z16.b - z19.b}, p1, [x0]
        incb    x1
        incb    x0, all, mul #4
        whilelo p1.b, w1, w2
        b.any   .L2
        ret

There's a few things that could be improved here:
* Use x0 for limit
* Use a single predicate (avoid multiple incb instructions)
* factor in the cost of movprfx somehow (i.e. the destructive semantics of the MLA/MLS), and prefer to use mul and add/sub

A better SVE loop would look a lot like Neon:
ld_st_4(char*):
        add     x1, x0, 4096
        ptrue   p0.b, all
.L2:
        ld4b    {z0.b - z3.b}, p0/z, [x0]
        add     z4.b, z1.b, z0.b
        add     z4.b, z4.b, z2.b
        mul     z4.b, p0/m, z4.b, z3.b
        sub     z16.b, z0.b, z4.b
        add     z17.b, z4.b, z1.b
        sub     z18.b, z2.b, z4.b
        add     z19.b, z4.b, z3.b
        st4b    {z16.b - z19.b}, p0, [x0]
        incb    x0, all, mul #4
        cmp     x1, x0
        bne     .L2
        ret


---


### compiler : `gcc`
### title : `asan initialization-order-fiasco false positive`
### open_at : `2021-02-20T23:26:00Z`
### last_modified_date : `2021-02-25T13:49:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99185
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.1`
### severity : `normal`
### contents :
The default constructor does not initialize primitive types.

a.cc:
=====
struct A {
  int value;
  A() = default;
};
A g;

b.cc:
=====
struct A {
  int value;
  A() = default;
};
extern A g;

int b = []() {
    g.value = 1;
    return 0;
}();

int main() {
    return 0;
}

$ g++ -fsanitize=address -std=c++20 a.cc b.cc; ./a.out

=================================================================
==3844820==ERROR: AddressSanitizer: initialization-order-fiasco on address 0x000000404160 at pc 0x0000004013c6 bp 0x7fff2371ecc0 sp 0x7fff2371ecb0
WRITE of size 4 at 0x000000404160 thread T0
    #0 0x4013c5 in b::{lambda()#1}::operator()() const (/home/kal/work/cxx/a.out+0x4013c5)
    #1 0x4012a8 in __static_initialization_and_destruction_0(int, int) (/home/kal/work/cxx/a.out+0x4012a8)
    #2 0x40134b in _GLOBAL__sub_I_b (/home/kal/work/cxx/a.out+0x40134b)
    #3 0x40142c in __libc_csu_init (/home/kal/work/cxx/a.out+0x40142c)
    #4 0x7f43cfb8b00d in __libc_start_main (/lib64/libc.so.6+0x2700d)
    #5 0x4010dd in _start (/home/kal/work/cxx/a.out+0x4010dd)

0x000000404160 is located 0 bytes inside of global variable 'g' defined in 'a.cc:5:3' (0x404160) of size 4
  registered at:
    #0 0x7f43d00b5cc8  (/lib64/libasan.so.6+0x37cc8)
    #1 0x401202 in _sub_I_00099_1 (/home/kal/work/cxx/a.out+0x401202)
    #2 0x40142c in __libc_csu_init (/home/kal/work/cxx/a.out+0x40142c)

SUMMARY: AddressSanitizer: initialization-order-fiasco (/home/kal/work/cxx/a.out+0x4013c5) in b::{lambda()#1}::operator()() const
Shadow bytes around the buggy address:
  0x0000800787d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0000800787e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0000800787f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x000080078800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x000080078810: 00 00 00 00 f9 f9 f9 f9 f9 f9 f9 f9 00 00 00 00
=>0x000080078820: f9 f9 f9 f9 f9 f9 f9 f9 00 00 00 00[f6]f6 f6 f6
  0x000080078830: f6 f6 f6 f6 00 00 00 00 04 f9 f9 f9 f9 f9 f9 f9
  0x000080078840: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x000080078850: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x000080078860: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x000080078870: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==3844820==ABORTING


---


### compiler : `gcc`
### title : `Optimise away vec_concat of 64-bit AdvancedSIMD operations with zeroes in aarch64`
### open_at : `2021-02-22T10:27:41Z`
### last_modified_date : `2023-05-31T16:46:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99195
### status : `NEW`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Motivating testcases:
#include <arm_neon.h>

#define ONE(OT,IT,OP,S)                         \
OT                                              \
foo_##OP##_##S (IT a, IT b)                     \
{                                               \
  IT zeros = vcreate_##S (0);                   \
  return vcombine_##S (v##OP##_##S (a, b), zeros);      \
}


#define FUNC(T,IS,OS,OP,S) ONE (T##x##OS##_t, T##x##IS##_t, OP, S)

#define OPTWO(T,IS,OS,S,OP1,OP2)        \
FUNC (T, IS, OS, OP1, S)                \
FUNC (T, IS, OS, OP2, S)

#define OPTHREE(T, IS, OS, S, OP1, OP2, OP3)    \
FUNC (T, IS, OS, OP1, S)        \
OPTWO (T, IS, OS, S, OP2, OP3)

#define OPFOUR(T,IS,OS,S,OP1,OP2,OP3,OP4)       \
FUNC (T, IS, OS, OP1, S)                \
OPTHREE (T, IS, OS, S, OP2, OP3, OP4)

#define OPFIVE(T,IS,OS,S,OP1,OP2,OP3,OP4, OP5)  \
FUNC (T, IS, OS, OP1, S)                \
OPFOUR (T, IS, OS, S, OP2, OP3, OP4, OP5)

#define OPSIX(T,IS,OS,S,OP1,OP2,OP3,OP4,OP5,OP6)        \
FUNC (T, IS, OS, OP1, S)                \
OPFIVE (T, IS, OS, S, OP2, OP3, OP4, OP5, OP6)

OPSIX (int8, 8, 16, s8, add, sub, mul, and, orr, eor)
OPSIX (int16, 4, 8, s16, add, sub, mul, and, orr, eor)
OPSIX (int32, 2, 4, s32, add, sub, mul, and, orr, eor)
OPFIVE (int64, 1, 2, s64, add, sub, and, orr, eor)

OPSIX (uint8, 8, 16, u8, add, sub, mul, and, orr, eor)
OPSIX (uint16, 4, 8, u16, add, sub, mul, and, orr, eor)
OPSIX (uint32, 2, 4, u32, add, sub, mul, and, orr, eor)
OPFIVE (uint64, 1, 2, u64, add, sub, and, orr, eor)

for example generates:
foo_add_s8:
        add     v0.8b, v0.8b, v1.8b
        mov     v0.8b, v0.8b
        ret

The 64-bit V8QI ADD instruction implicitly zeroes out the top bits of the 128-bit destination so the vec_concat with zeroes can be represented easily. However we don't have such pattern for all the AdvancedSIMd operations that we support. Indeed, it would bloat the MD files quite a bit. Can we come up with a define_subst scheme to auto-generate the patterns to match things like:
(set (reg:V16QI 93 [ <retval> ])
    (vec_concat:V16QI (plus:V8QI (reg:V8QI 98)
            (reg:V8QI 99))
        (const_vector:V8QI [
                (const_int 0 [0]) repeated x8
            ])))
?
Then we should be able to just generate:
foo_add_s8:
        add     v0.8b, v0.8b, v1.8b
        ret
etc.
The testcase above shows the problem for some of the simple binary ops, but there are many more instructions that can benefit from this.


---


### compiler : `gcc`
### title : `[11/12/13/14 Regression] Very large boolean expression leads to quite a few return statements`
### open_at : `2021-02-22T14:16:48Z`
### last_modified_date : `2023-09-04T08:09:37Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99199
### status : `NEW`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50233
C test case

The attached test case translated rather strangely with -O3.

The instruction sequence is very long, and it generates 42 return
instructions, quite a few of them at the end:

$ gcc -O3 -S tr.c
$ wc -l tr.s
504 tr.s
$ grep ret tr.s | wc -l
42

...

L24:
        movl    %r9d, %eax
        orl     %r8d, %eax
        xorl    $1, %eax
        jmp     .L112
.L131:
        ret
.L132:
        ret
.L140:
        ret
.L141:
        ret
.L145:
        ret
.L146:
        ret
.L148:
        ret
.L142:
        ret
.L143:
        ret
.L150:
        ret
.L152:
        ret
.L151:
        ret
.L138:
        ret
.L129:
        ret
.L147:
        ret
        .cfi_endproc

Seems to have been introduced with gcc 9, gcc 8 is fine.


---


### compiler : `gcc`
### title : `blend/shuffle`
### open_at : `2021-02-23T20:43:15Z`
### last_modified_date : `2021-12-23T21:27:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99228
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
Hello ggc team,
the compiler generates very inefficient code for the sgn functions (scalar and complex arguments)
https://godbolt.org/z/zvE3Mf

scalar
- float32/64: 2 conditional jumps instead of blend/shuffle
- float80: no fcmov
- integer: only cmov instead of blend/shuffle

complex
- float32/64: 4 conditional jumps instead of blend/shuffle
- float80: no fcmov
- integer: only cmov instead of blend/shuffle

For testing I have 3 versions each:
v1: total disaster
v2: better, only half of the jumps each time, but clang can't really handle that
v3: like v2, but clang seems to work too. If you remove [[likely]] from conditional_move like v1.

regards
Gero


---


### compiler : `gcc`
### title : `ivopts don't select the best candidates with -Os`
### open_at : `2021-02-26T11:15:32Z`
### last_modified_date : `2021-02-26T11:49:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99286
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50261
-c -march=rv32imafdc -mabi=ilp32d -Os ivopt_os.c -fdump-tree-ivopts-details

I have compared the assembly files and object files generated by different versions of the gcc.

One is:
$ /lhome/gengq/riscv64-linux-mastertest/bin/riscv64-unknown-linux-gnu-gcc -v
Using built-in specs.
COLLECT_GCC=/lhome/gengq/riscv64-linux-mastertest/bin/riscv64-unknown-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/lhome/gengq/riscv64-linux-mastertest/libexec/gcc/riscv64-unknown-linux-gnu/11.0.0/lto-wrapper
Target: riscv64-unknown-linux-gnu
Configured with: /lhome/gengq/riscv-gnu-toolchain-master/riscv-gnu-toolchain/riscv-gcc/configure --target=riscv64-unknown-linux-gnu --prefix=/lhome/gengq/riscv64-linux-mastertest --with-sysroot=/lhome/gengq/riscv64-linux-mastertest/sysroot --with-system-zlib --enable-shared --enable-tls --enable-languages=c,c++,fortran --disable-libmudflap --disable-libssp --disable-libquadmath --disable-libsanitizer --disable-nls --disable-bootstrap --src=.././riscv-gcc --disable-multilib --with-abi=lp64d --with-arch=rv64gc 'CFLAGS_FOR_TARGET=-O2   -mcmodel=medlow' 'CXXFLAGS_FOR_TARGET=-O2   -mcmodel=medlow'
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 11.0.0 20210209 (experimental) (GCC)

cmd is:
/lhome/gengq/riscv64-linux-mastertest/bin/riscv64-unknown-linux-gnu-gcc -march=rv32imafdc -mabi=ilp32d -Os ivopt_os.c -c

The other is：
$ /lhome/gengq/riscv64-linux-810test/bin/riscv32-unknown-linux-gnu-gcc -v
Using built-in specs.
COLLECT_GCC=/lhome/gengq/riscv64-linux-810test/bin/riscv32-unknown-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/lhome/gengq/riscv64-linux-810test/libexec/gcc/riscv32-unknown-linux-gnu/8.1.0/lto-wrapper
Target: riscv32-unknown-linux-gnu
Configured with: /lhome/gengq/riscv-gnu-toolchain-master/riscv-gnu-toolchain/riscv-gcc/configure --target=riscv32-unknown-linux-gnu --prefix=/lhome/gengq/riscv64-linux-810test --with-sysroot=/lhome/gengq/riscv64-linux-810test/sysroot --with-newlib --without-headers --disable-shared --disable-threads --with-system-zlib --enable-tls --enable-languages=c --disable-libatomic --disable-libmudflap --disable-libssp --disable-libquadmath --disable-libgomp --disable-nls --disable-bootstrap --src=.././riscv-gcc --with-pkgversion= --disable-multilib --with-abi=ilp32d --with-arch=rv32gc 'CFLAGS_FOR_TARGET=-O2  -mcmodel=medlow' 'CXXFLAGS_FOR_TARGET=-O2  -mcmodel=medlow' CC=gcc CXX=g++
Thread model: single
gcc version 8.1.0 ()

cmd is：
/lhome/gengq/riscv64-linux-810test/bin/riscv32-unknown-linux-gnu-gcc -march=rv32imafdc -mabi=ilp32d -Os ivopt_os.c -fdump-tree-all-details -c

The code generated by gcc11.0 is worse than by gcc8.1.0. I have done some analysis and I think the difference due to 'ivopts'.

It seems that gcc11.0 has done a more detailed job in 'ivopts'. For gcc11.0，there are 2 best candidate sets：
One is equivalent to what gcc8.0 used.
Another one is the final choice of gcc11.0. And its 'cost' is very close to the other one.
I noticed that: The second set include more invariants and less induction varibles. The code implementation prefers to use iv. And this preference can sway the final choice as the differences are minimal.
So,why prefer iv? Is there any better treatment here? What I can think of from my experience is that the inv variables are more atomic and have more potential to be optimized. But this also means that the inv may generate more intermediate variables if it is not optimised. Like this case, we chose to use more invs and also created more intermediate variables, which ended up overflowing the registers.

I'm not sure I've hit the nail on the head with my analysis, and I'd like to try to find a better solution.


---


### compiler : `gcc`
### title : `Built-in vec_splat generates sub-optimal code for -mcpu=power10`
### open_at : `2021-02-26T17:02:03Z`
### last_modified_date : `2022-03-08T16:20:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99293
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `10.2.1`
### severity : `normal`
### contents :
Created attachment 50263
Simplified test case

While adding code to Power Vector Library (PVECLIB), for the POWER10 target, I see strange code generation for Altivec built-in vec_splat for the vector long long type. I would expect a xxpermdi (xxspltd) based on the "Power Vector Intrinsic Programming Reference".

But I see the following generated:

0000000000000300 <test_vec_rlq_PWR10>:
     300:   67 02 69 7c     mfvsrld r9,vs35
     304:   67 4b 09 7c     mtvsrdd vs32,r9,r9
     308:   05 00 42 10     vrlq    v2,v2,v0
     30c:   20 00 80 4e     blr

While these seems to functionally correct, the trip through the GPR seems unnecessary. It requires two serially dependent instructions where a single xxspltd would do. I expected:

0000000000000300 <test_vec_rlq_PWR10>:
 300:   57 1b 63 f0     xxspltd vs35,vs35,1
 304:   05 18 42 10     vrlq    v2,v2,v3
 308:   20 00 80 4e     blr


The compiler was:

Compiler: gcc version 10.2.1 20210104 (Advance-Toolchain 14.0-2) [2093e873bb6c] (GCC)


---


### compiler : `gcc`
### title : `Need a recoverable version of __builtin_trap()`
### open_at : `2021-02-27T10:30:36Z`
### last_modified_date : `2021-08-26T12:38:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99299
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `10.2.1`
### severity : `enhancement`
### contents :
Linux kernel implements WARN() and WARN_ON() asserts using trap instructions.

Because gcc __builtin_trap() is not recoverable, Linux Kernel has hand code the trap, at the moment using 'twnei'. This leads to sub-optimal code generation.

As the powerpc trap instruction is recoverable as it generated a recoverable exception, it would be extremely usefull to also have a recoverable version of __builtin_trap() in gcc. Maybe call it __buitin_recoverable_trap()


---


### compiler : `gcc`
### title : `[11 Regression] range condition simplification after inlining`
### open_at : `2021-02-27T14:23:57Z`
### last_modified_date : `2021-03-10T16:41:57Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99305
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
bool foo(char c)
{
    return c >= '0' && c <= '9';
}

bool bar(char c)
{
    return c != '\0' && foo(c);
}

bool foobar(char c)
{
    return c != '\0' && c >= '0' && c <= '9';
}

// GCC 10
bar(char):
  sub edi, 48
  cmp dil, 9
  setbe al
  ret

// GCC 11
bar(char):
  xor eax, eax
  test dil, dil
  je .L3
  sub edi, 48
  cmp dil, 9
  setbe al
.L3:
  ret

https://godbolt.org/z/z4r9cv


---


### compiler : `gcc`
### title : `constexpr defined arrays within constexpr functions would benefit from lookup-tables`
### open_at : `2021-03-01T15:25:55Z`
### last_modified_date : `2021-03-03T00:19:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99320
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.1`
### severity : `normal`
### contents :
Hi gcc-team,

first of all, sorry if this is the wrong component, but I guess that this is a "missed-optimization" issue rather than a regular C++ issue, so I wasn't sure which component fit the most.

I have the following code (which can be further reduced, but I kept it as original as possible to reflect my use case):

```c++
#include <array>

struct foo
{
static constexpr char bar(unsigned idx)
{
    constexpr std::array<char, 256> lookup_table
    {
        [] () constexpr
        {
            std::array<char, 256> ret{};

            // reverse mapping for characters and their lowercase
            for (unsigned rnk = 0u; rnk < 15; ++rnk)
            {
                ret[rnk + 'A'] = rnk;
            }

            // set U equal to T
            ret['U'] = ret['T']; ret['u'] = ret['t'];

            // iupac characters get special treatment, because there is no N
            ret['R'] = ret['A']; ret['r'] = ret['A']; // A or G
            ret['Y'] = ret['C']; ret['y'] = ret['C']; // C or T
            ret['S'] = ret['C']; ret['s'] = ret['C']; // C or G
            ret['W'] = ret['A']; ret['w'] = ret['A']; // A or T
            ret['K'] = ret['G']; ret['k'] = ret['G']; // G or T
            ret['M'] = ret['A']; ret['m'] = ret['A']; // A or T
            ret['B'] = ret['C']; ret['b'] = ret['C']; // C or G or T
            ret['D'] = ret['A']; ret['d'] = ret['A']; // A or G or T
            ret['H'] = ret['A']; ret['h'] = ret['A']; // A or C or T
            ret['V'] = ret['A']; ret['v'] = ret['A']; // A or C or G

            return ret;
        }()
    };

    return lookup_table[idx];
}
};

int main(int argc, char const ** argv)
{
    return foo::bar(argc);
}

```

I wanted to switch from defining that lookup-table within the class (e.g. `static constexpr ... lookup_table = ...`) to define the lookup-table within the function directly, and I noticed that I had some performance regression in my benchmarks. Some micro benchmarks went from ~80ns to ~3000ns, but I also saw an impact on more "realistic" macro benchmarks.

After looking at the assembly https://godbolt.org/z/n9bo7W, I noticed that the table is "constructed" on each function call rather than a single lookup-instruction.

So I compared it to what clang does, and it seems that they are actually generating a static lookup table.

I know that this use case is quite niche, but it would be cool to have it nevertheless :)

Thank you!


---


### compiler : `gcc`
### title : `Poor codegen with simple varargs`
### open_at : `2021-03-02T11:49:10Z`
### last_modified_date : `2021-03-03T08:41:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99339
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `normal`
### contents :
These two functions should generate the same code, but the varargs version is much worse. And it is even worser still when you enable -fstack-protector-strong.

https://godbolt.org/z/noEYoh

#include <stdarg.h>

int test_va(int x, ...) {
    int i;
    va_list va;
    va_start(va, x);
    i = va_arg(va, int);
    va_end(va);
    return i + x;
}

int test_args(int x, int i) {
    return i + x;
}

# explicit args with or without stack protection
test_args:
        lea     eax, [rsi+rdi]
        ret

# without stack protection (why aren't dead stores to the stack being eliminated?)
test_va:
        lea     rax, [rsp+8]
        mov     QWORD PTR [rsp-40], rsi
        mov     QWORD PTR [rsp-64], rax
        lea     rax, [rsp-48]
        mov     QWORD PTR [rsp-56], rax
        mov     eax, DWORD PTR [rsp-40]
        mov     DWORD PTR [rsp-72], 8
        add     eax, edi
        ret

# with stack protection (yikes!)
test_va:
        sub     rsp, 88
        mov     QWORD PTR [rsp+40], rsi
        mov     rax, QWORD PTR fs:40
        mov     QWORD PTR [rsp+24], rax
        xor     eax, eax
        lea     rax, [rsp+96]
        mov     DWORD PTR [rsp], 8
        mov     QWORD PTR [rsp+8], rax
        lea     rax, [rsp+32]
        mov     QWORD PTR [rsp+16], rax
        mov     eax, DWORD PTR [rsp+40]
        add     eax, edi
        mov     rdx, QWORD PTR [rsp+24]
        sub     rdx, QWORD PTR fs:40
        jne     .L7
        add     rsp, 88
        ret
.L7:
        call    __stack_chk_fail


---


### compiler : `gcc`
### title : `No tree-switch-conversion under PIC`
### open_at : `2021-03-04T10:05:27Z`
### last_modified_date : `2021-09-05T00:29:17Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99383
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
== Input source ==
enum {
itm01, itm02, itm03, itm04, itm05, itm06, itm07, itm08, itm09, itm0A, itm0B,
itm0C, itm0D, itm0E, itm0F, itm10,
};
#define E(s) case s: return #s;
const char *r2i(unsigned int i) {
        switch (i) {
        E(itm01) E(itm02) E(itm03) E(itm04) E(itm05) E(itm06) E(itm07)
        E(itm08) E(itm09) E(itm10)
        }
        return "";
}


== Expected output ==

» g++-11 -v x.cpp -c -O2
» objdump -d x.o
0000000000000000 <_Z3r2ij>:
   0:   b8 00 00 00 00          mov    $0x0,%eax
   5:   83 ff 0f                cmp    $0xf,%edi
   8:   77 0a                   ja     14 <_Z3r2ij+0x14>
   a:   89 ff                   mov    %edi,%edi
   c:   48 8b 04 fd 00 00 00    mov    0x0(,%rdi,8),%rax
  13:   00 
  14:   c3                      ret    

0x0(,%rdi,8) points to a CSWTCH symbol generated by gcc's tree-switch-conversion.c.


== Observed behavior with extra flag ==

When enabling pic/PIC, tree-switch-conversion optimization is completely missed and a lot of lea/ret instruction pairs are generated.

» g++-11 -v x.cpp -c -O2 -fpic
Target: x86_64-suse-linux
gcc version 11.0.0 20210208 (experimental) [revision efcd941e86b507d77e90a1b13f621e036eacdb45] (SUSE Linux) 
...
» objdump -d x.o
…
  20:   48 8d 05 00 00 00 00    lea    0x0(%rip),%rax        # 27 <_Z3r2ij+0x27>
  27:   c3                      ret    
  28:   0f 1f 84 00 00 00 00    nopl   0x0(%rax,%rax,1)
  2f:   00 
  30:   48 8d 05 00 00 00 00    lea    0x0(%rip),%rax        # 37 <_Z3r2ij+0x37>
  37:   c3                      ret    
…


---


### compiler : `gcc`
### title : `Unoptimized tailcall with char and short as parameter (x86)`
### open_at : `2021-03-04T10:15:00Z`
### last_modified_date : `2021-03-04T21:03:11Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99384
### status : `RESOLVED`
### tags : `ABI, missed-optimization`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
#include <cstdint>

int foo(uint8_t x);
int foo(uint32_t x);

struct Y { uint8_t x; };
struct X { uint32_t x; };
int foo(Y x);
int foo(X x);

int d(uint8_t x) { return foo(x); }
int g(uint32_t x) { return foo(x); }
int i(Y x) { return foo(x); }
int j(X x) { return foo(x); }


d(unsigned char):
        movzx   edi, dil           <------- unnecessary mov
        jmp     foo(unsigned char)
g(unsigned int):
        jmp     foo(unsigned int)
i(Y):
        jmp     foo(Y)
j(X):
        jmp     foo(X)


https://godbolt.org/z/K1vYc4


---


### compiler : `gcc`
### title : `std::variant overhead much larger compared to clang`
### open_at : `2021-03-04T12:34:17Z`
### last_modified_date : `2021-09-18T08:57:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99386
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `c++`
### version : `10.2.0`
### severity : `normal`
### contents :
I've come across some code in an application I'm working on that makes use of std::variant. The overhead imposed by std::variant compared to a raw type is extremely high (700% and more). I created a little MWE to show this behavior:

https://github.com/milianw/cpp-variant-overhead

To reproduce, compile two versions in different build folders with both g++ or clang++:

```
CXX=g++ cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo
CXX=clang++ cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo
```

Then run both versions:
```
perf stat -r 5 -d ./variant 0
perf stat -r 5 -d ./variant 1
perf stat -r 5 -d ./variant 2
```

I put the measurements on my machine into the README.md. The gist is, the relative runtime overhead is huge when compiling with g++:

g++
uint64_t: 100%
std::variant<uint64_t>: 720%
std::variant<uint64_t, uint32_t>: 840%

clang++
uint64_t: 100%
std::variant<uint64_t>: 114%
std::variant<uint64_t, uint32_t>: 184%

The baseline for both g++/clang++ is roughly the same.


---


### compiler : `gcc`
### title : `s254 benchmark of TSVC is vectorized by clang and not by gcc`
### open_at : `2021-03-04T22:56:11Z`
### last_modified_date : `2022-10-17T10:39:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99394
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Clang is vectorizing s254 loop with -mtune=archive on znver2 leading to about 758% speedup. Loop is:

real_t s254(struct args_t * func_args)
{

//    scalar and array expansion
//    carry around variable

    initialise_arrays(__func__);
    gettimeofday(&func_args->t1, NULL);

    real_t x;
    for (int nl = 0; nl < 4*iterations; nl++) {
        x = b[LEN_1D-1];
        for (int i = 0; i < LEN_1D; i++) {
            a[i] = (b[i] + x) * (real_t).5;
            x = b[i];
        }
        dummy(a, b, c, d, e, aa, bb, cc, 0.);
    }

    gettimeofday(&func_args->t2, NULL);
    return calc_checksum(__func__);
}

and clang produces:
0000000000407d30 <s254>:
  407d30:       41 56                   push   %r14
  407d32:       53                      push   %rbx
  407d33:       48 83 ec 28             sub    $0x28,%rsp
  407d37:       49 89 fe                mov    %rdi,%r14
  407d3a:       bf 6b e2 42 00          mov    $0x42e26b,%edi
  407d3f:       e8 cc f8 00 00          call   417610 <initialise_arrays>
  407d44:       31 db                   xor    %ebx,%ebx
  407d46:       4c 89 f7                mov    %r14,%rdi
  407d49:       31 f6                   xor    %esi,%esi
  407d4b:       e8 10 93 ff ff          call   401060 <gettimeofday@plt>
  407d50:       c4 62 7d 18 05 af 62    vbroadcastss 0x262af(%rip),%ymm8        # 42e008 <_IO_stdin_used+0x8>
  407d57:       02 00 
  407d59:       c5 7c 11 04 24          vmovups %ymm8,(%rsp)
  407d5e:       66 90                   xchg   %ax,%ax
  407d60:       48 c7 c0 00 0c fe ff    mov    $0xfffffffffffe0c00,%rax
  407d67:       c4 e2 7d 18 05 8c a7    vbroadcastss 0x4a78c(%rip),%ymm0        # 4524fc <b+0x1f3fc>
  407d6e:       04 00 
  407d70:       c5 fc 28 88 00 25 45    vmovaps 0x452500(%rax),%ymm1
  407d77:       00 
  407d78:       c5 fc 28 90 20 25 45    vmovaps 0x452520(%rax),%ymm2
  407d7f:       00 
  407d80:       c5 fc 28 98 40 25 45    vmovaps 0x452540(%rax),%ymm3
  407d87:       00 
  407d88:       c4 e3 7d 06 c1 21       vperm2f128 $0x21,%ymm1,%ymm0,%ymm0
  407d8e:       c5 fc 28 a0 60 25 45    vmovaps 0x452560(%rax),%ymm4
  407d95:       00 
  407d96:       c5 fc c6 c1 03          vshufps $0x3,%ymm1,%ymm0,%ymm0
  407d9b:       c5 fc c6 c1 98          vshufps $0x98,%ymm1,%ymm0,%ymm0
  407da0:       c4 e3 75 06 ea 21       vperm2f128 $0x21,%ymm2,%ymm1,%ymm5
  407da6:       c5 d4 c6 ea 03          vshufps $0x3,%ymm2,%ymm5,%ymm5
  407dab:       c5 d4 c6 ea 98          vshufps $0x98,%ymm2,%ymm5,%ymm5
  407db0:       c4 e3 6d 06 f3 21       vperm2f128 $0x21,%ymm3,%ymm2,%ymm6
  407db6:       c5 cc c6 f3 03          vshufps $0x3,%ymm3,%ymm6,%ymm6
  407dbb:       c5 cc c6 f3 98          vshufps $0x98,%ymm3,%ymm6,%ymm6
  407dc0:       c4 e3 65 06 fc 21       vperm2f128 $0x21,%ymm4,%ymm3,%ymm7
  407dc6:       c5 c4 c6 fc 03          vshufps $0x3,%ymm4,%ymm7,%ymm7
  407dcb:       c5 c4 c6 fc 98          vshufps $0x98,%ymm4,%ymm7,%ymm7
  407dd0:       c5 f4 58 c0             vaddps %ymm0,%ymm1,%ymm0
  407dd4:       c5 ec 58 cd             vaddps %ymm5,%ymm2,%ymm1
  407dd8:       c5 e4 58 d6             vaddps %ymm6,%ymm3,%ymm2
  407ddc:       c5 dc 58 df             vaddps %ymm7,%ymm4,%ymm3
  407de0:       c5 bc 59 c0             vmulps %ymm0,%ymm8,%ymm0
  407de4:       c5 bc 59 c9             vmulps %ymm1,%ymm8,%ymm1
  407de8:       c5 bc 59 d2             vmulps %ymm2,%ymm8,%ymm2
  407dec:       c5 bc 59 db             vmulps %ymm3,%ymm8,%ymm3
  407df0:       c5 fc 29 80 00 19 47    vmovaps %ymm0,0x471900(%rax)
  407df7:       00 
  407df8:       c5 fc 29 88 20 19 47    vmovaps %ymm1,0x471920(%rax)
  407dff:       00 
  407e00:       c5 fc 29 90 40 19 47    vmovaps %ymm2,0x471940(%rax)
  407e07:       00 
  407e08:       c5 fc 29 98 60 19 47    vmovaps %ymm3,0x471960(%rax)
  407e0f:       00 
  407e10:       c5 fc 28 c4             vmovaps %ymm4,%ymm0
  407e14:       48 83 e8 80             sub    $0xffffffffffffff80,%rax
  407e18:       0f 85 52 ff ff ff       jne    407d70 <s254+0x40>
  407e1e:       bf 00 25 45 00          mov    $0x452500,%edi
  407e23:       be 00 31 43 00          mov    $0x433100,%esi
  407e28:       ba 00 19 47 00          mov    $0x471900,%edx
  407e2d:       b9 00 0d 49 00          mov    $0x490d00,%ecx
  407e32:       41 b8 00 01 4b 00       mov    $0x4b0100,%r8d
  407e38:       41 b9 00 f5 4c 00       mov    $0x4cf500,%r9d
  407e3e:       c5 f8 57 c0             vxorps %xmm0,%xmm0,%xmm0
  407e42:       68 00 f5 54 00          push   $0x54f500
  407e47:       68 00 f5 50 00          push   $0x50f500
  407e4c:       c5 f8 77                vzeroupper 
  407e4f:       e8 6c db 00 00          call   4159c0 <dummy>
  407e54:       c5 7c 10 44 24 10       vmovups 0x10(%rsp),%ymm8
  407e5a:       48 83 c4 10             add    $0x10,%rsp
  407e5e:       83 c3 01                add    $0x1,%ebx
  407e61:       81 fb 80 1a 06 00       cmp    $0x61a80,%ebx
  407e67:       0f 85 f3 fe ff ff       jne    407d60 <s254+0x30>
  407e6d:       49 83 c6 10             add    $0x10,%r14
  407e71:       4c 89 f7                mov    %r14,%rdi
  407e74:       31 f6                   xor    %esi,%esi
  407e76:       c5 f8 77                vzeroupper 
  407e79:       e8 e2 91 ff ff          call   401060 <gettimeofday@plt>
  407e7e:       bf 6b e2 42 00          mov    $0x42e26b,%edi
  407e83:       48 83 c4 28             add    $0x28,%rsp
  407e87:       5b                      pop    %rbx
  407e88:       41 5e                   pop    %r14
  407e8a:       e9 71 f1 01 00          jmp    427000 <calc_checksum>
  407e8f:       90                      nop


---


### compiler : `gcc`
### title : `s116 benchmark of TSVC is vectorized by clang and not by gcc`
### open_at : `2021-03-04T23:01:14Z`
### last_modified_date : `2023-10-09T06:30:52Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99395
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
s116 loop is:

real_t s116(struct args_t * func_args)
{

//    linear dependence testing

    initialise_arrays(__func__);
    gettimeofday(&func_args->t1, NULL);

    for (int nl = 0; nl < iterations*10; nl++) {
        for (int i = 0; i < LEN_1D - 5; i += 5) {
            a[i] = a[i + 1] * a[i];
            a[i + 1] = a[i + 2] * a[i + 1];
            a[i + 2] = a[i + 3] * a[i + 2];
            a[i + 3] = a[i + 4] * a[i + 3];
            a[i + 4] = a[i + 5] * a[i + 4];
        }
        dummy(a, b, c, d, e, aa, bb, cc, 0.);
    }

    gettimeofday(&func_args->t2, NULL);
    return calc_checksum(__func__);
}

and vectorized code produced by clang11 is about 2 times faster on zen3 machine

0000000000401d00 <s116>:
  401d00:       41 56                   push   %r14
  401d02:       53                      push   %rbx
  401d03:       50                      push   %rax
  401d04:       49 89 fe                mov    %rdi,%r14
  401d07:       bf 66 e1 42 00          mov    $0x42e166,%edi
  401d0c:       e8 ff 58 01 00          call   417610 <initialise_arrays>
  401d11:       31 db                   xor    %ebx,%ebx
  401d13:       4c 89 f7                mov    %r14,%rdi
  401d16:       31 f6                   xor    %esi,%esi
  401d18:       e8 43 f3 ff ff          call   401060 <gettimeofday@plt>
  401d1d:       eb 47                   jmp    401d66 <s116+0x66>
  401d1f:       90                      nop
  401d20:       bf 00 25 45 00          mov    $0x452500,%edi
  401d25:       be 00 31 43 00          mov    $0x433100,%esi
  401d2a:       ba 00 19 47 00          mov    $0x471900,%edx
  401d2f:       b9 00 0d 49 00          mov    $0x490d00,%ecx
  401d34:       41 b8 00 01 4b 00       mov    $0x4b0100,%r8d
  401d3a:       41 b9 00 f5 4c 00       mov    $0x4cf500,%r9d
  401d40:       c5 f8 57 c0             vxorps %xmm0,%xmm0,%xmm0
  401d44:       68 00 f5 54 00          push   $0x54f500
  401d49:       68 00 f5 50 00          push   $0x50f500
  401d4e:       e8 6d 3c 01 00          call   4159c0 <dummy>
  401d53:       48 83 c4 10             add    $0x10,%rsp
  401d57:       83 c3 01                add    $0x1,%ebx
  401d5a:       81 fb 40 42 0f 00       cmp    $0xf4240,%ebx
  401d60:       0f 84 9a 00 00 00       je     401e00 <s116+0x100>
  401d66:       c5 fa 10 05 92 07 05    vmovss 0x50792(%rip),%xmm0        # 452500 <a>
  401d6d:       00 
  401d6e:       31 c0                   xor    %eax,%eax
  401d70:       c5 fa 10 0c 85 04 25    vmovss 0x452504(,%rax,4),%xmm1
  401d77:       45 00 
  401d79:       c5 fa 59 c1             vmulss %xmm1,%xmm0,%xmm0
  401d7d:       c5 fa 11 04 85 00 25    vmovss %xmm0,0x452500(,%rax,4)
  401d84:       45 00 
  401d86:       c5 f8 10 04 85 08 25    vmovups 0x452508(,%rax,4),%xmm0
  401d8d:       45 00 
  401d8f:       c5 f0 c6 c8 00          vshufps $0x0,%xmm0,%xmm1,%xmm1
  401d94:       c5 f0 c6 c8 98          vshufps $0x98,%xmm0,%xmm1,%xmm1
  401d99:       c5 f8 59 c9             vmulps %xmm1,%xmm0,%xmm1
  401d9d:       c5 f8 11 0c 85 04 25    vmovups %xmm1,0x452504(,%rax,4)
  401da4:       45 00 
  401da6:       48 3d f5 7c 00 00       cmp    $0x7cf5,%rax
  401dac:       0f 87 6e ff ff ff       ja     401d20 <s116+0x20>
  401db2:       c4 e3 79 04 c0 e7       vpermilps $0xe7,%xmm0,%xmm0
  401db8:       c5 fa 10 0c 85 18 25    vmovss 0x452518(,%rax,4),%xmm1
  401dbf:       45 00 
  401dc1:       c5 fa 59 c1             vmulss %xmm1,%xmm0,%xmm0
  401dc5:       c5 fa 11 04 85 14 25    vmovss %xmm0,0x452514(,%rax,4)
  401dcc:       45 00 
  401dce:       c5 f8 10 04 85 1c 25    vmovups 0x45251c(,%rax,4),%xmm0
  401dd5:       45 00 
  401dd7:       c5 f0 c6 c8 00          vshufps $0x0,%xmm0,%xmm1,%xmm1
  401ddc:       c5 f0 c6 c8 98          vshufps $0x98,%xmm0,%xmm1,%xmm1
  401de1:       c5 f8 59 c9             vmulps %xmm1,%xmm0,%xmm1
  401de5:       c5 fa 10 04 85 28 25    vmovss 0x452528(,%rax,4),%xmm0
  401dec:       45 00 
  401dee:       c5 f8 11 0c 85 18 25    vmovups %xmm1,0x452518(,%rax,4)
  401df5:       45 00 
  401df7:       48 83 c0 0a             add    $0xa,%rax
  401dfb:       e9 70 ff ff ff          jmp    401d70 <s116+0x70>
  401e00:       49 83 c6 10             add    $0x10,%r14
  401e04:       4c 89 f7                mov    %r14,%rdi
  401e07:       31 f6                   xor    %esi,%esi
  401e09:       e8 52 f2 ff ff          call   401060 <gettimeofday@plt>
  401e0e:       bf 66 e1 42 00          mov    $0x42e166,%edi
  401e13:       48 83 c4 08             add    $0x8,%rsp
  401e17:       5b                      pop    %rbx
  401e18:       41 5e                   pop    %r14
  401e1a:       e9 e1 51 02 00          jmp    427000 <calc_checksum>
  401e1f:       90                      nop


---


### compiler : `gcc`
### title : `std::rotl and std::rotr Does not convert into ROTATE on the gimple level`
### open_at : `2021-03-04T23:09:56Z`
### last_modified_date : `2021-03-06T10:13:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99396
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
https://godbolt.org/z/snG9fs


---


### compiler : `gcc`
### title : `s152 benchmark of TSVC is vectorized by clang and not by gcc`
### open_at : `2021-03-04T23:18:36Z`
### last_modified_date : `2021-03-05T12:50:05Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99397
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
s152 is:
void s152s(real_t a[LEN_1D], real_t b[LEN_1D], real_t c[LEN_1D], int i)
{
    a[i] += b[i] * c[i];
}

real_t s152(struct args_t * func_args)
{

//    interprocedural data flow analysis
//    collecting information from a subroutine

    initialise_arrays(__func__);
    gettimeofday(&func_args->t1, NULL);

    for (int nl = 0; nl < iterations; nl++) {
        for (int i = 0; i < LEN_1D; i++) {
            b[i] = d[i] * e[i];
            s152s(a, b, c, i);
        }
        dummy(a, b, c, d, e, aa, bb, cc, 0.);
    }

    gettimeofday(&func_args->t2, NULL);
    return calc_checksum(__func__);
}


and clang11 vectorizes it as:
00000000004048b0 <s152>:
  4048b0:       41 56                   push   %r14
  4048b2:       53                      push   %rbx
  4048b3:       50                      push   %rax
  4048b4:       49 89 fe                mov    %rdi,%r14
  4048b7:       bf b7 e1 42 00          mov    $0x42e1b7,%edi
  4048bc:       e8 4f 2d 01 00          call   417610 <initialise_arrays>
  4048c1:       31 db                   xor    %ebx,%ebx
  4048c3:       4c 89 f7                mov    %r14,%rdi
  4048c6:       31 f6                   xor    %esi,%esi
  4048c8:       e8 93 c7 ff ff          call   401060 <gettimeofday@plt>
  4048cd:       0f 1f 00                nopl   (%rax)
  4048d0:       31 c0                   xor    %eax,%eax
  4048d2:       66 2e 0f 1f 84 00 00    cs nopw 0x0(%rax,%rax,1)
  4048d9:       00 00 00
  4048dc:       0f 1f 40 00             nopl   0x0(%rax)
  4048e0:       c5 fc 28 80 00 01 4b    vmovaps 0x4b0100(%rax),%ymm0
  4048e7:       00
  4048e8:       c5 fc 28 88 20 01 4b    vmovaps 0x4b0120(%rax),%ymm1
  4048ef:       00
  4048f0:       c5 fc 59 80 00 0d 49    vmulps 0x490d00(%rax),%ymm0,%ymm0
  4048f7:       00
  4048f8:       c5 f4 59 88 20 0d 49    vmulps 0x490d20(%rax),%ymm1,%ymm1
  4048ff:       00
  404900:       c5 fc 29 80 00 31 43    vmovaps %ymm0,0x433100(%rax)
  404907:       00
  404908:       c5 fc 29 88 20 31 43    vmovaps %ymm1,0x433120(%rax)
  40490f:       00
  404910:       c5 fc 28 90 00 19 47    vmovaps 0x471900(%rax),%ymm2
  404917:       00
  404918:       c5 fc 28 98 20 19 47    vmovaps 0x471920(%rax),%ymm3
  40491f:       00
  404920:       c4 e2 7d a8 90 00 25    vfmadd213ps 0x452500(%rax),%ymm0,%ymm2
  404927:       45 00
  404929:       c4 e2 75 a8 98 20 25    vfmadd213ps 0x452520(%rax),%ymm1,%ymm3
  404930:       45 00
  404932:       c5 fc 29 90 00 25 45    vmovaps %ymm2,0x452500(%rax)
  404939:       00
  40493a:       c5 fc 29 98 20 25 45    vmovaps %ymm3,0x452520(%rax)
  404941:       00
  404942:       48 83 c0 40             add    $0x40,%rax
  404946:       48 3d 00 f4 01 00       cmp    $0x1f400,%rax
  40494c:       75 92                   jne    4048e0 <s152+0x30>
  40494e:       bf 00 25 45 00          mov    $0x452500,%edi
  404953:       be 00 31 43 00          mov    $0x433100,%esi
  404958:       ba 00 19 47 00          mov    $0x471900,%edx
  40495d:       b9 00 0d 49 00          mov    $0x490d00,%ecx
  404962:       41 b8 00 01 4b 00       mov    $0x4b0100,%r8d
  404968:       41 b9 00 f5 4c 00       mov    $0x4cf500,%r9d
  40496e:       c5 f8 57 c0             vxorps %xmm0,%xmm0,%xmm0
  404972:       68 00 f5 54 00          push   $0x54f500
  404977:       68 00 f5 50 00          push   $0x50f500
  40497c:       c5 f8 77                vzeroupper 
  40497f:       e8 3c 10 01 00          call   4159c0 <dummy>
  404984:       48 83 c4 10             add    $0x10,%rsp
  404988:       83 c3 01                add    $0x1,%ebx
  40498b:       81 fb a0 86 01 00       cmp    $0x186a0,%ebx
  404991:       0f 85 39 ff ff ff       jne    4048d0 <s152+0x20>
  404997:       49 83 c6 10             add    $0x10,%r14
  40499b:       4c 89 f7                mov    %r14,%rdi
  40499e:       31 f6                   xor    %esi,%esi
  4049a0:       e8 bb c6 ff ff          call   401060 <gettimeofday@plt>
  4049a5:       bf b7 e1 42 00          mov    $0x42e1b7,%edi
  4049aa:       48 83 c4 08             add    $0x8,%rsp
  4049ae:       5b                      pop    %rbx
  4049af:       41 5e                   pop    %r14
  4049b1:       e9 4a 26 02 00          jmp    427000 <calc_checksum>
  4049b6:       66 2e 0f 1f 84 00 00    cs nopw 0x0(%rax,%rax,1)
  4049bd:       00 00 00 


We get:
real_t s152 (struct args_t * func_args)
{
  int i;
  int nl;
  static const char __func__[5] = "s152";
  struct timeval * _1;
  float _2;
  float _3;
  float _4;
  struct timeval * _5;
  real_t _16;
  long unsigned int _21;
  long unsigned int _22;
  real_t * _23;
  float _24;
  real_t * _25;
  float _26;
  real_t * _27;
  float _28;
  float _29;
  float _30;
  unsigned int ivtmp_48;
  unsigned int ivtmp_49;
  unsigned int ivtmp_50;
  unsigned int ivtmp_51;

  <bb 2> [local count: 108459]:
  initialise_arrays (&__func__);
  _1 = &func_args_12(D)->t1;
  gettimeofday (_1, 0B);
  goto <bb 5>; [100.00%]

  <bb 8> [local count: 1052266996]:

  <bb 3> [local count: 1063004409]:
  # i_40 = PHI <i_20(8), 0(5)>
  # ivtmp_51 = PHI <ivtmp_50(8), 32000(5)>
  _2 = d[i_40];
  _3 = e[i_40];
  _4 = _2 * _3;
  b[i_40] = _4;
  _21 = (long unsigned int) i_40;
  _22 = _21 * 4;
  _23 = &a + _22;
  _24 = *_23;
  _25 = &b + _22;
  _26 = *_25;
  _27 = &c + _22;
  _28 = *_27;
  _29 = _26 * _28;
  _30 = _24 + _29;
  *_23 = _30;
  i_20 = i_40 + 1;
  ivtmp_50 = ivtmp_51 - 1;
  if (ivtmp_50 != 0)
    goto <bb 8>; [98.99%]
  else
    goto <bb 4>; [1.01%]

  <bb 4> [local count: 10737416]:
  dummy (&a, &b, &c, &d, &e, &aa, &bb, &cc, 0.0);
  nl_18 = nl_39 + 1;
  ivtmp_48 = ivtmp_49 - 1;
  if (ivtmp_48 != 0)
    goto <bb 7>; [98.99%]
  else
    goto <bb 6>; [1.01%]

  <bb 7> [local count: 10628957]:

  <bb 5> [local count: 10737416]:
  # nl_39 = PHI <nl_18(7), 0(2)>
  # ivtmp_49 = PHI <ivtmp_48(7), 100000(2)>
  goto <bb 3>; [100.00%]

  <bb 6> [local count: 108459]:
  _5 = &func_args_12(D)->t2;
  gettimeofday (_5, 0B);
  _16 = calc_checksum (&__func__);
  return _16;
}

and fail at:
tsvc.c:699:27: note:   can tell at compile time that b[i_40] and *_25 alias
tsvc.c:686:14: missed:   not vectorized: compilation time alias: b[i_40] = _4;
_26 = *_25;
tsvc.c:699:27: note:  ***** Analysis failed with vector mode V8QI
tsvc.c:699:27: missed: couldn't vectorize loop
tsvc.c:689:8: note: vectorized 0 loops in function.

So I seem we get confused by:
  b[i_40] = _4;

and
  _21 = (long unsigned int) i_40;
  _22 = _21 * 4;
  _25 = &b + _22;

acessing same location...


---


### compiler : `gcc`
### title : `Miss to optimize vector permutation fed by CTOR and CTOR/CST`
### open_at : `2021-03-05T06:53:01Z`
### last_modified_date : `2021-09-17T06:38:46Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99398
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
#include "altivec.h"

vector long long foo(long long a, long long b) {
  vector long long v1 = {a, 0};
  vector long long v2 = {b, 0};
  vector unsigned char vc = {0,1,2,3,4,5,6,7, 16,17,18,19,20,21,22,23};
  vector long long vres = (vector long long)vec_perm ((vector unsigned char)v1, (vector unsigned char)v2, vc);
  return vres;
}

gcc -Ofast -mcpu=power9, it generates (asm on BE btw)

	mtvsrdd 32,3,9
	mtvsrdd 33,4,9
	lxv 34,0(10)
	vperm 2,0,1,2
	blr

But it can be optimized into:

	mtvsrdd 34,3,4
	blr

The gimple at optimized dumping looks like:

__vector long foo (long long int a, long long int b)
{
  __vector long vres;
  __vector long v2;
  __vector long v1;
  __vector unsigned char _5;
  __vector unsigned char _6;
  __vector unsigned char _7;

  <bb 2> [local count: 1073741824]:
  v1_2 = {a_1(D), 0};
  v2_4 = {b_3(D), 0};
  _5 = VIEW_CONVERT_EXPR<__vector unsigned char>(v1_2);
  _6 = VIEW_CONVERT_EXPR<__vector unsigned char>(v2_4);
  _7 = VEC_PERM_EXPR <_5, _6, { 0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23 }>;
  vres_8 = VIEW_CONVERT_EXPR<__vector long>(_7);
  return vres_8;

}

But it can look like:

__vector long foo (long long int a, long long int b)
{
  vector(2) long long int _10;

  <bb 2> [local count: 1073741824]:
  _10 = {a_1(D), b_3(D)};
  return _10;

}


---


### compiler : `gcc`
### title : `Rotate with mask not optimized on x86 for QI/HImode rotates`
### open_at : `2021-03-05T12:38:03Z`
### last_modified_date : `2022-05-27T18:43:09Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99405
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
+++ This bug was initially created as a clone of Bug #99396 +++

In
unsigned char f1 (unsigned char x, unsigned y) { return (x << (y & 7)) | (x >> (-y & 7)); }
unsigned short f2 (unsigned short x, unsigned y) { return (x << (y & 15)) | (x >> (-y & 15)); }
unsigned int f3 (unsigned int x, unsigned y) { return (x << (y & 31)) | (x >> (-y & 31)); }
unsigned char f4 (unsigned char x, unsigned y) { return (x >> (y & 7)) | (x << (-y & 7)); }
unsigned short f5 (unsigned short x, unsigned y) { return (x >> (y & 15)) | (x << (-y & 15)); }
unsigned int f6 (unsigned int x, unsigned y) { return (x >> (y & 31)) | (x << (-y & 31)); }
unsigned char f7 (unsigned char x, unsigned char y) { unsigned char v = y & 7; unsigned char w = -y & 7; return (x << v) | (x >> w); }
unsigned short f8 (unsigned short x, unsigned char y) { unsigned char v = y & 15; unsigned char w = -y & 15; return (x << v) | (x >> w); }
unsigned int f9 (unsigned int x, unsigned char y) { unsigned char v = y & 31; unsigned char w = -y & 31; return (x << v) | (x >> w); }
unsigned char f10 (unsigned char x, unsigned char y) { unsigned char v = y & 7; unsigned char w = -y & 7; return (x >> v) | (x << w); }
unsigned short f11 (unsigned short x, unsigned char y) { unsigned char v = y & 15; unsigned char w = -y & 15; return (x >> v) | (x << w); }
unsigned int f12 (unsigned int x, unsigned char y) { unsigned char v = y & 31; unsigned char w = -y & 31; return (x >> v) | (x << w); }
#ifdef __x86_64__
unsigned long long f13 (unsigned long long x, unsigned y) { return (x << (y & 63)) | (x >> (-y & 63)); }
unsigned long long f14 (unsigned long long x, unsigned y) { return (x >> (y & 63)) | (x << (-y & 63)); }
unsigned long long f15 (unsigned long long x, unsigned char y) { unsigned char v = y & 63; unsigned char w = -y & 63; return (x << v) | (x >> w); }
unsigned long long f16 (unsigned long long x, unsigned char y) { unsigned char v = y & 63; unsigned char w = -y & 63; return (x >> v) | (x << w); }
#endif

we don't optimize away the and instructions in f{1,2,4,5,7,8,10,11}.


---


### compiler : `gcc`
### title : `s243 benchmark of TSVC is vectorized by clang and not by gcc, missed DSE`
### open_at : `2021-03-05T14:01:56Z`
### last_modified_date : `2022-09-22T15:56:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99407
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
This testcase (from TSVC) is about 4 times faster on zen3 when built with clang.

typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
// array definitions
real_t flat_2d_array[LEN_2D*LEN_2D];

real_t x[LEN_1D];

real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D],
bb[LEN_2D][LEN_2D],cc[LEN_2D][LEN_2D],tt[LEN_2D][LEN_2D];

int indx[LEN_1D];

real_t* __restrict__ xx;
real_t* yy;
real_t s243(void)
{

//    node splitting
//    false dependence cycle breaking

    for (int nl = 0; nl < iterations; nl++) {
        for (int i = 0; i < LEN_1D-1; i++) {
            a[i] = b[i] + c[i  ] * d[i];
            b[i] = a[i] + d[i  ] * e[i];
            a[i] = b[i] + a[i+1] * d[i];
        }
    }
}

internal loop from clang is:
.LBB0_2:                                #   Parent Loop BB0_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
        vmovups c(%rcx), %ymm12
        vmovups c+32(%rcx), %ymm14
        vmovups d(%rcx), %ymm0
        vmovups d+32(%rcx), %ymm7
        vfmadd213ps     b(%rcx), %ymm0, %ymm12  # ymm12 = (ymm0 * ymm12) + mem
        vfmadd213ps     b+32(%rcx), %ymm7, %ymm14 # ymm14 = (ymm7 * ymm14) + mem
        vfmadd231ps     e(%rcx), %ymm0, %ymm12  # ymm12 = (ymm0 * mem) + ymm12
        vfmadd231ps     e+32(%rcx), %ymm7, %ymm14 # ymm14 = (ymm7 * mem) + ymm14
        vmovups %ymm12, b(%rcx)
        vmovups %ymm14, b+32(%rcx)
        vfmadd231ps     a+4(%rcx), %ymm0, %ymm12 # ymm12 = (ymm0 * mem) + ymm12
        vfmadd231ps     a+36(%rcx), %ymm7, %ymm14 # ymm14 = (ymm7 * mem) + ymm14
        vmovups %ymm12, a(%rcx)
        vmovups %ymm14, a+32(%rcx)
        addq    $64, %rcx
        cmpq    $127936, %rcx                   # imm = 0x1F3C0
        jne     .LBB0_2


---


### compiler : `gcc`
### title : `s3251 benchmark of TSVC vectorized by clang runs about 7 times faster compared to gcc`
### open_at : `2021-03-05T14:11:48Z`
### last_modified_date : `2023-01-14T22:33:07Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99408
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef float real_t;
#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D];
void
main(void)
{
    for (int nl = 0; nl < iterations; nl++) {
        for (int i = 0; i < LEN_1D-1; i++){
            a[i+1] = b[i]+c[i];
            b[i]   = c[i]*e[i];
            d[i]   = a[i]*e[i];
        }
    }
}

Built with -march=znver2 -Ofast I get:
main:
.LFB0:
        .cfi_startproc
        vmovaps c+127968(%rip), %xmm5
        vmovaps e+127968(%rip), %xmm4
        movl    $100000, %edx
        vmovq   c+127984(%rip), %xmm9
        vmovq   e+127984(%rip), %xmm10
        vmovss  c+127992(%rip), %xmm7
        vmovss  e+127992(%rip), %xmm3
        vmovss  c+127984(%rip), %xmm13
        vmulps  %xmm4, %xmm5, %xmm6
        vmulps  %xmm9, %xmm10, %xmm12
        vmulss  %xmm3, %xmm7, %xmm11
        .p2align 4
        .p2align 3
.L2:
        xorl    %eax, %eax
        .p2align 4
        .p2align 3
.L4:
        vmovaps c(%rax), %ymm2
        addq    $32, %rax
        vaddps  b-32(%rax), %ymm2, %ymm0
        vmovups %ymm0, a-28(%rax)
        vmulps  e-32(%rax), %ymm2, %ymm0
        vmovaps e-32(%rax), %ymm2
        vmovaps %ymm0, b-32(%rax)
        vmulps  a-32(%rax), %ymm2, %ymm0
        vmovaps %ymm0, d-32(%rax)
        cmpq    $127968, %rax
        jne     .L4
        vaddps  b+127968(%rip), %xmm5, %xmm1
        vaddss  b+127984(%rip), %xmm13, %xmm2
        decl    %edx
        vmovaps %xmm6, b+127968(%rip)
        vmovq   b+127984(%rip), %xmm0
        vmovlps %xmm12, b+127984(%rip)
        vaddps  %xmm0, %xmm9, %xmm0
        vmovups %xmm1, a+127972(%rip)
        vshufps $255, %xmm1, %xmm1, %xmm1
        vmulps  a+127968(%rip), %xmm4, %xmm8
        vunpcklps       %xmm2, %xmm1, %xmm1
        vaddss  b+127992(%rip), %xmm7, %xmm2
        vmovss  %xmm11, b+127992(%rip)
        vmulps  %xmm10, %xmm1, %xmm1
        vmovlps %xmm0, a+127988(%rip)
        vmovshdup       %xmm0, %xmm0
        vmulss  %xmm3, %xmm0, %xmm0
        vmovss  %xmm2, a+127996(%rip)
        jne     .L2
        vmovaps %xmm8, d+127968(%rip)
        vmovlps %xmm1, d+127984(%rip)
        vmovss  %xmm0, d+127992(%rip)
        vzeroupper
        ret


Clang does:

main:                                   # @main
        .cfi_startproc
# %bb.0:
        vbroadcastss    a(%rip), %ymm0
        vmovss  e+127968(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
        vmovss  e+127980(%rip), %xmm2           # xmm2 = mem[0],zero,zero,zero
        vmovss  c+127984(%rip), %xmm4           # xmm4 = mem[0],zero,zero,zero
        vmovss  e+127984(%rip), %xmm5           # xmm5 = mem[0],zero,zero,zero
        vmovss  c+127988(%rip), %xmm8           # xmm8 = mem[0],zero,zero,zero
        vmovss  e+127988(%rip), %xmm9           # xmm9 = mem[0],zero,zero,zero
        vmovss  c+127992(%rip), %xmm11          # xmm11 = mem[0],zero,zero,zero
        vmovss  e+127992(%rip), %xmm12          # xmm12 = mem[0],zero,zero,zero
        xorl    %eax, %eax
        vmovups %ymm0, -56(%rsp)                # 32-byte Spill
        vmovss  c+127968(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
        vmovss  %xmm1, -64(%rsp)                # 4-byte Spill
        vmulss  %xmm4, %xmm5, %xmm3
        vmulss  %xmm8, %xmm9, %xmm10
        vmulss  %xmm11, %xmm12, %xmm13
        vmovss  %xmm0, -60(%rsp)                # 4-byte Spill
        vmulss  %xmm0, %xmm1, %xmm0
        vmovss  e+127972(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
        vmovss  %xmm0, -68(%rsp)                # 4-byte Spill
        vmovss  c+127972(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
        vmovss  %xmm1, -76(%rsp)                # 4-byte Spill
        vmovss  %xmm0, -72(%rsp)                # 4-byte Spill
        vmulss  %xmm0, %xmm1, %xmm0
        vmovss  e+127976(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
        vmovss  %xmm0, -80(%rsp)                # 4-byte Spill
        vmovss  c+127976(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
        vmovss  %xmm1, -88(%rsp)                # 4-byte Spill
        vmovss  %xmm0, -84(%rsp)                # 4-byte Spill
        vmulss  %xmm0, %xmm1, %xmm0
        vmovss  c+127980(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
        vmovss  %xmm0, -92(%rsp)                # 4-byte Spill
        vmulss  %xmm1, %xmm2, %xmm0
       vmovss  %xmm0, -96(%rsp)                # 4-byte Spill
        .p2align        4, 0x90
.LBB0_1:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_2 Depth 2
        vmovups -56(%rsp), %ymm14               # 32-byte Reload
        xorl    %ecx, %ecx
        .p2align        4, 0x90
.LBB0_2:                                #   Parent Loop BB0_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
        vmovups c(%rcx), %ymm7
        vmovaps %ymm14, %ymm15
        vmovups e(%rcx), %ymm0
        vaddps  b(%rcx), %ymm7, %ymm14
        vmulps  %ymm7, %ymm0, %ymm7
        vperm2f128      $33, %ymm14, %ymm15, %ymm15 # ymm15 = ymm15[2,3],ymm14[0,1]
        vmovups %ymm14, a+4(%rcx)
        vmovups %ymm7, b(%rcx)
        vshufps $3, %ymm14, %ymm15, %ymm15      # ymm15 = ymm15[3,0],ymm14[0,0],ymm15[7,4],ymm14[4,4]
        vshufps $152, %ymm14, %ymm15, %ymm15    # ymm15 = ymm15[0,2],ymm14[1,2],ymm15[4,6],ymm14[5,6]
        vmulps  %ymm0, %ymm15, %ymm0
        vmovups %ymm0, d(%rcx)
        addq    $32, %rcx
        cmpq    $127968, %rcx                   # imm = 0x1F3E0
        jne     .LBB0_2
# %bb.3:                                #   in Loop: Header=BB0_1 Depth=1
        vextractf128    $1, %ymm14, %xmm0
        vmovss  -60(%rsp), %xmm7                # 4-byte Reload
                                        # xmm7 = mem[0],zero,zero,zero
        vmovss  -68(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
        incl    %eax
        vpermilps       $231, %xmm0, %xmm0      # xmm0 = xmm0[3,1,2,3]
        vmulss  -64(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
        vaddss  b+127968(%rip), %xmm7, %xmm7
        vmovss  %xmm6, b+127968(%rip)
        vmovss  -80(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
        vmovss  %xmm0, d+127968(%rip)
        vmovss  -72(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
        vmovss  %xmm7, a+127972(%rip)
       vmulss  -76(%rsp), %xmm7, %xmm7         # 4-byte Folded Reload
        vaddss  b+127972(%rip), %xmm0, %xmm0
        vmovss  %xmm6, b+127972(%rip)
        vmovss  -84(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
        vmovss  %xmm7, d+127972(%rip)
        vaddss  b+127976(%rip), %xmm6, %xmm7
        vmovss  -92(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
        vmovss  %xmm0, a+127976(%rip)
        vmulss  -88(%rsp), %xmm0, %xmm0         # 4-byte Folded Reload
        vmovss  %xmm6, b+127976(%rip)
        vmovss  -96(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
        vmovss  %xmm7, a+127980(%rip)
        vmulss  %xmm2, %xmm7, %xmm7
        vmovss  %xmm0, d+127976(%rip)
        vaddss  b+127980(%rip), %xmm1, %xmm0
        vmovss  %xmm7, d+127980(%rip)
        vmovss  %xmm6, b+127980(%rip)
        vaddss  b+127984(%rip), %xmm4, %xmm7
        vmovss  %xmm3, b+127984(%rip)
        vmovss  %xmm0, a+127984(%rip)
        vmulss  %xmm5, %xmm0, %xmm0
        vmovss  %xmm0, d+127984(%rip)
        vaddss  b+127988(%rip), %xmm8, %xmm0
        vmovss  %xmm10, b+127988(%rip)
        vaddss  b+127992(%rip), %xmm11, %xmm6
        vmovss  %xmm13, b+127992(%rip)
        vmovss  %xmm7, a+127988(%rip)
        vmulss  %xmm7, %xmm9, %xmm7
        vmovss  %xmm7, d+127988(%rip)
        vmovss  %xmm0, a+127992(%rip)
        vmulss  %xmm0, %xmm12, %xmm0
        vmovss  %xmm6, a+127996(%rip)
        vmovss  %xmm0, d+127992(%rip)
        cmpl    $100000, %eax                   # imm = 0x186A0
        jne     .LBB0_1
# %bb.4:
        vzeroupper
        retq

Runtie with clang is 0.443s and GCC 2.317s. With -fno-tree-vectorize I get 2.153s


---


### compiler : `gcc`
### title : `s252 benchmark of TSVC is vectorized by clang and not by gcc`
### open_at : `2021-03-05T14:23:27Z`
### last_modified_date : `2022-10-17T10:40:25Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99409
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;
#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D];

void main()
{

//    scalar and array expansion
//    loop with ambiguous scalar temporary

    real_t t, s;
    for (int nl = 0; nl < iterations; nl++) {
        t = (real_t) 0.;
        for (int i = 0; i < LEN_1D; i++) {
            s = b[i] * c[i];
            a[i] = s + t;
            t = s;
        }
    }

}

clang does:
main:                                   # @main
        .cfi_startproc
# %bb.0:
        xorl    %eax, %eax
        .p2align        4, 0x90
.LBB0_1:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_2 Depth 2
        vxorps  %xmm0, %xmm0, %xmm0
        movq    $-128000, %rcx                  # imm = 0xFFFE0C00
        .p2align        4, 0x90
.LBB0_2:                                #   Parent Loop BB0_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
        vmovups c+128000(%rcx), %ymm1
        vmovups c+128032(%rcx), %ymm2
        vmovups c+128064(%rcx), %ymm3
        vmovups c+128096(%rcx), %ymm4
        vmulps  b+128000(%rcx), %ymm1, %ymm1
        vmulps  b+128032(%rcx), %ymm2, %ymm2
        vmulps  b+128064(%rcx), %ymm3, %ymm3
        vmulps  b+128096(%rcx), %ymm4, %ymm4
        vperm2f128      $33, %ymm1, %ymm0, %ymm0 # ymm0 = ymm0[2,3],ymm1[0,1]
        vperm2f128      $33, %ymm2, %ymm1, %ymm5 # ymm5 = ymm1[2,3],ymm2[0,1]
        vperm2f128      $33, %ymm3, %ymm2, %ymm6 # ymm6 = ymm2[2,3],ymm3[0,1]
        vperm2f128      $33, %ymm4, %ymm3, %ymm7 # ymm7 = ymm3[2,3],ymm4[0,1]
        vshufps $3, %ymm1, %ymm0, %ymm0         # ymm0 = ymm0[3,0],ymm1[0,0],ymm0[7,4],ymm1[4,4]
        vshufps $3, %ymm2, %ymm5, %ymm5         # ymm5 = ymm5[3,0],ymm2[0,0],ymm5[7,4],ymm2[4,4]
        vshufps $3, %ymm3, %ymm6, %ymm6         # ymm6 = ymm6[3,0],ymm3[0,0],ymm6[7,4],ymm3[4,4]
        vshufps $3, %ymm4, %ymm7, %ymm7         # ymm7 = ymm7[3,0],ymm4[0,0],ymm7[7,4],ymm4[4,4]
        vshufps $152, %ymm1, %ymm0, %ymm0       # ymm0 = ymm0[0,2],ymm1[1,2],ymm0[4,6],ymm1[5,6]
        vshufps $152, %ymm2, %ymm5, %ymm5       # ymm5 = ymm5[0,2],ymm2[1,2],ymm5[4,6],ymm2[5,6]
        vshufps $152, %ymm3, %ymm6, %ymm6       # ymm6 = ymm6[0,2],ymm3[1,2],ymm6[4,6],ymm3[5,6]
        vshufps $152, %ymm4, %ymm7, %ymm7       # ymm7 = ymm7[0,2],ymm4[1,2],ymm7[4,6],ymm4[5,6]
        vaddps  %ymm0, %ymm1, %ymm0
        vaddps  %ymm5, %ymm2, %ymm1
        vaddps  %ymm6, %ymm3, %ymm2
        vaddps  %ymm7, %ymm4, %ymm3
        vmovups %ymm0, a+128000(%rcx)
        vmovups %ymm1, a+128032(%rcx)
        vmovups %ymm2, a+128064(%rcx)
        vmovups %ymm3, a+128096(%rcx)
        subq    $-128, %rcx
        vmovaps %ymm4, %ymm0
        jne     .LBB0_2
# %bb.3:                                #   in Loop: Header=BB0_1 Depth=1
        incl    %eax
        cmpl    $100000, %eax                   # imm = 0x186A0
        jne     .LBB0_1
# %bb.4:
        vzeroupper
        retq

s252.c:18:27: note:   worklist: examine stmt: _3 = s_11 + t_21;
s252.c:18:27: note:   vect_is_simple_use: operand _1 * _2, type of def: internal
s252.c:18:27: note:   mark relevant 5, live 0: s_11 = _1 * _2;
s252.c:18:27: note:   vect_is_simple_use: operand t_21 = PHI <s_11(8), 0.0(5)>, type of def: unknown
s252.c:18:27: missed:   Unsupported pattern.
s252.c:20:22: missed:   not vectorized: unsupported use in stmt.
s252.c:18:27: missed:  unexpected pattern.

  <bb 8> [local count: 1052266996]:

  <bb 3> [local count: 1063004409]:
  # t_21 = PHI <s_11(8), 0.0(5)>
  # i_23 = PHI <i_13(8), 0(5)>
  # ivtmp_20 = PHI <ivtmp_19(8), 32000(5)>
  _1 = b[i_23];
  _2 = c[i_23];
  s_11 = _1 * _2;
  _3 = s_11 + t_21;
  a[i_23] = _3;
  i_13 = i_23 + 1;
  ivtmp_19 = ivtmp_20 - 1;
  if (ivtmp_19 != 0)
    goto <bb 8>; [98.99%]
  else
    goto <bb 4>; [1.01%]


---


### compiler : `gcc`
### title : `s311, s312, s31111, s31111, s3110, vsumr benchmark of TSVC is vectorized by clang better than by gcc`
### open_at : `2021-03-05T14:30:55Z`
### last_modified_date : `2023-01-11T22:36:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99411
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
real_t a[LEN_1D];

int main()
{

//    reductions
//    sum reduction

    real_t sum;
    for (int nl = 0; nl < iterations*10; nl++) {
        sum = (real_t)0.;
        for (int i = 0; i < LEN_1D; i++) {
            sum += a[i];
        }
    }
  return sum > 4;
}

We produce with -O2 -march=znver2

.L2:
        movl    $a, %eax
        vxorps  %xmm0, %xmm0, %xmm0
        .p2align 4
        .p2align 3
.L3:
        vaddps  (%rax), %ymm0, %ymm0
        addq    $32, %rax
        cmpq    $a+128000, %rax
        jne     .L3
        vextractf128    $0x1, %ymm0, %xmm1
        decl    %edx
        vaddps  %xmm0, %xmm1, %xmm1
        vmovhlps        %xmm1, %xmm1, %xmm0
        vaddps  %xmm1, %xmm0, %xmm0
        vshufps $85, %xmm0, %xmm0, %xmm1
        vaddps  %xmm0, %xmm1, %xmm0
        jne     .L2
        xorl    %eax, %eax
        vcomiss .LC0(%rip), %xmm0
        seta    %al
        vzeroupper
        ret
        .cfi_endproc


clang does:
main:                                   # @main
        .cfi_startproc
# %bb.0:
        xorl    %eax, %eax
        .p2align        4, 0x90
.LBB0_1:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_2 Depth 2
        vxorps  %xmm0, %xmm0, %xmm0
        movq    $-128000, %rcx                  # imm = 0xFFFE0C00
        vxorps  %xmm1, %xmm1, %xmm1
        vxorps  %xmm2, %xmm2, %xmm2
        vxorps  %xmm3, %xmm3, %xmm3
        .p2align        4, 0x90
.LBB0_2:                                #   Parent Loop BB0_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
        vaddps  a+128000(%rcx), %ymm0, %ymm0
        vaddps  a+128032(%rcx), %ymm1, %ymm1
        vaddps  a+128064(%rcx), %ymm2, %ymm2
        vaddps  a+128096(%rcx), %ymm3, %ymm3
        subq    $-128, %rcx
        jne     .LBB0_2
# %bb.3:                                #   in Loop: Header=BB0_1 Depth=1
        incl    %eax
        cmpl    $1000000, %eax                  # imm = 0xF4240
        jne     .LBB0_1
# %bb.4:
        vaddps  %ymm0, %ymm1, %ymm0
        xorl    %eax, %eax
        vaddps  %ymm0, %ymm2, %ymm0
        vaddps  %ymm0, %ymm3, %ymm0
        vextractf128    $1, %ymm0, %xmm1
        vaddps  %xmm1, %xmm0, %xmm0
        vpermilpd       $1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
        vaddps  %xmm1, %xmm0, %xmm0
        vmovshdup       %xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
        vaddss  %xmm1, %xmm0, %xmm0
        vucomiss        .LCPI0_0(%rip), %xmm0
        seta    %al
        vzeroupper
        retq

On zen3 hardware gcc version runs 2.4s, while clang's 0.8s


---


### compiler : `gcc`
### title : `s352 benchmark of TSVC is vectorized by clang and not by gcc`
### open_at : `2021-03-05T14:54:32Z`
### last_modified_date : `2023-01-12T13:42:36Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99412
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256

real_t a[LEN_1D],b[LEN_1D];
int main ()
{

//    loop rerolling
//    unrolled dot product

    real_t dot;
    for (int nl = 0; nl < 8*iterations; nl++) {
        dot = (real_t)0.;
        for (int i = 0; i < LEN_1D; i += 5) {
            dot = dot + a[i] * b[i] + a[i + 1] * b[i + 1] + a[i + 2]
                * b[i + 2] + a[i + 3] * b[i + 3] + a[i + 4] * b[i + 4];
        }
    }

    return dot;
}


clang does:
main:                                   # @main
        .cfi_startproc
# %bb.0:
        xorl    %eax, %eax
        .p2align        4, 0x90
.LBB0_1:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_2 Depth 2
        vxorps  %xmm0, %xmm0, %xmm0
        movq    $-5, %rcx
        .p2align        4, 0x90
.LBB0_2:                                #   Parent Loop BB0_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
        vmovups b+20(,%rcx,4), %xmm1
        vmovss  b+36(,%rcx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
        vmulps  a+20(,%rcx,4), %xmm1, %xmm1
        vpermilpd       $1, %xmm1, %xmm3        # xmm3 = xmm1[1,0]
        vaddps  %xmm3, %xmm1, %xmm1
        vmovshdup       %xmm1, %xmm3            # xmm3 = xmm1[1,1,3,3]
        vaddss  %xmm3, %xmm1, %xmm1
        vfmadd231ss     a+36(,%rcx,4), %xmm2, %xmm1 # xmm1 = (xmm2 * mem) + xmm1
        addq    $5, %rcx
        vaddss  %xmm0, %xmm1, %xmm0
        cmpq    $31995, %rcx                    # imm = 0x7CFB
        jb      .LBB0_2
# %bb.3:                                #   in Loop: Header=BB0_1 Depth=1
        incl    %eax
        cmpl    $800000, %eax                   # imm = 0xC3500
        jne     .LBB0_1
# %bb.4:
        vcvttss2si      %xmm0, %eax
        retq


---


### compiler : `gcc`
### title : `s235, s2233, s275, s2275 and s233 benchmarks of TSVC is vectorized better by icc than gcc (loop interchange)`
### open_at : `2021-03-05T15:40:20Z`
### last_modified_date : `2021-03-18T08:26:15Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99414
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;
#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D],
aa[LEN_2D][LEN_2D],bb[LEN_2D][LEN_2D],cc[LEN_2D][LEN_2D],tt[LEN_2D][LEN_2D];
// %2.3
real_t main(struct args_t * func_args)
{

//    loop interchanging
//    imperfectly nested loops

    for (int nl = 0; nl < 200*(iterations/LEN_2D); nl++) {
        for (int i = 0; i < LEN_2D; i++) {
            a[i] += b[i] * c[i];
            for (int j = 1; j < LEN_2D; j++) {
                aa[j][i] = aa[j-1][i] + bb[j][i] * a[i];
            }
        }
    }
}


runs about 10 times faster on zen3 built by icc -O3 -ip -Ofast -g -march=core-avx2 -mtune=core-avx2 -vec s235.c

main:
# parameter 1: %rdi
..B1.1:                         # Preds ..B1.0
                                # Execution count [1.77e+00]
        .cfi_startproc
..___tag_value_main.1:
..L2:
                                                          #9.1
        pushq     %rbp                                          #9.1
        .cfi_def_cfa_offset 16
        movq      %rsp, %rbp                                    #9.1
        .cfi_def_cfa 6, 16
        .cfi_offset 6, -16
        andq      $-128, %rsp                                   #9.1
        subq      $128, %rsp                                    #9.1
        movl      $3, %edi                                      #9.1
        xorl      %esi, %esi                                    #9.1
        call      __intel_new_feature_proc_init                 #9.1
                                # LOE rbx r12 r13 r14 r15
..B1.12:                        # Preds ..B1.1
                                # Execution count [1.77e+00]
        vstmxcsr  (%rsp)                                        #9.1
        xorl      %eax, %eax                                    #14.5
        orl       $32832, (%rsp)                                #9.1
        vldmxcsr  (%rsp)                                        #9.1
                                # LOE rbx r12 r13 r14 r15 eax
..B1.2:                         # Preds ..B1.8 ..B1.12
                                # Execution count [7.83e+04]
        xorl      %edx, %edx                                    #15.9
                                # LOE rdx rbx r12 r13 r14 r15 eax
..B1.3:                         # Preds ..B1.3 ..B1.2
                                # Execution count [2.00e+07]
        vmovups   b(,%rdx,4), %ymm1                             #16.21
        lea       (,%rdx,4), %rcx                               #16.13
        vmovups   32+b(,%rdx,4), %ymm3                          #16.21
        vmovups   64+b(,%rdx,4), %ymm5                          #16.21
        vmovups   96+b(,%rdx,4), %ymm7                          #16.21
        vmovups   128+b(,%rdx,4), %ymm9                         #16.21
        vmovups   160+b(,%rdx,4), %ymm11                        #16.21
        vmovups   192+b(,%rdx,4), %ymm13                        #16.21
        vmovups   224+b(,%rdx,4), %ymm15                        #16.21
        vmovups   c(,%rdx,4), %ymm0                             #16.28
        vmovups   32+c(,%rdx,4), %ymm2                          #16.28
        vmovups   64+c(,%rdx,4), %ymm4                          #16.28
        vmovups   96+c(,%rdx,4), %ymm6                          #16.28
        vmovups   128+c(,%rdx,4), %ymm8                         #16.28
        vmovups   160+c(,%rdx,4), %ymm10                        #16.28
        vmovups   192+c(,%rdx,4), %ymm12                        #16.28
        vmovups   224+c(,%rdx,4), %ymm14                        #16.28
        vfmadd213ps a(,%rdx,4), %ymm0, %ymm1                    #16.13
        vfmadd213ps 32+a(,%rdx,4), %ymm2, %ymm3                 #16.13
        vfmadd213ps 64+a(,%rdx,4), %ymm4, %ymm5                 #16.13
        vfmadd213ps 96+a(,%rdx,4), %ymm6, %ymm7                 #16.13
        vfmadd213ps 128+a(,%rdx,4), %ymm8, %ymm9                #16.13
        vfmadd213ps 160+a(,%rdx,4), %ymm10, %ymm11              #16.13
        vfmadd213ps 192+a(,%rdx,4), %ymm12, %ymm13              #16.13
        vfmadd213ps 224+a(,%rdx,4), %ymm14, %ymm15              #16.13
        vmovups   %ymm1, a(%rcx)                                #16.13
        vmovups   %ymm3, 32+a(%rcx)                             #16.13
        vmovups   %ymm5, 64+a(%rcx)                             #16.13
        vmovups   %ymm7, 96+a(%rcx)                             #16.13
        vmovups   %ymm9, 128+a(%rcx)                            #16.13
        vmovups   %ymm11, 160+a(%rcx)                           #16.13
        vmovups   %ymm13, 192+a(%rcx)                           #16.13
        vmovups   %ymm15, 224+a(%rcx)                           #16.13
        addq      $64, %rdx                                     #15.9
        cmpq      $256, %rdx                                    #15.9
        jb        ..B1.3        # Prob 99%                      #15.9
                                # LOE rdx rbx r12 r13 r14 r15 eax
..B1.4:                         # Preds ..B1.3
                                # Execution count [7.83e+04]
        xorl      %ecx, %ecx                                    #17.13
        xorl      %edx, %edx                                    #17.13
                                # LOE rdx rbx r12 r13 r14 r15 eax ecx
..B1.5:                         # Preds ..B1.7 ..B1.4
                                # Execution count [2.00e+07]
        xorl      %esi, %esi                                    #15.9
                                # LOE rdx rbx rsi r12 r13 r14 r15 eax ecx
..B1.6:                         # Preds ..B1.6 ..B1.5
                                # Execution count [5.11e+09]
        vmovups   a(,%rsi,4), %ymm1                             #18.52
        lea       (%rdx,%rsi,4), %rdi                           #18.17
       vmovups   32+a(,%rsi,4), %ymm3                          #18.52
        vmovups   64+a(,%rsi,4), %ymm5                          #18.52
        vmovups   96+a(,%rsi,4), %ymm7                          #18.52
        vmovups   128+a(,%rsi,4), %ymm9                         #18.52
        vmovups   160+a(,%rsi,4), %ymm11                        #18.52
        vmovups   192+a(,%rsi,4), %ymm13                        #18.52
        vmovups   224+a(,%rsi,4), %ymm15                        #18.52
        vmovups   1024+bb(%rdx,%rsi,4), %ymm0                   #18.41
        vmovups   1056+bb(%rdx,%rsi,4), %ymm2                   #18.41
        vmovups   1088+bb(%rdx,%rsi,4), %ymm4                   #18.41
        vmovups   1120+bb(%rdx,%rsi,4), %ymm6                   #18.41
        vmovups   1152+bb(%rdx,%rsi,4), %ymm8                   #18.41
        vmovups   1184+bb(%rdx,%rsi,4), %ymm10                  #18.41
        vmovups   1216+bb(%rdx,%rsi,4), %ymm12                  #18.41
        vmovups   1248+bb(%rdx,%rsi,4), %ymm14                  #18.41
        vfmadd213ps aa(%rdx,%rsi,4), %ymm0, %ymm1               #18.52
        vfmadd213ps 32+aa(%rdx,%rsi,4), %ymm2, %ymm3            #18.52
        vfmadd213ps 64+aa(%rdx,%rsi,4), %ymm4, %ymm5            #18.52
        vfmadd213ps 96+aa(%rdx,%rsi,4), %ymm6, %ymm7            #18.52
        vfmadd213ps 128+aa(%rdx,%rsi,4), %ymm8, %ymm9           #18.52
        vfmadd213ps 160+aa(%rdx,%rsi,4), %ymm10, %ymm11         #18.52
        vfmadd213ps 192+aa(%rdx,%rsi,4), %ymm12, %ymm13         #18.52
        vfmadd213ps 224+aa(%rdx,%rsi,4), %ymm14, %ymm15         #18.52
        vmovups   %ymm1, 1024+aa(%rdi)                          #18.17
        vmovups   %ymm3, 1056+aa(%rdi)                          #18.17
        vmovups   %ymm5, 1088+aa(%rdi)                          #18.17
        vmovups   %ymm7, 1120+aa(%rdi)                          #18.17
        vmovups   %ymm9, 1152+aa(%rdi)                          #18.17
        vmovups   %ymm11, 1184+aa(%rdi)                         #18.17
        vmovups   %ymm13, 1216+aa(%rdi)                         #18.17
        vmovups   %ymm15, 1248+aa(%rdi)                         #18.17
        addq      $64, %rsi                                     #15.9
        cmpq      $256, %rsi                                    #15.9
        jb        ..B1.6        # Prob 99%                      #15.9
                                # LOE rdx rbx rsi r12 r13 r14 r15 eax ecx
..B1.7:                         # Preds ..B1.6
                                # Execution count [2.00e+07]
        incl      %ecx                                          #17.13
        addq      $1024, %rdx                                   #17.13
        cmpl      $255, %ecx                                    #17.13
        jb        ..B1.5        # Prob 99%                      #17.13
                                # LOE rdx rbx r12 r13 r14 r15 eax ecx
..B1.8:                         # Preds ..B1.7
                                # Execution count [7.83e+04]
        incl      %eax                                          #14.5
        cmpl      $78000, %eax                                  #14.5
        jb        ..B1.2        # Prob 99%                      #14.5
                                # LOE rbx r12 r13 r14 r15 eax
..B1.9:                         # Preds ..B1.8
                                # Execution count [1.00e+00]
        vzeroupper                                              #22.1
        xorl      %eax, %eax                                    #22.1
        movq      %rbp, %rsp                                    #22.1
        popq      %rbp                                          #22.1
        .cfi_def_cfa 7, 8
        .cfi_restore 6
        ret                                                     #22.1


---


### compiler : `gcc`
### title : `s115 benchmark of TSVC is vectorized by icc and not by gcc`
### open_at : `2021-03-05T15:50:14Z`
### last_modified_date : `2021-03-08T08:47:44Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99415
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256

real_t a[LEN_1D],aa[LEN_2D][LEN_2D];
void main()
{

    for (int nl = 0; nl < 1000*(iterations/LEN_2D); nl++) {
        for (int j = 0; j < LEN_2D; j++) {
            for (int i = j+1; i < LEN_2D; i++) {
                a[i] -= aa[j][i] * a[j];
            }
        }
    }

}

is built as:
main:
..B1.1:                         # Preds ..B1.0
                                # Execution count [1.17e-01]
        .cfi_startproc
..___tag_value_main.1:
..L2:
                                                          #9.1
        pushq     %rbp                                          #9.1
        .cfi_def_cfa_offset 16
        movq      %rsp, %rbp                                    #9.1
        .cfi_def_cfa 6, 16
        .cfi_offset 6, -16
        andq      $-128, %rsp                                   #9.1
        pushq     %r14                                          #9.1
        pushq     %r15                                          #9.1
        pushq     %rbx                                          #9.1
        subq      $104, %rsp                                    #9.1
        movl      $3, %edi                                      #9.1
        xorl      %esi, %esi                                    #9.1
        call      __intel_new_feature_proc_init                 #9.1
        .cfi_escape 0x10, 0x03, 0x0e, 0x38, 0x1c, 0x0d, 0x80, 0xff, 0xff, 0xff, 0x1a, 0x0d, 0xe8, 0xff, 0xff, 0xff, 0x22
        .cfi_escape 0x10, 0x0e, 0x0e, 0x38, 0x1c, 0x0d, 0x80, 0xff, 0xff, 0xff, 0x1a, 0x0d, 0xf8, 0xff, 0xff, 0xff, 0x22
        .cfi_escape 0x10, 0x0f, 0x0e, 0x38, 0x1c, 0x0d, 0x80, 0xff, 0xff, 0xff, 0x1a, 0x0d, 0xf0, 0xff, 0xff, 0xff, 0x22
                                # LOE rbx r12 r13 r14 r15
..B1.29:                        # Preds ..B1.1
                                # Execution count [1.17e-01]
        vstmxcsr  (%rsp)                                        #9.1
        xorl      %eax, %eax                                    #11.5
        orl       $32832, (%rsp)                                #9.1
        vldmxcsr  (%rsp)                                        #9.1
                                # LOE r12 r13 eax
..B1.2:                         # Preds ..B1.22 ..B1.29
                                # Execution count [4.50e+04]
        xorl      %r11d, %r11d                                  #12.9
        xorl      %edi, %edi                                    #12.9
        xorl      %ebx, %ebx                                    #12.9
        xorl      %r9d, %r9d                                    #12.9
        xorl      %esi, %esi                                    #12.9
                                # LOE rbx rsi r11 r12 r13 eax edi r9d
..B1.3:                         # Preds ..B1.21 ..B1.2
                                # Execution count [1.15e+07]
        incl      %edi                                          #13.28
        decl      %r9d                                          #13.28
        cmpl      $256, %edi                                    #13.35
        jge       ..B1.21       # Prob 50%                      #13.35
                                # LOE rbx rsi r11 r12 r13 eax edi r9d
..B1.4:                         # Preds ..B1.3
                                # Execution count [1.04e+07]
        lea       256(%r9), %r10d                               #13.35
        cmpl      $16, %r10d                                    #13.13
        jl        ..B1.25       # Prob 10%                      #13.13
                                # LOE rbx rsi r11 r12 r13 eax edi r9d r10d
..B1.5:                         # Preds ..B1.4
                                # Execution count [1.04e+07]
        lea       4+aa(%rsi,%rbx), %r8                          #14.25
        andq      $31, %r8                                      #13.13
        lea       (%rsi,%rbx), %r14                             #14.25
        movl      %r8d, %edx                                    #13.13
        negl      %edx                                          #13.13
        addl      $32, %edx                                     #13.13
        shrl      $2, %edx                                      #13.13
        testl     %r8d, %r8d                                    #13.13
        cmovne    %edx, %r8d                                    #13.13
        lea       16(%r8), %ecx                                 #13.13
        cmpl      %ecx, %r10d                                   #13.13
        jl        ..B1.25       # Prob 10%                      #13.13
                                # LOE rbx rsi r8 r11 r12 r13 r14 eax edi r9d r10d
..B1.6:                         # Preds ..B1.5
                                # Execution count [1.15e+07]
        movl      %r10d, %ecx                                   #13.13
        xorl      %r15d, %r15d                                  #13.13
        subl      %r8d, %ecx                                    #13.13
        xorl      %edx, %edx                                    #13.13
        andl      $15, %ecx                                     #13.13
        negl      %ecx                                          #13.13
        addl      %r10d, %ecx                                   #13.13
        testl     %r8d, %r8d                                    #13.13
        jbe       ..B1.10       # Prob 9%                       #13.13
                                # LOE rdx rbx rsi r8 r11 r12 r13 r14 r15 eax ecx edi r9d r10d
..B1.7:                         # Preds ..B1.6
                                # Execution count [1.04e+07]
        vmovss    a(%rbx), %xmm0                                #14.36
                                # LOE rdx rbx rsi r8 r11 r12 r13 r14 r15 eax ecx edi r9d r10d xmm0
..B1.8:                         # Preds ..B1.8 ..B1.7
                                # Execution count [3.33e+11]
        vmovss    4+aa(%rdx,%r14), %xmm1                        #14.25
        incq      %r15                                          #13.13
        vfnmadd213ss 4+a(%rdx,%rbx), %xmm0, %xmm1               #14.17
        vmovss    %xmm1, 4+a(%rdx,%rbx)                         #14.17
        addq      $4, %rdx                                      #13.13
        cmpq      %r8, %r15                                     #13.13
        jb        ..B1.8        # Prob 99%                      #13.13
                                # LOE rdx rbx rsi r8 r11 r12 r13 r14 r15 eax ecx edi r9d r10d xmm0
..B1.10:                        # Preds ..B1.8 ..B1.6
                                # Execution count [1.04e+07]
        vbroadcastss a(,%r11,4), %ymm0                          #14.36
        lea       (%r8,%r11), %r15                              #13.13
        movslq    %ecx, %rdx                                    #13.13
        .align    16,0x90
                                # LOE rdx rbx rsi r8 r11 r12 r13 r14 r15 eax ecx edi r9d r10d ymm0
..B1.11:                        # Preds ..B1.11 ..B1.10
                                # Execution count [3.33e+11]
        vmovups   4+aa(%r14,%r8,4), %ymm1                       #14.25
        vmovups   36+aa(%r14,%r8,4), %ymm2                      #14.25
        vfnmadd213ps 4+a(,%r15,4), %ymm0, %ymm1                 #14.17
        vfnmadd213ps 36+a(,%r15,4), %ymm0, %ymm2                #14.17
        vmovups   %ymm1, 4+a(,%r15,4)                           #14.17
        vmovups   %ymm2, 36+a(,%r15,4)                          #14.17
        addq      $16, %r8                                      #13.13
        addq      $16, %r15                                     #13.13
        cmpq      %rdx, %r8                                     #13.13
        jb        ..B1.11       # Prob 99%                      #13.13
                                # LOE rdx rbx rsi r8 r11 r12 r13 r14 r15 eax ecx edi r9d r10d ymm0
..B1.12:                        # Preds ..B1.11
                                # Execution count [1.04e+07]
        lea       1(%rcx), %r8d                                 #13.13
        cmpl      %r10d, %r8d                                   #13.13
        ja        ..B1.21       # Prob 50%                      #13.13
                                # LOE rdx rbx rsi r11 r12 r13 r14 eax ecx edi r9d r10d
..B1.13:                        # Preds ..B1.12
                                # Execution count [1.04e+07]
        movslq    %r10d, %r10                                   #13.13
        subq      %rdx, %r10                                    #13.13
        cmpq      $4, %r10                                      #13.13
        jl        ..B1.24       # Prob 10%                      #13.13
                                # LOE rdx rbx rsi r10 r11 r12 r13 r14 eax ecx edi r9d
..B1.14:                        # Preds ..B1.13
                                # Execution count [1.04e+07]
        movl      %r10d, %r8d                                   #13.13
        lea       (%r14,%rdx,4), %r14                           #14.25
        andl      $-4, %r8d                                     #13.13
        addq      %r11, %rdx                                    #13.13
        movslq    %r8d, %r8                                     #13.13
        xorl      %r15d, %r15d                                  #13.13
                                # LOE rdx rbx rsi r8 r10 r11 r12 r13 r14 r15 eax ecx edi r9d
..B1.15:                        # Preds ..B1.15 ..B1.14
                                # Execution count [3.33e+11]
        vbroadcastss a(%rbx), %xmm1                             #14.36
        vmovups   4+aa(%r14,%r15,4), %xmm0                      #14.25
        vfnmadd213ps 4+a(,%rdx,4), %xmm0, %xmm1                 #14.17
        addq      $4, %r15                                      #13.13
        vmovups   %xmm1, 4+a(,%rdx,4)                           #14.17
        addq      $4, %rdx                                      #13.13
        cmpq      %r8, %r15                                     #13.13
        jb        ..B1.15       # Prob 99%                      #13.13
                                # LOE rdx rbx rsi r8 r10 r11 r12 r13 r14 r15 eax ecx edi r9d
..B1.17:                        # Preds ..B1.15 ..B1.24 ..B1.26
                                # Execution count [1.15e+07]
        lea       (,%r8,4), %r14                                #13.13
        cmpq      %r10, %r8                                     #13.13
        jae       ..B1.21       # Prob 9%                       #13.13
                                # LOE rbx rsi r8 r10 r11 r12 r13 r14 eax ecx edi r9d
..B1.18:                        # Preds ..B1.17
                                # Execution count [1.04e+07]
        movslq    %ecx, %rcx                                    #14.17
        lea       (%rsi,%r11,4), %r15                           #14.25
        lea       (,%rcx,4), %rdx                               #14.25
        lea       (%rdx,%r11,4), %rdx                           #14.17
        lea       (%r15,%rcx,4), %rcx                           #14.25
                                # LOE rdx rcx rbx rsi r8 r10 r11 r12 r13 r14 eax edi r9d
..B1.19:                        # Preds ..B1.19 ..B1.18
                                # Execution count [3.33e+11]
        vmovss    a(,%r11,4), %xmm1                             #14.36
        incq      %r8                                           #13.13
        vmovss    4+aa(%r14,%rcx), %xmm0                        #14.25
        vfnmadd213ss 4+a(%r14,%rdx), %xmm0, %xmm1               #14.17
        vmovss    %xmm1, 4+a(%r14,%rdx)                         #14.17
        addq      $4, %r14                                      #13.13
        cmpq      %r10, %r8                                     #13.13
        jb        ..B1.19       # Prob 99%                      #13.13
                                # LOE rdx rcx rbx rsi r8 r10 r11 r12 r13 r14 eax edi r9d
..B1.21:                        # Preds ..B1.19 ..B1.25 ..B1.12 ..B1.17 ..B1.3
                                #      
                                # Execution count [1.15e+07]
        addq      $4, %rbx                                      #13.28
        addq      $1024, %rsi                                   #13.28
        incq      %r11                                          #13.28
        cmpl      $256, %edi                                    #12.9
        jb        ..B1.3        # Prob 99%                      #12.9
                                # LOE rbx rsi r11 r12 r13 eax edi r9d
..B1.22:                        # Preds ..B1.21
                                # Execution count [4.50e+04]
        .byte     15                                            #11.5
        .byte     31                                            #11.5
        .byte     128                                           #11.5
        .byte     0                                             #11.5
        .byte     0                                             #11.5
        .byte     0                                             #11.5
        .byte     0                                             #11.5
        incl      %eax                                          #11.5
        cmpl      $390000, %eax                                 #11.5
        jb        ..B1.2        # Prob 99%                      #11.5
                                # LOE r12 r13 eax
..B1.23:                        # Preds ..B1.22
                                # Execution count [1.17e-01]
        vzeroupper                                              #19.1
        xorl      %eax, %eax                                    #19.1
        addq      $104, %rsp                                    #19.1
        .cfi_restore 3
        popq      %rbx                                          #19.1
        .cfi_restore 15
        popq      %r15                                          #19.1
        .cfi_restore 14
        popq      %r14                                          #19.1
        movq      %rbp, %rsp                                    #19.1
        popq      %rbp                                          #19.1
        .cfi_def_cfa 7, 8
        .cfi_restore 6
        ret                                                     #19.1
        .cfi_def_cfa 6, 16
        .cfi_escape 0x10, 0x03, 0x0e, 0x38, 0x1c, 0x0d, 0x80, 0xff, 0xff, 0xff, 0x1a, 0x0d, 0xe8, 0xff, 0xff, 0xff, 0x22
        .cfi_offset 6, -16
        .cfi_escape 0x10, 0x0e, 0x0e, 0x38, 0x1c, 0x0d, 0x80, 0xff, 0xff, 0xff, 0x1a, 0x0d, 0xf8, 0xff, 0xff, 0xff, 0x22
        .cfi_escape 0x10, 0x0f, 0x0e, 0x38, 0x1c, 0x0d, 0x80, 0xff, 0xff, 0xff, 0x1a, 0x0d, 0xf0, 0xff, 0xff, 0xff, 0x22
                                # LOE
..B1.24:                        # Preds ..B1.13
                                # Execution count [1.04e+06]: Infreq
        xorl      %r8d, %r8d                                    #13.13
        jmp       ..B1.17       # Prob 100%                     #13.13
                                # LOE rbx rsi r8 r10 r11 r12 r13 eax ecx edi r9d
..B1.25:                        # Preds ..B1.5 ..B1.4
                                # Execution count [1.15e+06]: Infreq
        xorl      %ecx, %ecx                                    #13.13
        cmpl      $1, %r10d                                     #13.13
        jb        ..B1.21       # Prob 50%                      #13.13
                                # LOE rbx rsi r11 r12 r13 eax ecx edi r9d r10d
..B1.26:                        # Preds ..B1.25
                                # Execution count [5.77e+05]: Infreq
        movslq    %r10d, %r10                                   #13.13
        xorl      %r8d, %r8d                                    #13.13
        jmp       ..B1.17       # Prob 100%                     #13.13

which runs 0.7s while gcc binary needs 5.7s


---


### compiler : `gcc`
### title : `s211 benchmark of TSVC is vectorized by icc and not by gcc`
### open_at : `2021-03-05T16:20:54Z`
### last_modified_date : `2022-07-12T09:59:59Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99416
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D];
void main()
{

    for (int nl = 0; nl < iterations; nl++) {
        for (int i = 1; i < LEN_1D-1; i++) {
            a[i] = b[i - 1] + c[i] * d[i];
            b[i] = b[i + 1] - e[i] * d[i];
        }
    }
}


Icc produces:
ain:
..B1.1:                         # Preds ..B1.0
                                # Execution count [0.00e+00]
        .cfi_startproc
..___tag_value_ain.1:
..L2:
                                                          #9.1
        subq      $136, %rsp                                    #9.1
        .cfi_def_cfa_offset 144
        xorl      %edx, %edx                                    #11.5
        lea       12+d(%rip), %r8                               #14.38
        vmovss    (%r8), %xmm0                                  #14.38
        movl      $7, %edi                                      #13.38
        lea       12+e(%rip), %r9                               #14.38
        vmulss    (%r9), %xmm0, %xmm12                          #14.38
        xorl      %esi, %esi                                    #13.38
        lea       12+c(%rip), %r10                              #13.38
        vmulss    (%r10), %xmm0, %xmm0                          #13.38
        vmovss    16(%r8), %xmm4                                #14.38
        movl      $31977, %ecx                                  #12.9
        vmulss    16(%r9), %xmm4, %xmm14                        #14.38
        movl      $31975, %eax                                  #12.9
        lea       24+b(%rip), %r11                              #14.20
        vmovss    (%r11), %xmm11                                #14.20
        vmovss    4(%r8), %xmm6                                 #14.38
        vmovss    %xmm12, 104(%rsp)                             #14.38[spill]
        vmovss    %xmm11, 8(%rsp)                               #14.20[spill]
        vmulss    4(%r9), %xmm6, %xmm12                         #14.38
        vmulss    4(%r10), %xmm6, %xmm11                        #13.38
        vmovss    127984+d(%rip), %xmm6                         #14.38
        vmovss    8(%r8), %xmm13                                #14.38
        vmovss    %xmm14, 96(%rsp)                              #14.38[spill]
        vmulss    127984+e(%rip), %xmm6, %xmm14                 #14.38
        vmulss    8(%r9), %xmm13, %xmm1                         #14.38
        vmovss    %xmm14, 112(%rsp)                             #14.38[spill]
        vmovss    127988+d(%rip), %xmm14                        #14.38
        vmovss    %xmm1, 16(%rsp)                               #14.38[spill]
        vmulss    8(%r10), %xmm13, %xmm1                        #13.38
        vmulss    16(%r10), %xmm4, %xmm13                       #13.38
        vmulss    127988+e(%rip), %xmm14, %xmm4                 #14.38
        vmovss    %xmm4, 120(%rsp)                              #14.38[spill]
       vmulss    127988+c(%rip), %xmm14, %xmm4                 #13.38
        vmovss    -4(%r11), %xmm5                               #14.20
        vmovss    -8(%r8), %xmm2                                #14.38
        vmovss    12(%r8), %xmm15                               #14.38
        vmovss    %xmm4, 24(%rsp)                               #13.38[spill]
        vmovss    127992+d(%rip), %xmm4                         #14.38
        vmovss    %xmm5, (%rsp)                                 #14.20[spill]
        vmulss    -8(%r9), %xmm2, %xmm3                         #14.38
        vmulss    -8(%r10), %xmm2, %xmm5                        #13.38
        vmulss    12(%r9), %xmm15, %xmm2                        #14.38
        vmulss    12(%r10), %xmm15, %xmm15                      #13.38
        vmulss    127992+e(%rip), %xmm4, %xmm14                 #14.38
        vmulss    127992+c(%rip), %xmm4, %xmm4                  #13.38
        vmovss    -4(%r8), %xmm10                               #14.38
        vmulss    -4(%r9), %xmm10, %xmm7                        #14.38
        vmulss    -4(%r10), %xmm10, %xmm10                      #13.38
        vmovss    %xmm7, 88(%rsp)                               #14.38[spill]
        vmovss    %xmm4, 32(%rsp)                               #13.38[spill]
        vmovss    %xmm15, 56(%rsp)                              #13.31[spill]
        vmovss    %xmm14, 40(%rsp)                              #13.31[spill]
        vmovss    %xmm3, 80(%rsp)                               #13.31[spill]
        vmovss    -16(%r11), %xmm9                              #14.20
        vmovss    -12(%r11), %xmm8                              #14.20
        vmovss    -8(%r11), %xmm7                               #14.20
        vmovss    127984+c(%rip), %xmm4                         #13.31
        vmovss    %xmm1, 64(%rsp)                               #13.31[spill]
        vmovss    %xmm0, 48(%rsp)                               #13.31[spill]
        vmovss    %xmm2, 72(%rsp)                               #13.31[spill]
        vmovss    16(%rsp), %xmm14                              #13.31[spill]
        vmovss    8(%rsp), %xmm15                               #13.31[spill]
        vmovss    (%rsp), %xmm3                                 #13.31[spill]
                                # LOE rax rcx rbx rbp rsi rdi r12 r13 r14 r15 edx xmm3 xmm4 xmm5 xmm6 xmm7 xmm8 xmm9 xmm10 xmm11 xmm12 xmm13 xmm14 xmm15
..B1.2:                         # Preds ..B1.10 ..B1.1
                                # Execution count [1.00e+05]
        movq      %rdi, %r8                                     #12.9
        vsubss    80(%rsp), %xmm9, %xmm0                        #14.38[spill]
        vsubss    88(%rsp), %xmm8, %xmm1                        #14.38[spill]
        vsubss    104(%rsp), %xmm7, %xmm2                       #14.38[spill]
        vsubss    %xmm14, %xmm15, %xmm7                         #14.38
        vsubss    %xmm12, %xmm3, %xmm3                          #14.38
        vmovss    28+b(%rip), %xmm8                             #14.20
       vmovss    32+b(%rip), %xmm15                            #14.20
        vmovss    %xmm0, 4+b(%rip)                              #14.13
        vmovss    %xmm1, 8+b(%rip)                              #14.13
        vmovss    %xmm2, 12+b(%rip)                             #14.13
        vmovss    %xmm3, 16+b(%rip)                             #14.13
        vmovss    %xmm7, 20+b(%rip)                             #14.13
        vsubss    72(%rsp), %xmm8, %xmm9                        #14.38[spill]
        vsubss    96(%rsp), %xmm15, %xmm0                       #14.38[spill]
        vmovss    %xmm9, 24+b(%rip)                             #14.13
        vmovss    %xmm0, 28+b(%rip)                             #14.13
                                # LOE rax rcx rbx rbp rsi rdi r8 r12 r13 r14 r15 edx xmm4 xmm5 xmm6 xmm10 xmm11 xmm12 xmm13 xmm14
..B1.3:                         # Preds ..B1.3 ..B1.2
                                # Execution count [3.20e+09]
        vmovups   4+e(,%r8,4), %ymm1                            #14.31
        lea       (,%r8,4), %r9                                 #14.13
        vmovups   36+e(,%r8,4), %ymm3                           #14.31
        vmovups   68+e(,%r8,4), %ymm8                           #14.31
        vmovups   100+e(,%r8,4), %ymm15                         #14.31
        vmovups   4+d(,%r8,4), %ymm0                            #14.38
        vmovups   36+d(,%r8,4), %ymm2                           #14.38
        vmovups   68+d(,%r8,4), %ymm7                           #14.38
        vmovups   100+d(,%r8,4), %ymm9                          #14.38
        vfnmadd213ps 8+b(,%r8,4), %ymm0, %ymm1                  #14.38
        vfnmadd213ps 40+b(,%r8,4), %ymm2, %ymm3                 #14.38
        vfnmadd213ps 72+b(,%r8,4), %ymm7, %ymm8                 #14.38
        vfnmadd213ps 104+b(,%r8,4), %ymm9, %ymm15               #14.38
        vmovups   %ymm1, 4+b(%r9)                               #14.13
        vmovups   %ymm3, 36+b(%r9)                              #14.13
        vmovups   %ymm8, 68+b(%r9)                              #14.13
        vmovups   %ymm15, 100+b(%r9)                            #14.13
        addq      $32, %r8                                      #12.9
        cmpq      $31975, %r8                                   #12.9
        jb        ..B1.3        # Prob 99%                      #12.9
                                # LOE rax rcx rbx rbp rsi rdi r8 r12 r13 r14 r15 edx xmm4 xmm5 xmm6 xmm10 xmm11 xmm12 xmm13 xmm14
..B1.4:                         # Preds ..B1.3
                                # Execution count [1.00e+05]
        movq      %rsi, %r9                                     #12.9
        movq      %rcx, %r8                                     #12.9
                                # LOE rax rcx rbx rbp rsi rdi r8 r9 r12 r13 r14 r15 edx xmm4 xmm5 xmm6 xmm10 xmm11 xmm12 xmm13 xmm14
..B1.5:                         # Preds ..B1.5 ..B1.4
                                # Execution count [3.20e+09]
      vmovups   127904+e(,%r9,4), %xmm1                       #14.31
        vmovups   127904+d(,%r9,4), %xmm0                       #14.38
        vfnmadd213ps b(,%r8,4), %xmm0, %xmm1                    #14.38
        addq      $4, %r8                                       #12.9
        vmovups   %xmm1, 127904+b(,%r9,4)                       #14.13
        addq      $4, %r9                                       #12.9
        cmpq      $20, %r9                                      #12.9
        jb        ..B1.5        # Prob 99%                      #12.9
                                # LOE rax rcx rbx rbp rsi rdi r8 r9 r12 r13 r14 r15 edx xmm4 xmm5 xmm6 xmm10 xmm11 xmm12 xmm13 xmm14
..B1.6:                         # Preds ..B1.5
                                # Execution count [1.00e+05]
        vmovss    127996+b(%rip), %xmm9                         #14.20
        movq      %rdi, %r8                                     #12.9
        vmovss    127992+b(%rip), %xmm1                         #14.20
        vmovss    127988+b(%rip), %xmm2                         #14.20
        vaddss    b(%rip), %xmm5, %xmm7                         #13.38
        vaddss    4+b(%rip), %xmm10, %xmm3                      #13.38
        vsubss    40(%rsp), %xmm9, %xmm8                        #14.38[spill]
        vsubss    112(%rsp), %xmm2, %xmm2                       #14.38[spill]
        vsubss    120(%rsp), %xmm1, %xmm1                       #14.38[spill]
        vmovss    %xmm7, 4+a(%rip)                              #13.13
        vmovss    16+b(%rip), %xmm7                             #13.20
        vmovss    %xmm3, 8+a(%rip)                              #13.13
        vmovss    8+b(%rip), %xmm9                              #13.20
        vmovss    %xmm8, 127992+b(%rip)                         #14.13
        vmovss    12+b(%rip), %xmm8                             #13.20
        vmovss    %xmm2, 127984+b(%rip)                         #14.13
        vaddss    %xmm11, %xmm8, %xmm0                          #13.38
        vaddss    64(%rsp), %xmm7, %xmm3                        #13.38[spill]
        vaddss    48(%rsp), %xmm9, %xmm15                       #13.38[spill]
        vmovss    %xmm3, 20+a(%rip)                             #13.13
        vmovss    20+b(%rip), %xmm3                             #13.20
        vmovss    %xmm15, 12+a(%rip)                            #13.13
        vmovss    %xmm0, 16+a(%rip)                             #13.13
        vmovss    %xmm1, 127988+b(%rip)                         #14.13
        vmovss    %xmm9, (%rsp)                                 #13.13[spill]
        vaddss    56(%rsp), %xmm3, %xmm15                       #13.38[spill]
        vmovss    %xmm15, 24+a(%rip)                            #13.13
        vmovss    24+b(%rip), %xmm15                            #13.20
        vaddss    %xmm13, %xmm15, %xmm0                         #13.38
        vmovss    %xmm0, 28+a(%rip)                             #13.13
                               # LOE rax rcx rbx rbp rsi rdi r8 r12 r13 r14 r15 edx xmm1 xmm2 xmm3 xmm4 xmm5 xmm6 xmm7 xmm8 xmm10 xmm11 xmm12 xmm13 xmm14 xmm15
..B1.7:                         # Preds ..B1.7 ..B1.6
                                # Execution count [3.20e+09]
        vmovups   4+c(,%r8,4), %ymm9                            #13.31
        lea       (,%r8,4), %r9                                 #13.13
        vmovups   4+d(,%r8,4), %ymm0                            #13.38
        vfmadd213ps b(,%r8,4), %ymm0, %ymm9                     #13.38
        vmovups   36+d(,%r8,4), %ymm0                           #13.38
        vmovups   %ymm9, 4+a(%r9)                               #13.13
        vmovups   36+c(,%r8,4), %ymm9                           #13.31
        vfmadd213ps 32+b(,%r8,4), %ymm0, %ymm9                  #13.38
        vmovups   68+d(,%r8,4), %ymm0                           #13.38
        vmovups   %ymm9, 36+a(%r9)                              #13.13
        vmovups   68+c(,%r8,4), %ymm9                           #13.31
        vfmadd213ps 64+b(,%r8,4), %ymm0, %ymm9                  #13.38
        vmovups   100+d(,%r8,4), %ymm0                          #13.38
        vmovups   %ymm9, 68+a(%r9)                              #13.13
        vmovups   100+c(,%r8,4), %ymm9                          #13.31
        vfmadd213ps 96+b(,%r8,4), %ymm0, %ymm9                  #13.38
        addq      $32, %r8                                      #12.9
        vmovups   %ymm9, 100+a(%r9)                             #13.13
        cmpq      $31975, %r8                                   #12.9
        jb        ..B1.7        # Prob 99%                      #12.9
                                # LOE rax rcx rbx rbp rsi rdi r8 r12 r13 r14 r15 edx xmm1 xmm2 xmm3 xmm4 xmm5 xmm6 xmm7 xmm8 xmm10 xmm11 xmm12 xmm13 xmm14 xmm15
..B1.8:                         # Preds ..B1.7
                                # Execution count [1.00e+05]
        movq      %rsi, %r9                                     #12.9
        movq      %rax, %r8                                     #12.9
                                # LOE rax rcx rbx rbp rsi rdi r8 r9 r12 r13 r14 r15 edx xmm1 xmm2 xmm3 xmm4 xmm5 xmm6 xmm7 xmm8 xmm10 xmm11 xmm12 xmm13 xmm14 xmm15
..B1.9:                         # Preds ..B1.9 ..B1.8
                                # Execution count [3.20e+09]
        vmovups   127904+c(,%r9,4), %xmm9                       #13.31
        vmovups   127904+d(,%r9,4), %xmm0                       #13.38
        vfmadd213ps b(,%r8,4), %xmm0, %xmm9                     #13.38
        addq      $4, %r8                                       #12.9
        vmovups   %xmm9, 127904+a(,%r9,4)                       #13.13
        addq      $4, %r9                                       #12.9
        cmpq      $20, %r9                                      #12.9
        jb        ..B1.9        # Prob 99%                      #12.9
                                # LOE rax rcx rbx rbp rsi rdi r8 r9 r12 r13 r14 r15 edx xmm1 xmm2 xmm3 xmm4 xmm5 xmm6 xmm7 xmm8 xmm10 xmm11 xmm12 xmm13 xmm14 xmm15
..B1.10:                        # Preds ..B1.9
                               # Execution count [1.07e+09]
        incl      %edx                                          #11.5
        vmovss    127980+b(%rip), %xmm0                         #13.20
        vmovss    (%rsp), %xmm9                                 #[spill]
        vfmadd231ss %xmm6, %xmm4, %xmm0                         #13.38
        cmpl      $100000, %edx                                 #11.5
        jb        ..B1.2        # Prob 99%                      #11.5
                                # LOE rax rcx rbx rbp rsi rdi r12 r13 r14 r15 edx xmm0 xmm1 xmm2 xmm3 xmm4 xmm5 xmm6 xmm7 xmm8 xmm9 xmm10 xmm11 xmm12 xmm13 xmm14 xmm15
..B1.11:                        # Preds ..B1.10
                                # Execution count [1.00e+00]
        vmovss    %xmm0, 127984+a(%rip)                         #13.13
        vaddss    32(%rsp), %xmm1, %xmm1                        #13.38[spill]
        vaddss    24(%rsp), %xmm2, %xmm2                        #13.38[spill]
        vmovss    %xmm1, 127992+a(%rip)                         #13.13
        vmovss    %xmm2, 127988+a(%rip)                         #13.13
        vzeroupper                                              #17.1
        addq      $136, %rsp                                    #17.1
        .cfi_def_cfa_offset 8
        ret                                                     #17.1


---


### compiler : `gcc`
### title : `std::bit_cast generates more instructions than __builtin_bit_cast and memcpy with -march=native`
### open_at : `2021-03-06T20:12:59Z`
### last_modified_date : `2021-03-08T20:40:32Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99434
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
https://godbolt.org/z/5KWM8Y
struct u64x2_t
{
#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
    std::uint64_t high,low;
#else
    std::uint64_t low,high;
#endif
};
u64x2_t umul5(std::uint64_t a,std::uint64_t b) noexcept
{
    return std::bit_cast<u64x2_t>(static_cast<__uint128_t>(a)*b);
}

u64x2_t umul_builtin(std::uint64_t a,std::uint64_t b) noexcept
{
    return __builtin_bit_cast(u64x2_t,static_cast<__uint128_t>(a)*b);
}

assembly:
umul5(unsigned long, unsigned long):
        movq    %rdi, %rdx
        mulx    %rsi, %rdx, %rcx
        movq    %rdx, %rax
        movq    %rcx, %rdx
        ret
umul_builtin(unsigned long, unsigned long):
        movq    %rdi, %rdx
        mulx    %rsi, %rax, %rdx
        ret

There is another issue:

std::uint64_t umul128(std::uint64_t a,std::uint64_t b,std::uint64_t& high) noexcept
{
    __uint128_t res{static_cast<__uint128_t>(a)*b};
    high=static_cast<std::uint64_t>(res>>64);
    return static_cast<std::uint64_t>(res);
}
I cannot do this since this generates more instructions than using memcpy to pun types.

clang does not have this issue and all cases are dealt with correctly.


---


### compiler : `gcc`
### title : `[11 regression] ABI breakage with some static initialization`
### open_at : `2021-03-07T20:50:54Z`
### last_modified_date : `2022-02-12T21:58:06Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99456
### status : `RESOLVED`
### tags : `ABI, missed-optimization`
### component : `c++`
### version : `11.0`
### severity : `normal`
### contents :
Created attachment 50326
testcase

I've noticed some poorer behaviour in trunk than gcc10 with emitting inline vars.  The example when compiled with gcc-10 -std=c++17 emits only those vars that are reachable from the externally visible vars.  I.e. Var1...Var4 are NOT present.  Further all vars are statically initialized with no global initializer function.
(This is regardless of optimization level).

With trunk, all the inline vars are emitted, even when not referenced.  Further, there is a gloabl initializer function emitted, that only tests and sets their respective guard variables, with two exceptions.

Var3 and Var13 are dynamically initialized. This is an abi breakage.

I attach the two assembly files generated (the source presumes LP32 or LP64 ABI)


---


### compiler : `gcc`
### title : `Enhance scheduling to split instructions`
### open_at : `2021-03-08T10:49:56Z`
### last_modified_date : `2021-03-08T16:13:18Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99462
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Maybe the scheduler(s) can already do this (I have zero knowledge here).  For
example the x86 vec_concatv2di insn has alternatives that cause the instruction
to be split into multiple uops (vpinsrq, movhpd) when the 'insert' operand
is not XMM (but GPR or MEM).  We now have a peephole2 to split such cases:

+;; Further split pinsrq variants of vec_concatv2di to hide the latency
+;; the GPR->XMM transition(s).
+(define_peephole2
+  [(match_scratch:DI 3 "Yv")
+   (set (match_operand:V2DI 0 "sse_reg_operand")
+       (vec_concat:V2DI (match_operand:DI 1 "sse_reg_operand")
+                        (match_operand:DI 2 "nonimmediate_gr_operand")))]
+  "TARGET_64BIT && TARGET_SSE4_1
+   && !optimize_insn_for_size_p ()"
+  [(set (match_dup 3)
+        (match_dup 2))
+   (set (match_dup 0)
+       (vec_concat:V2DI (match_dup 1)
+                        (match_dup 3)))])

but in reality this is only profitable when we either can execute
two "bad" move uops in parallel (thus when originally composing
two GPRs or two MEMs) or when we can schedule one "bad" move much
earlier.

Thus, can the scheduler already "split" an instruction - say split
away a load uop and issue it early when a scratch register is available?

(the reverse alternative is to not expose multi-uop insns before scheduling
and only merge them later - during scheduling?)

How does GCC deal with situations like this?


---


### compiler : `gcc`
### title : `Convert fixed index addition to array address offset`
### open_at : `2021-03-08T17:20:53Z`
### last_modified_date : `2021-03-09T10:03:40Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99470
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
These two functions do the same thing but f() is the cleaner source code (especially when arr is a std::array) while g() generates better code:

https://gcc.godbolt.org/z/vTT399

#include <cstdint>

inline int8_t arr[256];

bool f(int a, int b) {
    return arr[a+128] == arr[b+128];
}

bool g(int a, int b) {
    return (arr+128)[a] == (arr+128)[b];
}

f(int, int):
        sub     edi, -128
        sub     esi, -128
        lea     rax, arr[rip]
        movsx   rdi, edi
        movsx   rsi, esi
        movzx   edx, BYTE PTR [rax+rsi]
        cmp     BYTE PTR [rax+rdi], dl
        sete    al
        ret
g(int, int):
        lea     rax, arr[rip+128]
        movsx   rdi, edi
        movsx   rsi, esi
        movzx   edx, BYTE PTR [rax+rsi]
        cmp     BYTE PTR [rax+rdi], dl
        sete    al
        ret

In addition to only doing the +128 once, it also ends up being completely free in g() because the assembler (or linker?) folds the addition into the address calculation by adjusting the offset of the rip-relative address. In the godbolt link, you can see that when compiled to binary, the LEA instruction uses the same form in both f() and g(), so the addition really is free in g().


---


### compiler : `gcc`
### title : `redundant conditional zero-initialization not eliminated`
### open_at : `2021-03-08T20:24:43Z`
### last_modified_date : `2023-05-06T23:17:21Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99473
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
All three functions below should result in equivalently optimal code (and, ideally IL) but only g1() and g3() result in the same assembly, and only g1() result in optimal, branchless GIMPLE.

$ cat x.c && gcc -O2 -S -Wall -o/dev/stdout x.c
void f (int*);

void g1 (int i)
{
  int x;
  if (i)
    x = i;
  else
    x = 0;
  f (&x);
}


void g2 (int i)
{
  int x;
  if (i) {
    x = i;
    f (&x);
  }
  else {
    x = 0;
    f (&x);
  }
}

void g3 (int i)
{
  int x = 0;
  if (i)
    x = i;
  f (&x);
}
	.file	"x.c"
	.text
	.p2align 4
	.globl	g1
	.type	g1, @function
g1:
.LFB0:
	.cfi_startproc
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movl	%edi, 12(%rsp)
	leaq	12(%rsp), %rdi
	call	f
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE0:
	.size	g1, .-g1
	.p2align 4
	.globl	g2
	.type	g2, @function
g2:
.LFB1:
	.cfi_startproc
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	testl	%edi, %edi
	je	.L5
	movl	%edi, 12(%rsp)
	leaq	12(%rsp), %rdi
	call	f
	addq	$24, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L5:
	.cfi_restore_state
	leaq	12(%rsp), %rdi
	movl	$0, 12(%rsp)
	call	f
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1:
	.size	g2, .-g2
	.p2align 4
	.globl	g3
	.type	g3, @function
g3:
.LFB2:
	.cfi_startproc
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movl	%edi, 12(%rsp)
	leaq	12(%rsp), %rdi
	call	f
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2:
	.size	g3, .-g3
	.ident	"GCC: (GNU) 11.0.1 20210308 (experimental)"
	.section	.note.GNU-stack,"",@progbits


---


### compiler : `gcc`
### title : `[mips64] over-strict refusal to emit tail calls`
### open_at : `2021-03-09T18:03:47Z`
### last_modified_date : `2021-03-10T08:09:16Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99491
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
mips_function_ok_for_sibcall refuses to generate sibcalls (except locally) on mips64 due to %gp being call-saved and the possibility that the callee is a lazy resolver stub. This is presumably correct-ish on dynamic-linked platforms with lazy resolver, due to the resolver using the caller's value of %gp, but completely gratuitous on platforms that are static linked or don't use lazy resolver, such as musl libc.

Moreover, the problem could be fixed even for lazy-resolver targets by generating an indirect function call reference that forcibly loads the address and can't go through a lazy resolver, rather than a PLT-like reference that might.


---


### compiler : `gcc`
### title : `Missing memmove detection`
### open_at : `2021-03-10T02:52:10Z`
### last_modified_date : `2021-03-17T14:31:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99504
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
cat test.c

typedef struct
{
    unsigned char r,g,b,c;
}pixel;

void
foo_memmov (pixel* p, pixel* q, int n)
{
    for (int i = 0; i != n; i++)
      *p++ = *q++; 
}

void
foo_int (int* p, int* q, int n)
{
    for (int i = 0; i != n; i++)
      *p++ = *q++; 
}

gcc -Ofast -march=skylake

https://godbolt.org/z/hbnTfM

1. Not memmov optimization
2. Why didn't vectorizer optimize foo_memmov like foo_int did.


---


### compiler : `gcc`
### title : `Failure to detect bswap pattern`
### open_at : `2021-03-10T15:54:27Z`
### last_modified_date : `2021-11-25T20:28:23Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99520
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
#include <stdint.h>

 uint32_t endian_fix32( uint32_t x ){
    return (x<<24) + ((x<<8)&0xff0000) + ((x>>8)&0xff00) + (x>>24);

}

For aarch64 clang manages to optimise it to:
endian_fix32(unsigned int):                      // @endian_fix32(unsigned int)
        rev     w0, w0
        ret

GCC doesn't:
endian_fix32(unsigned int):
        lsl     w1, w0, 8
        lsr     w2, w0, 8
        and     w2, w2, 65280
        lsr     w3, w0, 24
        and     w1, w1, 16711680
        add     w0, w3, w0, lsl 24
        orr     w1, w1, w2
        add     w0, w1, w0
        ret

Is there something missing in the bswap pass?


---


### compiler : `gcc`
### title : `[11 Regression] Performance regression since gcc 9 (argument passing / register allocation)`
### open_at : `2021-03-10T22:36:26Z`
### last_modified_date : `2023-07-07T10:39:14Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99531
### status : `NEW`
### tags : `missed-optimization, ra`
### component : `target`
### version : `10.2.0`
### severity : `normal`
### contents :
For this source:

int func(int, int, int, int, int, int);
int caller(int a, int b, int c, int d, int e) { return func(0, a, b, c, d, e); }

the code generated for caller is:

pushq %r12
movl %r8d, %r9d
popq %r12
movl %ecx, %r8d
movl %edx, %ecx
movl %esi, %edx
movl %edi, %esi
xorl %edi, %edi
jmp func

gcc 9 started producing the useless push/pop pair.

Mailing list link: https://gcc.gnu.org/pipermail/gcc-help/2021-February/139885.html


---


### compiler : `gcc`
### title : `unexplained warning on "uninitialized value" in std::normal_distribution`
### open_at : `2021-03-11T01:36:21Z`
### last_modified_date : `2022-08-06T02:57:45Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99536
### status : `RESOLVED`
### tags : `diagnostic, missed-optimization`
### component : `tree-optimization`
### version : `5.3.1`
### severity : `normal`
### contents :
Although the example in http://www.cplusplus.com/reference/random/normal_distribution/ compiled fine, I got a warning on uninitialized value in a similar code below.

#include <random>
void foo(double);
int main()
{
  std::default_random_engine generator;
  std::normal_distribution<double> norm_dist(0,1);
  for(int i=0; i<3; i++)
    foo(norm_dist(generator));
  return 0;
}

When compiled with "g++ -std=c++11 -Wall -ffast-math -O -c foo.cpp" using gcc5.3.1 on CentOS7.2, I got:

foo.cpp:8:30: warning: ‘norm_dist.std::normal_distribution::_M_saved’ may be used uninitialized in this function [-Wmaybe-uninitialized] foo(norm_dist(generator));

Note that without -O, -ffast-math, or the for-loop, the code compiled fine without any warning.


---


### compiler : `gcc`
### title : `aarch64: csel is used for cold scalar computation which affects performance`
### open_at : `2021-03-11T13:19:10Z`
### last_modified_date : `2021-12-23T21:34:49Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99551
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `10.0`
### severity : `normal`
### contents :
this is an optimization bug, i don't know which layer it should
be fixed so i report it as target bug.

cold path affects performance of hot code because csel is used:

long foo(long x, int c)
{
    if (__builtin_expect(c,0))
        x = (x + 15) & ~15;
    return x;
}

 
compiles to

foo:
        cmp     w1, 0
        add     x1, x0, 15
        and     x1, x1, -16
        csel    x0, x1, x0, ne
        ret

i think it would be better to use a branch if the user
explicitly marked the computation cold.
e.g. this is faster if c is always 0:

long foo(long x, int c)
{
    if (__builtin_expect(c,0)) {
        asm ("");
        x = (x + 15) & ~15;
    }
    return x;
}

foo:
        cbnz    w1, .L7
        ret
.L7:
        add     x0, x0, 15
        and     x0, x0, -16
        ret


---


### compiler : `gcc`
### title : `Improving __builtin_add_overflow performance on x86-64`
### open_at : `2021-03-15T08:46:29Z`
### last_modified_date : `2021-09-02T09:29:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99591
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `normal`
### contents :
This is with gcc (GCC) 10.2.1 20201125 (Red Hat 10.2.1-9) on x86-64. For the function:

_Bool signed1_overflow (signed char a, signed char b)
{
  signed char r;
  return __builtin_add_overflow (a, b, &r);
}

gcc generates the code:

signed1_overflow:
        movsbl  %sil, %esi
        movsbl  %dil, %edi
        addb    %sil, %dil
        seto    %al
        ret

The movsbl instructions are unnecessary and can be omitted.


For the function:

_Bool signed2_overflow (short a, short b)
{
  short r;
  return __builtin_add_overflow (a, b, &r);
}

gcc generates:

signed2_overflow:
        movswl  %di, %edi
        movswl  %si, %esi
        xorl    %eax, %eax
        addw    %si, %di
        jo      .L8
.L6:
        andl    $1, %eax
        ret
.L8:
        movl    $1, %eax
        jmp     .L6

Better would be this:

signed2_overflow:
        addw    %si, %di
        seto    %al
        retq

There are similar opportunities for improvement in __builtin_sub_overflow and __builtin_mul_overflow.

This bug report follows up on this discussion about Gnulib:

https://lists.gnu.org/r/bug-gnulib/2021-03/msg00078.html
https://lists.gnu.org/r/bug-gnulib/2021-03/msg00079.html
https://lists.gnu.org/r/bug-gnulib/2021-03/msg00080.html


---


### compiler : `gcc`
### title : `fails to infer local-dynamic TLS model from hidden visibility`
### open_at : `2021-03-16T15:29:33Z`
### last_modified_date : `2023-07-26T09:40:55Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99619
### status : `RESOLVED`
### tags : `missed-optimization, visibility`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
Thread-local variables with hidden visibility don't need to use the "general-dynamic" TLS model: they can use "local-dynamic" model, which is more efficient when more than one variable is accessed. This is documented in "ELF handling for thread-local storage".

Testcase:

__attribute__((visibility("hidden")))
extern __thread int a, b;

int f()
{
    return a + b;
}

clang -O2 -fpic emits:
f:
        .cfi_startproc
        push    rax
        .cfi_def_cfa_offset 16
        lea     rdi, [rip + a@TLSLD]
        call    __tls_get_addr@PLT
        mov     rcx, rax
        mov     eax, dword ptr [rax + b@DTPOFF]
        add     eax, dword ptr [rcx + a@DTPOFF]
        pop     rcx
        .cfi_def_cfa_offset 8
        ret

gcc -O2 -fpic emits:
f:
        .cfi_startproc
        push    rbx
        .cfi_def_cfa_offset 16
        .cfi_offset 3, -16
        data16  lea rdi, a@tlsgd[rip]
        .value  0x6666
        rex64
        call    __tls_get_addr@PLT
        mov     rbx, rax
        data16  lea rdi, b@tlsgd[rip]
        .value  0x6666
        rex64
        call    __tls_get_addr@PLT
        mov     eax, DWORD PTR [rax]
        add     eax, DWORD PTR [rbx]
        pop     rbx
        .cfi_def_cfa_offset 8
        ret


---


### compiler : `gcc`
### title : `Subtract with borrow (SBB) missed optimization`
### open_at : `2021-03-16T16:53:49Z`
### last_modified_date : `2023-08-08T07:15:04Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99620
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.0`
### severity : `enhancement`
### contents :
Hi.

For the 128-bit precision subtraction: SUB + SBB the optimization depends on the how the carry bit condition is specified in the code. In the first case below everything works nicely, but in the second we have unnecessary CMP in the final code.

I believe the second carry bit condition is simpler (does not require unsigned integer wrapping behavior) and does not have dependency on the first subtraction. 


using u64 = unsigned long;

struct u128
{
    u64 l;
    u64 h;
};

auto sub_good(u128 a, u128 b)
{
    auto l = a.l - b.l;
    auto k = l > a.l;
    auto h = a.h - b.h - k;
    return u128{l, h};
}

auto sub_bad(u128 a, u128 b)
{
    auto l = a.l - b.l;
    auto k = a.l < b.l;
    auto h = a.h - b.h - k;
    return u128{l, h};
}


sub_good(u128, u128):
        mov     rax, rdi
        sub     rax, rdx
        sbb     rsi, rcx
        mov     rdx, rsi
        ret
sub_bad(u128, u128):
        cmp     rdi, rdx
        mov     rax, rdi
        sbb     rsi, rcx
        sub     rax, rdx
        mov     rdx, rsi
        ret


If you think this is easy to fix, I would like to give it a try if I could get some pointers where to start.


---


### compiler : `gcc`
### title : `s1113 benchmark of TSVC is unrolled by icc and not by gcc and runs faster on znver3`
### open_at : `2021-03-17T18:20:21Z`
### last_modified_date : `2021-12-22T10:42:30Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99633
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `enhancement`
### contents :
typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256
// array definitions

real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D];

int main(struct args_t * func_args)
{

//    linear dependence testing
//    one iteration dependency on a(LEN_1D/2) but still vectorizable

    //initialise_arrays(__func__);
    //gettimeofday(&func_args->t1, NULL);

    for (int nl = 0; nl < 2*iterations; nl++) {
        for (int i = 0; i < LEN_1D; i++) {
            a[i] = a[LEN_1D/2] + b[i];
        }
        //dummy(a, b, c, d, e, aa, bb, cc, 0.);
    }

    return a[10];
}

Is unrolled twice by icc and runs 1.5s instead of 2.6s when built with gcc. -funroll-loops fixes the issue, but it suggests we may want to unroll by default on zver3


---


### compiler : `gcc`
### title : `s132 and s281 benchmarks of TSVC on zen3 benefits from -mno-fma`
### open_at : `2021-03-17T21:24:03Z`
### last_modified_date : `2021-08-11T01:08:34Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99638
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;

#define iterations 1000000
#define LEN_1D 32000
#define LEN_2D 256
// array definitions
real_t flat_2d_array[LEN_2D*LEN_2D];

real_t x[LEN_1D];

real_t a[LEN_1D],b[LEN_1D],c[LEN_1D],d[LEN_1D],e[LEN_1D],
aa[LEN_2D][LEN_2D],bb[LEN_2D][LEN_2D],cc[LEN_2D][LEN_2D],tt[LEN_2D][LEN_2D];

int indx[LEN_1D];

real_t* __restrict__ xx;
real_t* yy;


// %2.5

void main()
{
//    global data flow analysis
//    loop with multiple dimension ambiguous subscripts

    int m = 0;
    int j = m;
    int k = m+1;
    for (int nl = 0; nl < 400*iterations; nl++) {
        for (int i= 1; i < LEN_2D; i++) {
            aa[j][i] = aa[k][i-1] + b[i] * c[1];
        }
        dummy();
    }
}

compiled with -Ofast -march=native runs 4.4s compared to 4.2s with -Ofast -march=native -mno-fma


---


### compiler : `gcc`
### title : `Duplicated constant in V2SI/V4SI`
### open_at : `2021-03-17T21:37:52Z`
### last_modified_date : `2021-03-21T19:04:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99639
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `target`
### version : `11.0`
### severity : `enhancement`
### contents :
Take C++ code:
void  foo(int (&arr)[42])
{
    for (unsigned i = 0; i < sizeof(arr) / sizeof(arr[0]); ++i)
        arr[i] = 43;
}
---- CUT ----
Current we produce:
foo(int (&) [42]):
        movi    v0.4s, 0x2b
        movi    v1.2s, 0x2b
        stp     q0, q0, [x0]
        stp     q0, q0, [x0, 32]
        stp     q0, q0, [x0, 64]
        stp     q0, q0, [x0, 96]
        stp     q0, q0, [x0, 128]
        str     d1, [x0, 160]
        ret
----- CUT ----
But d1 is the same as q0 really.  And we don't need to have it.


---


### compiler : `gcc`
### title : `pdp-11 target produces inefficient code for sign extend`
### open_at : `2021-03-18T14:21:22Z`
### last_modified_date : `2021-03-19T22:10:47Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99645
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `target`
### version : `9.0`
### severity : `normal`
### contents :
This issue was revealed by bug 84437.  The reproducer given there now produces "correct" code but it's quite inefficient because it uses a shift by 15 for sign extend.  On pdp11 that's not the best answer.


---


### compiler : `gcc`
### title : `s111 benchmark of TSVC preffers -mprefer-avx128 on zen3`
### open_at : `2021-03-18T14:55:28Z`
### last_modified_date : `2021-03-18T15:18:41Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99646
### status : `NEW`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
typedef float real_t;

#define iterations 100000
#define LEN_1D 32000
#define LEN_2D 256

real_t a[LEN_1D],b[LEN_1D],aa[LEN_2D][LEN_2D];
void main()
{
//    linear dependence testing
//    no dependence - vectorizable

    for (int nl = 0; nl < 2*iterations; nl++) {
        for (int i = 1; i < LEN_1D; i += 2) {
            a[i] = a[i - 1] + b[i];
        }
        dummy();
    }

}

takes 0.73s with -march=native -Ofast -mprefer-avx128 and 0.81s with -march=native -Ofast

128bit version is:
main:
.LFB0:
        .cfi_startproc
        pushq   %rbx
        .cfi_def_cfa_offset 16
        .cfi_offset 3, -16
        movl    $200000, %ebx
.L2:
        xorl    %eax, %eax
        .p2align 4
        .p2align 3
.L4:
        vmovaps a(%rax), %xmm2
        vmovups b+4(%rax), %xmm3
        addq    $32, %rax
        vshufps $136, a-16(%rax), %xmm2, %xmm0
        vshufps $136, b-12(%rax), %xmm3, %xmm1
        vaddps  %xmm1, %xmm0, %xmm0
        vmovss  %xmm0, a-28(%rax)
        vextractps      $1, %xmm0, a-20(%rax)
        vextractps      $2, %xmm0, a-12(%rax)
        vextractps      $3, %xmm0, a-4(%rax)
        cmpq    $127968, %rax
        jne     .L4
        vmovss  b+127972(%rip), %xmm0
        xorl    %eax, %eax
        vaddss  a+127968(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127972(%rip)
        vmovss  a+127976(%rip), %xmm0
        vaddss  b+127980(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127980(%rip)
        vmovss  a+127984(%rip), %xmm0
        vaddss  b+127988(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127988(%rip)
        vmovss  a+127992(%rip), %xmm0
        vaddss  b+127996(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127996(%rip)
        call    dummy


main:
.LFB0:
        .cfi_startproc
        pushq   %rbp
        .cfi_def_cfa_offset 16
        .cfi_offset 6, -16
        movq    %rsp, %rbp
        .cfi_def_cfa_register 6
        pushq   %rbx
        .cfi_offset 3, -24
        movl    $200000, %ebx
        andq    $-32, %rsp
        .p2align 4
        .p2align 3
.L2:
        xorl    %eax, %eax
        .p2align 4
        .p2align 3
.L4:
        vmovaps a(%rax), %ymm4
        vmovups b+4(%rax), %ymm5
        addq    $64, %rax
        vshufps $136, a-32(%rax), %ymm4, %ymm1
        vperm2f128      $3, %ymm1, %ymm1, %ymm2
        vshufps $68, %ymm2, %ymm1, %ymm0
        vshufps $238, %ymm2, %ymm1, %ymm2
        vshufps $136, b-28(%rax), %ymm5, %ymm1
        vinsertf128     $1, %xmm2, %ymm0, %ymm0
        vperm2f128      $3, %ymm1, %ymm1, %ymm2
        vshufps $68, %ymm2, %ymm1, %ymm3
        vshufps $238, %ymm2, %ymm1, %ymm2
        vinsertf128     $1, %xmm2, %ymm3, %ymm1
        vaddps  %ymm1, %ymm0, %ymm0
        vmovss  %xmm0, a-60(%rax)
        vextractps      $1, %xmm0, a-52(%rax)
        vextractps      $2, %xmm0, a-44(%rax)
        vextractps      $3, %xmm0, a-36(%rax)
        vextractf128    $0x1, %ymm0, %xmm0
        vmovss  %xmm0, a-28(%rax)
        vextractps      $1, %xmm0, a-20(%rax)
        vextractps      $2, %xmm0, a-12(%rax)
        vextractps      $3, %xmm0, a-4(%rax)
        cmpq    $127936, %rax
        jne     .L4
        vmovaps a+127936(%rip), %xmm6
        vmovups b+127940(%rip), %xmm7
        xorl    %eax, %eax
        vshufps $136, a+127952(%rip), %xmm6, %xmm0
        vshufps $136, b+127956(%rip), %xmm7, %xmm1
        vaddps  %xmm1, %xmm0, %xmm0
        vmovss  %xmm0, a+127940(%rip)
        vextractps      $1, %xmm0, a+127948(%rip)
        vextractps      $2, %xmm0, a+127956(%rip)
        vextractps      $3, %xmm0, a+127964(%rip)
        vmovss  b+127972(%rip), %xmm0
        vaddss  a+127968(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127972(%rip)
        vmovss  b+127980(%rip), %xmm0
        vaddss  a+127976(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127980(%rip)
        vmovss  b+127988(%rip), %xmm0
        vaddss  a+127984(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127988(%rip)
        vmovss  a+127992(%rip), %xmm0
        vaddss  b+127996(%rip), %xmm0, %xmm0
        vmovss  %xmm0, a+127996(%rip)
        vzeroupper
        call    dummy


---


### compiler : `gcc`
### title : `Converting argument _Complex double to double vector causes STLF stall`
### open_at : `2021-03-19T13:36:17Z`
### last_modified_date : `2023-05-15T07:03:20Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99668
### status : `NEW`
### tags : `missed-optimization`
### component : `middle-end`
### version : `11.0`
### severity : `enhancement`
### contents :
Consider

_Complex double a, b;
typedef double v2df __attribute__((vector_size(16)));
v2df foo (_Complex double x)
{
  return *(v2df *)&x;
}

which on GIMPLE is represented as the simple

v2df foo (complex double x)
{
  v2df _2;
  _2 = VIEW_CONVERT_EXPR<v2df>(x_3(D));
  return _2;
}

but since on x86_64 _Complex double are passed in %xmm0 and %xmm1 we end up
with spilling x to the stack:

foo:
.LFB0:
        .cfi_startproc
        movsd   %xmm0, -24(%rsp)
        movsd   %xmm1, -16(%rsp)
        movupd  -24(%rsp), %xmm0
        ret

Ideally GIMPLE would know that a V_C_E isn't a good representation here
but improving RTL expansion should be possible as well.  We expand
x_3(D) to (concat:DC ...) but the V_C_E expansion path misses special-casing
of this.  The MEM_REF expansion code can deal with this so eventually
splitting out parts of that support is possible.

In the end we want a simple

        movlhps %xmm1, %xmm0


---


### compiler : `gcc`
### title : `Cannot elide aggregate parameter setup`
### open_at : `2021-03-22T15:19:35Z`
### last_modified_date : `2022-02-04T00:50:54Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99712
### status : `RESOLVED`
### tags : `missed-optimization`
### component : `rtl-optimization`
### version : `11.0`
### severity : `normal`
### contents :
On arm32 for

struct X { int a; int b; int c; int d; int e; };

volatile int i;

void bar (struct X);
void foo (struct X x)
{
  i = x.a;
//  i = x.e;
}

we are not able to elide the argument setup generated by RTL expansion:

foo:
        @ args = 20, pretend = 16, frame = 0
        @ frame_needed = 0, uses_anonymous_args = 0
        @ link register save eliminated.
        sub     sp, sp, #16
        sub     ip, sp, #4
        add     sp, sp, #16
        stmib   ip, {r0, r1, r2, r3}
        movw    r3, #:lower16:.LANCHOR0
        movt    r3, #:upper16:.LANCHOR0
        str     r0, [r3]
        bx      lr

note the stmib and all the stack slot setup while we were able to CSE
the "load" to r0.  It looks like the frame setup is not subject to DSE,
it's generated as

(insn 2 4 3 2 (parallel [
            (set (mem/c:SI (reg/f:SI 107 virtual-incoming-args) [2 x+0 S4 A32])
                (reg:SI 0 r0))
            (set (mem/c:SI (plus:SI (reg/f:SI 107 virtual-incoming-args)
                        (const_int 4 [0x4])) [2 x+4 S4 A32])
                (reg:SI 1 r1))
            (set (mem/c:SI (plus:SI (reg/f:SI 107 virtual-incoming-args)
                        (const_int 8 [0x8])) [2 x+8 S4 A32])
                (reg:SI 2 r2))
            (set (mem/c:SI (plus:SI (reg/f:SI 107 virtual-incoming-args)
                        (const_int 12 [0xc])) [2 x+12 S4 A32])
                (reg:SI 3 r3))
        ]) "t.c":7:1 -1
     (nil))
(note 3 2 6 2 NOTE_INSN_FUNCTION_BEG)


---


### compiler : `gcc`
### title : `code pessimization when using wrapper classes around SIMD types`
### open_at : `2021-03-23T13:22:02Z`
### last_modified_date : `2021-07-14T13:04:50Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99728
### status : `ASSIGNED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `10.2.1`
### severity : `normal`
### contents :
Created attachment 50456
test case

I originally reported this at https://gcc.gnu.org/pipermail/gcc-help/2021-March/139976.html, but I'm now fairly confident that this warrants a PR.

The test case needs to be processed on x86_64 with the command

g++ -mfma -O3 -std=c++17 -ffast-math -S testcase.cc

Code for two functions will be generated, and I would expect that the generated assembler for both should be identical. However, for the version using the wrapper class around __m256d, g++ does not seem to recognize the dead stores at the end of the loop and leaves them inside the loop body instead of moving them after the final jump instruction of the loop, which reduces performance considerably.

clang++ generates nearly identical code for both functions and manages to remove the dead stores, so I think that g++ might be able to do better here and is not pessimizing the code due to some C++ intricacies.


---


### compiler : `gcc`
### title : `[11/12 Regression] missing optimization of a repeated conditional`
### open_at : `2021-03-23T20:56:47Z`
### last_modified_date : `2023-05-29T10:04:26Z`
### link : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=99739
### status : `UNCONFIRMED`
### tags : `missed-optimization`
### component : `tree-optimization`
### version : `11.0`
### severity : `normal`
### contents :
Prior to r11-5805 both ff() and gg() in the test case below resulted in optimal code.  With the change, the second conditional in g() is no longer recognized as equivalent to the first and so the function isn't optimized as expected.

Incidentally, the same regression was also introduced once before: in r235653.

$ cat x.c && gcc -O1 -S -Wall -fdump-tree-optimized=/dev/stdout x.c
static inline int f (int i, int j, int k)
{
  int x = 1;

  if (i && j && k)
    x = 2;

  if (i && j && k)
    return x;

  return -1;
}

void ff (int i, int j, int k)
{
  int x = f (i, j, k);
  if (x == 1)
    __builtin_abort ();
}


static inline int g (int i, int j, int k)
{
  int x = 1;

  if (i && j && k)
    x = 2;

  if (i && k && j)
    return x;

  return -1;
}

void gg (int i, int j, int k)
{
  int x = g (i, j, k);
  if (x == 1)
    __builtin_abort ();
}

;; Function ff (ff, funcdef_no=1, decl_uid=1951, cgraph_uid=2, symbol_order=1)

void ff (int i, int j, int k)
{
  <bb 2> [local count: 1073741824]:
  return;

}



;; Function gg (gg, funcdef_no=3, decl_uid=1963, cgraph_uid=4, symbol_order=3)

Removing basic block 6
Removing basic block 7
void gg (int i, int j, int k)
{
  _Bool _7;
  _Bool _8;
  _Bool _11;
  _Bool _14;
  _Bool _16;

  <bb 2> [local count: 1073741824]:
  _7 = i_2(D) != 0;
  _8 = j_3(D) != 0;
  _14 = k_4(D) != 0;
  _11 = _7 & _14;
  _16 = _8 & _11;
  if (_16 != 0)
    goto <bb 3>; [94.27%]
  else
    goto <bb 5>; [5.73%]

  <bb 3> [local count: 1012175616]:
  if (k_4(D) != 0)
    goto <bb 5>; [100.00%]
  else
    goto <bb 4>; [0.00%]

  <bb 4> [count: 0]:
  __builtin_abort ();

  <bb 5> [local count: 1073741824]:
  return;

}


---
